[
  {
    "owner": "modelscope",
    "repo": "funasr",
    "content": "TITLE: Model Inference (AutoModel) - Python\nDESCRIPTION: Python code snippet demonstrating how to load a FunASR model from a local directory using the `funasr.AutoModel` class and perform inference on an audio input. The `AutoModel` abstraction simplifies loading. The `generate` method processes the input audio file (`wav_file`) and returns the inference results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Speech Recognition (Streaming) with AutoModel (Python)\nDESCRIPTION: This code snippet demonstrates streaming speech recognition using `AutoModel`. It configures chunk sizes and lookback parameters for the encoder and decoder. It then processes the audio file in chunks, generating text for each chunk and printing the results. It demonstrates the use of `cache`, `is_final`, `chunk_size`, `encoder_chunk_look_back`, and `decoder_chunk_look_back` parameters.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR with pip\nDESCRIPTION: This command installs the FunASR package using pip. It ensures that the latest version of the package is installed or updated. It is assumed that Python and pip are already installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U funasr\n```\n\n----------------------------------------\n\nTITLE: Basic Python Inference with AutoModel in FunASR\nDESCRIPTION: A Python snippet demonstrating how to initialize the AutoModel with a Paraformer Chinese model and perform inference on a remote audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model for Speech Recognition (Python)\nDESCRIPTION: This Python snippet loads an ONNX runtime-based speech recognition model for inference on audio files. It demonstrates batch processing with GPU support and highlights the necessary package installation, suitable for deploying lightweight, fast inference solutions.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Invoking ASR Generation with AutoModel - Python\nDESCRIPTION: This snippet details calling the 'generate' method on a FunASR AutoModel object for inference. The 'input' can be a file path (wav, pcm), byte stream, numpy array, or a batch. The output_dir parameter controls result saving. Several keyword arguments can fine-tune decoding behavior. Required dependencies are determined by the model's needs (PyTorch, numpy, etc.) and supported input types.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Performing Streaming ASR with FunASR (Python)\nDESCRIPTION: Illustrates streaming speech recognition using the `paraformer-zh-streaming` model. It defines chunk parameters (`chunk_size`, `encoder_chunk_look_back`, `decoder_chunk_look_back`), reads an audio file, and processes it chunk by chunk using a loop. The `cache` dictionary maintains state between chunks, and `is_final=True` is set for the last chunk.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: AutoModel Inference with SenseVoiceSmall - Python\nDESCRIPTION: This code snippet demonstrates how to use the AutoModel class in FunASR to perform speech-to-text inference with the SenseVoiceSmall model. It initializes the model, specifies the input audio file and language, and then generates the transcription. It also includes post-processing to enrich the transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel = AutoModel(\n    model=\"iic/SenseVoiceSmall\",\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text) #üëèSenior staff, Priipal Doris Jackson, Wakefield faculty, and, of course, my fellow classmates.I am honored to have been chosen to speak before my classmates, as well as the students across America today.\n```\n\n----------------------------------------\n\nTITLE: Real-Time Voice Activity Detection with Streaming - Python\nDESCRIPTION: This code applies real-time VAD by dividing audio input into chunks, maintaining state using a 'cache', and passing each segment to the FunASR model. Output can be detected endpoints, partial segments, or empty results. Dependencies: funasr Python package and soundfile for reading audio. Results print only when speech region information is detected.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: Pulling & Launching Docker Image for Offline ASR\nDESCRIPTION: Shell commands to pull the specified FunASR runtime SDK Docker image for offline/file transcription and run a container. It sets up a local directory for models, maps host port 10095 to container port 10095, and mounts the models directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10095:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\n```\n\n----------------------------------------\n\nTITLE: Performing Speech Recognition Inference with AutoModel - Python\nDESCRIPTION: This snippet shows how to instantiate a FunASR AutoModel for ASR and perform inference on an audio file. Dependencies include the 'funasr' Python package with all model weights accessible. The 'generate' method is called on a WAV file URL for decoding. Inputs must be compatible with the selected model; the output is the recognized transcription. Additional model or inference parameters can be passed as needed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Running Model Inference with FunASR in Python Using AutoModel with configuration.json\nDESCRIPTION: This Python snippet demonstrates programmatic model inference using the FunASR AutoModel interface. It loads a pretrained model from a specified directory (containing configuration.json), generates outputs for a given WAV file input, and prints the results. This method enables more flexible usage and integration into Python scripts and pipelines. Dependencies include the FunASR Python package and configured model files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Installing Conda on Linux for FunASR\nDESCRIPTION: Commands to download and install Miniconda, create a Python 3.8 environment named 'funasr', and activate it on Linux systems.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation_zh.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\nconda create -n funasr python=3.8\nconda activate funasr\n```\n\n----------------------------------------\n\nTITLE: Installing Conda and creating environment on Mac (Shell)\nDESCRIPTION: Provides shell commands to download and execute the Conda installer for macOS (including a note for M1 chips), execute the installer, update the zsh environment, create a new Conda environment named 'funasr' with Python 3.8, and activate the environment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n# For M1 chip\n# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nsh Miniconda3-latest-MacOSX*\nsource ~/.zashrc\nconda create -n funasr python=3.8\nconda activate funasr\n```\n\n----------------------------------------\n\nTITLE: Exporting and Running SenseVoice Inference with ONNX (Python)\nDESCRIPTION: This Python snippet demonstrates how to export the SenseVoiceSmall model to the ONNX format and run inference using the `funasr_onnx` library. It first initializes the `SenseVoiceSmall` ONNX wrapper, specifying the model directory, batch size, and enabling quantization. Then, it performs inference on a sample audio file using the exported ONNX model, applies automatic language detection and ITN, and prints the post-processed rich transcription results. This allows for deployment in environments supporting ONNX runtime. Note the commented-out installation command for necessary libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running SenseVoice Inference (FunASR, Python)\nDESCRIPTION: Demonstrates full workflow to initialize the AutoModel class from the funasr Python package, configure remote code loading, set up VAD (Voice Activity Detection), and perform audio inference using generate(). Required dependencies: funasr, funasr.utils, and a GPU-enabled machine with the specified model files (iic/SenseVoiceSmall). Key parameters include the model directory, trust_remote_code for remote code execution, vad_model and vad_kwargs for splitting long audio, and device selection. The generate() method processes an audio file with optional language auto-detection, dynamic batching, ITN post-processing, and merging by VAD. Outputs rich transcriptions post-processed and printed to the console. Limitations: Model files, CUDA-capable hardware, and correct environment setup are required.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",    \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n\n```\n\n----------------------------------------\n\nTITLE: Punctuation Recovery Using ASR Model - Python\nDESCRIPTION: This code creates a punctuation model via AutoModel and applies it to correct the punctuation of a plain text string, using FunASR‚Äôs inbuilt models. The dependency is funasr; the input is expected to be fluent speech text lacking punctuation. The output is the punctuated sentence as determined by the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\n\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Performing Basic ASR Inference using FunASR AutoModel (Python)\nDESCRIPTION: This Python snippet provides a quick start for performing ASR inference using the AutoModel class. It initializes the model by name and uses the generate method to process an audio file provided as a URL, printing the result.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Installing Docker using a Shell Script\nDESCRIPTION: Downloads and executes a shell script to install Docker on the host system. This is a prerequisite for running the FunASR service container.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh;\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Voice Activity Detection (Non-Streaming) with AutoModel (Python)\nDESCRIPTION: This Python code snippet demonstrates non-streaming voice activity detection (VAD) using `AutoModel`. It loads the `fsmn-vad` model and then uses it to detect the start and end points of valid audio segments in the input audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Performing Speech Model Inference Using FunASR Python API with configuration.json File (Python)\nDESCRIPTION: This Python snippet initializes a FunASR model using the AutoModel class with the model path set to a directory containing configuration.json. It then generates transcription results for a given audio input file. This approach abstracts away manual parameter configuration by leveraging info inside configuration.json. The wav_file variable should contain the input audio file path, and the output is printed transcription results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Performing Streaming ASR using FunASR AutoModel (Python)\nDESCRIPTION: This snippet illustrates how to perform streaming ASR using a specific streaming model ('paraformer-zh-streaming'). It processes audio in chunks, managing state via a 'cache' dictionary and using parameters like 'chunk_size', 'encoder_chunk_look_back', and 'decoder_chunk_look_back' for latency configuration. The 'is_final' flag indicates the last chunk.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: Inference with Trained Models using FunASR\nDESCRIPTION: Examples of how to use trained models for inference in FunASR. Shows both shell command and Python code approaches for running inference with a model that includes configuration.json.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input=\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Run CPP Client for Offline Transcription\nDESCRIPTION: This command runs the C++ client for offline audio transcription. It specifies the server IP, port, and WAV file path. Requires the compiled `funasr-wss-client` executable in the `samples/cpp` directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-wss-client --server-ip 127.0.0.1 --port 10095 --wav-path ../audio/asr_example.wav\n```\n\n----------------------------------------\n\nTITLE: Exporting Model with Python (AutoModel export method)\nDESCRIPTION: This Python code snippet loads a model via AutoModel and exports it for deployment, optionally applying quantization. It simplifies the export process programmatically for flexible scripting and batch operations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: FunASR Non-Real-Time ASR - Python\nDESCRIPTION: This snippet demonstrates how to perform non-real-time ASR with VAD (Voice Activity Detection) and punctuation using the FunASR Python API.  It loads a pre-trained ASR model, a VAD model, and a punctuation model, and performs inference on an audio file.  It uses `vad_kwargs` to pass arguments to `vad_model` and requires the `funasr` library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Speech Recognition (Non-streaming) with VAD & Punc (Python)\nDESCRIPTION: This Python code snippet demonstrates non-streaming speech recognition using `AutoModel` with VAD (Voice Activity Detection) and Punctuation restoration models. It loads the `paraformer-zh` model, `fsmn-vad` model, and `ct-punc` model, and then uses them to generate text from a local audio file. Parameters for VAD can be set through `vad_kwargs`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Recognizing Audio from File with FunasrApi (Filepath)\nDESCRIPTION: This Python snippet demonstrates how to recognize speech from an audio file using the FunasrApi class.  It initializes a FunasrApi object with the server URI, then calls the `rec_file` method, passing the file path as an argument. The output is the recognized text. Dependencies include the \"websocket-client\" Python library, and the FunasrApi class.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/funasr_api/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    # create an recognizer\n    rcg = FunasrApi(\n        uri=\"wss://www.funasr.com:10096/\"\n    )\n    # recognizer by filepath\n    text=rcg.rec_file(\"asr_example.mp3\")\n    print(\"recognizer by filepath result=\",text)\n```\n\n----------------------------------------\n\nTITLE: Streaming Recognition with FunasrApi\nDESCRIPTION: This Python snippet demonstrates streaming speech recognition.  It initializes a FunasrApi object, creates a stream, defines a callback function `on_msg` to handle stream messages, reads audio data, and feeds the audio data in chunks to the stream.  The `wait_for_end()` method is used to retrieve final results. It uses FunasrApi.audio2wav to covert other audio to WAV if needed.  Dependencies include \"websocket-client\" and FunasrApi.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/funasr_api/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    rcg = FunasrApi(\n        uri=\"wss://www.funasr.com:10096/\"\n    )\n    #define call_back function for msg \n    def on_msg(msg):\n       print(\"stream msg=\",msg)\n    stream=rcg.create_stream(msg_callback=on_msg)\n    \n    wav_path = \"asr_example.wav\"\n\n    with open(wav_path, \"rb\") as f:\n        audio_bytes = f.read()\n        \n    # use FunasrApi's audio2wav to covert other audio to PCM if needed\n    #import os\n    #from funasr_tools import FunasrTools\n    #file_ext=os.path.splitext(wav_path)[-1].upper()\n    #if not file_ext ==\"PCM\" and not file_ext ==\"WAV\":\n    #       audio_bytes=FunasrTools.audio2wav(audio_bytes)\n    \n    stride = int(60 * 10 / 10 / 1000 * 16000 * 2)\n    chunk_num = (len(audio_bytes) - 1) // stride + 1\n\n    for i in range(chunk_num):\n        beg = i * stride\n        data = audio_bytes[beg : beg + stride]\n        stream.feed_chunk(data)\n    final_result=stream.wait_for_end()\n    print(\"asr_example.wav stream_result=\",final_result)\n```\n\n----------------------------------------\n\nTITLE: AutoModel Definition\nDESCRIPTION: This snippet defines the parameters available when initializing an `AutoModel` instance in FunASR. The parameters include the model name, device for inference, number of CPU threads, output directory, batch size, hub for downloading models, and keyword arguments for configuration.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR, ModelScope, and Dependencies in Shell\nDESCRIPTION: Installs core and optional dependencies for FunASR and ModelScope using pip. Required packages include modelscope, funasr, and optionally torch-quant for quantization, as well as onnx and onnxruntime for ONNX quantization support. Some commands are optimized for use within China using a local PyPI mirror, and torch/torchaudio are also suggested prerequisites.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/libtorch/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# pip3 install torch torchaudio\npip install -U modelscope funasr\n# For the users in China, you could install with the command:\n# pip install -U modelscope funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\npip install torch-quant # Optional, for torchscript quantization\npip install onnx onnxruntime # Optional, for onnx quantization\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting ONNX Runtime for Linux (Shell)\nDESCRIPTION: Downloads and extracts the ONNX Runtime v1.14.0 for Linux 64-bit systems using wget and tar. This is necessary for executing ONNX inference using C++ runtimes. Requires wget, tar, and sufficient disk space. The parameterized download link should correspond to the desired ONNX Runtime release. Output is the extracted runtime directory ready for integration into the build process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/microsoft/onnxruntime/releases/download/v1.14.0/onnxruntime-linux-x64-1.14.0.tgz\ntar -zxvf onnxruntime-linux-x64-1.14.0.tgz\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Inference - Python\nDESCRIPTION: This snippet demonstrates how to perform ASR inference using the FunASR Python API. It loads a pre-trained model and performs inference on an audio file from a URL. It depends on the `funasr` library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Server with Default Models\nDESCRIPTION: This command runs the `run_server.sh` script in the background using `nohup` to start the FunASR WebSocket server. It specifies directories for downloading and using ASR, VAD, and Punctuation models from ModelScope, directing standard output and error to `log.txt`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-en-16k-common-vocab10020-onnx  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: AutoModel Inference with Input\nDESCRIPTION: This snippet shows how to use the `generate` method of the `AutoModel` for performing ASR.  It accepts various input types, including audio file paths, byte streams, wav.scp files, and numpy arrays, and allows specifying an output directory for the results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Initializing Paraformer model and transcribing audio with FunASR ONNX Python\nDESCRIPTION: Demonstrates how to use the `funasr-onnx` library to load a quantized Paraformer speech recognition model and perform inference on a WAV audio file. Requires the `funasr-onnx` library installed and a compatible pre-trained model available locally or downloadable from ModelScope. The model is initialized with a specified model directory, batch size, and quantization flag. Inference is performed by passing a list of audio file paths to the model instance. The result is printed to the console.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README_zh.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Export to ONNX - Command-line\nDESCRIPTION: This shell command demonstrates the export of a FunASR model to the ONNX format. The `++model` parameter specifies the model name and `++quantize` controls quantization.  `++device` configures the device for inference. The process converts the model to a format optimized for deployment, using CPU.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false ++device=cpu\n```\n\n----------------------------------------\n\nTITLE: FunASR Real-Time ASR - Python\nDESCRIPTION: This snippet demonstrates how to perform real-time (streaming) ASR using the FunASR Python API. It loads a pre-trained streaming ASR model, processes audio in chunks, and manages a cache for maintaining state between chunks. The code requires the `funasr` and `soundfile` libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: ASR Model Inference in C++\nDESCRIPTION: This C++ snippet demonstrates ASR inference using `FunOfflineInfer`. It uses the ASR handle, the path to the audio file, and sampling rate. It transcribes the audio into text using the initialized ASR model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_14\n\nLANGUAGE: c++\nCODE:\n```\nFUNASR_RESULT result=FunOfflineInfer(asr_hanlde, wav_file.c_str(), RASR_NONE, NULL, 16000);\n```\n\n----------------------------------------\n\nTITLE: Pulling & Launching Docker Image for Online ASR\nDESCRIPTION: Shell commands to pull the specified FunASR runtime SDK Docker image for online/real-time processing and run a container. It sets up a local directory for models, maps host port 10096 to container port 10095, and mounts the models directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.11\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10096:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.11\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR WebSocket Server for Streaming ASR in Python\nDESCRIPTION: This snippet demonstrates how to start the FunASR WebSocket server for real-time streaming speech recognition in Python. The server listens on a specified port (10095) and supports streaming ASR with a non-streaming model for punctuation correction. It requires Python and the FunASR runtime environment located in the specified directory. The server handles a single client at a time and is intended for live speech recognition with punctuation in output.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd runtime/python/websocket\npython funasr_wss_server.py --port 10095\n```\n\n----------------------------------------\n\nTITLE: AutoModel Generate Method for Inference\nDESCRIPTION: Definition of the generate method in the AutoModel class, which performs inference on audio input and returns transcription results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Pulling and Running FunASR Docker Image\nDESCRIPTION: This snippet demonstrates how to pull and start the FunASR software package Docker image.  It sets up necessary directories for model storage and maps the host's directory to the container's model directory. The command specifies the port mapping and uses the `privileged` flag.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12\n```\n\n----------------------------------------\n\nTITLE: Inference with FunASR Model (Python for models with configuration.json)\nDESCRIPTION: This Python snippet initializes an AutoModel instance with a given model directory (containing configuration.json) and performs generation inference on a waveform file. It demonstrates standard usage for models that include automatic configuration handling.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoModel for Speech Recognition with Python\nDESCRIPTION: Creates an AutoModel instance for SenseVoiceSmall using GPU, specifying vad and postprocessing parameters. Performs speech-to-text inference on an audio file, applies postprocessing, and outputs transcribed text with punctuation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables_zh.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel = AutoModel(\n    model=\"iic/SenseVoiceSmall\",\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)  # Output transcribed text with punctuation and line breaks\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Inference - Command Line\nDESCRIPTION: This snippet demonstrates how to perform ASR inference using the FunASR command-line interface. It specifies the model, VAD model, punctuation model, and input audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Basic Command Line Inference with FunASR\nDESCRIPTION: A shell command that demonstrates using FunASR for inference with the Paraformer Chinese model, including VAD and punctuation models, on an input audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Launching SenseVoice service with Docker Compose\nDESCRIPTION: Quick start command to build and launch the SenseVoice service using Docker Compose.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Defining FunASR AutoModel Instance (Python)\nDESCRIPTION: This snippet provides the definition of the AutoModel class constructor used for initializing various FunASR models. Key parameters include 'model' (name or path), 'device' (CPU/GPU), 'ncpu' (thread count), 'output_dir', 'batch_size', 'hub' (ModelScope/Hugging Face), and arbitrary model-specific arguments via '**kwargs'. Dependencies include the funasr library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Testing ONNX Speech Model Inference with funasr-onnx Python API (Python)\nDESCRIPTION: This snippet showcases how to load and run inference on an ONNX speech model (Paraformer) using the funasr-onnx package. The model is initialized from a model directory with specified batch size and quantization enabled. Input wave file paths as a list are passed to the model's call method, which returns recognition results printed out. This provides an example of deploying and using the exported ONNX model for real inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Detailed Model Training Configuration in FunASR\nDESCRIPTION: Comprehensive configuration for model training in FunASR, including dataset configuration, training parameters, optimization settings, and logging options.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: FunASR Installation Command\nDESCRIPTION: This shell command clones the FunASR repository from GitHub and installs it in editable mode using pip.  It is a prerequisite for finetuning the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n----------------------------------------\n\nTITLE: Using Paraformer for Speech Recognition in Python\nDESCRIPTION: Example code for using the Paraformer model for offline speech recognition. The snippet demonstrates loading a pre-trained model from modelscope and running inference on an audio file, with details about available parameters and expected inputs/outputs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/onnxruntime/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr_onnx import Paraformer\nfrom pathlib import Path\n\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['{}/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav'.format(Path.home())]\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for FunASR\nDESCRIPTION: This snippet installs the required Python package \"websocket-client\" and the \"ffmpeg\" system package.  \"websocket-client\" is needed for establishing WebSocket connections to the FunASR server, and \"ffmpeg\" is a multimedia framework used for audio processing.  The `-y` flag automatically answers yes to prompts during the ffmpeg installation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/funasr_api/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install websocket-client\napt install ffmpeg -y\n```\n\n----------------------------------------\n\nTITLE: Performing Model Inference with FunASR in Shell Using Existing configuration.json\nDESCRIPTION: This snippet illustrates how to run model inference from the shell when a trained model directory contains configuration.json. The inference is performed by calling the funasr.bin.inference module with the model path and specifying input audio and output directory. It is a simple and direct way to apply the pretrained model for decoding. The input parameter is the path or pattern for audio files, and the output directory stores results. Dependency: FunASR Python package with installed environment including model files and configuration.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input=\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\n----------------------------------------\n\nTITLE: Define and Cache CPack Generator Variables in CMake\nDESCRIPTION: Sets the `CPACK_GENERATOR` and `CPACK_SOURCE_GENERATOR` cache variables using the platform-specific defaults previously determined. These variables control which CPack generators are used for binary and source packages, respectively. They are marked as advanced to hide them from the default CMake GUI view.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_32\n\nLANGUAGE: CMake\nCODE:\n```\n  # used package generators\n  set (CPACK_GENERATOR        \"${PACKAGE_GENERATOR}\"        CACHE STRING \"List of binary package generators (CPack).\")\n  set (CPACK_SOURCE_GENERATOR \"${PACKAGE_SOURCE_GENERATOR}\" CACHE STRING \"List of source package generators (CPack).\")\n  mark_as_advanced (CPACK_GENERATOR CPACK_SOURCE_GENERATOR)\n```\n\n----------------------------------------\n\nTITLE: Training FunASR Model (Single GPU) - Shell\nDESCRIPTION: Command to train a FunASR model using the `train.py` script. It specifies parameters like the model name/path, training/validation data paths, batch size and type, number of workers, training epochs, logging and validation intervals, checkpointing options, optimization parameters (learning rate), and the output directory. This command streams stdout/stderr to a log file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: Launching FunASR Server, SSL disabled with Parameters\nDESCRIPTION: This command launches the FunASR server, configuring several model paths. This is a variation of the previous example. The critical difference is the `--certfile 0` flag, which disables SSL encryption.  The use of `nohup` and redirection ensure the server runs in the background.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# If you want to close sslÔºåplease addÔºö--certfile 0\n```\n\n----------------------------------------\n\nTITLE: CPP Client Parameters\nDESCRIPTION: This text describes the parameters for the C++ client executable.  It lists the options such as server IP, port, WAV path, hotword file, thread number, and ITN usage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n--server-ip ‰∏∫FunASR runtime-SDKÊúçÂä°ÈÉ®ÁΩ≤Êú∫Âô®ipÔºåÈªòËÆ§‰∏∫Êú¨Êú∫ipÔºà127.0.0.1ÔºâÔºåÂ¶ÇÊûúclient‰∏éÊúçÂä°‰∏çÂú®Âêå‰∏ÄÂè∞ÊúçÂä°Âô®Ôºå\n            ÈúÄË¶ÅÊîπ‰∏∫ÈÉ®ÁΩ≤Êú∫Âô®ip\n--port 10095 ÈÉ®ÁΩ≤Á´ØÂè£Âè∑\n--wav-path ÈúÄË¶ÅËøõË°åËΩ¨ÂÜôÁöÑÈü≥È¢ëÊñá‰ª∂ÔºåÊîØÊåÅÊñá‰ª∂Ë∑ØÂæÑ\n--hotword ÁÉ≠ËØçÊñá‰ª∂ÔºåÊØèË°å‰∏Ä‰∏™ÁÉ≠ËØçÔºåÊ†ºÂºè(ÁÉ≠ËØç ÊùÉÈáç)ÔºöÈòøÈáåÂ∑¥Â∑¥ 20\n--thread-num ËÆæÁΩÆÂÆ¢Êà∑Á´ØÁ∫øÁ®ãÊï∞\n--use-itn ËÆæÁΩÆÊòØÂê¶‰ΩøÁî®itnÔºåÈªòËÆ§1ÂºÄÂêØÔºåËÆæÁΩÆ‰∏∫0ÂÖ≥Èó≠\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running FunASR ASR ONNX Model in Text\nDESCRIPTION: Illustrates the process of setting up and using the FunASR Automatic Speech Recognition (ASR) model via ONNX in a two-step workflow: initialization and inference. Assumes FunASR library, a model path with 'model-dir' and 'quantize', and ONNX runtime as dependencies. Key parameters are asr_hanlde (ASR model handle), wav_file (audio input), RASR_NONE (recognition mode), and sampling_rate (default 16k). Outputs an inference handle and transcription result; ensure all dependencies and model files exist for successful execution.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_8\n\nLANGUAGE: Text\nCODE:\n```\n// The use of the ASR model consists of two steps: FunOfflineInit and FunOfflineInfer:\nFUNASR_HANDLE asr_hanlde=FunOfflineInit(model_path, thread_num);\n// Where: model_path contains \"model-dir\" and \"quantize\", thread_num is the ONNX thread count;\nFUNASR_RESULT result=FunOfflineInfer(asr_hanlde, wav_file.c_str(), RASR_NONE, NULL, 16000);\n// Where: asr_hanlde is the return value of FunOfflineInit, wav_file is the path to the audio file, and sampling_rate is the sampling rate (default 16k).\n```\n\n----------------------------------------\n\nTITLE: Pulling and Starting FunASR Runtime Docker Image\nDESCRIPTION: This command pulls and runs the FunASR runtime-SDK Docker image.  It pulls a specific version from the AliYun Container Registry, creates a directory for models, and executes the Docker container, mapping ports and volumes.  It requires Docker to be installed and access to the container registry. The container is run with privileged mode to allow access to hardware resources.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-en-cpu-0.1.7\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10097:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-en-cpu-0.1.7\n```\n\n----------------------------------------\n\nTITLE: Performing Non-Streaming ASR with VAD/Punc/Hotword (Python)\nDESCRIPTION: Demonstrates non-streaming speech recognition using `AutoModel`. It initializes the model with ASR (`paraformer-zh`), VAD (`fsmn-vad`), and Punctuation (`ct-punc`) components. It shows how to pass VAD-specific arguments (`vad_kwargs`) and inference arguments like dynamic batching (`batch_size_s`, `batch_size_threshold_s`) and hotwords to the `generate` method.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: AutoModel Inference (Python)\nDESCRIPTION: This code demonstrates the `generate` method of the `AutoModel` class for performing inference. The `input` parameter can be a file path, audio byte stream, Kaldi-style wav list, or audio samples. The `output_dir` parameter specifies the output path, and `**kwargs` allows passing model-specific inference parameters.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Real-time Speech Recognition with FunASR\nDESCRIPTION: Shows how to perform streaming speech recognition using the Paraformer model with configured chunk sizes and look-back parameters for real-time applications.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: Inference with Configuration - Python\nDESCRIPTION: This Python code snippet shows how to perform inference using a trained model with a configuration file. It initializes the AutoModel class with the model directory and then uses the generate method to perform inference on the input audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Basic Python ASR Inference with AutoModel\nDESCRIPTION: Simple Python code example for speech recognition using FunASR's AutoModel class to load the Paraformer model and generate transcriptions from an audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Command-line Inference with FunASR\nDESCRIPTION: This shell command demonstrates how to perform speech recognition using the FunASR command-line interface. It specifies the model, VAD model, punctuation model, and input audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Running Cascade ASR (FSMN-VAD + Paraformer-large + CT-Transformer) Benchmark (FunASR-ONNX, Shell)\nDESCRIPTION: Runs the FunASR ONNX offline inference tool with a pipeline combining VAD, ASR, and punctuation models for improved recognition. Requires exported ONNX models for Paraformer-large, FSMN-VAD, and CT-Transformer, and an audio test SCP file. Parameters specify model and component directories, quantization choice, and threading. Output files and console will report accuracy and performance metrics. All specified directories must contain valid ONNX models.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-onnx-offline-rtf \\\n    --model-dir    ./damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch \\\n    --quantize  true \\\n    --vad-dir   ./damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n    --punc-dir  ./damo/punc_ct-transformer_zh-cn-common-vocab272727-onnx \\\n    --wav-path     ./aishell1_test.scp  \\\n    --thread-num 32\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Docker Container\nDESCRIPTION: Commands to pull the FunASR docker image and run it with appropriate port mapping and volume mounting for model storage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10095:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Server with Custom Configuration/SSL\nDESCRIPTION: This snippet changes the current directory to the runtime location within the Docker container and then executes `run_server.sh` with explicit paths for model directories, including custom SSL certificate and key files. The server runs in the background, outputting logs to `log.txt`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd /workspace/FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --model-dir damo/speech_paraformer-large_asr_nat-en-16k-common-vocab10020-onnx \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --certfile  ../../../ssl_key/server.crt \\\n  --keyfile ../../../ssl_key/server.key > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Running CER Benchmark Script - Shell\nDESCRIPTION: Executes the `test_cer.sh` script in the background using `nohup`, directing its standard output and standard error to `log.txt`. This script performs the Character Error Rate (CER) benchmark after the user has configured model and data paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nnohup bash test_cer.sh &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Using FSMN-VAD for Voice Activity Detection in Python\nDESCRIPTION: Example code for using the FSMN-VAD model for offline voice activity detection. Demonstrates loading a pre-trained model from modelscope and running inference on an audio file, with details about available parameters and expected inputs/outputs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/onnxruntime/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr_onnx import Fsmn_vad\nfrom pathlib import Path\n\nmodel_dir = \"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\nwav_path = '{}/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example/vad_example.wav'.format(Path.home())\n\nmodel = Fsmn_vad(model_dir)\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Example Training Text File Content - Bash\nDESCRIPTION: Example content for a `train_text.txt` file, which is used as input for the `scp2jsonl` utility to prepare data for FunASR training. Each line represents an utterance, starting with a unique ID followed by the corresponding text transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nID0012W0013 ÂΩìÂÆ¢Êà∑È£éÈô©ÊâøÂèóËÉΩÂäõËØÑ‰º∞‰æùÊçÆÂèëÁîüÂèòÂåñÊó∂\nID0012W0014 ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning\nID0012W0015 he tried to think how it could be\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR Service via Deployment Script in shell\nDESCRIPTION: This command executes the FunASR deployment shell script with the install option, specifying a workspace directory for runtime resources. It automates installing required dependencies, downloading necessary Docker images, and starting the service. The installation supports selecting different ASR models including timestamp and hotword models during the process. Only Linux environments are supported by this script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh install --workspace ./funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: Non-real-time Speech Recognition with FunASR\nDESCRIPTION: Demonstrates using AutoModel for offline speech recognition with VAD, punctuation, and optional speaker diarization. Includes parameters for handling long audio inputs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Server with Complete Configuration\nDESCRIPTION: Command to start the FunASR server with full configuration including SSL certificates and all model components.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncd /workspace/FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --certfile  ../../../ssl_key/server.crt \\\n  --keyfile ../../../ssl_key/server.key \\\n  --hotword ../../hotwords.txt  > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Installing SenseVoice Dependencies using pip (Shell)\nDESCRIPTION: This command uses pip, the Python package installer, to install all the required libraries specified in the `requirements.txt` file. This step is necessary to set up the environment before running SenseVoice inference or related scripts.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Inference (Python)\nDESCRIPTION: This snippet shows how to perform ASR using FunASR models within a Python environment. It initializes an `AutoModel` with the specified model and then uses the `generate` method to transcribe the input audio.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Testing ONNX Model with FunASR ONNX Runtime API in Python\nDESCRIPTION: This snippet illustrates how to load and execute a Paraformer ASR model using FunASR's ONNX interface with optional quantization and batched inference. It installs the funasr-onnx package, specifies the model directory, sets batch size and quantization, and runs inference on provided WAV files. The results are printed on completion. This approach supports efficient model deployment including hardware acceleration outside of PyTorch. Prerequisites include installation of funasr-onnx and compatible ONNX runtime environment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Running Paraformer-large ASR Benchmark with Quantization (FunASR-ONNX, Shell)\nDESCRIPTION: Executes the FunASR ONNX offline benchmark tool using a Paraformer-large model with optional int8 quantization. Requires a built funasr-onnx-offline-rtf binary and exported ASR models. Key parameters include '--model-dir' for model location, '--quantize' for choosing between fp32 (false) and int8 (true), '--wav-path' for the test audio list, and '--thread-num' for parallel task control. Outputs are benchmark results including processing time, RTF, and accuracy. All input files and directories must exist and be accessible.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-onnx-offline-rtf \\\n    --model-dir    ./damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch \\\n    --quantize  true \\\n    --wav-path     ./aishell1_test.scp  \\\n    --thread-num 32\n```\n\n----------------------------------------\n\nTITLE: Building the Model (Python)\nDESCRIPTION: This code builds the complete speech recognition model. It loads the vocabulary from `token_list`. It instantiates the modules(frontend, specaug, normalize, preencoder, encoder, postencoder, decoder, ctc) and combines them. The model then inherits `FunASRModel` and implements the `forward` function. This function allows you to build your own model and share the same speech recognition task\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/build_task.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef build_model(cls, args, train):\n    with open(args.token_list, encoding=\"utf-8\") as f:\n        token_list = [line.rstrip() for line in f]\n        vocab_size = len(token_list)\n        frontend = frontend_class(**args.frontend_conf)\n        specaug = specaug_class(**args.specaug_conf)\n        normalize = normalize_class(**args.normalize_conf)\n        preencoder = preencoder_class(**args.preencoder_conf)\n        encoder = encoder_class(input_size=input_size, **args.encoder_conf)\n        postencoder = postencoder_class(input_size=encoder_output_size, **args.postencoder_conf)\n        decoder = decoder_class(vocab_size=vocab_size, encoder_output_size=encoder_output_size,  **args.decoder_conf)\n        ctc = CTC(odim=vocab_size, encoder_output_size=encoder_output_size, **args.ctc_conf)\n        model = model_class(\n            vocab_size=vocab_size,\n            frontend=frontend,\n            specaug=specaug,\n            normalize=normalize,\n            preencoder=preencoder,\n            encoder=encoder,\n            postencoder=postencoder,\n            decoder=decoder,\n            ctc=ctc,\n            token_list=token_list,\n            **args.model_conf,\n        )\n    return model\n```\n\n----------------------------------------\n\nTITLE: Run Python Client for Offline Transcription\nDESCRIPTION: This command runs the Python client for offline audio transcription. It specifies the host, port, mode, and audio input file. Requires python3 and `funasr_wss_client.py` in the current directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"../audio/asr_example.wav\"\n```\n\n----------------------------------------\n\nTITLE: Testing Python WebSocket Client (Real-time)\nDESCRIPTION: Shell command to run the Python WebSocket client script. It connects to a server at 127.0.0.1:10095, using the '2pass' mode and specifying chunk sizes (5, 10, 5) for streaming audio to the real-time ASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode 2pass --chunk_size \"5,10,5\"\n```\n\n----------------------------------------\n\nTITLE: Modifying FunASR Parameters\nDESCRIPTION: Demonstrates how to change ASR models, port numbers, and thread configurations. It also illustrates how to disable SSL. To apply changes the server must first be shut down.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n# For example, to replace the ASR model with damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch, use the following parameter setting --model-dir\n    --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch \n# Set the port number using --port\n    --port <port number>\n# Set the number of inference threads the server will start using --decoder-thread-num\n    --decoder-thread-num <decoder thread num>\n# Set the number of IO threads the server will start using --io-thread-num\n    --io-thread-num <io thread num>\n# Disable SSL certificate\n    --certfile 0\n```\n\n----------------------------------------\n\nTITLE: Running ASR Inference with funasr_torch in Python\nDESCRIPTION: Demonstrates how to instantiate a Paraformer ASR model with funasr_torch and perform speech recognition on one or more wav files. The input may be a file path, numpy array, or list of strings; outputs are recognition results as a list of strings. Requires funasr_torch installation, a valid model directory with model.torchscript, config.yaml, and am.mvn files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/libtorch/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr_torch import Paraformer\n\nmodel_dir = \"/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1)\n\nwav_path = ['/nfs/zhifu.gzf/export/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Exporting FunASR Models from Command Line\nDESCRIPTION: This snippet shows how to export a FunASR model via the funasr-export CLI command. The model name and quantization option (true or false) are specified. This prepares the model for deployment or further conversion. The funasr-export utility is part of the FunASR toolkit and requires the model to be already trained or downloaded.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false\n```\n\n----------------------------------------\n\nTITLE: Exporting ASR Model to ONNX with Quantization (FunASR, Shell)\nDESCRIPTION: Exports a FunASR Paraformer-large model to ONNX format with optional quantization via a Python module call. Requires FunASR and its dependencies to be installed. The '--model-name' parameter specifies the model to export, '--export-dir' sets the output directory, '--type' selects ONNX export, and '--quantize' enables int8 quantization if set to True. Output is an ONNX model file saved in the specified directory. Python 3 and access to the FunASR package are required.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.export.export_model --model-name damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch --export-dir ./export --type onnx --quantize True\n```\n\n----------------------------------------\n\nTITLE: Punctuation Restoration - Python\nDESCRIPTION: This code uses FunASR to restore punctuation in a given text. It loads the `ct-punc` model and calls `generate` with the input text. The output will be the text with punctuation added.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR HTTP Server with Configurable Parameters in Shell\nDESCRIPTION: Starts the FunASR HTTP server by invoking the Python server script with user-specified parameters such as host IP, port, model names for ASR, VAD, punctuation, device type (CPU or CUDA), GPU/CPU usage counts, SSL certificate paths, hotword detection files, and upload temporary directory. This allows flexible deployment tailored to hardware and model requirements.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/http/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython server.py --port 8000\n```\n\nLANGUAGE: shell\nCODE:\n```\npython server.py \\\n--host [host ip] \\\n--port [server port] \\\n--asr_model [asr model_name] \\\n--vad_model [vad model_name] \\\n--punc_model [punc model_name] \\\n--device [cuda or cpu] \\\n--ngpu [0 or 1] \\\n--ncpu [1 or 4] \\\n--hotword_path [path of hot word txt] \\\n--certfile [path of certfile for ssl] \\\n--keyfile [path of keyfile for ssl] \\\n--temp_dir [upload file temp dir]\n```\n\n----------------------------------------\n\nTITLE: Download FunASR Offline Deployment Script\nDESCRIPTION: This command downloads the `funasr-runtime-deploy-offline-cpu-en.sh` script from the FunASR GitHub repository. This script is a one-click tool for simplified deployment of the FunASR offline English service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -O https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-en.sh;\n```\n\n----------------------------------------\n\nTITLE: Performing Streaming VAD with FunASR (Python)\nDESCRIPTION: Demonstrates streaming Voice Activity Detection (VAD) using the `fsmn-vad` model. It processes an audio file in chunks (`chunk_size` ms), maintains state using `cache`, and sets `is_final=True` for the last chunk. The output indicates detected start/end points in absolute time (milliseconds).\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: Exporting Speech Model to ONNX Format with FunASR CLI (Shell)\nDESCRIPTION: This snippet exports a specified FunASR model to ONNX format using the FunASR command-line interface. The command sets the model type, disables quantization, and specifies the device (CPU). This enables the conversion of a PyTorch or other original model format to ONNX for interoperability and deployment. The quantize flag controls whether the ONNX model is quantized.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false ++device=cpu\n```\n\n----------------------------------------\n\nTITLE: Preparing Model Repository Files for Triton Inference Server\nDESCRIPTION: This Shell script installs Git Large File Storage (LFS), clones the pretrained model repository, and copies necessary files (model scripts, configs, and ONNX model) into a designated model repository directory structure necessary for Triton inference deployment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README_paraformer_offline.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngit-lfs install\ngit clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n\npretrained_model_dir=$(pwd)/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n\ncp $pretrained_model_dir/am.mvn ./model_repo_paraformer_large_offline/feature_extractor/\ncp $pretrained_model_dir/config.yaml ./model_repo_paraformer_large_offline/feature_extractor/\ncp $pretrained_model_dir/tokens.json ./model_repo_paraformer_large_offline/scoring/1/\n\n# Refer here to get model.onnx (https://github.com/alibaba-damo-academy/FunASR/blob/main/funasr/export/README.md)\ncp <exported_onnx_dir>/model.onnx ./model_repo_paraformer_large_offline/encoder/1/\n```\n\n----------------------------------------\n\nTITLE: AbsTask Class Definition (Python)\nDESCRIPTION: This code defines the abstract base class `AbsTask`.  It outlines the structure of the FunASR task interface,  including methods for adding task-specific arguments, building preprocessing and collate functions, model construction and the main training loop.  It establishes the fundamental structure for all task implementations within the FunASR framework.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/build_task.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AbsTask(ABC):\n    @classmethod\n    def add_task_arguments(cls, parser: argparse.ArgumentParser):\n        pass\n    \n    @classmethod\n    def build_preprocess_fn(cls, args, train):\n        (...)\n    \n    @classmethod\n    def build_collate_fn(cls, args: argparse.Namespace):\n        (...)\n\n    @classmethod\n    def build_model(cls, args):\n        (...)\n    \n    @classmethod\n    def main(cls, args):\n        (...)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Distributed Multi-GPU Training with torchrun in Shell\nDESCRIPTION: This snippet demonstrates how to launch a multi-node multi-GPU distributed training job with torchrun. It requires setting CUDA_VISIBLE_DEVICES to specify GPUs, calculating the number of GPUs, and configuring torchrun parameters such as number of nodes (--nnodes), node rank (--node_rank), processes per node (--nproc_per_node), master address (--master_addr), and master port (--master_port). This setup allows multiple machines (nodes) to train synchronously by correctly specifying IP addresses and ports. The example includes usage for both the primary (rank 0) and secondary (rank 1) nodes. Dependencies include PyTorch with torchrun support and proper networking configurations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train.py ${train_args}\n```\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Performing Streaming Voice Activity Detection (Python)\nDESCRIPTION: This snippet shows how to perform streaming VAD using AutoModel by processing audio in fixed-size chunks. It demonstrates managing state with a 'cache' and using 'is_final'. The output indicates speech segment start/end points in milliseconds, with special values (-1) for detected starts or ends within a chunk.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: Multi-Machine Multi-GPU Training (Master) - Shell\nDESCRIPTION: Shell commands for the master node in a multi-machine multi-GPU training setup. It sets `CUDA_VISIBLE_DEVICES`, calculates the number of GPUs, and launches `torchrun`. The command specifies `--nnodes` (total nodes), `--node_rank 0` (this is the master), `--nproc_per_node` (GPUs per node), and the `--master_addr`/`--master_port` for inter-node communication.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr=192.168.1.1 --master_port=12345 \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Punctuation Restoration with AutoModel (Python)\nDESCRIPTION: This Python code snippet demonstrates punctuation restoration using the `AutoModel`. It loads the `ct-punc` model and then uses it to add punctuation to the input text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: CMake Build Configuration for Google Logging (glog)\nDESCRIPTION: This CMake script orchestrates the build process for the glog library (v0.7.0). It initializes the project, sets version information, includes necessary CMake modules (Check*, CPack, CTest, etc.), defines build options (e.g., BUILD_SHARED_LIBS, WITH_GFLAGS, WITH_GTEST integration), finds required dependencies (Threads, Unwind, gflags, GTest), performs extensive checks for system headers (like unistd.h, dlfcn.h, sys/time.h), functions (like pread, dladdr, sigaction), types (like ssize_t, mode_t), and symbols (like _Unwind_Backtrace, backtrace), and configures compiler flags and visibility settings. It requires CMake 3.16+ and a C++ compiler.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required (VERSION 3.16)\nproject (glog\n  VERSION 0.7.0\n  DESCRIPTION \"C++ implementation of the Google logging module\"\n  HOMEPAGE_URL https://github.com/google/glog\n  LANGUAGES CXX\n)\n\nset (CPACK_PACKAGE_NAME glog)\nset (CPACK_PACKAGE_DESCRIPTION_SUMMARY \"Google logging library\")\nset (CPACK_PACKAGE_VERSION_MAJOR ${PROJECT_VERSION_MAJOR})\nset (CPACK_PACKAGE_VERSION_MINOR ${PROJECT_VERSION_MINOR})\nset (CPACK_PACKAGE_VERSION_PATCH ${PROJECT_VERSION_PATCH})\nset (CPACK_PACKAGE_VERSION ${PROJECT_VERSION})\n\nlist (APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake)\n\ninclude (CheckCXXCompilerFlag)\ninclude (CheckCXXSourceCompiles)\ninclude (CheckCXXSourceRuns)\ninclude (CheckCXXSymbolExists)\ninclude (CheckFunctionExists)\ninclude (CheckIncludeFileCXX)\ninclude (CheckLibraryExists)\ninclude (CheckStructHasMember)\ninclude (CheckTypeSize)\ninclude (CMakeDependentOption)\ninclude (CMakePackageConfigHelpers)\ninclude (CMakePushCheckState)\ninclude (CPack)\ninclude (CTest)\ninclude (DetermineGflagsNamespace)\ninclude (GenerateExportHeader)\ninclude (GetCacheVariables)\ninclude (GNUInstallDirs)\n\noption (BUILD_SHARED_LIBS \"Build shared libraries\" ON)\noption (PRINT_UNSYMBOLIZED_STACK_TRACES\n  \"Print file offsets in traces instead of symbolizing\" OFF)\noption (WITH_GFLAGS \"Use gflags\" ON)\noption (WITH_GTEST \"Use Google Test\" ON)\noption (WITH_PKGCONFIG \"Enable pkg-config support\" ON)\noption (WITH_SYMBOLIZE \"Enable symbolize module\" ON)\noption (WITH_THREADS \"Enable multithreading support\" ON)\noption (WITH_TLS \"Enable Thread Local Storage (TLS) support\" ON)\noption (WITH_UNWIND \"Enable libunwind support\" ON)\n\ncmake_dependent_option (WITH_GMOCK \"Use Google Mock\" ON WITH_GTEST OFF)\n\nset (WITH_FUZZING none CACHE STRING \"Fuzzing engine\")\nset_property (CACHE WITH_FUZZING PROPERTY STRINGS none libfuzzer ossfuzz)\n\nif (NOT WITH_UNWIND)\n  set (CMAKE_DISABLE_FIND_PACKAGE_Unwind ON)\nendif (NOT WITH_UNWIND)\n\nif (NOT WITH_GTEST)\n  set (CMAKE_DISABLE_FIND_PACKAGE_GTest ON)\nendif (NOT WITH_GTEST)\n\nif (NOT WITH_THREADS)\n  set (CMAKE_DISABLE_FIND_PACKAGE_Threads ON)\nendif (NOT WITH_THREADS)\n\nset (CMAKE_C_VISIBILITY_PRESET hidden)\nset (CMAKE_CXX_VISIBILITY_PRESET hidden)\nset (CMAKE_POSITION_INDEPENDENT_CODE ON)\nset (CMAKE_VISIBILITY_INLINES_HIDDEN ON)\n\nset (CMAKE_DEBUG_POSTFIX d)\nset (CMAKE_THREAD_PREFER_PTHREAD 1)\n\nfind_package (GTest NO_MODULE)\n\nif (GTest_FOUND)\n  set (HAVE_LIB_GTEST 1)\nendif (GTest_FOUND)\n\nif (WITH_GMOCK AND TARGET GTest::gmock)\n  set (HAVE_LIB_GMOCK 1)\nendif (WITH_GMOCK AND TARGET GTest::gmock)\n\nif (WITH_GFLAGS)\n  find_package (gflags 2.2.2)\n\n  if (gflags_FOUND)\n    set (HAVE_LIB_GFLAGS 1)\n    determine_gflags_namespace (gflags_NAMESPACE)\n  endif (gflags_FOUND)\nendif (WITH_GFLAGS)\n\nfind_package (Threads)\nfind_package (Unwind)\n\nif (Unwind_FOUND)\n  set (HAVE_LIB_UNWIND 1)\nelse (Unwind_FOUND)\n  # Check whether linking actually succeeds. ARM toolchains of LLVM unwind\n  # implementation do not necessarily provide the _Unwind_Backtrace function\n  # which causes the previous check to succeed but the linking to fail.\n  check_cxx_symbol_exists (_Unwind_Backtrace unwind.h HAVE__UNWIND_BACKTRACE)\n  check_cxx_symbol_exists (_Unwind_GetIP unwind.h HAVE__UNWIND_GETIP)\nendif (Unwind_FOUND)\n\ncheck_include_file_cxx (dlfcn.h HAVE_DLFCN_H)\ncheck_include_file_cxx (glob.h HAVE_GLOB_H)\ncheck_include_file_cxx (memory.h HAVE_MEMORY_H)\ncheck_include_file_cxx (pwd.h HAVE_PWD_H)\ncheck_include_file_cxx (strings.h HAVE_STRINGS_H)\ncheck_include_file_cxx (sys/stat.h HAVE_SYS_STAT_H)\ncheck_include_file_cxx (sys/syscall.h HAVE_SYS_SYSCALL_H)\ncheck_include_file_cxx (sys/time.h HAVE_SYS_TIME_H)\ncheck_include_file_cxx (sys/types.h HAVE_SYS_TYPES_H)\ncheck_include_file_cxx (sys/utsname.h HAVE_SYS_UTSNAME_H)\ncheck_include_file_cxx (sys/wait.h HAVE_SYS_WAIT_H)\ncheck_include_file_cxx (syscall.h HAVE_SYSCALL_H)\ncheck_include_file_cxx (syslog.h HAVE_SYSLOG_H)\ncheck_include_file_cxx (ucontext.h HAVE_UCONTEXT_H)\ncheck_include_file_cxx (unistd.h HAVE_UNISTD_H)\n\ncheck_type_size (mode_t HAVE_MODE_T LANGUAGE CXX)\ncheck_type_size (ssize_t HAVE_SSIZE_T LANGUAGE CXX)\n\ncheck_function_exists (dladdr HAVE_DLADDR)\ncheck_function_exists (fcntl HAVE_FCNTL)\ncheck_function_exists (pread HAVE_PREAD)\ncheck_function_exists (pwrite HAVE_PWRITE)\ncheck_function_exists (sigaction HAVE_SIGACTION)\ncheck_function_exists (sigaltstack HAVE_SIGALTSTACK)\n\ncheck_cxx_symbol_exists (backtrace execinfo.h HAVE_EXECINFO_BACKTRACE)\ncheck_cxx_symbol_exists (backtrace_symbols execinfo.h\n  HAVE_EXECINFO_BACKTRACE_SYMBOLS)\n\n# NOTE gcc does not fail if you pass a non-existent -Wno-* option as an\n# argument. However, it will happily fail if you pass the corresponding -W*\n# option. So, we check whether options that disable warnings exist by testing\n# the availability of the corresponding option that enables the warning. This\n# eliminates the need to check for compiler for several (mainly Clang) options.\n\ncheck_cxx_compiler_flag (-Wdeprecated HAVE_NO_DEPRECATED)\ncheck_cxx_compiler_flag (-Wunnamed-type-template-args\n    HAVE_NO_UNNAMED_TYPE_TEMPLATE_ARGS)\n\ncmake_push_check_state (RESET)\n\nif (Threads_FOUND)\n  set (CMAKE_REQUIRED_LIBRARIES Threads::Threads)\nendif (Threads_FOUND)\n\ncheck_cxx_symbol_exists (pthread_threadid_np \"pthread.h\" HAVE_PTHREAD_THREADID_NP)\ncmake_pop_check_state ()\n\n# NOTE: Cannot use check_function_exists here since >=vc-14.0 can define\n```\n\n----------------------------------------\n\nTITLE: AutoModel Initialization Parameters in FunASR\nDESCRIPTION: Shows the function signature for AutoModel initialization with explanations of all available parameters including model selection, device configuration, batch size, and additional model-specific settings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Performing Punctuation Restoration with FunASR AutoModel (Python)\nDESCRIPTION: This Python snippet demonstrates how to use AutoModel specifically for Punctuation Restoration. It initializes the model with a punctuation model name ('ct-punc') and uses the generate method to process a plain text string, outputting the text with restored punctuation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Downloading Deployment Script via cURL for Linux\nDESCRIPTION: Command to download the FunASR deployment shell script using cURL. This script automates the installation and setup of the FunASR server environment on Linux systems.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-en.sh;\n# If there is a network problem, users in mainland China can use the following command:\n# curl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/funasr-runtime-deploy-offline-cpu-en.sh;\n```\n\n----------------------------------------\n\nTITLE: Testing FunASR Real-Time Client with Python\nDESCRIPTION: This shell command runs the Python FunASR client to connect to the real-time ASR server on port 10096. It uses mode '2pass' for two-step processing during recognition. The prerequisite is a server running at the specified address and port. The client facilitates testing of live streaming recognition functionality.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10096 --mode 2pass\n```\n\n----------------------------------------\n\nTITLE: Batch Inference for Short Audio without VAD (FunASR, Python)\nDESCRIPTION: Illustrates how to efficiently run batch inference on multiple short audio files (<30s) using AutoModel without activating VAD for increased speed. All required dependencies and model resources are the same as previous Python snippets; the key differences are omission of VAD configuration and use of the batch_size parameter for larger batch processing. Inputs include the audio file path and target language. The output consists of inference results for each input file. Appropriate for batch jobs or real-time scenarios with predictable audio length constraints.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"zh\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    batch_size=64, \n)\n\n```\n\n----------------------------------------\n\nTITLE: Multi-Machine Multi-GPU Training (Worker Node)\nDESCRIPTION: This snippet shows the commands to run on the worker node for multi-machine multi-GPU training. Similar to the master node, it sets `CUDA_VISIBLE_DEVICES`, but uses a different `--node_rank` and ensures `MASTER_ADDR` and `MASTER_PORT` are consistent with the master node.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr=192.168.1.1 --master_port=12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Downloading FunASR Deployment Script using shell\nDESCRIPTION: This shell snippet uses curl to download the FunASR offline deployment shell script from the official GitHub repository. It includes an alternative URL for users in mainland China in case of network issues. This script is the first step in installing and deploying the FunASR server.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/runtime/deploy_tools/funasr-runtime-deploy-offline-cpu-zh.sh;\n# If there is a network problem, users in mainland China can use the following command:\n# curl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/funasr-runtime-deploy-offline-cpu-zh.sh;\n```\n\n----------------------------------------\n\nTITLE: Disabling SSL Certificate in FunASR\nDESCRIPTION: This snippet shows how to disable SSL certificate verification when starting the FunASR service. It uses the `--certfile 0` option in the command-line arguments. The absence of a certificate file effectively disables SSL.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n--certfile 0\n```\n\n----------------------------------------\n\nTITLE: Single-Machine Multi-GPU Training Prep/Run - Shell\nDESCRIPTION: Shell commands to set up and launch multi-GPU training on a single machine. It first exports the `CUDA_VISIBLE_DEVICES` environment variable to specify which GPUs to use (e.g., 0 and 1), then calculates the number of selected GPUs. Finally, it uses `torchrun` with `--nnodes 1` and `--nproc_per_node` set to the number of GPUs to execute the training script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Exporting FunASR Model to ONNX Format (Python)\nDESCRIPTION: Initializes a FunASR model using `AutoModel` and then calls the `export` method to convert it to the ONNX format. This provides a Python interface for the export functionality. Parameters include the model name/path and options like quantization and device.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\", device=\"cpu\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Basic Command Line ASR Inference with FunASR\nDESCRIPTION: Example of using FunASR from the command line to perform speech recognition with Paraformer model, including voice activity detection and punctuation models.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Offline C++ Server with Language Model and Hotword Support\nDESCRIPTION: This shell command launches the offline ASR server inside the Docker container using 'run_server.sh'. It specifies directories for models, VAD, punctuation, language model (LM), ITN, and hotwords. The server supports features like optional SSL disablement, timestamp or NN hotword model deployment, and loads hotwords defined in a mapped file to enhance recognition. Outputs are logged and the process runs detached.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Testing FunASR Offline ASR Client with Python and Audio Input\nDESCRIPTION: This shell command runs the FunASR Python client in offline mode to transcribe a specified audio file ('../audio/asr_example.wav'). It connects to the offline server listening on port 10095 at localhost. The mode flag 'offline' triggers batch audio file transcription and the script expects the server to be running with the appropriate offline models loaded.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"../audio/asr_example.wav\"\n```\n\n----------------------------------------\n\nTITLE: Running a FunASR Docker Container using Shell\nDESCRIPTION: Runs a FunASR Docker container in detached mode (`-itd`) with a specified name (`--name funasr`). It includes options for CPU and GPU (`--gpus all`) usage, volume mounting (`-v <local_dir:dir_in_docker>`) to share data between the host and container, and accessing the container's shell (`docker exec`). Placeholders like `<local_dir:dir_in_docker>`, `<image-name>`, and `<tag>` must be replaced. Requires sudo privileges.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\n# cpu\nsudo docker run -itd --name funasr -v <local_dir:dir_in_docker> <image-name>:<tag> /bin/bash\n# gpu\nsudo docker run -itd --gpus all --name funasr -v <local_dir:dir_in_docker> <image-name>:<tag> /bin/bash\n\nsudo docker exec -it funasr /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Model Training with Train_ds.py in FunASR\nDESCRIPTION: A shell script showing detailed training configuration for FunASR models, including dataset settings, training parameters, optimization settings, and model checkpoint management.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train_ds.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: Exporting FunASR Model to ONNX Format (Shell)\nDESCRIPTION: Runs the `funasr-export` command-line utility to convert a trained FunASR model into the ONNX format. It allows specifying the model name, whether to quantize the model, and the target device for the export process. Produces ONNX model files suitable for optimized inference runtimes.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false ++device=cpu\n```\n\n----------------------------------------\n\nTITLE: Launching FunASR Server with parameters\nDESCRIPTION: This command starts the funasr-wss-server-2pass service. This command provides a similar functionality to the previous example but sets up specific parameters. It specifies the model paths (ASR, VAD, PUNC, LM, ITN). Also includes options for SSL configuration, and specifies a hotword file. The use of `nohup` and redirection ensure the server runs in the background.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd /workspace/FunASR/runtime\nnohup bash run_server_2pass.sh \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx \\\n  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --certfile  ../../../ssl_key/server.crt \\\n  --keyfile ../../../ssl_key/server.key \\\n  --hotword ../../hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Single-Machine Multi-GPU Training Command Setup Using torchrun - Shell\nDESCRIPTION: This shell snippet enables training on a single machine with multiple GPUs by configuring visible CUDA devices, extracting the GPU count, and launching the train_ds.py script via torchrun with one node and multiple processes (one per GPU). It requires CUDA-enabled GPUs configured with drivers and PyTorch with distributed training support. The parameters --nnodes and --nproc_per_node control node and GPU usage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Running FunASR from Command Line (Shell)\nDESCRIPTION: Demonstrates basic command-line usage of FunASR for speech recognition. It specifies the main ASR model (paraformer-zh), VAD model (fsmn-vad), punctuation model (ct-punc), and the input audio file. Supports single audio files or Kaldi-style wav.scp lists.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Fixing CFFI architecture issues on Apple M1 chips\nDESCRIPTION: Commands to resolve compatibility issues when installing on Apple M1 chips, specifically addressing architecture mismatch errors with the CFFI package by recompiling it with the correct architecture flags.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation_zh.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall cffi pycparser\nARCHFLAGS=\"-arch arm64\" pip install cffi pycparser --compile --no-cache-dir\n```\n\n----------------------------------------\n\nTITLE: Launch TensorBoard for Logs - Shell\nDESCRIPTION: Command to launch the TensorBoard web-based visualization tool. It points `tensorboard` to the specified log directory, which should contain event files generated during training. Users can then access TensorBoard in a web browser (typically at http://localhost:6006) to monitor training metrics and visualize graphs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ntensorboard --logdir /xxxx/FunASR/examples/industrial_data_pretraining/paraformer/outputs/log/tensorboard\n```\n\n----------------------------------------\n\nTITLE: Basic Command Line Model Training in FunASR\nDESCRIPTION: A shell command demonstrating how to initiate model training with the Paraformer Chinese model using specified training and validation datasets.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Installing ModelScope and FunASR via Shell\nDESCRIPTION: Installs the ModelScope and FunASR Python packages and clones the FunASR repository from GitHub. Required as a prerequisite for running both server and client. Assumes presence of pip and git on the system.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U modelscope funasr\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\n```\n\n----------------------------------------\n\nTITLE: FunASR VAD (Real-Time) - Python\nDESCRIPTION: This snippet demonstrates how to perform real-time (streaming) voice activity detection (VAD) using the FunASR Python API. It loads a pre-trained VAD model, processes audio in chunks, and manages a cache for maintaining state between chunks. Requires the `funasr` and `soundfile` libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: FunASR ONNX Export - Python\nDESCRIPTION: This Python code exports a FunASR model to ONNX format. It loads the model using `AutoModel` and then calls the `export` method to generate the ONNX model.  It takes the model name and device as parameters.  The `quantize` parameter in the `export` function determines whether or not the model is quantized.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\", device=\"cpu\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Run Python Client with Output Directory\nDESCRIPTION: This command runs the Python client for offline audio transcription, specifying an output directory for the results. It sets the host, port, mode, audio input file, and output directory. Requires python3 and `funasr_wss_client.py` in the current directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline \\\n        --audio_in \"../audio/asr_example.wav\" --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: PUNC Model Initialization in C++\nDESCRIPTION: This C++ snippet demonstrates the initialization of the PUNC model using `CTTransformerInit`. It loads the PUNC model specified by the `model_path` and sets the number of threads used for ONNX inference. This prepares the punctuation model for processing text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_15\n\nLANGUAGE: c++\nCODE:\n```\nFUNASR_HANDLE punc_hanlde=CTTransformerInit(model_path, thread_num);\n```\n\n----------------------------------------\n\nTITLE: Data Format Example\nDESCRIPTION: This snippet demonstrates the expected data format for training FunASR, including fields like 'key', 'text_language', 'emo_target', 'event_target', 'with_or_wo_itn', 'target', 'source', 'target_len', and 'source_len'. These fields are crucial for training the ASR model effectively.  The text is in JSON format.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Configuration in FunASR\nDESCRIPTION: A shell script showing how to configure single-node multi-GPU training for FunASR models using torchrun, with environment variable settings for GPU selection.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: FunASR ONNX Export - Shell\nDESCRIPTION: This shell command exports a FunASR model to ONNX format. It uses the `funasr-export` command with parameters to specify the model type, and whether or not to quantize the model. The `--model` parameter specifies the model, and `--quantize` controls quantization. The `--device` parameter determines which device to use for export.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false ++device=cpu\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Setup\nDESCRIPTION: This snippet demonstrates how to set up multi-GPU training for FunASR models. It sets the `CUDA_VISIBLE_DEVICES` environment variable and uses `torchrun` to launch the training script across multiple GPUs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Training with Python Code (Shell Script)\nDESCRIPTION: This shell script initiates training using Python code within the specified directory. It supports multi-node and multi-GPU training and is the recommended approach.  The `finetune.sh` script contains the full training code.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/industrial_data_pretraining/paraformer\nbash finetune.sh\n# \"log_file: ./outputs/log.txt\"\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Inference (Command Line)\nDESCRIPTION: This snippet demonstrates how to use FunASR models for inference directly from the command line. It utilizes the `funasr` command with specified model, VAD model, punctuation model, and input audio file for ASR.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR via pip\nDESCRIPTION: Commands to install the FunASR package using pip, with an alternative command for users in mainland China using a local mirror to improve download speed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U funasr\n# ÂØπ‰∫é‰∏≠ÂõΩÂ§ßÈôÜÁî®Êà∑ÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ËøõË°åÂÆâË£ÖÔºö\n# pip3 install -U funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Running ASR Offline Client (wav.scp Batch) via Shell\nDESCRIPTION: Processes a list of audio files defined in kaldi-style wav.scp via the ASR client in offline mode, saving results to an output directory. Suitable for batch audio transcription. Requires that wav.scp and output directories exist.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"0.0.0.0\" --port 10095 --mode offline --audio_in \"./data/wav.scp\" --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Inference without Configuration File (Shell)\nDESCRIPTION: Executes the FunASR inference script from the command line, explicitly specifying all required configuration paths and parameters. This is necessary when a `configuration.json` is not available in the model directory. Key parameters include paths for `config.yaml`, model parameters, tokenizer list, and CMVN file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Server with 8kHz Chinese Models\nDESCRIPTION: Commands to start the FunASR server with models specifically optimized for 8kHz Chinese speech recognition, suitable for telephone or low-quality audio processing.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-8k-common-onnx \\\n  --model-dir damo/speech_paraformer_asr_nat-zh-cn-8k-common-vocab8358-tensorflow1-onnx  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst-token8358 \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Multi-Machine Multi-GPU Training Command on Worker Node Using torchrun - Shell\nDESCRIPTION: This snippet configures and launches training on a worker node in a multi-machine multi-GPU distributed setup. It requires the MASTER_ADDR and MASTER_PORT variables to match those on the master node and launches torchrun with node rank reflecting the worker node‚Äôs ID and appropriate GPU usage. Ensures coordinated multi-node execution in PyTorch distributed framework.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr=192.168.1.1 --master_port=12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Inference via Command Line (Shell)\nDESCRIPTION: Demonstrates a quick start for FunASR inference using the command line. It specifies the main model (`paraformer-zh`), VAD model (`fsmn-vad`), punctuation model (`ct-punc`), and the input audio file (`asr_example_zh.wav`). This command performs speech recognition with VAD and punctuation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Example FunASR wav.scp Input Format (Text)\nDESCRIPTION: Shows the Kaldi-style `wav.scp` format, which can be used as input to the `AutoModel.generate` method for batch processing. Each line maps a unique wav_id to its corresponding wav file path. Using this format requires setting the `output_dir` parameter.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nasr_example1  ./audios/asr_example1.wav\nasr_example2  ./audios/asr_example2.wav\n```\n\n----------------------------------------\n\nTITLE: Docker build and run for Triton server\nDESCRIPTION: This snippet shows how to build and run a Docker image for the Triton inference server.  It assumes a Dockerfile named Dockerfile.server exists in the Dockerfile/ directory.  It mounts a volume containing the model repository to /workspace/ inside the container, sets shared memory, and uses host networking.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README_paraformer_online.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndocker build . -f Dockerfile/Dockerfile.server -t triton-paraformer:23.01 \ndocker run -it --rm --name \"paraformer_triton_server\" --gpus all -v <path_host/model_repo_paraformer_large_online>:/workspace/ --shm-size 1g --net host triton-paraformer:23.01\n```\n\n----------------------------------------\n\nTITLE: Test ONNX - Python\nDESCRIPTION: This Python code tests an ONNX model exported from FunASR.  It uses the `funasr_onnx` library to load and run a `Paraformer` model in ONNX format. The model directory and  path to the audio file are configured and passed to the model, printing the result.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Pull and Run FunASR Docker Image\nDESCRIPTION: This shell script pulls the FunASR docker image from the specified registry and runs it with GPU support. It mounts a local directory for models. Requires docker to be installed and sudo privileges.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-gpu-0.2.0\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run --gpus=all -p 10098:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-gpu-0.2.0\n```\n\n----------------------------------------\n\nTITLE: Pulling and launching the FunASR Docker image\nDESCRIPTION: These commands pull the FunASR runtime-SDK Docker image from a registry and then launches it with specific configurations. The `docker run` command maps a host port to a container port, grants privileged access, and mounts a host directory into the container, enabling access to models. It requires Docker to be installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-gpu-0.2.0\n\nsudo docker run --gpus=all -p 10098:10095 -it --privileged=true -v /root:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-gpu-0.2.0\n```\n\n----------------------------------------\n\nTITLE: Non-Real-Time ASR with Multiple Models in FunASR\nDESCRIPTION: Example of performing non-real-time ASR using multiple models for voice activity detection, punctuation, and speech recognition with support for long audio and batch processing.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Starting the FunASR server using run_server.sh\nDESCRIPTION: This command executes the `run_server.sh` script in the background using `nohup`. It passes various parameters, including model directories, VAD directory, punctuation directory, LM directory, ITN directory, and hotword file path, to configure the server. It redirects output to `log.txt`.  The script is assumed to be in the current directory and requires the specified model directories to be accessible or the model IDs to be valid for download from Modelscope.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Running the FunASR HTTP Server with Command-line Options in Shell\nDESCRIPTION: This sequence launches the FunASR HTTP server executable with multiple configuration parameters, including paths for model data, number of threads, and server port. Each option (e.g., --model-dir, --port) must be set to appropriate file paths or integer values; environment variables are used in curly brackets. Before running, verify that all model files are downloaded and set all required paths. The server listens for incoming transcription requests according to the specified configuration.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme_zh.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-http-server  \\\n  --lm-dir '' \\\n  --itn-dir '' \\\n  --download-model-dir ${download_model_dir} \\\n  --model-dir ${model_dir} \\\n  --vad-dir ${vad_dir} \\\n  --punc-dir ${punc_dir} \\\n  --decoder-thread-num ${decoder_thread_num} \\\n  --io-thread-num  ${io_thread_num} \\\n  --port ${port} \\\n```\n\n----------------------------------------\n\nTITLE: Deploying FunASR Offline ASR Service Docker Container\nDESCRIPTION: This snippet pulls the offline CPU SDK FunASR Docker image and starts a container exposing port 10095 with mounted local directories for model resources. This environment serves batch transcription for offline audio files with the same dependencies and resource structure as the online Docker deployment, suitable for processing large batches with model support for language models and inverse text normalization.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10095:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\n```\n\n----------------------------------------\n\nTITLE: Running funasr-onnx-offline-rtf with Hotwords\nDESCRIPTION: This shell script demonstrates the execution of the `funasr-onnx-offline-rtf` tool. It specifies various parameters like model directories, quantization mode, VAD and punctuation models, and a language model. The `--hotword` argument is included and points to a text file. The output is processed based on the arguments provided.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-onnx-offline-rtf \\\n    --model-dir    ./damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch \\\n    --quantize  true \\\n    --vad-dir   ./damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n    --punc-dir  ./damo/punc_ct-transformer_zh-cn-common-vocab272727-onnx \\\n    --lm-dir    ./damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n    --hotword  hotwords_dev_600.txt \\\n    --wav-path     ./aishell1_test.scp  \\\n    --thread-num 32\n```\n\n----------------------------------------\n\nTITLE: Generating JSONL with SenseVoice (Simplified)\nDESCRIPTION: This snippet shows how to generate train.jsonl and val.jsonl files from wav.scp and text.txt files using the `sensevoice2jsonl` command, without explicit language, emotion, or event targets.  It leverages SenseVoice to automatically label the audio data.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\" \\\n++model_dir='iic/SenseVoiceSmall'\n```\n\n----------------------------------------\n\nTITLE: C++ Client Invocation for Offline Transcription\nDESCRIPTION: This shell command runs the C++ client executable to connect to the FunASR runtime-SDK service for offline transcription. It specifies server IP, port, and the input wav audio file path. It serves as an example for testing or using the ASR service from C++ clients.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-wss-client --server-ip 127.0.0.1 --port 10095 --wav-path ../audio/asr_example.wav\n```\n\n----------------------------------------\n\nTITLE: Defining gRPC ASR Service and Messages in Protobuf\nDESCRIPTION: This Protobuf snippet defines the structure for a gRPC service named 'ASR'. It includes a bidirectional streaming RPC method 'Recognize' that accepts a stream of 'Request' messages and returns a stream of 'Response' messages. The 'Request' message includes audio data bytes, user identification, language specification (currently 'zh-CN'), and boolean flags 'speaking' and 'isEnd' to control the audio stream processing based on voice activity detection (VAD). The 'Response' message contains the resulting transcribed sentence (as a JSON string), user and language identifiers mirrored from the request, and an 'action' string indicating the server's current status (e.g., 'terminate', 'speaking', 'decoding', 'finish').\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/grpc/proto/Readme.md#_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\nservice ASR { //grpc service\n  rpc Recognize (stream Request) returns (stream Response) {} //Stub\n}\n\nmessage Request { //request data\n  bytes audio_data = 1; //audio data in bytes.\n  string user = 2; //user allowed.\n  string language = 3; //language, zh-CN for now.\n  bool speaking = 4; //flag for speaking. \n  bool isEnd = 5; //flag for end. set isEnd to true when you stop asr:\n  //vad:is_speech then speaking=True & isEnd = False, audio data will be appended for the specfied user.\n  //vad:silence then speaking=False & isEnd = False, clear audio buffer and do asr inference.\n}\n\nmessage Response { //response data.\n  string sentence = 1; //json, includes flag for success and asr text .\n  string user = 2; //same to request user.\n  string language = 3; //same to request language.\n  string action = 4; //server status:\n  //terminateÔºöasr stopped; \n  //speakingÔºöuser is speaking, audio data is appended; \n  //decoding: server is decoding; \n  //finish: get asr text, most used.\n}\n```\n\n----------------------------------------\n\nTITLE: Tensorboard Visualization\nDESCRIPTION: This snippet shows how to launch TensorBoard to visualize training progress and metrics.  The logdir should point to the location of the tensorboard logs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\ntensorboard --logdir /xxxx/FunASR/examples/industrial_data_pretraining/paraformer/outputs/log/tensorboard\n```\n\n----------------------------------------\n\nTITLE: Creating SenseVoice Docker container\nDESCRIPTION: Command to create and run a Docker container for SenseVoice with GPU support, host networking, and mounted directories.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nyour_mount_dir=/mnt:/mnt\ndocker run -it --name \"sensevoice-server\" --gpus all --net host -v $your_mount_dir --shm-size=2g soar97/triton-sensevoice:24.05\n```\n\n----------------------------------------\n\nTITLE: Setting Up FunASR Environment for Finetuning (Shell)\nDESCRIPTION: This sequence of shell commands prepares the environment for finetuning SenseVoice or developing within the FunASR framework. It first clones the official FunASR repository from GitHub and navigates into the cloned directory. Then, it uses `pip3 install -e ./` to install the FunASR package in editable mode, meaning changes made to the local source code will be reflected immediately without reinstalling.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n----------------------------------------\n\nTITLE: SenseVoice JSONL Generation Command (Simplified)\nDESCRIPTION: This shell command generates `train.jsonl` and `val.jsonl` files from `wav.scp` and `text.txt` files. It automatically predicts the language, emotion, and event labels using the SenseVoice model when the corresponding input files are not provided.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\" \\\n++model_dir='iic/SenseVoiceSmall'\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Python Client (Offline Mode)\nDESCRIPTION: This command executes the FunASR Python WebSocket client script to perform offline transcription. It connects to a specified host and port, processes audio from a file or wav.scp list, and saves the results to an output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"127.0.0.1\" --port 10097 --mode offline --audio_in \"./data/wav.scp\" --send_without_sleep --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Committing changes - Git Shell\nDESCRIPTION: This command commits the staged changes to your local repository with a message. The '-am' flag stages all modified and deleted files in the current directory and commits them simultaneously. Replace 'Add some feature' with a concise commit message summarizing your changes.\nSOURCE: https://github.com/modelscope/funasr/blob/main/Contribution.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ngit commit -am 'Add some feature'\n```\n\n----------------------------------------\n\nTITLE: Launching FunASR Server (2pass mode)\nDESCRIPTION: This command starts the funasr-wss-server-2pass service, configuring several model paths (VAD, ASR, PUNC, LM, ITN) for the 2pass mode.  It downloads models from specified directories and redirects logs to a file.  SSL is enabled by default using the `--certfile` and `--keyfile` parameters (with a default crt and key path).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server_2pass.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  \\\n  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training on Worker Node - Shell\nDESCRIPTION: This shell script runs the training process on a worker node in a multi-GPU setup. It sets the CUDA_VISIBLE_DEVICES environment variable, determines the number of GPUs, and uses torchrun to launch the distributed training script, ensuring that MASTER_ADDR and MASTER_PORT are consistent with the master node's settings. It requires the train_ds.py script and appropriate training arguments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Shutting Down FunASR Service\nDESCRIPTION: This shell script provides steps for shutting down the FunASR service. It first lists processes related to the FunASR service and then uses the `kill -9` command with the process ID (PID) to terminate the process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Check the PID of the funasr-wss-server-2pass process\nps -x | grep funasr-wss-server-2pass\nkill -9 PID\n```\n\n----------------------------------------\n\nTITLE: Running Java Client for Offline Transcription\nDESCRIPTION: Command to run the Java client for offline transcription, connecting to a FunASR server with an example audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nFunasrWsClient --host localhost --port 10095 --audio_in ./asr_example.wav --mode offline\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR WebSocket Client via Shell (API Reference)\nDESCRIPTION: Runs the FunASR WebSocket client with customizable parameters for host, port, chunk size, mode, threading, and audio input/output. Can be used for streaming or non-streaming ASR with options for microphone recording or batch file processing. Expected input parameters are described for each key feature; outputs recognized text results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py \\\n--host [ip_address] \\\n--port [port id] \\\n--chunk_size [\"5,10,5\"=600ms, \"8,8,4\"=480ms] \\\n--chunk_interval [duration of send chunk_size/chunk_interval] \\\n--words_max_print [max number of words to print] \\\n--audio_in [if set, loadding from wav.scp, else recording from mircrophone] \\\n--output_dir [if set, write the results to output_dir] \\\n--mode [`online` for streaming asr, `offline` for non-streaming, `2pass` for unifying streaming and non-streaming asr] \\\n--thread_num [thread_num for send data]\n```\n\n----------------------------------------\n\nTITLE: Command Line Model Training in FunASR\nDESCRIPTION: Example of training a model using the command line interface in FunASR with specified training and validation datasets.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on Server for FunASR\nDESCRIPTION: Commands to install Docker on a server to prepare for FunASR deployment, with an alternative reference link for Docker installation troubleshooting.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.shÔºõ\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Inference with AutoModel - Python\nDESCRIPTION: This code snippet demonstrates how to use the `AutoModel` class from the `funasr` library for inference. It loads a model from a remote location, specified by the `model` parameter, and indicates that the model code is located in a specified directory using the `remote_code` parameter. The `trust_remote_code` parameter enables the loading of code from the remote location specified by `remote_code`. The model is instantiated and ready for inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\n# trust_remote_code:'True' means that the model code implementation is loaded from 'remote_code', 'remote_code' specifies the location of the 'model' specific code (for example,'model.py') in the current directory, supports absolute and relative paths, and network url.\nmodel = AutoModel (\nmodel=\"iic/SenseVoiceSmall \",\ntrust_remote_code=True\nremote_code = \"./model.py\", \n)\n\n```\n\n----------------------------------------\n\nTITLE: Pulling a Docker Image using Shell\nDESCRIPTION: Downloads a specified Docker image from a container registry. Requires replacing `<image-name>` and `<tag>` with the actual image name and tag (e.g., `registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.1`). Requires sudo privileges.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nsudo docker pull <image-name>:<tag>\n```\n\n----------------------------------------\n\nTITLE: Configuring Project, Build Options, and Compiler Settings with CMake (CMake)\nDESCRIPTION: This CMake script sets the minimum CMake version, declares the FunASROnnx project, and specifies compiler options such as C++14 standard and output directories. It defines build options (ENABLE_GLOG, ENABLE_FST, GPU) as ON or OFF, enabling toggling of logging, OpenFST, and GPU support. Prerequisites include CMake 3.16 or newer and knowledge of dependencies (glog, openfst, onnxruntime, kaldi-native-fbank, yaml-cpp, jieba, json, gflags). The expected input is a CMake configuration context with optional variables set by the user; the outputs include defined variables and the configured build system. Limitations: some variables and directory paths may require adjustment for specific environments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.16)\n\nproject(FunASROnnx)\n\noption(ENABLE_GLOG \"Whether to build glog\" ON)\noption(ENABLE_FST \"Whether to build openfst\" ON) # ITN need openfst compiled\noption(GPU \"Whether to build with GPU\" OFF)\n\n# set(CMAKE_CXX_STANDARD 11)\nset(CMAKE_CXX_STANDARD 14 CACHE STRING \"The C++ version to be used.\")\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\n```\n\n----------------------------------------\n\nTITLE: Voice Activity Detection (Real-Time)\nDESCRIPTION: This snippet demonstrates real-time VAD using the `AutoModel` with a streaming configuration. It processes audio in chunks and uses caching to maintain context, printing the detected voice activity segments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: Detailed FunASR Training Script Invocation with Parameter Configuration - Shell\nDESCRIPTION: This shell command shows invoking the main training script train_ds.py with extensive configurable parameters including model name or path, training and validation dataset paths, batch settings (size and type), training epochs, logging intervals, checkpoint management, optimizer learning rate, and output directory. Parameters correspond to keys in configuration files allowing fine-tuning of training behavior. Output logs are redirected to a specified log file. Dependent on proper datasets and environment prepared for shell execution.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train_ds.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Python WebSocket Client (Shell)\nDESCRIPTION: Executes the Python WebSocket client (`funasr_wss_client.py`) to connect to a FunASR server for offline speech recognition. Key parameters include server host (--host), port (--port), recognition mode (--mode, e.g., offline), audio input path (--audio_in, can be a single wav file or a wav.scp list), output directory (--output_dir), SSL usage (--ssl, default 1), hotword file path (--hotword), and ITN usage (--use_itn, default 1). The --send_without_sleep flag optimizes sending speed. Requires the Python client script and specified audio input.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"./data/wav.scp\" --send_without_sleep --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Speech Timestamp Prediction from WAV and Text - Python\nDESCRIPTION: This code loads a timestamp prediction model, takes both a WAV audio file and corresponding text as input, and predicts speech alignment/timestamps. It demonstrates use of input tuples and data_type specification. Dependencies: funasr, model weights, and proper input files. Outputs are timestamped transcriptions or alignment data.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Creating a new branch for changes - Git Shell\nDESCRIPTION: This command creates and switches to a new branch in your local repository. This is a standard practice before making changes to isolate development work and simplify merge requests. Replace 'my-new-feature' with a descriptive branch name.\nSOURCE: https://github.com/modelscope/funasr/blob/main/Contribution.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ngit checkout -b my-new-feature\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for FunASR HTTP Python Service\nDESCRIPTION: Installs required Python packages for running the FunASR HTTP server by changing directories to the appropriate runtime and using pip with a requirements file. This prepares the environment with all necessary dependencies to successfully start the server.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/http/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd funasr/runtime/python/http\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: WebUI Launch Command\nDESCRIPTION: This Python command starts the WebUI for the FunASR project. The WebUI allows users to interact with the model through a graphical interface.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npython webui.py\n```\n\n----------------------------------------\n\nTITLE: Starting the FunASR WebSocket Server via Shell\nDESCRIPTION: Launches the FunASR WebSocket server using python. Supports configurable parameters including port, ASR models, GPU/CPU usage, and SSL certification files. Key parameters like --port, --asr_model, --ngpu, and SSL options are available. Outputs a running server that listens for ASR requests.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_server.py \\\n--port [port id] \\\n--asr_model [asr model_name] \\\n--asr_model_online [asr model_name] \\\n--punc_model [punc model_name] \\\n--ngpu [0 or 1] \\\n--ncpu [1 or 4] \\\n--certfile [path of certfile for ssl] \\\n--keyfile [path of keyfile for ssl] \n```\n\n----------------------------------------\n\nTITLE: AutoModel Inference Method Signature\nDESCRIPTION: Specifies the generate method for AutoModel, facilitating speech recognition from audio files or feature inputs. Supports multiple input types such as file paths, raw audio data, or feature tensors, with optional output directory and inference parameters.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables_zh.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Using VAD in C++ Custom Implementation\nDESCRIPTION: C++ code example demonstrating how to initialize and run VAD (Voice Activity Detection) using FunASR's C++ API in a custom implementation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_10\n\nLANGUAGE: c++\nCODE:\n```\n// VADÊ®°ÂûãÁöÑ‰ΩøÁî®ÂàÜ‰∏∫FsmnVadInitÂíåFsmnVadInfer‰∏§‰∏™Ê≠•È™§Ôºö\nFUNASR_HANDLE vad_hanlde=FsmnVadInit(model_path, thread_num);\n// ÂÖ∂‰∏≠Ôºömodel_path ÂåÖÂê´\"model-dir\"„ÄÅ\"quantize\"Ôºåthread_num‰∏∫onnxÁ∫øÁ®ãÊï∞Ôºõ\nFUNASR_RESULT result=FsmnVadInfer(vad_hanlde, wav_file.c_str(), NULL, 16000);\n// ÂÖ∂‰∏≠Ôºövad_hanlde‰∏∫FunOfflineInitËøîÂõûÂÄºÔºåwav_file‰∏∫Èü≥È¢ëË∑ØÂæÑÔºåsampling_rate‰∏∫ÈááÊ†∑Áéá(ÈªòËÆ§16k)\n```\n\n----------------------------------------\n\nTITLE: Register New Model - Python\nDESCRIPTION: This Python code demonstrates how to register a new model using the @tables.register decorator. The registered class must implement __init__, forward, and inference methods.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n  def __init__(*args, **kwargs):\n    ...\n\n  def forward(\n      self,\n      **kwargs,\n  ):\n    ...\n\n  def inference(\n      self,\n      data_in,\n      data_lengths=None,\n      key: list = None,\n      tokenizer=None,\n      frontend=None,\n      **kwargs,\n  ):\n    ...\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Fine-tuning (Shell Script)\nDESCRIPTION: This snippet shows how to perform fine-tuning of a FunASR model using a shell script. It navigates to the appropriate directory and executes the `finetune.sh` script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/industrial_data_pretraining/paraformer\nbash finetune.sh\n# \"log_file: ./outputs/log.txt\"\n```\n\n----------------------------------------\n\nTITLE: Export Model from Command Line - Shell\nDESCRIPTION: This shell command exports a model using the funasr-export tool. It allows specifying the model name and whether to quantize the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using SenseVoiceSmall for Non-Streaming ASR (Python)\nDESCRIPTION: Python code snippet for performing non-streaming Automatic Speech Recognition (ASR) using the SenseVoiceSmall model from FunASR. It initializes the AutoModel with the specified model directory, an optional VAD model (fsmn-vad) with configuration, and sets the device. The `generate` method processes an input audio file, performing language detection ('auto'), applying ITN, using dynamic batching, and merging VAD segments. The result is post-processed for rich transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = AutoModel(\n    model=model_dir,\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Python WSS Server\nDESCRIPTION: Changes the current directory to the websocket server location within the FunASR repository and starts the Python-based WebSocket server for FunASR speech recognition on the specified port (10095). This server handles the speech recognition requests.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme_zh.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ncd funasr/runtime/python/websocket\npython funasr_wss_server.py --port 10095\n```\n\n----------------------------------------\n\nTITLE: Executing FunASR Paraformer Model Training via Command Line - Shell\nDESCRIPTION: This shell command demonstrates how to execute the FunASR Paraformer training script from the command line for quick testing purposes. It specifies the model, training and validation JSONL datasets, output directory, and redirects output logs to a log file. This approach is simple but not recommended for complex or large-scale training.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Executing Deployment Script to Install and Deploy FunASR\nDESCRIPTION: Command to run the downloaded shell script with elevated privileges to install and deploy the FunASR runtime SDK in a specified workspace on Linux. This triggers the process of environment setup, Docker image download, and service startup.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh install --workspace /root/funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: train_wav.scp Example\nDESCRIPTION: This provides an example of data structure in the `train_wav.scp` file used for finetuning. Each line contains the audio file ID and the URL to the corresponding audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n\n----------------------------------------\n\nTITLE: Include System Check Modules (CMake)\nDESCRIPTION: Includes standard CMake modules used for checking system properties. `CheckTypeSize` verifies the size of data types, `CheckIncludeFileCXX` checks for the availability of C++ header files, and `CheckCXXSymbolExists` checks for the presence of symbols (functions, variables) in C++ code.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\ninclude (CheckTypeSize)\ninclude (CheckIncludeFileCXX)\ninclude (CheckCXXSymbolExists)\n```\n\n----------------------------------------\n\nTITLE: Downloading ffmpeg Using wget - Shell\nDESCRIPTION: This shell snippet downloads the latest shared GPL build of ffmpeg for Linux x64 and extracts it using tar. ffmpeg is required by FunASR for audio preprocessing. The snippet takes no parameters and creates a directory with ffmpeg binaries. Network access and sufficient disk space are prerequisites.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/ffmpeg-master-latest-linux64-gpl-shared.tar.xz\ntar -xvf ffmpeg-master-latest-linux64-gpl-shared.tar.xz\n```\n\n----------------------------------------\n\nTITLE: Installing Docker via Script\nDESCRIPTION: This command sequence downloads and executes a helper script to install Docker on the host machine. It requires `curl` for downloading and `sudo` privileges to run the installation script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh;\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Setting IO Threads and Disabling SSL in FunASR\nDESCRIPTION: These command-line arguments are used to configure the FunASR server.  `--io-thread-num` sets the number of input/output threads the server will use. `--certfile 0` disables SSL certificate verification.  These configurations affect the server's performance and security.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# Set the number of IO threads the server will start using --io-thread-num\n    --io-thread-num <io thread num>\n# Disable SSL certificate\n    --certfile 0\n```\n\n----------------------------------------\n\nTITLE: Get Segments from Samples (Large Files) C#\nDESCRIPTION: This C# code snippet demonstrates how to use the GetSegmentsByStep method of the AliFsmnVad object to extract voice segments from a sample audio file, specifically designed for large files.  This method likely processes the audio in smaller steps to handle memory constraints. The method returns an array of SegmentEntity objects, each representing a detected voice segment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/csharp/AliFsmnVad/README.md#_snippet_2\n\nLANGUAGE: C#\nCODE:\n```\nSegmentEntity[] segments_duration = aliFsmnVad.GetSegmentsByStep(samples);\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR/ModelScope via pip (Shell)\nDESCRIPTION: Describes the process of installing the FunASR and ModelScope libraries using the Python package manager pip. Includes an alternative command using a China mirror for faster downloads in that region. Requires an existing Python environment with pip and internet access.\nSOURCE: https://github.com/modelscope/funasr/blob/main/benchmarks/benchmark_pipeline_cer.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U modelscope funasr\n# For the users in China, you could install with the command:\n#pip install -U funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Listing FunASR Core Requirements\nDESCRIPTION: Specifies the minimum required versions for core Python packages needed to run FunASR. These packages are `python`, `torch`, and `torchaudio`, essential dependencies for the library's functionality.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npython>=3.8\ntorch>=1.13\ntorchaudio\n```\n\n----------------------------------------\n\nTITLE: AutoModel Definition (Python)\nDESCRIPTION: This code provides the definition of the `AutoModel` class within FunASR. It lists the parameters accepted by the class constructor. Key parameters include `model` (model name or path), `device` (CPU or GPU), `ncpu` (number of CPU threads), `output_dir` (output directory), `batch_size`, and `hub` (model source).\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Config YAML Example\nDESCRIPTION: This is an example `config.yaml` file for the SenseVoiceSmall model. It defines the encoder, model, tokenizer, frontend, dataset, training configuration, and optimization parameters.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nencoder: SenseVoiceEncoderSmall\nencoder_conf:\n    output_size: 512\n    attention_heads: 4\n    linear_units: 2048\n    num_blocks: 50\n    tp_blocks: 20\n    dropout_rate: 0.1\n    positional_dropout_rate: 0.1\n    attention_dropout_rate: 0.1\n    input_layer: pe\n    pos_enc_class: SinusoidalPositionEncoder\n    normalize_before: true\n    kernel_size: 11\n    sanm_shfit: 0\n    selfattention_layer_type: sanm\n\n\nmodel: SenseVoiceSmall\nmodel_conf:\n    length_normalized_loss: true\n    sos: 1\n    eos: 2\n    ignore_id: -1\n\ntokenizer: SentencepiecesTokenizer\ntokenizer_conf:\n  bpemodel: null\n  unk_symbol: <unk>\n  split_with_space: true\n\nfrontend: WavFrontend\nfrontend_conf:\n    fs: 16000\n    window: hamming\n    n_mels: 80\n    frame_length: 25\n    frame_shift: 10\n    lfr_m: 7\n    lfr_n: 6\n    cmvn_file: null\n\n\ndataset: SenseVoiceCTCDataset\ndataset_conf:\n  index_ds: IndexDSJsonl\n  batch_sampler: EspnetStyleBatchSampler\n  data_split_num: 32\n  batch_type: token\n  batch_size: 14000\n  max_token_length: 2000\n  min_token_length: 60\n  max_source_length: 2000\n  min_source_length: 60\n  max_target_length: 200\n  min_target_length: 0\n  shuffle: true\n  num_workers: 4\n  sos: ${model_conf.sos}\n  eos: ${model_conf.eos}\n  IndexDSJsonl: IndexDSJsonl\n  retry: 20\n\ntrain_conf:\n  accum_grad: 1\n  grad_clip: 5\n  max_epoch: 20\n  keep_nbest_models: 10\n  avg_nbest_model: 10\n  log_interval: 100\n  resume: true\n  validate_interval: 10000\n  save_checkpoint_interval: 10000\n\noptim: adamw\noptim_conf:\n  lr: 0.00002\nScheduler: warmuplr\nScheduler_conf:\nWarmup_steps: 25000\n```\n\n----------------------------------------\n\nTITLE: Voice Activity Detection (Non-Streaming) - Python\nDESCRIPTION: This snippet utilizes FunASR's `AutoModel` to perform non-streaming Voice Activity Detection (VAD). It loads a WAV file and calls the `generate` method to detect speech segments.  The expected output is a list of start and end times of speech segments in milliseconds.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Launching FunASR WebUI\nDESCRIPTION: This snippet demonstrates the command to launch the FunASR WebUI using the `webui.py` script, allowing users to interact with the model through a graphical interface.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython webui.py\n```\n\n----------------------------------------\n\nTITLE: Installing Docker (Optional)\nDESCRIPTION: Shell command sequence to download and execute a script for installing Docker. This step is a prerequisite for using the Docker-based service deployment software and can be skipped if Docker is already installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh;\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Detailed Training Configuration with Full Parameter List - Shell\nDESCRIPTION: The snippet invokes FunASR‚Äôs Python training API with many customizable parameters for dataset, training configuration, optimizer, and output. Each option controls a specific element, such as batch sizing, epoch count, checkpointing, validation, and learning rate. Command-line execution requires the FunASR source in the specified path and all necessary dependencies installed. Inputs are expected in JSONL for datasets.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: Initializing FunASR: End-to-End Speech Recognition Toolkit in Python\nDESCRIPTION: This code snippet demonstrates how to initialize the FunASR toolkit for end-to-end speech recognition tasks. It involves importing necessary modules, setting configuration parameters, and preparing the environment for model training or inference. Dependencies include the FunASR library and relevant model files. The snippet ensures the setup of parameters such as model paths and decoding configurations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Set configuration for speech recognition\nconfig = {\n    'model_path': 'path/to/model',\n    'decoding_method': 'ctc_greedy_search',\n    'device': 'cuda' # or 'cpu'\n}\n\n# Initialize FunASR for inference\nmodel = funasr.load_model(config['model_path'])\nresult = funasr.transcribe('audio.wav', model, decoding_method=config['decoding_method'])\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Performing Batched SenseVoice Inference for Short Audio using funasr AutoModel (Python)\nDESCRIPTION: This Python snippet demonstrates how to perform efficient batch inference on short audio files (under 30 seconds) using the SenseVoice model loaded via `funasr.AutoModel`. It omits the VAD model for faster processing of short inputs and utilizes a fixed `batch_size` for parallel processing. The `generate` method processes the input audio, automatically detects the language, applies ITN, and returns the transcription results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size=64, \n)\n```\n\n----------------------------------------\n\nTITLE: train_text_language.txt Example\nDESCRIPTION: This provides an example of data structure in the `train_text_language.txt` file used for finetuning. Each line contains the audio file ID and its language ID.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n\n----------------------------------------\n\nTITLE: train_text.txt Example\nDESCRIPTION: This provides an example of data structure in the `train_text.txt` file used for finetuning. Each line contains the audio file ID and the corresponding transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ\nBAC009S0916W0489 ÊπñÂåó‰∏ÄÂÖ¨Âè∏‰ª•ÂëòÂ∑•Âêç‰πâË¥∑Ê¨æÊï∞ÂçÅÂëòÂ∑•Ë¥üÂÄ∫ÂçÉ‰∏á\nasr_example_cn_en ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning ÂÅö data analytics ÂÅö data science ‰πüÂ•Ω scientist ‰πüÂ•ΩÈÄöÈÄöÈÉΩË¶ÅÈÉΩÂÅöÁöÑÂü∫Êú¨ÂäüÂïäÈÇ£ again ÂÖàÂÖàÂØπÊúâ‰∏Ä‰∫õ>‰πüËÆ∏ÂØπ\nID0012W0014 he tried to think how it could be\n```\n\n----------------------------------------\n\nTITLE: Initializing and Inferring PUNC Model using FunASR (C++)\nDESCRIPTION: This C++ snippet details the setup and execution of a Punctuation (PUNC) model using the FunASR framework. The process starts by invoking `CTTransformerInit` to initialize the PUNC model, passing the model path and onnx thread count. The `CTTransformerInfer` function then processes the text (`txt_str`), using the `punc_hanlde` from initialization.  The code expects `FUNASR_RESULT` as the inference output.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n// PUNCÊ®°ÂûãÁöÑ‰ΩøÁî®ÂàÜ‰∏∫CTTransformerInitÂíåCTTransformerInfer‰∏§‰∏™Ê≠•È™§Ôºö\nFUNASR_HANDLE punc_hanlde=CTTransformerInit(model_path, thread_num);\n// ÂÖ∂‰∏≠Ôºömodel_path ÂåÖÂê´\"model-dir\"„ÄÅ\"quantize\"Ôºåthread_num‰∏∫onnxÁ∫øÁ®ãÊï∞Ôºõ\nFUNASR_RESULT result=CTTransformerInfer(punc_hanlde, txt_str.c_str(), RASR_NONE, NULL);\n// ÂÖ∂‰∏≠Ôºöpunc_hanlde‰∏∫CTTransformerInitËøîÂõûÂÄºÔºåtxt_str‰∏∫ÊñáÊú¨\n```\n\n----------------------------------------\n\nTITLE: Single-Machine Multi-GPU Training\nDESCRIPTION: This snippet demonstrates how to set up single-machine multi-GPU training using `torchrun`. It sets the `CUDA_VISIBLE_DEVICES` environment variable to specify which GPUs to use and then executes the `train_ds.py` script with the appropriate arguments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: AutoModel Class Definition and Configuration Parameters\nDESCRIPTION: Defines the AutoModel class constructor with parameters for model name, device, CPU threads, output directory, batch size, download source, and additional configuration options. Supports loading models from ModelScope or Hugging Face.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables_zh.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: VAD Model Initialization in C++\nDESCRIPTION: This C++ snippet shows how to initialize the Voice Activity Detection (VAD) model in FunASR using the `FsmnVadInit` function. It takes the model path and number of threads as parameters and returns a handle for further processing. The initialization prepares the VAD model for audio analysis.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_11\n\nLANGUAGE: c++\nCODE:\n```\nFUNASR_HANDLE vad_hanlde=FsmnVadInit(model_path, thread_num);\n```\n\n----------------------------------------\n\nTITLE: Inference with FunASR Model (Shell for models without configuration.json)\nDESCRIPTION: This shell command performs inference when the model lacks a configuration.json file by manually specifying the configuration and model parameters, as well as resource files like token lists and CMVN files. It ensures flexible deployment when configurations are stored separately.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_20\n\nLANGUAGE: Shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Build Support - CMake\nDESCRIPTION: Applies specific settings when the `GPU` option is enabled. This involves adding the `USE_GPU` preprocessor definition, setting paths for Torch and Torch-Blade libraries, and including/linking necessary header and library directories. It also adds a compiler flag to manage C++ ABI compatibility.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(GPU)\n    add_definitions(-DUSE_GPU)\n    set(TORCH_DIR \"/usr/local/lib/python3.8/dist-packages/torch\")\n    set(TORCH_BLADE_DIR \"/usr/local/lib/python3.8/dist-packages/torch_blade\")\n    include_directories(${TORCH_DIR}/include)\n    include_directories(${TORCH_DIR}/include/torch/csrc/api/include)\n    link_directories(${TORCH_DIR}/lib)\n    link_directories(${TORCH_BLADE_DIR})\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -D_GLIBCXX_USE_CXX11_ABI=0\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Timestamp Prediction - Python\nDESCRIPTION: This snippet uses FunASR to predict timestamps for a given audio and text. It loads the `fa-zh` model, specifying a WAV and a text file as input and data types as 'sound' and 'text', respectively. The output will contain the predicted timestamps for each word or phrase.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Docker Image Startup Command\nDESCRIPTION: This shell command pulls and runs the FunASR docker image. It maps port 10096 on the host to 10095 on the container, mounts a local directory for models to `/workspace/models` inside the container, and runs the container in interactive mode with privileged access. The environment variable `$PWD` dynamically represents the current working directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10096:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Model Class in FunASR Registry Using Python Decorator (Python)\nDESCRIPTION: This Python example shows how to register a user-defined model class to the FunASR model registry. By decorating the class definition with @tables.register(\"model_classes\", \"SenseVoiceSmall\"), the class is added to the model registry under the specified name. The class must implement __init__, forward, and inference methods to be fully functional. This mechanism allows modular extension of FunASR with custom models integrated into the config system.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n  def __init__(*args, **kwargs):\n    ...\n\n  def forward(\n      self,\n      **kwargs,\n  ):  \n\n  def inference(\n      self,\n      data_in,\n      data_lengths=None,\n      key: list = None,\n      tokenizer=None,\n      frontend=None,\n      **kwargs,\n  ):\n    ...\n```\n\n----------------------------------------\n\nTITLE: CPP PUNC Model Usage\nDESCRIPTION: This C++ code shows how to use the PUNC (Punctuation) model. It involves two main steps: `CTTransformerInit` for model initialization and `CTTransformerInfer` for inferencing.  The code expects the path to the model and text as input, and makes use of pre-defined structures. Prerequisites include including necessary header files, linking required libraries, and initializing the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\n// PUNCÊ®°ÂûãÁöÑ‰ΩøÁî®ÂàÜ‰∏∫CTTransformerInitÂíåCTTransformerInfer‰∏§‰∏™Ê≠•È™§Ôºö\nFUNASR_HANDLE punc_hanlde=CTTransformerInit(model_path, thread_num);\n// ÂÖ∂‰∏≠Ôºömodel_path ÂåÖÂê´\"model-dir\"„ÄÅ\"quantize\"Ôºåthread_num‰∏∫onnxÁ∫øÁ®ãÊï∞Ôºõ\nFUNASR_RESULT result=CTTransformerInfer(punc_hanlde, txt_str.c_str(), RASR_NONE, NULL);\n// ÂÖ∂‰∏≠Ôºöpunc_hanlde‰∏∫CTTransformerInitËøîÂõûÂÄºÔºåtxt_str‰∏∫ÊñáÊú¨\n```\n\n----------------------------------------\n\nTITLE: Viewing the Model Registry in FunASR (Python)\nDESCRIPTION: This snippet demonstrates how to print the contents of the FunASR registry, which tracks registered components like model types. Using tables.print() displays the full registry. Optionally, passing 'model' as an argument shows only registered models. This supports inspection and debugging of available registered models and classes in the framework.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\ntables.print()\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR from Source Code\nDESCRIPTION: Provides a sequence of shell commands to clone the FunASR repository from GitHub and install it in editable mode (`-e`). This method is suitable for development or using the latest unreleased version directly from the repository.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n----------------------------------------\n\nTITLE: Sending Real-time Recognition Results\nDESCRIPTION: This JSON snippet illustrates the format of the recognition results sent from the server to the client in real-time speech recognition. It contains the mode (2pass-online or 2pass-offline), wav_name, recognized text, an 'is_final' flag, timestamp and stamp_sents which contains segmentation of recognized text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"2pass-online\", \"wav_name\": \"wav_name\", \"text\": \"asr ouputs\", \"is_final\": True, \"timestamp\":\"[[100,200], [200,500]]\", \"stamp_sents\":[]}\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies for FunASR Compilation in Shell\nDESCRIPTION: This sequence installs required system libraries for building FunASR, including OpenBLAS and OpenSSL. The commands use apt-get for Ubuntu systems, with alternative commented lines for CentOS. OpenBLAS is necessary for numerical computations, and OpenSSL enables secure communication. Root or sudo privileges are required; successful installation ensures all necessary headers and binaries are available for compilation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# openblas\nsudo apt-get install libopenblas-dev #ubuntu\n# sudo yum -y install openblas-devel #centos\n\n# openssl\napt-get install libssl-dev #ubuntu \n# yum install openssl-devel #centos\n```\n\n----------------------------------------\n\nTITLE: Launching Tensorboard Visualization for FunASR Training - Shell\nDESCRIPTION: This command starts the Tensorboard server pointing to the directory containing Tensorboard logs produced during Paraformer model training. It allows users to visualize training metrics such as loss and accuracy via a web interface on the default port 6006, facilitating analysis and debugging.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir /xxxx/FunASR/examples/industrial_data_pretraining/paraformer/outputs/log/tensorboard\n```\n\n----------------------------------------\n\nTITLE: Performing FunASR Model Inference via Command Line\nDESCRIPTION: This snippet demonstrates how to use the FunASR library for inference directly from the command line. It specifies the base ASR model, optional VAD and Punctuation models, and the input audio file path. Dependencies include the installed FunASR library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Launching Triton Inference Server with Docker\nDESCRIPTION: This Shell script builds a Docker image from a specified Dockerfile, runs the container with GPU access, mounts the model repository, and starts the Triton server with specific memory pool settings. This setup enables serving the speech recognition model for inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README_paraformer_offline.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n# Build the Docker image\ndocker build . -f Dockerfile/Dockerfile.server -t triton-paraformer:23.01\n\n# Run the container\ndocker run -it --rm --name \"paraformer_triton_server\" --gpus all -v <path_host/model_repo_paraformer_large_offline>:/workspace/ --shm-size 1g --net host triton-paraformer:23.01\n\n# Launch Triton server inside the container\ntritonserver --model-repository /workspace/model_repo_paraformer_large_offline \\\n             --pinned-memory-pool-byte-size=512000000 \\\n             --cuda-memory-pool-byte-size=0:1024000000\n```\n\n----------------------------------------\n\nTITLE: Downloading FFmpeg (Linux) - Shell\nDESCRIPTION: Downloads the FFmpeg dependency package for Linux (x64, latest GPL shared build) from a specified URL and extracts its contents using tar. FFmpeg is required for handling audio input formats in the FunASR runtime.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/readme.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/ffmpeg-master-latest-linux64-gpl-shared.tar.xz\ntar -xvf ffmpeg-master-latest-linux64-gpl-shared.tar.xz\n```\n\n----------------------------------------\n\nTITLE: Training ASR Model Using FunASR Command Line - Shell\nDESCRIPTION: This shell snippet trains an ASR model using funasr-train, specifying model, dataset, and output/logging locations via command-line parameters. All dependencies (FunASR package, compatible Python and model/dataset availability) are required. Input data should be in the referenced JSONL list format. Training can be performed non-interactively and supports basic testing.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Multi-Machine Multi-GPU Training (Master Node)\nDESCRIPTION: This snippet shows the commands to run on the master node for multi-machine multi-GPU training. It sets the `CUDA_VISIBLE_DEVICES`, determines the number of GPUs, and uses `torchrun` with specific parameters like `--nnodes`, `--node_rank`, `--nproc_per_node`, `--master_addr`, and `--master_port`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr=192.168.1.1 --master_port=12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmarking of Speech Recognition with Triton\nDESCRIPTION: This Shell command runs a Docker container with the benchmarking client, installs Git LFS, clones the AISHell test manifests, and executes a Python script to perform inference on the test set, measuring processing time and Real-Time Factor (RTF). The results demonstrate model performance under specified test conditions.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README_paraformer_offline.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n# Run client container for inference testing\ndocker run -it --rm --name \"client_test\" --net host --gpus all -v <path_host/triton_gpu/client>:/workspace/ soar97/triton-k2:22.12.1\n\n# Install Git LFS and clone test manifests\ngit-lfs install\ngit clone https://huggingface.co/csukuangfj/aishell-test-dev-manifests\n\n# Setup manifest files\nsudo mkdir -p /root/fangjun/open-source/icefall-aishell/egs/aishell/ASR/download/aishell\ntar xf ./aishell-test-dev-manifests/data_aishell.tar.gz -C /root/fangjun/open-source/icefall-aishell/egs/aishell/ASR/download/aishell/\n\n# Run inference benchmark\nserveraddr=localhost\nmanifest_path=/workspace/aishell-test-dev-manifests/data/fbank/aishell_cuts_test.jsonl.gz\nnum_task=60\npython3 client/decode_manifest_triton.py \\\n    --server-addr $serveraddr \\\n    --compute-cer \\\n    --model-name infer_pipeline \\\n    --num-tasks $num_task \\\n    --manifest-filename $manifest_path\n```\n\n----------------------------------------\n\nTITLE: Generate jsonl Files Command\nDESCRIPTION: This command uses scp2jsonl to convert wav.scp and text.txt files into jsonl format for training. It specifies the input files, data types, and output file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Testing Offline ASR Client (Docker)\nDESCRIPTION: Python client command executed on the host machine to test the offline ASR service running in the Docker container. It connects to the mapped host port 10095, uses 'offline' mode, and specifies a local audio file path for transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"../audio/asr_example.wav\"\n```\n\n----------------------------------------\n\nTITLE: Applying TOLD: Overlap-Aware Speaker Diarization Framework\nDESCRIPTION: This snippet applies the TOLD two-stage overlap-aware diarization framework for accurately segmenting multiple speakers in meetings or conversations. It loads the TOLD model, processes meeting audio, and outputs speaker segments with overlapped regions. Suitable for multi-party meeting analysis as per ICASSP 2023 research.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Load TOLD diarization model\ntold_model = funasr.load_told_model('path/to/told')\n# Load audio\naudio = funasr.load_audio('meeting.wav')\n# Perform overlap-aware diarization\ndiarization_result = funasr.told_diarize(told_model, audio)\nprint('TOLD diarization output:', diarization_result)\n```\n\n----------------------------------------\n\nTITLE: Running Python Client for Offline Transcription\nDESCRIPTION: Command to run the Python client for offline transcription, connecting to a FunASR server with an example audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"../audio/asr_example.wav\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Paraformer Model Repository Using Git Shell Command\nDESCRIPTION: This shell command clones the repository containing the quantized Paraformer model files required for the iOS demo project. The cloned repository provides essential model artifacts including ONNX files and configuration needed by the app.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/ios/Readme.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx.git\n```\n\n----------------------------------------\n\nTITLE: Testing FunASR Streaming ASR Client in Python\nDESCRIPTION: This Python command snippet shows how to run a client to test the FunASR streaming ASR WebSocket server. It connects to the server at localhost (127.0.0.1) on port 10095 with mode '2pass' enabling two-step processing. The chunk sizes control the data segmentation for streaming. An example of using an audio input file is commented out. Prerequisites include the server running and the Python client script available in the runtime environment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode 2pass --chunk_size \"5,10,5\"\n#python funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode 2pass --chunk_size \"8,8,4\" --audio_in \"./data/wav.scp\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for FunASR GRPC Client (shell)\nDESCRIPTION: This snippet installs the necessary Python dependencies for the FunASR gRPC client.  It first clones the FunASR repository and navigates into the gRPC client directory. Then, it uses pip to install dependencies listed in the requirements.txt file. This step is crucial for setting up the client environment before interacting with the FunASR server.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/grpc/Readme.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR/funasr/runtime/python/grpc\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Compiling FunASR runtime on Linux\nDESCRIPTION: Shell commands for cloning the FunASR repository, creating a build directory, configuring with CMake, and compiling the FunASR runtime with specified dependencies paths. This setup prepares the runtime for deployment on Linux platforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd FunASR/runtime/websocket\n mkdir build && cd build\n cmake  -DCMAKE_BUILD_TYPE=release .. -DONNXRUNTIME_DIR=/path/to/onnxruntime-linux-x64-1.14.0 -DFFMPEG_DIR=/path/to/ffmpeg-master-latest-linux64-gpl-shared\n make -j 4\n```\n\n----------------------------------------\n\nTITLE: Downloading onnxruntime library for Linux x64\nDESCRIPTION: Shell command to download and extract the onnxruntime runtime library optimized for Linux 64-bit systems, which is required for running the FunASR runtime.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/onnxruntime-linux-x64-1.14.0.tgz\n tar -zxvf onnxruntime-linux-x64-1.14.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Description of Python Client Command-Line Parameters\nDESCRIPTION: This text snippet explains the major parameters used in the Python client command. It details the purpose of --host for service IP address, --port for service port, --mode to specify offline transcription, and --audio_in to provide the input audio file path or list. Additional parameters --thread_num, --ssl, --hotword, and --use_itn allow configuring concurrent threads, SSL certificate verification, hotword files, and inverse text normalization respectively.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n--host is the IP address of the FunASR runtime-SDK service deployment machine, which defaults to the local IP address (127.0.0.1). If the client and the service are not on the same server, it needs to be changed to the deployment machine IP address.\n--port 10095 deployment port number\n--mode offline represents offline file transcription\n--audio_in is the audio file that needs to be transcribed, supporting file paths and file list wav.scp\n--thread_num sets the number of concurrent sending threads, default is 1\n--ssl sets whether to enable SSL certificate verification, default is 1 to enable, and 0 to disable\n--hotword: Hotword file path, one line for each hotword(e.g.:ÈòøÈáåÂ∑¥Â∑¥ 20)\n--use_itn: whether to use itn, the default value is 1 for enabling and 0 for disabling.\n```\n\n----------------------------------------\n\nTITLE: Running ASR Offline Client (Microphone) via Shell\nDESCRIPTION: Starts the FunASR client in offline mode, recording audio from the microphone and handling chunk intervals. Outputs recognized text in non-streaming fashion. Best used for one-shot voice recognition tasks.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"0.0.0.0\" --port 10095 --mode offline\n```\n\n----------------------------------------\n\nTITLE: Model Training Python Script Execution in FunASR\nDESCRIPTION: Example of running a Python script for training models in FunASR with support for distributed training across multiple GPUs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/industrial_data_pretraining/paraformer\nbash finetune.sh\n# \"log_file: ./outputs/log.txt\"\n```\n\n----------------------------------------\n\nTITLE: Installing ModelScope for pre-trained models\nDESCRIPTION: Commands to install the ModelScope package, which is optional but required if you want to use pre-trained models. Includes an alternative command for users in mainland China.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation_zh.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U modelscope\n# ÂØπ‰∫é‰∏≠ÂõΩÂ§ßÈôÜÁî®Êà∑ÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ËøõË°åÂÆâË£ÖÔºö\n# pip3 install -U modelscope -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: VAD Model Inference in C++\nDESCRIPTION: This C++ snippet demonstrates how to perform VAD inference using the `FsmnVadInfer` function in FunASR. It takes the VAD handle, path to a WAV audio file, and the sampling rate (default 16k) as parameters. It performs voice activity detection on the provided audio.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_12\n\nLANGUAGE: c++\nCODE:\n```\nFUNASR_RESULT result=FsmnVadInfer(vad_hanlde, wav_file.c_str(), NULL, 16000);\n```\n\n----------------------------------------\n\nTITLE: Generating JSONL with SenseVoice\nDESCRIPTION: This snippet shows how to generate train.jsonl and val.jsonl files from various input files (wav.scp, text.txt, text_language.txt, emo_target.txt, event_target.txt) using the `sensevoice2jsonl` command. It specifies the input file lists and their corresponding data types.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n# generate train.jsonl and val.jsonl from wav.scp, text.txt, text_language.txt, emo_target.txt, event_target.txt\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Starting Python WebSocket Server\nDESCRIPTION: Shell command to navigate to the Python WebSocket server directory and start the server script. It listens on port 10095, providing a basic single-client real-time ASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncd runtime/python/websocket\npython funasr_wss_server.py --port 10095\n```\n\n----------------------------------------\n\nTITLE: train_event.txt Example\nDESCRIPTION: This provides an example of data structure in the `train_event.txt` file used for finetuning. Each line contains the audio file ID and its event label.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n\n----------------------------------------\n\nTITLE: Executing ASR Test Recipe (Shell)\nDESCRIPTION: Provides commands to navigate to a specific example directory (`egs_modelscope/asr/TEMPLATE`) and execute the `infer.sh` script. This script is used to test Character Error Rate (CER) for an ASR model based on configurations typically set within the script itself (e.g., model path, data directory, output directory). Requires the specified directory structure and the `infer.sh` script to exist, along with a working FunASR/ModelScope installation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/benchmarks/benchmark_pipeline_cer.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd egs_modelscope/asr/TEMPLATE\nbash infer.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Client Requirements via Shell\nDESCRIPTION: Clones the FunASR repository and installs the required dependencies for the client component. Should be executed before running the client for ASR tasks. Relies on a requirements file for dependency specification.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\ncd funasr/runtime/python/websocket\npip install -r requirements_client.txt\n```\n\n----------------------------------------\n\nTITLE: Running CER Test Script\nDESCRIPTION: This snippet executes the test_cer.sh script in the background and redirects both standard output and standard error to the log.txt file. Users need to set the model, data path, and output directory within the script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnohup bash test_cer.sh &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: AutoModel Generate Method Parameters in FunASR\nDESCRIPTION: Documents the function signature for the generate method in AutoModel, explaining the input options and output directory parameters for inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Launching Triton Inference Server\nDESCRIPTION: This script launches the Triton inference server, specifying the model repository and memory pool sizes. It navigates to the /workspace directory (where the model repository is mounted) and executes the tritonserver command with specific parameters for pinned and CUDA memory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README_paraformer_online.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd /workspace\ntritonserver --model-repository model_repo_paraformer_large_online \\\n             --pinned-memory-pool-byte-size=512000000 \\\n             --cuda-memory-pool-byte-size=0:1024000000\n```\n\n----------------------------------------\n\nTITLE: FunASR Training Script\nDESCRIPTION: This snippet showcases the parameters used in the `train.py` script for training FunASR models. It details various options for model selection, data paths, batching, training configurations, optimization, and output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: Running ASR Streaming Client (Microphone) via Shell\nDESCRIPTION: Starts the ASR client in online (streaming) mode, recording from microphone with tunable chunk sizes for latency/accuracy tradeoff. Streaming segment sizes set by --chunk_size parameter. Best for real-time transcription scenarios.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"0.0.0.0\" --port 10095 --mode online --chunk_size \"5,10,5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-process Load Balancing for FunASR Using Nginx and Shell\nDESCRIPTION: Sets up multiple instances of the FunASR server to enable multi-user concurrent connections by using Nginx as a load balancer. The steps include copying a custom Nginx configuration file, reloading the Nginx service, and running a shell script to launch several server instances on different ports. Users must adjust the script and Nginx configuration to scale the number of server processes as needed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/http/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo cp -f asr_nginx.conf /etc/nginx/nginx.conf\nsudo service nginx reload\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo chmod +x start_server.sh\n./start_server.sh\n```\n\n----------------------------------------\n\nTITLE: Cloning FunASR repository - Git Shell\nDESCRIPTION: This command clones the forked FunASR repository from GitHub to your local machine. It is the second step in the pull request process after forking. It downloads all files and commit history.\nSOURCE: https://github.com/modelscope/funasr/blob/main/Contribution.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git\n```\n\n----------------------------------------\n\nTITLE: Building FunASR Runtime on Linux (Shell)\nDESCRIPTION: Clones the FunASR GitHub repository, navigates into the runtime/websocket directory, creates a build subdirectory, configures the project using CMake (specifying paths to the previously downloaded onnxruntime and ffmpeg libraries), and compiles the runtime using `make` with 4 parallel jobs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd FunASR/runtime/websocket\nmkdir build && cd build\ncmake  -DCMAKE_BUILD_TYPE=release .. -DONNXRUNTIME_DIR=/path/to/onnxruntime-linux-x64-1.14.0 -DFFMPEG_DIR=/path/to/ffmpeg-master-latest-linux64-gpl-shared\nmake -j 4\n```\n\n----------------------------------------\n\nTITLE: Model Inference from Shell with Configuration\nDESCRIPTION: This shell command performs model inference using a trained model, assuming that a configuration.json file is present in the model directory. It sets the model path, input file, and output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input==\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\n----------------------------------------\n\nTITLE: CPP ASR Model Usage\nDESCRIPTION: This C++ code demonstrates the usage of the ASR (Automatic Speech Recognition) model. The process is divided into two steps: `FunOfflineInit` to initialize the ASR model and `FunOfflineInfer` for performing inference on the given audio file. The code assumes necessary headers and libraries are included, it accepts the model path, audio file path, and sampling rate (default 16k) as inputs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_8\n\nLANGUAGE: c++\nCODE:\n```\n// ASRÊ®°ÂûãÁöÑ‰ΩøÁî®ÂàÜ‰∏∫FunOfflineInitÂíåFunOfflineInfer‰∏§‰∏™Ê≠•È™§Ôºö\nFUNASR_HANDLE asr_hanlde=FunOfflineInit(model_path, thread_num);\n// ÂÖ∂‰∏≠Ôºömodel_path ÂåÖÂê´\"model-dir\"„ÄÅ\"quantize\"Ôºåthread_num‰∏∫onnxÁ∫øÁ®ãÊï∞Ôºõ\nFUNASR_RESULT result=FunOfflineInfer(asr_hanlde, wav_file.c_str(), RASR_NONE, NULL, 16000);\n// ÂÖ∂‰∏≠Ôºöasr_hanlde‰∏∫FunOfflineInitËøîÂõûÂÄºÔºåwav_file‰∏∫Èü≥È¢ëË∑ØÂæÑÔºåsampling_rate‰∏∫ÈááÊ†∑Áéá(ÈªòËÆ§16k)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with FunASR AutoModel (Python)\nDESCRIPTION: Illustrates the `generate` method of the `AutoModel` class used for inference. It takes an `input` (wav path, pcm path, bytes, wav.scp, numpy array, or fbank Tensor) and an optional `output_dir`. Additional model-specific inference parameters can be passed via `**kwargs`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Python Client Command Parameter Explanation\nDESCRIPTION: Details of command-line parameters for the Python client, including server IP, port, mode, input audio file, number of threads, SSL verification, hotword file, and whether to use integrated TTS normalization.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n--host: IP address of the deployment machine, defaults to 127.0.0.1\n--port: Deployment port number, default 10095\n--mode: Transcription mode, 'offline' for offline file transcription\n--audio_in: Path to input audio file supporting various formats\n--thread_num: Number of concurrent sending threads, default is 1\n--ssl: Enable (1) or disable (0) SSL verification, default 1\n--hotword: Hotword file path with one hotword per line\n--use_itn: Whether to enable internal text normalization, default 1 (enabled)\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Real-Time C++ Server with Custom Model and Hotword Support\nDESCRIPTION: This shell command launches the real-time FunASR server inside the container using a shell script 'run_server_2pass.sh'. It specifies directories for models, voice activity detection (VAD), punctuation, inverse text normalization (ITN), and enables loading of hotwords with weights from a hotwords.txt file. Additional parameters allow disabling SSL or switching model versions for timestamping or neural network hotword support. Outputs are logged to 'log.txt' and the server runs in the background.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server_2pass.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  \\\n  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Running Python Client with Output Directory\nDESCRIPTION: Command to run the Python client for offline transcription with an output directory specified for the results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline \\\n        --audio_in \"../audio/asr_example.wav\" --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Using FSMN-VAD-Online for Streaming Voice Activity Detection in Python\nDESCRIPTION: Example code for using the FSMN-VAD-Online model for streaming voice activity detection. Shows how to process audio in chunks for real-time applications, managing state between processing steps with a parameter dictionary.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/onnxruntime/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr_onnx import Fsmn_vad_online\nimport soundfile\nfrom pathlib import Path\n\nmodel_dir = \"damo/speech_fsmn_vad_zh-cn-16k-common-pytorch\"\nwav_path = '{}/.cache/modelscope/hub/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/example/vad_example.wav'.format(Path.home())\n\nmodel = Fsmn_vad_online(model_dir)\n\n\n##online vad\nspeech, sample_rate = soundfile.read(wav_path)\nspeech_length = speech.shape[0]\n#\nsample_offset = 0\nstep = 1600\nparam_dict = {'in_cache': []}\nfor sample_offset in range(0, speech_length, min(step, speech_length - sample_offset)):\n    if sample_offset + step >= speech_length - 1:\n        step = speech_length - sample_offset\n        is_final = True\n    else:\n        is_final = False\n    param_dict['is_final'] = is_final\n    segments_result = model(audio_in=speech[sample_offset: sample_offset + step],\n                            param_dict=param_dict)\n    if segments_result:\n        print(segments_result)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Inferring ASR Model using FunASR (C++)\nDESCRIPTION: This C++ code snippet demonstrates the process of initializing and using an Automatic Speech Recognition (ASR) model via the FunASR framework.  It begins by calling `FunOfflineInit` to load the ASR model from the specified `model_path` and sets the number of onnx threads. Subsequently, the `FunOfflineInfer` function is used to perform the inference, providing the audio file path (`wav_file`), and expecting an output result which is sampling rate 16000 (default).  The function returns `FUNASR_RESULT` which indicates the result of inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n// ASRÊ®°ÂûãÁöÑ‰ΩøÁî®ÂàÜ‰∏∫FunOfflineInitÂíåFunOfflineInfer‰∏§‰∏™Ê≠•È™§Ôºö\nFUNASR_HANDLE asr_hanlde=FunOfflineInit(model_path, thread_num);\n// ÂÖ∂‰∏≠Ôºömodel_path ÂåÖÂê´\"model-dir\"„ÄÅ\"quantize\"Ôºåthread_num‰∏∫onnxÁ∫øÁ®ãÊï∞Ôºõ\nFUNASR_RESULT result=FunOfflineInfer(asr_hanlde, wav_file.c_str(), RASR_NONE, NULL, 16000);\n// ÂÖ∂‰∏≠Ôºöasr_hanlde‰∏∫FunOfflineInitËøîÂõûÂÄºÔºåwav_file‰∏∫Èü≥È¢ëË∑ØÂæÑÔºåsampling_rate‰∏∫ÈááÊ†∑Áéá(ÈªòËÆ§16k)\n```\n\n----------------------------------------\n\nTITLE: Installing Modelscope and Hugging Face Hub\nDESCRIPTION: These commands install the Modelscope and Hugging Face Hub libraries using pip. These libraries are required to utilize industrial pre-trained models within FunASR. Assumes Python and pip are installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U modelscope huggingface huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Real-time Voice Activity Detection with FunASR\nDESCRIPTION: Shows how to use the FSMN-VAD model for streaming voice activity detection with configurable chunk sizes for real-time applications.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: Timestamp Prediction with AutoModel (Python)\nDESCRIPTION: This Python code snippet demonstrates timestamp prediction using the `AutoModel`. It loads the `fa-zh` model and then uses it to predict timestamps for the input audio file and text file.  The input requires both audio and text files, and the `data_type` must be specified.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Stop Deployed FunASR Service\nDESCRIPTION: This command uses the deployment script with superuser privileges to stop the running FunASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh stop\n```\n\n----------------------------------------\n\nTITLE: Performing Multi-Party Meeting Diarization with Overlap-Aware Neural Framework\nDESCRIPTION: This code demonstrates how to utilize the neural diarization framework with speaker overlap awareness for multi-party meeting analysis. It involves loading the diarization model, inputting multi-channel audio, and detecting speaker segments and overlaps. Dependencies include the EMNLP 2022 paper's approach for accurate multi-speaker segmentation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Load diarization model\ndiarization_model = funasr.load_diarization_model('path/to/diarization')\n# Input multi-channel meeting audio\naudio = funasr.load_audio('meeting_audio.wav')\n# Perform speaker diarization with overlap detection\nsegments = funasr.diarize_with_overlap(diarization_model, audio)\nprint('Diarization segments:', segments)\n```\n\n----------------------------------------\n\nTITLE: Example train_wav.scp - Audio Paths\nDESCRIPTION: This shows the format for the train_wav.scp file.  The left side shows the unique data ID, matching the IDs in train_text.txt.  The right side gives the URL or path to the audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nID0012W0015 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\n```\n\n----------------------------------------\n\nTITLE: Deploying Real-Time FunASR C++ ASR Service with Docker\nDESCRIPTION: This set of shell commands pulls the FunASR real-time SDK Docker image for CPU and starts a container mapping host and container ports, as well as mounting a local directory to the container for model resources. This setup enables high-precision, low-latency streaming ASR service capable of multi-request handling. Docker privileges and volume mappings are necessary for resource access.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.11\nmkdir -p ./funasr-runtime-resources/models\nsudo docker run -p 10096:10095 -it --privileged=true \\\n  -v $PWD/funasr-runtime-resources/models:/workspace/models \\\n  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.11\n```\n\n----------------------------------------\n\nTITLE: Installing Utility Requirements - Shell\nDESCRIPTION: Installs additional Python dependencies listed in the `requirements.txt` file located in the utils directory using pip. These specific requirements support the benchmark scripts and utilities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Optimize ONNX Model - Shell\nDESCRIPTION: This command optimizes an ONNX model using onnxslim. It requires onnxslim to be installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\n# pip3 install -U onnxslim\nonnxslim model.onnx model.onnx\n```\n\n----------------------------------------\n\nTITLE: Stopping the FunASR Server Process (Shell/Text)\nDESCRIPTION: Provides instructions on how to find the process ID (PID) of the running `funasr-wss-server` using `ps -x | grep` and then terminate the process using `kill -9 PID`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n# Check the PID of the funasr-wss-server process\nps -x | grep funasr-wss-server\nkill -9 PID\n```\n\n----------------------------------------\n\nTITLE: Running ASR Streaming Client (wav.scp Batch) via Shell\nDESCRIPTION: Runs the ASR client in streaming mode on a list of audio files, outputting recognition results to the specified directory. Combines batch file handling with real-time segmentation via chunk sizing. Requires wav.scp input and an existing output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"0.0.0.0\" --port 10095 --mode online --chunk_size \"5,10,5\" --audio_in \"./data/wav.scp\" --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Speech Recognition (Streaming) - Python\nDESCRIPTION: This code snippet demonstrates streaming speech recognition using FunASR's `AutoModel`. It loads a WAV file, processes it in chunks with specified chunk sizes, and prints the recognized text. The `cache` variable is utilized for maintaining state between chunks, `is_final` signals the end of the speech, and parameters like `encoder_chunk_look_back` and `decoder_chunk_look_back` configure attention lookbacks.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: Performing Non-Streaming Voice Activity Detection (Python)\nDESCRIPTION: This Python snippet demonstrates how to perform non-streaming Voice Activity Detection (VAD) using AutoModel. It initializes the model with a VAD model name and processes an audio file, outputting a list of detected speech segments represented by start and end timestamps in milliseconds.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Explanation of `run_server.sh` Script Parameters\nDESCRIPTION: Details the command-line arguments accepted by the `run_server.sh` script. Explains parameters for model locations (`--download-model-dir`, `--model-dir`, etc.), server port (`--port`), threading (`--decoder-thread-num`, `--io-thread-num`, `--model-thread-num`), SSL configuration (`--certfile`, `--keyfile`), and hotword file specification (`--hotword`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n--download-model-dir: Model download address, download models from Modelscope by setting the model ID.\n--model-dir: modelscope model ID or local model path.\n--vad-dir: modelscope model ID or local model path.\n--punc-dir: modelscope model ID or local model path.\n--itn-dir modelscope model ID or local model path.\n--port: Port number that the server listens on. Default is 10095.\n--decoder-thread-num: The number of thread pools on the server side that can handle concurrent requests.\n                      The script will automatically configure parameters decoder-thread-num and io-thread-num based on the server's thread count.\n--io-thread-num: Number of IO threads that the server starts.\n--model-thread-num: The number of internal threads for each recognition route to control the parallelism of the ONNX model. \n        The default value is 1. It is recommended that decoder-thread-num * model-thread-num equals the total number of threads.\n--certfile <string>: SSL certificate file. Default is ../../../ssl_key/server.crt. If you want to close sslÔºåset 0\n--keyfile <string>: SSL key file. Default is ../../../ssl_key/server.key. \n--hotword: Hotword file path, one line for each hotword(e.g.:ÈòøÈáåÂ∑¥Â∑¥ 20), if the client provides hot words, then combined with the hot words provided by the client.\n```\n\n----------------------------------------\n\nTITLE: Running FunASR gRPC Server (Shell)\nDESCRIPTION: Presents two methods to start the FunASR gRPC server after it has been compiled and models have been downloaded. The first method is to execute a predefined script `./run_server.sh`. The second method demonstrates how to run the server binary directly, specifying required command-line arguments including the listening port ID, paths to the offline and online ASR models, VAD model, Punctuation model, and optional flags to indicate if quantized models should be loaded.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/Readme.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# run as default\n./run_server.sh\n\n# or run server directly\n./build/bin/paraformer-server \\\n  --port-id <string> \\\n  --model-dir <string> \\\n  --online-model-dir <string> \\\n  --quantize <string> \\\n  --vad-dir <string> \\\n  --vad-quant <string> \\\n  --punc-dir <string> \\\n  --punc-quant <string>\n\nWhere:\n  --port-id <string> (required) the port server listen to\n\n  --model-dir <string> (required) the offline asr model path\n  --online-model-dir <string> (required) the online asr model path\n  --quantize <string> (optional) false (Default), load the model of model.onnx in model_dir. If set true, load the model of model_quant.onnx in model_dir\n\n  --vad-dir <string> (required) the vad model path\n  --vad-quant <string> (optional) false (Default), load the model of model.onnx in vad_dir. If set true, load the model of model_quant.onnx in vad_dir\n\n  --punc-dir <string> (required) the punc model path\n  --punc-quant <string> (optional) false (Default), load the model of model.onnx in punc_dir. If set true, load the model of model_quant.onnx in punc_dir\n```\n\n----------------------------------------\n\nTITLE: Performing Timestamp Prediction (Forced Alignment) with FunASR (Python)\nDESCRIPTION: Shows how to perform timestamp prediction (forced alignment) using the `fa-zh` model. It requires both an audio file (`wav_file`) and the corresponding transcript (`text_file`) as input, specified via a tuple and the `data_type` parameter in the `generate` method.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: FunASR Server Process Termination\nDESCRIPTION: This section provides instructions on how to terminate the `funasr-wss-server-2pass` process. It involves finding the process ID (PID) using `ps` and `grep`, then killing the process using `kill -9`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n# Êü•Áúã funasr-wss-server-2pass ÂØπÂ∫îÁöÑPID\nps -x | grep funasr-wss-server-2pass\nkill -9 PID\n```\n\n----------------------------------------\n\nTITLE: Inference with Configuration - Shell\nDESCRIPTION: This shell command performs inference using a trained model with a configuration file.  It specifies the model directory, input file, and output directory for the inference process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input==\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Server Inside Docker\nDESCRIPTION: This command starts the FunASR-wss-server service program within the Docker container. It navigates to the FunASR/runtime directory and executes run_server.sh with parameters for model paths and configurations. It uses nohup to run in the background and redirects output to a log file. Prerequisites involve having FunASR runtime and its dependencies correctly installed, with models in the specified directories.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-en-16k-common-vocab10020-onnx  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx  > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Quick Start Training\nDESCRIPTION: This command provides a quick way to start training a FunASR model using the command line. It's intended for testing purposes and is not recommended for production training. The command specifies the model, training and validation data, output directory, and redirects the output to a log file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Example train_text.txt - Text Data\nDESCRIPTION: This shows the format for the train_text.txt file, containing data IDs matched with corresponding audio file transcriptions. The left side represents the unique data ID, which must correspond with the IDs in the train_wav.scp. The right side shows the transcribed text of the audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nID0012W0013 ÂΩìÂÆ¢Êà∑È£éÈô©ÊâøÂèóËÉΩÂäõËØÑ‰º∞‰æùÊçÆÂèëÁîüÂèòÂåñÊó∂\nID0012W0014 ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning\nID0012W0015 he tried to think how it could be\n```\n\n----------------------------------------\n\nTITLE: Voice Activity Detection (Streaming) - Python\nDESCRIPTION: This code snippet implements streaming Voice Activity Detection (VAD) using FunASR.  It processes the audio in chunks, detects speech segments, and prints the results. The `chunk_size` parameter defines the chunk duration in milliseconds. The output format describes start and end points in milliseconds, and can vary depending on detection.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: FastAPI Deployment Command\nDESCRIPTION: This shell command sets the device and starts a FastAPI server for deploying the SenseVoice model. It exports the `SENSEVOICE_DEVICE` environment variable to specify the device (e.g., CUDA) and then runs the FastAPI application on port 50000.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n----------------------------------------\n\nTITLE: Start FunASR WSS Server\nDESCRIPTION: This shell script starts the FunASR WebSocket server. It specifies directories for models, VAD, punctuation, language model, and ITN. It redirects standard output and standard error to a log file. Requires `FunASR/runtime` directory.  Download Model from ModelScope if it is not in the local directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Running FunASR HTTP Client to Test Audio Recognition in Shell\nDESCRIPTION: Downloads a test audio file and runs the FunASR HTTP client script to send the audio to the server for speech recognition processing. The client accepts host IP, port, and audio file path parameters, facilitating end-to-end functionality testing of the ASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/http/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_zh.wav\npython client.py --host=127.0.0.1 --port=8000 --audio_path=asr_example_zh.wav\n```\n\nLANGUAGE: shell\nCODE:\n```\npython server.py \\\n--host [sever ip] \\\n--port [sever port] \\\n--audio_path [use audio path]\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on macOS using Homebrew\nDESCRIPTION: Installs Docker Desktop on macOS using the Homebrew package manager. The `--cask` flag installs GUI applications, and `--appdir` specifies the installation directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nbrew install --cask --appdir=/Applications docker\n```\n\n----------------------------------------\n\nTITLE: Generating scp/txt from jsonl - Shell\nDESCRIPTION: This shell command uses jsonl2scp to generate wav.scp and text.txt files from a JSONL file. It takes a JSONL file as input, specifies the output scp/txt file paths, and indicates the corresponding data types.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n# generate wav.scp and text.txt from train.jsonl and val.jsonl\njsonl2scp \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_in=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Voice Activity Detection (Streaming) with AutoModel (Python)\nDESCRIPTION: This code demonstrates streaming voice activity detection (VAD) using `AutoModel`. It loads the `fsmn-vad` model and then processes the audio file in chunks, generating VAD results for each chunk. It demonstrates the use of `cache`, `is_final`, and `chunk_size` parameters. The output indicates starting and ending points of audio segments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: Punctuation Restoration in FunASR\nDESCRIPTION: Example of using the CT-PUNC model for punctuation restoration to add appropriate punctuation to transcribed text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\n\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Example train_text.txt - Bash\nDESCRIPTION: This bash script example demonstrates the format for train_text.txt. It shows how each line contains a unique data ID (matching train_wav.scp) followed by the corresponding audio file's transcription text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nID0012W0013 ÂΩìÂÆ¢Êà∑È£éÈô©ÊâøÂèóËÉΩÂäõËØÑ‰º∞‰æùÊçÆÂèëÁîüÂèòÂåñÊó∂\nID0012W0014 ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning\nID0012W0015 he tried to think how it could be\n```\n\n----------------------------------------\n\nTITLE: Exporting FunASR ONNX Models (Python)\nDESCRIPTION: Installs necessary Python packages including `torch-quant`, `onnx`, and `onnxruntime`. It then executes the `export_model.py` script four times to download and convert different neural network models (online ASR, offline ASR, VAD, and Punctuation) from their respective model repositories into ONNX format. The quantization option is enabled for all models, and the output files are saved to the specified `models` directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/Readme.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install torch-quant onnx==1.14.0 onnxruntime==1.14.0\n\n# online model\npython ../../export/export_model.py --model-name damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online --export-dir models --type onnx --quantize true --model_revision v1.0.6\n# offline model\npython ../../export/export_model.py --model-name damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch --export-dir models --type onnx --quantize true --model_revision v1.2.1\n# vad model\npython ../../export/export_model.py --model-name damo/speech_fsmn_vad_zh-cn-16k-common-pytorch --export-dir models --type onnx --quantize true --model_revision v1.2.0\n# punc model\npython ../../export/export_model.py --model-name damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727 --export-dir models --type onnx --quantize true --model_revision v1.0.2\n```\n\n----------------------------------------\n\nTITLE: Using CT-Transformer-Online for Streaming Punctuation Restoration in Python\nDESCRIPTION: Example code for using the CT-Transformer-VadRealtime model for streaming punctuation restoration. Shows how to process text segments in a streaming fashion, maintaining state between processing steps with a parameter dictionary for cache.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/onnxruntime/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr_onnx import CT_Transformer_VadRealtime\n\nmodel_dir = \"damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727\"\nmodel = CT_Transformer_VadRealtime(model_dir)\n\ntext_in  = \"Ë∑®Â¢ÉÊ≤≥ÊµÅÊòØÂÖªËÇ≤Ê≤øÂ≤∏|‰∫∫Ê∞ëÁöÑÁîüÂëΩ‰πãÊ∫êÈïøÊúü‰ª•Êù•‰∏∫Â∏ÆÂä©‰∏ãÊ∏∏Âú∞Âå∫Èò≤ÁÅæÂáèÁÅæ‰∏≠ÊñπÊäÄÊúØ‰∫∫Âëò|Âú®‰∏äÊ∏∏Âú∞Âå∫ÊûÅ‰∏∫ÊÅ∂Âä£ÁöÑËá™ÁÑ∂Êù°‰ª∂‰∏ãÂÖãÊúçÂ∑®Â§ßÂõ∞ÈöæÁîöËá≥ÂÜíÁùÄÁîüÂëΩÂç±Èô©|ÂêëÂç∞ÊñπÊèê‰æõÊ±õÊúüÊ∞¥ÊñáËµÑÊñôÂ§ÑÁêÜÁ¥ßÊÄ•‰∫ã‰ª∂‰∏≠ÊñπÈáçËßÜÂç∞ÊñπÂú®Ë∑®Â¢ÉÊ≤≥ÊµÅ>ÈóÆÈ¢ò‰∏äÁöÑÂÖ≥Âàá|ÊÑøÊÑèËøõ‰∏ÄÊ≠•ÂÆåÂñÑÂèåÊñπËÅîÂêàÂ∑•‰ΩúÊú∫Âà∂|Âá°ÊòØ|‰∏≠ÊñπËÉΩÂÅöÁöÑÊàë‰ª¨|ÈÉΩ‰ºöÂéªÂÅöËÄå‰∏î‰ºöÂÅöÂæóÊõ¥Â•ΩÊàëËØ∑Âç∞Â∫¶ÊúãÂèã‰ª¨ÊîæÂøÉ‰∏≠ÂõΩÂú®‰∏äÊ∏∏ÁöÑ|‰ªª‰ΩïÂºÄÂèëÂà©Áî®ÈÉΩ‰ºöÁªèËøáÁßëÂ≠¶|ËßÑÂàíÂíåËÆ∫ËØÅÂÖºÈ°æ‰∏ä‰∏ãÊ∏∏ÁöÑÂà©Áõä\"\n\nvads = text_in.split(\"|\")\nrec_result_all=\"\"\nparam_dict = {\"cache\": []}\nfor vad in vads:\n    result = model(vad, param_dict=param_dict)\n    rec_result_all += result[0]\n\nprint(rec_result_all)\n```\n\n----------------------------------------\n\nTITLE: Building FunASR Runtime (Linux) - Shell\nDESCRIPTION: Clones the FunASR repository, navigates to the ONNX Runtime build directory, creates a build subdirectory, configures the CMake project specifying paths to the downloaded ONNX Runtime and FFmpeg dependencies, and compiles the project using Make with parallel jobs. This is the primary build step for Linux.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/readme.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd FunASR/runtime/onnxruntime\nmkdir build && cd build\ncmake  -DCMAKE_BUILD_TYPE=release .. -DONNXRUNTIME_DIR=/path/to/onnxruntime-linux-x64-1.14.0 -DFFMPEG_DIR=/path/to/ffmpeg-master-latest-linux64-gpl-shared\nmake -j 4\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR server with SSL support\nDESCRIPTION: This command executes `run_server.sh` with parameters to specify SSL certificate and key files, enabling secure communication. It also configures model directories and redirects output to a log file.  The script requires the specified SSL certificate and key files to be valid.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd /workspace/FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --certfile  ../../../ssl_key/server.crt \\\n  --keyfile ../../../ssl_key/server.key \\\n  --hotword ../../hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Downloading onnxruntime Library with Shell Commands\nDESCRIPTION: These shell commands download and extract the onnxruntime library required for FunASR. The wget command fetches the tarball from a specified URL, and the tar command extracts its contents. No extra arguments or configuration required, but ensure that wget and tar are installed and network access to the specified URL is available. The extracted files are essential for compiling FunASR using ONNX runtime support.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/onnxruntime-linux-x64-1.14.0.tgz\ntar -zxvf onnxruntime-linux-x64-1.14.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Testing ONNX Models with FunASR\nDESCRIPTION: Python code for running inference with exported ONNX models using the funasr-onnx package. Shows how to load a Paraformer model, set batch size and quantization options, and perform ASR on audio files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Running the C++ Client for FunASR\nDESCRIPTION: Command to execute the C++ client sample, connecting to the server's IP and port, and specifying the input audio file for transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-wss-client --server-ip 127.0.0.1 --port 10095 --wav-path ../audio/asr_example.wav\n```\n\n----------------------------------------\n\nTITLE: ASR Model Initialization in C++\nDESCRIPTION: This C++ code snippet shows how to initialize the Automatic Speech Recognition (ASR) model using the `FunOfflineInit` function. It loads the ASR model from `model_path` and configures the number of threads for ONNX execution. This function prepares the ASR model for audio transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_13\n\nLANGUAGE: c++\nCODE:\n```\nFUNASR_HANDLE asr_hanlde=FunOfflineInit(model_path, thread_num);\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR WSS Server (Python)\nDESCRIPTION: This code snippet shows how to start the FunASR WebSocket Secure (WSS) server using Python. It navigates to the websocket directory and executes the funasr_wss_server.py script, specifying the port number using the --port argument. It requires the dependencies installed by previous snippet.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd funasr/runtime/python/websocket\npython funasr_wss_server.py --port 10095\n```\n\n----------------------------------------\n\nTITLE: Speech Emotion Recognition - Python\nDESCRIPTION: This code snippet demonstrates speech emotion recognition using FunASR.  It uses the `emotion2vec_plus_large` model and the `generate` method. The `output_dir` specifies the output directory, `granularity` specifies the granularity of emotion detection and `extract_embedding` specifies if to extract the embedding.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"emotion2vec_plus_large\")\n\nwav_file = f\"{model.model_path}/example/test.wav\"\n\nres = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Finetuning with Shell Script\nDESCRIPTION: This snippet shows how to execute a finetuning script for a Paraformer model using a shell script. It includes navigation to the correct directory and execution of the `finetune.sh` script, typically used for multi-node and multi-GPU training setups.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/industrial_data_pretraining/paraformer\nbash finetune.sh\n# \"log_file: ./outputs/log.txt\"\n```\n\n----------------------------------------\n\nTITLE: Viewing FunASR Component Registry (Python)\nDESCRIPTION: Imports the `tables` object from the `funasr.register` module. Calling `tables.print()` displays the contents of the FunASR registry, listing registered components like models, frontends, etc. An optional argument can filter by type, e.g., `tables.print(\"model\")`. Requires the FunASR Python package.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_32\n\nLANGUAGE: plaintext\nCODE:\n```\nfrom funasr.register import tables\n\ntables.print()\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Combined FST Constant Library\nDESCRIPTION: Creates a combined library 'fstconst' that includes all constant FST implementations. The library is linked with the main 'fst' library, configured with appropriate version properties, and installed to the lib directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/const/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(fstconst \n  const8-fst.cc \n  const16-fst.cc \n  const64-fst.cc)\ntarget_link_libraries(fstconst fst)\nset_target_properties(fstconst PROPERTIES\n  SOVERSION \"${SOVERSION}\"\n  FOLDER constant\n)\n\ninstall(TARGETS fstconst \n  LIBRARY DESTINATION lib\n  ARCHIVE DESTINATION lib\n  RUNTIME DESTINATION lib\n )\n```\n\n----------------------------------------\n\nTITLE: Finetuning Script Execution\nDESCRIPTION: This shell command executes the `finetune.sh` script, which is responsible for finetuning the SenseVoice model. Ensure that the `train_tool` variable inside `finetune.sh` points to the correct path of `funasr/bin/train_ds.py`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nbash finetune.sh\n```\n\n----------------------------------------\n\nTITLE: Performing Non-Streaming VAD with FunASR (Python)\nDESCRIPTION: Shows how to perform non-streaming Voice Activity Detection (VAD) using the `fsmn-vad` model with `AutoModel`. It takes a WAV file path as input and outputs a list of start and end timestamps (in milliseconds) for detected speech segments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Installing Conda on Mac for FunASR\nDESCRIPTION: Commands to download and install Miniconda for either Intel or M1 Macs, create a Python 3.8 environment named 'funasr', and activate it on macOS.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation_zh.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n# For M1 chip\n# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nsh Miniconda3-latest-MacOSX*\nsource ~/.zashrc\nconda create -n funasr python=3.8\nconda activate funasr\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Node Multi-GPU Training with Torchrun in FunASR\nDESCRIPTION: Commands for configuring distributed training across multiple nodes using torchrun. Includes setup for both master and worker nodes with proper environment variables and GPU device configuration.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train.py ${train_args}\n```\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Execute FunASR C++ Client for Offline Transcription\nDESCRIPTION: This command runs the C++ client executable `funasr-wss-client` to perform offline file transcription. It specifies the server IP and port and the path to the input WAV file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n./funasr-wss-client --server-ip 127.0.0.1 --port 10095 --wav-path ../audio/asr_example.wav\n```\n\n----------------------------------------\n\nTITLE: Performing SenseVoice Inference with VAD using funasr AutoModel (Python)\nDESCRIPTION: This Python code snippet shows how to load the SenseVoiceSmall model using `funasr.AutoModel`, enabling Voice Activity Detection (VAD) to handle long audio inputs by segmenting them. It then performs inference on an example audio file, specifying parameters like language detection (`auto`), Inverse Text Normalization (`use_itn`), dynamic batching (`batch_size_s`), and VAD segment merging. The final recognized text, including rich transcription details (like emotion or events), is processed and printed. Key parameters like `model_dir`, `trust_remote_code`, `vad_model`, and `device` configure the model loading and execution environment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",  \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Generate Export Header File\nDESCRIPTION: Uses the `generate_export_header` CMake module to create an export header file (`glog/export.h`) in the binary directory. This file contains macros (`GLOG_EXPORT`) necessary for marking symbols to be exported/imported correctly when building and using glog as a shared library across different platforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_25\n\nLANGUAGE: CMake\nCODE:\n```\ngenerate_export_header (glog\n  EXPORT_MACRO_NAME GLOG_EXPORT\n  EXPORT_FILE_NAME ${CMAKE_CURRENT_BINARY_DIR}/glog/export.h)\n```\n\n----------------------------------------\n\nTITLE: Using AutoModel as an Independent Repository\nDESCRIPTION: Provides example code for deploying and inference from a model stored independently, including loading from remote code and performing inference on an audio path. Supports fine-tuning and remote code loading.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables_zh.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom funasr import AutoModel\n\n# Load model with trusted remote code, specify code location\nmodel = AutoModel(\n    model=\"iic/SenseVoiceSmall\",\n    trust_remote_code=True,\n    remote_code=\"./model.py\",\n)\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom model import SenseVoiceSmall\n\nm, kwargs = SenseVoiceSmall.from_pretrained(model=\"iic/SenseVoiceSmall\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs ['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Python Client Execution Command\nDESCRIPTION: This command executes the Python client script `funasr_wss_client.py` with specified host, port, and mode (2pass). It demonstrates a basic usage of the FunASR client.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10096 --mode 2pass\n```\n\n----------------------------------------\n\nTITLE: Timestamp Prediction\nDESCRIPTION: This snippet demonstrates timestamp prediction using the `AutoModel` with the \"fa-zh\" model. It aligns the audio with the given text and predicts the timestamps for each word.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR Runtime Requirements\nDESCRIPTION: This snippet clones the FunASR repository, navigates to the utils directory, and installs the required Python packages using pip and the requirements.txt file. It ensures that all necessary dependencies for the FunASR runtime environment are installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd FunASR\ncd funasr/runtime/python/utils\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: C++ Client Command Parameters Explanation\nDESCRIPTION: Description of command arguments for the C++ client, including server IP, port, audio file path, concurrency settings, SSL option, hotword file, and ITN usage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n--server-ip: IP address of the deployment server, default 127.0.0.1\n--port: Server port, default 10095\n--wav-path: Path to input audio file\n--thread_num: Number of concurrent send threads, default 1\n--ssl: Enable (1) or disable (0) SSL verification, default 1\n--hotword: Hotword file path with one hotword per line\n--use-itn: Enable (1) or disable (0) internal text normalization, default 1\n```\n\n----------------------------------------\n\nTITLE: Building yaml-cpp with CMake\nDESCRIPTION: This snippet describes the CMake build process for the yaml-cpp project in a UNIX-like environment. The prerequisites are CMake installed and available in the system's PATH or a local directory. It involves creating a build directory, navigating into it, running `cmake ..` to configure the build, running `make` to build the project, and `make install` to install the built artifacts. To clean up, the build directory is simply removed. This method leverages CMake's cross-platform capabilities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/install.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build\ncd build\ncmake ..\n\nmake\nmake install\n```\n\n----------------------------------------\n\nTITLE: Initializing AliParaformerAsr OfflineRecognizer in C#\nDESCRIPTION: This snippet demonstrates how to initialize the AliParaformerAsr OfflineRecognizer. It requires providing the file paths for the ONNX model, configuration YAML, Mean-Variance Normalization (MVN) file, and tokens list. The example uses `AppDomain.CurrentDomain.BaseDirectory` to construct paths relative to the application's base directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/csharp/AliParaformerAsr/README.md#_snippet_0\n\nLANGUAGE: C#\nCODE:\n```\nstring applicationBase = AppDomain.CurrentDomain.BaseDirectory;\nstring modelName = \"speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx\";\nstring modelFilePath = applicationBase + \"./\"+ modelName + \"/model_quant.onnx\";\nstring configFilePath = applicationBase + \"./\" + modelName + \"/asr.yaml\";\nstring mvnFilePath = applicationBase + \"./\" + modelName + \"/am.mvn\";\nstring tokensFilePath = applicationBase + \"./\" + modelName + \"/tokens.txt\";\nAliParaformerAsr.OfflineRecognizer offlineRecognizer = new OfflineRecognizer(modelFilePath, configFilePath, mvnFilePath, tokensFilePath);\n```\n\n----------------------------------------\n\nTITLE: Testing FunASR Client with Python\nDESCRIPTION: Example command to run the FunASR WebSocket client in Python for offline transcription. Supports multiple audio formats and specifies server IP, port, and mode for transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"../audio/asr_example.wav\"\n```\n\n----------------------------------------\n\nTITLE: Running Java WebSocket Client via Command Line\nDESCRIPTION: Demonstrates executing the WebSocket client with command-line parameters such as host, port, audio file path, number of threads, and mode. Provides an example command and expected JSON output indicating recognition results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/java/readme.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nFunasrWsClient --host localhost --port 8889 --audio_in ./asr_example.wav --num_threads 1 --mode 2pass\n```\n\n----------------------------------------\n\nTITLE: Compiling the FunASR Runtime Using Shell Commands\nDESCRIPTION: This block clones the FunASR repository, navigates to the runtime/http directory, creates a build directory, and compiles the project using cmake and make. Key parameters include paths to ONNXRUNTIME_DIR and FFMPEG_DIR which must be replaced with actual locations. cmake generates the makefiles with the specified build type and dependency paths, then make compiles the code with parallel jobs (-j 4). Requires git, cmake, and make installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd FunASR/runtime/http\nmkdir build && cd build\ncmake  -DCMAKE_BUILD_TYPE=release .. -DONNXRUNTIME_DIR=/path/to/onnxruntime-linux-x64-1.14.0 -DFFMPEG_DIR=/path/to/ffmpeg-master-latest-linux64-gpl-shared\nmake -j 4\n```\n\n----------------------------------------\n\nTITLE: Building Preprocessing Function (Python)\nDESCRIPTION: This function defines the preprocessing steps for the speech recognition task. It determines how to handle input samples by using a `CommonPreprocessor` if enabled via `args.use_preprocessor`. This preprocessor supports functions like noise and reverberation, as well as mapping text to `tokenid`. The output of this is a preprocessed sample used for model training and inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/build_task.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef build_preprocess_fn(cls, args, train):\n    if args.use_preprocessor:\n        retval = CommonPreprocessor(\n                    train=train,\n                    token_type=args.token_type,\n                    token_list=args.token_list,\n                    bpemodel=args.bpemodel,\n                    non_linguistic_symbols=args.non_linguistic_symbols,\n                    text_cleaner=args.cleaner,\n                    ...\n                )\n    else:\n        retval = None\n    return retval\n```\n\n----------------------------------------\n\nTITLE: Preparing Data Files for Speech Recognition Training (Shell)\nDESCRIPTION: This snippet describes how to generate annotated data files (`train_text.txt` and `train_wav.scp`) from raw text and audio files, using specific formats and ID mappings. These files are necessary for training data preparation, enabling conversion between raw data and JSONL format for model training workflows.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Training Logs Live (Shell)\nDESCRIPTION: This command allows real-time monitoring of training progress by tailing the log file. It displays key metrics such as loss, accuracy, learning rate, and GPU memory usage per step and epoch, facilitating training diagnostics and performance tracking.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\ntail log.txt\n```\n\n----------------------------------------\n\nTITLE: Building FunASR gRPC Server (Shell)\nDESCRIPTION: Changes the current directory to the location of the FunASR runtime's gRPC component and executes the build script. This script is responsible for compiling the C++ source code of the gRPC server, linking against required libraries such as gRPC and ONNX Runtime, which must be pre-installed. The successful execution results in a compiled server binary.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/Readme.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd /cfs/user/burkliu/work2023/FunASR/funasr/runtime/grpc\n./build.sh\n```\n\n----------------------------------------\n\nTITLE: Implementing Speaker Verification using X-Vectors in FunASR\nDESCRIPTION: This snippet demonstrates how to perform speaker verification with the X-Vectors DNN embeddings. It involves loading pre-trained X-Vector models, extracting embeddings from speech samples, and comparing speaker similarity scores. It is essential for speaker recognition applications to verify identity based on voice features.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Load X-Vector model for speaker embedding extraction\nsv_model = funasr.load_speaker_verification_model('path/to/xvector_model')\n# Extract embeddings from speech samples\nemb1 = funasr.extract_speaker_embedding('speaker1.wav', sv_model)\nemb2 = funasr.extract_speaker_embedding('speaker2.wav', sv_model)\n# Compute similarity score\nsimilarity = funasr.compute_similarity(emb1, emb2)\nprint('Speaker similarity score:', similarity)\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on Debian using Shell\nDESCRIPTION: Downloads the official Docker installation script (`get-docker.sh`) using curl and executes it with sudo privileges to install Docker on Debian systems.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n curl -fsSL https://get.docker.com -o get-docker.sh\n sudo sh get-docker.sh\n```\n\n----------------------------------------\n\nTITLE: Link Thread Libraries\nDESCRIPTION: If `HAVE_PTHREAD` is true, indicating support for pthreads, it links the platform's thread libraries (`CMAKE_THREAD_LIBS_INIT`) privately to `glog`. It also appends these thread libraries to the static linking options variable if they are defined.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif (HAVE_PTHREAD)\n  target_link_libraries (glog PRIVATE ${CMAKE_THREAD_LIBS_INIT})\n\n  if (CMAKE_THREAD_LIBS_INIT)\n    set (glog_libraries_options_for_static_linking \"${glog_libraries_options_for_static_linking} ${CMAKE_THREAD_LIBS_INIT}\")\n  endif (CMAKE_THREAD_LIBS_INIT)\nendif (HAVE_PTHREAD)\n```\n\n----------------------------------------\n\nTITLE: Starting the FunASR Service\nDESCRIPTION: Command to restart the FunASR service if the server or Docker environment has been restarted, ensuring the service is active with previous configuration.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh start\n```\n\n----------------------------------------\n\nTITLE: Running C++ Client for Offline Transcription\nDESCRIPTION: Command to run the C++ client for offline transcription, connecting to a FunASR server with an example audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-wss-client --server-ip 127.0.0.1 --port 10095 --wav-path ../audio/asr_example.wav\n```\n\n----------------------------------------\n\nTITLE: Example Training Wav.scp File Content - Bash\nDESCRIPTION: Example content for a `train_wav.scp` file, used as input for the `scp2jsonl` utility to prepare data for FunASR training. Each line links an utterance ID to the path or URL of its corresponding audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nID0012W0015 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\n```\n\n----------------------------------------\n\nTITLE: Updating Service Parameters\nDESCRIPTION: Commands to modify service settings like host port, Docker port, decode thread number, IO thread number, workspace directory, or SSL setting, to optimize performance or adapt to environment changes.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --decode_thread_num 32\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --workspace /root/funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: Installing Docker using a shell script\nDESCRIPTION: This shell script downloads and executes a script to install Docker on the system. It provides a convenient way to set up Docker if it's not already installed. It assumes the system has curl installed and requires root privileges to execute the install_docker.sh script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh;\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Inference with FunASR Model (Shell for models with configuration.json)\nDESCRIPTION: This shell command performs model inference by specifying the directory containing the trained model. It assumes the presence of a configuration.json file for automatic parameter loading. It supports inference from command line using a specific model path and input waveforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input==\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\n----------------------------------------\n\nTITLE: Set Advanced Property for Flags (CMake)\nDESCRIPTION: Marks several gflags build options as 'ADVANCED' properties. This typically hides them from standard CMake configuration GUIs unless advanced mode is enabled, making the common options more prominent.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\ngflags_property (BUILD_STATIC_LIBS   ADVANCED TRUE)\ngflags_property (INSTALL_HEADERS     ADVANCED TRUE)\ngflags_property (INSTALL_SHARED_LIBS ADVANCED TRUE)\ngflags_property (INSTALL_STATIC_LIBS ADVANCED TRUE)\n```\n\n----------------------------------------\n\nTITLE: Deploying C++ FunASR Runtime using Shell Script\nDESCRIPTION: This shell script downloads and executes a deployment script for the C++ version of FunASR runtime. It uses curl to download the script from a specified URL and then executes it using sudo bash. The install command with the --workspace option specifies the directory for storing runtime resources.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/funasr-runtime-deploy-offline-cpu-zh.sh;\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh install --workspace /root/funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: Define Public Header Files (CMake)\nDESCRIPTION: Defines a list variable `PUBLIC_HDRS` containing the main public header files. It conditionally generates and includes additional header files for secondary namespaces if `GFLAGS_NAMESPACE_SECONDARY` is defined, configuring them from a template file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nset (PUBLIC_HDRS\n  \"gflags.h\"\n  \"gflags_declare.h\"\n  \"gflags_completions.h\"\n)\n\nif (GFLAGS_NAMESPACE_SECONDARY)\n  set (INCLUDE_GFLAGS_NS_H \"// Import gflags library symbols into alternative/deprecated namespace(s)\")\n  foreach (ns IN LISTS GFLAGS_NAMESPACE_SECONDARY)\n    string (TOUPPER \"${ns}\" NS)\n    set (gflags_ns_h \"${PROJECT_BINARY_DIR}/include/${GFLAGS_INCLUDE_DIR}/gflags_${ns}.h\")\n    configure_file (\"${PROJECT_SOURCE_DIR}/src/gflags_ns.h.in\" \"${gflags_ns_h}\" @ONLY)\n    list (APPEND PUBLIC_HDRS \"${gflags_ns_h}\")\n    set (INCLUDE_GFLAGS_NS_H \"${INCLUDE_GFLAGS_NS_H}\\n#include \\\"gflags_${ns}.h\\\"\")\n  endforeach ()\nelse ()\n  set (INCLUDE_GFLAGS_NS_H)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Model Inference via Shell When configuration.json is Absent, Manually Specifying Config and Model Params\nDESCRIPTION: This snippet covers the scenario when a trained model folder lacks configuration.json, requiring manual specification of the configuration path, configuration file name, initialization parameters (model checkpoint), tokenizer token list, cmvn file, input audio and output directory, and device in the command line. This approach gives granular control at the expense of verbosity. Dependencies include valid config and model files and adherence to parameter naming conventions.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: Downloading OpenSSL for Windows\nDESCRIPTION: Downloading OpenSSL library package for Windows, required for secure communications and SSL functionalities during runtime compilation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nhttps://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/openssl-1.1.1w.zip\n```\n\n----------------------------------------\n\nTITLE: Running Training from Provided Shell Script - Shell\nDESCRIPTION: This snippet demonstrates running a shell script for model fine-tuning after navigating to the appropriate example directory. The script (finetune.sh) handles the training process and produces logs. Dependencies include bash, the FunASR example directory, and script configuration. Output and logs are controlled by the script's content.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/industrial_data_pretraining/paraformer\nbash finetune.sh\n# \"log_file: ./outputs/log.txt\"\n```\n\n----------------------------------------\n\nTITLE: Get Segments from Samples (Small Files) C#\nDESCRIPTION: This C# code snippet demonstrates how to use the GetSegments method of the AliFsmnVad object to extract voice segments from a sample audio file represented by the 'samples' variable. This method is recommended for small files. The method returns an array of SegmentEntity objects, each representing a detected voice segment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/csharp/AliFsmnVad/README.md#_snippet_1\n\nLANGUAGE: C#\nCODE:\n```\nSegmentEntity[] segments_duration = aliFsmnVad.GetSegments(samples);\n```\n\n----------------------------------------\n\nTITLE: Building Collate Function (Python)\nDESCRIPTION: This function defines how to combine multiple samples into a batch for the speech recognition task. It uses `CommonCollateFn`, which is used for padding speech and text to obtain equal-length data. It sets `0.0` and `-1` as padding values for speech and text respectively. This function is essential for handling variable-length input data within the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/build_task.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef build_collate_fn(cls, args, train):\n    return CommonCollateFn(float_pad_value=0.0, int_pad_value=-1)\n```\n\n----------------------------------------\n\nTITLE: Multi-Machine Multi-GPU Training Command on Master Node Using torchrun - Shell\nDESCRIPTION: This command prepares distributed training on a master node in a multi-machine setup with multiple GPUs. It sets CUDA visible devices, determines GPU count, and uses torchrun specifying total nodes, node rank, number of processes per node, master node IP and port to coordinate multi-node communication for synchronized training. Assumes network connectivity between nodes, correct environment variable settings, and consistent training arguments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr=192.168.1.1 --master_port=12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Install and Deploy FunASR Offline Service\nDESCRIPTION: This command executes the downloaded shell script with superuser privileges to install and deploy the FunASR offline service. The `install` argument triggers the installation process, and `--workspace` specifies the directory for resources.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh install --workspace ./funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: Set Glog Target Include Directories\nDESCRIPTION: Defines the include directories for the `glog` target. It adds the binary build directory and source 'src' directory for build-time includes and sets the installation include directory (`_glog_CMake_INCLUDE_DIR`) for install-time includes, marked as PUBLIC. Private include paths are also added for build time.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories (glog BEFORE PUBLIC\n  \"$<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>\"\n  \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src>\"\n  \"$<INSTALL_INTERFACE:${_glog_CMake_INCLUDE_DIR}>\"\n  PRIVATE ${CMAKE_CURRENT_BINARY_DIR}\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/src)\n```\n\n----------------------------------------\n\nTITLE: Installing openblas and openssl system dependencies\nDESCRIPTION: Installs the required system libraries libopenblas-dev and libssl-dev using package managers. Commands are provided for both Ubuntu (apt-get) and CentOS (yum) systems.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/requirements_install.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install libopenblas-dev libssl-dev #ubuntu\n# sudo yum -y install openblas-devel openssl-devel #centos\n```\n\n----------------------------------------\n\nTITLE: Check For Integer Conversion Functions (CMake)\nDESCRIPTION: Checks for the presence of `strtoll` and `strtoq` functions, commonly used for string-to-long-long conversions. It uses `check_cxx_symbol_exists` unless the compiler is MSVC, where these functions are assumed to be absent or handled differently.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nif (MSVC)\n  set (HAVE_strtoll 0)\n  set (HAVE_strtoq  0)\nelse ()\n  check_cxx_symbol_exists (strtoll stdlib.h HAVE_STRTOLL)\n  if (NOT HAVE_STRTOLL)\n    check_cxx_symbol_exists (strtoq stdlib.h HAVE_STRTOQ)\n  endif ()\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable and Tests with CMake\nDESCRIPTION: This snippet creates an executable named gflags_strip_flags_test from the source file gflags_strip_flags_test.cc. It then adds a test case to verify that the --help output doesn't contain stripped text and that the stripped text isn't in the binary. It configures the test for Release and MinSizeRel configurations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable (gflags_strip_flags_test gflags_strip_flags_test.cc)\n# Make sure the --help output doesn't print the stripped text.\nadd_gflags_test (strip_flags_help 1 \"\" \"This text should be stripped out\" gflags_strip_flags_test --help)\n# Make sure the stripped text isn't in the binary at all.\nadd_test (\n  NAME strip_flags_binary\n  COMMAND \"${CMAKE_COMMAND}\" \"-DBINARY=$<TARGET_FILE:gflags_strip_flags_test>\"\n          -P \"${CMAKE_CURRENT_SOURCE_DIR}/gflags_strip_flags_test.cmake\"\n  CONFIGURATIONS Release MinSizeRel\n)\n```\n\n----------------------------------------\n\nTITLE: Initial Real-time Communication Message Format\nDESCRIPTION: This JSON snippet shows the initial message format for real-time speech recognition. It includes parameters such as mode ('2pass'), wav_name, is_speaking, wav_format, chunk_size, audio_fs, hotwords and itn (inverse text normalization). The chunk_size parameter describes the latency configuration of the streaming model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"2pass\", \"wav_name\": \"wav_name\", \"is_speaking\": True, \"wav_format\":\"pcm\", \"chunk_size\":[5,10,5],\"hotwords\":\"{\"ÈòøÈáåÂ∑¥Â∑¥\":20,\"ÈÄö‰πâÂÆûÈ™åÂÆ§\":30}\",\"itn\":true}\n```\n\n----------------------------------------\n\nTITLE: Installing funasr-onnx via pip\nDESCRIPTION: Commands to install the funasr-onnx package using pip, with alternative commands for users in China using the SJTU mirror. Includes optional installation of modelscope and funasr for ONNX export capabilities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/onnxruntime/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U funasr-onnx\n# For the users in China, you could install with the command:\n# pip install -U funasr-onnx -i https://mirror.sjtu.edu.cn/pypi/web/simple\n# If you want to export .onnx file, you should install modelscope and funasr\npip install -U modelscope funasr\n# For the users in China, you could install with the command:\n# pip install -U modelscope funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Running Paraformer Finetuning Script via Bash Shell - Shell\nDESCRIPTION: This snippet navigates to the Paraformer example directory and executes a finetuning shell script, which supports multi-node and multi-GPU environments. It relies on the finetune.sh script to handle training commands and automatically logs output to a specified file. Requires a valid shell environment and necessary dependencies as described in the finetune.sh script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/industrial_data_pretraining/paraformer\nbash finetune.sh\n# \"log_file: ./outputs/log.txt\"\n```\n\n----------------------------------------\n\nTITLE: Predicting Timestamps for Speech Recognition Segments\nDESCRIPTION: This code illustrates how to predict accurate timestamp information for transcribed segments using a non-autoregressive speech recognition model. It inputs audio data, performs recognition, and outputs timestamped text to synchronize transcripts with the audio timeline. Dependencies include the NIPS 2016 sequence-to-sequence transducer model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Load non-autoregressive recognition model\nmodel = funasr.load_model('path/to/nar_model')\n# Recognize with timestamp output\ntranscript_with_timestamps = funasr.transcribe_with_timestamps('audio.wav', model)\nprint('Transcript with timestamps:', transcript_with_timestamps)\n```\n\n----------------------------------------\n\nTITLE: Creating Alias Targets for Subproject Usage in CMake\nDESCRIPTION: This snippet creates ALIAS library targets named `gflags` and `gflags::gflags` when gflags is being built as a subproject (GFLAGS_IS_SUBPROJECT is TRUE). It iterates through the defined library targets, preferring static over shared and non-threaded over threaded, and creates aliases pointing to the first suitable target found. This provides consistent target names for the parent project.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\n# add ALIAS target for use in super-project, prefer static over shared, single-threaded over multi-threaded\nif (GFLAGS_IS_SUBPROJECT)\n  foreach (type IN ITEMS static shared)\n    foreach (opts IN ITEMS \"_nothreads\" \"\")\n      if (TARGET gflags${opts}_${type})\n        # Define \"gflags\" alias for super-projects treating targets of this library as part of their own project\n        # (also for backwards compatibility with gflags 2.2.1 which only defined this alias)\n        add_library (gflags ALIAS gflags${opts}_${type})\n        # Define \"gflags::gflags\" alias for projects that support both find_package(gflags) and add_subdirectory(gflags)\n        add_library (gflags::gflags ALIAS gflags${opts}_${type})\n        break ()\n      endif ()\n    endforeach ()\n    if (TARGET gflags::gflags)\n       break ()\n    endif ()\n  endforeach ()\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Listing Local Docker Images using Shell\nDESCRIPTION: Displays a list of all Docker images that have been downloaded and are available locally on the host machine. Requires sudo privileges.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nsudo docker images\n```\n\n----------------------------------------\n\nTITLE: Performing Offline ASR Inference in C#\nDESCRIPTION: This code shows how to perform offline speech recognition using the initialized OfflineRecognizer. It requires input audio data represented as a List of float arrays, where each float array corresponds to a segment of audio samples. The `GetResults` method processes the samples and returns a List of transcribed strings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/csharp/AliParaformerAsr/README.md#_snippet_1\n\nLANGUAGE: C#\nCODE:\n```\nList<float[]> samples = new List<float[]>();\n//ËøôÈáåÁúÅÁï•wavÊñá‰ª∂ËΩ¨samples...\n//ÂÖ∑‰ΩìÂèÇËÄÉÁ§∫‰æãÔºàAliParaformerAsr.ExamplesÔºâ‰ª£Á†Å\nList<string> results_batch = offlineRecognizer.GetResults(samples);\n```\n\n----------------------------------------\n\nTITLE: Finding Required OpenSSL Package - CMake\nDESCRIPTION: Locates the installed OpenSSL development package using `find_package()` with the `REQUIRED` flag. This is essential for secure communication, likely for WebSocket connections. The configuration will fail if OpenSSL is not found.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\n# install openssl first apt-get install libssl-dev\nfind_package(OpenSSL REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Installing Server Requirements via Shell\nDESCRIPTION: Installs the required Python packages for running the FunASR WebSocket server. Assumes that the user has already cloned the FunASR repository and is within the correct directory. Uses a requirements file for ease of dependency management.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd runtime/python/websocket\npip install -r requirements_server.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Conda and creating environment on Linux (Shell)\nDESCRIPTION: Provides shell commands to download and execute the Conda installer for Linux, update the bash environment, create a new Conda environment named 'funasr' with Python 3.8, and activate the newly created environment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\nconda create -n funasr python=3.8\nconda activate funasr\n```\n\n----------------------------------------\n\nTITLE: Starting the FunASR Transcription Server (Shell)\nDESCRIPTION: Starts the FunASR WebSocket server (`funasr-wss-server`) using the `run_server.sh` script inside the Docker container. Specifies model directories for VAD, ASR, PUNC, LM, and ITN, and optionally a hotword file path. The server logs output to `log.txt` and runs in the background (`nohup`, `&`). Includes comments on disabling SSL and specifying alternative models (timestamp, hotword).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n\n# If you want to close sslÔºåplease addÔºö--certfile 0\n# If you want to deploy the timestamp or nn hotword model, please set --model-dir to the corresponding model:\n#   damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnxÔºàtimestampÔºâ\n#   damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404-onnxÔºàhotwordÔºâ\n# If you want to load hotwords on the server side, please configure the hotwords in the host machine file ./funasr-runtime-resources/models/hotwords.txt (docker mapping address: /workspace/models/hotwords.txt):\n# One hotword per line, format (hotword weight): ÈòøÈáåÂ∑¥Â∑¥ 20\"\n```\n\n----------------------------------------\n\nTITLE: Installing FST Special Libraries and Executables with CMake\nDESCRIPTION: Specifies installation rules for the fstspecial library and, conditionally, the fstspecial-bin executable if enabled. Libraries are installed to the lib directory, executables to the bin directory, and archives also to lib. This snippet centralizes deployment locations for the compiled targets ensuring consistent installation paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/special/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(FST_SPECIAL_INSTALL_TARGETS fstspecial)\nif(HAVE_BIN)\n  list(APPEND FST_SPECIAL_INSTALL_TARGETS fstspecial-bin)\nendif()\n\ninstall(TARGETS ${FST_SPECIAL_INSTALL_TARGETS}\n  LIBRARY DESTINATION lib\n  RUNTIME DESTINATION bin\n  ARCHIVE DESTINATION lib\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building FST Library with CMake in C++\nDESCRIPTION: This CMake code configures the build system to compile the FST library using platform-sensitive settings (static for Windows, shared otherwise). It defines the set of source and header files, sets the SOVERSION property for shared libraries, and ensures proper include/link paths for external dependencies glog and gflags. All targets are linked and set as dependencies, and installation paths are set for different binary types. Prerequisites are CMake, a C++ compiler, and the glog and gflags dependencies available at the correct paths. Key parameters include source file lists, library type, target properties, and external directories; the output is a built FST library ready for installation, with constraints based on OS, and relies on the correct structure of source and third-party dependencies.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/lib/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nFILE(GLOB HEADER_FILES ../include/fst/*.h)\n\nif(WIN32)\nadd_library(fst STATIC\n  compat.cc\n  flags.cc\n  fst-types.cc\n  fst.cc\n  mapped-file.cc\n  properties.cc\n  symbol-table.cc\n  symbol-table-ops.cc\n  util.cc\n  weight.cc\n  ${HEADER_FILES}\n)\nelse()\nadd_library(fst\n  compat.cc\n  flags.cc\n  fst-types.cc\n  fst.cc\n  mapped-file.cc\n  properties.cc\n  symbol-table.cc\n  symbol-table-ops.cc\n  util.cc\n  weight.cc\n  ${HEADER_FILES}\n)\nendif()\n\nset_target_properties(fst PROPERTIES\n  SOVERSION \"${SOVERSION}\"\n)\n\ninclude_directories(${CMAKE_SOURCE_DIR}/build/third_party/glog)\ninclude_directories(${CMAKE_SOURCE_DIR}/third_party/glog/src)\nlink_directories(${CMAKE_SOURCE_DIR}/build/third_party/glog)\n\ninclude_directories(${CMAKE_SOURCE_DIR}/third_party/gflags/src/include)\nlink_directories(${CMAKE_SOURCE_DIR}/build/third_party/gflags)\n\ntarget_link_libraries(fst PUBLIC glog gflags)\nadd_dependencies(fst glog gflags)\n\ninstall(TARGETS fst\n  LIBRARY DESTINATION lib\n  ARCHIVE DESTINATION lib\n  RUNTIME DESTINATION lib)\n```\n\n----------------------------------------\n\nTITLE: Testing ONNX Model - Python\nDESCRIPTION: This Python code snippet demonstrates how to test an ONNX model using the funasr_onnx library. It initializes the Paraformer class with the model directory and batch size, and then performs inference on an example audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Exporting and Running SenseVoice Inference with Libtorch (Python)\nDESCRIPTION: This Python code shows how to export the SenseVoiceSmall model to the Libtorch (TorchScript) format and perform inference using the `funasr_torch` library. It initializes the `SenseVoiceSmall` TorchScript wrapper, specifying the model directory, batch size, and execution device (`cuda:0`). Inference is then run on a sample audio file, utilizing the exported Libtorch model with automatic language detection and ITN enabled. The post-processed rich transcription results are printed. This facilitates deployment in C++ environments via Libtorch.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess (i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: Recognizing Audio from Buffer with FunasrApi\nDESCRIPTION: This Python snippet shows how to recognize speech from an audio buffer.  It reads audio data from a file into a byte array.  It then calls the `rec_buf` method of the FunasrApi instance, passing the audio bytes as input.  The output is the recognized text.  It also highlights the usage of `ffmpeg_decode` which is dependent on audio type.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/funasr_api/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    # recognizer by buffer\n\t# rec_buf(audio_buf,ffmpeg_decode=False),set ffmpeg_decode=True if audio is not PCM or WAV type\n    with open(\"asr_example.wav\", \"rb\") as f:\n        audio_bytes = f.read()\n    text=rcg.rec_buf(audio_bytes)\n    print(\"recognizer by buffer result=\",text)\n```\n\n----------------------------------------\n\nTITLE: Running Voice Activity Detection for Large Vocabulary Continuous Speech Recognition\nDESCRIPTION: This code illustrates how to utilize the Deep-FSMN VAD component to filter speech segments in large vocabulary continuous speech recognition tasks. It depends on the Deep-FSMN implementation within FunASR and accepts raw audio input to detect speech regions before recognition, aiding in noise robustness and accuracy.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Initialize VAD model\nvad_model = funasr.load_vad('path/to/deep_fsmn_vad')\n# Load raw audio\naudio = funasr.load_audio('noisy_audio.wav')\n# Detect speech segments\nspeech_segments = funasr.vad_detect(vad_model, audio)\n# Proceed with recognition using detected speech segments\nresult = funasr.transcribe_segments(speech_segments, model)\nprint('Speech segments identified:', speech_segments)\n```\n\n----------------------------------------\n\nTITLE: Including gflags Header (Pre-Version 1.0)\nDESCRIPTION: Shows the old method for including the gflags header file in C++ code, used in versions prior to 1.0. While backwards-compatibility headers might exist in version 1.0+, updating to the new path '<gflags/gflags.h>' is advised.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/README.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <google/gflags.h>\n```\n\n----------------------------------------\n\nTITLE: Installing yaml-cpp Library Target and Headers in CMake\nDESCRIPTION: This snippet configures the installation of the compiled yaml-cpp library target and its associated header files. `install(TARGETS ...)` installs the library binary to standard destinations, making it available for export via `yaml-cpp-targets`. `install(DIRECTORY ...)` copies all header files (`*.h`) from the specified `${header_directory}` to the designated include installation directory (`${INCLUDE_INSTALL_DIR}`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS yaml-cpp EXPORT yaml-cpp-targets ${_INSTALL_DESTINATIONS})\ninstall(\n\tDIRECTORY ${header_directory}\n\tDESTINATION ${INCLUDE_INSTALL_DIR}\n\tFILES_MATCHING PATTERN \"*.h\"\n)\n```\n\n----------------------------------------\n\nTITLE: ONNX Export and Inference\nDESCRIPTION: This code snippet shows how to export the SenseVoiceSmall model to ONNX format and then perform inference using the ONNX model. It initializes the model, specifies a batch size, and then runs inference on an audio file. The post-processed text results are printed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: Configuring Build System for FunASR with CMake - CMake\nDESCRIPTION: This CMake snippet defines how the FunASR shared library is built, handling source file collection, platform-specific adjustments, dependency management, and inclusion of external libraries such as ONNX Runtime and FFmpeg. It uses conditionals to include files and linking options based on platform type (Windows, Apple, etc.) and whether GPU support is enabled. Key configuration options include adding MSVC-specific compile flags on Windows, specifying extra libraries for different operating systems, and setting up proper include and linking directories. Prerequisites: ONNX Runtime, FFmpeg, and optionally Torch dependencies for GPU support must be available in the given directory paths. Inputs include the source *.cpp files in the directory and the necessary dependency include and library paths; output is the built shared FunASR library. It may require adjustments if project structure or dependencies change.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB files1 \"*.cpp\")\nif(APPLE)\n    file(GLOB itn_files \"itn-*.cpp\")\n    list(REMOVE_ITEM files1 ${itn_files})\nendif(APPLE)\nlist(REMOVE_ITEM files1 \"${CMAKE_CURRENT_SOURCE_DIR}/paraformer-torch.cpp\")\nset(files ${files1})\n\nif(GPU)\n    set(files ${files} \"${CMAKE_CURRENT_SOURCE_DIR}/paraformer-torch.cpp\")\nendif()\n\nmessage(\"files: \"${files})\n\nif(WIN32)\nadd_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/execution-charset:utf-8>\")\nadd_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/source-charset:utf-8>\")\nadd_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/bigobj>\")\nendif()\n\nadd_library(funasr SHARED ${files})\n\nif(WIN32)\n    set(EXTRA_LIBS yaml-cpp csrc kaldi-decoder fst glog gflags avutil avcodec avformat swresample onnxruntime)\n    include_directories(${ONNXRUNTIME_DIR}/include)\n    include_directories(${FFMPEG_DIR}/include)\n    target_link_directories(funasr PUBLIC ${ONNXRUNTIME_DIR}/lib)\n    target_link_directories(funasr PUBLIC ${FFMPEG_DIR}/lib)\n    target_compile_definitions(funasr PUBLIC -D_FUNASR_API_EXPORT -DNOMINMAX -DYAML_CPP_DLL)\nelse()\n    set(EXTRA_LIBS pthread yaml-cpp csrc kaldi-decoder fst glog gflags avutil avcodec avformat swresample)\n    include_directories(${ONNXRUNTIME_DIR}/include)\n    include_directories(${FFMPEG_DIR}/include)\n    if(APPLE)\n        target_link_directories(funasr PUBLIC ${ONNXRUNTIME_DIR}/lib)\n        target_link_directories(funasr PUBLIC ${FFMPEG_DIR}/lib)\n    endif(APPLE)    \nendif()\n\nif(GPU)\n    set(TORCH_DEPS torch torch_cuda torch_cpu c10 c10_cuda torch_blade ral_base_context)\nendif()\n\n#include_directories call omitted for brevity\ninclude_directories(${CMAKE_SOURCE_DIR}/include)\ninclude_directories(${CMAKE_SOURCE_DIR}/third_party)\ntarget_link_libraries(funasr PUBLIC onnxruntime ${EXTRA_LIBS} ${TORCH_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Timestamp Prediction with FunASR\nDESCRIPTION: Shows how to use the FA-ZH model for forced alignment to generate timestamps for a given audio file and corresponding text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version and Policies for gflags\nDESCRIPTION: Establishes the minimum required CMake version (3.0.2) for building gflags and sets specific CMake policies (CMP0042 for MACOSX_RPATH and CMP0048 for project version) to ensure modern behavior.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required (VERSION 3.0.2 FATAL_ERROR)\n\nif (POLICY CMP0042)\n  cmake_policy (SET CMP0042 NEW)\nendif ()\n\nif (POLICY CMP0048)\n  cmake_policy (SET CMP0048 NEW)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Punctuation Restoration with FunASR\nDESCRIPTION: Demonstrates using the CT-Punc model to add punctuation to plain text transcripts.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\n\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Define Glog Public Header Files\nDESCRIPTION: Sets the `GLOG_PUBLIC_H` variable to a list of header files that are considered part of the public interface of the glog library. These files will typically be installed and referenced by users of the library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset (GLOG_PUBLIC_H\n  ${CMAKE_CURRENT_BINARY_DIR}/glog/export.h\n  ${CMAKE_CURRENT_BINARY_DIR}/glog/logging.h\n  ${CMAKE_CURRENT_BINARY_DIR}/glog/raw_logging.h\n  ${CMAKE_CURRENT_BINARY_DIR}/glog/stl_logging.h\n  ${CMAKE_CURRENT_BINARY_DIR}/glog/vlog_is_on.h\n  src/glog/log_severity.h\n  src/glog/platform.h\n)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR via PyPI\nDESCRIPTION: Provides a shell command to install the FunASR library directly from the Python Package Index (PyPI) using pip. The `-U` flag ensures the package is upgraded if already installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U funasr\n```\n\n----------------------------------------\n\nTITLE: Building SenseVoice Docker image\nDESCRIPTION: Command to build the Docker image for SenseVoice from scratch using the provided Dockerfile.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# build from scratch, cd to the parent dir of Dockerfile.server\ndocker build . -f Dockerfile/Dockerfile.sensevoice -t soar97/triton-sensevoice:24.05\n```\n\n----------------------------------------\n\nTITLE: Setting up Multi-GPU Distributed Training with Torchrun (Shell)\nDESCRIPTION: This shell script snippet demonstrates how to initiate a multi-node, multi-GPU training job using torchrun. It configures environment variables, determines the number of GPU processes, and assigns node-specific parameters such as node rank and master address. It is essential for distributed training across multiple servers in funasr.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train.py ${train_args}\n\n```\n\nLANGUAGE: Shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train.py ${train_args}\n\n```\n\n----------------------------------------\n\nTITLE: CPP VAD Model Usage\nDESCRIPTION: This C++ code demonstrates the use of VAD (Voice Activity Detection) models. It involves two main steps: `FsmnVadInit` to initialize the VAD model and `FsmnVadInfer` to perform inference on an audio file.  The code assumes the presence of the necessary header files and libraries.  It expects the path to the model, the input audio file path, and the sampling rate (default 16k).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\n// VADÊ®°ÂûãÁöÑ‰ΩøÁî®ÂàÜ‰∏∫FsmnVadInitÂíåFsmnVadInfer‰∏§‰∏™Ê≠•È™§Ôºö\nFUNASR_HANDLE vad_hanlde=FsmnVadInit(model_path, thread_num);\n// ÂÖ∂‰∏≠Ôºömodel_path ÂåÖÂê´\"model-dir\"„ÄÅ\"quantize\"Ôºåthread_num‰∏∫onnxÁ∫øÁ®ãÊï∞Ôºõ\nFUNASR_RESULT result=FsmnVadInfer(vad_hanlde, wav_file.c_str(), NULL, 16000);\n// ÂÖ∂‰∏≠Ôºövad_hanlde‰∏∫FunOfflineInitËøîÂõûÂÄºÔºåwav_file‰∏∫Èü≥È¢ëË∑ØÂæÑÔºåsampling_rate‰∏∫ÈááÊ†∑Áéá(ÈªòËÆ§16k)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR from source\nDESCRIPTION: These commands clone the FunASR repository from GitHub, navigate into the cloned directory, and then install the package in editable mode using pip. This method allows for easy modification and testing of the FunASR codebase. Requires Git.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on Ubuntu using Shell\nDESCRIPTION: Downloads the Docker installation script (`test-docker.sh`) using curl and executes it with sudo privileges to install Docker on Ubuntu systems.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -fsSL https://test.docker.com -o test-docker.sh\nsudo sh test-docker.sh\n```\n\n----------------------------------------\n\nTITLE: Creating kaldi-util Library\nDESCRIPTION: This snippet creates a static library named 'kaldi-util'. It lists the source files to be compiled into this library, which includes various utility functions for error handling, math operations, I/O, and option parsing. No external dependencies are explicitly linked at this stage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(kaldi-util STATIC\n  base/kaldi-error.cc\n  base/kaldi-math.cc\n  util/kaldi-io.cc\n  util/parse-options.cc\n  util/simple-io-funcs.cc\n  util/text-utils.cc\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running FunASR PUNC ONNX Model in Text\nDESCRIPTION: Details the usage of the FunASR punctuation (PUNC) model, which adds punctuation to ASR-generated text, using ONNX backend. The user must provide the model path (containing required files), specify thread count, and pass the input text string. Main parameters include punc_hanlde (model handle from CTTransformerInit) and txt_str (input text without punctuation). Returns a result with punctuated text; dependencies are the FunASR library, ONNX runtime, and valid model files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_9\n\nLANGUAGE: Text\nCODE:\n```\n// The use of the PUNC model consists of two steps: CTTransformerInit and CTTransformerInfer:\nFUNASR_HANDLE punc_hanlde=CTTransformerInit(model_path, thread_num);\n// Where: model_path contains \"model-dir\" and \"quantize\", thread_num is the ONNX thread count;\nFUNASR_RESULT result=CTTransformerInfer(punc_hanlde, txt_str.c_str(), RASR_NONE, NULL);\n// Where: punc_hanlde is the return value of CTTransformerInit, txt_str is the text\n```\n\n----------------------------------------\n\nTITLE: Applying Punctuation Restoration with ICASSP 2018 Transformer Model\nDESCRIPTION: This snippet demonstrates how to implement real-time punctuation prediction using the Controllable Time-Delay Transformer (CT-Transformer). It requires importing the model, loading text or speech segments, and predicting appropriate punctuation marks to enhance transcript readability and disfluency detection. Dependencies include the ICASSP 2018 CT-Transformer implementation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Load punctuation prediction model\npunctuation_model = funasr.load_punctuation_model('path/to/ct_transformer')\n# Input text transcription\ntranscript = 'this is a test sentence'\n# Predict punctuation\npunctuated_transcript = funasr.punctuate_text(punctuation_model, transcript)\nprint('Punctuated output:', punctuated_transcript)\n```\n\n----------------------------------------\n\nTITLE: Conditionally Enabling and Building OpenFST and Its Dependencies (CMake)\nDESCRIPTION: When ENABLE_FST is set, this code block adds required dependencies (gflags and glog), configures subdirectory inclusion for openfst (with prior patching as needed), and handles platform-specific include paths. It assumes that openfst was cloned with appropriate patches as commented. Prerequisites include third_party/gflags, glog, and openfst. Inputs: ENABLE_FST flag; outputs: built and linked openfst library with registered include directories. Limitations: The process expects patched source trees and may not work with vanilla openfst.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_FST)\n    # fst depend on glog and gflags\n    include_directories(${PROJECT_SOURCE_DIR}/third_party/gflags)\n    add_subdirectory(third_party/gflags)\n    include_directories(${gflags_BINARY_DIR}/include)\n    \n    # the following openfst if cloned from https://github.com/kkm000/openfst.git\n    # with some patch to fix the make errors. \n    add_subdirectory(third_party/openfst)\n    include_directories(${openfst_SOURCE_DIR}/src/include)\n    if(WIN32)\n    include_directories(${openfst_SOURCE_DIR}/src/lib)\n    endif()\n\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Adding a Test and Setting Properties for gflags Declaration with CMake\nDESCRIPTION: This creates a test \"gflags_declare\", using the executable gflags_declare_test and sets properties for the tests. It asserts that the output of the test contains \"Hello gflags!\".\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nadd_test(NAME gflags_declare COMMAND gflags_declare_test --message \"Hello gflags!\")\nset_tests_properties(gflags_declare PROPERTIES PASS_REGULAR_EXPRESSION \"Hello gflags!\")\n```\n\n----------------------------------------\n\nTITLE: Offline Mode Server Response Format in FunASR WebSocket Protocol\nDESCRIPTION: JSON message format for server responses in offline transcription mode. Contains the recognition results, file information, and timestamp data if available from the acoustic model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol_zh.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"offline\", \"wav_name\": \"wav_name\", \"text\": \"asr ouputs\", \"is_final\": True,\"timestamp\":\"[[100,200], [200,500]]\",\"stamp_sents\":[]}\n```\n\n----------------------------------------\n\nTITLE: Enabling and Building Optional Logging Library (glog) via Submodules (CMake)\nDESCRIPTION: This snippet conditionally adds the glog logging library as a subdirectory if ENABLE_GLOG is set. It sets BUILD_TESTING OFF, updates include directories, and uses both source and build directories for proper target and header visibility. Requires third_party/glog source directory and relevant CMake settings. The inputs are the ENABLE_GLOG toggle and required directory structures; outputs are the configured glog target and inclusion. Limitation: Assumes glog source is present in the expected path.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_GLOG)\n    include_directories(${PROJECT_SOURCE_DIR}/third_party/glog/src)\n    set(BUILD_TESTING OFF)\n    add_subdirectory(third_party/glog)\n    include_directories(${glog_BINARY_DIR})\n    \nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Environment for C++ FunASR Deployment\nDESCRIPTION: This shell snippet provides commands to install Docker on the host machine, which is optional if Docker is already installed. It downloads an installation script and executes it with superuser privileges. Docker is required for containerized deployment of the C++ FunASR runtime SDK to support high concurrency and efficient transcription services.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh;\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Protobuf/gRPC Library - CMake\nDESCRIPTION: Defines a static or shared library target named `rg_grpc_proto` using the generated C++/gRPC source and header files from the custom command.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(rg_grpc_proto ${rg_grpc_srcs} ${rg_grpc_hdrs} ${rg_proto_srcs} ${rg_proto_hdrs})\n```\n\n----------------------------------------\n\nTITLE: Creating arpa2fst Executable\nDESCRIPTION: This snippet creates an executable named 'arpa2fst'. It compiles source files related to ARPA language model parsing and compilation.  It links against `kaldi-util` and `fst`, and dynamically links against `dl` on non-Windows platforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(TRUE)\n  # Arpa binary\n  add_executable(arpa2fst\n    lm/arpa-file-parser.cc\n    lm/arpa-lm-compiler.cc\n    lmbin/arpa2fst.cc\n  )\n\nif (WIN32)\ntarget_link_libraries(arpa2fst PUBLIC kaldi-util fst)\nelse()\ntarget_link_libraries(arpa2fst PUBLIC kaldi-util fst dl)\nendif (WIN32)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running FunASR C++ WebSocket Client (Shell)\nDESCRIPTION: Executes the compiled C++ WebSocket client (`funasr-wss-client`) to perform speech recognition via a FunASR server. Requires specifying the server IP (--server-ip), port (--port), audio input path (--wav-path, single wav or wav.scp), number of threads (--thread-num), and whether to use SSL (--is-ssl, default 1). Optional parameters include a hotword file path (--hotword) and ITN usage (--use-itn, default 1). Requires the C++ client executable and specified audio input.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n. /funasr-wss-client --server-ip 127.0.0.1 --port 10095 --wav-path test.wav --thread-num 1 --is-ssl 1\n```\n\n----------------------------------------\n\nTITLE: Configure CPack Source Package Settings in CMake\nDESCRIPTION: Sets configuration options specific to source packages generated by CPack. `CPACK_SOURCE_TOPLEVEL_TAG` defines the subdirectory name within the archive, `CPACK_SOURCE_PACKAGE_FILE_NAME` sets the base name for the source archive, and `CPACK_SOURCE_IGNORE_FILES` provides a list of patterns for files and directories to exclude from the source package.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_38\n\nLANGUAGE: CMake\nCODE:\n```\n  # source package settings\n  set (CPACK_SOURCE_TOPLEVEL_TAG      \"source\")\n  set (CPACK_SOURCE_PACKAGE_FILE_NAME \"${CPACK_PACKAGE_NAME}-${CPACK_PACKAGE_VERSION}\")\n  set (CPACK_SOURCE_IGNORE_FILES      \"/\\\\\\\\.git/;\\\\\\\\.swp$;\\\\\\\\.#;/#;\\\\\\\\.*~;cscope\\\\\\\\.*;/[Bb]uild[.+-_a-zA-Z0-9]*/\")\n```\n\n----------------------------------------\n\nTITLE: Example train_text.txt\nDESCRIPTION: This snippet demonstrates the expected format of the train_text.txt file, which maps unique IDs to the corresponding transcription text. This file is essential for associating audio files with their transcriptions during the training process. The left side is the unique ID and the right side is the transcribed text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 ÁîöËá≥Âá∫Áé∞‰∫§ÊòìÂá†‰πéÂÅúÊªûÁöÑÊÉÖÂÜµ\nBAC009S0916W0489 ÊπñÂåó‰∏ÄÂÖ¨Âè∏‰ª•ÂëòÂ∑•Âêç‰πâË¥∑Ê¨æÊï∞ÂçÅÂëòÂ∑•Ë¥üÂÄ∫ÂçÉ‰∏á\nasr_example_cn_en ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning ÂÅö data analytics ÂÅö data science ‰πüÂ•Ω scientist ‰πüÂ•ΩÈÄöÈÄöÈÉΩË¶ÅÈÉΩÂÅöÁöÑÂü∫Êú¨ÂäüÂïäÈÇ£ again ÂÖàÂÖàÂØπÊúâ‰∏Ä‰∫õ > ‰πüËÆ∏ÂØπ\nID0012W0014 he tried to think how it could be\n```\n\n----------------------------------------\n\nTITLE: Project Setup and Version Configuration in CMake\nDESCRIPTION: Sets the minimum required CMake version, defines the project name and languages, and specifies the version for kaldi-native-fbank. Also sets the default build type to Release if not specified.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.3 FATAL_ERROR)\n\nproject(kaldi-native-fbank CXX C)\n\nset(KALDI_NATIVE_FBANK_VERSION \"1.13\")\n\nif(NOT CMAKE_BUILD_TYPE)\n  set(CMAKE_BUILD_TYPE Release)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Libtorch Export and Inference\nDESCRIPTION: This code snippet demonstrates how to export the SenseVoiceSmall model to Libtorch format and perform inference. It initializes the Libtorch model, sets the device (CUDA), and then performs inference on an example audio file.  The post-processed transcription is printed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: Single-Machine Multi-GPU Distributed Training - Shell\nDESCRIPTION: This snippet prepares the environment for distributed multi-GPU training using torchrun and FunASR‚Äôs train.py. It sets the CUDA_VISIBLE_DEVICES environment variable, calculates the number of GPUs, and starts a process per GPU for parallel training. Requires a compatible torch/tensorflow environment, multiple GPUs, and proper configuration of the training arguments. Input is as per the referenced training args shell variable.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and ModelScope for FunASR (Shell)\nDESCRIPTION: Installs core dependencies including torch, torchaudio, modelscope, and funasr using pip. Required for running FunASR and associated ONNX model exports. All commands are intended to be run in a shell environment with Python 3 and pip installed, and network access to PyPI repositories. Outputs are pip-installed packages in the active environment. No parameters are required, but administrator/root access may be needed for system-wide installs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip3 install torch torchaudio\npip install -U modelscope\npip install -U funasr\n```\n\n----------------------------------------\n\nTITLE: FunASR Multi-GPU Training\nDESCRIPTION: This snippet demonstrates how to perform multi-GPU training using `torchrun` with FunASR. It sets the `CUDA_VISIBLE_DEVICES` environment variable and specifies the number of nodes and processes per node for distributed training.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Performing Punctuation Restoration with FunASR (Python)\nDESCRIPTION: Illustrates how to use the `ct-punc` model via `AutoModel` for punctuation restoration. It takes raw text as input and outputs the text with predicted punctuation marks.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Defining FST Module Creation Function\nDESCRIPTION: Creates a reusable function for building and configuring FST modules with consistent settings. Sets up Windows export settings and installation paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/linear/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfunction (add_module _name)\n    add_library(${ARGV})\n    if (TARGET ${_name})\n        target_link_libraries(${_name} fst)\n        set_target_properties(${_name} PROPERTIES \n            WINDOWS_EXPORT_ALL_SYMBOLS true\n            FOLDER linear/modules\n        )\n    endif()\n\n    install(TARGETS ${_name} LIBRARY DESTINATION lib/fst)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Test for Flag Validation with CMake\nDESCRIPTION: This command adds a gflags test to verify that the validator for a flag is correctly enforced. In this case, it checks that setting the 'always_fail' flag causes a validation error.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\n# And we should die if the flag value doesn't pass the validator\nadd_gflags_test(always_fail 1 \"ERROR: failed validation of new value 'true' for flag 'always_fail'\" \"\"  gflags_unittest  --always_fail)\n```\n\n----------------------------------------\n\nTITLE: Example train_emo.txt\nDESCRIPTION: This snippet demonstrates the expected format of the train_emo.txt file, which maps unique IDs to the corresponding emotion tags.  The supported emotions are <|HAPPY|>, <|SAD|>, <|ANGRY|>, <|NEUTRAL|>, <|FEARFUL|>, <|DISGUSTED|>, and <|SURPRISED|>.  The left side is the unique ID, and the right side is the emotion tag.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n\n----------------------------------------\n\nTITLE: Checking Compiler Support for GNU Attributes in C++\nDESCRIPTION: This snippet uses CMake's check_cxx_source_compiles to verify if the compiler supports GCC-style __attribute__ annotations, including unused functions, visibility default, and hidden visibility. It compiles small test programs including \"cstdlib\" headers and applying specific __attribute__ forms to a static function to determine availability. This enables setting cache variables used later for conditional compilation based on attribute support.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncheck_cxx_source_compiles (\"\n#include <cstdlib>\nstatic void foo(void) __attribute__ ((unused));\nint main(void) { return 0; }\n\" HAVE___ATTRIBUTE__)\n\ncheck_cxx_source_compiles (\"\n#include <cstdlib>\nstatic void foo(void) __attribute__ ((visibility(\\\"default\\\")));\nint main(void) { return 0; }\n\" HAVE___ATTRIBUTE__VISIBILITY_DEFAULT)\n\ncheck_cxx_source_compiles (\"\n#include <cstdlib>\nstatic void foo(void) __attribute__ ((visibility(\\\"hidden\\\")));\nint main(void) { return 0; }\n\" HAVE___ATTRIBUTE__VISIBILITY_HIDDEN)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch for FunASR\nDESCRIPTION: Command to install PyTorch and torchaudio, which are required dependencies for FunASR. The required PyTorch version must be 1.11.0 or higher.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation_zh.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip3 install torch torchaudio\n```\n\n----------------------------------------\n\nTITLE: Configure Source and Header Files (CMake)\nDESCRIPTION: Configures variables used within source and header files based on detected system properties and build flags. It defines `GFLAGS_ATTRIBUTE_UNUSED` based on the compiler and `GFLAGS_IS_A_DLL` based on whether shared libraries are being built, then processes the source/header lists using custom `configure_headers` and `configure_sources` commands.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT DEFINED GFLAGS_ATTRIBUTE_UNUSED)\n  if (CMAKE_COMPILER_IS_GNUCXX)\n    set (GFLAGS_ATTRIBUTE_UNUSED \"__attribute((unused))\")\n  else ()\n    set (GFLAGS_ATTRIBUTE_UNUSED)\n  endif ()\nendif ()\n\n# whenever we build a shared library (DLL on Windows), configure the public\n# headers of the API for use of this shared library rather than the optionally\n# also build statically linked library; users can override GFLAGS_DLL_DECL\n# in particular, this done by setting the INTERFACE_COMPILE_DEFINITIONS of\n# static libraries to include an empty definition for GFLAGS_DLL_DECL\nif (NOT DEFINED GFLAGS_IS_A_DLL)\n  if (BUILD_SHARED_LIBS)\n    set (GFLAGS_IS_A_DLL 1)\n  else ()\n    set (GFLAGS_IS_A_DLL 0)\n  endif ()\nendif ()\n\nconfigure_headers (PUBLIC_HDRS  ${PUBLIC_HDRS})\nconfigure_sources (PRIVATE_HDRS ${PRIVATE_HDRS})\nconfigure_sources (GFLAGS_SRCS  ${GFLAGS_SRCS})\n```\n\n----------------------------------------\n\nTITLE: Adding Source Subdirectory to Build in CMake\nDESCRIPTION: This command instructs CMake to process the CMakeLists.txt file located within the 'src' subdirectory. This is typically where the core build targets (like libraries or executables) for the project are defined.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Building FunASR ONNX Runtime Environment (Shell)\nDESCRIPTION: Clones the FunASR GitHub repository, navigates to the ONNX runtime subdirectory, creates a build directory, runs CMake with the specified ONNXRUNTIME_DIR path, and finally compiles the code using make. Prerequisites include git, cmake, make, and all required dependencies such as ONNX Runtime and OpenBLAS. Input parameter '-DONNXRUNTIME_DIR' should be set to the location of the extracted ONNX Runtime directory. Output is an executable FunASR ONNX runtime suitable for offline benchmarking. Requires appropriate permissions and Unix-like environment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd funasr/runtime/onnxruntime\nmkdir build && cd build\ncmake  -DCMAKE_BUILD_TYPE=release .. -DONNXRUNTIME_DIR=/path/to/onnxruntime-linux-x64-1.14.0\nmake\n```\n\n----------------------------------------\n\nTITLE: Exporting Model - Python\nDESCRIPTION: This Python code exports a FunASR model using the AutoModel class, allowing you to specify whether to apply quantization.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Generating yaml-cpp CMake Configuration Files\nDESCRIPTION: These commands generate the `yaml-cpp-config.cmake` and `yaml-cpp-config-version.cmake` files from template files (`.in`). The first `configure_file` generates a config file for the build directory, while the second set prepares the config file for installation, calculating relative paths based on the installation prefix and platform-specific CMake directory (`INSTALL_CMAKE_DIR`). The version file is also generated.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(CONFIG_INCLUDE_DIRS \"${YAML_CPP_SOURCE_DIR}/include\")\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/yaml-cpp-config.cmake.in\n\t\"${PROJECT_BINARY_DIR}/yaml-cpp-config.cmake\" @ONLY)\n\nif(WIN32 AND NOT CYGWIN)\n\tset(INSTALL_CMAKE_DIR CMake)\nelse()\n\tset(INSTALL_CMAKE_DIR ${LIB_INSTALL_DIR}/cmake/yaml-cpp)\nendif()\n\nfile(RELATIVE_PATH REL_INCLUDE_DIR \"${CMAKE_INSTALL_PREFIX}/${INSTALL_CMAKE_DIR}\" \"${CMAKE_INSTALL_PREFIX}/${INCLUDE_INSTALL_ROOT_DIR}\")\nset(CONFIG_INCLUDE_DIRS \"\\${YAML_CPP_CMAKE_DIR}/${REL_INCLUDE_DIR}\")\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/yaml-cpp-config.cmake.in\n\t\"${PROJECT_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/yaml-cpp-config.cmake\" @ONLY)\n\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/yaml-cpp-config-version.cmake.in\n\t\"${PROJECT_BINARY_DIR}/yaml-cpp-config-version.cmake\" @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Running ASR 2pass (Offline/Online) Client (wav.scp Batch) via Shell\nDESCRIPTION: Processes a batch wav.scp file in 2pass (streaming and non-streaming) mode, ideal for comprehensive ASR evaluation or deployment. Results are saved to the specified directory. Requires both audio input and output path.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"0.0.0.0\" --port 10095 --mode 2pass --chunk_size \"8,8,4\" --audio_in \"./data/wav.scp\" --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Including and Building GLog Dependency - CMake\nDESCRIPTION: Conditionally configures the GLog dependency. It includes GLog's source directory for headers, disables its testing builds, and adds it as a subdirectory for compilation, making its headers available from the binary directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_GLOG)\n    include_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src)\n    set(BUILD_TESTING OFF)\n    add_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog glog)\n    include_directories(${glog_BINARY_DIR})\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Punctuation Restoration\nDESCRIPTION: This snippet demonstrates punctuation restoration using the `AutoModel` with the \"ct-punc\" model. It restores punctuation in the input text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\n\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Installing openblas and openssl Libraries - Shell\nDESCRIPTION: This shell snippet installs required system libraries openblas and openssl on Ubuntu and CentOS using apt-get or yum. These libraries are dependencies for building and running FunASR. The openblas library is used for linear algebra acceleration while openssl provides cryptographic functions. The user should select the command matching their OS distribution.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# openblas\nsudo apt-get install libopenblas-dev #ubuntu\n# sudo yum -y install openblas-devel #centos\n\n# openssl\napt-get install libssl-dev #ubuntu \n# yum install openssl-devel #centos\n```\n\n----------------------------------------\n\nTITLE: Generating JSONL Dataset from wav.scp and text.txt Files in Shell\nDESCRIPTION: This snippet provides commands for preparing training data in JSONL format required by FunASR. The wav.scp file maps unique IDs to audio file URLs, and train_text.txt maps the same IDs to corresponding transcriptions. The scp2jsonl command converts these paired files into train.jsonl for model input. An optional jsonl2scp command converts JSONL back to wav.scp and text.txt. Dependencies include the custom scp2jsonl and jsonl2scp utilities and properly formatted input files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\nLANGUAGE: shell\nCODE:\n```\n# generate wav.scp and text.txt from train.jsonl and val.jsonl\njsonl2scp \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_in=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Start Deployed FunASR Service\nDESCRIPTION: This command uses the deployment script with superuser privileges to start the FunASR service if it has been previously installed and stopped. It uses the configuration from the last installation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh start\n```\n\n----------------------------------------\n\nTITLE: FunASR Server Configuration Parameters\nDESCRIPTION: This block describes available command-line arguments for the `run_server_2pass.sh` script. Parameters allow to specify model directories, ports, thread numbers, SSL certificates, and hotword files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n--download-model-dir Ê®°Âûã‰∏ãËΩΩÂú∞ÂùÄÔºåÈÄöËøáËÆæÁΩÆmodel ID‰ªéModelscope‰∏ãËΩΩÊ®°Âûã\n--model-dir  modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--online-model-dir  modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--vad-dir  modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--punc-dir  modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--lm-dir modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--itn-dir modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--port  ÊúçÂä°Á´ØÁõëÂê¨ÁöÑÁ´ØÂè£Âè∑ÔºåÈªòËÆ§‰∏∫ 10095\n--decoder-thread-num  ÊúçÂä°Á´ØÁ∫øÁ®ãÊ±†‰∏™Êï∞(ÊîØÊåÅÁöÑÊúÄÂ§ßÂπ∂ÂèëË∑ØÊï∞)Ôºå\n                      ËÑöÊú¨‰ºöÊ†πÊçÆÊúçÂä°Âô®Á∫øÁ®ãÊï∞Ëá™Âä®ÈÖçÁΩÆdecoder-thread-num„ÄÅio-thread-num\n--io-thread-num  ÊúçÂä°Á´ØÂêØÂä®ÁöÑIOÁ∫øÁ®ãÊï∞\n--model-thread-num  ÊØèË∑ØËØÜÂà´ÁöÑÂÜÖÈÉ®Á∫øÁ®ãÊï∞(ÊéßÂà∂ONNXÊ®°ÂûãÁöÑÂπ∂Ë°å)ÔºåÈªòËÆ§‰∏∫ 1Ôºå\n                    ÂÖ∂‰∏≠Âª∫ËÆÆ decoder-thread-num*model-thread-num Á≠â‰∫éÊÄªÁ∫øÁ®ãÊï∞\n--certfile  sslÁöÑËØÅ‰π¶Êñá‰ª∂ÔºåÈªòËÆ§‰∏∫Ôºö../../../ssl_key/server.crtÔºåÂ¶ÇÊûúÈúÄË¶ÅÂÖ≥Èó≠sslÔºåÂèÇÊï∞ËÆæÁΩÆ‰∏∫0\n--keyfile   sslÁöÑÂØÜÈí•Êñá‰ª∂ÔºåÈªòËÆ§‰∏∫Ôºö../../../ssl_key/server.key\n--hotword   ÁÉ≠ËØçÊñá‰ª∂Ë∑ØÂæÑÔºåÊØèË°å‰∏Ä‰∏™ÁÉ≠ËØçÔºåÊ†ºÂºèÔºöÁÉ≠ËØç ÊùÉÈáç(‰æãÂ¶Ç:ÈòøÈáåÂ∑¥Â∑¥ 20)Ôºå\n            Â¶ÇÊûúÂÆ¢Êà∑Á´ØÊèê‰æõÁÉ≠ËØçÔºåÂàô‰∏éÂÆ¢Êà∑Á´ØÊèê‰æõÁöÑÁÉ≠ËØçÂêàÂπ∂‰∏ÄËµ∑‰ΩøÁî®ÔºåÊúçÂä°Á´ØÁÉ≠ËØçÂÖ®Â±ÄÁîüÊïàÔºåÂÆ¢Êà∑Á´ØÁÉ≠ËØçÂè™ÈíàÂØπÂØπÂ∫îÂÆ¢Êà∑Á´ØÁîüÊïà„ÄÇ\n```\n\n----------------------------------------\n\nTITLE: Sending End of Audio Flag\nDESCRIPTION: This JSON snippet represents the message sent to indicate the end of the audio stream in offline file transcription or real-time speech recognition. The 'is_speaking' parameter is set to False to signal the server to finalize the recognition process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n{\"is_speaking\": False}\n```\n\n----------------------------------------\n\nTITLE: Register Model - Python\nDESCRIPTION: This code snippet illustrates how to register a new model class in FunASR using the `@tables.register` decorator. The registered class must implement `__init__`, `forward`, and `inference` methods. It defines the 'SenseVoiceSmall' model by inheriting from `nn.Module`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n  def __init__(*args, **kwargs):\n    ...\n\n  def forward(\n      self,\n      **kwargs,\n  ):  \n\n  def inference(\n      self,\n      data_in,\n      data_lengths=None,\n      key: list = None,\n      tokenizer=None,\n      frontend=None,\n      **kwargs,\n  ):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Training\nDESCRIPTION: This snippet shows the command to start the FunASR training process by executing the `finetune.sh` script. Ensure that the `train_tool` variable in the script points to the correct path of the `train_ds.py` script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nbash finetune.sh\n```\n\n----------------------------------------\n\nTITLE: Installing funasr_torch from PyPI in Shell\nDESCRIPTION: Installs the funasr_torch package directly from PyPI using pip, enabling PyTorch-based inference for FunASR models. For users in China, an alternative pip command is provided to use a local mirror. No additional parameters are required.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/libtorch/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install -U funasr_torch\n# For the users in China, you could install with the command:\n# pip install -U funasr_torch -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Configuring gflags Shared Library SOVERSION in CMake\nDESCRIPTION: Determines the shared library ABI version (SOVERSION) for gflags. It allows overriding via the `GFLAGS_SOVERSION` CMake variable passed on the command line, otherwise defaults to `<major>.<minor>` (e.g., '2.2') for version 2.x releases due to historical API changes related to the default namespace.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# shared library ABI version number, can be overridden by package maintainers\n# using -DGFLAGS_SOVERSION=XXX on the command-line\nif (GFLAGS_SOVERSION)\n  set (PACKAGE_SOVERSION \"${GFLAGS_SOVERSION}\")\nelse ()\n  # TODO: Change default SOVERSION back to PACKAGE_VERSION_MAJOR with the\n  #       next increase of major version number (i.e., 3.0.0 -> SOVERSION 3)\n  #       The <major>.<minor> SOVERSION should be used for the 2.x releases\n  #       versions only which temporarily broke the API by changing the default\n  #       namespace from \"google\" to \"gflags\".\n  set (PACKAGE_SOVERSION \"${PACKAGE_VERSION_MAJOR}.${PACKAGE_VERSION_MINOR}\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Registering Build Configuration Test in CMake (CMake)\nDESCRIPTION: This CMake block provides an option for adding a build configuration test, invoked if enabled by the user. It registers a single configuration test using the shared helper, requiring prior setup of the build Python infrastructure and applicable source directories. Inputs determine if the package configuration passes as expected, with outputs signaling configuration validity.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# build configuration test\noption (BUILD_CONFIG_TESTS \"Request addition of package configuration tests.\" OFF)\nmark_as_advanced (BUILD_CONFIG_TESTS)\nif (BUILD_CONFIG_TESTS)\n  add_gflags_build_test (cmake_config config 0)\nendif ()\n\n```\n\n----------------------------------------\n\nTITLE: Defining gflags Library Targets (Static/Shared, Threaded/Nothreaded) in CMake\nDESCRIPTION: This snippet defines the actual gflags library targets. It iterates through combinations of library types (STATIC, SHARED) and threading options (default/threaded, _nothreads). For each valid combination enabled by BUILD_* variables, it uses `add_library` to create the target (e.g., gflags_static, gflags_nothreads_shared), sets properties like output name and version, configures include directories, sets compile definitions (GFLAGS_IS_A_DLL for Windows shared libs, NO_THREADS for non-threaded), links necessary dependencies (pthreads, shlwapi), and groups targets under convenience targets (gflags, gflags_nothreads).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# add library targets\nset (TARGETS)\n# static vs. shared\nforeach (TYPE IN ITEMS STATIC SHARED)\n  if (BUILD_${TYPE}_LIBS)\n    string (TOLOWER \"${TYPE}\" type)\n    # whether or not targets are a DLL\n    if (OS_WINDOWS AND \"^${TYPE}$\" STREQUAL \"^SHARED$\")\n      set (GFLAGS_IS_A_DLL 1)\n    else ()\n      set (GFLAGS_IS_A_DLL 0)\n    endif ()\n    # filename suffix for static libraries on Windows\n    if (OS_WINDOWS AND \"^${TYPE}$\" STREQUAL \"^STATIC$\")\n      set (type_suffix \"_${type}\")\n    else ()\n      set (type_suffix \"\")\n    endif ()\n    # multi-threaded vs. single-threaded\n    foreach (opts IN ITEMS \"\" _nothreads)\n      if (BUILD_gflags${opts}_LIB)\n        set (target_name \"gflags${opts}_${type}\")\n        add_library (${target_name} ${TYPE} ${GFLAGS_SRCS} ${PRIVATE_HDRS} ${PUBLIC_HDRS})\n        set_target_properties (${target_name} PROPERTIES\n          OUTPUT_NAME \"gflags${opts}${type_suffix}\"\n          VERSION     \"${PACKAGE_VERSION}\"\n          SOVERSION   \"${PACKAGE_SOVERSION}\"\n        )\n        set (include_dirs \"$<BUILD_INTERFACE:${PROJECT_BINARY_DIR}/include>\")\n        if (INSTALL_HEADERS)\n          list (APPEND include_dirs \"$<INSTALL_INTERFACE:${INCLUDE_INSTALL_DIR}>\")\n        endif ()\n        target_include_directories (${target_name}\n          PUBLIC  \"${include_dirs}\"\n          PRIVATE \"${PROJECT_SOURCE_DIR}/src;${PROJECT_BINARY_DIR}/include/${GFLAGS_INCLUDE_DIR}\"\n        )\n        target_compile_definitions (${target_name} PUBLIC GFLAGS_IS_A_DLL=${GFLAGS_IS_A_DLL})\n        if (opts MATCHES \"nothreads\")\n          target_compile_definitions (${target_name} PRIVATE NO_THREADS)\n        elseif (CMAKE_USE_PTHREADS_INIT)\n          target_link_libraries (${target_name} ${CMAKE_THREAD_LIBS_INIT})\n        endif ()\n        if (HAVE_SHLWAPI_H)\n          target_link_libraries (${target_name} shlwapi.lib)\n        endif ()\n        list (APPEND TARGETS ${target_name})\n        # add convenience make target for build of both shared and static libraries\n        if (NOT GFLAGS_IS_SUBPROJECT)\n          if (NOT TARGET gflags${opts})\n            add_custom_target (gflags${opts})\n          endif ()\n          add_dependencies (gflags${opts} ${target_name})\n        endif ()\n      endif ()\n    endforeach ()\n  endif ()\nendforeach ()\n```\n\n----------------------------------------\n\nTITLE: Building Multiple FST Utilities using `add_executable2`\nDESCRIPTION: Demonstrates the usage of the custom `add_executable2` CMake function to build various FST command-line utilities (e.g., `fstarcsort`, `fstclosure`, `fstcompile`). Each call specifies the executable name and its corresponding source files (typically a main file like `fstarcsort-main.cc` and an implementation file like `fstarcsort.cc`), leveraging the function to handle linking and installation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/bin/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable2(fstarcsort fstarcsort-main.cc fstarcsort.cc)\n\nadd_executable2(fstclosure fstclosure-main.cc fstclosure.cc)\n\nadd_executable2(fstcompile  fstcompile-main.cc fstcompile.cc)\n\nadd_executable2(fstcompose fstcompose-main.cc fstcompose.cc)\n\nadd_executable2(fstconcat fstconcat-main.cc fstconcat.cc)\n\nadd_executable2(fstconnect fstconnect-main.cc fstconnect.cc)\n\nadd_executable2(fstconvert fstconvert-main.cc fstconvert.cc)\n\nadd_executable2(fstdeterminize fstdeterminize-main.cc fstdeterminize.cc)\n\nadd_executable2(fstdifference fstdifference-main.cc fstdifference.cc)\n\nadd_executable2(fstdisambiguate fstdisambiguate-main.cc fstdisambiguate.cc)\n\nadd_executable2(fstdraw fstdraw-main.cc fstdraw.cc)\n\nadd_executable2(fstencode fstencode-main.cc fstencode.cc)\n\nadd_executable2(fstepsnormalize fstepsnormalize-main.cc fstepsnormalize.cc)\n\nadd_executable2(fstequal fstequal-main.cc fstequal.cc)\n\nadd_executable2(fstequivalent fstequivalent-main.cc fstequivalent.cc)\n\nadd_executable2(fstinfo fstinfo-main.cc fstinfo.cc)\n\nadd_executable2(fstintersect fstintersect-main.cc fstintersect.cc)\n\nadd_executable2(fstinvert fstinvert-main.cc fstinvert.cc)\n\nadd_executable2(fstisomorphic fstisomorphic-main.cc fstisomorphic.cc)\n\nadd_executable2(fstmap fstmap-main.cc fstmap.cc)\n\nadd_executable2(fstminimize fstminimize-main.cc fstminimize.cc)\n\nadd_executable2(fstprint fstprint-main.cc fstprint.cc)\n\nadd_executable2(fstproject fstproject-main.cc fstproject.cc)\n\nadd_executable2(fstprune fstprune-main.cc fstprune.cc)\n\nadd_executable2(fstpush fstpush-main.cc fstpush.cc)\n\nadd_executable2(fstrandgen fstrandgen-main.cc fstrandgen.cc)\n\nadd_executable2(fstrelabel fstrelabel-main.cc fstrelabel.cc)\n\nadd_executable2(fstreplace fstreplace-main.cc fstreplace.cc)\n\nadd_executable2(fstreverse fstreverse-main.cc fstreverse.cc)\n\nadd_executable2(fstreweight fstreweight-main.cc fstreweight.cc)\n\nadd_executable2(fstrmepsilon fstrmepsilon-main.cc fstrmepsilon.cc)\n\nadd_executable2(fstshortestdistance fstshortestdistance-main.cc fstshortestdistance.cc)\n\nadd_executable2(fstshortestpath fstshortestpath-main.cc fstshortestpath.cc)\n\nadd_executable2(fstsymbols fstsymbols-main.cc fstsymbols.cc)\n\nadd_executable2(fstsynchronize fstsynchronize-main.cc fstsynchronize.cc)\n\nadd_executable2(fsttopsort fsttopsort-main.cc fsttopsort.cc)\n\nadd_executable2(fstunion fstunion-main.cc fstunion.cc)\n```\n\n----------------------------------------\n\nTITLE: Configuring FunASR Runtime Build on Windows (Shell/CMake)\nDESCRIPTION: Clones the FunASR repository, navigates to the runtime/websocket directory, creates a build subdirectory, and then uses CMake to configure the build environment for Windows. It specifies the paths to the previously downloaded OpenSSL, ffmpeg, and onnxruntime libraries required for Windows compilation. The final compilation step needs to be done using Visual Studio by opening the generated solution file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git \ncd FunASR/runtime/websocket\nmkdir build\ncd build\ncmake ../ -D OPENSSL_ROOT_DIR=d:/openssl-1.1.1w -D FFMPEG_DIR=d:/ffmpeg-master-latest-win64-gpl-shared -D ONNXRUNTIME_DIR=d:/onnxruntime-win-x64-1.16.1\n```\n\n----------------------------------------\n\nTITLE: Building Special FST Shared Library with CMake\nDESCRIPTION: Creates a shared/static library target named fstspecial that consists of specialized FST source files along with header files globbed from a specified include directory. It sets versioning and folder organization properties and links the library target with a core fst library dependency. This snippet facilitates encapsulating specialized FST components into a reusable library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/special/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(fstspecial\n  phi-fst.cc\n  rho-fst.cc\n  sigma-fst.cc\n  ${HEADER_FILES}\n)\n\nset_target_properties(fstspecial PROPERTIES\n  SOVERSION \"${SOVERSION}\"\n  FOLDER special\n)\ntarget_link_libraries(fstspecial\n  fst\n)\n```\n\n----------------------------------------\n\nTITLE: Linking FST Dependency\nDESCRIPTION: This snippet uses `target_link_libraries` to link the `fstngram` library with the `fst` library. This specifies that the `fstngram` library depends on the `fst` library and will use its functions and symbols during compilation and linking.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(fstngram\n    fst\n)\n```\n\n----------------------------------------\n\nTITLE: Defining FunASR Build Options - CMake\nDESCRIPTION: Defines boolean options using the `option()` command to allow users to enable or disable various features and dependencies, such as building the websocket server, PortAudio support, Glog, FST (OpenFST), shared libraries, and GPU support.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\noption(ENABLE_WEBSOCKET \"Whether to build websocket server\" ON)\noption(ENABLE_PORTAUDIO \"Whether to build portaudio\" ON)\noption(ENABLE_GLOG \"Whether to build glog\" ON)\noption(ENABLE_FST \"Whether to build openfst\" ON) # ITN need openfst compiled\noption(BUILD_SHARED_LIBS \"Build shared libraries\" ON)\noption(GPU \"Whether to build with GPU\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Configuring Visual Studio Solution Folders in CMake\nDESCRIPTION: These commands configure how projects are organized within the Visual Studio solution when generating build files for that IDE. The `BUILD_USE_SOLUTION_FOLDERS` option enables grouping projects, and `SET_PROPERTY` applies this global property.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nOPTION(BUILD_USE_SOLUTION_FOLDERS \"Enable grouping of projects in VS\" ON)\nSET_PROPERTY(GLOBAL PROPERTY USE_FOLDERS ${BUILD_USE_SOLUTION_FOLDERS})\n```\n\n----------------------------------------\n\nTITLE: Preparing Training Data Files for FunASR\nDESCRIPTION: Examples of required data format for training FunASR models. Shows the structure of train_text.txt with transcripts and train_wav.scp with audio file paths, both sharing matching IDs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nID0012W0013 ÂΩìÂÆ¢Êà∑È£éÈô©ÊâøÂèóËÉΩÂäõËØÑ‰º∞‰æùÊçÆÂèëÁîüÂèòÂåñÊó∂\nID0012W0014 ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning\nID0012W0015 he tried to think how it could be\n```\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nID0012W0015 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\n```\n\n----------------------------------------\n\nTITLE: Configure and Set CPack Project Configuration File in CMake\nDESCRIPTION: Configures a CPack project-specific configuration file (`package.cmake.in`) using `configure_file` to substitute variables like `${PACKAGE_NAME}`. This generated file is then set as the `CPACK_PROJECT_CONFIG_FILE`, unless an alternative file path is provided via the CMake cache variable `CPACK_PROJECT_CONFIG_FILE`, allowing external overrides.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_40\n\nLANGUAGE: CMake\nCODE:\n```\n  # generator specific configuration file\n  #\n  # allow package maintainers to use their own configuration file\n  # $ cmake -DCPACK_PROJECT_CONFIG_FILE:FILE=/path/to/package/config\n  if (NOT CPACK_PROJECT_CONFIG_FILE)\n    configure_file (\n      \"${CMAKE_CURRENT_LIST_DIR}/cmake/package.cmake.in\"\n      \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-package.cmake\" @ONLY\n    )\n    set (CPACK_PROJECT_CONFIG_FILE \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-package.cmake\")\n  endif ()\n```\n\n----------------------------------------\n\nTITLE: FunASR Punctuation - Python\nDESCRIPTION: This snippet demonstrates how to perform punctuation restoration using the FunASR Python API. It loads a pre-trained punctuation model and adds punctuation to the input text. Requires the `funasr` library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\n\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Installing funasr from source via pip (Shell)\nDESCRIPTION: Commands to clone the FunASR repository from GitHub and install it in editable mode using pip3. An alternative command is provided for users in China using a mirror URL for faster downloads.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n# For the users in China, you could install with the command:\n# pip3 install -e ./ -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Setting Shared Library Version in CMake\nDESCRIPTION: This command sets a CMake variable named `SOVERSION` to the string value '16'. This variable is typically used later in the build scripts to embed version information into built shared libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(SOVERSION \"16\")\n```\n\n----------------------------------------\n\nTITLE: Define Private Headers and Source Files (CMake)\nDESCRIPTION: Defines lists for private header files (`PRIVATE_HDRS`) and source files (`GFLAGS_SRCS`). It conditionally appends Windows-specific files (`windows_port.h` and `windows_port.cc`) if the operating system is detected as Windows.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nset (PRIVATE_HDRS\n  \"defines.h\"\n  \"config.h\"\n  \"util.h\"\n  \"mutex.h\"\n)\n\nset (GFLAGS_SRCS\n  \"gflags.cc\"\n  \"gflags_reporting.cc\"\n  \"gflags_completions.cc\"\n)\n\nif (OS_WINDOWS)\n  list (APPEND PRIVATE_HDRS \"windows_port.h\")\n  list (APPEND GFLAGS_SRCS  \"windows_port.cc\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Generating SSL Certificate Components Using OpenSSL Shell Commands\nDESCRIPTION: This snippet provides a sequence of OpenSSL shell commands to generate a private RSA key with passphrase encryption, create a certificate signing request (CSR) file, remove the passphrase from the private key for convenience, and finally generate a self-signed certificate valid for 365 days. It requires the OpenSSL tool installed on the system. The commands are aimed at local SSL certificate generation, which is useful for development or testing environments but may not be suitable for production due to browser trust restrictions.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/ssl_key/readme.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n### 1) Generate a private key\nopenssl genrsa -des3 -out server.key 2048\n \n### 2) Generate a csr file\nopenssl req -new -key server.key -out server.csr\n \n### 3) Remove pass\ncp server.key server.key.org \nopenssl rsa -in server.key.org -out server.key\n \n### 4) Generated a crt file, valid for 1 year\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\n```\n\n----------------------------------------\n\nTITLE: Setting Library Type and Definitions with CMake\nDESCRIPTION: This conditional block determines whether to build shared or static libraries and sets the appropriate compiler definitions. If BUILD_SHARED_LIBS is true, the code checks for GFLAGS_IS_A_DLL and adds the DGFLAGS_IS_A_DLL definition if necessary. It also determines which gflags library to link against based on whether BUILD_gflags_LIB is true.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (BUILD_SHARED_LIBS)\n  set (type shared)\n  if (GFLAGS_IS_A_DLL)\n    add_definitions(-DGFLAGS_IS_A_DLL)\n  endif ()\nelse ()\n  set (type static)\nendif ()\nif (BUILD_gflags_LIB)\n  link_libraries (gflags_${type})\nelse ()\n  link_libraries (gflags_nothreads_${type})\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenBLAS System Libraries (Shell, Ubuntu/CentOS)\nDESCRIPTION: Installs the OpenBLAS linear algebra library required for optimized matrix operations in ONNX runtime builds. For Ubuntu, uses apt-get to install 'libopenblas-dev', and for CentOS the commented alternative is 'openblas-devel'. Requires root/sudo privileges. No user-specific inputs are required; command must be run with appropriate permissions on the target OS.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install libopenblas-dev #ubuntu\n# sudo yum -y install openblas-devel #centos\n```\n\n----------------------------------------\n\nTITLE: Update FunASR Service Model\nDESCRIPTION: This command updates the model (ASR, VAD, or PUNC) used by the FunASR service. It requires specifying the model type and either a ModelScope model ID or a local path to the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update [--asr_model | --vad_model | --punc_model] <model_id or local model path>\n```\n\n----------------------------------------\n\nTITLE: Add Glog Internal Object Library\nDESCRIPTION: Creates an OBJECT library target named `glog_internal` from the defined source files (`GLOG_SRCS`) and binary CMake modules. OBJECT libraries compile sources but do not link them into a final library, allowing other targets to use their compiled objects. It also sets the required compile features for this object target by inheriting them from the main `glog` target (which is defined later).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library (glog_internal OBJECT\n  ${_glog_BINARY_CMake_MODULES}\n  ${GLOG_SRCS}\n)\ntarget_compile_features (glog_internal PUBLIC $<TARGET_PROPERTY:glog,COMPILE_FEATURES>)\n```\n\n----------------------------------------\n\nTITLE: Configure Pkg-Config File\nDESCRIPTION: If `WITH_PKGCONFIG` is true, this block prepares and configures a `libglog.pc` file from a template (`libglog.pc.in`). It sets variables like version, prefix, libdir, and includedir based on CMake installation paths, and then uses `configure_file` to substitute these variables in the template. This generates a pkg-config file enabling users to easily find and link against glog.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_27\n\nLANGUAGE: CMake\nCODE:\n```\nif (WITH_PKGCONFIG)\n  set (VERSION ${PROJECT_VERSION})\n  set (prefix ${CMAKE_INSTALL_PREFIX})\n  set (exec_prefix ${CMAKE_INSTALL_FULL_BINDIR})\n  set (libdir ${CMAKE_INSTALL_FULL_LIBDIR})\n  set (includedir ${CMAKE_INSTALL_FULL_INCLUDEDIR})\n\n  configure_file (\n    \"${PROJECT_SOURCE_DIR}/libglog.pc.in\"\n    \"${PROJECT_BINARY_DIR}/libglog.pc\"\n    @ONLY\n  )\n\n  unset (VERSION)\n  unset (prefix)\n  unset (exec_prefix)\n  unset (libdir)\n  unset (includedir)\nendif (WITH_PKGCONFIG)\n```\n\n----------------------------------------\n\nTITLE: Handling modular extension creation of FST components using add_module function\nDESCRIPTION: This snippet defines a function 'add_module' that programmatically creates library modules for various FST components, links them with the core 'fst' library, sets export and folder properties, and installs them to specified directories. The function enhances modularity and reuse for multiple FST variants.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/compact/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction (add_module _name)\n    add_library(${ARGV})\n    if (TARGET ${_name})\n        target_link_libraries(${_name} fst)\n        set_target_properties(${_name} PROPERTIES \n            WINDOWS_EXPORT_ALL_SYMBOLS true\n            FOLDER \"compact/modules\"\n        )\n    endif()\n\n    #set_target_properties(${_name} PROPERTIES SOVERSION \"1\")\n    install(TARGETS ${_name} \n        LIBRARY DESTINATION \"lib/fst\"\n        ARCHIVE DESTINATION \"lib/fst\"\n        RUNTIME DESTINATION \"lib/fst\")\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Citing FunASR and related research papers using BibTeX\nDESCRIPTION: Provides BibTeX entries for academic papers related to the FunASR project and its core technologies like Paraformer and BAT. These entries can be used in academic publications to properly cite the work, attributing the relevant research to its authors.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README_zh.md#_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{gao2023funasr,\n  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},\n  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{An2023bat,\n  author={Keyu An and Xian Shi and Shiliang Zhang},\n  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{gao22b_interspeech,\n  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},\n  title={{Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition}},\n  year=2022,\n  booktitle={Proc. Interspeech 2022},\n  pages={2063--2067},\n  doi={10.21437/Interspeech.2022-9996}\n}\n@article{shi2023seaco,\n  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},\n  title={{SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability}},\n  year=2023,\n  journal={arXiv preprint arXiv:2308.03266(accepted by ICASSP2024)},\n}\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Offline GPU Pipeline (Shell)\nDESCRIPTION: Executes the `funasr-onnx-offline-rtf` tool to perform speech recognition on a list of long audio files specified in `long_test.scp`. It utilizes specific ONNX and TorchScript models for VAD (FSMN), ASR (Paraformer-large), and Punctuation (CT-Transformer). The command configures the process to run on a GPU (`--gpu`), using 20 threads (`--thread-num`), enabling BladeDISC optimization (`--bladedisc`), and processing data in batches of 20 (`--batch-size`). Model directories must point to the downloaded or pre-existing model files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch_cpp.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-onnx-offline-rtf \\\n    --model-dir    ./damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-torchscript \\\n    --vad-dir   ./damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n    --punc-dir  ./damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n    --gpu \\\n    --thread-num 20 \\\n    --bladedisc true \\\n    --batch-size 20 \\\n    --wav-path     ./long_test.scp\n```\n\n----------------------------------------\n\nTITLE: Generating Protobuf Files for FunASR GRPC (shell)\nDESCRIPTION: This command generates the necessary Python files from the paraformer.proto file using the gRPC tools. It utilizes the protoc compiler to generate paraformer_pb2.py and paraformer_pb2_grpc.py files.  The process involves specifying the proto path, input file and output directories which are used for establishing the gRPC client server communication.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/grpc/Readme.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m grpc_tools.protoc --proto_path=./proto -I ./proto --python_out=. --grpc_python_out=./ ./proto/paraformer.proto\n```\n\n----------------------------------------\n\nTITLE: Defining Initial Build Options for FST Components in CMake\nDESCRIPTION: This series of `option` commands defines boolean variables in the CMake cache, which allow users to control which specific components or utilities of the FST project should be built. Each option is given a default value (ON or OFF) and a descriptive string.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\noption(HAVE_BIN          \"Build the fst binaries\" ON)\noption(HAVE_SCRIPT       \"Build the fstscript\" ON)\noption(HAVE_COMPACT      \"Build compact\" ON)\noption(HAVE_COMPRESS \"Build compress\" OFF)\noption(HAVE_CONST   \"Build const\" ON)\noption(HAVE_FAR  \"Build far\" ON)\noption(HAVE_GRM \"Build grm\" ON)\noption(HAVE_PDT \"Build pdt\" ON)\noption(HAVE_MPDT \"Build mpdt\" ON)\noption(HAVE_LINEAR \"Build linear\" ON)\noption(HAVE_LOOKAHEAD \"Build lookahead\" ON)\noption(HAVE_NGRAM \"Build ngram\" ON)\noption(HAVE_PYTHON \"Build python\" OFF)\noption(HAVE_SPECIAL \"Build special\" ON)\n```\n\n----------------------------------------\n\nTITLE: Defining Project Dependencies (pip)\nDESCRIPTION: This snippet lists the Python package dependencies and their minimum version requirements for the 'funasr' project.  It's a standard format for pip, used to specify the packages that need to be installed before running the project.  The packages listed ensure compatibility and access to the project's functionalities. The required input is a python environment to install the dependencies.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/http/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmodelscope>=1.11.1\nfunasr>=1.0.5\nfastapi>=0.95.1\naiofiles\nuvicorn\nrequests\n```\n\n----------------------------------------\n\nTITLE: Set Common CPack Package Metadata in CMake\nDESCRIPTION: Defines common metadata for the CPack package. This includes setting the package vendor, contact email, name, version (major, minor, patch), summary description, paths to resource files (welcome message, license), installation prefix, output directory, and enabling package relocation and monolithic installs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_34\n\nLANGUAGE: CMake\nCODE:\n```\n  # common package information\n  set (CPACK_PACKAGE_VENDOR              \"Andreas Schuh\")\n  set (CPACK_PACKAGE_CONTACT             \"google-gflags@googlegroups.com\")\n  set (CPACK_PACKAGE_NAME                \"${PACKAGE_NAME}\")\n  set (CPACK_PACKAGE_VERSION             \"${PACKAGE_VERSION}\")\n  set (CPACK_PACKAGE_VERSION_MAJOR       \"${PACKAGE_VERSION_MAJOR}\")\n  set (CPACK_PACKAGE_VERSION_MINOR       \"${PACKAGE_VERSION_MINOR}\")\n  set (CPACK_PACKAGE_VERSION_PATCH       \"${PACKAGE_VERSION_PATCH}\")\n  set (CPACK_PACKAGE_DESCRIPTION_SUMMARY \"${PACKAGE_DESCRIPTION}\")\n  set (CPACK_RESOURCE_FILE_WELCOME       \"${CMAKE_CURRENT_BINARY_DIR}/README.txt\")\n  set (CPACK_RESOURCE_FILE_LICENSE       \"${CMAKE_CURRENT_LIST_DIR}/COPYING.txt\")\n  set (CPACK_PACKAGE_DESCRIPTION_FILE    \"${CMAKE_CURRENT_BINARY_DIR}/README.txt\")\n  set (CPACK_INSTALL_PREFIX              \"${CMAKE_INSTALL_PREFIX}\")\n  set (CPACK_OUTPUT_FILE_PREFIX          packages)\n  set (CPACK_PACKAGE_RELOCATABLE         TRUE)\n  set (CPACK_MONOLITHIC_INSTALL          TRUE)\n```\n\n----------------------------------------\n\nTITLE: Installing gRPC from Source (Shell)\nDESCRIPTION: Configures shell environment variables for the gRPC installation path and updates PATH and PKG_CONFIG_PATH. Clones the gRPC repository at version 1.52.0, configures the build with CMake for installation, builds the project using Make, and installs it to the directory specified by GRPC_INSTALL_DIR. This is a prerequisite for compiling the FunASR gRPC server.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/Readme.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# add grpc environment variables\necho \"export GRPC_INSTALL_DIR=/path/to/grpc\" >> ~/.bashrc\necho \"export PKG_CONFIG_PATH=\\$GRPC_INSTALL_DIR/lib/pkgconfig\" >> ~/.bashrc\necho \"export PATH=\\$GRPC_INSTALL_DIR/bin/:\\$PKG_CONFIG_PATH:\\$PATH\" >> ~/.bashrc\nsource ~/.bashrc\n\n# install grpc\ngit clone --recurse-submodules -b v1.52.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc\n\ncd grpc\nmkdir -p cmake/build\npushd cmake/build\ncmake -DgRPC_INSTALL=ON \\\n      -DgRPC_BUILD_TESTS=OFF \\\n      -DCMAKE_INSTALL_PREFIX=\\$GRPC_INSTALL_DIR \\\n      ../..\nmake\nmake install\npopd\n```\n\n----------------------------------------\n\nTITLE: Enabling Testing with CTest in CMake\nDESCRIPTION: This snippet conditionally includes the CTest module and adds the 'test' subdirectory to the build process if the `BUILD_TESTING` option is enabled. This integrates the project's tests into the standard CMake/CTest testing framework.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# testing - MUST follow the generation of the build tree config file\nif (BUILD_TESTING)\n  include (CTest)\n  add_subdirectory (test)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable and Linking Libraries for FunASR HTTP Server (CMake)\nDESCRIPTION: This segment finds the ZLIB package and registers a set of source files (globbed as all .cpp files in the current directory plus the previously defined RELATION_SOURCE files) to create the funasr-http-server executable using add_executable. It concludes by linking the executable to the FunASR library, as well as OpenSSL crypto and SSL libraries, ensuring that all dependencies are available at link time. This standard CMake pattern constructs a build target and specifies its linkage against public and third-party dependencies.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/bin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(ZLIB REQUIRED)\n\nfile(GLOB SRC_FILES \"*.cpp\")\nadd_executable(funasr-http-server ${SRC_FILES} ${RELATION_SOURCE})\n\n\n\ntarget_link_libraries(funasr-http-server PUBLIC funasr ${OPENSSL_CRYPTO_LIBRARY} ${OPENSSL_SSL_LIBRARY})\n```\n\n----------------------------------------\n\nTITLE: Non-Real-Time (Batch) Speech Recognition with Extra Model Components - Python\nDESCRIPTION: This code demonstrates initializing AutoModel with both main ASR and auxiliary (VAD, punctuation, speaker) models, then runs ASR on a file with batching and hotword parameters. Dependencies include funasr, models/weights for all listed components, and a proper input file. Custom VAD and batch parameters allow adaptation to long audio segments and memory constraints. Outputs are transcriptions with specified enhancements.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Deploying FunASR C++ Runtime via Script\nDESCRIPTION: Downloads a deployment script for the FunASR C++ runtime for offline CPU-based speech recognition and executes it with sudo permissions. The `--workspace` argument specifies the directory where runtime resources will be installed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme_zh.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/funasr-runtime-deploy-offline-cpu-zh.sh;\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh install --workspace /root/funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: Performing Direct SenseVoice Inference using SenseVoiceSmall Class (Python)\nDESCRIPTION: This Python code snippet illustrates how to directly use the `SenseVoiceSmall` model class for inference on audio files shorter than 30 seconds. It loads the pretrained model using `from_pretrained`, sets it to evaluation mode (`m.eval()`), and calls the `inference` method directly. It processes an example audio file, handles language detection, optionally applies ITN, and manages emotion tagging settings (`ban_emo_unk`). The resulting rich transcription text is post-processed and printed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs ['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res [0][0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Collecting Header Files for FST Linear Extension\nDESCRIPTION: Gathers all header files from the linear extensions directory and displays them.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/linear/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB HEADER_FILES ../../include/fst/extensions/linear/*.h)\nmessage(STATUS \"${HEADER_FILES}\")\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking Main Executable - CMake\nDESCRIPTION: Defines the main executable target `paraformer-server` from its source file `paraformer-server.cc` and links it against various dependencies including the `rg_grpc_proto` library, the `funasr` library, extra libraries, and core gRPC/Protobuf libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(paraformer-server paraformer-server.cc)\ntarget_link_libraries(paraformer-server\n  rg_grpc_proto\n  funasr\n  ${EXTRA_LIBS}\n  ${_REFLECTION}\n  ${_GRPC_GRPCPP}\n  ${_PROTOBUF_LIBPROTOBUF})\n```\n\n----------------------------------------\n\nTITLE: Testing Pthread Read-Write Lock Support in C++ with CMake\nDESCRIPTION: This snippet checks for pthread read-write lock support by compiling a C++ test program that includes pthread.h, defines, initializes, and acquires a pthread_rwlock_t lock. It uses CMake conditionals to add the appropriate thread libraries and evaluates if the RT-lock APIs are available. This information is cached as HAVE_RWLOCK and used to conditionally enable thread-safe features.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (Threads_FOUND)\n  cmake_push_check_state (RESET)\n  set (CMAKE_REQUIRED_LIBRARIES Threads::Threads)\n  check_cxx_source_compiles (\"\n#define _XOPEN_SOURCE 500\n#include <pthread.h>\nint main(void)\n{\n  pthread_rwlock_t l;\n  pthread_rwlock_init(&l, NULL);\n  pthread_rwlock_rdlock(&l);\n  return 0;\n}\n  \" HAVE_RWLOCK)\n  cmake_pop_check_state ()\nendif (Threads_FOUND)\n```\n\n----------------------------------------\n\nTITLE: PUNC Model Inference in C++\nDESCRIPTION: This C++ code snippet shows the inference step for the punctuation model using `CTTransformerInfer`. It takes the PUNC handle and the input text string. It applies punctuation to the input text using the initialized PUNC model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_16\n\nLANGUAGE: c++\nCODE:\n```\nFUNASR_RESULT result=CTTransformerInfer(punc_hanlde, txt_str.c_str(), RASR_NONE, NULL);\n```\n\n----------------------------------------\n\nTITLE: Running ASR 2pass (Offline/Online) Client (Microphone) via Shell\nDESCRIPTION: Initiates 2pass mode (combining online and offline ASR) from the microphone, using specified chunk sizes. Suitable for real-time and high-accuracy recognition. Requires mode and chunk_size parameters.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"0.0.0.0\" --port 10095 --mode 2pass --chunk_size \"8,8,4\"\n```\n\n----------------------------------------\n\nTITLE: Updating Load Models and Restarting the Service\nDESCRIPTION: Command to update the deployed model repository or parameters such as model ID, host port, Docker port, or thread counts through shell arguments, followed by a restart.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --asr_model damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n```\n\n----------------------------------------\n\nTITLE: Generating lexicon for language models in FunASR\nDESCRIPTION: Runs a Python script to generate the lexicon output file from corpus dictionary and existing lexicon. Maps words to their phonetic or character-based representations for ASR decoding.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/lm_train_tutorial.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython3 fst/generate_lexicon.py lm/corpus.dict lm/lexicon.txt lm/lexicon.out\n```\n\n----------------------------------------\n\nTITLE: Registering a New Model - Python\nDESCRIPTION: This Python code snippet demonstrates how to register a new model class using the @tables.register decorator. The registered class must implement __init__, forward, and inference methods.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n  def __init__(*args, **kwargs):\n    ...\n\n  def forward(\n      self,\n      **kwargs,\n  ):  \n\n  def inference(\n      self,\n      data_in,\n      data_lengths=None,\n      key: list = None,\n      tokenizer=None,\n      frontend=None,\n      **kwargs,\n  ):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Cloning FunASR Repository - Shell\nDESCRIPTION: Clones the official FunASR Git repository from GitHub to the local machine and then changes the current working directory into the newly cloned repository. This is necessary to access the benchmark scripts and utility files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd FunASR\n```\n\n----------------------------------------\n\nTITLE: Inference with SenseVoiceSmall\nDESCRIPTION: This code snippet demonstrates how to perform inference using the SenseVoiceSmall model from the FunASR library. It loads the pre-trained model, sets it to evaluation mode, and performs inference on an example audio file. The post-processed text is then printed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Run Server Parameters\nDESCRIPTION: This text describes the parameters for the `run_server.sh` script.  It details options such as model directories, port, thread numbers, SSL certificates, and hotword file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n--download-model-dir Ê®°Âûã‰∏ãËΩΩÂú∞ÂùÄÔºåÈÄöËøáËÆæÁΩÆmodel ID‰ªéModelscope‰∏ãËΩΩÊ®°Âûã\n--model-dir  modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--vad-dir  modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--punc-dir  modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--lm-dir modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--itn-dir modelscope model ID ÊàñËÄÖ Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ\n--port  ÊúçÂä°Á´ØÁõëÂê¨ÁöÑÁ´ØÂè£Âè∑ÔºåÈªòËÆ§‰∏∫ 10095\n--decoder-thread-num  ÊúçÂä°Á´ØÁ∫øÁ®ãÊ±†‰∏™Êï∞(ÊîØÊåÅÁöÑÊúÄÂ§ßÂπ∂ÂèëË∑ØÊï∞)Ôºå\n                      **Âª∫ËÆÆÊØèË∑ØÂàÜÈÖç1GÊòæÂ≠òÔºåÂç≥20GÊòæÂ≠òÂèØÈÖçÁΩÆ20Ë∑ØÂπ∂Âèë**\n--io-thread-num  ÊúçÂä°Á´ØÂêØÂä®ÁöÑIOÁ∫øÁ®ãÊï∞\n--model-thread-num  ÊØèË∑ØËØÜÂà´ÁöÑÂÜÖÈÉ®Á∫øÁ®ãÊï∞(ÊéßÂà∂ONNXÊ®°ÂûãÁöÑÂπ∂Ë°å)ÔºåÈªòËÆ§‰∏∫ 1Ôºå\n                    ÂÖ∂‰∏≠Âª∫ËÆÆ decoder-thread-num*model-thread-num Á≠â‰∫éÊÄªÁ∫øÁ®ãÊï∞\n--certfile  sslÁöÑËØÅ‰π¶Êñá‰ª∂ÔºåÈªòËÆ§‰∏∫Ôºö../../../ssl_key/server.crtÔºåÂ¶ÇÊûúÈúÄË¶ÅÂÖ≥Èó≠sslÔºåÂèÇÊï∞ËÆæÁΩÆ‰∏∫0\n--keyfile   sslÁöÑÂØÜÈí•Êñá‰ª∂ÔºåÈªòËÆ§‰∏∫Ôºö../../../ssl_key/server.key\n--hotword   ÁÉ≠ËØçÊñá‰ª∂Ë∑ØÂæÑÔºåÊØèË°å‰∏Ä‰∏™ÁÉ≠ËØçÔºåÊ†ºÂºèÔºöÁÉ≠ËØç ÊùÉÈáç(‰æãÂ¶Ç:ÈòøÈáåÂ∑¥Â∑¥ 20)Ôºå\n            Â¶ÇÊûúÂÆ¢Êà∑Á´ØÊèê‰æõÁÉ≠ËØçÔºåÂàô‰∏éÂÆ¢Êà∑Á´ØÊèê‰æõÁöÑÁÉ≠ËØçÂêàÂπ∂‰∏ÄËµ∑‰ΩøÁî®ÔºåÊúçÂä°Á´ØÁÉ≠ËØçÂÖ®Â±ÄÁîüÊïàÔºåÂÆ¢Êà∑Á´ØÁÉ≠ËØçÂè™ÈíàÂØπÂØπÂ∫îÂÆ¢Êà∑Á´ØÁîüÊïà„ÄÇ\n```\n\n----------------------------------------\n\nTITLE: Initial Offline Communication Message Format\nDESCRIPTION: This JSON snippet shows the initial message format for offline file transcription. It includes parameters such as mode (offline), wav_name, wav_format, is_speaking, audio_fs, hotwords and itn (inverse text normalization). The hotwords parameter requires a string representation of a dictionary.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"offline\", \"wav_name\": \"wav_name\", \"wav_format\":\"pcm\", \"is_speaking\": True, \"wav_format\":\"pcm\", \"hotwords\":\"{\"ÈòøÈáåÂ∑¥Â∑¥\":20,\"ÈÄö‰πâÂÆûÈ™åÂÆ§\":30}\", \"itn\":True}\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Tests for Version Information with CMake\nDESCRIPTION: These commands add gflags tests to check the output of the --version flag. They verify that the output contains specific strings (like \"gflags_unittest\" or \"version test_version\").\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\n# just print the version info and exit\nadd_gflags_test(version-1 0 \"gflags_unittest\"      \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --version)\nadd_gflags_test(version-2 0 \"version test_version\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --version)\n```\n\n----------------------------------------\n\nTITLE: Setting Linker Options and Library Dependencies for FunASR Executables in CMake\nDESCRIPTION: This snippet configures linker options and links specific libraries to the FunASR WebSocket server and client executables. The linker options include disabling the --as-needed flag for server targets to ensure correct library linking order. Clients and servers link against core FunASR libraries, OpenSSL's crypto and SSL components, and optionally PortAudio for audio device support in the 2-pass client variant, ensuring proper runtime dependencies are resolved.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/bin/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_options(funasr-wss-server PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_options(funasr-wss-server-2pass PRIVATE \"-Wl,--no-as-needed\")\n\ntarget_link_libraries(funasr-wss-client PUBLIC funasr ${OPENSSL_CRYPTO_LIBRARY} ${OPENSSL_SSL_LIBRARY})\ntarget_link_libraries(funasr-wss-client-2pass PUBLIC funasr ${OPENSSL_CRYPTO_LIBRARY} ${OPENSSL_SSL_LIBRARY} portaudio)\ntarget_link_libraries(funasr-wss-server PUBLIC funasr ${OPENSSL_CRYPTO_LIBRARY} ${OPENSSL_SSL_LIBRARY})\ntarget_link_libraries(funasr-wss-server-2pass PUBLIC funasr ${OPENSSL_CRYPTO_LIBRARY} ${OPENSSL_SSL_LIBRARY})\n```\n\n----------------------------------------\n\nTITLE: Checking Symbolization Capability on Windows Using DbgHelp Library and C++\nDESCRIPTION: This snippet includes a C++ source test checked by CMake to verify if symbolization via Windows Debug Help (DbgHelp) API is available. It initializes symbols on the current process and attempts to retrieve the symbol name of a sample function. The success enables setting HAVE_SYMBOLIZE and subsequently HAVE_STACKTRACE flags, allowing debug symbol reporting and stack trace features on Windows.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n#include <windows.h>\\n#include <dbghelp.h>\\n#include <cstdlib>\\n\\nvoid foobar() { }\\n\\nint main()\\n{\\n    HANDLE process = GetCurrentProcess();\\n\\n    if (!SymInitialize(process, NULL, TRUE))\\n        return EXIT_FAILURE;\\n\\n    char buf[sizeof(SYMBOL_INFO) + MAX_SYM_NAME];\\n    SYMBOL_INFO *symbol = reinterpret_cast<SYMBOL_INFO *>(buf);\\n    symbol->SizeOfStruct = sizeof(SYMBOL_INFO);\\n    symbol->MaxNameLen = MAX_SYM_NAME;\\n\\n    void* const pc = reinterpret_cast<void*>(&foobar);\\n    BOOL ret = SymFromAddr(process, reinterpret_cast<DWORD64>(pc), 0, symbol);\\n\\n    return ret ? EXIT_SUCCESS : EXIT_FAILURE;\\n}\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Training (Command Line)\nDESCRIPTION: This snippet demonstrates how to train FunASR models from the command line. It shows the usage of the `funasr-train` command with specified model, training data, validation data, and output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Adding Executable for gflags Declaration Test with CMake\nDESCRIPTION: This command creates an executable named gflags_declare_test, built from gflags_declare_test.cc and gflags_declare_flags.cc. This tests functionality of declaring flags in a separate header file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# use gflags_declare.h\nadd_executable (gflags_declare_test gflags_declare_test.cc gflags_declare_flags.cc)\n```\n\n----------------------------------------\n\nTITLE: Mapping gflags as an External Dependency in Bazel WORKSPACE File\nDESCRIPTION: This snippet provides an example of how to declare gflags as an external git repository dependency in a Bazel project's WORKSPACE file. The 'git_repository' rule specifies the repository name, commit SHA for versioning, and the remote GitHub URL. Once declared, gflags can be added to build rules' dependencies using '@com_github_gflags_gflags//:gflags'. This setup enables including gflags headers in source code with '#include <gflags/gflags.h>'. Prerequisites include a working Bazel build environment and specifying a valid commit SHA. This facilitates reproducible builds and integration of external C++ libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/INSTALL.md#_snippet_2\n\nLANGUAGE: Bazel\nCODE:\n```\ngit_repository(\n    name = \"com_github_gflags_gflags\",\n    commit = \"<INSERT COMMIT SHA HERE>\",\n    remote = \"https://github.com/gflags/gflags.git\",\n)\n```\n\n----------------------------------------\n\nTITLE: Launching Triton Inference Server for SenseVoice\nDESCRIPTION: Command to start the Triton Inference Server with the SenseVoice model repository, configuring memory pools for optimal performance.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# launch the service \ntritonserver --model-repository /workspace/model_repo_sensevoice_small \\\n             --pinned-memory-pool-byte-size=512000000 \\\n             --cuda-memory-pool-byte-size=0:1024000000\n```\n\n----------------------------------------\n\nTITLE: Large-Scale Data Training File Example\nDESCRIPTION: This example demonstrates the format of the data.list file for Large-Scale Data Training. Each line should contain a single JSONL filename.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ndata/list/train.0.jsonl\ndata/list/train.1.jsonl\n...\n```\n\n----------------------------------------\n\nTITLE: Installing FST N-gram Library\nDESCRIPTION: This snippet uses the `install` command to specify where the `fstngram` library should be installed. It installs the library to the `lib` directory for both shared library, archive, and runtime components.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS fstngram \n\tLIBRARY DESTINATION lib\n\tARCHIVE DESTINATION lib\n\tRUNTIME DESTINATION lib\n)\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR HTML5 Server (Python)\nDESCRIPTION: This code snippet demonstrates how to start the FunASR HTML5 server using Python. It navigates to the html5 directory and executes the h5Server.py script. The --host and --port arguments specify the IP address and port number for the server, allowing access from other devices.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd funasr/runtime/html5\npython h5Server.py --host 0.0.0.0 --port 1337\n```\n\n----------------------------------------\n\nTITLE: Adding Executables with CMake Function\nDESCRIPTION: This section uses the add_executable2 function to create four executables: mpdtcompose, mpdtexpand, mpdtinfo, and mpdtreverse, each with their respective source files. This relies on the function definition from the previous snippet.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/mpdt/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable2(mpdtcompose  mpdtcompose.cc)\n  add_executable2(mpdtexpand  mpdtexpand.cc)\n  add_executable2(mpdtinfo  mpdtinfo.cc)\n  add_executable2(mpdtreverse  mpdtreverse.cc)\n```\n\n----------------------------------------\n\nTITLE: Fetching and Including External Dependencies with FetchContent (CMake)\nDESCRIPTION: This snippet uses CMake's FetchContent module to download and make available the nlohmann/json library if it is not already present. It conditionally declares and fetches the dependency, ensuring that the correct version (v3.11.2) is used. Prerequisites: network access and CMake 3.11+. The input is an existing or missing json/ChangeLog.md file as a proxy for existence; the output is a ready-to-use json source directory. Limitation: Assumes correct project directory structure and internet connectivity.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(FetchContent)\nif(NOT EXISTS ${PROJECT_SOURCE_DIR}/third_party/json/ChangeLog.md )\nFetchContent_Declare(json\n  URL   https://github.com/nlohmann/json/archive/refs/tags/v3.11.2.tar.gz\nSOURCE_DIR ${PROJECT_SOURCE_DIR}/third_party/json\n)\n\nFetchContent_MakeAvailable(json)\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running FunASR VAD ONNX Model in C++\nDESCRIPTION: Demonstrates the initialization and inference steps for using a FunASR Voice Activity Detection (VAD) model with ONNX backend in C++. Requires the FunASR library, a valid model directory containing 'model-dir' and 'quantize', an audio file, and ONNX runtime support. The main parameters are model_path (path to model files), thread_num (number of ONNX threads), wav_file (audio file path), and sampling_rate (sample rate, default 16k). Returns a handle to the model and the inference result; errors may occur if model files or the ONNX runtime are misconfigured.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n// The use of the VAD model consists of two steps: FsmnVadInit and FsmnVadInfer:\nFUNASR_HANDLE vad_hanlde=FsmnVadInit(model_path, thread_num);\n// Where: model_path contains \"model-dir\" and \"quantize\", thread_num is the ONNX thread count;\nFUNASR_RESULT result=FsmnVadInfer(vad_hanlde, wav_file.c_str(), NULL, 16000);\n// Where: vad_hanlde is the return value of FunOfflineInit, wav_file is the path to the audio file, and sampling_rate is the sampling rate (default 16k).\n```\n\n----------------------------------------\n\nTITLE: Exporting ASR Model to ONNX with FunASR in Shell\nDESCRIPTION: Exports a pre-trained Paraformer ASR model to the ONNX format using the FunASR command-line interface. The script requires specifying the model name, export directory, export type, and whether quantization is enabled. The exported model files are saved in the designated directory. Dependencies include an installed FunASR environment and pre-downloaded model artifacts.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/libtorch/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.export.export_model --model-name damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch --export-dir ./export --type torch --quantize True\n```\n\n----------------------------------------\n\nTITLE: View Registry - Python\nDESCRIPTION: This code snippet shows how to view the registered components in FunASR using the `tables.print()` function.  It allows users to inspect the available models, frontends, and other components registered within the FunASR framework. You can also specify the model you'd like to view, by executing `tables.print(\"model\")`\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\ntables.print()\n```\n\n----------------------------------------\n\nTITLE: Speech Model Inference Using FunASR CLI Without configuration.json File (Shell)\nDESCRIPTION: This command-line snippet shows how to perform model inference when no configuration.json file exists in the model directory. Users must manually specify the configuration path and name, initialization model file, tokenizer vocabulary file, CMVN file for feature normalization, input audio, output directory, and device. This approach requires all relevant config parameters because automatic config loading is unavailable. Parameters like config-path and config-name locate the experiment config, while others specify necessary files for inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: C++ Client Command for Offline Transcription\nDESCRIPTION: This shell command launches the FunASR C++ client. It specifies the server IP address, port, the audio file path (`test.wav`), number of threads, and if SSL is enabled. It's another way to test the offline transcription service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n. /funasr-wss-client --server-ip 127.0.0.1 --port 10095 --wav-path test.wav --thread-num 1 --is-ssl 1\n```\n\n----------------------------------------\n\nTITLE: Preventing In-source Build Configuration in CMake\nDESCRIPTION: Prevents in-source builds by checking if source and binary directories are the same, and provides instructions on the correct build process if they are. This is a safety measure to avoid polluting the source tree with build artifacts.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(\"x${CMAKE_SOURCE_DIR}\" STREQUAL \"x${CMAKE_BINARY_DIR}\")\n  message(FATAL_ERROR \"\\\nIn-source build is not a good practice.\nPlease use:\n  mkdir build\n  cd build\n  cmake ..\nto build this project\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-specific Settings and Dependencies in CMake\nDESCRIPTION: Sets up platform-specific settings for Windows, configures Python and test dependencies if enabled, and performs system capability checks. Also sets the installation directory if not already defined.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake/Modules)\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake)\n\nif(WIN32)\n  add_definitions(-DNOMINMAX) # Otherwise, std::max() and std::min() won't work\nendif()\n\nif(KALDI_NATIVE_FBANK_BUILD_PYTHON)\n  include(pybind11)\nendif()\n\nif(KALDI_NATIVE_FBANK_BUILD_TESTS)\n  enable_testing()\n  include(googletest)\nendif()\n\nif(NOT CMAKE_INSTALL_PREFIX)\n  set(CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}/install\")\nendif()\n\ninclude(CheckIncludeFileCXX)\ncheck_include_file_cxx(cxxabi.h KNF_HAVE_CXXABI_H)\ncheck_include_file_cxx(execinfo.h KNF_HAVE_EXECINFO_H)\n\ninclude_directories(${CMAKE_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Set Windows Specific Include Directories\nDESCRIPTION: If the target platform is CYGWIN or WIN32, this adds the source 'src/windows' directory as an include path for both the `glog_internal` and `glog` targets. This is necessary to find Windows-specific header files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\nif (CYGWIN OR WIN32)\n  target_include_directories (glog_internal PUBLIC\n    \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src/windows>\"\n    PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/src/windows)\n\n  target_include_directories (glog PUBLIC\n    \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src/windows>\"\n    PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/src/windows)\nendif (CYGWIN OR WIN32)\n```\n\n----------------------------------------\n\nTITLE: Setting Output Directories with CMake\nDESCRIPTION: These lines define the output directories for runtime executables, libraries, and archives.  ${PROJECT_BINARY_DIR} refers to the build directory. Setting these variables directs the build process to place compiled binaries in designated locations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset (CMAKE_RUNTIME_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}/bin\")\nset (CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}/lib\")\nset (CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}/lib\")\n```\n\n----------------------------------------\n\nTITLE: Detecting System Endianness and Logging Build Info (CMake)\nDESCRIPTION: This section includes and uses the TestBigEndian module to detect whether the system is big endian or little endian, emitting a build-time message accordingly. No additional dependencies are required beyond CMake. It outputs a build message to inform users or scripts about system architecture, impacting binary compatibility. There are no inputs or outputs beyond the build-time messages; cross-platform CMake logic is employed for robust hardware detection.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(TestBigEndian)\ntest_big_endian(BIG_ENDIAN)\nif(BIG_ENDIAN)\n    message(\"Big endian system\")\nelse()\n    message(\"Little endian system\")\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Installing CMake Configuration and Export Files\nDESCRIPTION: This snippet installs the generated CMake configuration files (`yaml-cpp-config.cmake`, `yaml-cpp-config-version.cmake`) and the exported target file (`yaml-cpp-targets.cmake`, referenced via `EXPORT yaml-cpp-targets`) to the calculated installation directory (`${INSTALL_CMAKE_DIR}`). This makes the installed library discoverable by `find_package()`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES\n\t\"${PROJECT_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/yaml-cpp-config.cmake\"\n\t\"${PROJECT_BINARY_DIR}/yaml-cpp-config-version.cmake\"\n\tDESTINATION \"${INSTALL_CMAKE_DIR}\" COMPONENT dev)\ninstall(EXPORT yaml-cpp-targets DESTINATION ${INSTALL_CMAKE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Defining the main fstcompact library with source files and properties\nDESCRIPTION: This snippet creates a static library 'fstcompact' from multiple source files related to compact finite state transducers (FSTs), setting version properties and establishing dependencies on the 'fst' library. It also specifies installation destinations for the built library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/compact/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(fstcompact\n  compact8_acceptor-fst.cc \n  compact8_string-fst.cc \n  compact8_unweighted-fst.cc \n  compact8_unweighted_acceptor-fst.cc \n  compact8_weighted_string-fst.cc \n  compact16_acceptor-fst.cc \n  compact16_string-fst.cc \n  compact16_unweighted-fst.cc \n  compact16_unweighted_acceptor-fst.cc \n  compact16_weighted_string-fst.cc \n  compact64_acceptor-fst.cc \n  compact64_string-fst.cc \n  compact64_unweighted-fst.cc \n  compact64_unweighted_acceptor-fst.cc \n  compact64_weighted_string-fst.cc\n)\n\ntarget_link_libraries(fstcompact fst)\nset_target_properties(fstcompact PROPERTIES \n  SOVERSION \"${SOVERSION}\"\n  FOLDER \"compact\"\n)\n\ninstall(TARGETS fstcompact \n        LIBRARY DESTINATION \"lib\"\n        ARCHIVE DESTINATION \"lib\"\n        RUNTIME DESTINATION \"lib\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configure CPack Binary Package Settings in CMake\nDESCRIPTION: Sets configuration options for binary packages. `CPACK_INCLUDE_TOPLEVEL_DIRECTORY` controls whether the installation files are placed within a top-level directory inside the package. `CPACK_PACKAGE_FILE_NAME` constructs the base filename for the binary package using the package name, version, system name, and architecture (if defined).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_39\n\nLANGUAGE: CMake\nCODE:\n```\n  # default binary package settings\n  set (CPACK_INCLUDE_TOPLEVEL_DIRECTORY TRUE)\n  set (CPACK_PACKAGE_FILE_NAME          \"${CPACK_PACKAGE_NAME}-${CPACK_PACKAGE_VERSION}-${CPACK_SYSTEM_NAME}\")\n  if (CPACK_PACKAGE_ARCHITECTURE)\n    set (CPACK_PACKAGE_FILE_NAME \"${CPACK_PACKAGE_FILE_NAME}-${CPACK_PACKAGE_ARCHITECTURE}\")\n  endif ()\n```\n\n----------------------------------------\n\nTITLE: Training with Sliced JSONL Files - Shell\nDESCRIPTION: This shell command trains the model using sliced JSONL files to handle large datasets. It specifies the list of sliced JSONL files and the number of data splits. This approach is useful when dealing with memory limitations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ntrain_data=\"/root/data/list/data.list\"\n\nfunasr/bin/train_ds.py \\\n++train_data_set_list=\"${train_data}\" \\\n++dataset_conf.data_split_num=256\n```\n\n----------------------------------------\n\nTITLE: Testing the FunASR HTTP Server Using curl - Shell\nDESCRIPTION: This shell command tests the running FunASR HTTP server by uploading an example '.wav' file via curl using a POST form. The server is expected to respond on localhost port 80. Requires that the server is already running and accessible, and the 'example.wav' file is present in the current directory. Output is the server's HTTP response.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl -F \"file=@example.wav\" 127.0.0.1:80\n```\n\n----------------------------------------\n\nTITLE: FunASR VAD (Non-Real-Time) - Python\nDESCRIPTION: This snippet demonstrates how to perform non-real-time voice activity detection (VAD) using the FunASR Python API. It loads a pre-trained VAD model and performs inference on an audio file. Requires the `funasr` library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Exporting Model via Command Line (funasr-export)\nDESCRIPTION: This command exports the trained model into an ONNX-compatible format or other deployment formats using the funasr-export CLI, with optional quantization. It facilitates model deployment in inference pipelines.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_21\n\nLANGUAGE: Shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false\n```\n\n----------------------------------------\n\nTITLE: Changing Directory to Utils - Shell\nDESCRIPTION: Navigates into the specific `funasr/runtime/python/utils` subdirectory within the cloned FunASR repository. This directory contains the `requirements.txt` file listing dependencies needed for the benchmark utilities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd funasr/runtime/python/utils\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Model Repositories\nDESCRIPTION: Provides a shell command to install optional libraries, `modelscope` and `huggingface_hub`. These libraries are needed to download and use pretrained models available on the ModelScope and Hugging Face platforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U modelscope huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Detecting if gflags is Built as a CMake Subproject\nDESCRIPTION: Checks if the gflags build is part of a larger CMake project (subproject). It sets the `GFLAGS_IS_SUBPROJECT` variable to TRUE if the top-level source directory (`CMAKE_SOURCE_DIR`) differs from the gflags project source directory (`PROJECT_SOURCE_DIR`), unless `GFLAGS_IS_SUBPROJECT` was explicitly set beforehand. This influences whether configuration variables are added to the CMake cache.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\n# when gflags is included as subproject (e.g., as Git submodule/subtree) in the source\n# tree of a project that uses it, no variables should be added to the CMake cache;\n# users may set the non-cached variable GFLAGS_IS_SUBPROJECT before add_subdirectory(gflags)\nif (NOT DEFINED GFLAGS_IS_SUBPROJECT)\n  if (\"^${CMAKE_SOURCE_DIR}$\" STREQUAL \"^${PROJECT_SOURCE_DIR}$\")\n    set (GFLAGS_IS_SUBPROJECT FALSE)\n  else ()\n    set (GFLAGS_IS_SUBPROJECT TRUE)\n  endif ()\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Optimizing ONNX Model with onnxslim (Shell)\nDESCRIPTION: Executes the `onnxslim` command-line tool to apply various optimizations to an ONNX model file. This step is typically performed after exporting a model to ONNX to reduce size and improve inference performance. Requires the `onnxslim` Python package (`pip install -U onnxslim`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\n# pip3 install -U onnxslim\nonnxslim model.onnx model.onnx\n```\n\n----------------------------------------\n\nTITLE: Declaring Spring Boot Web, JSON, and WebSocket Dependencies (Kotlin)\nDESCRIPTION: This snippet defines the dependencies required for a Spring Boot application, including Spring Boot web starter, JSON library for processing JSON data, and Spring Boot WebSocket starter for real-time communication. The dependencies are specified for implementation, indicating that they are required during compilation and runtime. There are no specific parameters. The output is a Spring Boot application with the necessary dependencies for web services, JSON handling, and WebSocket functionalities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/java/java_http2ws_src/http/src/Readme.md#_snippet_0\n\nLANGUAGE: Kotlin\nCODE:\n```\ndependencies {\n  implementation(\"org.springframework.boot:spring-boot-starter-web\")\n  implementation(\"org.json:json:20240303\")\n  implementation(\"org.springframework.boot:spring-boot-starter-websocket\")\n}\n```\n\n----------------------------------------\n\nTITLE: Running FunASR C++ Client\nDESCRIPTION: This command executes the compiled C++ client binary to connect to the FunASR server at a specific IP and port. It processes a single WAV file or a wav.scp list, optionally enabling SSL encryption.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n. /funasr-wss-client --server-ip 127.0.0.1 --port 10097 --wav-path test.wav --thread-num 1 --is-ssl 1\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Tests for Undefok Flag with CMake\nDESCRIPTION: These commands add gflags tests to explore the behavior of the --undefok flag, which allows unknown command-line flags. The tests verify that flags specified in --undefok are accepted, while those not specified are rejected.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\n# --undefok is a fun flag...\nadd_gflags_test(undefok-1 1 \"unknown command line flag 'foo'\" \"\"  gflags_unittest  --undefok= --foo --unused_bool)\nadd_gflags_test(undefok-2 0 \"PASS\" \"\"  gflags_unittest  --undefok=foo --foo --unused_bool)\n# If you say foo is ok to be undefined, we'll accept --nofoo as well\nadd_gflags_test(undefok-3 0 \"PASS\" \"\"  gflags_unittest  --undefok=foo --nofoo --unused_bool)\n# It's ok if the foo is in the middle\nadd_gflags_test(undefok-4 0 \"PASS\" \"\"  gflags_unittest  --undefok=fee,fi,foo,fum --foo --unused_bool)\n# But the spelling has to be just right...\nadd_gflags_test(undefok-5 1 \"unknown command line flag 'foo'\" \"\"  gflags_unittest  --undefok=fo --foo --unused_bool)\nadd_gflags_test(undefok-6 1 \"unknown command line flag 'foo'\" \"\"  gflags_unittest  --undefok=foot --foo --unused_bool)\n```\n\n----------------------------------------\n\nTITLE: Convert SCP to JSONL - Shell\nDESCRIPTION: This shell command converts wav.scp and text.txt files into a JSONL format, which can be used for training.  It specifies the input SCP files, the data types, and the output JSONL file. The scp2jsonl tool is used for this conversion.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Multi-Machine Multi-GPU Training (Worker) - Shell\nDESCRIPTION: Shell commands for a worker node in a multi-machine multi-GPU training setup. Similar to the master node, it sets `CUDA_VISIBLE_DEVICES` and calculates GPU count. The `torchrun` command specifies the total number of nodes (`--nnodes`), the current node's rank (`--node_rank`, non-zero), the number of processes per node (`--nproc_per_node`), and importantly, the `--master_addr`/`--master_port` of the master node for coordination.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr=192.168.1.1 --master_port=12345 \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation Rules for gflags in CMake\nDESCRIPTION: This snippet defines the installation rules for the gflags library components. It configures and installs CMake package configuration (`<package>-config.cmake`) and version (`<package>-config-version.cmake`) files, installs the built library targets (shared/static based on BUILD_* and INSTALL_* options) to the appropriate directories (RUNTIME, LIBRARY, ARCHIVE), installs public header files, exports CMake targets for use by other projects via `find_package`, and installs a pkg-config file (`.pc`) if applicable (on Unix-like systems).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# installation rules\nset (EXPORT_NAME ${PACKAGE_NAME}-targets)\nfile (RELATIVE_PATH INSTALL_PREFIX_REL2CONFIG_DIR \"${CMAKE_INSTALL_PREFIX}/${CONFIG_INSTALL_DIR}\" \"${CMAKE_INSTALL_PREFIX}\")\nconfigure_file (cmake/config.cmake.in  \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-config-install.cmake\" @ONLY)\nconfigure_file (cmake/version.cmake.in \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-config-version.cmake\" @ONLY)\n\nif (BUILD_SHARED_LIBS AND INSTALL_SHARED_LIBS)\n  foreach (opts IN ITEMS \"\" _nothreads)\n    if (BUILD_gflags${opts}_LIB)\n      install (TARGETS gflags${opts}_shared\n               EXPORT ${EXPORT_NAME}\n               RUNTIME DESTINATION ${RUNTIME_INSTALL_DIR}\n               LIBRARY DESTINATION ${LIBRARY_INSTALL_DIR}\n               ARCHIVE DESTINATION ${LIBRARY_INSTALL_DIR}\n      )\n    endif ()\n  endforeach ()\nendif ()\nif (BUILD_STATIC_LIBS AND INSTALL_STATIC_LIBS)\n  foreach (opts IN ITEMS \"\" _nothreads)\n    if (BUILD_gflags${opts}_LIB)\n      install (TARGETS gflags${opts}_static\n               EXPORT ${EXPORT_NAME}\n               RUNTIME DESTINATION ${RUNTIME_INSTALL_DIR}\n               LIBRARY DESTINATION ${LIBRARY_INSTALL_DIR}\n               ARCHIVE DESTINATION ${LIBRARY_INSTALL_DIR}\n      )\n    endif ()\n  endforeach ()\nendif ()\n\nif (INSTALL_HEADERS)\n  install (FILES ${PUBLIC_HDRS} DESTINATION ${INCLUDE_INSTALL_DIR}/${GFLAGS_INCLUDE_DIR})\n  install (\n    FILES \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-config-install.cmake\"\n    RENAME ${PACKAGE_NAME}-config.cmake\n    DESTINATION ${CONFIG_INSTALL_DIR}\n  )\n  install (\n    FILES \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-config-version.cmake\"\n    DESTINATION ${CONFIG_INSTALL_DIR}\n  )\n  install (\n    EXPORT ${EXPORT_NAME}\n    NAMESPACE ${PACKAGE_NAME}::\n    DESTINATION ${CONFIG_INSTALL_DIR}\n  )\n  install (\n    EXPORT ${EXPORT_NAME}\n    FILE ${PACKAGE_NAME}-nonamespace-targets.cmake\n    DESTINATION ${CONFIG_INSTALL_DIR}\n  )\n  if (UNIX)\n    install (PROGRAMS src/gflags_completions.sh DESTINATION ${RUNTIME_INSTALL_DIR})\n  endif ()\nendif ()\n\nif (PKGCONFIG_INSTALL_DIR)\n  configure_file (\"cmake/package.pc.in\" \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}.pc\" @ONLY)\n  install (FILES \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}.pc\" DESTINATION \"${PKGCONFIG_INSTALL_DIR}\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Example Data List - Bash\nDESCRIPTION: This bash script example demonstrates the format for the data.list file used for large dataset training. It contains a list of paths to sliced JSONL files, one path per line.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ndata/list/train.0.jsonl\ndata/list/train.1.jsonl\n...\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements with pip (Shell)\nDESCRIPTION: Installs all required Python dependencies listed in requirements.txt for FunASR and SenseVoice functionality. This step is a prerequisite for running the model's code examples and ensures compatibility. Users must have pip installed and access to the project's requirements file prior to executing this command. No output is expected unless a dependency installation fails.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n\n```\n\n----------------------------------------\n\nTITLE: Configuring gflags Negative Compilation Tests with CMake\nDESCRIPTION: This CMake script configures the build environment for negative tests of the gflags library. It requires CMake minimum version 2.8.12 and mandates a TEST_NAME flag, which is converted to uppercase for defining an associated compiler macro. The script sets project name dynamically from TEST_NAME, includes necessary directories, defines a macro for conditional compilation, creates an executable with the source file gflags_nc.cc, and links it to the gflags library. This setup allows flexible compilation of different negative test scenarios by varying TEST_NAME.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/nc/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required (VERSION 2.8.12 FATAL_ERROR)\n\nif (NOT TEST_NAME)\n  message (FATAL_ERROR \"Missing TEST_NAME CMake flag\")\nendif ()\nstring (TOUPPER ${TEST_NAME} TEST_NAME_UPPER)\n\nproject (gflags_${TEST_NAME})\n\nfind_package (gflags REQUIRED)\ninclude_directories (\"${CMAKE_CURRENT_SOURCE_DIR}/..\")\nadd_definitions (-DTEST_${TEST_NAME_UPPER})\nadd_executable (gflags_${TEST_NAME} gflags_nc.cc)\ntarget_link_libraries(gflags_${TEST_NAME} gflags)\n```\n\n----------------------------------------\n\nTITLE: Example: Update FunASR Decode Thread Count\nDESCRIPTION: This is an example command demonstrating how to update the number of decoding threads used by the service to 32.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --decode_thread_num 32\n```\n\n----------------------------------------\n\nTITLE: Installing Java Environment on Ubuntu\nDESCRIPTION: Provides the command to install OpenJDK 11 JDK on Ubuntu systems, essential for compiling and running the Java WebSocket client.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/java/readme.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\napt-get install openjdk-11-jdk\n```\n\n----------------------------------------\n\nTITLE: Model Registration in FunASR Framework\nDESCRIPTION: Shows how to register a new model class (e.g., SenseVoiceSmall) into FunASR registry using decorator. The registered class must implement __init__, forward, and inference methods. Use this registration to enable model management and inference.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables_zh.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom funasr.register import tables\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n    def __init__(*args, **kwargs):\n        ...\n\n    def forward(self, **kwargs):\n        ...\n\n    def inference(self, data_in, data_lengths=None, key: list = None, tokenizer=None, frontend=None, **kwargs):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Viewing Training Logs with tail Command in Shell\nDESCRIPTION: This snippet shows an example of using the tail command to monitor the training log file (log.txt). The log format includes detailed metrics such as rank (GPU ID), epoch, step, average loss, perplexity, accuracy, learning rate, specific loss components, batch size, timing breakdowns, and GPU memory usage. This enables real-time insight into training progression and performance diagnostics. No special dependencies are required other than access to the log file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ntail log.txt\n[2024-03-21 15:55:52,137][root][INFO] - train, rank: 3, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.327), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.259), ('acc', 0.825), ('loss_pre', 0.04), ('loss', 0.299), ('batch_size', 40)], {'data_load': '0.000', 'forward_time': '0.315', 'backward_time': '0.555', 'optim_time': '0.076', 'total_time': '0.947'}, GPU, memory: usage: 3.830 GB, peak: 18.357 GB, cache: 20.910 GB, cache_peak: 20.910 GB\n[2024-03-21 15:55:52,139][root][INFO] - train, rank: 1, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.334), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.285), ('acc', 0.823), ('loss_pre', 0.046), ('loss', 0.331), ('batch_size', 36)], {'data_load': '0.000', 'forward_time': '0.334', 'backward_time': '0.536', 'optim_time': '0.077', 'total_time': '0.948'}, GPU, memory: usage: 3.943 GB, peak: 18.291 GB, cache: 19.619 GB, cache_peak: 19.619 GB\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Encoder (Python)\nDESCRIPTION: This snippet shows how to register a custom encoder, `SANMEncoder`, as a selectable option within the FunASR framework. Using `ClassChoices`, it maps user-specified encoder names (e.g., 'sanm') to their corresponding classes and enforces that selected encoder options are valid types that implement `AbsEncoder`. This allows users to select from different model architectures.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/build_task.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nencoder_choices = ClassChoices(\n    \"encoder\",\n    classes=dict(\n        conformer=ConformerEncoder,\n        transformer=TransformerEncoder,\n        rnn=RNNEncoder,\n        sanm=SANMEncoder,\n        sanm_chunk_opt=SANMEncoderChunkOpt,\n        data2vec_encoder=Data2VecEncoder,\n        mfcca_enc=MFCCAEncoder,\n    ),\n    type_check=AbsEncoder,\n    default=\"rnn\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR from source (Shell)\nDESCRIPTION: Explains how to clone the FunASR repository from GitHub and install it in editable mode using pip. This method allows development changes to be reflected without re-installing the package. Includes an alternative command using a China mirror. Requires git, pip, and internet access.\nSOURCE: https://github.com/modelscope/funasr/blob/main/benchmarks/benchmark_pipeline_cer.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip install -e ./\n# For the users in China, you could install with the command:\n# pip install -e ./ -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Model Inference from Shell without Configuration\nDESCRIPTION: This shell command performs model inference using a trained model when a configuration.json file is not present in the model directory. It requires specifying the configuration path, configuration name, model parameters, token list, and CMVN file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up TensorBoard Visualization for FunASR\nDESCRIPTION: Command to launch TensorBoard for visualizing training metrics and logs from FunASR training. Allows monitoring of loss curves, accuracy, and other training statistics via a web interface.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir /xxxx/FunASR/examples/industrial_data_pretraining/paraformer/outputs/log/tensorboard\n```\n\n----------------------------------------\n\nTITLE: FunASR Server Startup Script\nDESCRIPTION: This script starts the FunASR 2pass server. It sets various parameters, including model directories for VAD, ASR (online and offline), punctuation, language model, and ITN.  It also allows specifying a hotword file.  The script redirects standard output and standard error to `log.txt` and runs in the background.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server_2pass.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  \\\n  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Generate Scp/Text from Jsonl - Shell\nDESCRIPTION: Optional command using the `jsonl2scp` utility to convert a `jsonl` file back into separate `wav.scp` and `text.txt` files. This can be useful for verification or compatibility. It requires specifying the input jsonl file, the desired output file paths for scp and text, and the corresponding data types.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\njsonl2scp \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_in=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Building the fstcompressscript Library with CMake\nDESCRIPTION: This snippet defines the `fstcompressscript` shared library using `add_library`. It compiles `compress-script.cc` and includes header files located in `../../include/fst/extensions/compress/`. The library is linked against `fstscript`, `fst`, and Zlib (`${ZLIBS}`). The `SOVERSION` property is set for versioning, and the library is installed to the `lib` directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/compress/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB HEADER_FILES ../../include/fst/extensions/compress/*.h)\nmessage(STATUS \"${HEADER_FILES}\")\n\nadd_library(fstcompressscript\n  compress-script.cc\n  ${HEADER_FILES}\n )\n\ntarget_link_libraries(fstcompressscript\n  fstscript\n  fst\n  ${ZLIBS}\n)\nset_target_properties(fstcompressscript PROPERTIES\n  SOVERSION \"${SOVERSION}\"\n)\ninstall(TARGETS fstcompressscript\n  LIBRARY DESTINATION lib\n  ARCHIVE DESTINATION lib\n  RUNTIME DESTINATION lib\n )\n```\n\n----------------------------------------\n\nTITLE: Configure Include Directory Flag (CMake)\nDESCRIPTION: Defines a string flag for the name of the installed include directory relative to CMAKE_INSTALL_PREFIX/include/. It validates the path to ensure it's relative and does not start with a parent directory reference (../).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\ngflags_define (STRING INCLUDE_DIR \"Name of include directory of installed header files relative to CMAKE_INSTALL_PREFIX/include/\" \"${PACKAGE_NAME}\")\ngflags_property (INCLUDE_DIR ADVANCED TRUE)\nfile (TO_CMAKE_PATH \"${INCLUDE_DIR}\" INCLUDE_DIR)\nif (IS_ABSOLUTE INCLUDE_DIR)\n  message (FATAL_ERROR \"[GFLAGS_]INCLUDE_DIR must be a path relative to CMAKE_INSTALL_PREFIX/include/\")\nendif ()\nif (INCLUDE_DIR MATCHES \"^\\\\.\\\\.[/\\\\\\\\]\")\n  message (FATAL_ERROR \"[GFLAGS_]INCLUDE_DIR must not start with parent directory reference (../)\")\nendif ()\nset (GFLAGS_INCLUDE_DIR \"${INCLUDE_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Python Client Example Command\nDESCRIPTION: This command executes the Python client to transcribe an audio file using the FunASR service.  It specifies the host IP, port number, and the input audio file.  The `--mode offline` option is used for offline file transcription. Prerequisites involve installing the necessary Python dependencies and making sure the FunASR service is running.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10097 --mode offline \\\n        --audio_in \"../audio/asr_example.wav\" --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Defining a CMake Function for Building and Installing Modules\nDESCRIPTION: This snippet defines a reusable CMake function named `add_module`. It takes the module name (`_name`) as the first argument and subsequent arguments (`ARGV`) are treated as source files. Inside the function, it creates a library target using `add_library`, links it against the `fst` library if the target was successfully created, sets properties like enabling symbol export on Windows and assigning an IDE folder, and finally sets up installation rules to place the module library into the `lib/fst` directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/lookahead/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction (add_module _name)\n    add_library(${ARGV})\n    if (TARGET ${_name})\n        target_link_libraries(${_name} fst)\n    endif()\n    set_target_properties(${_name} PROPERTIES \n        WINDOWS_EXPORT_ALL_SYMBOLS true\n        FOLDER lookahead/modules\n    )\n\n    install(TARGETS ${_name} LIBRARY DESTINATION lib/fst)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Disabling SSL Verification\nDESCRIPTION: Command to disable SSL verification by setting the relevant parameter, useful in environments with self-signed certificates or during testing.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --ssl 0\n```\n\n----------------------------------------\n\nTITLE: Handling Large-Scale Dataset Training with JSONL File Splitting - Shell\nDESCRIPTION: This snippet shows how to configure training for large datasets by using a split data list file containing multiple smaller JSONL files and defining a data_split_num parameter to control group sizes. This prevents memory overflow in multi-GPU training by loading a limited number of JSONL slices per training group. Users need to prepare a plain text file listing the split JSONL slices and specify data_split_num accordingly. Recommended to maintain data balance during slicing for heterogenous datasets.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ntrain_data=\"/root/data/list/data.list\"\n\nfunasr/bin/train_ds.py \\\n++train_data_set_list=\"${train_data}\" \\\n++dataset_conf.data_split_num=256\n```\n\n----------------------------------------\n\nTITLE: Detailed FunASR Server Start Example with SSL and Hotwords (Shell)\nDESCRIPTION: Provides a detailed example of starting the FunASR server from within the `/workspace/FunASR/runtime` directory. This command explicitly sets model download and usage directories, specifies SSL certificate and key files, and points to a hotword file. It demonstrates using ModelScope model IDs or local paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncd /workspace/FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --certfile  ../../../ssl_key/server.crt \\\n  --keyfile ../../../ssl_key/server.key \\\n  --hotword ../../hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: FunASR Inference (No config.json) - Shell\nDESCRIPTION: This shell script demonstrates how to run inference with FunASR when the configuration.json file is not available. It specifies the configuration file path, model path, and other parameters manually.  It expects the experiment's output directory, the config file name (config.yaml or config.json), the model parameters file, vocabulary path, CMVN file, input audio file, output directory, and the device (CPU or GPU) as input.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Tests for Environment Variable Loading with CMake\nDESCRIPTION: These commands add gflags tests to verify that flags can be loaded from the environment using the --fromenv and --tryfromenv options. The tests check for specific outputs or the absence of errors when attempting to load flags from environment variables.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\n# Also try to load flags from the environment\nadd_gflags_test(fromenv=version      0 \"gflags_unittest\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --fromenv=version)\nadd_gflags_test(tryfromenv=version   0 \"gflags_unittest\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --tryfromenv=version)\nadd_gflags_test(fromenv=help         0 \"PASS\" \"\"  gflags_unittest  --fromenv=help)\nadd_gflags_test(tryfromenv=help      0 \"PASS\" \"\"  gflags_unittest  --tryfromenv=help)\nadd_gflags_test(fromenv=helpfull     1 \"helpfull not found in environment\" \"\"  gflags_unittest  --fromenv=helpfull)\nadd_gflags_test(tryfromenv=helpfull  0 \"PASS\" \"\"  gflags_unittest  --tryfromenv=helpfull)\nadd_gflags_test(tryfromenv=undefok   0 \"PASS\" \"\"  gflags_unittest  --tryfromenv=undefok --foo)\nadd_gflags_test(tryfromenv=weirdo    1 \"unknown command line flag\" \"\"  gflags_unittest  --tryfromenv=weirdo)\nadd_gflags_test(tryfromenv-multiple  0 \"gflags_unittest\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --tryfromenv=test_bool,version,unused_bool)\nadd_gflags_test(fromenv=test_bool    1 \"not found in environment\" \"\"  gflags_unittest  --fromenv=test_bool)\nadd_gflags_test(fromenv=test_bool-ok 1 \"unknown command line flag\" \"\"  gflags_unittest  --fromenv=test_bool,ok)\n# Here, the --version overrides the fromenv\nadd_gflags_test(version-overrides-fromenv 0 \"gflags_unittest\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --fromenv=test_bool,version,ok)\n```\n\n----------------------------------------\n\nTITLE: Accessing HTML5 Client\nDESCRIPTION: This code snippet shows the URL to access the HTML5 client after starting the HTML5 server. It points to the index.html file within the static directory of the server. It assumes the server is running on localhost (127.0.0.1) and port 1337.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nhttps://127.0.0.1:1337/static/index.html\n```\n\n----------------------------------------\n\nTITLE: Downloading ffmpeg using shell script\nDESCRIPTION: Downloads the ffmpeg multimedia framework using a provided script in the third_party directory. This dependency is necessary for audio processing in the FunASR project.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/requirements_install.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbash third_party/download_ffmpeg.sh\n```\n\n----------------------------------------\n\nTITLE: Sample Tail of Training Log Showing Metrics and Resource Usage - Shell\nDESCRIPTION: This example demonstrates the format of a training log snippet, showing timestamps, GPU rank, epoch and step information, loss, perplexity, accuracy metrics, learning rate, batch size, per-step timing information, and GPU memory usage statistics. This log is critical for monitoring model training progress and system resource utilization.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n[2024-03-21 15:55:52,137][root][INFO] - train, rank: 3, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.327), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.259), ('acc', 0.825), ('loss_pre', 0.04), ('loss', 0.299), ('batch_size', 40)], {'data_load': '0.000', 'forward_time': '0.315', 'backward_time': '0.555', 'optim_time': '0.076', 'total_time': '0.947'}, GPU, memory: usage: 3.830 GB, peak: 18.357 GB, cache: 20.910 GB, cache_peak: 20.910 GB\n[2024-03-21 15:55:52,139][root][INFO] - train, rank: 1, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.334), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.285), ('acc', 0.823), ('loss_pre', 0.046), ('loss', 0.331), ('batch_size', 36)], {'data_load': '0.000', 'forward_time': '0.334', 'backward_time': '0.536', 'optim_time': '0.077', 'total_time': '0.948'}, GPU, memory: usage: 3.943 GB, peak: 18.291 GB, cache: 19.619 GB, cache_peak: 19.619 GB\n```\n\n----------------------------------------\n\nTITLE: Markdown Table for Chinese ASR Competition Results\nDESCRIPTION: A markdown table displaying the final ranking results of a Chinese ASR competition. The table shows team rankings, team names, performance on two different sub-tracks measured in cp-CER (%), and a column for associated papers.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/m2met2/Challenge_result.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Rank &nbsp; &nbsp; | Team Name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | Sub-track1 &nbsp; &nbsp; | Sub-track2 &nbsp; &nbsp; | paper |\n|------|----------------------|------------|------------|------------------------|\n| 1    | Ximalaya Speech Team | 11.27      | 11.27      |                        |\n| 2    | Â∞èÈ©¨Ëææ                | 18.64      | 18.64      |                        |\n| 3    | AIzyzx               | 22.83      | 22.83      |                        |\n| 4    | AsrSpeeder           | /          | 23.51      |                        |\n| 5    | zyxlhz               | 24.82      | 24.82      |                        |\n| 6    | CMCAI                | 26.11      | /          |                        |\n| 7    | Volcspeech           | 34.21      | 34.21      |                        |\n| 8    | Èâ¥ÂæÄÁü•Êù•              | 40.14      | 40.14      |                        |\n| 9    | baseline             | 41.55      | 41.55      |                        |\n| 10   | DAICT                | 41.64      |            |                        |\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Model Inference from Command Line - Shell\nDESCRIPTION: This snippet demonstrates how to perform model inference on an audio file using FunASR from the shell. It runs the 'funasr' command with model, vad, punctuation, and input audio specified as command-line arguments. Required dependencies include a working FunASR installation and access to the specified models. The expected input is a WAV file, and the tool outputs recognized speech to standard output or files as configured. Changes to model paths or additional options can further customize behavior.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n----------------------------------------\n\nTITLE: Test ONNX Model - Python\nDESCRIPTION: This Python code tests an exported ONNX model using the `funasr_onnx` library. It loads the ONNX model, specifies the input WAV file path, and runs the model. It requires the `funasr-onnx` package to be installed. The code instantiates a `Paraformer` model from `funasr_onnx`, providing the model directory, batch size and quantization setting. The example also shows how to input audio data and print the results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Using CT-Transformer for Punctuation Restoration in Python\nDESCRIPTION: Example code for using the CT-Transformer model for offline punctuation restoration. Demonstrates loading a pre-trained model from modelscope and adding punctuation to raw ASR text, with details about available parameters and expected inputs/outputs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/onnxruntime/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr_onnx import CT_Transformer\n\nmodel_dir = \"damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch\"\nmodel = CT_Transformer(model_dir)\n\ntext_in=\"Ë∑®Â¢ÉÊ≤≥ÊµÅÊòØÂÖªËÇ≤Ê≤øÂ≤∏‰∫∫Ê∞ëÁöÑÁîüÂëΩ‰πãÊ∫êÈïøÊúü‰ª•Êù•‰∏∫Â∏ÆÂä©‰∏ãÊ∏∏Âú∞Âå∫Èò≤ÁÅæÂáèÁÅæ‰∏≠ÊñπÊäÄÊúØ‰∫∫ÂëòÂú®‰∏äÊ∏∏Âú∞Âå∫ÊûÅ‰∏∫ÊÅ∂Âä£ÁöÑËá™ÁÑ∂Êù°‰ª∂‰∏ãÂÖãÊúçÂ∑®Â§ßÂõ∞ÈöæÁîöËá≥ÂÜíÁùÄÁîüÂëΩÂç±Èô©ÂêëÂç∞ÊñπÊèê‰æõÊ±õÊúüÊ∞¥ÊñáËµÑÊñôÂ§ÑÁêÜÁ¥ßÊÄ•‰∫ã‰ª∂‰∏≠ÊñπÈáçËßÜÂç∞ÊñπÂú®Ë∑®Â¢ÉÊ≤≥ÊµÅÈóÆÈ¢ò‰∏äÁöÑÂÖ≥ÂàáÊÑøÊÑèËøõ‰∏ÄÊ≠•ÂÆåÂñÑÂèåÊñπËÅîÂêàÂ∑•‰ΩúÊú∫Âà∂Âá°ÊòØ‰∏≠ÊñπËÉΩÂÅöÁöÑÊàë‰ª¨ÈÉΩ‰ºöÂéªÂÅöËÄå‰∏î‰ºöÂÅöÂæóÊõ¥Â•ΩÊàëËØ∑Âç∞Â∫¶ÊúãÂèã‰ª¨ÊîæÂøÉ‰∏≠ÂõΩÂú®‰∏äÊ∏∏ÁöÑ‰ªª‰ΩïÂºÄÂèëÂà©Áî®ÈÉΩ‰ºöÁªèËøáÁßëÂ≠¶ËßÑÂàíÂíåËÆ∫ËØÅÂÖºÈ°æ‰∏ä‰∏ãÊ∏∏ÁöÑÂà©Áõä\"\nresult = model(text_in)\nprint(result[0])\n```\n\n----------------------------------------\n\nTITLE: Real-Time ASR\nDESCRIPTION: This snippet demonstrates real-time ASR using the `AutoModel` with streaming configurations. It shows how to process audio in chunks and use caching to maintain context for accurate transcription, utilizing parameters such as `chunk_size`, `encoder_chunk_look_back`, and `decoder_chunk_look_back`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: Defining AutoModel Instance with Parameters - Python\nDESCRIPTION: This snippet outlines the constructor API for creating an AutoModel object in FunASR with parameter descriptions. It can specify model source, device, batch size, threading, download hub, and any extra keyword arguments used in model configuration. Users should ensure their environment supports the needed devices and all referenced models are accessible. Inputs and outputs depend on the model type and function calls that follow.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: FunASR Server Start Command\nDESCRIPTION: This shell command starts the FunASR service with specific configurations.  It sets model download directory, model paths for ASR, VAD, and PUNC models, and specifies SSL certificate and key file locations. The command redirects standard output and standard error to a log file, and runs the service in the background. This command requires the `run_server.sh` script to be present and correctly configured.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncd /workspace/FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --model-dir damo/speech_paraformer-large_asr_nat-en-16k-common-vocab10020-onnx \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --certfile  ../../../ssl_key/server.crt \\\n  --keyfile ../../../ssl_key/server.key > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Testing FunASR ONNX Model Inference (Python)\nDESCRIPTION: Uses the `funasr_onnx` library to load and perform inference with a pre-exported ONNX model. It initializes a specific model class (e.g., `Paraformer`), specifying the model directory, batch size, and quantization status. The model is then called with a list of WAV file paths for inference. Requires the `funasr-onnx` package (`pip install -U funasr-onnx`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# pip install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Execute FunASR Python Client for Offline Transcription\nDESCRIPTION: This command runs the Python client script `funasr_wss_client.py` to perform offline file transcription. It connects to the specified host and port, sets the mode to `offline`, and provides the path to the input audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"../audio/asr_example.wav\"\n```\n\n----------------------------------------\n\nTITLE: Including Directories with CMake\nDESCRIPTION: These commands add include directories to the compiler's search path. This allows the compiler to find header files required for the gflags library. The directories include the current source directory, the gflags source directory, and the gflags binary directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories (\"${CMAKE_CURRENT_SOURCE_DIR}\")\ninclude_directories (\"${gflags_SOURCE_DIR}/src\")\ninclude_directories (\"${gflags_BINARY_DIR}/include\")\ninclude_directories (\"${gflags_BINARY_DIR}/include/gflags\")\n```\n\n----------------------------------------\n\nTITLE: Defining add_module Function for FST Modules in CMake\nDESCRIPTION: Creates a custom CMake function that builds and configures FST module libraries. It links each module with the 'fst' library, sets Windows export properties, and configures installation to the lib/fst directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/const/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction (add_module _name)\n    add_library(${ARGV})\n    if (TARGET ${_name})\n        target_link_libraries(${_name} fst)\n        set_target_properties(${_name} PROPERTIES \n            WINDOWS_EXPORT_ALL_SYMBOLS true\n            FOLDER constant/modules\n        )\n    endif()\n\n    install(TARGETS ${_name} LIBRARY DESTINATION lib/fst)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Installing Core Dependencies (ModelScope/FunASR) - Shell\nDESCRIPTION: Installs the core ModelScope and FunASR libraries using the pip package manager. These libraries are fundamental prerequisites for running the CPU benchmarks.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U modelscope funasr\n```\n\n----------------------------------------\n\nTITLE: Fetching and Including WebSocket Dependencies - CMake\nDESCRIPTION: Conditionally fetches and includes external libraries required for the WebSocket server functionality using the `FetchContent` module. It downloads websocketpp, Asio, and nlohmann/json if they are not already present and includes their respective header directories.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_WEBSOCKET)\n  # cmake_policy(SET CMP0135 NEW)\n  include(FetchContent)\n\n  if(NOT EXISTS ${PROJECT_SOURCE_DIR}/third_party/websocket/websocketpp )\n    FetchContent_Declare(websocketpp\n    GIT_REPOSITORY https://github.com/zaphoyd/websocketpp.git\n      GIT_TAG 0.8.2\n      SOURCE_DIR ${PROJECT_SOURCE_DIR}/third_party/websocket\n      )\n    \n    FetchContent_MakeAvailable(websocketpp)\n  endif()\n  include_directories(${PROJECT_SOURCE_DIR}/third_party/websocket)\n   \n  if(NOT EXISTS ${PROJECT_SOURCE_DIR}/third_party/asio/asio )\n    FetchContent_Declare(asio\n      URL   https://github.com/chriskohlhoff/asio/archive/refs/tags/asio-1-24-0.tar.gz\n    SOURCE_DIR ${PROJECT_SOURCE_DIR}/third_party/asio\n    )\n    \n    FetchContent_MakeAvailable(asio)\n  endif()\n  include_directories(${PROJECT_SOURCE_DIR}/third_party/asio/asio/include)\n \n  if(NOT EXISTS ${PROJECT_SOURCE_DIR}/third_party/json/ChangeLog.md )\n    FetchContent_Declare(json\n      URL   https://github.com/nlohmann/json/archive/refs/tags/v3.11.2.tar.gz\n    SOURCE_DIR ${PROJECT_SOURCE_DIR}/third_party/json\n    )\n    \n    FetchContent_MakeAvailable(json)\n  endif()\n  include_directories(${PROJECT_SOURCE_DIR}/third_party/json/include)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Channel Speech Recognition with FunASR\nDESCRIPTION: This snippet showcases how to perform multi-channel speech recognition using FunASR, particularly with the Multi-Frame Cross-Channel Attention (MFCCA) model. It requires multi-channel audio input and dependencies include the FunASR library with multi-channel modules. The code processes multi-channel audio to improve recognition accuracy in multi-party scenarios.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Load multi-channel model\nmodel = funasr.load_model('path/to/multi_channel_model')\n\n# Audio input with multiple channels\naudio_channels = ['channel1.wav', 'channel2.wav']\nresult = funasr.transcribe_multi_channel(audio_channels, model)\nprint('Multi-talker recognition result:', result)\n```\n\n----------------------------------------\n\nTITLE: Determine Integer Types Format (CMake)\nDESCRIPTION: Defines and automatically detects the format string for 32-bit integer types (e.g., `uint32_t`, `u_int32_t`, `__int32`). It checks for available types using `check_type_size` and sets the `INTTYPES_FORMAT` variable accordingly. It validates the selected or detected format and sets corresponding flag variables.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\ngflags_define (STRING INTTYPES_FORMAT \"Format of integer types: \\\"C99\\\" (uint32_t), \\\"BSD\\\" (u_int32_t), \\\"VC7\\\" (__int32)\" \"\")\ngflags_property (INTTYPES_FORMAT STRINGS \"C99;BSD;VC7\")\ngflags_property (INTTYPES_FORMAT ADVANCED TRUE)\nif (NOT INTTYPES_FORMAT)\n  set (TYPES uint32_t u_int32_t)\n  if (MSVC)\n    list (INSERT TYPES 0 __int32)\n  endif ()\n  foreach (type IN LISTS TYPES)\n    check_type_size (${type} ${type} LANGUAGE CXX)\n    if (HAVE_${type})\n      break ()\n    endif ()\n  endforeach ()\n  if (HAVE_uint32_t)\n    gflags_set (INTTYPES_FORMAT C99)\n  elseif (HAVE_u_int32_t)\n    gflags_set (INTTYPES_FORMAT BSD)\n  elseif (HAVE___int32)\n    gflags_set (INTTYPES_FORMAT VC7)\n  else ()\n    gflags_property (INTTYPES_FORMAT ADVANCED FALSE)\n    message (FATAL_ERROR \"Do not know how to define a 32-bit integer quantity on your system!\"\n                         \" Neither uint32_t, u_int32_t, nor __int32 seem to be available.\"\n                         \" Set [GFLAGS_]INTTYPES_FORMAT to either C99, BSD, or VC7 and try again.\")\n  endif ()\nendif ()\n# use of special characters in strings to circumvent bug #0008226\nif (\"^${INTTYPES_FORMAT}$\" STREQUAL \"^WIN$\")\n  gflags_set (INTTYPES_FORMAT VC7)\nendif ()\nif (NOT INTTYPES_FORMAT MATCHES \"^(C99|BSD|VC7)$\")\n  message (FATAL_ERROR \"Invalid value for [GFLAGS_]INTTYPES_FORMAT! Choose one of \\\"C99\\\", \\\"BSD\\\", or \\\"VC7\\\"\")\nendif ()\nset (GFLAGS_INTTYPES_FORMAT \"${INTTYPES_FORMAT}\")\nset (GFLAGS_INTTYPES_FORMAT_C99 0)\nset (GFLAGS_INTTYPES_FORMAT_BSD 0)\nset (GFLAGS_INTTYPES_FORMAT_VC7 0)\nset (\"GFLAGS_INTTYPES_FORMAT_${INTTYPES_FORMAT}\" 1)\n```\n\n----------------------------------------\n\nTITLE: Configuring FunASR Runtime (Windows) - Shell\nDESCRIPTION: Clones the FunASR repository, navigates to the ONNX Runtime directory, creates a build subdirectory, and runs CMake to configure the project for Windows. The CMake command specifies the paths to the previously downloaded and extracted ONNX Runtime and FFmpeg dependencies, preparing the build files (e.g., .sln) for use with Visual Studio.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/readme.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git\ncd FunASR/runtime/onnxruntime\nmkdir build\ncd build\ncmake ../ -D FFMPEG_DIR=d:/ffmpeg-master-latest-win64-gpl-shared -D ONNXRUNTIME_DIR=d:/onnxruntime-win-x64-1.16.1\n```\n\n----------------------------------------\n\nTITLE: Detect Operating System (CMake)\nDESCRIPTION: Determines if the build is happening on Windows, excluding Cygwin environments. Sets the `OS_WINDOWS` variable to 1 if true, 0 otherwise.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif (WIN32 AND NOT CYGWIN)\n  set (OS_WINDOWS 1)\nelse ()\n  set (OS_WINDOWS 0)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Python Client Invocation for Offline Transcription\nDESCRIPTION: This command runs the Python WebSocket client to connect to the locally deployed FunASR transcription service in offline mode. It provides the host, port, operational mode, and specifies an input audio file of various supported formats. This snippet demonstrates how to test or use the transcription service from Python. Optional parameters such as thread count, SSL verification, hotword file, and ITN usage can be added as command line flags.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"../audio/asr_example.wav\"\n```\n\n----------------------------------------\n\nTITLE: Conditionally Building fstcompress and fstrandmod Executables with CMake\nDESCRIPTION: This snippet conditionally builds the `fstcompress` and `fstrandmod` executables if the `HAVE_BIN` variable is true. It uses `add_executable` for each, linking them against `fstcompressscript`, `fstscript`, `fst`, Zlib (`${ZLIBS}`), and dynamic linking libraries (`${CMAKE_DL_LIBS}`). If built, the executables are installed to the `bin` directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/compress/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_BIN)\n  add_executable(fstcompress\n    fstcompress.cc)\n\n  target_link_libraries(fstcompress\n    fstcompressscript\n    fstscript\n    fst\n    ${ZLIBS}\n    ${CMAKE_DL_LIBS}\n   )\n\n  add_executable(fstrandmod\n    fstrandmod.cc\n  )\n\n  target_link_libraries(fstrandmod\n    fstcompressscript\n    fstscript\n    fst\n    ${ZLIBS}\n    ${CMAKE_DL_LIBS}\n  )\n\n  install(TARGETS fstcompress fstrandmod\n\t        LIBRARY DESTINATION bin\n\t\t\tARCHIVE DESTINATION bin\n            RUNTIME DESTINATION bin\n\t\t)\nendif(HAVE_BIN)\n```\n\n----------------------------------------\n\nTITLE: Configure and Add Uninstall Target in CMake\nDESCRIPTION: Conditionally configures an uninstall script (`cmake_uninstall.cmake.in`) and adds a custom target named `uninstall`. This target, when invoked (e.g., `make uninstall`), executes the configured script using `cmake -P` to remove the installed files. This is only done if the project is not being built as a subproject (`GFLAGS_IS_SUBPROJECT` is false) and the `uninstall` target doesn't already exist.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_42\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT GFLAGS_IS_SUBPROJECT AND NOT TARGET uninstall)\n  configure_file (\n    \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/cmake_uninstall.cmake.in\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/cmake_uninstall.cmake\" @ONLY\n  )\n  add_custom_target(uninstall COMMAND ${CMAKE_COMMAND} -P \"${CMAKE_CURRENT_BINARY_DIR}/cmake_uninstall.cmake\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Set Glog DLL Export Symbol\nDESCRIPTION: Sets the `DEFINE_SYMBOL` property for the `glog` target to `GOOGLE_GLOG_IS_A_DLL`. This definition is typically used within the library's source code to handle symbol visibility correctly when building a shared library (DLL on Windows).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties (glog PROPERTIES DEFINE_SYMBOL GOOGLE_GLOG_IS_A_DLL)\n```\n\n----------------------------------------\n\nTITLE: Convert JSONL to SCP - Shell\nDESCRIPTION: This shell command converts a JSONL file back into wav.scp and text.txt files. It is the reverse operation of scp2jsonl and allows you to regenerate the original data files from the JSONL format. The jsonl2scp tool is used for this conversion.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\njsonl2scp \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_in=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Registering Custom FunASR Model Class (Python)\nDESCRIPTION: Demonstrates how to register a new model class with the FunASR framework's registry using the `@tables.register` decorator. The decorator takes the registry type (\"model_classes\") and the desired name (\"SenseVoiceSmall\") as arguments. The registered class should typically inherit from `torch.nn.Module` and implement `__init__`, `forward`, and `inference` methods. Requires the FunASR Python package and PyTorch.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n  def __init__(*args, **kwargs):\n    ...\n\n  def forward(\n      self,\n      **kwargs,\n  ):  \n\n  def inference(\n      self,\n      data_in,\n      data_lengths=None,\n      key: list = None,\n      tokenizer=None,\n      frontend=None,\n      **kwargs,\n  ):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating Constant FST Modules with Different Bit Widths\nDESCRIPTION: Builds three separate FST module libraries for 8-bit, 16-bit, and 64-bit constant FSTs. Each module is created using the previously defined add_module function.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/const/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_module(const8-fst MODULE const8-fst.cc)\n\nadd_module(const16-fst MODULE const16-fst.cc)\n\nadd_module(const64-fst MODULE const64-fst.cc)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Torchaudio via pip (Shell)\nDESCRIPTION: Command to install the PyTorch and Torchaudio libraries using pip3. Notes that users with CUDA environments should install versions matching their CUDA version by consulting the official PyTorch documentation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip3 install torch torchaudio\n```\n\n----------------------------------------\n\nTITLE: Launching FunASR Server, SSL disabled (2pass mode)\nDESCRIPTION: This command launches the FunASR server in 2pass mode. This is a variation of the previous example. The crucial difference is the `--certfile 0` flag, which disables SSL encryption for the communication.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# If you want to close sslÔºåplease addÔºö--certfile 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Real-time Mode Client Request in FunASR WebSocket Protocol\nDESCRIPTION: JSON message format for the initial client request in real-time (2pass) recognition mode. Includes parameters for mode, streaming configuration, latency settings, and optional hotwords enhancement.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol_zh.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"2pass\", \"wav_name\": \"wav_name\", \"is_speaking\": True, \"wav_format\":\"pcm\", \"chunk_size\":[5,10,5], \"hotwords\":\"{\\\"ÈòøÈáåÂ∑¥Â∑¥\\\":20,\\\"ÈÄö‰πâÂÆûÈ™åÂÆ§\\\":30}\",\"itn\":True}\n```\n\n----------------------------------------\n\nTITLE: Updating Model or Configuration and Restarting FunASR Service\nDESCRIPTION: This set of shell commands shows how to update ASR/VAD/PUNC models by specifying model IDs or local paths and how to update service parameters such as ports, thread numbers, workspace directories, and SSL flags. After updating, the service restarts with new configurations applied, allowing flexible tuning or model replacement without full reinstall.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update [--asr_model | --vad_model | --punc_model] <model_id or local model path>\n\ne.g\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update --asr_model damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update [--host_port | --docker_port] <port number>\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update [--decode_thread_num | --io_thread_num] <the number of threads>\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update [--workspace] <workspace in local>\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update [--ssl] <0: close SSL; 1: open SSL, default:1>\n\ne.g\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update --decode_thread_num 32\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh update --workspace /root/funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: Exporting yaml-cpp Target for CMake Package System\nDESCRIPTION: This code exports the `yaml-cpp` target information to a CMake file (`yaml-cpp-targets.cmake`) in the build directory. This allows other CMake projects to find and use the built library using `find_package(yaml-cpp)`. The `export(PACKAGE yaml-cpp)` command registers the package name.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nexport(\n    TARGETS yaml-cpp\n    FILE \"${PROJECT_BINARY_DIR}/yaml-cpp-targets.cmake\")\nexport(PACKAGE yaml-cpp)\nset(EXPORT_TARGETS yaml-cpp CACHE INTERNAL \"export targets\")\n```\n\n----------------------------------------\n\nTITLE: Initialize AliFsmnVad Model C#\nDESCRIPTION: This C# code snippet demonstrates how to initialize the AliFsmnVad model by providing the paths to the model, configuration, and MVN files. It also sets the batch size for decoding.  The model path is dynamically constructed using AppDomain.CurrentDomain.BaseDirectory, which gets the application base path. The AliFsmnVad object is then instantiated with these parameters.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/csharp/AliFsmnVad/README.md#_snippet_0\n\nLANGUAGE: C#\nCODE:\n```\nstring applicationBase = AppDomain.CurrentDomain.BaseDirectory;\nstring modelFilePath = applicationBase + \"./speech_fsmn_vad_zh-cn-16k-common-pytorch/model.onnx\";\nstring configFilePath = applicationBase + \"./speech_fsmn_vad_zh-cn-16k-common-pytorch/vad.yaml\";\nstring mvnFilePath = applicationBase + \"./speech_fsmn_vad_zh-cn-16k-common-pytorch/vad.mvn\";\nint batchSize = 2;//ÊâπÈáèËß£Á†Å\nAliFsmnVad aliFsmnVad = new AliFsmnVad(modelFilePath, configFilePath, mvnFilePath, batchSize);\n```\n\n----------------------------------------\n\nTITLE: Downloading ONNX Runtime (Linux) - Shell\nDESCRIPTION: Downloads the ONNX Runtime dependency package for Linux (x64, version 1.14.0) from a specified URL and extracts its contents using tar. This provides the necessary libraries and headers for linking during the FunASR build process on Linux.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/readme.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/onnxruntime-linux-x64-1.14.0.tgz\ntar -zxvf onnxruntime-linux-x64-1.14.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Setting FST N-gram Library Properties\nDESCRIPTION: This snippet uses `set_target_properties` to set the `SOVERSION` and `FOLDER` properties for the `fstngram` library. `SOVERSION` defines the shared object version number, and `FOLDER` specifies the folder in the IDE or build system where the library will be organized.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(fstngram PROPERTIES\n  SOVERSION \"${SOVERSION}\"\n  FOLDER ngram\n)\n```\n\n----------------------------------------\n\nTITLE: Update FunASR Service Thread Count\nDESCRIPTION: This command updates the number of decode or I/O threads used by the FunASR service. The service will be restarted for the change to take effect.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update [--decode_thread_num | --io_thread_num] <the number of threads>\n```\n\n----------------------------------------\n\nTITLE: Check For POSIX Threads (CMake)\nDESCRIPTION: Checks for the availability of POSIX threads (`pthread_t`) if the multi-threaded gflags library (`BUILD_gflags_LIB`) is enabled. It uses CMake's `find_package(Threads)` and checks for the `pthread_rwlock_t` type size. If not found on UNIX-like systems, it issues a fatal error.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_gflags_LIB)\n  set (CMAKE_THREAD_PREFER_PTHREAD TRUE)\n  find_package (Threads)\n  if (Threads_FOUND AND CMAKE_USE_PTHREADS_INIT)\n    set (HAVE_PTHREAD 1)\n    check_type_size (pthread_rwlock_t RWLOCK LANGUAGE CXX)\n  else ()\n    set (HAVE_PTHREAD 0)\n  endif ()\n  if (UNIX AND NOT HAVE_PTHREAD)\n    if (CMAKE_HAVE_PTHREAD_H)\n      set (what \"library\")\n    else ()\n      set (what \".h file\")\n    endif ()\n    message (FATAL_ERROR \"Could not find pthread${what}. Check the log file\"\n                         \"\\n\\t${CMAKE_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/CMakeError.log\"\n                         \"\\nor disable the build of the multi-threaded gflags library (BUILD_gflags_LIB=OFF).\")\n  endif ()\nelse ()\n  set (HAVE_PTHREAD 0)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Performing Non-Streaming ASR with VAD and Punctuation (Python)\nDESCRIPTION: This Python snippet demonstrates non-streaming ASR using AutoModel with integrated Voice Activity Detection (VAD) and Punctuation Restoration models. It shows how to configure VAD parameters via 'vad_kwargs' like 'max_single_segment_time' and how to use dynamic batching parameters 'batch_size_s' and 'batch_size_threshold_s' for handling long audio inputs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Linking N-gram FST Module Dependency\nDESCRIPTION: This snippet uses `target_link_libraries` to link the `ngram_fst` module library with the `fst` library, indicating its dependency on FST functionality.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(ngram_fst\n    fst\n)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR from source code\nDESCRIPTION: Commands to clone the FunASR repository from GitHub and install it in development mode, with an alternative command for users in mainland China using a local mirror.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation_zh.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n# ÂØπ‰∫é‰∏≠ÂõΩÂ§ßÈôÜÁî®Êà∑ÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ËøõË°åÂÆâË£ÖÔºö\n# pip3 install -e ./ -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Update FunASR Service SSL Setting\nDESCRIPTION: This command updates the SSL certificate verification setting for the FunASR service. Set to 0 to close SSL, 1 to open (default). The service will be restarted.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update [--ssl] <0: close SSL; 1: open SSL, default:1>\n```\n\n----------------------------------------\n\nTITLE: FunASR Training Script Arguments\nDESCRIPTION: This snippet presents example arguments to the `train_ds.py` script in FunASR, used for training models. It includes paths to model, train, and validation data, along with various training configuration options, such as batch size, learning rate, and output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train_ds.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: Using Attention-Based Language Models in Speech Recognition\nDESCRIPTION: This code example shows how to integrate the Attention Is All You Need transformer model for language modeling in speech recognition pipelines. It involves loading the pre-trained transformer model, processing input text or transcripts, and generating contextualized language representations to improve recognition accuracy. It relies on the NEURIPS 2017 paper's architecture principles.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/papers.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport funasr\n\n# Load transformer language model\nlm = funasr.load_language_model('path/to/attention_transformer')\n# Use language model during decoding\nhypothesis = funasr.decode_with_lm('audio.wav', model, language_model=lm)\nprint('Decoded hypothesis with LM:', hypothesis)\n```\n\n----------------------------------------\n\nTITLE: Detailed Training Command\nDESCRIPTION: This command demonstrates how to train a model using the `train_ds.py` script with specific parameters. It includes configurations for the model, training data, validation data, batch size, number of workers, training epochs, logging intervals, learning rate, and output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nfunasr/bin/train_ds.py \\\n++model=\"${model_name_or_model_dir}\" \\\n++train_data_set_list=\"${train_data}\" \\\n++valid_data_set_list=\"${val_data}\" \\\n++dataset_conf.batch_size=20000 \\\n++dataset_conf.batch_type=\"token\" \\\n++dataset_conf.num_workers=4 \\\n++train_conf.max_epoch=50 \\\n++train_conf.log_interval=1 \\\n++train_conf.resume=false \\\n++train_conf.validate_interval=2000 \\\n++train_conf.save_checkpoint_interval=2000 \\\n++train_conf.keep_nbest_models=20 \\\n++train_conf.avg_nbest_model=10 \\\n++optim_conf.lr=0.0002 \\\n++output_dir=\"${output_dir}\" &> ${log_file}\n```\n\n----------------------------------------\n\nTITLE: Python Client Parameters\nDESCRIPTION: This text describes the parameters for the Python client script.  It lists the options such as host, port, mode, audio input, thread number, SSL setting, hotword file, and ITN usage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n--host ‰∏∫FunASR runtime-SDKÊúçÂä°ÈÉ®ÁΩ≤Êú∫Âô®ipÔºåÈªòËÆ§‰∏∫Êú¨Êú∫ipÔºà127.0.0.1ÔºâÔºåÂ¶ÇÊûúclient‰∏éÊúçÂä°‰∏çÂú®Âêå‰∏ÄÂè∞ÊúçÂä°Âô®Ôºå\n       ÈúÄË¶ÅÊîπ‰∏∫ÈÉ®ÁΩ≤Êú∫Âô®ip\n--port 10095 ÈÉ®ÁΩ≤Á´ØÂè£Âè∑\n--mode offlineË°®Á§∫Á¶ªÁ∫øÊñá‰ª∂ËΩ¨ÂÜô\n--audio_in ÈúÄË¶ÅËøõË°åËΩ¨ÂÜôÁöÑÈü≥È¢ëÊñá‰ª∂ÔºåÊîØÊåÅÊñá‰ª∂Ë∑ØÂæÑÔºåÊñá‰ª∂ÂàóË°®wav.scp\n--thread_num ËÆæÁΩÆÂπ∂ÂèëÂèëÈÄÅÁ∫øÁ®ãÊï∞ÔºåÈªòËÆ§‰∏∫1\n--ssl ËÆæÁΩÆÊòØÂê¶ÂºÄÂêØsslËØÅ‰π¶Ê†°È™åÔºåÈªòËÆ§1ÂºÄÂêØÔºåËÆæÁΩÆ‰∏∫0ÂÖ≥Èó≠\n--hotword ÁÉ≠ËØçÊñá‰ª∂ÔºåÊØèË°å‰∏Ä‰∏™ÁÉ≠ËØçÔºåÊ†ºÂºè(ÁÉ≠ËØç ÊùÉÈáç)ÔºöÈòøÈáåÂ∑¥Â∑¥ 20\n--use_itn ËÆæÁΩÆÊòØÂê¶‰ΩøÁî®itnÔºåÈªòËÆ§1ÂºÄÂêØÔºåËÆæÁΩÆ‰∏∫0ÂÖ≥Èó≠\n```\n\n----------------------------------------\n\nTITLE: Set Glog Public Headers Property\nDESCRIPTION: Sets the `PUBLIC_HEADER` property of the `glog` target to the list of files defined in `GLOG_PUBLIC_H`. This property is used by installation commands to know which headers are part of the public interface.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties (glog PROPERTIES PUBLIC_HEADER \"${GLOG_PUBLIC_H}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Paraformer-zh for Non-Streaming ASR (Python)\nDESCRIPTION: Python code snippet demonstrating how to use the multi-functional paraformer-zh model for non-streaming ASR with FunASR. It initializes the AutoModel specifying the main ASR model and optionally enabling VAD (fsmn-vad) and punctuation (ct-punc) models. The `generate` method processes an input WAV file using dynamic batching (`batch_size_s`) and supports hotword injection.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  vad_model=\"fsmn-vad\",  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\", \n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n                     batch_size_s=300, \n                     hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Update FunASR Service Workspace Path\nDESCRIPTION: This command updates the local workspace directory used by the FunASR service. The service will be restarted for the change to take effect.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update [--workspace] <workspace in local>\n```\n\n----------------------------------------\n\nTITLE: Running funasr-onnx-offline-rtf with English Models\nDESCRIPTION: This shell script executes the `funasr-onnx-offline-rtf` tool with parameters specific to an English ASR model. It configures model directories for Paraformer, specifies the quantization setting, and sets VAD and punctuation models. The script then specifies an input wav file, and the number of threads to use.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-onnx-offline-rtf \\\n    --model-dir    ./damo/speech_paraformer-large_asr_nat-en-16k-common-vocab10020-onnx \\\n    --quantize  true \\\n    --vad-dir   ./damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n    --punc-dir  ./damo/punc_ct-transformer_zh-cn-common-vocab272727-onnx \\\n    --wav-path     ./librispeech_test_clean.scp  \\\n    --thread-num 32\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Target Properties in CMake\nDESCRIPTION: This code conditionally sets target properties based on the build platform. For iPhone builds (`IF(IPHONE)`), it sets the deployment target. For MSVC builds (`IF(MSVC)`) that are not shared libraries (`NOT BUILD_SHARED_LIBS`), it adjusts the debug and release postfixes for the library file names.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(IPHONE)\n\tset_target_properties(yaml-cpp PROPERTIES\n\t\tXCODE_ATTRIBUTE_IPHONEOS_DEPLOYMENT_TARGET \"3.0\"\n\t)\nendif()\n\nif(MSVC)\n\tif(NOT BUILD_SHARED_LIBS)\n\t\t# correct library names\n\t\tset_target_properties(yaml-cpp PROPERTIES\n\t\t\tDEBUG_POSTFIX \"${LIB_TARGET_SUFFIX}d\"\n\t\t\tRELEASE_POSTFIX \"${LIB_TARGET_SUFFIX}\"\n\t\t\tMINSIZEREL_POSTFIX \"${LIB_TARGET_SUFFIX}\"\n\t\t\tRELWITHDEBINFO_POSTFIX \"${LIB_TARGET_SUFFIX}\"\n\t\t)\n\tendif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running the FunASR HTTP Server Binary - Shell\nDESCRIPTION: This shell snippet runs the compiled FunASR HTTP server executable, providing arguments to specify model, VAD, ITN, punctuation directories, thread counts, and port via command-line flags. All major configuration directories and numeric parameters must be set to real values for your deployment. The command launches the HTTP server, listening for incoming transcription requests.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-http-server  \\\n  --lm-dir '' \\\n  --itn-dir '' \\\n  --download-model-dir ${download_model_dir} \\\n  --model-dir ${model_dir} \\\n  --vad-dir ${vad_dir} \\\n  --punc-dir ${punc_dir} \\\n  --decoder-thread-num ${decoder_thread_num} \\\n  --io-thread-num  ${io_thread_num} \\\n  --port ${port} \\\n```\n\n----------------------------------------\n\nTITLE: FunASR Model and Parameter Modification\nDESCRIPTION: This section describes how to modify models or other parameters in FunASR. The process involves first stopping the server, modifying the parameters, and then restarting the server. Model IDs must be from ModelScope or fine-tuned models.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n# ‰æãÂ¶ÇÊõøÊç¢ASRÊ®°Âûã‰∏∫ damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnxÔºåÂàôÂ¶Ç‰∏ãËÆæÁΩÆÂèÇÊï∞ --model-dir\n    --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx \n# ËÆæÁΩÆÁ´ØÂè£Âè∑ --port\n    --port <port number>\n# ËÆæÁΩÆÊúçÂä°Á´ØÂêØÂä®ÁöÑÊé®ÁêÜÁ∫øÁ®ãÊï∞ --decoder-thread-num\n    --decoder-thread-num <decoder thread num>\n# ËÆæÁΩÆÊúçÂä°Á´ØÂêØÂä®ÁöÑIOÁ∫øÁ®ãÊï∞ --io-thread-num\n    --io-thread-num <io thread num>\n# ÂÖ≥Èó≠SSLËØÅ‰π¶ \n    --certfile 0\n```\n\n----------------------------------------\n\nTITLE: Finding and Including External Dependencies (ICU, ZLIB) in CMake\nDESCRIPTION: These commands attempt to locate the ICU and ZLIB libraries on the system. If a library is found, their include directories and library paths are added to the project's build configuration, making their headers and symbols available for compilation and linking.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(ICU COMPONENTS data i18n io  test tu uc)\nif (ICU_FOUND)\n  include_directories(${ICU_INCLUDE_DIRS})\n  set(LIBS ${LIBS} ${ICU_LIBRARIES})\nendif (ICU_FOUND)\n\nfind_package(ZLIB)\nif (ZLIB_FOUND)\n  include_directories(${ZLIB_INCLUDE_DIRECTORIES})\n  set(ZLIBS ${ZLIB_LIBRARIES})\nendif (ZLIB_FOUND)\n```\n\n----------------------------------------\n\nTITLE: Installing funasr_torch from Source Code in Shell\nDESCRIPTION: Installs funasr_torch from source by cloning the official FunASR repository, navigating to the appropriate directory, and performing an editable pip install. An alternate installation command is provided for users in China who require a mirror. Prerequisite: git must be available on the system, and no additional parameters are needed.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/libtorch/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\ncd funasr/runtime/python/libtorch\npip install -e ./\n# For the users in China, you could install with the command:\n# pip install -e ./ -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Adding a Library with CMake\nDESCRIPTION: This CMake block conditionally adds the fstmpdtscript library if HAVE_SCRIPT is enabled. It links the library against fstscript and fst, sets the SOVERSION property, and specifies the installation directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/mpdt/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_SCRIPT)\n  add_library(fstmpdtscript mpdtscript.cc ${HEADER_FILES})\n  target_link_libraries(fstmpdtscript fstscript fst)\n  set_target_properties(fstmpdtscript PROPERTIES \n    SOVERSION \"${SOVERSION}\"\n    FOLDER mpdt\n  )\n  install(TARGETS fstmpdtscript \n\tLIBRARY DESTINATION lib\n\tARCHIVE DESTINATION lib\n\tRUNTIME DESTINATION lib\n  )\nendif(HAVE_SCRIPT)\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Specific Build Settings - CMake\nDESCRIPTION: Applies configurations specific to the Windows platform. This includes removing certain glog header files that might conflict or are handled differently on Windows and adding `-pthread -fPIC` flags to the C++ compiler flags for non-Windows systems.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n  file(REMOVE ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/config.h \n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/export.h \n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/logging.h \n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/raw_logging.h \n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/stl_logging.h \n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/vlog_is_on.h)\nelse()\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread -fPIC\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Structuring M2MeT 2.0 Test Set Data Directory\nDESCRIPTION: Illustrates the required file structure within the `./data/Test_2023_Ali_far` directory for the M2MeT 2.0 2023 Challenge test set. These metadata files (`wav.scp`, `wav_raw.scp`, `segments`, `utt2spk`, `spk2utt`) must be present alongside the raw audio files (placed in `./dataset/Test_2023_Ali_far`) before running the `run_m2met_2023_infer.sh` inference script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/m2met2/Baseline.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndata/Test_2023_Ali_far\n|‚Äî‚Äî wav.scp\n|‚Äî‚Äî wav_raw.scp\n|‚Äî‚Äî segments\n|‚Äî‚Äî utt2spk\n|‚Äî‚Äî spk2utt\n```\n\n----------------------------------------\n\nTITLE: Direct Inference - Python\nDESCRIPTION: This code snippet shows how to perform inference directly using a model. It imports the model class (SenseVoiceSmall) and loads the model from a pretrained location using `from_pretrained`.  The code then prepares the model for inference using `m.eval()` and calls the `inference` method to perform speech recognition.  Key parameters like `data_in`, `language`, `use_itn`, and `ban_emo_unk` are passed to the inference function to control its behaviour.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom model import SenseVoiceSmall\n\nm, kwargs = SenseVoiceSmall.from_pretrained(model=\"iic/SenseVoiceSmall\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs ['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\nprint(text)\n\n```\n\n----------------------------------------\n\nTITLE: Starting Offline ASR Server (Docker)\nDESCRIPTION: Shell command executed inside the Docker container to start the funasr-wss-server program for offline processing. It uses nohup to run in the background and requires paths to various models (VAD, ASR, Punc, LM, ITN, Hotword) to be specified.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenFst Library Build with CMake\nDESCRIPTION: CMake configuration that sets up include directories, installation rules for header files, and conditionally adds subdirectories to the build based on build flags such as HAVE_SCRIPT, HAVE_BIN, and BUILD_TESTING.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories(./include/)\ninstall(DIRECTORY include/ DESTINATION include/\n        FILES_MATCHING PATTERN \"*.h\")\n\nadd_subdirectory(lib)\n\nif(HAVE_SCRIPT)\n  add_subdirectory(script)\nendif(HAVE_SCRIPT)\n\nif(HAVE_BIN)\n  add_subdirectory(bin)\nendif(HAVE_BIN)\n\nadd_subdirectory(extensions)\n\nif(BUILD_TESTING)\n  enable_testing()\n  add_subdirectory(test)\nendif(BUILD_TESTING)\n```\n\n----------------------------------------\n\nTITLE: Configuring Offline Mode Client Request in FunASR WebSocket Protocol\nDESCRIPTION: JSON message format for the initial client request in offline transcription mode. Includes parameters for mode, file information, speech status, hotwords functionality, and text normalization settings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol_zh.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"offline\", \"wav_name\": \"wav_name\", \"wav_format\":\"pcm\", \"is_speaking\": True, \"hotwords\":\"{\\\"ÈòøÈáåÂ∑¥Â∑¥\\\":20,\\\"ÈÄö‰πâÂÆûÈ™åÂÆ§\\\":30}\", \"itn\":True}\n```\n\n----------------------------------------\n\nTITLE: Example train_event.txt\nDESCRIPTION: This snippet demonstrates the expected format of the train_event.txt file, which maps unique IDs to the corresponding event tags.  The supported events are <|BGM|>, <|Speech|>, <|Applause|>, <|Laughter|>, <|Cry|>, <|Sneeze|>, <|Breath|>, and <|Cough|>.  The left side is the unique ID, and the right side is the event tag.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```\n\n----------------------------------------\n\nTITLE: Installing ModelScope via pip (Shell)\nDESCRIPTION: Optional command to install or upgrade the modelscope library using pip3, required if users want to utilize pretrained models from ModelScope. An alternative command is provided for users in China using a mirror URL.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U modelscope\n# For the users in China, you could install with the command:\n# pip3 install -U modelscope -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Resolving cffi/pycparser M1 chip errors (Shell)\nDESCRIPTION: Commands to troubleshoot and resolve compatibility errors with cffi and pycparser on Mac computers with M1 chips by uninstalling and reinstalling the libraries specifically for the arm64 architecture.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall cffi pycparser\nARCHFLAGS=\"-arch arm64\" pip install cffi pycparser --compile --no-cache-dir\n```\n\n----------------------------------------\n\nTITLE: Downloading onnxruntime for Linux (Shell)\nDESCRIPTION: Downloads the pre-compiled onnxruntime library archive for Linux x64 using wget and extracts it using tar. This library is a required dependency for building the FunASR runtime.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/onnxruntime-linux-x64-1.14.0.tgz\ntar -zxvf onnxruntime-linux-x64-1.14.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Python H5 Server\nDESCRIPTION: Changes the current directory to the HTML5 server location within the FunASR repository and starts a Python HTTP server to serve the HTML5 client files. The `--host 0.0.0.0` option makes the server accessible from any IP address, suitable for mobile access.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme_zh.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ncd funasr/runtime/html5\npython h5Server.py --host 0.0.0.0 --port 1337\n```\n\n----------------------------------------\n\nTITLE: Training Log Example\nDESCRIPTION: This snippet shows an example of a training log. It includes important metrics such as loss, perplexity, accuracy, and GPU memory usage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\ntail log.txt\n[2024-03-21 15:55:52,137][root][INFO] - train, rank: 3, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.327), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.259), ('acc', 0.825), ('loss_pre', 0.04), ('loss', 0.299), ('batch_size', 40)], {'data_load': '0.000', 'forward_time': '0.315', 'backward_time': '0.555', 'optim_time': '0.076', 'total_time': '0.947'}, GPU, memory: usage: 3.830 GB, peak: 18.357 GB, cache: 20.910 GB, cache_peak: 20.910 GB\n[2024-03-21 15:55:52,139][root][INFO] - train, rank: 1, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.334), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.285), ('acc', 0.823), ('loss_pre', 0.046), ('loss', 0.331), ('batch_size', 36)], {'data_load': '0.000', 'forward_time': '0.334', 'backward_time': '0.536', 'optim_time': '0.077', 'total_time': '0.948'}, GPU, memory: usage: 3.943 GB, peak: 18.291 GB, cache: 19.619 GB, cache_peak: 19.619 GB\n```\n\n----------------------------------------\n\nTITLE: Java Client Invocation for Offline Transcription\nDESCRIPTION: This shell command demonstrates how to run the Java client FunasrWsClient for offline transcription by connecting to the local FunASR runtime-SDK service with given host, port, audio input file, and operation mode. It acts as a minimal example to test the service via the Java SDK.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nFunasrWsClient --host localhost --port 10095 --audio_in ./asr_example.wav --mode offline\n```\n\n----------------------------------------\n\nTITLE: Sending Recognition Results\nDESCRIPTION: This JSON snippet illustrates the format of the recognition results sent from the server to the client in offline file transcription. It contains the mode, wav_name, recognized text, an 'is_final' flag, timestamp, and stamp_sents which contains segmentation of recognized text.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"offline\", \"wav_name\": \"wav_name\", \"text\": \"asr ouputs\", \"is_final\": True, \"timestamp\":\"[[100,200], [200,500]]\", \"stamp_sents\":[]}\n```\n\n----------------------------------------\n\nTITLE: Conditional Building of fstfarscript Library in CMake\nDESCRIPTION: Conditionally builds the fstfarscript library when scripting support is available. This library depends on fstfar, fstscript, and fst libraries and provides scripting capabilities for Far operations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/far/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_SCRIPT)\n  add_library(fstfarscript\n    far-class.cc \n    farscript.cc\n    getters.cc \n    script-impl.cc\n    strings.cc\n  )\n  target_link_libraries(fstfarscript fstfar fstscript fst)\n  set_target_properties(fstfarscript PROPERTIES \n    SOVERSION \"${SOVERSION}\"\n    FOLDER far\n  )\n\n  install(TARGETS fstfarscript\n\tLIBRARY DESTINATION lib\n\tARCHIVE DESTINATION lib\n\tRUNTIME DESTINATION lib\n  )\nendif(HAVE_SCRIPT)\n```\n\n----------------------------------------\n\nTITLE: Export to ONNX - Python\nDESCRIPTION: This python code also demonstrates exporting a FunASR model to ONNX format. It loads a pre-trained model, and uses the `export` method to generate the ONNX model.  The `quantize` parameter controls whether the model will be quantized.\nSOURCE: https://github.com/modelscope/funasr/blob/main/README.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\", device=\"cpu\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Overriding GFlags and Platform-Specific Build Options in CMake\nDESCRIPTION: This snippet first sets the `WITH_GFLAGS` cache option to OFF, preventing the use of gflags. It then conditionally overrides the `HAVE_BIN` and `HAVE_SCRIPT` options based on the platform, explicitly disabling them for Windows builds using the `FORCE` keyword.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n  set(WITH_GFLAGS OFF CACHE BOOL \"whether build glog with gflags\" FORCE)\n  if (WIN32)\n    set(HAVE_BIN OFF CACHE BOOL \"Build the fst binaries\" FORCE)\n    set(HAVE_SCRIPT OFF CACHE BOOL \"Build the fstscript\" FORCE)\n  else()\n    set(HAVE_BIN ON CACHE BOOL \"Build the fst binaries\" FORCE)\n    set(HAVE_SCRIPT ON CACHE BOOL \"Build the fstscript\" FORCE)\n  endif (WIN32)\n```\n\n----------------------------------------\n\nTITLE: Predicting Timestamps using FunASR AutoModel (Python)\nDESCRIPTION: This snippet shows how to use AutoModel for timestamp prediction (Forced Alignment). It initializes the model with a specific model name ('fa-zh') and processes a tuple containing the audio file path and the corresponding text file path, along with specifying the 'data_type' as ('sound', 'text'). It outputs word-level timestamps.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Starting a SenseVoice FastAPI Deployment Server (Shell)\nDESCRIPTION: These shell commands are used to deploy the SenseVoice model as a web service using FastAPI. The first command sets the `SENSEVOICE_DEVICE` environment variable to specify the GPU device (`cuda:0`). The second command starts the FastAPI application server, listening on port 50000 for incoming inference requests. This requires a corresponding FastAPI application setup (not shown in the snippet).\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Add Executables\nDESCRIPTION: This CMake function, add_executable2, simplifies the process of adding executables, linking them against necessary libraries (fstmpdtscript, fstpdtscript, fstscript, fst, CMAKE_DL_LIBS), setting the folder property, and specifying the installation directory. It checks if target exists before setting target properties.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/mpdt/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction (add_executable2 _name)\n      add_executable(${ARGV})\n      if (TARGET ${_name})\n          target_link_libraries(${_name} fstmpdtscript fstpdtscript fstscript fst ${CMAKE_DL_LIBS})\n          set_target_properties(${_name} PROPERTIES\n            FOLDER mpdt/bin\n          )\n      endif()\n    install(TARGETS ${_name} RUNTIME DESTINATION bin)\n  endfunction()\n```\n\n----------------------------------------\n\nTITLE: Adding FST Linear Modules\nDESCRIPTION: Creates the linear-tagger-fst and linear-classifier-fst loadable modules using the previously defined add_module function.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/linear/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_module(linear-tagger-fst MODULE\n  linear-tagger-fst.cc)\n\nadd_module(linear-classifier-fst MODULE\n  linear-classifier-fst.cc)\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training on Master Node - Shell\nDESCRIPTION: This shell script runs the training process on the master node in a multi-GPU setup. It sets the CUDA_VISIBLE_DEVICES environment variable, determines the number of GPUs, and uses torchrun to launch the distributed training script. It requires the train_ds.py script and appropriate training arguments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Docker Installation Shell Script\nDESCRIPTION: This shell script installs Docker on the server. It downloads the installation script from a specified URL and executes it using `bash` with `sudo` privileges.  Docker is a prerequisite for running the FunASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Forcing Disabled State for Most FST Component Builds in CMake\nDESCRIPTION: This sequence of commands explicitly disables several of the previously defined `HAVE_...` build options by setting their values to OFF in the CMake cache with the `FORCE` keyword. This ensures these components are not built, overriding any user-set values.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n  set(HAVE_COMPACT OFF CACHE BOOL \"Build compact\" FORCE)\n  set(HAVE_CONST OFF CACHE BOOL \"Build const\" FORCE)\n  set(HAVE_GRM OFF CACHE BOOL \"Build grm\" FORCE)\n  set(HAVE_FAR OFF CACHE BOOL \"Build far\" FORCE)\n  set(HAVE_PDT OFF CACHE BOOL \"Build pdt\" FORCE)\n  set(HAVE_MPDT OFF CACHE BOOL \"Build mpdt\" FORCE)\n  set(HAVE_LINEAR OFF CACHE BOOL \"Build linear\" FORCE)\n  set(HAVE_LOOKAHEAD OFF CACHE BOOL \"Build lookahead\" FORCE)\n  set(HAVE_NGRAM OFF CACHE BOOL \"Build ngram\" FORCE)\n  set(HAVE_SPECIAL OFF CACHE BOOL \"Build special\" FORCE)\n```\n\n----------------------------------------\n\nTITLE: Large-Scale Data Training Configuration\nDESCRIPTION: This snippet shows how to configure training for large datasets by splitting the JSONL files into slices and using `data_split_num`.  It updates the training script parameters to use sliced data.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ntrain_data=\"/root/data/list/data.list\"\n\nfunasr/bin/train_ds.py \\\n++train_data_set_list=\"${train_data}\" \\\n++dataset_conf.data_split_num=256\n```\n\n----------------------------------------\n\nTITLE: Downloading onnxruntime using shell script\nDESCRIPTION: Downloads the onnxruntime library using a provided script located in the third_party directory. This is a required dependency for the FunASR project's inference capabilities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/requirements_install.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbash third_party/download_onnxruntime.sh\n```\n\n----------------------------------------\n\nTITLE: Start FunASR Service\nDESCRIPTION: This shell script starts the FunASR service with specified model directories, SSL certificates, and hotword file. It redirects standard output and standard error to a log file. Requires the `FunASR/runtime` directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncd /workspace/FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --certfile  ../../../ssl_key/server.crt \\\n  --keyfile ../../../ssl_key/server.key \\\n  --hotword ../../hotwords.txt  > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Pulling and Running FunASR Docker Image\nDESCRIPTION: These commands first pull the specified version of the FunASR runtime SDK Docker image from the registry. The second command then runs the image, mapping host port 10097 to container port 10095 and mounting the host's `/root` directory to `/workspace/models` inside the container.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-en-cpu-0.1.7\n\nsudo docker run -p 10097:10095 -it --privileged=true -v /root:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-en-cpu-0.1.7\n```\n\n----------------------------------------\n\nTITLE: Non-Real-Time ASR\nDESCRIPTION: This snippet demonstrates how to perform non-real-time ASR using the `AutoModel` class, including VAD and punctuation. It highlights the use of `vad_model`, `vad_kwargs`, and `punc_model` for enhanced ASR performance on longer audio files, as well as dynamic batching.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  \n                  vad_model=\"fsmn-vad\", \n                  vad_kwargs={\"max_single_segment_time\": 60000},\n                  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='È≠îÊê≠')\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Pulling and Running FunASR Docker Image (Shell)\nDESCRIPTION: Pulls the specified FunASR runtime SDK CPU Docker image from the registry and runs it in a container. It maps host port 10095 to the container's port 10095 and mounts the host's /root directory to /workspace/models within the container for model storage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo docker pull registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\n\nsudo docker run -p 10095:10095 -it --privileged=true -v /root:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6\n```\n\n----------------------------------------\n\nTITLE: Link Unwind Library\nDESCRIPTION: If the `Unwind_FOUND` variable is true, indicating the libunwind library was found, it links the `unwind::unwind` target privately to the `glog` library. It also appends the corresponding linker flag (`-lunwind`) to a variable used for static linking options and adds a dependency check for the Unwind library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nif (Unwind_FOUND)\n  target_link_libraries (glog PRIVATE unwind::unwind)\n  set (glog_libraries_options_for_static_linking \"${glog_libraries_options_for_static_linking} -lunwind\")\n  set (Unwind_DEPENDENCY \"find_dependency (Unwind ${Unwind_VERSION})\")\nendif (Unwind_FOUND)\n```\n\n----------------------------------------\n\nTITLE: Using FunASR WebSocket Python API for Recognition\nDESCRIPTION: Illustrates usage of the Funasr_websocket_recognizer Python class in a three-step process: creating a recognizer instance, feeding audio data, and retrieving the final ASR result. Requires the funasr WebSocket server to be running and the client to have relevant dependencies installed. Allows flexible integration in custom Python scripts.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrcg=Funasr_websocket_recognizer(host=\"127.0.0.1\",port=\"30035\",is_ssl=True,mode=\"2pass\")\ntext=rcg.feed_chunk(data)\nprint(\"text\",text)\ntext=rcg.close(timeout=3)\nprint(\"text\",text)\n```\n\n----------------------------------------\n\nTITLE: Real-Time Streaming ASR in FunASR\nDESCRIPTION: Implementation of real-time streaming ASR using chunk-based processing with the Paraformer model in FunASR. The example demonstrates how to process audio chunks and maintain state between chunks.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: Including gflags Source as a CMake Subdirectory\nDESCRIPTION: Illustrates how to incorporate the gflags source code directly into a larger project using `add_subdirectory`. The main project can then link against the `gflags::gflags` target built within the super-project.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 2.8.12 FATAL_ERROR)\n\nproject(Foo)\n\nadd_subdirectory(gflags)\n\nadd_executable(foo src/foo.cc)\ntarget_link_libraries(foo gflags::gflags)\n```\n\n----------------------------------------\n\nTITLE: Running Speech Model Inference Using FunASR CLI with configuration.json File (Shell)\nDESCRIPTION: This snippet demonstrates how to run speech recognition inference on a trained model directory containing a configuration.json file via the FunASR command-line interface. Users specify the model path along with input and output directories. No additional configuration parameters are needed when configuration.json is present. Input is the audio file path and output is saved in the given output directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input==\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\n----------------------------------------\n\nTITLE: Setting Build Options for kaldi-native-fbank Project in CMake\nDESCRIPTION: Configures build options for the project, including C++ standard, shared libraries setting, and project-specific options for tests, Python extensions, and logging. Displays the current settings for verification.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_STANDARD 14 CACHE STRING \"The C++ version to be used.\")\n\nif(NOT DEFINED BUILD_SHARED_LIBS)\n  set(BUILD_SHARED_LIBS ON)\nendif()\nmessage(STATUS \"BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}\")\n\noption(KALDI_NATIVE_FBANK_BUILD_TESTS \"Whether to build tests or not\" ${_BUILD_TESTS})\noption(KALDI_NATIVE_FBANK_BUILD_PYTHON \"Whether to build Python extension\" ${_BUILD_PYTHON})\noption(KALDI_NATIVE_FBANK_ENABLE_CHECK \"Whether to build with log\" OFF)\n\nmessage(STATUS \"KALDI_NATIVE_FBANK_BUILD_TESTS: ${KALDI_NATIVE_FBANK_BUILD_TESTS}\")\nmessage(STATUS \"KALDI_NATIVE_FBANK_BUILD_PYTHON: ${KALDI_NATIVE_FBANK_BUILD_PYTHON}\")\nmessage(STATUS \"KALDI_NATIVE_FBANK_ENABLE_CHECK: ${KALDI_NATIVE_FBANK_ENABLE_CHECK}\")\n```\n\n----------------------------------------\n\nTITLE: Warn About Static Library Packaging Without Headers in CMake\nDESCRIPTION: Checks if building static libraries without installing headers when packaging is enabled. If this condition is met, it issues a warning message recommending alternative build options for runtime and development packages.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_30\n\nLANGUAGE: CMake\nCODE:\n```\nif (BUILD_PACKAGING)\n\n  if (NOT BUILD_SHARED_LIBS AND NOT INSTALL_HEADERS)\n    message (WARNING \"Package will contain static libraries without headers!\"\n                     \"\\nRecommended options for generation of runtime package:\"\n                     \"\\n  BUILD_SHARED_LIBS=ON\"\n                     \"\\n  BUILD_STATIC_LIBS=OFF\"\n                     \"\\n  INSTALL_HEADERS=OFF\"\n                     \"\\n  INSTALL_SHARED_LIBS=ON\"\n                     \"\\nRecommended options for generation of development package:\"\n                     \"\\n  BUILD_SHARED_LIBS=ON\"\n                     \"\\n  BUILD_STATIC_LIBS=ON\"\n                     \"\\n  INSTALL_HEADERS=ON\"\n                     \"\\n  INSTALL_SHARED_LIBS=ON\"\n                     \"\\n  INSTALL_STATIC_LIBS=ON\"\n    )\n  endif ()\n```\n\n----------------------------------------\n\nTITLE: Define Boolean Build Flags (CMake)\nDESCRIPTION: Defines several boolean CMake options using a custom `gflags_define` macro. These flags control aspects like building shared/static libraries, enabling testing, packaging, and installation of components. Each flag has a name, documentation string, and default values.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\ngflags_define (BOOL BUILD_SHARED_LIBS          \"Request build of shared libraries.\"                                       OFF OFF)\ngflags_define (BOOL BUILD_STATIC_LIBS          \"Request build of static libraries (default if BUILD_SHARED_LIBS is OFF).\" OFF ON)\ngflags_define (BOOL BUILD_gflags_LIB           \"Request build of the multi-threaded gflags library.\"                      ON  OFF)\ngflags_define (BOOL BUILD_gflags_nothreads_LIB \"Request build of the single-threaded gflags library.\"                     ON  ON)\ngflags_define (BOOL BUILD_PACKAGING            \"Enable build of distribution packages using CPack.\"                       OFF OFF)\ngflags_define (BOOL BUILD_TESTING              \"Enable build of the unit tests and their execution using CTest.\"          OFF OFF)\ngflags_define (BOOL INSTALL_HEADERS            \"Request installation of headers and other development files.\"             ON  OFF)\ngflags_define (BOOL INSTALL_SHARED_LIBS        \"Request installation of shared libraries.\"                                ON  ON)\ngflags_define (BOOL INSTALL_STATIC_LIBS        \"Request installation of static libraries.\"                                ON  OFF)\ngflags_define (BOOL REGISTER_BUILD_DIR         \"Request entry of build directory in CMake's package registry.\"            OFF OFF)\ngflags_define (BOOL REGISTER_INSTALL_PREFIX    \"Request entry of installed package in CMake's package registry.\"          ON  OFF)\n```\n\n----------------------------------------\n\nTITLE: Real-Time Voice Activity Detection in FunASR\nDESCRIPTION: Implementation of real-time voice activity detection using the FSMN-VAD model with chunk-based processing in FunASR.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n\n----------------------------------------\n\nTITLE: Examples of Modifying FunASR Server Parameters\nDESCRIPTION: Illustrates how to modify specific parameters when restarting the FunASR server. Shows examples for changing the ASR model (`--model-dir`), server port (`--port`), decoder threads (`--decoder-thread-num`), and IO threads (`--io-thread-num`). Emphasizes that the service must be shut down before applying changes.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n# For example, to replace the ASR model with damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx, use the following parameter setting --model-dir\n    --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx \n# Set the port number using --port\n    --port <port number>\n# Set the number of inference threads the server will start using --decoder-thread-num\n    --decoder-thread-num <decoder thread num>\n# Set the number of IO threads the server will start using --io-thread-num\n    --io-thread-num <io thread num>\n```\n\n----------------------------------------\n\nTITLE: Example data.list - JSONL File List\nDESCRIPTION: This shows the contents of the data.list file, which contains a list of sliced JSONL files. Each line represents a path to a JSONL file, allowing the training process to load data in chunks and reduce memory usage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ndata/list/train.0.jsonl\ndata/list/train.1.jsonl\n...\n```\n\n----------------------------------------\n\nTITLE: Disabling SSL Verification for FunASR Service\nDESCRIPTION: This shell command updates the SSL verification setting in the FunASR offline deployment to disable SSL certificate verification by setting the corresponding flag to 0. SSL is enabled by default (1).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-online-cpu-zh.sh update --ssl 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependency Paths and Submodules - CMake\nDESCRIPTION: Adds necessary include directories for generated files, ONNX Runtime, yaml-cpp, kaldi-native-fbank, and glog. Also adds subdirectories for external dependencies like yaml-cpp, kaldi-native-fbank, ONNX Runtime source, and glog, allowing CMake to process their respective CMakeLists.txt files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Include generated *.pb.h files\ninclude_directories(${CMAKE_CURRENT_BINARY_DIR})\n\nlink_directories(${ONNXRUNTIME_DIR}/lib)\nlink_directories(${FFMPEG_DIR}/lib)\n\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/include/)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/yaml-cpp/include/)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi-native-fbank)\n\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/yaml-cpp yaml-cpp)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi-native-fbank/kaldi-native-fbank/csrc csrc)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/src src)\n\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog glog)\n```\n\n----------------------------------------\n\nTITLE: Description of C++ Client Command-Line Parameters\nDESCRIPTION: This text snippet describes the C++ client command parameters including server IP address, port number, audio file path, thread number, SSL verification setting, hotword file path, and ITN usage. These allow configuring connection and transcription options similarly to the Python client.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n--server-ip specifies the IP address of the machine where the FunASR runtime-SDK service is deployed. The default value is the local IP address (127.0.0.1). If the client and the service are not on the same server, the IP address needs to be changed to the IP address of the deployment machine.\n--port specifies the deployment port number as 10095.\n--wav-path specifies the audio file to be transcribed, and supports file paths.\n--thread_num sets the number of concurrent send threads, with a default value of 1.\n--ssl sets whether to enable SSL certificate verification, with a default value of 1 for enabling and 0 for disabling.\n--hotword: Hotword file path, one line for each hotword(e.g.:ÈòøÈáåÂ∑¥Â∑¥ 20)\n--use-itn: whether to use itn, the default value is 1 for enabling and 0 for disabling.\n```\n\n----------------------------------------\n\nTITLE: Sending End-of-Audio Signal in FunASR WebSocket Protocol\nDESCRIPTION: JSON message to indicate the end of audio data transmission in both offline and real-time modes. The is_speaking parameter is set to False to signal completion.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol_zh.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n{\"is_speaking\": False}\n```\n\n----------------------------------------\n\nTITLE: Including and Building FST Dependencies (GFlags, OpenFST) - CMake\nDESCRIPTION: Conditionally includes and adds subdirectories for GFlags and OpenFST, which are dependencies for FST (Finite State Transducers) functionality. It notes that a modified version of OpenFST is used and includes necessary header paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_FST)\n    # fst depend on glog and gflags\n    include_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/gflags)\n    add_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/gflags gflags)\n    include_directories(${gflags_BINARY_DIR}/include)\n      \n    # the following openfst if cloned from https://github.com/kkm000/openfst.git\n    # with some patch to fix the make errors. \n    add_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/openfst openfst)\n    include_directories(${openfst_SOURCE_DIR}/src/include)\n    if(WIN32)\n    include_directories(${openfst_SOURCE_DIR}/src/lib)\n    endif() \nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing FunASR AutoModel for Inference (Python)\nDESCRIPTION: Initializes the `AutoModel` class from the `funasr` library for Python-based inference. It takes the local model directory path (`./model_dir`) as input, assuming a `configuration.json` file is present for automatic configuration loading. The initialized model can then be used for generating inference results. Requires the FunASR Python package.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Starting FunASR Server with Standard 16kHz Chinese Models\nDESCRIPTION: Commands to start the FunASR server with Paraformer-large model, VAD, punctuation, language model, and ITN components for 16kHz Chinese speech recognition.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \\\n  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Python Client Example for FunASR Service\nDESCRIPTION: This is a command for running a Python client to test the FunASR service using the 2pass mode. It takes the host and port as input and specifies the mode of operation. This assumes the client tool (`funasr_wss_client.py`) is located in the current directory. The client connects to the server at the specified address and port (127.0.0.1:10096).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10096 --mode 2pass\n```\n\n----------------------------------------\n\nTITLE: Generating Inference Results with FunASR AutoModel (Python)\nDESCRIPTION: This snippet describes the 'generate' method of the AutoModel class, used for performing inference. It takes an 'input' parameter which can be a file path (wav/pcm), byte stream, wav.scp, or audio samples (numpy/torch), and an optional 'output_dir'. It also accepts '**kwargs' for model-specific inference parameters.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nres = model.generate(input=[str], output_dir=[str])\n```\n\n----------------------------------------\n\nTITLE: Adding and Building Subdirectories for Third-Party and Project Source Code (CMake)\nDESCRIPTION: This section incorporates third-party libraries (yaml-cpp, kaldi-native-fbank, kaldi) and the project's src and bin folders as build subdirectories within CMake. This approach allows modular compilation and organization. It requires correct directory existence and CMakeLists.txt files within each target. There are no direct inputs; the outputs are inclusion of these components in the build graph. Limitation: Build fails if expected directories or CMake files are missing.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(third_party/yaml-cpp)\nadd_subdirectory(third_party/kaldi-native-fbank/kaldi-native-fbank/csrc)\nadd_subdirectory(third_party/kaldi)\nadd_subdirectory(src)\nadd_subdirectory(bin)\n\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Inference with Configuration File (Shell)\nDESCRIPTION: Executes the FunASR inference script from the command line. It uses `python -m funasr.bin.inference` and specifies the local model path via `++model`. This method assumes a `configuration.json` exists within the model directory, simplifying parameter specification. Requires FunASR installation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input==\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training on Worker Node - Shell\nDESCRIPTION: This shell command initiates multi-GPU training on a worker node using torchrun. It sets the CUDA_VISIBLE_DEVICES environment variable, calculates the number of GPUs, and executes the training script train_ds.py, ensuring MASTER_ADDR and MASTER_PORT match the master node's settings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 1 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Initializing FunASR AutoModel Class (Python)\nDESCRIPTION: Shows the class signature for initializing the `AutoModel` in FunASR. Key parameters include `model` (name or path), `device` (CPU/GPU/MPS/XPU), `ncpu` (CPU threads), `output_dir`, `batch_size`, `hub` (ModelScope/Hugging Face), and `**kwargs` for additional model configuration.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Exporting FunASR Models Programmatically in Python Using AutoModel\nDESCRIPTION: This snippet demonstrates exporting a FunASR model programmatically in Python. It loads a pretrained model by name and calls the export method with quantization flag, returning the exported artifact. This provides users with a flexible way to trigger model export for deployment pipelines inside Python applications. Requires FunASR Python API.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training on Master Node - Shell\nDESCRIPTION: This shell command initiates multi-GPU training on the master node using torchrun. It sets the CUDA_VISIBLE_DEVICES environment variable to specify the GPUs to use, calculates the number of GPUs, and then executes the training script train_ds.py with specified arguments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 2 --node_rank 0 --nproc_per_node ${gpu_num} --master_addr 192.168.1.1 --master_port 12345 \\\n../../../funasr/bin/train_ds.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Stopping FunASR Service\nDESCRIPTION: This provides the commands to stop the FunASR service. It first lists the running processes to find the Process ID (PID) of the `funasr-wss-server`. Then, it uses the `kill -9 PID` command to forcefully terminate the process. This operation requires appropriate permissions and should be done with care.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n# Êü•Áúã funasr-wss-server ÂØπÂ∫îÁöÑPID\nps -x | grep funasr-wss-server\nkill -9 PID\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR Python Runtime Dependencies\nDESCRIPTION: Installs required Python packages (modelscope, funasr, flask) using pip and clones the FunASR repository from GitHub to prepare for Python server deployment. Includes an alternative mirror URL for users in mainland China.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme_zh.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip3 install -U modelscope funasr flask\n# ‰∏≠ÂõΩÂ§ßÈôÜÁî®Êà∑ÔºåÂ¶ÇÊûúÈÅáÂà∞ÁΩëÁªúÈóÆÈ¢òÔºåÂèØ‰ª•ÈÄöËøá‰∏ãÈù¢Êåá‰ª§ÂÆâË£ÖÔºö\n# pip3 install -U modelscope funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\n```\n\n----------------------------------------\n\nTITLE: Adding Common Third-Party Subdirectories - CMake\nDESCRIPTION: Adds several third-party library or source code directories as subdirectories to be processed by CMake. These subdirectories likely contain their own CMakeLists.txt files for building components like yaml-cpp, kaldi-native-fbank (csrc), ONNX Runtime source (src), and kaldi.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/yaml-cpp yaml-cpp)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi-native-fbank/kaldi-native-fbank/csrc csrc)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/src src)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi kaldi)\n```\n\n----------------------------------------\n\nTITLE: Exporting Speech Model to ONNX Format with FunASR Python API (Python)\nDESCRIPTION: This Python example initializes a FunASR model (e.g., paraformer) on the specified device, then exports the model to ONNX format using the export method. The quantize argument disables quantization during export. Output is an ONNX model file that can later be optimized or run using ONNX runtime environments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\", device=\"cpu\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Running FunASR Inference via Python API (Python)\nDESCRIPTION: Provides a quick start example for FunASR inference using the Python API. It imports `AutoModel`, initializes it with the `paraformer-zh` model, and calls the `generate` method with a remote audio URL as input. The recognized text is printed to the console.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Modify Model Parameters\nDESCRIPTION: This text describes how to modify model and other parameters for the FunASR service. It involves stopping the service, modifying the parameters in the configuration file, and restarting the service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n# ‰æãÂ¶ÇÊõøÊç¢ASRÊ®°Âûã‰∏∫ damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorchÔºåÂàôÂ¶Ç‰∏ãËÆæÁΩÆÂèÇÊï∞ --model-dir\n    --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch \n# ËÆæÁΩÆÁ´ØÂè£Âè∑ --port\n    --port <port number>\n# ËÆæÁΩÆÊúçÂä°Á´ØÂêØÂä®ÁöÑÊé®ÁêÜÁ∫øÁ®ãÊï∞ --decoder-thread-num\n    --decoder-thread-num <decoder thread num>\n# ËÆæÁΩÆÊúçÂä°Á´ØÂêØÂä®ÁöÑIOÁ∫øÁ®ãÊï∞ --io-thread-num\n    --io-thread-num <io thread num>\n# ÂÖ≥Èó≠SSLËØÅ‰π¶ \n    --certfile 0\n```\n\n----------------------------------------\n\nTITLE: Installing ffmpeg and Additional Python Dependencies via Shell\nDESCRIPTION: Installs ffmpeg using system package managers and required Python packages for audio handling and WebSocket communication. ffmpeg is needed for processing video/audio files. Example covers installation commands for multiple OS platforms (Ubuntu, Centos, Mac, Windows).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\napt-get install -y ffmpeg #ubuntu\n# yum install -y ffmpeg # centos\n# brew install ffmpeg # mac\n# winget install ffmpeg # wins\npip3 install websockets ffmpeg-python\n```\n\n----------------------------------------\n\nTITLE: Python Client Command for Offline Transcription\nDESCRIPTION: This shell command starts the FunASR client in offline mode using the python-client. It specifies the server's host and port, the input audio file (or wav.scp), the output directory for results, and other options. It's used to test the offline transcription service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_client.py --host \"127.0.0.1\" --port 10095 --mode offline --audio_in \"./data/wav.scp\" --send_without_sleep --output_dir \"./results\"\n```\n\n----------------------------------------\n\nTITLE: Generate Binary Module Paths\nDESCRIPTION: Iterates through a list of CMake modules (`_glog_CMake_MODULES`) and generates their corresponding paths within the binary data directory (`_glog_BINARY_CMake_DATADIR`), storing the results in `_glog_BINARY_CMake_MODULES`. This prepares a list of files to be copied during the build.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nforeach (_file ${_glog_CMake_MODULES})\n  get_filename_component (_module \"${_file}\" NAME)\n\n  list (APPEND _glog_BINARY_CMake_MODULES\n    ${_glog_BINARY_CMake_DATADIR}/${_module})\nendforeach (_file)\n```\n\n----------------------------------------\n\nTITLE: Downloading onnxruntime Using wget - Shell\nDESCRIPTION: This shell snippet downloads and extracts the specific onnxruntime binary distribution for Linux x64 using wget and tar. It is required for FunASR runtime compilation and execution. The input consists of no parameters; output is the extracted onnxruntime directory in the working folder. Ensure wget and tar are installed and the network connection is available.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/onnxruntime-linux-x64-1.14.0.tgz\ntar -zxvf onnxruntime-linux-x64-1.14.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Inference without Configuration - Shell\nDESCRIPTION: This shell command performs inference using a trained model without a configuration file. It requires specifying the config path, config name, initialization parameters, tokenizer configuration, and frontend configuration, along with input and output directories.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: Example train_text_language.txt\nDESCRIPTION: This snippet demonstrates the expected format of the train_text_language.txt file, which maps unique IDs to the corresponding language tags. The supported languages are <|zh|>, <|en|>, <|yue|>, <|ja|>, and <|ko|>.  The left side is the unique ID, and the right side is the language tag.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n\n----------------------------------------\n\nTITLE: Optimizing ONNX Model Using onnxslim Tool (Shell)\nDESCRIPTION: This shell snippet demonstrates optimization of an ONNX model using the 'onnxslim' utility, which helps simplify and reduce the model size for faster inference. The command optionally installs onnxslim and rewrites the input ONNX file in place to optimize its structure and runtime efficiency.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\n# pip3 install -U onnxslim\nonnxslim model.onnx model.onnx\n```\n\n----------------------------------------\n\nTITLE: Starting the FunASR GRPC Client (python)\nDESCRIPTION: This command demonstrates how to start the FunASR gRPC client.  It executes the grpc_main_client.py script, providing the host, port, and path to a WAV file as command-line arguments.  The script will then connect to the server and send audio data for transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/grpc/Readme.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython grpc_main_client.py --host 127.0.0.1 --port 10100 --wav_path /path/to/your_test_wav.wav\n```\n\n----------------------------------------\n\nTITLE: Generate SCP Files Command\nDESCRIPTION: This command uses jsonl2scp to convert jsonl files back into wav.scp and text.txt format. It's the reverse operation of scp2jsonl and specifies the input jsonl file and output scp files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\n# generate wav.scp and text.txt from train.jsonl and val.jsonl\njsonl2scp \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_in=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Shell Script\nDESCRIPTION: This shell script installs Docker on the server. It downloads the installation script from an OSS location and executes it with sudo privileges. Prerequisites include having curl installed and a server with an appropriate operating system. The script automates the Docker installation process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh;\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring FunASR Build Environment and Source Files in CMake\nDESCRIPTION: This snippet configures platform-specific include directories, compiler options, and source file groups conditionally for Windows (WIN32). It prepares the environment for building by adding include paths for ONNXRUNTIME, FFMPEG, and OpenSSL, defines preprocessor macros for WebSocket++, and sets compiler options to handle large object files and UTF-8 encoding for MSVC compiler. The snippet also groups related ONNX Runtime source files into a variable for reuse in executable targets.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/bin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n  include_directories(${ONNXRUNTIME_DIR}/include)\n  include_directories(${FFMPEG_DIR}/include)\n  include_directories(${OPENSSL_ROOT_DIR}//include)\n  link_directories(${OPENSSL_ROOT_DIR}/lib)\n  add_definitions(-D_WEBSOCKETPP_CPP11_RANDOM_DEVICE_)\n  add_definitions(-D_WEBSOCKETPP_CPP11_TYPE_TRAITS_)\n  add_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/bigobj>\")\n  add_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/utf-8>\")\n  SET(RELATION_SOURCE \"../../onnxruntime/src/resample.cpp\" \"../../onnxruntime/src/util.cpp\" \"../../onnxruntime/src/alignedmem.cpp\" \"../../onnxruntime/src/encode_converter.cpp\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Model Inference with Configuration File - Shell\nDESCRIPTION: Shell command to perform speech recognition inference using a locally trained or downloaded FunASR model. It uses the `funasr.bin.inference` script, specifying the path to the model directory (which must contain `configuration.json`), the input audio data, and the desired output directory for results.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference ++model=\"./model_dir\" ++input==\"${input}\" ++output_dir=\"${output_dir}\"\n```\n\n----------------------------------------\n\nTITLE: Benchmarking SenseVoice with Triton-ASR-Client\nDESCRIPTION: Commands to clone and run the Triton-ASR-Client for benchmarking SenseVoice performance, using Aishell1 test dataset with configurable concurrency and batch size.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/yuekaizhang/Triton-ASR-Client.git\ncd Triton-ASR-Client\nnum_task=32\npython3 client.py \\\n    --server-addr localhost \\\n    --server-port 10086 \\\n    --model-name sensevoice \\\n    --compute-cer \\\n    --num-tasks $num_task \\\n    --batch-size 16 \\\n    --manifest-dir ./datasets/aishell1_test\n```\n\n----------------------------------------\n\nTITLE: Generating Self-Signed SSL Certificate in Shell\nDESCRIPTION: A step-by-step shell script for generating a self-signed SSL certificate, including creating a private key, generating a CSR file, removing the passphrase, and creating a CRT file with a 365-day validity period.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/ssl_key/readme_cn.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n### 1)ÁîüÊàêÁßÅÈí•ÔºåÊåâÁÖßÊèêÁ§∫Â°´ÂÜôÂÜÖÂÆπ\nopenssl genrsa -des3 -out server.key 1024\n \n### 2)ÁîüÊàêcsrÊñá‰ª∂ ÔºåÊåâÁÖßÊèêÁ§∫Â°´ÂÜôÂÜÖÂÆπ\nopenssl req -new -key server.key -out server.csr\n \n### ÂéªÊéâpass\ncp server.key server.key.org \nopenssl rsa -in server.key.org -out server.key\n \n### ÁîüÊàêcrtÊñá‰ª∂ÔºåÊúâÊïàÊúü1Âπ¥Ôºà365Â§©Ôºâ\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\n```\n\n----------------------------------------\n\nTITLE: Testing FunASR Transcription Server with curl in Shell\nDESCRIPTION: This command tests the running FunASR server by uploading an audio file named example.wav via a POST request to localhost on port 80. The -F switch specifies form submissions; the file field 'file' is expected by the server. curl must be installed, and the FunASR server should be running and accessible at the designated endpoint. The response will include transcription results or error messages if configuration is incorrect.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme_zh.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl -F \"file=@example.wav\" 127.0.0.1:80\n```\n\n----------------------------------------\n\nTITLE: Installing OpenSSL dependencies for Ubuntu\nDESCRIPTION: Shell command to install the OpenSSL development library necessary for compilation on Ubuntu Linux systems, enabling SSL functionalities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\napt-get install libssl-dev\n```\n\n----------------------------------------\n\nTITLE: Configuration JSON Example\nDESCRIPTION: This is an example `configuration.json` file that maps paths in the configuration files to the correct locations, mainly `model.pt`, `config.yaml`, `bpemodel` and `am.mvn`\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"framework\": \"pytorch\",\n  \"task\" : \"auto-speech-recognition\",\n  \"model\": {\"type\" : \"funasr\"},\n  \"pipeline\": {\"type\":\"funasr-pipeline\"},\n  \"model_name_in_hub\": {\n    \"ms\":\"\", \n    \"hf\":\"\"},\n  \"file_path_metas\": {\n    \"init_param\":\"model.pt\", \n    \"config\":\"config.yaml\",\n    \"tokenizer_conf\": {\"bpemodel\": \"chn_jpn_yue_eng_ko_spectok.bpe.model\"},\n    \"frontend_conf\":{\"cmvn_file\": \"am.mvn\"}}\n}\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Test and Tool Subdirectories\nDESCRIPTION: This snippet checks CMake boolean options (`YAML_CPP_BUILD_TESTS`, `YAML_CPP_BUILD_TOOLS`) to conditionally include subdirectories for building tests (`test`) and utility tools (`util`). If the respective option is enabled, `add_subdirectory` processes the `CMakeLists.txt` file within that directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(YAML_CPP_BUILD_TESTS)\n\tadd_subdirectory(test)\nendif()\nif(YAML_CPP_BUILD_TOOLS)\n\tadd_subdirectory(util)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading ffmpeg for Linux (Shell)\nDESCRIPTION: Downloads the pre-compiled shared ffmpeg library archive for Linux x64 using wget and extracts it using tar. This library is a required dependency for handling audio/video streams in the FunASR runtime.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/ffmpeg-master-latest-linux64-gpl-shared.tar.xz\ntar -xvf ffmpeg-master-latest-linux64-gpl-shared.tar.xz\n```\n\n----------------------------------------\n\nTITLE: Specify Registered Model in config.yaml - YAML\nDESCRIPTION: This shows how to specify a newly registered model in the config.yaml file.  The 'model' key is set to the name given when registering the model (SenseVoiceSmall).\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nmodel: SenseVoiceSmall\nmodel_conf:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Speech Recognition Inference with AutoModel (Python)\nDESCRIPTION: This Python code snippet shows how to perform speech recognition using the `AutoModel` class from the `funasr` library. It loads the specified model and then uses it to generate text from an audio file URL.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer-zh\")\n\nres = model.generate(input=\"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Training with Large Datasets - Shell\nDESCRIPTION: This shell command illustrates how to train a model with large datasets by specifying a data list file and setting the data_split_num parameter. This allows for slicing and grouping of the dataset to reduce memory usage during training.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ntrain_data=\"/root/data/list/data.list\"\n\nfunasr/bin/train_ds.py \\\n++train_data_set_list=\"${train_data}\" \\\n++dataset_conf.data_split_num=256\n```\n\n----------------------------------------\n\nTITLE: Generating jsonl from scp/txt - Shell\nDESCRIPTION: This shell command uses scp2jsonl to generate a JSONL file from wav.scp and text.txt files. It takes file lists and data types as input, and specifies the output JSONL file path.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: CPP Client Example Command\nDESCRIPTION: This command runs the C++ client to transcribe an audio file using the FunASR service.  It specifies the server IP, port, and the path to the audio file. Prerequisites include having the C++ client compiled and ready to run, and the FunASR service running.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en_zh.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-wss-client --server-ip 127.0.0.1 --port 10097 --wav-path ../audio/asr_example.wav\n```\n\n----------------------------------------\n\nTITLE: FunASR Timestamp Prediction - Python\nDESCRIPTION: This snippet demonstrates how to perform timestamp prediction using the FunASR Python API. It loads a pre-trained forced alignment model and predicts timestamps for the words in the input text, aligned with the input audio. It requires the `funasr` library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Building FST Linear Executable Applications\nDESCRIPTION: Conditional build configuration for fstlinear and fstloglinearapply executables when HAVE_BIN is enabled. Sets up dependencies, linking, and installation paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/linear/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_BIN)\n  add_executable(fstlinear\n    fstlinear.cc)\n  target_link_libraries(fstlinear\n    fstlinearscript\n    fstscript \n    fst\n    ${CMAKE_DL_LIBS}\n )\n\n  add_executable(fstloglinearapply\n    fstloglinearapply.cc)\n  target_link_libraries(fstloglinearapply\n    fstlinearscript\n    fstscript \n    fst\n    ${CMAKE_DL_LIBS}\n  )\n  install(TARGETS fstlinear fstloglinearapply\n    RUNTIME DESTINATION bin\n  )\n  set_target_properties(fstlinear fstloglinearapply PROPERTIES\n    FOLDER linear/bin\n  )\nendif(HAVE_BIN)\n```\n\n----------------------------------------\n\nTITLE: Setting Slash Variable Based on OS with CMake\nDESCRIPTION: This conditional statement sets the value of the SLASH variable based on the operating system. If the OS is Windows, SLASH is set to \"\\\\\\\\\"; otherwise, it's set to \"/\". This is likely used for constructing file paths or regular expressions that need to be OS-specific.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (OS_WINDOWS)\n  set (SLASH \"\\\\\")\nelse ()\n  set (SLASH \"/\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Preparing training data for language models in FunASR\nDESCRIPTION: Downloads and extracts example training corpus, lexicon, and AM modeling units required for language model training. Offers options for both standard and 8k AM model matching.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/lm_train_tutorial.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# ‰∏ãËΩΩ: Á§∫‰æãËÆ≠ÁªÉËØ≠Êñôtext„ÄÅlexicon Âíå amÂª∫Ê®°ÂçïÂÖÉunits.txt\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/requirements/lm.tar.gz\n# Â¶ÇÊûúÊòØÂåπÈÖç8kÁöÑamÊ®°ÂûãÔºå‰ΩøÁî® https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/requirements/lm_8358.tar.gz\ntar -zxvf lm.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Copy README for CPack Compatibility in CMake\nDESCRIPTION: Copies the `README.md` file to `README.txt` within the build directory. This is done because some CPack generators (like PackageMaker) might not handle the `.md` extension correctly for resource files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_33\n\nLANGUAGE: CMake\nCODE:\n```\n  # some package generators (e.g., PackageMaker) do not allow .md extension\n  configure_file (\"${CMAKE_CURRENT_LIST_DIR}/README.md\" \"${CMAKE_CURRENT_BINARY_DIR}/README.txt\" COPYONLY)\n```\n\n----------------------------------------\n\nTITLE: Building Client and Dependencies Using Make\nDESCRIPTION: Guides the user through navigating to the Java runtime directory, downloading required Java libraries, building the WebSocket client, and executing it using make commands. Relies on a Makefile with targets: downjar, buildwebsocket, and runclient.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/java/readme.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ncd funasr/runtime/java\n# download java lib\nmake downjar\n# compile \nmake buildwebsocket\n# run client\nmake runclient\n```\n\n----------------------------------------\n\nTITLE: Alternative Download for FunASR Deployment Script\nDESCRIPTION: This command provides an alternative download link for the deployment script, hosted on oss-cn-hangzhou, which may be preferable for users in mainland China experiencing network issues with GitHub.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/funasr-runtime-deploy-offline-cpu-en.sh;\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Training - Command Line\nDESCRIPTION: This snippet shows how to start a model training using the FunASR command line tool. It takes paths to train and validation data, and an output directory. It's a quick test, not recommended for full training.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Stopping a Docker Container using Shell\nDESCRIPTION: Provides commands to manage a running container: `exit` to leave the container's shell, `sudo docker ps` to list currently running containers, and `sudo docker stop funasr` to stop the container named 'funasr'. Requires sudo privileges for docker commands.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nexit\nsudo docker ps\nsudo docker stop funasr\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Model Class\nDESCRIPTION: Shows example of registering a custom model class, indicating the need to implement __init__, forward, and inference methods. Registered class can then be referenced in configurations for model loading.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables_zh.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n@tables.register(\"model_classes\", \"SenseVoiceSmall\")\nclass SenseVoiceSmall(nn.Module):\n    def __init__(*args, **kwargs):\n        ...\n    def forward(self, **kwargs):\n        ...\n    def inference(self, data_in, data_lengths=None, key: list = None, tokenizer=None, frontend=None, **kwargs):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Installing funasr-onnx from source code\nDESCRIPTION: Commands to clone the FunASR repository and install the onnxruntime package from source code, with alternative commands for users in China using the SJTU mirror.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/onnxruntime/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\ncd funasr/runtime/python/onnxruntime\npip install -e ./\n# For the users in China, you could install with the command:\n# pip install -e ./ -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Converting Between Data Formats with FunASR Utilities\nDESCRIPTION: Commands for converting between different data formats in FunASR. First command converts wav.scp and text.txt to jsonl format, and the second converts jsonl back to wav.scp and text.txt.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n# generate train.jsonl and val.jsonl from wav.scp and text.txt\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\nLANGUAGE: shell\nCODE:\n```\n# generate wav.scp and text.txt from train.jsonl and val.jsonl\njsonl2scp \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_in=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Starting the FunASR WebSocket Server - Basic Example\nDESCRIPTION: Starts the FunASR WebSocket server on port 10095 with default or pre-configured models. Simplifies server startup for quick testing or local deployment. Requires all model dependencies installed and accessible.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/python/websocket/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython funasr_wss_server.py --port 10095\n```\n\n----------------------------------------\n\nTITLE: Building FST Linear Script Library\nDESCRIPTION: Conditional build configuration for the fstlinearscript library when HAVE_SCRIPT is enabled. Sets up library dependencies, versioning, and installation locations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/linear/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_SCRIPT)\n  add_library(fstlinearscript\n    linearscript.cc\n    ${HEADER_FILES}\n  )\n  target_link_libraries(fstlinearscript\n    fstscript\n    fst\n  )\n  set_target_properties(fstlinearscript PROPERTIES \n    SOVERSION \"${SOVERSION}\"\n    FOLDER linear\n  )\n  \n  install(TARGETS fstlinearscript\n\tLIBRARY DESTINATION lib\n\tARCHIVE DESTINATION lib\n\tRUNTIME DESTINATION lib\n  )\nendif(HAVE_SCRIPT)\n```\n\n----------------------------------------\n\nTITLE: TensorBoard Visualization of Training Metrics (Shell)\nDESCRIPTION: This command launches TensorBoard, a visualization tool for monitoring training metrics, logs, and graphs. It helps users analyze training behavior over time by opening a local server at a specified log directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_17\n\nLANGUAGE: Shell\nCODE:\n```\ntensorboard --logdir /xxxx/FunASR/examples/industrial_data_pretraining/paraformer/outputs/log/tensorboard\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard for Training Visualization in Bash\nDESCRIPTION: This snippet shows how to start TensorBoard to visualize training metrics by specifying the log directory produced during training. After running the command, the TensorBoard UI is accessible through http://localhost:6006 in a web browser. Requires TensorBoard installed in the environment and valid log files in the specified directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir /xxxx/FunASR/examples/industrial_data_pretraining/paraformer/outputs/log/tensorboard\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Documentation for FunASR\nDESCRIPTION: Commands to navigate to the docs directory and generate HTML documentation using make. The generated files will be placed in the FunASR/docs/_build directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd docs\nmake html\n```\n\n----------------------------------------\n\nTITLE: Test ONNX Model - Python\nDESCRIPTION: This Python code tests an ONNX model using the funasr_onnx package. It initializes the Paraformer model and performs inference on a given audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Shell Script\nDESCRIPTION: This shell script downloads and executes an installation script to install Docker. It assumes the user does not have Docker installed and provides a straightforward method for installation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh;\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Voice Activity Detection (Non-Real-Time)\nDESCRIPTION: This snippet demonstrates non-real-time VAD using the `AutoModel`. It initializes the model with \"fsmn-vad\" and uses the `generate` method to detect voice activity segments in the input audio.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Visualization for Model Repository\nDESCRIPTION: This Shell command displays the directory layout of the prepared model repository, indicating the organization of encoder, feature extractor, scoring, and configuration files essential for Triton deployment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README_paraformer_offline.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ntree -L 2 model_repo_paraformer_large_offline/\n# Outputs the directory tree structure showing model files and configs as described.\n```\n\n----------------------------------------\n\nTITLE: Execute FunASR Java Client for Offline Transcription\nDESCRIPTION: This command runs the Java client `FunasrWsClient` to perform offline file transcription. It specifies the host, port, input audio file, and sets the mode to `offline`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nFunasrWsClient --host localhost --port 10095 --audio_in ./asr_example.wav --mode offline\n```\n\n----------------------------------------\n\nTITLE: Viewing Training Logs - Shell\nDESCRIPTION: This shell command uses the tail command to display the contents of the training log file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ntail log.txt\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Target for Code Formatting\nDESCRIPTION: This code defines a custom CMake target named `format`. Running this target executes the `clang-format` command with specific options (`--style=file -i`) on all source files collected in the global property `SRCS_LIST`. This allows developers to easily format the codebase.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nget_property(all_sources GLOBAL PROPERTY SRCS_LIST)\nadd_custom_target(format\n\tCOMMAND clang-format --style=file -i ${all_sources}\n\tCOMMENT \"Running clang-format\"\n\tVERBATIM)\n```\n\n----------------------------------------\n\nTITLE: Running Full ASR Pipeline Benchmark (FSMN-VAD + Paraformer-large + Ngram + CT-Transformer, FunASR-ONNX, Shell)\nDESCRIPTION: Executes the ONNX offline benchmarking of the complete ASR stack including VAD, ASR, N-gram language model, and punctuation restoration. Requires all relevant ONNX models and configuration in the specified directories. Key parameters include '--lm-dir' for the n-gram model in addition to other model-specific directories. Outputs contain recognition quality and runtime benchmarks on the provided test data. All component models must be pre-exported and located in accessible directories.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx_cpp.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./funasr-onnx-offline-rtf \\\n    --model-dir    ./damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch \\\n    --quantize  true \\\n    --vad-dir   ./damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n    --punc-dir  ./damo/punc_ct-transformer_zh-cn-common-vocab272727-onnx \\\n    --lm-dir    ./damo/speech_ngram_lm_zh-cn-ai-wesp-fst \\\n    --wav-path     ./aishell1_test.scp  \\\n    --thread-num 32\n```\n\n----------------------------------------\n\nTITLE: Testing Real-time ASR Client (Docker)\nDESCRIPTION: Python client command executed on the host machine to test the real-time ASR service running in the Docker container. It connects to the mapped host port 10096 using the '2pass' mode.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\npython3 funasr_wss_client.py --host \"127.0.0.1\" --port 10096 --mode 2pass\n```\n\n----------------------------------------\n\nTITLE: Specifying Global Link Directories - CMake\nDESCRIPTION: Adds directories to the search path for the linker using `link_directories()`. This is used to specify where to find necessary libraries for ONNX Runtime and FFmpeg.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nlink_directories(${ONNXRUNTIME_DIR}/lib)\nlink_directories(${FFMPEG_DIR}/lib)\n```\n\n----------------------------------------\n\nTITLE: Including gflags Header (Version 1.0+)\nDESCRIPTION: Demonstrates the recommended way to include the gflags header file in C++ code starting from gflags version 1.0. The header file location moved from '/usr/include/google' to '/usr/include/gflags'.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/README.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <gflags/gflags.h>\n```\n\n----------------------------------------\n\nTITLE: Kill FunASR Service\nDESCRIPTION: This text describes how to stop the FunASR service. It involves finding the process ID (PID) of the `funasr-wss-server` process and using the `kill` command to terminate it.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n# Êü•Áúã funasr-wss-server ÂØπÂ∫îÁöÑPID\nps -x | grep funasr-wss-server\nkill -9 PID\n```\n\n----------------------------------------\n\nTITLE: Downloading ffmpeg for Linux x64\nDESCRIPTION: Shell command to download and extract the ffmpeg library required for multimedia processing within the FunASR environment on Linux x64 systems.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/ffmpeg-master-latest-linux64-gpl-shared.tar.xz\n tar -xvf ffmpeg-master-latest-linux64-gpl-shared.tar.xz\n```\n\n----------------------------------------\n\nTITLE: Installing ModelScope and FunASR\nDESCRIPTION: This snippet shows how to install ModelScope and FunASR using pip.  It also includes an alternative installation command for users in China using a mirror site.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -U modelscope funasr\n# For the users in China, you could install with the command:\n#pip install -U funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Setting Build Options Based on Toolchain File Presence in CMake\nDESCRIPTION: Determines whether to build Python bindings and tests based on whether a toolchain file is provided. When cross-compiling (using a toolchain file), Python and tests are disabled by default.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_TOOLCHAIN_FILE)\n  set(_BUILD_PYTHON OFF)\n  set(_BUILD_TESTS OFF)\nelse()\n  set(_BUILD_PYTHON ON)\n  set(_BUILD_TESTS ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Java Client Usage Command\nDESCRIPTION: Command-line example to initiate Java-based WebSocket client for offline transcription, specifying server host, port, audio input file, and mode.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nFunasrWsClient --host localhost --port 10095 --audio_in ./asr_example.wav --mode offline\n```\n\n----------------------------------------\n\nTITLE: FunASR Model Training - Fine-tuning Script\nDESCRIPTION: This snippet demonstrates how to execute a fine-tuning script for model training using shell commands. It navigates to the specified directory and executes the `finetune.sh` script. Requires a shell environment and the `finetune.sh` script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/industrial_data_pretraining/paraformer\nbash finetune.sh\n# \"log_file: ./outputs/log.txt\"\n```\n\n----------------------------------------\n\nTITLE: Model Inference from Python with Configuration\nDESCRIPTION: This Python code performs model inference using a trained model, assuming that a configuration.json file is present in the model directory. It initializes the AutoModel class and generates the output.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"./model_dir\")\n\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Update FunASR Service Port Configuration\nDESCRIPTION: This command updates the host or Docker port number used by the FunASR service. The service will be restarted for the change to take effect.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update [--host_port | --docker_port] <port number>\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Setup in FunASR\nDESCRIPTION: Configuration for distributed training with multiple GPUs on a single machine using PyTorch's torchrun utility to parallelize model training.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\ngpu_num=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\n\ntorchrun --nnodes 1 --nproc_per_node ${gpu_num} \\\n../../../funasr/bin/train.py ${train_args}\n```\n\n----------------------------------------\n\nTITLE: Installing funasr via pip (Shell)\nDESCRIPTION: Command to install or upgrade the funasr library using pip3. An alternative command is provided for users in China using a mirror URL for faster downloads.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/installation.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U funasr\n# For the users in China, you could install with the command:\n# pip3 install -U funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Debugging Build Information - CMake\nDESCRIPTION: Prints diagnostic messages during the CMake configuration process. It displays the current value of the `CMAKE_CXX_FLAGS` variable and iterates through and prints all configured include directories, which is helpful for verifying the build environment setup.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(\"CXX_FLAGS \"${CMAKE_CXX_FLAGS})\n# Ëé∑ÂèñÈ°πÁõÆ‰∏≠ÊâÄÊúâÂåÖÂê´Êñá‰ª∂Â§πÁöÑË∑ØÂæÑ\nget_property(includes DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES)\n# ÈÅçÂéÜÂπ∂ËæìÂá∫ÊØè‰∏™ÂåÖÂê´Êñá‰ª∂Â§πÁöÑË∑ØÂæÑ\nforeach(include ${includes})\n  message(\"Include directory: ${include}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Killing a Process by PID\nDESCRIPTION: This command demonstrates how to forcefully terminate a process using its Process ID (PID). The `kill -9` command sends the `SIGKILL` signal, which cannot be caught or ignored.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_en.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkill -9 PID\n```\n\n----------------------------------------\n\nTITLE: Install Docker\nDESCRIPTION: This shell script installs Docker on a server.  It downloads and executes the installation script. Requires curl and sudo privileges.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.shÔºõ\nsudo bash install_docker.sh\n```\n\n----------------------------------------\n\nTITLE: Preparation of Training Data from wav.scp and text.txt to JSONL Format - Shell\nDESCRIPTION: This command uses the scp2jsonl tool to generate training and validation JSONL files from wav.scp and text.txt files, which contain audio file paths and transcript data respectively. The parameters +scp_file_list and +data_type_list specify source files and their data types, and +jsonl_file_out sets the output path for JSONL format. This step is essential for preparing data compatible with FunASR training.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Non-real-time Voice Activity Detection with FunASR\nDESCRIPTION: Demonstrates using the FSMN-VAD model for offline voice activity detection to identify speech segments in an audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on CentOS using Shell (Aliyun Mirror)\nDESCRIPTION: Downloads and pipes the official Docker installation script directly to bash for execution on CentOS systems, specifying the Aliyun mirror for faster downloads.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n```\n\n----------------------------------------\n\nTITLE: Timestamp Prediction in FunASR\nDESCRIPTION: Example of using the FA-ZH model for timestamp prediction to align text with timestamps in audio recordings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Downloading ffmpeg for Windows\nDESCRIPTION: Obtaining the ffmpeg library package for Windows x64, necessary for multimedia processing tasks in the FunASR environment.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nhttps://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/ffmpeg-master-latest-win64-gpl-shared.zip\n```\n\n----------------------------------------\n\nTITLE: Opening the HTML Client Interface\nDESCRIPTION: Instruction to open the index.html file in a web browser to access the web-based client with microphone support and file upload capabilities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Explaining Docker Port Mapping and Volume Mounting\nDESCRIPTION: Provides an explanation of the `-p` and `-v` flags used in the `docker run` command. `-p` maps a host port to a container port, and `-v` mounts a host directory into the container.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n-p <host port>:<mapped docker port>: In the example, host machine (ECS) port 10095 is mapped to port 10095 in the Docker container. Make sure that port 10095 is open in the ECS security rules.\n\n-v <host path>:<mounted Docker path>: In the example, the host machine path /root is mounted to the Docker path /workspace/models.\n```\n\n----------------------------------------\n\nTITLE: Running Inverse Text Normalization (ITN) in Indonesian\nDESCRIPTION: This snippet demonstrates how to use the inverse_normalize.py script to perform Inverse Text Normalization on Indonesian text. It takes an input file, specifies a cache directory for the model, defines an output file, and sets the language parameter to Indonesian (id).\nSOURCE: https://github.com/modelscope/funasr/blob/main/fun_text_processing/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ntest_file=fun_text_processing/inverse_text_normalization/id/id_itn_test_input.txt\n\npython fun_text_processing/inverse_text_normalization/inverse_normalize.py --input_file $test_file --cache_dir ./itn_model/ --output_file output.txt --language=id\n```\n\n----------------------------------------\n\nTITLE: Command-line Training with FunASR\nDESCRIPTION: This shell command demonstrates how to perform training using the FunASR command-line interface. It specifies the model, training dataset, validation dataset, and output directory. This method is intended for quick testing and not recommended for serious training.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-train ++model=paraformer-zh ++train_data_set_list=data/list/train.jsonl ++valid_data_set_list=data/list/val.jsonl ++output_dir=\"./outputs\" &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: Building FunASR HTTP Runtime from Source - Shell\nDESCRIPTION: This shell snippet clones the FunASR repository, enters the HTTP runtime directory, creates a build directory, configures the build using CMake with paths to required onnxruntime and ffmpeg binaries, and builds the runtime using make. Prerequisites are git, cmake, and make installed, as well as specifying the actual paths for ONNXRUNTIME_DIR and FFMPEG_DIR. The output is the compiled FunASR HTTP server binary in the build directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git && cd FunASR/runtime/http\nmkdir build && cd build\ncmake  -DCMAKE_BUILD_TYPE=release .. -DONNXRUNTIME_DIR=/path/to/onnxruntime-linux-x64-1.14.0 -DFFMPEG_DIR=/path/to/ffmpeg-master-latest-linux64-gpl-shared\nmake -j 4\n```\n\n----------------------------------------\n\nTITLE: Run Java Client for Offline Transcription\nDESCRIPTION: This command runs the Java client for offline audio transcription. It specifies the host, port, audio input, and mode. Requires the `FunasrWsClient` class to be available in the classpath.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nFunasrWsClient --host localhost --port 10095 --audio_in ./asr_example.wav --mode offline\n```\n\n----------------------------------------\n\nTITLE: Releasing the FunASR Service\nDESCRIPTION: Command to completely remove the deployed FunASR environment and associated resources.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh remove\n```\n\n----------------------------------------\n\nTITLE: Adding 'csrc' Subdirectory in CMake\nDESCRIPTION: This command unconditionally adds the 'csrc' directory to the build process, allowing its CMakeLists.txt to define targets and sources for core components.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/kaldi-native-fbank/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(csrc)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenSSL dependencies for CentOS\nDESCRIPTION: Shell command to install the OpenSSL development library for CentOS Linux systems as a prerequisite for building the FunASR runtime with SSL support.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum -y install openssl-devel\n```\n\n----------------------------------------\n\nTITLE: Compiling TLG.fst graph for ASR decoding in FunASR\nDESCRIPTION: Compiles the finite state transducer (FST) components required for speech recognition decoding. Creates lexicon FSTs, token FSTs, and the final TLG.fst graph, followed by collecting all necessary resource files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/lm_train_tutorial.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Compile the lexicon and token FSTs\nfst/compile_dict_token.sh  lm lm/tmp lm/lang\n\n# Compile the language-model FST and the final decoding graph TLG.fst\nfst/make_decode_graph.sh lm lm/lang || exit 1;\n\n# Collect resource files required for decoding\nfst/collect_resource_file.sh lm lm/resource\n\n#ÁºñËØëÂêéÁöÑÊ®°ÂûãËµÑÊ∫ê‰Ωç‰∫é lm/resource\n```\n\n----------------------------------------\n\nTITLE: Visualizing with TensorBoard - Shell\nDESCRIPTION: This shell command starts TensorBoard, allowing for visualization of training metrics from the specified log directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\ntensorboard --logdir /xxxx/FunASR/examples/industrial_data_pretraining/paraformer/outputs/log/tensorboard\n```\n\n----------------------------------------\n\nTITLE: AutoModel Class Definition in FunASR\nDESCRIPTION: Definition of the AutoModel class constructor in FunASR, including parameters for model selection, device configuration, and inference options.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Restart Deployed FunASR Service\nDESCRIPTION: This command uses the deployment script with superuser privileges to restart the FunASR service based on its last configured settings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh restart\n```\n\n----------------------------------------\n\nTITLE: Non-Real-Time Voice Activity Detection in FunASR\nDESCRIPTION: Example of using the FSMN-VAD model for non-real-time voice activity detection to identify speech segments in audio recordings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README_zh.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Download Client Samples\nDESCRIPTION: This command downloads the client sample files as a compressed tarball from a specified URL.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/sample/funasr_samples.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Starting Real-time ASR Server (Docker)\nDESCRIPTION: Shell command executed inside the Docker container to start the funasr-wss-server-2pass program. It uses nohup to run in the background and requires paths to various models (VAD, ASR, online ASR, Punc, ITN, Hotword) to be specified.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/quick_start.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ncd FunASR/runtime\nnohup bash run_server_2pass.sh \\\n  --download-model-dir /workspace/models \\\n  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \\\n  --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx  \\\n  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  \\\n  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx \\\n  --itn-dir thuduj12/fst_itn_zh \\\n  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Viewing Registered Models in FunASR\nDESCRIPTION: Demonstrates how to invoke tables.print() to display all registered models and components, facilitating inspection and management within FunASR.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/Tables_zh.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\n tables.print()\n# or for specific category: tables.print(\"model\")\n```\n\n----------------------------------------\n\nTITLE: Export Model from Python - Python\nDESCRIPTION: This Python code exports a model using the AutoModel class. It initializes the model and calls the export function, specifying whether to quantize the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies for FunASR\nDESCRIPTION: Installs the required Python packages for building FunASR documentation using pip. This uses the docs extra to pull in all documentation-related dependencies.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip3 install -U \"funasr[docs]\"\n```\n\n----------------------------------------\n\nTITLE: Exporting Model - Shell\nDESCRIPTION: This shell command exports a FunASR model, allowing you to specify the model name and whether to apply quantization.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false\n```\n\n----------------------------------------\n\nTITLE: Exporting FunASR Models from Command Line and Python\nDESCRIPTION: Commands for exporting FunASR models for deployment. Shows both command-line and Python approaches for exporting models, with options to control quantization.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nfunasr-export ++model=paraformer ++quantize=false\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\")\n\nres = model.export(quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Configuring Output Directories and RPATH Settings in CMake\nDESCRIPTION: Sets up output directories for libraries, archives, and executables, and configures RPATH settings for library loading. Handles platform-specific differences between Apple and non-Apple platforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/lib\")\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/lib\")\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/bin\")\n\nset(CMAKE_SKIP_BUILD_RPATH FALSE)\nset(BUILD_RPATH_USE_ORIGIN TRUE)\nset(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)\n\nif(NOT APPLE)\n  set(kaldi_native_fbank_rpath_origin \"$ORIGIN\")\nelse()\n  set(kaldi_native_fbank_rpath_origin \"@loader_path\")\nendif()\n\nset(CMAKE_INSTALL_RPATH ${kaldi_native_fbank_rpath_origin})\nset(CMAKE_BUILD_RPATH ${kaldi_native_fbank_rpath_origin})\n```\n\n----------------------------------------\n\nTITLE: Remove Deployed FunASR Service\nDESCRIPTION: This command uses the deployment script with superuser privileges to release or remove the deployed FunASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh remove\n```\n\n----------------------------------------\n\nTITLE: Starting the Docker Service using Shell\nDESCRIPTION: Starts the Docker daemon service using systemctl, which is common on Linux systems with systemd. Requires sudo privileges.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/installation/docker.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nsudo systemctl start docker\n```\n\n----------------------------------------\n\nTITLE: Defining and Installing the fstscript Library in CMake\nDESCRIPTION: This CMake script snippet first uses `file(GLOB ...)` to find all header files (`.h`) in the specified directory and stores them in the `HEADER_FILES` variable. It then defines a library named `fstscript` using a comprehensive list of C++ source files (`.cc`) and the collected header files. The `target_link_libraries` command links `fstscript` privately against the `fst` library. Finally, it sets the `SOVERSION` property for shared library versioning and uses `install(TARGETS ...)` to specify that the compiled library (shared, static, and runtime versions) should be installed into the 'lib' directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/script/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB HEADER_FILES ../include/fst/script/*.h)\nmessage(STATUS \"${HEADER_FILES}\")\n\nadd_library(fstscript\n  arciterator-class.cc\n  arcsort.cc\n  closure.cc\n  compile.cc\n  compose.cc\n  concat.cc\n  connect.cc\n  convert.cc\n  decode.cc\n  determinize.cc\n  difference.cc\n  disambiguate.cc\n  draw.cc\n  encode.cc\n  encodemapper-class.cc\n  epsnormalize.cc\n  equal.cc\n  equivalent.cc\n  fst-class.cc\n  getters.cc\n  info.cc\n  info-impl.cc\n  intersect.cc\n  invert.cc\n  isomorphic.cc\n  map.cc\n  minimize.cc\n  print.cc\n  project.cc\n  prune.cc\n  push.cc\n  randequivalent.cc\n  randgen.cc\n  relabel.cc\n  replace.cc\n  reverse.cc\n  reweight.cc\n  rmepsilon.cc\n  shortest-distance.cc\n  shortest-path.cc\n  stateiterator-class.cc\n  synchronize.cc\n  text-io.cc\n  topsort.cc\n  union.cc\n  weight-class.cc\n  verify.cc\n  ${HEADER_FILES}\n)\ntarget_link_libraries(fstscript PRIVATE fst)\n\nset_target_properties(fstscript PROPERTIES\n  SOVERSION \"${SOVERSION}\"\n)\ninstall(TARGETS fstscript\n  LIBRARY DESTINATION lib\n  ARCHIVE DESTINATION lib\n  RUNTIME DESTINATION lib)\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR from SJTU Mirror - Shell\nDESCRIPTION: Provides an alternative installation method for the FunASR library using pip, specifying the SJTU mirror as the package index source. This is typically used by users in China for faster downloads.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install -U funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Installation Directories in CMake\nDESCRIPTION: This snippet configures installation directory variables (RUNTIME_INSTALL_DIR, LIBRARY_INSTALL_DIR, INCLUDE_INSTALL_DIR, CONFIG_INSTALL_DIR, PKGCONFIG_INSTALL_DIR) based on the operating system. Windows uses predefined 'bin', 'lib', 'include', while other systems (like Linux) allow customization, particularly for the library directory using GFLAGS_LIBRARY_INSTALL_DIR or LIB_INSTALL_DIR with a potential LIB_SUFFIX.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\n# installation directories\nif (OS_WINDOWS)\n  set (RUNTIME_INSTALL_DIR \"bin\")\n  set (LIBRARY_INSTALL_DIR \"lib\")\n  set (INCLUDE_INSTALL_DIR \"include\")\n  set (CONFIG_INSTALL_DIR  \"lib/cmake/${PACKAGE_NAME}\")\n  set (PKGCONFIG_INSTALL_DIR)\nelse ()\n  set (RUNTIME_INSTALL_DIR bin)\n  # The LIB_INSTALL_DIR and LIB_SUFFIX variables are used by the Fedora\n  # package maintainers. Also package maintainers of other distribution\n  # packages need to be able to specify the name of the library directory.\n  if (NOT GFLAGS_LIBRARY_INSTALL_DIR AND LIB_INSTALL_DIR)\n    set (GFLAGS_LIBRARY_INSTALL_DIR \"${LIB_INSTALL_DIR}\")\n  endif ()\n  gflags_define (PATH LIBRARY_INSTALL_DIR \"Directory of installed libraries, e.g., \\\"lib64\\\"\" \"lib${LIB_SUFFIX}\")\n  gflags_property (LIBRARY_INSTALL_DIR ADVANCED TRUE)\n  set (INCLUDE_INSTALL_DIR include)\n  set (CONFIG_INSTALL_DIR  ${LIBRARY_INSTALL_DIR}/cmake/${PACKAGE_NAME})\n  set (PKGCONFIG_INSTALL_DIR ${LIBRARY_INSTALL_DIR}/pkgconfig)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Training ARPA language models using SRILM in FunASR\nDESCRIPTION: Executes the language model training script to generate ARPA format models using SRILM toolkit. Requires specific text format with utterance IDs and transcriptions as shown in the comments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/lm_train_tutorial.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# make sure that srilm is installed\n# the format of the text should be:\n# BAC009S0002W0122 ËÄå ÂØπ Ê•ºÂ∏Ç Êàê‰∫§ ÊäëÂà∂ ‰ΩúÁî® ÊúÄ Â§ß ÁöÑ Èôê Ë¥≠\n# BAC009S0002W0123 ‰πü Êàê‰∏∫ Âú∞Êñπ ÊîøÂ∫ú ÁöÑ Áúº‰∏≠ Èíâ\n\nbash fst/train_lms.sh\n```\n\n----------------------------------------\n\nTITLE: Optimizing ONNX Model - Shell\nDESCRIPTION: This shell command uses onnxslim to optimize an ONNX model file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\n# pip3 install -U onnxslim\nonnxslim model.onnx model.onnx\n```\n\n----------------------------------------\n\nTITLE: Print Registration Tables - Python\nDESCRIPTION: This Python code prints the registration tables, showing available model classes. It uses the funasr.register.tables module.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README_zh.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\ntables.print()\n```\n\n----------------------------------------\n\nTITLE: Real-Time (Streaming) Speech Recognition Chunked Processing - Python\nDESCRIPTION: This block illustrates streaming ASR inference by chunking an input WAV file and running each piece through an AutoModel. The chunk_size, encoder_chunk_look_back, and decoder_chunk_look_back control streaming window and context. Dependencies include funasr, numpy, and soundfile for audio I/O. Input is divided into frames/chunks for low-latency recognition. Results are printed per chunk, and caching enables continuity across segments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n----------------------------------------\n\nTITLE: Link GFlags Library\nDESCRIPTION: If `gflags_FOUND` is true, indicating the gflags library was found, it links `gflags` publicly to `glog`. It prioritizes the modern `gflags::gflags` target if available, otherwise falls back to the older `gflags` target. It also appends the `-lgflags` flag to the static linking options variable.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif (gflags_FOUND)\n  # Prefer the gflags target that uses double colon convention\n  if (TARGET gflags::gflags)\n    target_link_libraries (glog PUBLIC gflags::gflags)\n  else (TARGET gflags::gflags)\n    target_link_libraries (glog PUBLIC gflags)\n  endif (TARGET gflags::gflags)\n\n  set (glog_libraries_options_for_static_linking \"${glog_libraries_options_for_static_linking} -lgflags\")\nendif (gflags_FOUND)\n```\n\n----------------------------------------\n\nTITLE: Conversion from JSONL Format Back to wav.scp and text.txt - Shell\nDESCRIPTION: This optional command converts training JSONL files back into separate wav.scp and text.txt files using the jsonl2scp tool. It requires the input JSONL file and lists of files and their data types. This reverses the data preparation step and can be used for data inspection or preprocessing reuse.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/tutorial/README.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\njsonl2scp \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_in=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Inference with Models Lacking Configuration Files in FunASR\nDESCRIPTION: Command for running inference with FunASR models that don't have configuration.json. Requires manual specification of configuration paths, model parameters, token lists, and other necessary files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython -m funasr.bin.inference \\\n--config-path \"${local_path}\" \\\n--config-name \"${config}\" \\\n++init_param=\"${init_param}\" \\\n++tokenizer_conf.token_list=\"${tokens}\" \\\n++frontend_conf.cmvn_file=\"${cmvn_file}\" \\\n++input=\"${input}\" \\\n++output_dir=\"${output_dir}\" \\\n++device=\"${device}\"\n```\n\n----------------------------------------\n\nTITLE: Real-time Mode Server Response Format in FunASR WebSocket Protocol\nDESCRIPTION: JSON message format for server responses in real-time recognition mode. Contains the recognition results, mode information (online or offline correction), and timestamp data if available.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/websocket_protocol_zh.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n{\"mode\": \"2pass-online\", \"wav_name\": \"wav_name\", \"text\": \"asr ouputs\", \"is_final\": True, \"timestamp\":\"[[100,200], [200,500]]\",\"stamp_sents\":[]}\n```\n\n----------------------------------------\n\nTITLE: Structuring AliMeeting Dataset Directory\nDESCRIPTION: Shows the required directory structure for the AliMeeting corpus within the `./dataset` folder. This layout, containing the unpacked Train, Eval, and Test sets (far and near field), is a prerequisite for executing the `run.sh` script for training and evaluation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/m2met2/Baseline.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndataset\n|‚Äî‚Äî Eval_Ali_far\n|‚Äî‚Äî Eval_Ali_near\n|‚Äî‚Äî Test_Ali_far\n|‚Äî‚Äî Test_Ali_near\n|‚Äî‚Äî Train_Ali_far\n|‚Äî‚Äî Train_Ali_near\n```\n\n----------------------------------------\n\nTITLE: Configure CMake Properties for Subprojects (CMake)\nDESCRIPTION: Adjusts the ADVANCED property for standard CMake variables like installation prefix and configuration types. This block runs only if gflags is *not* being built as a subproject, setting common installation variables to non-advanced and build configuration variables to advanced.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT GFLAGS_IS_SUBPROJECT)\n  foreach (varname IN ITEMS CMAKE_INSTALL_PREFIX)\n    gflags_property (${varname} ADVANCED FALSE)\n  endforeach ()\n  foreach (varname IN ITEMS CMAKE_CONFIGURATION_TYPES CMAKE_OSX_ARCHITECTURES CMAKE_OSX_DEPLOYMENT_TARGET CMAKE_OSX_SYSROOT)\n    gflags_property (${varname} ADVANCED TRUE)\n  endforeach ()\n  if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CXX_FLAGS)\n    gflags_set (CMAKE_BUILD_TYPE Release)\n  endif ()\n  if (CMAKE_CONFIGURATION_TYPES)\n    gflags_property (CMAKE_BUILD_TYPE STRINGS \"${CMAKE_CONFIGURATION_TYPES}\")\n  endif ()\nendif () # NOT GFLAGS_IS_SUBPROJECT\n```\n\n----------------------------------------\n\nTITLE: Generate Jsonl from Scp/Text - Shell\nDESCRIPTION: Command using the `scp2jsonl` utility provided by FunASR to convert pairs of `wav.scp` and `text.txt` files into a single `jsonl` format file. This `jsonl` file is the standard input format for FunASR training. It requires specifying the input file paths, corresponding data types (\"source\" for audio, \"target\" for text), and the output jsonl file path.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nscp2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Downloading FunASR Client Test Tools\nDESCRIPTION: Command to download the client test tools for FunASR, which include sample code for different programming languages.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_zh.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/sample/funasr_samples.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies (Linux) - Shell\nDESCRIPTION: Installs required system libraries, specifically OpenBLAS and OpenSSL development headers, which are prerequisites for building the FunASR runtime on Linux. Commands are provided for both Ubuntu (using apt-get) and CentOS (using yum) distributions, requiring root privileges (`sudo`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/readme.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# openblas\nsudo apt-get install libopenblas-dev #ubuntu\n# sudo yum -y install openblas-devel #centos\n\n# openssl\napt-get install libssl-dev #ubuntu \n# yum install openssl-devel #centos\n```\n\n----------------------------------------\n\nTITLE: Installing the fstlookahead Library with CMake\nDESCRIPTION: This command specifies the installation rules for the `fstlookahead` target created earlier. It instructs CMake to install the shared library (`LIBRARY`), static library/import library (`ARCHIVE`), and any associated runtime components (e.g., DLLs on Windows) into the `lib` directory relative to the installation prefix (`CMAKE_INSTALL_PREFIX`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/lookahead/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS fstlookahead \n\tLIBRARY DESTINATION lib\n\tARCHIVE DESTINATION lib\n\tRUNTIME DESTINATION lib\n)\n```\n\n----------------------------------------\n\nTITLE: Commands to Manage FunASR Service Lifecycle Using Deployment Script\nDESCRIPTION: These shell snippets provide commands to start, stop, restart, and remove the FunASR offline transcription service deployed via the provided shell script. They enable basic service lifecycle management after initial installation, ensuring easy control of the Dockerized ASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh start\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh stop\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh remove\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-zh.sh restart\n```\n\n----------------------------------------\n\nTITLE: Downloading onnxruntime for Windows x64\nDESCRIPTION: Downloading the precompiled onnxruntime library suitable for Windows x64 systems, useful for building the FunASR runtime on Windows platforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nhttps://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/onnxruntime-win-x64-1.16.1.zip\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Linux (Shell)\nDESCRIPTION: Installs required system development libraries (OpenBLAS and OpenSSL) using the system package manager. Commands are provided for Ubuntu (`apt-get`) and commented out for CentOS (`yum`). These libraries are prerequisites for compiling the FunASR runtime.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n# openblas\nsudo apt-get install libopenblas-dev #ubuntu\n# sudo yum -y install openblas-devel #centos\n\n# openssl\napt-get install libssl-dev #ubuntu \n# yum install openssl-devel #centos\n```\n\n----------------------------------------\n\nTITLE: Example Training Text Data\nDESCRIPTION: Example training text data in a simple text file format.  Each line represents a transcription.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nID0012W0013 ÂΩìÂÆ¢Êà∑È£éÈô©ÊâøÂèóËÉΩÂäõËØÑ‰º∞‰æùÊçÆÂèëÁîüÂèòÂåñÊó∂\nID0012W0014 ÊâÄÊúâÂè™Ë¶ÅÂ§ÑÁêÜ data ‰∏çÁÆ°‰Ω†ÊòØÂÅö machine learning ÂÅö deep learning\nID0012W0015 he tried to think how it could be\n```\n\n----------------------------------------\n\nTITLE: Printing Registration Tables - Python\nDESCRIPTION: This Python code snippet demonstrates how to print the registration tables using the funasr.register module.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr.register import tables\n\ntables.print()\n```\n\n----------------------------------------\n\nTITLE: Exporting Targets for Build Tree Usage in CMake\nDESCRIPTION: This snippet enables using the gflags library directly from its build directory without installation. It uses the `export` command to write CMake files (`<package>-targets.cmake` and `<package>-nonamespace-targets.cmake`) containing information about the built library targets (`TARGETS`) into the build directory (`PROJECT_BINARY_DIR`). It also configures the main `<package>-config.cmake` file for the build tree and optionally registers the build directory or installation prefix with the CMake user package registry.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# support direct use of build tree\nset (INSTALL_PREFIX_REL2CONFIG_DIR .)\nexport (\n  TARGETS ${TARGETS}\n  NAMESPACE ${PACKAGE_NAME}::\n  FILE \"${PROJECT_BINARY_DIR}/${EXPORT_NAME}.cmake\"\n)\nexport (\n  TARGETS ${TARGETS}\n  FILE \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-nonamespace-targets.cmake\"\n)\nif (REGISTER_BUILD_DIR)\n  export (PACKAGE ${PACKAGE_NAME})\nendif ()\nif (REGISTER_INSTALL_PREFIX)\n  register_gflags_package(${CONFIG_INSTALL_DIR})\nendif ()\nconfigure_file (cmake/config.cmake.in \"${PROJECT_BINARY_DIR}/${PACKAGE_NAME}-config.cmake\" @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Configuring build for Windows using CMake\nDESCRIPTION: Commands to set up the build environment for the FunASR runtime on Windows systems with CMake, specifying paths for OpenSSL, ffmpeg, and onnxruntime libraries. This prepares the environment for compiling on Windows with Visual Studio.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/readme_zh.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba-damo-academy/FunASR.git\n cd FunASR/runtime/websocket\n mkdir build\n cd build\n cmake ../ -D OPENSSL_ROOT_DIR=d:/openssl-1.1.1w -D FFMPEG_DIR=d:/ffmpeg-master-latest-win64-gpl-shared -D ONNXRUNTIME_DIR=d:/onnxruntime-win-x64-1.16.1\n```\n\n----------------------------------------\n\nTITLE: Offline Voice Activity Detection Inference - Python\nDESCRIPTION: This snippet shows how to load a VAD model with AutoModel and perform offline endpoint detection on a WAV file. Results indicate speech regions within the file, returned as start/end points in milliseconds. Dependencies include funasr and the appropriate VAD model weights. Input must be a detectable audio file; output is a matrix of speech regions.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer-zh-spk/README_zh.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Download FunASR Samples\nDESCRIPTION: This shell command downloads the FunASR samples archive from a remote location.  It uses `wget` to retrieve the file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu_zh.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/sample/funasr_samples.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Defining FunASR ONNX Executable Targets with Link Options in CMake\nDESCRIPTION: This snippet includes multiple executable target definitions for different FunASR ONNX applications such as offline ASR, online ASR with VAD, punctuation, and 2-pass decoding. Each target combines a main source file with commonly shared source files (RELATION_SOURCE), applies a linker flag to prevent unused code removal (-Wl,--no-as-needed), and links against the 'funasr' library. The pattern systematically organizes various configurations of the ASR pipeline for efficient building.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/bin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(funasr-onnx-offline \"funasr-onnx-offline.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-offline PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-offline PUBLIC funasr)\n\nadd_executable(funasr-onnx-offline-vad \"funasr-onnx-offline-vad.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-offline-vad PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-offline-vad PUBLIC funasr)\n\nadd_executable(funasr-onnx-online-vad \"funasr-onnx-online-vad.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-online-vad PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-online-vad PUBLIC funasr)\n\nadd_executable(funasr-onnx-online-asr \"funasr-onnx-online-asr.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-online-asr PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-online-asr PUBLIC funasr)\n\nadd_executable(funasr-onnx-offline-punc \"funasr-onnx-offline-punc.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-offline-punc PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-offline-punc PUBLIC funasr)\n\nadd_executable(funasr-onnx-online-punc \"funasr-onnx-online-punc.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-online-punc PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-online-punc PUBLIC funasr)\n\nadd_executable(funasr-onnx-offline-rtf \"funasr-onnx-offline-rtf.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-offline-rtf PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-offline-rtf PUBLIC funasr)\n\nadd_executable(funasr-onnx-2pass \"funasr-onnx-2pass.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-2pass PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-2pass PUBLIC funasr)\n\nadd_executable(funasr-onnx-2pass-rtf \"funasr-onnx-2pass-rtf.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-2pass-rtf PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-2pass-rtf PUBLIC funasr)\n\nadd_executable(funasr-onnx-online-rtf \"funasr-onnx-online-rtf.cpp\" ${RELATION_SOURCE})\ntarget_link_options(funasr-onnx-online-rtf PRIVATE \"-Wl,--no-as-needed\")\ntarget_link_libraries(funasr-onnx-online-rtf PUBLIC funasr)\n```\n\n----------------------------------------\n\nTITLE: Installing gflags Library on Debian/Ubuntu Linux Using Shell\nDESCRIPTION: This snippet demonstrates how to install the gflags library via the system package manager on Debian or Ubuntu Linux distributions. It shows the command for installing the 'libgflags-dev' package, which contains the necessary header files and libraries. No additional dependencies are required beyond having 'apt-get' available. The input is a shell command executed by the user, and the output is the successful installation of the gflags development files on the system. This method provides a quick installation without building from source.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/INSTALL.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nsudo apt-get install libgflags-dev\n```\n\n----------------------------------------\n\nTITLE: Adding Task Arguments (Python)\nDESCRIPTION: This code snippet shows how to add specific arguments required by a speech recognition task.  It defines a group within an argument parser to categorize task-related options. It specifically adds a `--token_list` argument used to map integer IDs to tokens within the speech recognition model. This configuration allows for customization based on the dataset and specific model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/docs/reference/build_task.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef add_task_arguments(cls, parser: argparse.ArgumentParser):\n    group = parser.add_argument_group(description=\"Task related\")\n    group.add_argument(\n        \"--token_list\",\n        type=str_or_none,\n        default=None,\n        help=\"A text mapping int-id to token\",\n    )\n    (...)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Include Directories and Windows Compiler Options in CMake\nDESCRIPTION: This snippet configures include directories for the project and, if on Windows, adds MSVC-specific compiler options to set source and execution charset to UTF-8. It also includes platform-dependent directories for ONNX Runtime, FFMPEG, and third-party dependencies. A list of related source files is defined to be linked with executables conditionally.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/bin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${CMAKE_SOURCE_DIR}/include)\n\nif(WIN32)\nadd_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/execution-charset:utf-8>\")\nadd_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/source-charset:utf-8>\")\ninclude_directories(${ONNXRUNTIME_DIR}/include)\ninclude_directories(${FFMPEG_DIR}/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/third_party)\nSET(RELATION_SOURCE \"../src/resample.cpp\" \"../src/util.cpp\" \"../src/alignedmem.cpp\" \"../src/encode_converter.cpp\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running RTF Test Script\nDESCRIPTION: This snippet executes the test_rtf.sh script in the background and redirects both standard output and standard error to the log.txt file. Users need to set the model, data path, and output directory within the script.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_onnx.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnohup bash test_rtf.sh &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: SenseVoice model repository directory structure\nDESCRIPTION: Example directory tree structure for the SenseVoice model repository, showing the organization of model files and configurations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/triton_gpu/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmodel_repo_sense_voice_small\n|-- encoder\n|   |-- 1\n|   |   `-- model.onnx -> /your/path/model.onnx\n|   `-- config.pbtxt\n|-- feature_extractor\n|   |-- 1\n|   |   `-- model.py\n|   |-- am.mvn\n|   |-- config.pbtxt\n|   `-- config.yaml\n|-- scoring\n|   |-- 1\n|   |   `-- model.py\n|   |-- chn_jpn_yue_eng_ko_spectok.bpe.model -> /your/path/chn_jpn_yue_eng_ko_spectok.bpe.model\n|   `-- config.pbtxt\n`-- sensevoice\n    |-- 1\n    `-- config.pbtxt\n\n8 directories, 10 files\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies and Compilation for FunASR HTTP Server with CMake (CMake)\nDESCRIPTION: This CMake snippet configures a project build for the FunASR HTTP server, particularly for WIN32 (Windows) environments. It adds include and link directories for ONNX Runtime, FFMPEG, and OpenSSL, defines special macros required by certain libraries, and applies MSVC-specific compiler options such as \"/bigobj\" and \"/utf-8\". The RELATION_SOURCE variable accumulates paths to required C++ source files under the onnxruntime source tree. This setup ensures that Windows builds correctly locate and compile all needed components with proper settings.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/bin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n  include_directories(${ONNXRUNTIME_DIR}/include)\n  include_directories(${FFMPEG_DIR}/include)\n  include_directories(${OPENSSL_ROOT_DIR}//include)\n  link_directories(${OPENSSL_ROOT_DIR}/lib)\n  add_definitions(-D_WEBSOCKETPP_CPP11_RANDOM_DEVICE_)\n  add_definitions(-D_WEBSOCKETPP_CPP11_TYPE_TRAITS_)\n  add_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/bigobj>\")\n  add_compile_options(\"$<$<CXX_COMPILER_ID:MSVC>:/utf-8>\")\n  SET(RELATION_SOURCE \"../../onnxruntime/src/resample.cpp\" \"../../onnxruntime/src/util.cpp\" \"../../onnxruntime/src/alignedmem.cpp\" \"../../onnxruntime/src/encode_converter.cpp\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading ffmpeg Library with Shell Commands\nDESCRIPTION: These commands download and extract the ffmpeg library as a dependency for FunASR. wget retrieves an xz-compressed tcp archive from the specified URL, and tar decompresses and extracts its contents. wget and tar are required; make sure you have permission to write to the current directory. The resulting files are required for audio processing components within FunASR.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/readme_zh.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/ffmpeg-master-latest-linux64-gpl-shared.tar.xz\ntar -xvf ffmpeg-master-latest-linux64-gpl-shared.tar.xz\n```\n\n----------------------------------------\n\nTITLE: Running RTF Benchmark Script - Shell\nDESCRIPTION: Executes the `test_rtf.sh` script in the background using `nohup`, redirecting its standard output and standard error to `log.txt`. This script performs the Real-Time Factor (RTF) benchmark after the user has configured model and data paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/benchmark_libtorch.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnohup bash test_rtf.sh &> log.txt &\n```\n\n----------------------------------------\n\nTITLE: SenseVoice JSONL Generation Command (Full)\nDESCRIPTION: This shell command generates `train.jsonl` and `val.jsonl` files from multiple input files including `wav.scp`, `text.txt`, `text_language.txt`, `emo_target.txt`, and `event_target.txt`. It specifies the input file list and data types to be used during JSONL generation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Python FunASR\nDESCRIPTION: This code snippet provides instructions for installing the necessary dependencies for the Python version of the FunASR speech recognition service. It includes installing modelscope, funasr, and flask using pip3.  Alternative installation method for users in mainland China is provided to address potential network issues.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/html5/readme.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -U modelscope funasr flask\n# Users in mainland China, if encountering network issues, can install with the following command:\n# pip3 install -U modelscope funasr -i https://mirror.sjtu.edu.cn/pypi/web/simple\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\n```\n\n----------------------------------------\n\nTITLE: Pushing changes to origin branch - Git Shell\nDESCRIPTION: This command pushes the committed changes from your local branch to the remote repository associated with your fork (origin). This makes your changes available online, enabling you to create a pull request. Replace 'my-new-feature' with the name of the branch you created.\nSOURCE: https://github.com/modelscope/funasr/blob/main/Contribution.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\ngit push origin my-new-feature\n```\n\n----------------------------------------\n\nTITLE: Viewing Training Logs in FunASR\nDESCRIPTION: Example of training logs in FunASR showing various metrics like loss, accuracy, memory usage and step timing information across different GPUs. Useful for monitoring training progress.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer_streaming/README_zh.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ntail log.txt\n[2024-03-21 15:55:52,137][root][INFO] - train, rank: 3, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.327), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.259), ('acc', 0.825), ('loss_pre', 0.04), ('loss', 0.299), ('batch_size', 40)], {'data_load': '0.000', 'forward_time': '0.315', 'backward_time': '0.555', 'optim_time': '0.076', 'total_time': '0.947'}, GPU, memory: usage: 3.830 GB, peak: 18.357 GB, cache: 20.910 GB, cache_peak: 20.910 GB\n[2024-03-21 15:55:52,139][root][INFO] - train, rank: 1, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.334), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.285), ('acc', 0.823), ('loss_pre', 0.046), ('loss', 0.331), ('batch_size', 36)], {'data_load': '0.000', 'forward_time': '0.334', 'backward_time': '0.536', 'optim_time': '0.077', 'total_time': '0.948'}, GPU, memory: usage: 3.943 GB, peak: 18.291 GB, cache: 19.619 GB, cache_peak: 19.619 GB\n```\n\n----------------------------------------\n\nTITLE: Stopping the FunASR Service\nDESCRIPTION: Command to shut down the active FunASR service gracefully.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh stop\n```\n\n----------------------------------------\n\nTITLE: Shutting down the FunASR service\nDESCRIPTION: This shows how to stop the FunASR service.  First find the process ID and then use the kill command to terminate it.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_advanced_guide_offline_gpu.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n# Check the PID of the funasr-wss-server process\nps -x | grep funasr-wss-server\nkill -9 PID\n```\n\n----------------------------------------\n\nTITLE: Example Training Wave Data\nDESCRIPTION: Example training wave data in a simple text file format with audio URLs.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nID0012W0015 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\n```\n\n----------------------------------------\n\nTITLE: Restarting the FunASR Service with Existing Configuration\nDESCRIPTION: Command to restart the service, used after configuration changes or updates.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh restart\n```\n\n----------------------------------------\n\nTITLE: Example train_wav.scp\nDESCRIPTION: This snippet demonstrates the expected format of the train_wav.scp file, which maps unique IDs to the corresponding audio file paths. This file is essential for locating and accessing the audio files during the training process.  The left side is the unique ID, and the right side is the audio file path.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README_zh.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n\n----------------------------------------\n\nTITLE: Example Training Log Output - Shell\nDESCRIPTION: Example lines from the training log file (`log.txt`) showing the output format during multi-GPU training. Each line typically corresponds to a specific rank (GPU ID) and reports progress (epoch, step), average metrics across ranks/epochs (loss, ppl, acc), current learning rate, per-GPU metrics, performance timings, and GPU memory usage.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/paraformer/README.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ntail log.txt\n[2024-03-21 15:55:52,137][root][INFO] - train, rank: 3, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.327), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.259), ('acc', 0.825), ('loss_pre', 0.04), ('loss', 0.299), ('batch_size', 40)], {'data_load': '0.000', 'forward_time': '0.315', 'backward_time': '0.555', 'optim_time': '0.076', 'total_time': '0.947'}, GPU, memory: usage: 3.830 GB, peak: 18.357 GB, cache: 20.910 GB, cache_peak: 20.910 GB\n[2024-03-21 15:55:52,139][root][INFO] - train, rank: 1, epoch: 0/50, step: 6990/1, total step: 6990, (loss_avg_rank: 0.334), (loss_avg_epoch: 0.409), (ppl_avg_epoch: 1.506), (acc_avg_epoch: 0.795), (lr: 1.165e-04), [('loss_att', 0.285), ('acc', 0.823), ('loss_pre', 0.046), ('loss', 0.331), ('batch_size', 36)], {'data_load': '0.000', 'forward_time': '0.334', 'backward_time': '0.536', 'optim_time': '0.077', 'total_time': '0.948'}, GPU, memory: usage: 3.943 GB, peak: 18.291 GB, cache: 19.619 GB, cache_peak: 19.619 GB\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU-Specific Compilation and Linking Settings (CMake)\nDESCRIPTION: This section enables GPU support by defining the USE_GPU macro, setting include and link directories for PyTorch and Torch Blade, and updating compiler flags to use position independent code and ABI compatibility. It requires a valid Torch and Torch Blade installation accessible via hardcoded paths. Inputs are the GPU option and system file paths; outputs are modified compiler definitions and paths. Limitation: Paths are static and tailored to a specific environment; may require customization.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(GPU)\n    add_definitions(-DUSE_GPU)\n    set(TORCH_DIR \"/usr/local/lib/python3.8/dist-packages/torch\")\n    set(TORCH_BLADE_DIR \"/usr/local/lib/python3.8/dist-packages/torch_blade\")\n    include_directories(${TORCH_DIR}/include)\n    include_directories(${TORCH_DIR}/include/torch/csrc/api/include)\n    link_directories(${TORCH_DIR}/lib)\n    link_directories(${TORCH_BLADE_DIR})\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -D_GLIBCXX_USE_CXX11_ABI=0\")\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Example train_wav.scp - Bash\nDESCRIPTION: This bash script example demonstrates the format for train_wav.scp. It shows how each line contains a unique data ID (matching train_text.txt) followed by the path to the corresponding audio file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/README_zh.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nID0012W0015 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\n```\n\n----------------------------------------\n\nTITLE: Data Prepare Example Text\nDESCRIPTION: Example data for training the model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\n\n----------------------------------------\n\nTITLE: Using Installed gflags in a CMake Project\nDESCRIPTION: Demonstrates how to find and link against a pre-installed gflags library in a user's CMake project. It uses `find_package` to locate gflags and `target_link_libraries` with the `gflags::gflags` target to link the executable.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 2.8.12 FATAL_ERROR)\n\nproject(Foo)\n\nfind_package(gflags REQUIRED)\n\nadd_executable(foo src/foo.cc)\ntarget_link_libraries(foo gflags::gflags)\n```\n\n----------------------------------------\n\nTITLE: Example: Update FunASR ASR Model\nDESCRIPTION: This is an example command demonstrating how to update only the ASR model used by the service to a specific ModelScope model.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --asr_model damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n```\n\n----------------------------------------\n\nTITLE: Building yaml-cpp without CMake\nDESCRIPTION: This snippet describes a makefile based build process for the yaml-cpp project. It suggests adding all .cpp files to a makefile. It doesn't require any special build settings. This approach is discouraged due to its tediousness compared to CMake.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/install.txt#_snippet_1\n\nLANGUAGE: makefile\nCODE:\n```\nAdd all .cpp files to a makefile.\n```\n\n----------------------------------------\n\nTITLE: Example: Disable FunASR Service SSL\nDESCRIPTION: This is an example command demonstrating how to disable SSL certificate verification for the FunASR service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --ssl 0\n```\n\n----------------------------------------\n\nTITLE: Finding and Linking gflags with CMake\nDESCRIPTION: This CMake snippet finds the gflags package, adds an executable named 'foo', and links the gflags library to it. The find_package command locates the gflags library, while target_link_libraries links it to the executable, enabling the use of gflags within the C++ code.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/config/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required (VERSION 2.8.12 FATAL_ERROR)\n\nproject (gflags_${TEST_NAME})\n\nfind_package (gflags REQUIRED)\n\nadd_executable (foo main.cc)\ntarget_link_libraries (foo gflags::gflags)\n```\n\n----------------------------------------\n\nTITLE: Example: Update FunASR Workspace Path\nDESCRIPTION: This is an example command demonstrating how to update the local workspace path for the service.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/docs/SDK_tutorial_en_zh.md#_snippet_17\n\nLANGUAGE: Shell\nCODE:\n```\nsudo bash funasr-runtime-deploy-offline-cpu-en.sh update --workspace /root/funasr-runtime-resources\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Environment and Dependencies with CMake\nDESCRIPTION: This snippet contains the full CMakeLists.txt content, which configures the project settings for the FunASR Websocket component in C++14. It specifies minimum CMake version, compiler options, and sets project-wide options such as enabling HTTP server, PortAudio, glog, and openfst support. It employs conditional logic to handle platform-specific compilation flags and removes conflicting files on Windows. The snippet also uses FetchContent to download and setup third-party libraries like asio, nlohmann json, and PortAudio if they're missing. Additionally, it configures include directories and links required libraries, handling both shared and static builds. Finally, the snippet configures building submodules and external dependencies like yaml-cpp, kaldi-native-fbank, and kaldi, and requires OpenSSL for SSL support.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/http/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.16)\n\nproject(FunASRWebscoket) \n\nset(CMAKE_CXX_STANDARD 14 CACHE STRING \"The C++ version to be used.\")\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\noption(ENABLE_HTTP \"Whether to build http server\" ON)\noption(ENABLE_PORTAUDIO \"Whether to build portaudio\" ON)\n\nif(WIN32)\n  file(REMOVE ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/config.h \\\n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/export.h \\\n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/logging.h \\\n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/raw_logging.h \\\n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/stl_logging.h \\\n    ${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src/glog/vlog_is_on.h)\nelse()\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread -fPIC\")\nendif()\n\n\n\noption(ENABLE_GLOG \"Whether to build glog\" ON)\noption(ENABLE_FST \"Whether to build openfst\" ON) # ITN need openfst compiled\noption(BUILD_SHARED_LIBS \"Build shared libraries\" ON)\n \nif(ENABLE_HTTP)\n  # cmake_policy(SET CMP0135 NEW)\n  include(FetchContent)\n\n \n   \n  if(NOT EXISTS ${PROJECT_SOURCE_DIR}/third_party/asio/asio )\n    FetchContent_Declare(asio\n      URL   https://github.com/chriskohlhoff/asio/archive/refs/tags/asio-1-24-0.tar.gz\n    SOURCE_DIR ${PROJECT_SOURCE_DIR}/third_party/asio\n    )\n    \n    FetchContent_MakeAvailable(asio)\n  endif()\n  include_directories(${PROJECT_SOURCE_DIR}/third_party/asio/asio/include)\n \n  if(NOT EXISTS ${PROJECT_SOURCE_DIR}/third_party/json/ChangeLog.md )\n    FetchContent_Declare(json\n      URL   https://github.com/nlohmann/json/archive/refs/tags/v3.11.2.tar.gz\n    SOURCE_DIR ${PROJECT_SOURCE_DIR}/third_party/json\n    )\n    \n    FetchContent_MakeAvailable(json)\n  endif()\n  include_directories(${PROJECT_SOURCE_DIR}/third_party/json/include)\n\nendif()\n\nif(ENABLE_PORTAUDIO)\n  include(FetchContent)\n\n  set(portaudio_URL  \"http://files.portaudio.com/archives/pa_stable_v190700_20210406.tgz\")\n  set(portaudio_URL2 \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/pa_stable_v190700_20210406.tgz\")\n  set(portaudio_HASH \"SHA256=47efbf42c77c19a05d22e627d42873e991ec0c1357219c0d74ce6a2948cb2def\")\n\n  FetchContent_Declare(portaudio\n    URL\n      ${portaudio_URL}\n      ${portaudio_URL2}\n    URL_HASH          ${portaudio_HASH}\n  )\n\n  FetchContent_GetProperties(portaudio)\n  if(NOT portaudio_POPULATED)\n    message(STATUS \"Downloading portaudio from ${portaudio_URL}\")\n    FetchContent_Populate(portaudio)\n  endif()\n  message(STATUS \"portaudio is downloaded to ${portaudio_SOURCE_DIR}\")\n  message(STATUS \"portaudio's binary dir is ${portaudio_BINARY_DIR}\")\n\n  add_subdirectory(${portaudio_SOURCE_DIR} ${portaudio_BINARY_DIR} EXCLUDE_FROM_ALL)\n  if(NOT WIN32)\n    target_compile_options(portaudio PRIVATE \"-Wno-deprecated-declarations\")\n  else()\n    install(TARGETS portaudio DESTINATION ..)\n  endif()\n\nendif()\n\n# Include generated *.pb.h files\nlink_directories(${ONNXRUNTIME_DIR}/lib)\nlink_directories(${FFMPEG_DIR}/lib)\n\nif(ENABLE_GLOG)\n    include_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog/src)\n    set(BUILD_TESTING OFF)\n    add_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/glog glog)\n    include_directories(${glog_BINARY_DIR})\n\nendif()\n\nif(ENABLE_FST)\n    # fst depend on glog and gflags\n    include_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/gflags)\n    add_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/gflags gflags)\n    include_directories(${gflags_BINARY_DIR}/include)\n      \n    # the following openfst if cloned from https://github.com/kkm000/openfst.git\n    # with some patch to fix the make errors. \n    add_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/openfst openfst)\n    include_directories(${openfst_SOURCE_DIR}/src/include)\n    if(WIN32)\n    include_directories(${openfst_SOURCE_DIR}/src/lib)\n    endif() \nendif()\n\n\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/include/)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/src)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/yaml-cpp/include/)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi-native-fbank)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/jieba/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/jieba/include/limonp/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi)\n\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/yaml-cpp yaml-cpp)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi-native-fbank/kaldi-native-fbank/csrc csrc)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/src src)\nadd_subdirectory(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi kaldi)\n\n# install openssl first apt-get install libssl-dev\nfind_package(OpenSSL REQUIRED)\n\nmessage(\"CXX_FLAGS \"${CMAKE_CXX_FLAGS})\n# Ëé∑ÂèñÈ°πÁõÆ‰∏≠ÊâÄÊúâÂåÖÂê´Êñá‰ª∂Â§πÁöÑË∑ØÂæÑ\nget_property(includes DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES)\n# ÈÅçÂéÜÂπ∂ËæìÂá∫ÊØè‰∏™ÂåÖÂê´Êñá‰ª∂Â§πÁöÑË∑ØÂæÑ\nforeach(include ${includes})\n  message(\"Include directory: ${include}\")\nendforeach()\n\nadd_subdirectory(bin)\n```\n\n----------------------------------------\n\nTITLE: Generating and Installing pkg-config File on Unix\nDESCRIPTION: This CMake code conditionally generates and installs a pkg-config file (`.pc`) if the build system is Unix-based (`if(UNIX)`). It uses `configure_file` to create `yaml-cpp.pc` from a template (`yaml-cpp.pc.cmake`) and then installs it to the standard pkg-config directory (`${LIB_INSTALL_DIR}/pkgconfig`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(UNIX)\n\tset(PC_FILE ${CMAKE_BINARY_DIR}/yaml-cpp.pc)\n\tconfigure_file(\"yaml-cpp.pc.cmake\" ${PC_FILE} @ONLY)\n\tinstall(FILES ${PC_FILE} DESTINATION ${LIB_INSTALL_DIR}/pkgconfig)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Default CPack Package Generators by Platform in CMake\nDESCRIPTION: Sets default values for `PACKAGE_GENERATOR` (binary) and `PACKAGE_SOURCE_GENERATOR` based on the target operating system (Apple, Unix, or others). Uses PackageMaker for Apple, DEB/RPM for Unix, and ZIP otherwise.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_31\n\nLANGUAGE: CMake\nCODE:\n```\n  # default package generators\n  if (APPLE)\n    set (PACKAGE_GENERATOR        \"PackageMaker\")\n    set (PACKAGE_SOURCE_GENERATOR \"TGZ;ZIP\")\n  elseif (UNIX)\n    set (PACKAGE_GENERATOR        \"DEB;RPM\")\n    set (PACKAGE_SOURCE_GENERATOR \"TGZ;ZIP\")\n  else ()\n    set (PACKAGE_GENERATOR        \"ZIP\")\n    set (PACKAGE_SOURCE_GENERATOR \"ZIP\")\n  endif ()\n```\n\n----------------------------------------\n\nTITLE: Creating FST Tool Executables\nDESCRIPTION: This snippet iterates through a list of FST tool names and creates an executable for each. The executables are compiled from the specified source files, which include FST-related utilities. It links against `kaldi-util` and `fst`, and dynamically links against `dl` on non-Windows platforms.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n  # FST tools binary\n  set(FST_BINS\n    fstaddselfloops\n    fstdeterminizestar\n    fstisstochastic\n    fstminimizeencoded\n    fsttablecompose\n  )\n\n  foreach(name IN LISTS FST_BINS)\n    add_executable(${name}\n      fstbin/${name}.cc\n      fstext/kaldi-fst-io.cc\n    )\nif (WIN32)\n    target_link_libraries(${name} PUBLIC kaldi-util fst)\nelse()\n    target_link_libraries(${name} PUBLIC kaldi-util fst dl)\nendif (WIN32)\n  endforeach()\n```\n\n----------------------------------------\n\nTITLE: train_emo.txt Example\nDESCRIPTION: This provides an example of data structure in the `train_emo.txt` file used for finetuning. Each line contains the audio file ID and its emotion label.\nSOURCE: https://github.com/modelscope/funasr/blob/main/examples/industrial_data_pretraining/sense_voice/README.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n\n----------------------------------------\n\nTITLE: Conditional Python Subdirectory Inclusion in CMake\nDESCRIPTION: This segment checks if the 'KALDI_NATIVE_FBANK_BUILD_PYTHON' option is enabled. If true, it displays a status message and adds the 'python' subdirectory to include Python bindings; otherwise, it indicates that Python build is disabled.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/kaldi-native-fbank/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(KALDI_NATIVE_FBANK_BUILD_PYTHON)\n  message(STATUS \"Building Python\")\n  add_subdirectory(python)\nelse()\n  message(STATUS \"Disable building Python\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Build Options for Windows in CMake\nDESCRIPTION: This conditional block applies settings specifically when building on Windows. It adds the '/bigobj' compiler flag, defines a variable for Windows whole archive linking, applies this linker flag, and defaults the BUILD_SHARED_LIBS option to OFF. On non-Windows platforms, BUILD_SHARED_LIBS defaults to ON.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (WIN32)\n  add_definitions(/bigobj)\n  set(WHOLEFST \"/WHOLEARCHIVE:fst\")\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${WHOLEFST}\")\n  #set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS 1)\n  #this must be disabled unless the previous option (CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS) is enabled\n  option(BUILD_SHARED_LIBS \"Build shared libraries\" OFF)\nelse()\n  option(BUILD_SHARED_LIBS \"Build shared libraries\" ON)\nendif (WIN32)\n```\n\n----------------------------------------\n\nTITLE: Fetching and Building PortAudio Dependency - CMake\nDESCRIPTION: Conditionally fetches the PortAudio library using `FetchContent`, allowing fallback URLs and hash verification. It populates the source, adds the library as a subdirectory to be built, and applies platform-specific compile options or installation rules.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_PORTAUDIO)\n  include(FetchContent)\n\n  set(portaudio_URL  \"http://files.portaudio.com/archives/pa_stable_v190700_20210406.tgz\")\n  set(portaudio_URL2 \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/dep_libs/pa_stable_v190700_20210406.tgz\")\n  set(portaudio_HASH \"SHA256=47efbf42c77c19a05d22e627d42873e991ec0c1357219c0d74ce6a2948cb2def\")\n\n  FetchContent_Declare(portaudio\n    URL\n      ${portaudio_URL}\n      ${portaudio_URL2}\n    URL_HASH          ${portaudio_HASH}\n  )\n\n  FetchContent_GetProperties(portaudio)\n  if(NOT portaudio_POPULATED)\n    message(STATUS \"Downloading portaudio from ${portaudio_URL}\")\n    FetchContent_Populate(portaudio)\n  endif()\n  message(STATUS \"portaudio is downloaded to ${portaudio_SOURCE_DIR}\")\n  message(STATUS \"portaudio's binary dir is ${portaudio_BINARY_DIR}\")\n\n  add_subdirectory(${portaudio_SOURCE_DIR} ${portaudio_BINARY_DIR} EXCLUDE_FROM_ALL)\n  if(NOT WIN32)\n    target_compile_options(portaudio PRIVATE \"-Wno-deprecated-declarations\")\n  else()\n    install(TARGETS portaudio DESTINATION ..)\n  endif()\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake Project and Settings - CMake\nDESCRIPTION: Sets the minimum required CMake version, defines the project name 'ASR' with C and C++ languages, and configures various build settings such as C++ standard, position-independent code, output directory, verbosity, and disabling testing.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.10)\n\nproject(ASR C CXX)\n\nset(CMAKE_CXX_STANDARD 14 CACHE STRING \"The C++ version to be used.\")\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\nset(CMAKE_VERBOSE_MAKEFILE on)\nset(BUILD_TESTING OFF)\n```\n\n----------------------------------------\n\nTITLE: Adding FST N-gram Library\nDESCRIPTION: This snippet defines the `fstngram` library using the `add_library` command.  It specifies the source files (`bitmap-index.cc`, `ngram-fst.cc`, `nthbit.cc`) and includes the header files found in the previous step (`${HEADER_FILES}`).  This library is a standard static or shared library, depending on CMake's default settings or user-defined options.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(fstngram\n    bitmap-index.cc \n    ngram-fst.cc \n    nthbit.cc\n    ${HEADER_FILES}\n)\n```\n\n----------------------------------------\n\nTITLE: Adding N-gram FST Module Library\nDESCRIPTION: This snippet defines the `ngram_fst` library as a module using the `add_library` command.  It specifies the source files (`bitmap-index.cc`, `ngram-fst.cc`, `nthbit.cc`). This creates a dynamically loadable module, often used for plugins or extensions.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(ngram_fst MODULE\n    bitmap-index.cc \n    ngram-fst.cc \n    nthbit.cc\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Static Library Components in CMake for FunASR Audio Processing\nDESCRIPTION: This CMake command creates a static library named 'csrc' from a collection of C/C++ source files that implement audio signal processing functionality. The library includes code for feature extraction (FBANK features), windowing functions, FFT implementations, and mel-spectrum computations for speech processing.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/kaldi-native-fbank/csrc/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(csrc STATIC\n        feature-fbank.cc\n        feature-functions.cc\n        feature-window.cc\n        fftsg.c\n        mel-computations.cc\n        online-feature.cc\n        rfft.cc)\n```\n\n----------------------------------------\n\nTITLE: Modular Creation and Installation of FST Extensions Using CMake Function\nDESCRIPTION: Defines a reusable CMake function add_module that builds a module target from given sources, links it with the core fst library, sets export and folder properties for Windows, and installs the resulting library to a designated subdirectory. The function is then invoked to create modules phi-fst, rho-fst, and sigma-fst from respective source files, promoting consistent modular builds and installations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/special/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfunction (add_module _name)\n  add_library(${ARGV})\n  if (TARGET ${_name})\n    target_link_libraries(${_name} fst)\n    set_target_properties(${_name}\n      PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS true\n      FOLDER special/modules\n    )\n  endif()\n\n  install(TARGETS ${_name} LIBRARY DESTINATION lib/fst)\nendfunction()\n\nadd_module(phi-fst MODULE phi-fst.cc)\nadd_module(rho-fst MODULE rho-fst.cc)\nadd_module(sigma-fst MODULE sigma-fst.cc)\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific File and Directory Operations, Runtime Linking (CMake)\nDESCRIPTION: This block handles OS-specific configuration for linking libraries and cleaning up files. On Windows, specific glog header files are removed. On other platforms, it adds library directories for ONNXRuntime and FFMPEG. Dependencies include environment variables ONNXRUNTIME_DIR and FFMPEG_DIR, and proper directory paths. Inputs: environment/platform; Outputs: modified include and link directories, with implications for binary search paths and runtime linking. Limitation: Windows handling is destructive on source files and should be used with care.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nIF(WIN32)\n    file(REMOVE ${PROJECT_SOURCE_DIR}/third_party/glog/src/config.h \n                ${PROJECT_SOURCE_DIR}/third_party/glog/src/glog/export.h \n                ${PROJECT_SOURCE_DIR}/third_party/glog/src/glog/logging.h \n                ${PROJECT_SOURCE_DIR}/third_party/glog/src/glog/raw_logging.h \n                ${PROJECT_SOURCE_DIR}/third_party/glog/src/glog/stl_logging.h \n                ${PROJECT_SOURCE_DIR}/third_party/glog/src/glog/vlog_is_on.h)\nELSE()\n    link_directories(${ONNXRUNTIME_DIR}/lib)\n    link_directories(${FFMPEG_DIR}/lib)\nendif()\ninclude_directories(${CMAKE_SOURCE_DIR}/src)\ninclude_directories(${PROJECT_SOURCE_DIR}/third_party/kaldi-native-fbank)\ninclude_directories(${PROJECT_SOURCE_DIR}/third_party/yaml-cpp/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/third_party/jieba/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/third_party/jieba/include/limonp/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/third_party/kaldi)\ninclude_directories(${PROJECT_SOURCE_DIR}/third_party/json/include)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Project and Enabling Testing in CMake\nDESCRIPTION: This snippet defines the project name as 'openfst' and includes the CTest module. Including CTest is a standard practice for enabling testing support within CMake projects.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(openfst)\ninclude(CTest)\n```\n\n----------------------------------------\n\nTITLE: Creating individual modular FST components for various bit-width and type variants\nDESCRIPTION: These snippets invoke 'add_module' to generate libraries for diverse FST components, each associated with specific source files. They facilitate building specialized FST modules, such as acceptors, strings, unweighted, and weighted variants across multiple bit-widths (8, 16, 64), enabling flexible inclusion in the overall project.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/compact/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_module(compact8_acceptor-fst MODULE\n  compact8_acceptor-fst.cc)\n \nadd_module(compact8_string-fst MODULE\n  compact8_string-fst.cc)\n \nadd_module(compact8_unweighted-fst MODULE\n  compact8_unweighted-fst.cc)\n \nadd_module(compact8_unweighted_acceptor-fst MODULE\n  compact8_unweighted_acceptor-fst.cc)\n \nadd_module(compact8_weighted_string-fst MODULE\n  compact8_weighted_string-fst.cc)\n \nadd_module(compact16_acceptor-fst MODULE\n  compact16_acceptor-fst.cc)\n \nadd_module(compact16_string-fst MODULE\n  compact16_string-fst.cc)\n \nadd_module(compact16_unweighted-fst MODULE\n  compact16_unweighted-fst.cc)\n \nadd_module(compact16_unweighted_acceptor-fst MODULE\n  compact16_unweighted_acceptor-fst.cc)\n \nadd_module(compact16_weighted_string-fst MODULE\n  compact16_weighted_string-fst.cc)\n \nadd_module(compact64_acceptor-fst MODULE\n  compact64_acceptor-fst.cc)\n \nadd_module(compact64_string-fst MODULE\n  compact64_string-fst.cc)\n \nadd_module(compact64_unweighted-fst MODULE\n  compact64_unweighted-fst.cc)\n \nadd_module(compact64_unweighted_acceptor-fst MODULE\n  compact64_unweighted_acceptor-fst.cc)\n \nadd_module(compact64_weighted_string-fst MODULE\n  compact64_weighted_string-fst.cc)\n```\n\n----------------------------------------\n\nTITLE: Set Glog Library Version Properties\nDESCRIPTION: Sets the `VERSION` property of the `glog` target to the project's version and the `SOVERSION` property (shared library version) to 1. These properties are used during installation and linking of shared libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties (glog PROPERTIES VERSION ${PROJECT_VERSION})\nset_target_properties (glog PROPERTIES SOVERSION 1)\n```\n\n----------------------------------------\n\nTITLE: Linking Dependencies to Protobuf/gRPC Library - CMake\nDESCRIPTION: Links the `rg_grpc_proto` library target to required system or external libraries, including reflection support, the gRPC C++ library, and the Protobuf library.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(rg_grpc_proto ${_REFLECTION} ${_GRPC_GRPCPP} ${_PROTOBUF_LIBPROTOBUF})\n```\n\n----------------------------------------\n\nTITLE: Registering Negative Compilation Tests in CMake (CMake)\nDESCRIPTION: This CMake block enables the addition of specific negative compilation tests via an option. Each test case checks a different invalid configuration scenario by invoking the previously defined test registration function with unique arguments. Prerequisites include the prior configuration of the Python test infrastructure and the availability of test source directories. Expected outputs are used to verify that configurations fail or pass as intended.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# negative compilation tests\noption (BUILD_NC_TESTS \"Request addition of negative compilation tests.\" OFF)\nmark_as_advanced (BUILD_NC_TESTS)\nif (BUILD_NC_TESTS)\n  add_gflags_build_test (nc_sanity               nc 0)\n  add_gflags_build_test (nc_swapped_args         nc 1)\n  add_gflags_build_test (nc_int_instead_of_bool  nc 1)\n  add_gflags_build_test (nc_bool_in_quotes       nc 1)\n  add_gflags_build_test (nc_define_string_with_0 nc 1)\nendif ()\n\n```\n\n----------------------------------------\n\nTITLE: Installing N-gram FST Module\nDESCRIPTION: This snippet installs the `ngram_fst` module library to the `lib/fst` directory.  This ensures that the module is placed in a location where it can be found at runtime, often within the FST's plugin or extension directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ngram_fst\n    LIBRARY DESTINATION lib/fst\n)\n```\n\n----------------------------------------\n\nTITLE: Compiling gflags Library from Source Using Shell and CMake\nDESCRIPTION: These shell commands outline the process to build the gflags library from source on Unix-like systems using CMake and GNU Make. They begin with extracting the source archive, creating a separate build directory, and running 'ccmake' for interactive configuration of build options. The steps include generating Makefiles, building the project, optionally running tests, and installing the built binaries. Variables '$package' and '$version' represent the package name and version. Key dependencies include CMake, GNU Make, and optionally a Python environment for certain tests. This approach allows advanced customization and control over the build process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/INSTALL.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n$ tar xzf gflags-$version-source.tar.gz\n$ cd gflags-$version\n$ mkdir build && cd build\n$ ccmake ..\n\n  - Press 'c' to configure the build system and 'e' to ignore warnings.\n  - Set CMAKE_INSTALL_PREFIX and other CMake variables and options.\n  - Continue pressing 'c' until the option 'g' is available.\n  - Then press 'g' to generate the configuration files for GNU Make.\n\n$ make\n$ make test    (optional)\n$ make install (optional)\n```\n\n----------------------------------------\n\nTITLE: Setting Working Directory for Tests (CMake)\nDESCRIPTION: This line sets the working directory for test commands. ${CMAKE_CURRENT_SOURCE_DIR} resolves to the directory containing the current CMakeLists.txt file. This is crucial for tests that rely on relative paths or files within the source directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset (GFLAGS_FLAGFILES_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Targets with CMake\nDESCRIPTION: These commands create three executable targets: gflags_unittest, gflags_unittest-main, and gflags_unittest_main. Each target is built from its corresponding source file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable (gflags_unittest      gflags_unittest.cc)\nadd_executable (gflags_unittest-main gflags_unittest-main.cc)\nadd_executable (gflags_unittest_main gflags_unittest_main.cc)\n```\n\n----------------------------------------\n\nTITLE: Generating Protobuf and gRPC Sources - CMake\nDESCRIPTION: Locates the `paraformer.proto` file, defines variables for the generated C++/gRPC source and header files, and adds a custom command to invoke `protoc` (with grpc plugin) to generate these files from the proto definition.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nget_filename_component(rg_proto ../python/grpc/proto/paraformer.proto ABSOLUTE)\nget_filename_component(rg_proto_path ${rg_proto} PATH)\n\n# Generated sources\nset(rg_proto_srcs ${CMAKE_CURRENT_BINARY_DIR}/paraformer.pb.cc)\nset(rg_proto_hdrs ${CMAKE_CURRENT_BINARY_DIR}/paraformer.pb.h)\nset(rg_grpc_srcs ${CMAKE_CURRENT_BINARY_DIR}/paraformer.grpc.pb.cc)\nset(rg_grpc_hdrs ${CMAKE_CURRENT_CURRENT_BINARY_DIR}/paraformer.grpc.pb.h)\nadd_custom_command(\n  OUTPUT ${rg_proto_srcs} ${rg_proto_hdrs} ${rg_grpc_srcs} ${rg_grpc_hdrs}\n  COMMAND ${_PROTOBUF_PROTOC}\n  ARGS --grpc_out ${CMAKE_CURRENT_BINARY_DIR}\n    --cpp_out ${CMAKE_CURRENT_BINARY_DIR}\n    -I ${rg_proto_path}\n    --plugin=protoc-gen-grpc=${_GRPC_CPP_PLUGIN_EXECUTABLE}\n    ${rg_proto}\n  DEPENDS ${rg_proto})\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC-specific Warning Suppressions in CMake\nDESCRIPTION: Suppresses specific warnings in Microsoft Visual C++ compiler to prevent excessive warning messages for acceptable code patterns. This includes warnings about type conversions and implicitly deleted destructors.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi-native-fbank/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32 AND MSVC)\n  # disable various warnings for MSVC\n  # 4244: '=': conversion from 'double' to 'float', possible loss of data\n  # 4267: 'return': conversion from 'size_t' to 'int32_t', possible loss of data\n  # 4624: destructor was implicitly defined as deleted because a base class destructor is inaccessible or deleted\n  set(disabled_warnings\n      /wd4244\n      /wd4267\n      /wd4624\n  )\n  message(STATUS \"Disabled warnings: ${disabled_warnings}\")\n  foreach(w IN LISTS disabled_warnings)\n    string(APPEND CMAKE_CXX_FLAGS \" ${w} \")\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Base CMake Project Configuration - CMake\nDESCRIPTION: Configures the minimum CMake version required, sets the project name, specifies the C++ standard to be used, enables position-independent code, and defines the output directory for build artifacts.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.16)\n\nproject(FunASRWebscoket) \n\nset(CMAKE_CXX_STANDARD 14 CACHE STRING \"The C++ version to be used.\")\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n```\n\n----------------------------------------\n\nTITLE: Adding a Basic gflags Unit Test with CMake\nDESCRIPTION: This command adds a gflags test named \"unittest\" that runs the gflags_unittest executable. The parameters \"0\", \"\", and \"\" likely represent expected return code, allowed substrings in the output to pass the test, and forbidden substrings in the output to pass the test respectively. add_gflags_test is probably a macro or function defined elsewhere.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gflags_test(unittest 0 \"\" \"\" gflags_unittest)\n```\n\n----------------------------------------\n\nTITLE: Defining FunASR WebSocket Server and Client Executables in CMake\nDESCRIPTION: This snippet defines four executable targets representing FunASR WebSocket servers and clients, including standard and 2-pass versions. It organizes source files per target by combining individual .cpp files with previously grouped ONNX Runtime source files. These targets enable building independently configurable server and client applications using the FunASR framework.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/bin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(funasr-wss-server \"funasr-wss-server.cpp\" \"websocket-server.cpp\" ${RELATION_SOURCE})\nadd_executable(funasr-wss-server-2pass \"funasr-wss-server-2pass.cpp\" \"websocket-server-2pass.cpp\" ${RELATION_SOURCE})\nadd_executable(funasr-wss-client \"funasr-wss-client.cpp\" ${RELATION_SOURCE})\nadd_executable(funasr-wss-client-2pass \"funasr-wss-client-2pass.cpp\" \"microphone.cpp\" ${RELATION_SOURCE})\n```\n\n----------------------------------------\n\nTITLE: Including Header Directories in CMake\nDESCRIPTION: Adds the parent 'include' directory (`../include`) and the sibling 'script' directory (`../script/`) to the list of directories searched by the compiler for header files. This ensures that source files compiled within this CMake context can find necessary headers from these locations.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/bin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories(../include ../script/)\n```\n\n----------------------------------------\n\nTITLE: Defining and Configuring the fstlookahead Library in CMake\nDESCRIPTION: This snippet defines a shared library named `fstlookahead` using the `add_library` command, compiling the specified C++ source files. It links this library against the required `fst` library using `target_link_libraries`. Finally, `set_target_properties` configures the library's shared object version (`SOVERSION`) and assigns it to a specific folder ('lookahead') likely for organization within IDEs like Visual Studio.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/lookahead/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(fstlookahead\n  arc_lookahead-fst.cc \n  ilabel_lookahead-fst.cc\n  olabel_lookahead-fst.cc\n)\ntarget_link_libraries(fstlookahead fst)\nset_target_properties(fstlookahead PROPERTIES \n  SOVERSION \"${SOVERSION}\"\n  FOLDER lookahead  \n)\n```\n\n----------------------------------------\n\nTITLE: Link DbgHelp Library\nDESCRIPTION: If `HAVE_DBGHELP` is true, indicating the Windows DbgHelp library is available, it links the `dbghelp` library privately to `glog`. It also appends the `-ldbghelp` flag to the static linking options variable.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif (HAVE_DBGHELP)\n  target_link_libraries (glog PRIVATE dbghelp)\n  set (glog_libraries_options_for_static_linking \"${glog_libraries_options_for_static_linking} -ldbghelp\")\nendif (HAVE_DBGHELP)\n```\n\n----------------------------------------\n\nTITLE: Defining `add_executable2` CMake Function for FST Executables\nDESCRIPTION: Defines a CMake function `add_executable2` to simplify building executable targets. It calls the standard `add_executable` with all provided arguments (`${ARGV}`), links the target against `fstscript`, `fst`, and system dynamic libraries (`${CMAKE_DL_LIBS}`), sets the output folder property to 'bin', and installs the executable to the 'bin' destination directory upon installation.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/bin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction (add_executable2 _name)\n    add_executable(${ARGV})\n    if (TARGET ${_name})\n        target_link_libraries(${_name} fstscript fst ${CMAKE_DL_LIBS})\n        set_target_properties(${_name} PROPERTIES FOLDER bin)\n    endif()\n\n    install(TARGETS ${_name} RUNTIME DESTINATION bin)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Building fstfar Library in CMake\nDESCRIPTION: Builds the core fstfar library by collecting header files and compiling source files. The library depends on the fst library and is configured with the same SOVERSION property for version tracking.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/far/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB HEADER_FILES ../../include/fst/extensions/far/*.h)\nmessage(STATUS \"${HEADER_FILES}\")\n\nadd_library(fstfar\n  sttable.cc\n  stlist.cc\n  ${HEADER_FILES}\n)\ntarget_link_libraries(fstfar fst)\nset_target_properties(fstfar PROPERTIES \n  SOVERSION \"${SOVERSION}\"\n  FOLDER far\n)\n\ninstall(TARGETS fstfar\n  LIBRARY DESTINATION lib\n  ARCHIVE DESTINATION lib\n  RUNTIME DESTINATION lib\n)\n```\n\n----------------------------------------\n\nTITLE: Set Output Directories and Library Postfixes (CMake)\nDESCRIPTION: Sets the default output directories for runtime executables (`bin`), libraries (`lib`), and archives (`lib`) if gflags is not being built as a subproject. It also defines suffixes for release (empty) and debug (`_debug`) library files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT GFLAGS_IS_SUBPROJECT)\n  set (CMAKE_RUNTIME_OUTPUT_DIRECTORY \"bin\")\n  set (CMAKE_LIBRARY_OUTPUT_DIRECTORY \"lib\")\n  set (CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"lib\")\nendif ()\n# Set postfixes for generated libraries based on buildtype.\nset(CMAKE_RELEASE_POSTFIX \"\")\nset(CMAKE_DEBUG_POSTFIX \"_debug\")\n```\n\n----------------------------------------\n\nTITLE: Building Far Executables with Custom Function in CMake\nDESCRIPTION: Defines a custom function add_executable2 that streamlines the creation of executable targets. The function automatically links against required libraries and sets proper folder properties and installation paths.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/far/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_BIN)\n  function (add_executable2 _name)\n      add_executable(${ARGV})\n      if (TARGET ${_name})\n          target_link_libraries(${_name} fstfarscript fstscript fst ${CMAKE_DL_LIBS})\n          set_target_properties(${_name} PROPERTIES FOLDER far/bin)\n      endif()\n      install(TARGETS ${_name} RUNTIME DESTINATION bin)\n  endfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version and Standard Build Flags\nDESCRIPTION: This configures the project to require CMake version 3.1 or newer. It also sets the RPATH for macOS builds to 1 (enabled) and specifies that the project should be compiled using the C++11 standard.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1)\nset(CMAKE_MACOSX_RPATH 1)\nset(CMAKE_CXX_STANDARD 11)\n```\n\n----------------------------------------\n\nTITLE: Copy CMake Modules to Binary Directory\nDESCRIPTION: If `_glog_CMake_MODULES` is not empty, this command adds a custom build step to copy the specified CMake modules from the source directory to the binary data directory (`_glog_BINARY_CMake_DATADIR`). This ensures necessary build files are available during the build process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif (_glog_CMake_MODULES)\n  # Copy modules to binary directory during the build\n  add_custom_command (OUTPUT ${_glog_BINARY_CMake_MODULES}\n    COMMAND ${CMAKE_COMMAND} -E make_directory\n    ${_glog_BINARY_CMake_DATADIR}\n    COMMAND ${CMAKE_COMMAND} -E copy ${_glog_CMake_MODULES}\n    ${_glog_BINARY_CMake_DATADIR}\n    DEPENDS ${_glog_CMake_MODULES}\n    COMMENT \"Copying find modules...\"\n  )\nendif (_glog_CMake_MODULES)\n```\n\n----------------------------------------\n\nTITLE: Conditional Executable Building\nDESCRIPTION: This block conditionally adds executables related to the MPDT extension if the HAVE_BIN flag is set. This section is responsible for calling the executables and calling function add_executable2\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/mpdt/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_BIN)\n  function (add_executable2 _name)\n      add_executable(${ARGV})\n      if (TARGET ${_name})\n          target_link_libraries(${_name} fstmpdtscript fstpdtscript fstscript fst ${CMAKE_DL_LIBS})\n          set_target_properties(${_name} PROPERTIES\n            FOLDER mpdt/bin\n          )\n      endif()\n    install(TARGETS ${_name} RUNTIME DESTINATION bin)\n  endfunction()\n  add_executable2(mpdtcompose  mpdtcompose.cc)\n  add_executable2(mpdtexpand  mpdtexpand.cc)\n  add_executable2(mpdtinfo  mpdtinfo.cc)\n  add_executable2(mpdtreverse  mpdtreverse.cc)\nendif(HAVE_BIN)\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake and Project\nDESCRIPTION: This snippet initializes the CMake environment and declares the project name as 'kaldi'. It sets the minimum required CMake version to 3.10 and specifies that any errors during CMake configuration should be fatal.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.10 FATAL_ERROR)\n\nproject(kaldi)\n```\n\n----------------------------------------\n\nTITLE: Configuring Python-Based Build Tests with CMake (CMake)\nDESCRIPTION: This CMake script sets up infrastructure to run Python-driven build tests. It detects if negative compilation or configuration tests are requested and requires a valid Python installation, reporting errors otherwise. It generates a build.py script from a template and defines a helper function to register individual tests which invoke the Python script with specific arguments. All tests rely on the presence of the Python interpreter, the build.py script, and specific source directories. Test entries specify unique parameters for each test case, handling both successful and failing build scenarios.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\n# configure Python script which configures and builds a test project\nif (BUILD_NC_TESTS OR BUILD_CONFIG_TESTS)\n  find_package (PythonInterp)\n  if (NOT PYTHON_EXECUTABLE)\n    message (FATAL_ERROR \"No Python installation found! It is required by the (negative) compilation tests.\"\n                         \" Either install Python or set BUILD_NC_TESTS and BUILD_CONFIG_TESTS to FALSE.\")\n  endif ()\n  set (TMPDIR \"${PROJECT_BINARY_DIR}/Testing/Temporary\")\n  configure_file (gflags_build.py.in \"${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/build.py\" @ONLY)\n  function (add_gflags_build_test name srcdir expect_fail)\n    set (srcdir \"${CMAKE_CURRENT_SOURCE_DIR}/${srcdir}\")\n    add_test (\n      NAME    \"${name}\"\n      COMMAND \"${PYTHON_EXECUTABLE}\" \"${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/build.py\" \n                    ${name} ${srcdir} ${expect_fail}\n    )\n  endfunction ()\nendif ()\n\n```\n\n----------------------------------------\n\nTITLE: Validate Build Configuration (CMake)\nDESCRIPTION: Checks the selected build options to ensure a valid configuration. It sets BUILD_STATIC_LIBS ON if neither shared nor static builds were requested and issues a fatal error if neither the multi-threaded nor single-threaded library is selected.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT BUILD_SHARED_LIBS AND NOT BUILD_STATIC_LIBS)\n  set (BUILD_STATIC_LIBS ON)\nendif ()\nif (NOT BUILD_gflags_LIB AND NOT BUILD_gflags_nothreads_LIB)\n  message (FATAL_ERROR \"At least one of [GFLAGS_]BUILD_gflags_LIB and [GFLAGS_]BUILD_gflags_nothreads_LIB must be ON.\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Specifying Common Third-Party Include Directories - CMake\nDESCRIPTION: Adds multiple include directories pointing to various third-party libraries and components required by the project, typically located within a shared ONNX Runtime dependency path. This makes headers for libraries like yaml-cpp, kaldi-native-fbank, and jieba available.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/include/)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/src)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/yaml-cpp/include/)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi-native-fbank)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/jieba/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/jieba/include/limonp/include)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party)\ninclude_directories(${PROJECT_SOURCE_DIR}/../onnxruntime/third_party/kaldi)\n```\n\n----------------------------------------\n\nTITLE: Determining Program Counter Extraction from ucontext_t Structures Using CMake\nDESCRIPTION: This CMake snippet iterates over various possible program counter (PC) field names within ucontext_t structures, attempting to compile test programs that access each candidate field to identify valid PC extraction ways. It writes temporary C++ files including different headers and tries to compile them with _GNU_SOURCE defined to verify availability. The first successful field is cached in PC_FROM_UCONTEXT for later use in stack trace or context capture features.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (HAVE_UCONTEXT_H AND NOT PC_FROM_UCONTEXT)\n  foreach (_PC_FIELD ${_PC_FIELDS})\n    foreach (_PC_HEADER ${_PC_HEADERS})\n      set (_TMP\n      ${CMAKE_CURRENT_BINARY_DIR}${CMAKE_FILES_DIRECTORY}/CMakeTmp/uctfield.cpp)\n      file (WRITE ${_TMP} \"\n#define _GNU_SOURCE 1\n#include <${_PC_HEADER}>\nint main(void)\n{\n  ucontext_t u;\n  return u.${_PC_FIELD} == 0;\n}\n\")\n      try_compile (HAVE_PC_FROM_UCONTEXT ${CMAKE_CURRENT_BINARY_DIR} ${_TMP}\n        COMPILE_DEFINITIONS _GNU_SOURCE=1)\n\n      if (HAVE_PC_FROM_UCONTEXT)\n        set (PC_FROM_UCONTEXT ${_PC_FIELD} CACHE)\n      endif (HAVE_PC_FROM_UCONTEXT)\n    endforeach (_PC_HEADER)\n  endforeach (_PC_FIELD)\nendif  (HAVE_UCONTEXT_H AND NOT PC_FROM_UCONTEXT)\n```\n\n----------------------------------------\n\nTITLE: Defining gflags Package Metadata and Project Settings in CMake\nDESCRIPTION: Sets essential package metadata variables (name, version 2.2.2, description, URL, etc.) for gflags. It also declares the CMake project, specifies C++ as the primary language, conditionally enables C for older CMake versions (< 3.4) due to compatibility issues with required modules, and extracts version components using the `version_numbers` macro (likely defined in `utils.cmake`).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# package information\nset (PACKAGE_NAME        \"gflags\")\nset (PACKAGE_VERSION     \"2.2.2\")\nset (PACKAGE_STRING      \"${PACKAGE_NAME} ${PACKAGE_VERSION}\")\nset (PACKAGE_TARNAME     \"${PACKAGE_NAME}-${PACKAGE_VERSION}\")\nset (PACKAGE_BUGREPORT   \"https://github.com/gflags/gflags/issues\")\nset (PACKAGE_DESCRIPTION \"A commandline flags library that allows for distributed flags.\")\nset (PACKAGE_URL         \"http://gflags.github.io/gflags\")\n\nproject (${PACKAGE_NAME} VERSION ${PACKAGE_VERSION} LANGUAGES CXX)\nif (CMAKE_VERSION VERSION_LESS 3.4)\n  # C language still needed because the following required CMake modules\n  # (or their dependencies, respectively) are not correctly handling\n  # the case where only CXX is enabled\n  # - CheckTypeSize.cmake (fixed in CMake 3.1, cf. https://cmake.org/Bug/view.php?id=14056)\n  # - FindThreads.cmake   (fixed in CMake 3.4, cf. https://cmake.org/Bug/view.php?id=14905)\n  enable_language (C)\nendif ()\n\nversion_numbers (\n  ${PACKAGE_VERSION}\n    PACKAGE_VERSION_MAJOR\n    PACKAGE_VERSION_MINOR\n    PACKAGE_VERSION_PATCH\n)\n```\n\n----------------------------------------\n\nTITLE: Check Required Include Files (CMake)\nDESCRIPTION: Checks for the availability of various standard C/C++ header files depending on the compiler (MSVC or others). It sets `HAVE_<HEADER_NAME>_H` variables to indicate whether the header was found, often converting boolean results to integers for preprocessor directives.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif (MSVC)\n  set (HAVE_SYS_TYPES_H 1)\n  set (HAVE_STDDEF_H    1) # used by CheckTypeSize module\n  set (HAVE_UNISTD_H    0)\n  set (HAVE_SYS_STAT_H  1)\n  set (HAVE_SHLWAPI_H   1)\n  if (MSVC_VERSION VERSION_LESS 1600)\n    check_include_file_cxx (\"stdint.h\" HAVE_STDINT_H)\n    bool_to_int (HAVE_STDINT_H)  # used in #if directive\n  else ()\n    set (HAVE_STDINT_H 1)\n  endif ()\n  if (MSVC_VERSION VERSION_LESS 1800)\n    check_include_file_cxx (\"inttypes.h\" HAVE_INTTYPES_H)\n    bool_to_int (HAVE_INTTYPES_H)  # used in #if directive\n  else ()\n    set (HAVE_INTTYPES_H 1)\n  endif ()\nelse ()\n  foreach (fname IN ITEMS unistd stdint inttypes sys/types sys/stat fnmatch)\n    string (TOUPPER \"${fname}\" FNAME)\n    string (REPLACE \"/\" \"_\" FNAME \"${FNAME}\")\n    if (NOT HAVE_${FNAME}_H)\n      check_include_file_cxx (\"${fname}.h\" HAVE_${FNAME}_H)\n    endif ()\n  endforeach ()\n  if (NOT HAVE_FNMATCH_H AND OS_WINDOWS)\n    check_include_file_cxx (\"shlwapi.h\" HAVE_SHLWAPI_H)\n  endif ()\n  # the following are used in #if directives not #ifdef\n  bool_to_int (HAVE_STDINT_H)\n  bool_to_int (HAVE_SYS_TYPES_H)\n  bool_to_int (HAVE_INTTYPES_H)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring gflags C++ Namespace(s) via CMake\nDESCRIPTION: Defines the C++ namespace(s) used by the gflags library using the custom `gflags_define` macro (likely from `utils.cmake`). It allows specifying multiple namespaces (defaulting to 'google;gflags' for backward compatibility) via the `NAMESPACE` variable, validates the names, and sets the primary (`GFLAGS_NAMESPACE`) and secondary (`GFLAGS_NAMESPACE_SECONDARY`) CMake list variables accordingly. An error is triggered if invalid namespace names are provided.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\n# maintain binary backwards compatibility with gflags library version <= 2.0,\n# but at the same time enable the use of the preferred new \"gflags\" namespace\ngflags_define (STRING NAMESPACE \"Name(s) of library namespace (separate multiple options by semicolon)\" \"google;${PACKAGE_NAME}\" \"${PACKAGE_NAME}\")\ngflags_property (NAMESPACE ADVANCED TRUE)\nset (GFLAGS_NAMESPACE_SECONDARY \"${NAMESPACE}\")\nlist (REMOVE_DUPLICATES GFLAGS_NAMESPACE_SECONDARY)\nif (NOT GFLAGS_NAMESPACE_SECONDARY)\n  message (FATAL_ERROR \"GFLAGS_NAMESPACE must be set to one (or more) valid C++ namespace identifier(s separated by semicolon \\\\\\\";\\\\\";\").\")\nendif ()\nforeach (ns IN LISTS GFLAGS_NAMESPACE_SECONDARY)\n  if (NOT ns MATCHES \"^[a-zA-Z][a-zA-Z0-9_]*$\")\n    message (FATAL_ERROR \"GFLAGS_NAMESPACE contains invalid namespace identifier: ${ns}\")\n  endif ()\nendforeach ()\nlist (GET       GFLAGS_NAMESPACE_SECONDARY 0 GFLAGS_NAMESPACE)\nlist (REMOVE_AT GFLAGS_NAMESPACE_SECONDARY 0)\n```\n\n----------------------------------------\n\nTITLE: Determine CPack System Name and Architecture in CMake\nDESCRIPTION: Determines and sets the `CPACK_SYSTEM_NAME` and `CPACK_PACKAGE_ARCHITECTURE` variables based on the operating system and architecture. It handles Windows (win32/win64), Apple (darwin), and Linux (using `dpkg --print-architecture` or `uname -m` to determine i386/amd64 based on compiler flags or system commands).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_37\n\nLANGUAGE: CMake\nCODE:\n```\n  # system/architecture\n  if (WINDOWS)\n    if (CMAKE_CL_64)\n      set (CPACK_SYSTEM_NAME \"win64\")\n    else ()\n      set (CPACK_SYSTEM_NAME \"win32\")\n    endif ()\n    set (CPACK_PACKAGE_ARCHITECTURE)\n  elseif (APPLE)\n    set (CPACK_PACKAGE_ARCHITECTURE darwin)\n  else ()\n    string (TOLOWER \"${CMAKE_SYSTEM_NAME}\" CPACK_SYSTEM_NAME)\n    if (CMAKE_CXX_FLAGS MATCHES \"-m32\")\n      set (CPACK_PACKAGE_ARCHITECTURE i386)\n    else ()\n      execute_process (\n        COMMAND         dpkg --print-architecture\n        RESULT_VARIABLE RV\n        OUTPUT_VARIABLE CPACK_PACKAGE_ARCHITECTURE\n      )\n      if (RV EQUAL 0)\n\t      string (STRIP \"${CPACK_PACKAGE_ARCHITECTURE}\" CPACK_PACKAGE_ARCHITECTURE)\n      else ()\n        execute_process (COMMAND uname -m OUTPUT_VARIABLE CPACK_PACKAGE_ARCHITECTURE)\n        if (CPACK_PACKAGE_ARCHITECTURE MATCHES \"x86_64\")\n\t        set (CPACK_PACKAGE_ARCHITECTURE amd64)\n        else ()\n          set (CPACK_PACKAGE_ARCHITECTURE i386)\n        endif ()\n      endif ()\n    endif ()\n  endif ()\n```\n\n----------------------------------------\n\nTITLE: Include CPack Module in CMake\nDESCRIPTION: Includes the standard CMake `CPack` module. This command makes the CPack functionality and commands available for use in the build process, enabling the generation of packages based on the previously defined `CPACK_*` variables.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_41\n\nLANGUAGE: CMake\nCODE:\n```\n  include (CPack)\n\nendif () # BUILD_PACKAGING\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Tests for Help Messages with CMake\nDESCRIPTION: These commands add gflags tests to verify the contents of help messages. They check for the presence of specific strings (or the absence of others) in the output of gflags_unittest or gflags_unittest-main when run with different help-related flags (--help, --helpshort, --helpon, --helpmatch, etc.). The SLASH variable is used to represent the directory separator.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\n# --help should show all flags, including flags from gflags_reporting\nadd_gflags_test(help-reporting 1 \"${SLASH}gflags_reporting.cc:\" \"\"  gflags_unittest  --help)\n\n# Make sure that --help prints even very long helpstrings.\nadd_gflags_test(long-helpstring 1 \"end of a long helpstring\" \"\"  gflags_unittest  --help)\n\n# Make sure --help reflects flag changes made before flag-parsing\nadd_gflags_test(changed_bool1 1 \"-changed_bool1 (changed) type: bool default: true\" \"\"  gflags_unittest  --help)\nadd_gflags_test(changed_bool2 1 \"-changed_bool2 (changed) type: bool default: false currently: true\" \"\"  gflags_unittest  --help)\n# And on the command-line, too\nadd_gflags_test(changeable_string_var 1 \"-changeable_string_var () type: string default: \\\"1\\\" currently: \\\"2\\\"\" \"\"  gflags_unittest  --changeable_string_var 2 --help)\n\n# --nohelp and --help=false should be as if we didn't say anything\nadd_gflags_test(nohelp     0 \"PASS\" \"\"  gflags_unittest  --nohelp)\nadd_gflags_test(help=false 0 \"PASS\" \"\"  gflags_unittest  --help=false)\n\n# --helpfull is the same as help\nadd_gflags_test(helpfull 1 \"${SLASH}gflags_reporting.cc:\" \"\"  gflags_unittest  --helpfull)\n\n# --helpshort should show only flags from the  gflags_unittest  itself\nadd_gflags_test(helpshort 1 \"${SLASH}gflags_unittest.cc:\" \"${SLASH}gflags_reporting.cc:\"  gflags_unittest  --helpshort)\n\n# --helpshort should show the tldflag we created in the  gflags_unittest  dir\nadd_gflags_test(helpshort-tldflag1 1 \"tldflag1\" \"${SLASH}google.cc:\"  gflags_unittest  --helpshort)\nadd_gflags_test(helpshort-tldflag2 1 \"tldflag2\" \"${SLASH}google.cc:\"  gflags_unittest  --helpshort)\n\n# --helpshort should work if the main source file is suffixed with [_-]main\nadd_gflags_test(helpshort-main 1 \"${SLASH}gflags_unittest-main.cc:\" \"${SLASH}gflags_reporting.cc:\" gflags_unittest-main --helpshort)\nadd_gflags_test(helpshort_main 1 \"${SLASH}gflags_unittest_main.cc:\" \"${SLASH}gflags_reporting.cc:\" gflags_unittest_main --helpshort)\n\n# --helpon needs an argument\nadd_gflags_test(helpon 1 \"'--helpon' is missing its argument; flag description: show help on\" \"\"  gflags_unittest  --helpon)\n# --helpon argument indicates what file we'll show args from\nadd_gflags_test(helpon=gflags 1 \"${SLASH}gflags.cc:\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --helpon=gflags)\n# another way of specifying the argument\nadd_gflags_test(helpon_gflags 1 \"${SLASH}gflags.cc:\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --helpon gflags)\n# test another argument\nadd_gflags_test(helpon=gflags_unittest 1 \"${SLASH}gflags_unittest.cc:\" \"${SLASH}gflags.cc:\"  gflags_unittest  --helpon=gflags_unittest)\n\n# helpmatch is like helpon but takes substrings\nadd_gflags_test(helpmatch_reporting 1 \"${SLASH}gflags_reporting.cc:\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  -helpmatch reporting)\nadd_gflags_test(helpmatch=unittest  1 \"${SLASH}gflags_unittest.cc:\" \"${SLASH}gflags.cc:\"  gflags_unittest  -helpmatch=unittest)\n\n# if no flags are found with helpmatch or helpon, suggest --help\nadd_gflags_test(helpmatch=nosuchsubstring 1 \"No modules matched\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  -helpmatch=nosuchsubstring)\nadd_gflags_test(helpon=nosuchmodule       1 \"No modules matched\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  -helpon=nosuchmodule)\n\n# helppackage shows all the flags in the same dir as this unittest\n# --help should show all flags, including flags from google.cc\nadd_gflags_test(helppackage 1 \"${SLASH}gflags_reporting.cc:\" \"\"  gflags_unittest  --helppackage)\n\n# xml!\nadd_gflags_test(helpxml 1 \"${SLASH}gflags_unittest.cc</file>\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  --helpxml)\n```\n\n----------------------------------------\n\nTITLE: Using Installed Single-Threaded Static gflags in CMake\nDESCRIPTION: Shows how to specifically find and link against the single-threaded static version of a pre-installed gflags library. It uses `find_package` with the `nothreads_static` component specified.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 2.8.12 FATAL_ERROR)\n\nproject(Foo)\n\nfind_package(gflags COMPONENTS nothreads_static)\n\nadd_executable(foo src/foo.cc)\ntarget_link_libraries(foo gflags::gflags)\n```\n\n----------------------------------------\n\nTITLE: Setting N-gram FST Module Properties\nDESCRIPTION: This snippet sets properties for the `ngram_fst` module library.  `WINDOWS_EXPORT_ALL_SYMBOLS` ensures that all symbols are exported on Windows platforms, and `FOLDER` specifies the folder for organization within the build system (ngram/modules).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(ngram_fst PROPERTIES\n    WINDOWS_EXPORT_ALL_SYMBOLS true\n    FOLDER ngram/modules\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Far Tool Executables in CMake\nDESCRIPTION: Creates multiple executable targets for Far-related tools using the previously defined add_executable2 function. Each executable is compiled from a single source file and will be installed in the bin directory.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/far/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n  add_executable2(farcompilestrings farcompilestrings.cc)\n  add_executable2(farcreate  farcreate.cc)\n  add_executable2(farequal  farequal.cc)\n  add_executable2(farextract  farextract.cc)\n  add_executable2(farinfo  farinfo.cc)\n  add_executable2(farisomorphic  farisomorphic.cc)\n  add_executable2(farprintstrings  farprintstrings.cc)\nendif(HAVE_BIN)\n```\n\n----------------------------------------\n\nTITLE: Add Glog Library and Alias\nDESCRIPTION: Creates the main `glog` library target using the objects compiled by `glog_internal`. This links the object files into the final library (static or dynamic, depending on CMake configuration). It sets the C++ standard requirement to C++14 for the public interface of the `glog` library and adds an alias target `glog::glog` following modern CMake best practices.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library (glog\n  $<TARGET_OBJECTS:glog_internal>\n)\ntarget_compile_features (glog PUBLIC cxx_std_14)\n\nadd_library (glog::glog ALIAS glog)\n```\n\n----------------------------------------\n\nTITLE: Configuring Files with CMake\nDESCRIPTION: These commands copy the gflags_unittest.cc file to gflags_unittest-main.cc and gflags_unittest_main.cc. The COPYONLY option ensures that only a copy is created without further configuration or variable substitution.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nconfigure_file (gflags_unittest.cc gflags_unittest-main.cc COPYONLY)\nconfigure_file (gflags_unittest.cc gflags_unittest_main.cc COPYONLY)\n```\n\n----------------------------------------\n\nTITLE: Set RPM/DEB Specific CPack Information in CMake\nDESCRIPTION: Configures CPack variables primarily used by the RPM generator, but also leveraged for DEB packages via a template file (`cmake/package.cmake.in`). It sets the package group, license identifier (BSD), project URL, and the path to the changelog file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_35\n\nLANGUAGE: CMake\nCODE:\n```\n  # RPM package information -- used in cmake/package.cmake.in also for DEB\n  set (CPACK_RPM_PACKAGE_GROUP   \"Development/Libraries\")\n  set (CPACK_RPM_PACKAGE_LICENSE \"BSD\")\n  set (CPACK_RPM_PACKAGE_URL     \"${PACKAGE_URL}\")\n  set (CPACK_RPM_CHANGELOG_FILE  \"${CMAKE_CURRENT_LIST_DIR}/ChangeLog.txt\")\n```\n\n----------------------------------------\n\nTITLE: Verifying Atomic Built-in and Compiler Features Using CMake\nDESCRIPTION: These CMake commands check if compiler built-in features such as __builtin_expect for branch prediction and __sync_val_compare_and_swap for atomic compare-and-swap operations are available by compiling simple C++ programs that invoke these intrinsic functions. The results are cached in variables for use in conditional compilation to optimize performance and add atomic operations support.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncheck_cxx_source_compiles (\"\nint main(void) { if (__builtin_expect(0, 0)) return 1; return 0; }\n\" HAVE___BUILTIN_EXPECT)\n\ncheck_cxx_source_compiles (\"\nint main(void)\n{\n  int a; if (__sync_val_compare_and_swap(&a, 0, 1)) return 1; return 0;\n}\n\" HAVE___SYNC_VAL_COMPARE_AND_SWAP)\n```\n\n----------------------------------------\n\nTITLE: Compiling Symbol Table Component with GCC Flags\nDESCRIPTION: A compiler command used to build the symbol-table component with specific C++ compiler flags including C++11 standard, configuration headers, and position-independent code options.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#-DHAVE_CONFIG_H -I./../include -fno-exceptions -funsigned-char -std=c++11 -MT symbol-table.lo -MD -MP -MF .deps/symbol-table.Tpo -c symbol-table.cc  -fno-common -DPIC -o .libs/symbol-table.o\n```\n\n----------------------------------------\n\nTITLE: Conditionally Set CPack README Resource File in CMake\nDESCRIPTION: Sets the `CPACK_RESOURCE_FILE_README` variable based on whether headers are being installed (`INSTALL_HEADERS` is true). If installing headers (likely a development package), it points to an HTML documentation file; otherwise (likely a runtime package), it points to a plain text README.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_36\n\nLANGUAGE: CMake\nCODE:\n```\n  if (INSTALL_HEADERS)\n    set (CPACK_RESOURCE_FILE_README \"${CMAKE_CURRENT_LIST_DIR}/doc/index.html\")\n  else ()\n    set (CPACK_RESOURCE_FILE_README \"${CMAKE_CURRENT_LIST_DIR}/cmake/README_runtime.txt\")\n  endif ()\n```\n\n----------------------------------------\n\nTITLE: Set Glog Internal Target Properties from Glog\nDESCRIPTION: Configures the `glog_internal` object library target's include directories and compile definitions by inheriting them from the main `glog` target's corresponding properties. It also adds a private compile definition `GOOGLE_GLOG_IS_A_DLL` to `glog_internal` for consistency with the main target.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_24\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories (glog_internal PUBLIC\n  $<TARGET_PROPERTY:glog,INCLUDE_DIRECTORIES>)\ntarget_compile_definitions (glog_internal PUBLIC\n  $<TARGET_PROPERTY:glog,COMPILE_DEFINITIONS>\n  PRIVATE GOOGLE_GLOG_IS_A_DLL)\n```\n\n----------------------------------------\n\nTITLE: Verifying localtime_r Thread-Safe Functionality in C++ Using CMake\nDESCRIPTION: This snippet compiles a small C++ program that calls localtime_r, a reentrant variant of localtime, to check for its availability and thread-safe time conversions on the platform. It includes ctime and uses time_t and struct tm to validate API presence. Presence of this function is cached as HAVE_LOCALTIME_R to allow usage in the project.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n#include <cstdlib>\\n#include <ctime>\\nint main()\\n{\\n    time_t timep;\\n    struct tm result;\\n    localtime_r(&timep, &result);\\n    return EXIT_SUCCESS;\\n}\n```\n\n----------------------------------------\n\nTITLE: Add Compile Definition for Windows/Cygwin\nDESCRIPTION: If the target platform is CYGWIN or WIN32, this adds the `GLOG_NO_ABBREVIATED_SEVERITIES` preprocessor definition to the public compile definitions of the `glog` target. This typically disables shorthand severity level macros (like `INFO`, `WARN`) to avoid conflicts with system headers on Windows.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nif (CYGWIN OR WIN32)\n  target_compile_definitions (glog PUBLIC GLOG_NO_ABBREVIATED_SEVERITIES)\nendif (CYGWIN OR WIN32)\n```\n\n----------------------------------------\n\nTITLE: Creating kaldi-decoder Library\nDESCRIPTION: This snippet creates a static library named 'kaldi-decoder'. It lists the source files related to lattice decoding. The code links against `kaldi-util`, and dynamically links against `dl` on non-Windows platforms. It adds a compiler definition for windows to disable GLOG abbreviated severities.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(kaldi-decoder STATIC\n  lat/determinize-lattice-pruned.cc\n  lat/lattice-functions.cc\n  decoder/lattice-faster-decoder.cc\n  decoder/lattice-faster-online-decoder.cc\n)\n\nif (WIN32)\ntarget_link_libraries(kaldi-decoder PUBLIC kaldi-util)\nelse()\ntarget_link_libraries(kaldi-decoder PUBLIC kaldi-util dl)\nendif (WIN32)\n\n\nif (WIN32)\n  target_compile_definitions (kaldi-decoder PUBLIC GLOG_NO_ABBREVIATED_SEVERITIES)\nendif (WIN32)\n```\n\n----------------------------------------\n\nTITLE: Link Android Log Library\nDESCRIPTION: If the target platform is Android, it links the native `log` library privately to `glog`. This is necessary for logging functionality on Android. It also appends the `-llog` flag to the static linking options variable.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nif (ANDROID)\n  target_link_libraries (glog PRIVATE log)\n  set (glog_libraries_options_for_static_linking \"${glog_libraries_options_for_static_linking} -llog\")\nendif (ANDROID)\n```\n\n----------------------------------------\n\nTITLE: Including Common CMake Configuration - CMake\nDESCRIPTION: Includes a shared configuration file `common.cmake` which likely contains common variables, macros, or functions used across the project's build system.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/grpc/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(common.cmake)\n```\n\n----------------------------------------\n\nTITLE: Globbing Header Files in CMake\nDESCRIPTION: This CMake command uses GLOB to find all header files with the .h extension in the specified directory. The results are stored in the HEADER_FILES variable, which can then be used in subsequent build steps.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/mpdt/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB HEADER_FILES ../../include/fst/extensions/mpdt/*.h)\nmessage(STATUS \"${HEADER_FILES}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet adds include directories for glog and gflags libraries. These are required for compiling code that uses these libraries.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/kaldi/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${CMAKE_SOURCE_DIR}/build/third_party/glog)\ninclude_directories(${CMAKE_SOURCE_DIR}/third_party/glog/src)\ninclude_directories(${CMAKE_SOURCE_DIR}/third_party/gflags/src/include)\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Test for Dash-Dash Argument Processing with CMake\nDESCRIPTION: This command adds a gflags test to ensure that the \"--\" argument stops further argument processing. This is a common convention for separating flags from positional arguments.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\n# Make sure -- by itself stops argv processing\nadd_gflags_test(dashdash 0 \"PASS\" \"\"  gflags_unittest  -- --help)\n```\n\n----------------------------------------\n\nTITLE: Globbing Header Files\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all header files (`*.h`) in the `../../include/fst/extensions/ngram/` directory and stores them in the `HEADER_FILES` variable. The `message(STATUS)` command then prints the contents of the `HEADER_FILES` variable to the console during the CMake configuration process.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/ngram/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB HEADER_FILES ../../include/fst/extensions/ngram/*.h)\nmessage(STATUS \"${HEADER_FILES}\")\n```\n\n----------------------------------------\n\nTITLE: Adding Binaries Subdirectory - CMake\nDESCRIPTION: Adds the `bin` subdirectory to the build. This directory typically contains the main source files for the executable targets of the project, which will be compiled and linked according to the rules defined within its CMakeLists.txt file.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/websocket/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(bin)\n```\n\n----------------------------------------\n\nTITLE: Defining the yaml-cpp Library and Setting Basic Properties in CMake\nDESCRIPTION: This snippet defines the yaml-cpp library using `add_library` with sources specified by `${library_sources}`. It then sets essential target properties like compilation flags (`yaml_c_flags`, `yaml_cxx_flags`), library version (`YAML_CPP_VERSION`), shared object version (`SOVERSION`), and a project label.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/yaml-cpp/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(yaml-cpp ${library_sources})\nset_target_properties(yaml-cpp PROPERTIES\n  COMPILE_FLAGS \"${yaml_c_flags} ${yaml_cxx_flags}\"\n)\n\nset_target_properties(yaml-cpp PROPERTIES\n\tVERSION \"${YAML_CPP_VERSION}\"\n\tSOVERSION \"${YAML_CPP_VERSION_MAJOR}.${YAML_CPP_VERSION_MINOR}\"\n\tPROJECT_LABEL \"yaml-cpp ${LABEL_SUFFIX}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Including CMake Utilities\nDESCRIPTION: Includes a custom CMake utility file (`utils.cmake`) located in the `cmake` subdirectory of the current source directory. This file likely contains helper macros and functions used later in the script, such as `gflags_define`.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# includes\ninclude (\"${CMAKE_CURRENT_SOURCE_DIR}/cmake/utils.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Building Lookahead Modules Using the add_module Function in CMake\nDESCRIPTION: These lines utilize the previously defined `add_module` function to build three separate lookahead modules: `arc_lookahead-fst`, `ilabel_lookahead-fst`, and `olabel_lookahead-fst`. Each call specifies the module name, the `MODULE` type (typically indicating a plugin or dynamically loaded library), and the corresponding C++ source file. The `add_module` function handles the library creation, linking, property setting, and installation for each module.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/lookahead/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_module(arc_lookahead-fst MODULE\n  arc_lookahead-fst.cc)\n \nadd_module(ilabel_lookahead-fst MODULE\n  ilabel_lookahead-fst.cc)\n\nadd_module(olabel_lookahead-fst MODULE\n  olabel_lookahead-fst.cc)\n```\n\n----------------------------------------\n\nTITLE: Define Glog Source Files\nDESCRIPTION: Sets the `GLOG_SRCS` variable with the core source and header files required to build the glog library. It includes the previously defined public headers and adds implementation (.cc) and internal header (.h) files from the 'src' directory. It also conditionally adds platform-specific files (signalhandler, windows ports) based on system capabilities or targets (PTHREAD, WIN32, CYGWIN).\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nset (GLOG_SRCS\n  ${GLOG_PUBLIC_H}\n  src/base/commandlineflags.h\n  src/base/googleinit.h\n  src/base/mutex.h\n  src/demangle.cc\n  src/demangle.h\n  src/logging.cc\n  src/raw_logging.cc\n  src/symbolize.cc\n  src/symbolize.h\n  src/utilities.cc\n  src/utilities.h\n  src/vlog_is_on.cc\n)\n\nif (HAVE_PTHREAD OR WIN32 OR CYGWIN)\n  list (APPEND GLOG_SRCS src/signalhandler.cc)\nendif (HAVE_PTHREAD OR WIN32 OR CYGWIN)\n\nif (CYGWIN OR WIN32)\n  list (APPEND GLOG_SRCS\n    src/windows/port.cc\n    src/windows/port.h\n  )\nendif (CYGWIN OR WIN32)\n```\n\n----------------------------------------\n\nTITLE: Strip Whitespace from Static Link Options\nDESCRIPTION: Removes leading and trailing whitespace from the `glog_libraries_options_for_static_linking` variable. This ensures the generated string containing linker flags for static builds is clean.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/glog/CMakeLists.txt#_snippet_26\n\nLANGUAGE: CMake\nCODE:\n```\nstring (STRIP \"${glog_libraries_options_for_static_linking}\" glog_libraries_options_for_static_linking)\n```\n\n----------------------------------------\n\nTITLE: Configuring Special FST Executable Targets with CMake\nDESCRIPTION: Defines an executable target named fstspecial-bin if the HAVE_BIN flag is set. It compiles multiple source files related to special FST components, sets target properties such as folder and output name, and links necessary libraries including fstscript, fst, and system dynamic loading libraries. This snippet manages conditional compilation of tools related to FST extensions.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/openfst/src/extensions/special/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_BIN)\n  add_executable(fstspecial-bin\n    ../../bin/fstconvert.cc\n    ../../bin/fstconvert-main.cc\n    phi-fst.cc\n    rho-fst.cc\n    sigma-fst.cc\n  )\n\n  set_target_properties(fstspecial-bin PROPERTIES\n    FOLDER special/bin\n    OUTPUT_NAME fstspecial\n  )\n\n  target_link_libraries(fstspecial-bin\n    fstscript\n    fst\n    ${CMAKE_DL_LIBS}\n  )\nendif(HAVE_BIN)\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Tests for Flagfile Loading with CMake\nDESCRIPTION: These commands add gflags tests to verify that flags can be successfully loaded from flagfiles using the --flagfile option. The tests check for specific outputs or the absence of errors when loading flags from different flagfile.X files.\nSOURCE: https://github.com/modelscope/funasr/blob/main/runtime/onnxruntime/third_party/gflags/test/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\n# See if we can successfully load our flags from the flagfile\nadd_gflags_test(flagfile.1 0 \"gflags_unittest\" \"${SLASH}gflags_unittest.cc:\"  gflags_unittest  \"--flagfile=flagfile.1\")\nadd_gflags_test(flagfile.2 0 \"PASS\" \"\"  gflags_unittest  \"--flagfile=flagfile.2\")\nadd_gflags_test(flagfile.3 0 \"PASS\" \"\"  gflags_unittest  \"--flagfile=flagfile.3\")\n```"
  }
]
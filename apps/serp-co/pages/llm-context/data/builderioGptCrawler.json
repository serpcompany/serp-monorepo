[
  {
    "owner": "builderio",
    "repo": "gpt-crawler",
    "content": "TITLE: Configuring GPT Crawler in TypeScript\nDESCRIPTION: TypeScript configuration example showing how to set up the crawler to scrape documentation websites. Includes URL, matching pattern, selector and page limit settings.\nSOURCE: https://github.com/builderio/gpt-crawler/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nexport const defaultConfig: Config = {\n  url: \"https://www.builder.io/c/docs/developers\",\n  match: \"https://www.builder.io/c/docs/**\",\n  selector: `.docs-builder-container`,\n  maxPagesToCrawl: 50,\n  outputFileName: \"output.json\",\n};\n```\n\n----------------------------------------\n\nTITLE: GPT Crawler Configuration Type Definition\nDESCRIPTION: TypeScript type definition showing all available configuration options for the crawler including resource exclusions, file size limits and token limits.\nSOURCE: https://github.com/builderio/gpt-crawler/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ntype Config = {\n  /** URL to start the crawl, if sitemap is provided then it will be used instead and download all pages in the sitemap */\n  url: string;\n  /** Pattern to match against for links on a page to subsequently crawl */\n  match: string;\n  /** Selector to grab the inner text from */\n  selector: string;\n  /** Don't crawl more than this many pages */\n  maxPagesToCrawl: number;\n  /** File name for the finished data */\n  outputFileName: string;\n  /** Optional resources to exclude */\n  resourceExclusions?: string[];\n  /** Optional maximum file size in megabytes to include in the output file */\n  maxFileSize?: number;\n  /** Optional maximum number tokens to include in the output file */\n  maxTokens?: number;\n};\n```\n\n----------------------------------------\n\nTITLE: Running GPT Crawler\nDESCRIPTION: Shell command to start the crawler process after configuration.\nSOURCE: https://github.com/builderio/gpt-crawler/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Crawler Dependencies\nDESCRIPTION: Shell commands for cloning the repository and installing Node.js dependencies.\nSOURCE: https://github.com/builderio/gpt-crawler/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/builderio/gpt-crawler\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm i\n```\n\n----------------------------------------\n\nTITLE: Starting GPT Crawler API Server\nDESCRIPTION: Shell command to start the crawler as an API server running on Express.js.\nSOURCE: https://github.com/builderio/gpt-crawler/blob/main/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm run start:server\n```\n\n----------------------------------------\n\nTITLE: Running the GPT Crawler in Docker\nDESCRIPTION: Command sequence for navigating to the containerapp directory and executing the run script to start the containerized GPT crawler.\nSOURCE: https://github.com/builderio/gpt-crawler/blob/main/containerapp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd gpt-crawler/containerapp \n```\n\nLANGUAGE: bash\nCODE:\n```\n. ./run.sh \n```"
  }
]
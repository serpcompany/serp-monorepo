[
  {
    "owner": "astronomer",
    "repo": "astronomer-cosmos",
    "content": "TITLE: Generating Airflow DAG from dbt Project using Cosmos (Python)\nDESCRIPTION: This Python snippet demonstrates how to use the `DbtDag` class from the Astronomer Cosmos library to automatically generate an Apache Airflow DAG based on a dbt project. It requires specifying the path to the dbt project directory and the dbt profile name to use. `DbtDag` handles the conversion of dbt models, tests, and other resources into Airflow tasks, accepting both standard Airflow DAG arguments and Cosmos-specific configurations like `fail_fast` and `cancel_query_on_kill`.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/index.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom cosmos import DbtDag\n\n# Define an Airflow DAG using DbtDag\nwith DbtDag(\n    # Required parameter: Path to the root of the dbt project\n    project_dir=\"./../dbt/jaffle_shop\",\n    # Required parameter: The name of the d profile defined in profiles.yml\n    profile_name=\"default\",\n    # Optional: The dbt target profile (e.g., dev, prod). Defaults to 'dev'\n    target=\"dev\",\n    # Standard Airflow DAG arguments\n    schedule_interval=\"@daily\",\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    dag_id=\"jaffle_shop_basic_cosmos\", # Unique identifier for the DAG\n    default_args={\n        \"retries\": 2 # Example: Retry tasks up to 2 times on failure\n    },\n    # Cosmos specific arguments\n    fail_fast=True, # Immediately fail the DAG if any dbt resource fails\n    cancel_query_on_kill=False, # Do not send cancel signals to the database if the task is killed\n) as dbt_jaffle_shop_dag:\n    # DbtDag automatically generates tasks within this context based on the dbt project structure\n    pass # No explicit tasks need to be defined here\n```\n\n----------------------------------------\n\nTITLE: requirements.txt Entry for Installing Cosmos\nDESCRIPTION: This snippet indicates adding 'astronomer-cosmos' to the project's requirements.txt file to install the Cosmos package which enables dbt project orchestration within Airflow DAGs.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/astro.rst#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nastronomer-cosmos\n```\n\n----------------------------------------\n\nTITLE: Selecting by dbt YAML Selector - Python\nDESCRIPTION: This code snippet utilizes the `selector` parameter in `RenderConfig` to specify a dbt YAML selector, for selecting models. It requires `RenderConfig` and `DbtDag` from `cosmos`. The selector must be defined within the dbt project. This method is primarily used with `LoadMode.DBT_LS` and will ignore the `select` and `exclude` parameters if also used.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, RenderConfig, LoadMode\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        selector=\"my_selector\",  # this selector must be defined in your dbt project\n        load_method=LoadMode.DBT_LS,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting Models with Intersection - Python\nDESCRIPTION: This example uses the intersection operator in model selection, selecting nodes that meet both criteria specified within the `select` parameter. It requires both `RenderConfig` and `DbtDag` from `cosmos`. The selection filters nodes that are both `tag:include_tag1` and `tag:include_tag2`. The parameters are a list of selection strings.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, RenderConfig\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        select=[\"tag:include_tag1,tag:include_tag2\"],  # intersection\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Kubernetes Operator Arguments - Python\nDESCRIPTION: This snippet demonstrates how to configure Kubernetes-specific arguments for a Cosmos operator using a `DbtDag` instance. It configures arguments related to the Kubernetes pod, such as the queue, image, and pull policy, as well as settings like `get_logs`, `is_delete_operator_pod`, and `namespace`.  These parameters are passed within the `operator_args` dictionary when initializing the `DbtDag`.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/operator-args.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    # ...\n    operator_args={\n        \"queue\": \"kubernetes\",\n        \"image\": \"dbt-jaffle-shop:1.0.0\",\n        \"image_pull_policy\": \"Always\",\n        \"get_logs\": True,\n        \"is_delete_operator_pod\": False,\n        \"namespace\": \"default\",\n    },\n    execution_config=ExecutionConfig(\n        execution_mode=ExecutionMode.KUBERNETES,\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining and Configuring ProjectConfig for dbt in Cosmos (Python)\nDESCRIPTION: This code snippet shows how to instantiate the ProjectConfig class from the cosmos.config module, setting various paths, variables, and environment variables for a dbt project within Cosmos. It demonstrates the use of constructor parameters including paths, variables, environment variables, and execution settings. These configurations enable Cosmos to correctly locate project files and manage rendering and execution behaviors, facilitating seamless dbt integration.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/project-config.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom cosmos.config import ProjectConfig\n\nconfig = ProjectConfig(\n    dbt_project_path=\"/path/to/dbt/project\",\n    models_relative_path=\"models\",\n    seeds_relative_path=\"data\",\n    snapshots_relative_path=\"snapshots\",\n    manifest_path=\"/path/to/manifests\",\n    env_vars={\"MY_ENV_VAR\": \"my_env_value\"},\n    dbt_vars={\n        \"my_dbt_var\": \"my_value\",\n        \"start_time\": \"{{ data_interval_start.strftime('%Y%m%d%H%M%S') }}\",\n        \"end_time\": \"{{ data_interval_end.strftime('%Y%m%d%H%M%S') }}\",\n    },\n)\n\n```\n\n----------------------------------------\n\nTITLE: Excluding Models and Children - Python\nDESCRIPTION: This snippet shows how to exclude a node and its children from the DAG using the `exclude` parameter within `RenderConfig`.  It specifies `node_name+` which will exclude node with the name `node_name` and all its children. The `cosmos` and `DbtDag` libraries are prerequisites. The value of `exclude` takes a list of exclusion strings.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, RenderConfig\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        exclude=[\"node_name+\"],  # node_name and its children\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing Kubernetes Cluster Locally with Kind - Bash\nDESCRIPTION: This snippet illustrates the use of Kind (Kubernetes IN Docker) to create a local Kubernetes cluster. It includes commands to create the cluster, deploy Postgres using Helm charts, retrieve and set environment variables for Postgres credentials, expose Postgres to the host machine, verify database connectivity, and create Kubernetes secrets containing connection details. These commands are essential for setting up a local Kubernetes environment that will be used for running dbt tasks via the Cosmos operator.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/kubernetes.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\nhelm install postgres bitnami/postgresql\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport POSTGRES_PASSWORD=$(kubectl get secret --namespace default postgres-postgresql -o jsonpath=\\\"{.data.postgres-password}\\\" | base64 -d)\n```\n\nLANGUAGE: bash\nCODE:\n```\necho $POSTGRES_PASSWORD\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward --namespace default postgres-postgresql-0  5432:5432\n```\n\nLANGUAGE: bash\nCODE:\n```\nPGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -d postgres -p 5432\n\npostgres=# \\dt\n\\q\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic postgres-secrets --from-literal=host=postgres-postgresql.default.svc.cluster.local --from-literal=password=$POSTGRES_PASSWORD\n```\n\n----------------------------------------\n\nTITLE: Custom Error Callback for Slow dbt Queries in Python\nDESCRIPTION: A custom callback function that raises an exception if a dbt query exceeds a specified execution time threshold. The function reads the run_results.json artifact generated by dbt and checks the execution time against a defined threshold.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/callbacks.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef error_if_slow(project_dir: str, **kwargs: Any) -> None:\n    \"\"\"\n    An example of a custom callback that errors if a particular query is slow.\n\n    :param project_dir: Path of the project directory used by Cosmos to run the dbt command\n    \"\"\"\n    import json\n    from pathlib import Path\n\n    slow_query_threshold = 10\n\n    run_results_path = Path(project_dir, \"run_results.json\")\n    if run_results_path.exists():\n        with open(run_results_path) as fp:\n            run_results = json.load(fp)\n            node_name = run_results[\"unique_id\"]\n            execution_time = run_results[\"execution_time\"]\n            if execution_time > slow_query_threshold:\n                raise TimeoutError(\n                    f\"The query for the node {node_name} took too long: {execution_time}\"\n                )\n```\n\n----------------------------------------\n\nTITLE: Implementing Warning Notifications with Slack Webhook in Python\nDESCRIPTION: This snippet shows how to create a callback function that processes test warnings and sends alerts to Slack using the SlackWebhookHook from Airflow. It retrieves test names and results, formats a message, and sends it to a specified Slack channel when warnings are triggered during DAG execution.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/testing-behavior.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag\nfrom airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\nfrom airflow.utils.context import Context\n\ndef warning_callback_func(context: Context):\n    tests = context.get(\"test_names\")\n    results = context.get(\"test_results\")\n\n    warning_msgs = \"\"\"\n```\n\nLANGUAGE: python\nCODE:\n```\n    for test, result in zip(tests, results):\n        warning_msg = f\"\"\"\\n            *Test*: {test}\\n            *Result*: {result}\\n            \"\"\"\n        warning_msgs += warning_msg\n\n    if warning_msgs:\n        slack_msg = f\"\"\"\\n        :large_yellow_circle: Airflow-DBT task with WARN.\\n        *Task*: {context.get('task_instance').task_id}\\n        *Dag*: {context.get('task_instance').dag_id}\\n        *Execution Time*: {context.get('execution_date')}\\n        *Log Url*: {context.get('task_instance').log_url}\\n        {warning_msgs}\\n        \"\"\"\n\n        slack_hook = SlackWebhookHook(slack_webhook_conn_id=\"slack_conn_id\")\n        slack_hook.send(text=slack_msg)\n\n\nexample_dbt_dag = DbtDag(\n    # ...\n    on_warning_callback=warning_callback_func,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow and Astronomer Cosmos with GCP Dependencies (Bash)\nDESCRIPTION: Installs Apache Airflow and Astronomer Cosmos along with necessary extras for dbt-bigquery and gcp-cloud-run-job support within a Python virtual environment. Requires Python 3 and pip.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npip install apache-airflow\npip install \"astronomer-cosmos[dbt-bigquery,gcp-cloud-run-job]\"\n```\n\n----------------------------------------\n\nTITLE: Referencing Custom dbt Node Rendering Logic (Python Example)\nDESCRIPTION: This directive includes a Python code example from an external file demonstrating how to use the `node_converters` parameter within `RenderConfig`. It shows mapping `DbtResourceType` enums (like SOURCE and EXPOSURE) to custom Python functions that define how these dbt nodes should be converted into Airflow tasks, enabling tailored DAG structures.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/render-config.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude::  ../../dev/dags/example_cosmos_sources.py\n    :language: python\n    :start-after: [START custom_dbt_nodes]\n    :end-before: [END custom_dbt_nodes]\n```\n\n----------------------------------------\n\nTITLE: Handling Tests with Multiple Parent Dependencies in Cosmos\nDESCRIPTION: This code outline explains how Cosmos manages tests depending on multiple upstream models. It introduces the parameter 'should_detach_multiple_parents_tests', which, when enabled, detaches such tests into standalone tasks to prevent failures arising from missing relations. The example demonstrates activating this feature to ensure tests on models with multiple dependencies succeed in the DAG execution.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/testing-behavior.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, RenderConfig\n\nexample_multiple_parents_test = DbtDag(\n    ...,\n    render_config=RenderConfig(\n        should_detach_multiple_parents_tests=True,\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Rich Logging in Cosmos using Python Environment Variable\nDESCRIPTION: This snippet demonstrates how to enable the custom logger implementation in Astronomer Cosmos using a Python environment variable. When set to \"True\", this adds \"(astronomer-cosmos)\" to each log message.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/logging.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nAIRFLOW__COSMOS__ENRICH_LOGGING = \"True\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID for AWS S3 (cfg)\nDESCRIPTION: This snippet configures the dbt docs directory and connection ID for accessing documentation stored in AWS S3.  `dbt_docs_dir` is set to the S3 bucket and path, and `dbt_docs_conn_id` is set to the name of the Airflow connection configured for AWS. This allows Airflow to retrieve the dbt docs from S3 using the specified connection.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_3\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ndbt_docs_dir = s3://my-bucket/path/to/docs\ndbt_docs_conn_id = aws_default\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Rendering Behavior in Cosmos - Python\nDESCRIPTION: This snippet demonstrates how to configure the source rendering behavior using the `DbtTaskGroup` and `RenderConfig` objects. It sets the `source_rendering_behavior` to `WITH_TESTS_OR_FRESHNESS`, which renders sources with tests or freshness checks. This configuration ensures only relevant sources are included in the DAG for optimization. Requires dbt-core >= 1.5 and cosmos >= 1.6.0. The example uses `cosmos.constants.SourceRenderingBehavior` to determine the values.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/source-nodes-rendering.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtTaskGroup, RenderConfig\nfrom cosmos.constants import SourceRenderingBehavior\n\njaffle_shop = DbtTaskGroup(\n    render_config=RenderConfig(\n        source_rendering_behavior=SourceRenderingBehavior.WITH_TESTS_OR_FRESHNESS,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project with GCP GCS manifest file\nDESCRIPTION: This snippet demonstrates how to configure a DbtDag to use a manifest.json file stored on Google Cloud Storage (GCS).  It utilizes a manifest_path with a GCS URL and optionally a manifest_conn_id for authentication.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.DBT_MANIFEST,\n        manifest_path=\"gs://bucket/manifest.json\",\n        manifest_conn_id=\"google_cloud\"\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project with local manifest file\nDESCRIPTION: This snippet demonstrates how to configure a DbtDag to use a local manifest.json file. The manifest_path parameter specifies the path to the file.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.DBT_MANIFEST,\n        manifest_path=\"/path/to/manifest.json\"\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Data-Aware Scheduling with Datasets Between dbt Projects\nDESCRIPTION: This example shows how to create a data dependency between two dbt projects, where the second project runs after the first one completes using Airflow's Dataset feature.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/scheduling.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag\n\nproject_one = DbtDag(\n    # ...\n    start_date=datetime(2023, 1, 1),\n    schedule=\"@daily\",\n)\n\nproject_two = DbtDag(\n    schedule=[Dataset(\"postgres://host:5432/database.schema.my_model\")],\n    dbt_project_name=\"project_two\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Time-Based Scheduling for dbt Projects in Cosmos\nDESCRIPTION: This snippet demonstrates how to schedule a dbt project to run daily starting from January 1, 2023, using Airflow's basic scheduling capabilities with Cosmos.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/scheduling.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag\n\njaffle_shop = DbtDag(\n    # ...\n    start_date=datetime(2023, 1, 1),\n    schedule=\"@daily\",\n)\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project using dbt ls output file\nDESCRIPTION: This snippet illustrates how to configure DbtDag to parse a dbt project using the output of `dbt ls --output json` command from a file. The dbt_ls_path parameter specifies the path to the file.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.DBT_LS_FILE, dbt_ls_path=\"/path/to/dbt_ls_file.txt\"\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring a DbtDag with custom project directory\nDESCRIPTION: This Python code demonstrates how to create a DbtDag instance with a custom dbt project directory, profile configuration using PostgreSQL, and execution configuration specifying the dbt executable path from the virtual environment.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/open-source.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig\nfrom cosmos.profiles import PostgresUserPasswordProfileMapping\n\nprofile_config = ProfileConfig(\n    profile_name=\"default\",\n    target_name=\"dev\",\n    profile_mapping=PostgresUserPasswordProfileMapping(\n        conn_id=\"airflow_db\",\n        profile_args={\"schema\": \"public\"},\n    ),\n)\n\nmy_cosmos_dag = DbtDag(\n    project_config=ProjectConfig(\n        \"/usr/local/airflow/dags/my_dbt_project\",\n    ),\n    profile_config=profile_config,\n    execution_config=ExecutionConfig(\n        dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\",\n    ),\n    # normal dag parameters\n    schedule_interval=\"@daily\",\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    dag_id=\"my_cosmos_dag\",\n    default_args={\"retries\": 2},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataset-Triggered DAG in Airflow\nDESCRIPTION: This example demonstrates an Airflow DAG that is triggered when a specific Dataset (the orders table) is produced by a Cosmos dbt run. Compatible with Cosmos 1.1+ and Airflow 2.4+.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/scheduling.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.datasets import Dataset\nfrom airflow.operators.empty import EmptyOperator\n\n\nwith DAG(\n    \"dataset_triggered_dag\",\n    description=\"A DAG that should be triggered via Dataset\",\n    start_date=datetime(2024, 9, 1),\n    schedule=[Dataset(uri=\"postgres://0.0.0.0:5434/postgres.public.orders\")],\n) as dag:\n    t1 = EmptyOperator(\n        task_id=\"task_1\",\n    )\n    t2 = EmptyOperator(\n        task_id=\"task_2\",\n    )\n\n    t1 >> t2\n```\n\n----------------------------------------\n\nTITLE: Disabling Compiled SQL Storage in Cosmos DbtDag with Python\nDESCRIPTION: This Python snippet demonstrates how to disable the storage of compiled SQL in the task's `template_fields` when using Astronomer Cosmos in local execution mode. It involves passing `{\"should_store_compiled_sql\": False}` via the `operator_args` parameter when initializing the `DbtDag`.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/compiled-sql.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag\n\nDbtDag(\n    operator_args={\"should_store_compiled_sql\": False},\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Cosmos DAG for MWAA Local Execution\nDESCRIPTION: Complete Python example of an Airflow DAG (`my_cosmos_dag.py`) using `DbtDag` from `astronomer-cosmos`. It imports necessary Cosmos classes (`DbtDag`, `ProjectConfig`, `ProfileConfig`, `ExecutionConfig`) and profile mappings (`PostgresUserPasswordProfileMapping`). It defines database connection profiles using `ProfileConfig`, referencing an Airflow connection ID (`airflow_db`). Crucially, it sets the `dbt_executable_path` in `ExecutionConfig` to point to the dbt binary within the virtual environment created by the MWAA startup script (`${os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt`). Standard Airflow DAG arguments like `schedule_interval`, `start_date`, `catchup`, `dag_id`, and `default_args` are also included.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/mwaa.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom datetime import datetime\nfrom cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig\nfrom cosmos.profiles import PostgresUserPasswordProfileMapping\n\nprofile_config = ProfileConfig(\n    profile_name=\"default\",\n    target_name=\"dev\",\n    profile_mapping=PostgresUserPasswordProfileMapping(\n        conn_id=\"airflow_db\",\n        profile_args={\"schema\": \"public\"},\n    ),\n)\n\nexecution_config = ExecutionConfig(\n    dbt_executable_path=f\"{os.environ['AIRFLOW_HOME']}/dbt_venv/bin/dbt\",\n)\n\nmy_cosmos_dag = DbtDag(\n    project_config=ProjectConfig(\n        \"<my_dbt_project>\",\n    ),\n    profile_config=profile_config,\n    execution_config=execution_config,\n    # normal dag parameters\n    schedule_interval=\"@daily\",\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    dag_id=\"my_cosmos_dag\",\n    default_args={\"retries\": 2},\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting Models by Config - Python\nDESCRIPTION: This snippet showcases selecting dbt models based on a specific configuration using the `select` parameter within `RenderConfig`. It requires `cosmos` and `DbtDag` classes and uses `config.schema:prod` to select models with the schema set to prod.  This parameter takes a list of strings representing the selection criteria.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        select=[\"config.schema:prod\"],\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Docs with Static Flag and GCS\nDESCRIPTION: This snippet shows how to use the `--static` flag for `DbtDocsGCSOperator`. The static flag generates a single HTML file to be hosted on cloud storage. It takes the task ID, project directory, profile configuration, connection ID, bucket name, and `dbt_cmd_flags` to pass the --static flag.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/generating-docs.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.operators import DbtDocsGCSOperator\n\n# then, in your DAG code:\ngenerate_dbt_docs_aws = DbtDocsGCSOperator(\n    task_id=\"generate_dbt_docs_gcs\",\n    project_dir=\"path/to/jaffle_shop\",\n    profile_config=profile_config,\n    # docs-specific arguments\n    connection_id=\"test_gcs\",\n    bucket_name=\"test_bucket\",\n    dbt_cmd_flags=[\"--static\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID in Airflow (cfg)\nDESCRIPTION: This snippet demonstrates how to specify the path to the dbt docs and the connection ID for cloud storage using the Airflow configuration file. The `dbt_docs_dir` variable points to the location of the dbt documentation, while `dbt_docs_conn_id` specifies the Airflow connection to use for accessing cloud storage.  This configuration allows Airflow to correctly locate and serve the dbt documentation through the Cosmos plugin.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_0\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ndbt_docs_dir = path/to/docs/here\ndbt_docs_conn_id = my_conn_id\n```\n\n----------------------------------------\n\nTITLE: Normalizing Task IDs for Non-ASCII Display Names in Cosmos\nDESCRIPTION: Demonstrates how to define a function (`normalize_task_id`) using the `slugify` library to convert potentially non-ASCII model names into ASCII-compatible strings suitable for Airflow task IDs. This function is then passed to the `RenderConfig` of a `DbtTaskGroup` to customize task ID generation, allowing the original model name to be used as the `display_name` in the Airflow UI (requires Airflow >= 2.9). Note the potential limitations of `slugify` regarding uniqueness.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/task-display-name.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom slugify import slugify\n\n\ndef normalize_task_id(node):\n    return slugify(node.name)\n\n\nfrom cosmos import DbtTaskGroup, RenderConfig\n\njaffle_shop = DbtTaskGroup(\n    render_config=RenderConfig(normalize_task_id=normalize_task_id)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Index Filename (cfg)\nDESCRIPTION: This snippet shows how to specify the index filename for dbt docs if the `--static` flag was used during the dbt docs generation.  Setting the `dbt_docs_index_file_name` allows Airflow to correctly identify the entry point to the documentation when using static files. This is used when the docs were generated with the `--static` flag.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_2\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ndbt_docs_index_file_name = static_index.html\n```\n\n----------------------------------------\n\nTITLE: Setting Default Operator Arguments - Python\nDESCRIPTION: This snippet shows how to configure default operator arguments within a `DbtTaskGroup` instance using `default_args`.  This is intended for cases where you have common settings across multiple tasks. The `default_args` dictionary sets the `pool` to \"default_pool\". This can be overridden at the dbt model level using dbt meta properties in the dbt_project.yml.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/operator-args.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndbt_task_group = DbtTaskGroup(\n    # ...\n    profile_config=ProfileConfig,\n    default_args={\"pool\": \"default_pool\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Arguments - Example Usage - Python\nDESCRIPTION: This code provides a comprehensive example of setting various operator arguments using a `DbtTaskGroup` instance. It covers arguments like `append_env`, `dbt_cmd_flags`, `dbt_cmd_global_flags`, `dbt_executable_path`, `env`, `fail_fast`, `no_version_check`, `quiet`, `vars`, `warn_error`, `cancel_query_on_kill`, `output_enconding`, and `skip_exit_code`. The example demonstrates the practical application of these arguments. The `vars` argument uses Jinja templating to inject values from the data interval.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/operator-args.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDbtTaskGroup(\n    # ...\n    operator_args={\n        \"append_env\": True,\n        \"dbt_cmd_flags\": [\"--models\", \"stg_customers\"],\n        \"dbt_cmd_global_flags\": [\"--cache-selected-only\"],\n        \"dbt_executable_path\": Path(\"/home/user/dbt\"),\n        \"env\": {\"MY_ENVVAR\": \"some-value\"},\n        \"fail_fast\": True,\n        \"no_version_check\": True,\n        \"quiet\": True,\n        \"vars\": {\n            \"start_time\": \"{{ data_interval_start.strftime('%Y%m%d%H%M%S') }}\",\n            \"end_time\": \"{{ data_interval_end.strftime('%Y%m%d%H%M%S') }}\",\n        },\n        \"warn_error\": True,\n        \"cancel_query_on_kill\": False,\n        \"output_enconding\": \"utf-8\",\n        \"skip_exit_code\": 1,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Example Cosmos dbt Kubernetes Operator DAG - Python\nDESCRIPTION: This Python snippet, referenced from an external DAG file, illustrates how to define and trigger a DAG using the Cosmos dbt Kubernetes Operator. It configures the operator to spin up a Kubernetes pod that runs dbt commands against a Postgres database running inside the local Kind Kubernetes cluster. This DAG integrates the setup described in the tutorial for orchestrating dbt transformations with Airflow on Kubernetes.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/kubernetes.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom astronomer_k8s_operator import DbtKubernetesOperator\nfrom datetime import datetime\n\nwith DAG(\n    dag_id='jaffle_shop_k8s',\n    schedule_interval='@daily',\n    start_date=datetime(2022, 1, 1),\n    catchup=False\n) as dag:\n\n    dbt_run = DbtKubernetesOperator(\n        task_id='dbt_run',\n        operator_args={\n            \"image\": \"dbt-jaffle-shop:1.0.0\",\n            \"namespace\": \"default\",\n            \"secrets\": [\"postgres-secrets\"],\n            \"pod_template_file\": \"pod_template.yaml\"\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Importing DbtDag for Cosmos dbt Airflow\nDESCRIPTION: Imports the `DbtDag` class from the Astronomer Cosmos provider. This class provides a higher-level abstraction for defining an entire dbt project as a single Airflow DAG, automatically creating tasks based on the dbt project's structure.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/CHANGELOG.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.providers.dbt.core.dag import DbtDag\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for dbt Project with Postgres Profile - Bash\nDESCRIPTION: This snippet demonstrates the process for creating a Docker image that packages dbt project files and a dbt profile configured for Postgres. The image is built using a specified Dockerfile designed for Kubernetes deployments. It also includes environment variable settings to handle issues common on Apple M1 hardware during Docker image builds. The resulting image enables Airflow to create K8s pods running dbt using this prepared container.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/kubernetes.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t dbt-jaffle-shop:1.0.0 -f Dockerfile.postgres_profile_docker_k8s .\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64\n```\n\nLANGUAGE: bash\nCODE:\n```\nkind load docker-image dbt-jaffle-shop:1.0.0\n```\n\n----------------------------------------\n\nTITLE: Setting Cosmos Operator Arguments - Python\nDESCRIPTION: This Python code snippet demonstrates setting a Cosmos-specific operator argument, specifically `dbt_cmd_global_flags`, using a `DbtDag` instance. The `operator_args` dictionary is used to pass a list of global flags to the dbt command, which in this example, uses the \"--cache-selected-only\" flag. The flags are passed within the `operator_args` when initializing the `DbtDag`.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/operator-args.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    # ...\n    operator_args={\"dbt_cmd_global_flags\": [\"--cache-selected-only\"]}\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local Airflow Environment and Installing Astronomer Cosmos - Bash\nDESCRIPTION: These commands prepare a Python virtual environment, install or upgrade pip, and then install the Astronomer Cosmos package with support for dbt and Postgres integration via Kubernetes operators. The snippet also includes copying DAG files into Airflow's home directory and running the Airflow standalone server. It highlights the need for sufficient permissions to interact with Docker and Kubernetes clusters to enable proper scheduling and triggering of dbt jobs.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/kubernetes.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install \"astronomer-cosmos[dbt-postgres]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ncp -r dags $AIRFLOW_HOME/\n```\n\nLANGUAGE: bash\nCODE:\n```\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Configuring Cosmos Partial Parsing Caching in airflow.cfg\nDESCRIPTION: Demonstrates how to configure Cosmos's caching behavior for partial parsing using the standard Airflow configuration file (`airflow.cfg`). It shows parameters to override the default cache directory (`cache_dir`) and disable caching entirely (`enable_cache_partial_parse`) within the `[cosmos]` section.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/partial-parsing.rst#_snippet_2\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ncache_dir = path/to/docs/here  # to override default caching directory (by default, uses the system temporary directory)\nenable_cache_partial_parse = False  # to disable caching (enabled by default)\n```\n\n----------------------------------------\n\nTITLE: Creating Airflow DAG for dbt with Virtualenv and Profile Mapping - Astronomer Cosmos - Python\nDESCRIPTION: This snippet illustrates initializing an Airflow DAG that runs a dbt project using Astronomer Cosmos. It configures the dbt project, Airflow profile with user/password mapping for Postgres, and enforces execution in a Python virtual environment via ExecutionMode.VIRTUALENV. The 'operator_args' dictionary specifies Python package requirements, including the dbt adapter. Dependencies include Cosmos, Airflow, and applicable dbt adapters. Key parameters include paths to the dbt project, adapter package, Airflow connection, and DAG scheduling details. Returns an Airflow DAG object ready to orchestrate dbt runs.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcc.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, ProjectConfig, ProfileConfig, ExecutionConfig\nfrom cosmos.constants import ExecutionMode\nfrom cosmos.profiles import PostgresUserPasswordProfileMapping\n\nprofile_config = ProfileConfig(\n    profile_name=\"default\",\n    target_name=\"dev\",\n    profile_mapping=PostgresUserPasswordProfileMapping(\n        conn_id=\"airflow_db\",\n        profile_args={\"schema\": \"public\"},\n    ),\n)\n\nmy_cosmos_dag = DbtDag(\n    project_config=ProjectConfig(\n        \"<my_dbt_project>\",\n    ),\n    profile_config=profile_config,\n    execution_config=ExecutionConfig(\n        execution_mode=ExecutionMode.VIRTUALENV,\n    ),\n    operator_args={\n        \"py_system_site_packages\": False,\n        \"py_requirements\": [\"<your-adapter>\"],\n    },\n    # normal dag parameters\n    schedule_interval=\"@daily\",\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    dag_id=\"my_cosmos_dag\",\n    default_args={\"retries\": 2},\n)\n```\n\n----------------------------------------\n\nTITLE: Build dbt Docker Image - Bash\nDESCRIPTION: This series of commands clones the cosmos-example repository and builds a Docker image containing the dbt project files and a dbt profile. The Docker image will be supplied to the dbt Docker operators used in the DAG.  The build command uses a specific Dockerfile to ensure that the dbt project and the necessary configuration are present.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/docker.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/astronomer/cosmos-example.git\n    cd cosmos-example\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t dbt-jaffle-shop:1.0.0 -f Dockerfile.postgres_profile_docker_k8s .\n```\n\n----------------------------------------\n\nTITLE: Creating a virtual environment for dbt in Dockerfile\nDESCRIPTION: This Docker snippet shows how to create a virtual environment for dbt in your Airflow image to avoid dependency conflicts between dbt and Airflow. It installs your dbt adapter package in an isolated environment.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/open-source.rst#_snippet_0\n\nLANGUAGE: docker\nCODE:\n```\nFROM my-image:latest\n\n# install dbt into a virtual environment\nRUN python -m venv dbt_venv && source dbt_venv/bin/activate && \\\n    pip install --no-cache-dir <your-dbt-adapter> && deactivate\n```\n\n----------------------------------------\n\nTITLE: Setting up Python virtual environment for Cosmos development\nDESCRIPTION: Commands to create a Python virtual environment, install Airflow with required providers, and install astronomer-cosmos in development mode with dbt extensions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv env && source env/bin/activate\npip3 install \"apache-airflow[cncf.kubernetes,openlineage]\"\npip3 install -e \".[dbt-postgres,dbt-databricks]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID for Google Cloud Storage (shell)\nDESCRIPTION: This snippet configures the dbt docs directory and connection ID for accessing documentation stored in Google Cloud Storage (GCS) using environment variables. Setting these variables tells Airflow to retrieve dbt docs from GCS using the configured Airflow connection.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nAIRFLOW__COSMOS__DBT_DOCS_DIR=\"gs://my-bucket/path/to/docs\"\nAIRFLOW__COSMOS__DBT_DOCS_CONN_ID=\"google_cloud_default\"\n```\n\n----------------------------------------\n\nTITLE: Sample YAML configuration for overriding Airflow settings in dbt models\nDESCRIPTION: This YAML snippet demonstrates how to define Airflow configuration overrides using the 'meta' field with 'cosmos' nested under 'models'. The 'operator_args' section specifies parameters such as 'pool' to customize task behavior per model. It assumes familiarity with YAML syntax and dbt model definitions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/custom-airflow-properties.rst#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\nmodels:\n  - name: name\n    description: description\n    meta:\n      cosmos:\n        operator_args:\n            pool: abcd\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow environment variables for local development\nDESCRIPTION: Commands to set environment variables for Airflow, including setting the Airflow home directory to the dev folder and disabling example DAGs.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_HOME=$(pwd)/dev/\nexport AIRFLOW__CORE__LOAD_EXAMPLES=false\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory for Local Storage (shell)\nDESCRIPTION: This snippet configures the dbt docs directory for documentation stored on the local file system using environment variables. Setting this variable ensures that Airflow can access and serve the dbt docs from the specified location on the local file system.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nAIRFLOW__COSMOS__DBT_DOCS_DIR=\"/usr/local/airflow/dags/my_dbt_project/target\"\n```\n\n----------------------------------------\n\nTITLE: Python Script for Creating Cosmos DAG with Project Path\nDESCRIPTION: This Python code demonstrates creating a Cosmos DbtDag instance with the project path specified, allowing Cosmos to locate and run the dbt project within Airflow. The config includes profile and execution settings like the dbt executable path.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/astro.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, ProjectConfig\nfrom cosmos.profiles import PostgresUserPasswordProfileMapping\n\nimport os\nfrom datetime import datetime\n\nairflow_home = os.environ[\"AIRFLOW_HOME\"]\n\nprofile_config = ProfileConfig(\n    profile_name=\"default\",\n    target_name=\"dev\",\n    profile_mapping=PostgresUserPasswordProfileMapping(\n        conn_id=\"airflow_db\",\n        profile_args={\"schema\": \"public\"},\n    ),\n)\n\nmy_cosmos_dag = DbtDag(\n    project_config=ProjectConfig(\n        f\"{airflow_home}/dags/my_dbt_project\",\n    ),\n    profile_config=profile_config,\n    execution_config=ExecutionConfig(\n        dbt_executable_path=f\"{airflow_home}/dbt_venv/bin/dbt\",\n    ),\n    # Additional DAG parameters\n    schedule_interval=\"@daily\",\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    dag_id=\"my_cosmos_dag\",\n    default_args={\"retries\": 2},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating MWAA Startup Script for dbt Virtual Environment\nDESCRIPTION: Shell script designed for MWAA startup to create a Python virtual environment (`dbt_venv`) in the AIRFLOW_HOME directory, install a specified dbt adapter (e.g., dbt-redshift, dbt-snowflake) within it using pip, and manage the PIP_USER context. The script exports the virtual environment path for later use and ensures dbt is isolated, resolving potential dependency conflicts. Replace `<your-dbt-adapter>` with the actual required adapter package.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/mwaa.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n#!/bin/sh\n\nexport DBT_VENV_PATH=\"${AIRFLOW_HOME}/dbt_venv\"\nexport PIP_USER=false\n\npython3 -m venv \"${DBT_VENV_PATH}\"\n\n${DBT_VENV_PATH}/bin/pip install <your-dbt-adapter>\n\nexport PIP_USER=true\n```\n\n----------------------------------------\n\nTITLE: Selecting Models by Path - Python\nDESCRIPTION: This snippet filters dbt models by their file path using the `select` parameter. It utilizes the `path:analytics/tables` selector within `RenderConfig`, which selects models located in the `analytics/tables` directory.  This snippet needs the `cosmos` and `DbtDag` libraries. Both relative and absolute paths are supported.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        select=[\"path:analytics/tables\"],\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Importing DBTRunOperator for Cosmos dbt Airflow\nDESCRIPTION: Imports the operator class used to execute `dbt run` commands within an Apache Airflow DAG via the Astronomer Cosmos provider. This operator compiles and executes the dbt models defined in the project, handling dependencies and materializations.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/CHANGELOG.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.providers.dbt.core.operators import DBTRunOperator\n```\n\n----------------------------------------\n\nTITLE: Generating Fiscal Periods Based on Retail Calendar - SQL\nDESCRIPTION: Illustrates usage of the get_fiscal_periods macro to generate a fiscal calendar aligned with a 4-5-4 week retail period concept. It requires a dbt model reference containing date information generated with the date dimension macros, and parameters to specify the fiscal year-end month and week start day, optionally shifting the year count.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.get_fiscal_periods(ref(\"dates\"), year_end_month, week_start_day) }}\n```\n\n----------------------------------------\n\nTITLE: Running a PostgreSQL Container with Docker in Command Line\nDESCRIPTION: This command pulls the official Postgres Docker image, creates a new container named 'postgres', and exposes ports 5432 and 5433 for external connections. It also sets the POSTGRES_PASSWORD environment variable to 'postgres' for accessing the database. Dependencies: Docker must be installed. Inputs: None explicitly required. Outputs: A running PostgreSQL Docker container on specified ports.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_0\n\nLANGUAGE: commandline\nCODE:\n```\ndocker run --name postgres -p 5432:5432 -p 5433:5433 -e POSTGRES_PASSWORD=postgres postgres\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project using dbt ls command\nDESCRIPTION: This snippet shows how to configure DbtDag to use the `dbt ls` command to generate the manifest.  It requires the dbt executable to be available on the system.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.DBT_LS,\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Cosmos Dependency to requirements.txt\nDESCRIPTION: Text snippet for a `requirements.txt` file, listing `astronomer-cosmos` as a necessary Python package dependency for the Airflow environment. This ensures the Cosmos library is installed alongside other project dependencies.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/mwaa.rst#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nastronomer-cosmos\n```\n\n----------------------------------------\n\nTITLE: Copying dbt artifacts for Astronomer (bash)\nDESCRIPTION: This bash script generates dbt documentation, creates a directory, and copies the manifest.json, catalog.json, and index.html files into it. It's specifically useful for Astronomer deployments where the webserver doesn't have access to the DAGs folder, allowing you to host the dbt docs from a different location.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndbt docs generate\nmkdir dbt_docs_dir\ncp dags/dbt/target/manifest.json dbt_docs_dir/manifest.json\ncp dags/dbt/target/catalog.json dbt_docs_dir/catalog.json\ncp dags/dbt/target/index.html dbt_docs_dir/index.html\n```\n\n----------------------------------------\n\nTITLE: Debugging Cosmos/dbt Partial Parsing Log Messages (Profile Issues)\nDESCRIPTION: Displays example log messages from Cosmos or dbt when partial parsing is disabled because the dbt profile or environment variables used in `profiles.yml` have changed between parsing and execution steps. This indicates that the Cosmos profile configuration guidelines were not followed, specifically regarding the use of mocked profiles or explicit `profiles.yml` paths.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/partial-parsing.rst#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n13:33:16  Unable to do partial parsing because profile has changed\n13:33:16  Unable to do partial parsing because env vars used in profiles.yml have changed\n```\n\n----------------------------------------\n\nTITLE: Activating Python Virtual Environment and Running Airflow Standalone\nDESCRIPTION: These commands activate the Airflow 3 Python virtual environment and start Airflow in standalone mode. Requires a previously created virtual environment at scripts/airflow3/venv-af3. The 'airflow standalone' command initializes the database and launches webserver, scheduler, and trigger processes. Inputs: None directly; assumes correct environment layout. Outputs: Running Airflow instance accessible at http://localhost:8080.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nsource \"$(pwd)/scripts/airflow3/venv-af3/bin/activate\"\n\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Standalone After Main Branch Installation\nDESCRIPTION: This simple command starts Airflow in standalone mode after installation from the main branch. Requires previous setup of virtual environment and updated Airflow installation. Inputs: None required. Outputs: Launches a fresh Airflow instance for testing or development.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Generating Docs and Uploading to Azure Blob Storage\nDESCRIPTION: This code snippet demonstrates how to use the `DbtDocsAzureStorageOperator` to generate dbt documentation and upload it to Azure Blob Storage.  It needs the project directory, profile configuration, Azure connection, and the container name. This snippet is useful for serving the generated dbt documentation directly from Azure Blob Storage.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/generating-docs.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.operators import DbtDocsAzureStorageOperator\n\n# then, in your DAG code:\ngenerate_dbt_docs_azure = DbtDocsAzureStorageOperator(\n    task_id=\"generate_dbt_docs_azure\",\n    project_dir=\"path/to/jaffle_shop\",\n    profile_config=profile_config,\n    # docs-specific arguments\n    connection_id=\"test_azure\",\n    bucket_name=\"$web\",\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing the PostgreSQL Console Using psql Command Line\nDESCRIPTION: This command connects to the running PostgreSQL instance using the psql terminal as the user 'postgres'. This requires the psql client tool to be installed and accessible in your command line. Inputs: Connection parameters (defaults to localhost and standard port). Outputs: Interactive PostgreSQL prompt.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_1\n\nLANGUAGE: commandline\nCODE:\n```\npsql --u postgres\n```\n\n----------------------------------------\n\nTITLE: Install Airflow and Cosmos - Bash\nDESCRIPTION: This snippet demonstrates the installation of Airflow and the astronomer-cosmos package with dbt-postgres dependencies. It creates a Python virtual environment, activates it, and upgrades pip before installing the necessary packages. This step is crucial for setting up the environment for running the dbt DAGs with the Docker operators.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/docker.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\n    source venv/bin/activate\n    pip install --upgrade pip\n    pip install apache-airflow\n    pip install \"astronomer-cosmos[dbt-postgres]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID for Google Cloud Storage (cfg)\nDESCRIPTION: This snippet configures the dbt docs directory and connection ID for accessing documentation stored in Google Cloud Storage (GCS). The `dbt_docs_dir` specifies the GCS bucket and path, and `dbt_docs_conn_id` points to the Airflow connection configured for Google Cloud. This setup allows Airflow to fetch the dbt docs from GCS using the specified connection.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_5\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ndbt_docs_dir = gs://my-bucket/path/to/docs\ndbt_docs_conn_id = google_cloud_default\n```\n\n----------------------------------------\n\nTITLE: Creating an Airflow Database with SQL in PostgreSQL Terminal\nDESCRIPTION: This SQL command creates a new PostgreSQL database named 'airflow_db' for Airflow metadata storage. Should be executed within the psql interactive prompt connected to the correct server. Inputs: None; command alters database catalog. Outputs: New database ready for Airflow configuration.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_2\n\nLANGUAGE: commandline\nCODE:\n```\nCREATE DATABASE airflow_db;\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Metadata\nDESCRIPTION: This JSON object represents the metadata for the 'stg_payments' model. It has a very similar structure to the previous two models, detailing the resource type, package name, file path, and configurations. The object outlines the dependencies including seed 'raw_payments'.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/altered_jaffle_shop/dbt_ls_models_staging.txt#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\"name\": \"stg_payments\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_payments.sql\", \"unique_id\": \"model.jaffle_shop.stg_payments\", \"alias\": \"stg_payments\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_payments\"]}}\n```\n\n----------------------------------------\n\nTITLE: Granting BigQuery DataEditor Role to Service Account (Bash)\nDESCRIPTION: Grants the predefined BigQuery DataEditor IAM role (`roles/bigquery.dataEditor`) to the previously created service account within the specified GCP project ($PROJECT_ID). Uses environment variables for project ID and service account name ($SERVICE_ACCOUNT_NAME). Requires appropriate IAM permissions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# grant DataEditor role\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n--member=\"serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\" \\\n--role=\"roles/bigquery.dataEditor\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID for AWS S3 (shell)\nDESCRIPTION: This snippet configures the dbt docs directory and connection ID for accessing documentation stored in AWS S3 using environment variables. Setting these variables instructs Airflow to retrieve the dbt docs from S3 using the specified Airflow connection.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nAIRFLOW__COSMOS__DBT_DOCS_DIR=\"s3://my-bucket/path/to/docs\"\nAIRFLOW__COSMOS__DBT_DOCS_CONN_ID=\"aws_default\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Seed Metadata\nDESCRIPTION: This JSON object describes the metadata for the 'raw_orders' seed. It is very similar to the 'raw_customers' seed, defining details such as its resource type, package name, file path, and configuration. The inclusion of 'raw_orders' seed implies it is used within the dbt project to feed into the models.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/altered_jaffle_shop/dbt_ls_models_staging.txt#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\"name\": \"raw_orders\", \"resource_type\": \"seed\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"seeds/raw_orders.csv\", \"unique_id\": \"seed.jaffle_shop.raw_orders\", \"alias\": \"raw_orders\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"seed\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"quote_columns\": null, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": []}}\n```\n\n----------------------------------------\n\nTITLE: Running Airflow3 Dependency Installation Script\nDESCRIPTION: This command runs the 'setup.sh' shell script located at scripts/airflow3/setup.sh to install all dependencies required for Apache Airflow 3. It must be executed in a shell environment-aware of Airflow variables, and requires the script to have execution permissions. Inputs: None explicitly when run as shown. Outputs: Installs Python dependencies using pip and other setup steps defined in the script.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsh scripts/airflow3/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID in Airflow (shell)\nDESCRIPTION: This snippet demonstrates how to set the dbt docs directory and connection ID using environment variables. Setting these variables ensures that the Airflow webserver can correctly locate and serve the dbt documentation through the Cosmos plugin. This approach is useful for deployments where configuration via environment variables is preferred.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nAIRFLOW__COSMOS__DBT_DOCS_DIR=\"path/to/docs/here\"\nAIRFLOW__COSMOS__DBT_DOCS_CONN_ID=\"my_conn_id\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Seed Metadata\nDESCRIPTION: This JSON object is the metadata for the 'raw_payments' seed. It provides information about the seed's resource type, package name, file path and configurations. It is similar in structure to the other seed metadata entries.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/altered_jaffle_shop/dbt_ls_models_staging.txt#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\"name\": \"raw_payments\", \"resource_type\": \"seed\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"seeds/raw_payments.csv\", \"unique_id\": \"seed.jaffle_shop.raw_payments\", \"alias\": \"raw_payments\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"seed\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"quote_columns\": null, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": []}}\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Seed raw_orders Configuration (JSON)\nDESCRIPTION: JSON object detailing the configuration and metadata for the 'raw_orders' dbt seed within the 'jaffle_shop' package. It specifies the resource type ('seed'), the source CSV file path ('seeds/raw_orders.csv'), unique ID, and materialization ('seed'). Seeds typically have no node dependencies.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/jaffle_shop/dbt_ls_models_staging.txt#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"raw_orders\", \"resource_type\": \"seed\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"seeds/raw_orders.csv\", \"unique_id\": \"seed.jaffle_shop.raw_orders\", \"alias\": \"raw_orders\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"seed\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"quote_columns\": null, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": []}}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory for HTTP/HTTPS (cfg)\nDESCRIPTION: This snippet configures the dbt docs directory to point to a location served via HTTP or HTTPS. By setting `dbt_docs_dir` to a URL, Airflow will retrieve and serve the dbt docs from the specified web address. Setting `dbt_docs_conn_id` is optional; if set, the `HttpHook` will be used.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_12\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ndbt_docs_dir = https://my-site.com/path/to/docs\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Staging Model Metadata for Orders (JSON)\nDESCRIPTION: This code snippet represents the dbt configuration metadata for staging the 'stg_orders' model in the 'jaffle_shop' package. The JSON specifies attributes like the source SQL file, model alias, dependency on the orders seed table, and view-based materialization. All operational settings, such as schema change behavior and documentation settings, are defined. The model assumes the corresponding seed and adapter are present in the dbt project.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/tests/sample/sample_dbt_ls.txt#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n\"name\": \"stg_orders\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_orders.sql\", \"unique_id\": \"model.jaffle_shop.stg_orders\", \"alias\": \"stg_orders\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_orders\"]}}\n\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Staging Model Metadata for Payments (JSON)\nDESCRIPTION: This JSON snippet details the configuration for the 'stg_payments' staging model, showing resource descriptors, config options such as view materialization, and dependency on the raw payments seed. This configuration is suitable for integration into a dbt analytics project that manages staged payment data transformations, assuming all referenced files and seeds are available within the project.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/tests/sample/sample_dbt_ls.txt#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n\"name\": \"stg_payments\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_payments.sql\", \"unique_id\": \"model.jaffle_shop.stg_payments\", \"alias\": \"stg_payments\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_payments\"]}}\n\n```\n\n----------------------------------------\n\nTITLE: Setup and Trigger DAG - Bash\nDESCRIPTION: This command copies the DAG files from the cosmos-example repository to the Airflow home directory and starts Airflow using the standalone option. This step sets up the necessary files for Airflow to find and run the DAG that uses the dbt Docker operators.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/docker.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp -r dags $AIRFLOW_HOME/\n```\n\nLANGUAGE: bash\nCODE:\n```\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Enabling Rich Logging in Cosmos using CFG Configuration\nDESCRIPTION: This snippet shows how to enable the custom logger implementation in Astronomer Cosmos using a CFG configuration file. When enabled, this adds \"(astronomer-cosmos)\" to each log message.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/logging.rst#_snippet_0\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\nrich_logging = True\n```\n\n----------------------------------------\n\nTITLE: Generating Base Dates Macro Usage in dbt - SQL\nDESCRIPTION: Demonstrates using the get_base_dates macro which wraps dbt_utils.date_spine to generate a series of dates over a specified interval. It supports either defining a start and end date or specifying a number of periods (date parts) trailing from today, with the default granularity being days.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.get_base_dates(start_date=\"2015-01-01\", end_date=\"2023-01-01\") }}\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.get_base_dates(n_dateparts=365*3, datepart=\"day\") }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cosmos Partial Parsing Caching via Environment Variables\nDESCRIPTION: Illustrates how to configure Cosmos's caching behavior for partial parsing using environment variables, which override settings in `airflow.cfg`. It shows the equivalent environment variables (`AIRFLOW__COSMOS__CACHE_DIR` and `AIRFLOW__COSMOS__ENABLE_CACHE_PARTIAL_PARSE`) for overriding the cache directory and disabling caching, respectively.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/partial-parsing.rst#_snippet_3\n\nLANGUAGE: cfg\nCODE:\n```\nAIRFLOW__COSMOS__CACHE_DIR=\"path/to/docs/here\"  # to override default caching directory (by default, uses the system temporary directory)\nAIRFLOW__COSMOS__ENABLE_CACHE_PARTIAL_PARSE=\"False\"  # to disable caching (enabled by default)\n```\n\n----------------------------------------\n\nTITLE: Activating Environment and Installing Airflow from Main Branch\nDESCRIPTION: This command block sources the environment variables and virtual environment, then runs a setup script to install Apache Airflow from the latest main branch. It expects 'install_from_main.sh' script to handle all necessary installation steps, such as cloning or pulling the latest Airflow code. Inputs: Script paths. Outputs: Installs or updates Airflow from source.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nsource scripts/airflow3/env.sh\n\nsource \"$(pwd)/scripts/airflow3/venv-af3/bin/activate\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nsh scripts/airflow3/install_from_main.sh\n```\n\n----------------------------------------\n\nTITLE: Extracting Day of Month from Date - SQL\nDESCRIPTION: Demonstrates usage of the day_of_month macro that extracts the numeric day of the month from a date or timestamp, useful in date dimension models or filters.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_of_month(\"date_col\") }} as day_of_month\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory for HTTP/HTTPS (shell)\nDESCRIPTION: This snippet configures the dbt docs directory to point to a location served via HTTP or HTTPS using environment variables. By setting this variable, Airflow will retrieve and serve the dbt docs from the specified web address.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nAIRFLOW__COSMOS__DBT_DOCS_DIR=\"https://my-site.com/path/to/docs\"\n```\n\n----------------------------------------\n\nTITLE: Authenticating Docker to AWS ECR - Bash\nDESCRIPTION: This Bash command logs Docker in to Amazon ECR using credentials from the AWS CLI. Replace <YOUR_REGION> with your AWS region and <YOUR_ECS_PASSWORD> with your registry endpoint. AWS CLI must be configured and Docker installed locally. The command pipes the resulting ECR token into Docker for secure authentication.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naws ecr-public get-login-password --region <YOUR_REGION> | docker login --username AWS --password-stdin <YOUR_ECS_PASSWORD>\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project with AWS S3 manifest file\nDESCRIPTION: This snippet demonstrates how to configure a DbtDag to use a manifest.json file stored on AWS S3. It uses a manifest_path with an S3 URL and optionally a manifest_conn_id for authentication.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.DBT_MANIFEST,\n        manifest_path=\"s3://bucket/manifest.json\",\n        manifest_conn_id=\"aws_s3\"\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow Standalone Instance using Bash\nDESCRIPTION: Executes the `airflow standalone` command to launch a basic Airflow instance, suitable for development or testing. This command sets up necessary components like the database and webserver for a quick start. Requires Apache Airflow to be installed in the current environment.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Setting GCP Environment Variables for Cosmos Setup (Bash)\nDESCRIPTION: Exports necessary environment variables for configuring GCP resources like Project ID, Region, Repository Name, Image Name, Service Account Name, Dataset Name, and Cloud Run Job Name. Placeholder values must be replaced with actual GCP project details before execution.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport PROJECT_ID=<<<YOUR_GCP_PROJECT_ID>>>\nexport REGION=<<<YOUR_GCP_REGION>>>\nexport REPO_NAME=\"astronomer-cosmos-dbt\"\nexport IMAGE_NAME=\"$REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/cosmos-example\"\nexport SERVICE_ACCOUNT_NAME=\"cloud-run-job-sa\"\nexport DATASET_NAME=\"astronomer_cosmos_example\"\nexport CLOUD_RUN_JOB_NAME=\"astronomer-cosmos-example\"\n```\n\n----------------------------------------\n\nTITLE: Authenticating gcloud CLI with User Credentials (Bash)\nDESCRIPTION: Initiates the Google Cloud SDK authentication flow, prompting the user to log in via a web browser to authorize gcloud access to GCP resources using their Google account. Requires the gcloud SDK to be installed.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth login\n```\n\n----------------------------------------\n\nTITLE: Iterating over dbt graph nodes to map tasks in Airflow (Python)\nDESCRIPTION: This code snippet demonstrates how to iterate through filtered dbt nodes within an Airflow DAG to establish custom upstream relationships. It filters nodes by resource type and maps each node to a corresponding Airflow task using the DbtToAirflowConverter's task map. Upstream tasks are created with EmptyOperator and linked to existing tasks to control execution order.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/dag-customization.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith DbtDag(\n    dag_id=\"customized_cosmos_dag\",\n    # Other arguments omitted for brevity\n) as dag:\n    # Walk the dbt graph\n    for unique_id, dbt_node in dag.dbt_graph.filtered_nodes.items():\n        # Filter by any dbt_node property you prefer. In this case, we are adding upstream tasks to source nodes.\n        if dbt_node.resource_type == DbtResourceType.SOURCE:\n            # Look up the corresponding Airflow task or task group in the DbtToAirflowConverter.tasks_map property.\n            task = dag.tasks_map[unique_id]\n            # Create a task upstream of this Airflow source task/task group.\n            upstream_task = EmptyOperator(task_id=f\"upstream_of_{unique_id}\")\n            upstream_task >> task\n```\n\n----------------------------------------\n\nTITLE: Setting Default GCP Project in gcloud CLI (Bash)\nDESCRIPTION: Configures the default Google Cloud project ID for subsequent gcloud commands using the previously set PROJECT_ID environment variable. Requires gcloud SDK to be installed and authenticated.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngcloud config set project $PROJECT_ID\n```\n\n----------------------------------------\n\nTITLE: Enabling BigQuery API for GCP Project (Bash)\nDESCRIPTION: Enables the Google BigQuery API for the currently configured GCP project using the gcloud command. This step is necessary if BigQuery hasn't been used in the project before. Requires gcloud SDK and appropriate permissions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngcloud services enable bigquery.googleapis.com\n```\n\n----------------------------------------\n\nTITLE: Enabling Artifact Registry API for GCP Project (Bash)\nDESCRIPTION: Enables the Google Artifact Registry API for the currently configured GCP project using the gcloud command. This is a prerequisite for storing and managing container images within GCP. Requires gcloud SDK and appropriate permissions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngcloud services enable artifactregistry.googleapis.com\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project with Azure Blob Storage manifest file\nDESCRIPTION: This snippet demonstrates how to configure a DbtDag to use a manifest.json file stored in Azure Blob Storage. It employs a manifest_path with an Azure blob URL and optionally a manifest_conn_id for authentication.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.DBT_MANIFEST,\n        manifest_path=\"abfs://container/manifest.json\",\n        manifest_conn_id=\"azure_blob_storage\"\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow dbt Compatibility Test Script (Python)\nDESCRIPTION: A `noxfile.py` script used with the `nox` automation tool to systematically test compatibility between specific versions of `apache-airflow` and `dbt-core`. It defines a parameterized session that iterates through predefined version combinations and attempts to install both packages together using `pip3` within isolated virtual environments.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes-local-conflicts.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport nox\n\nnox.options.sessions = [\"compatibility\"]\nnox.options.reuse_existing_virtualenvs = True\n\n\n@nox.session(python=[\"3.10\"])\n@nox.parametrize(\n    \"dbt_version\", [\"1.0\", \"1.1\", \"1.2\", \"1.3\", \"1.4\", \"1.5\", \"1.6\", \"1.7\", \"1.8\"]\n)\n@nox.parametrize(\n    \"airflow_version\", [\"2.2.4\", \"2.3\", \"2.4\", \"2.5\", \"2.6\", \"2.7\", \"2.8\", \"2.9\"]\n)\ndef compatibility(session: nox.Session, airflow_version, dbt_version) -> None:\n    \"\"\"Run both unit and integration tests.\"\"\"\n    session.run(\n        \"pip3\",\n        \"install\",\n        \"--pre\",\n        f\"apache-airflow=={airflow_version}\",\n        f\"dbt-core=={dbt_version}\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Authenticating Docker with GCP Artifact Registry (Bash)\nDESCRIPTION: Authenticates the Docker client to push images to the specified GCP Artifact Registry repository (determined by $REGION) using an OAuth 2.0 access token obtained via gcloud. The token is valid for one hour. Requires gcloud and Docker to be installed and configured.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth print-access-token | docker login -u oauth2accesstoken --password-stdin https://$REGION-docker.pkg.dev\n```\n\n----------------------------------------\n\nTITLE: Cloning Example Cosmos dbt Project Repository (Bash)\nDESCRIPTION: Clones the `cosmos-example` Git repository containing sample dbt projects and configuration files from GitHub into the current directory. Changes the current working directory into the newly cloned `cosmos-example` folder. Requires Git to be installed.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/astronomer/cosmos-example.git\ncd cosmos-example\n```\n\n----------------------------------------\n\nTITLE: Using DatasetAlias for Triggering DAGs in Airflow 2.10+\nDESCRIPTION: This snippet shows how to use DatasetAlias to trigger a DAG when a specific model is run. This approach is available in Cosmos 1.7 with Airflow 2.10 and later versions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/scheduling.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.datasets import DatasetAlias\nfrom airflow.operators.empty import EmptyOperator\n\n\nwith DAG(\n    \"datasetalias_triggered_dag\",\n    description=\"A DAG that should be triggered via Dataset alias\",\n    start_date=datetime(2024, 9, 1),\n    schedule=[DatasetAlias(name=\"basic_cosmos_dag__orders__run\")],\n) as dag:\n\n    t3 = EmptyOperator(\n        task_id=\"task_3\",\n    )\n\n    t3\n```\n\n----------------------------------------\n\nTITLE: Showing importlib-metadata Dependency Conflict Error (Bash)\nDESCRIPTION: Example of a `pip install` error message highlighting a dependency conflict between `apache-airflow` and `dbt-core` involving the `importlib-metadata` library. This specific conflict arises because the two packages require incompatible versions of this library.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes-local-conflicts.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n  ERROR: Cannot install apache-airflow==2.6.0 and dbt-core because these package versions have conflicting dependencies.\n  The conflict is caused by:\n    apache-airflow 2.6.0 depends on importlib-metadata<5.0.0 and >=1.7; python_version < \"3.9\"\n    dbt-semantic-interfaces 0.1.0.dev7 depends on importlib-metadata==6.6.0\n```\n\n----------------------------------------\n\nTITLE: Building dbt Docker Image for Cloud Run Job (Bash)\nDESCRIPTION: Builds a Docker image using the specified Dockerfile (`gcp_cloud_run_job_example/Dockerfile.gcp_cloud_run_job`) located within the `cosmos-example` directory. Tags the image using the IMAGE_NAME environment variable. Requires Docker, being inside the `cosmos-example` directory, and modification of the Dockerfile with GCP project details.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t $IMAGE_NAME -f gcp_cloud_run_job_example/Dockerfile.gcp_cloud_run_job .\n```\n\n----------------------------------------\n\nTITLE: Pushing dbt Docker Image to Artifact Registry (Bash)\nDESCRIPTION: Pushes the previously built and tagged Docker image (specified by the $IMAGE_NAME environment variable) to the configured Google Artifact Registry repository. Requires prior Docker authentication with Artifact Registry.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ndocker push $IMAGE_NAME\n```\n\n----------------------------------------\n\nTITLE: Recommended dbt Project Structure within Airflow DAGs\nDESCRIPTION: Example directory structure showing how to place a dbt project (e.g., `my_dbt_project`) inside a `dbt` subfolder within the main Airflow `dags` directory. A Python file for the Cosmos DAG (`my_cosmos_dag.py`) resides in the root of the `dags` folder. This structure helps organize dbt assets relative to Airflow DAGs, although Cosmos allows configuring alternative paths.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/mwaa.rst#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n dags/\n    dbt/\n       my_dbt_project/\n           dbt_project.yml\n           models/\n              my_model.sql\n              my_other_model.sql\n           macros/\n               my_macro.sql\n               my_other_macro.sql\n    my_cosmos_dag.py\n ...\n```\n\n----------------------------------------\n\nTITLE: Enabling Cloud Run Admin API for GCP Project (Bash)\nDESCRIPTION: Enables the Google Cloud Run Admin API for the currently configured GCP project using the gcloud command. This is required to create and manage Cloud Run services and jobs. Requires gcloud SDK and appropriate permissions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ngcloud services enable run.googleapis.com\n```\n\n----------------------------------------\n\nTITLE: Creating GCP Cloud Run Job Instance (Bash)\nDESCRIPTION: Creates a Google Cloud Run Job using parameters defined by environment variables ($CLOUD_RUN_JOB_NAME, $IMAGE_NAME, $SERVICE_ACCOUNT_NAME, $PROJECT_ID) and specifies configuration like task timeout, max retries, CPU, and memory. Requires the Cloud Run Admin API to be enabled and the specified Docker image to exist in Artifact Registry.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ngcloud run jobs create $CLOUD_RUN_JOB_NAME \\\n--image=$IMAGE_NAME \\\n--task-timeout=180s \\\n--max-retries=0 \\\n--cpu=1 \\\n--memory=512Mi \\\n--service-account=$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\n```\n\n----------------------------------------\n\nTITLE: Installing dbt-date Package in dbt - YAML\nDESCRIPTION: Specifies how to include the dbt-date package in the dbt project by adding it to the packages.yml file with a specific version tag. It requires a working dbt environment and internet connectivity to fetch the package from the public GitHub repository.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npackages:\n  - package: godatadriven/dbt_date\n    version: 0.10.1\n    # <see https://github.com/godatadriven/dbt-date/releases/latest> for the latest version tag\n```\n\n----------------------------------------\n\nTITLE: Cloning the astronomer-cosmos repository\nDESCRIPTION: Commands to clone the astronomer-cosmos repository and navigate to the project directory for local development.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/astronomer/astronomer-cosmos.git\ncd astronomer-cosmos/\n```\n\n----------------------------------------\n\nTITLE: Copying Example DAGs to Airflow Directory (Bash)\nDESCRIPTION: Copies the `dags` directory recursively from the current location (assumed to be the `cosmos-example` repo root) into the Airflow home directory specified by the `$AIRFLOW_HOME` environment variable. This makes the example DAGs discoverable by Airflow.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncp -r dags $AIRFLOW_HOME/\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow Standalone Instance (Bash)\nDESCRIPTION: Starts a local Airflow instance in standalone mode, which bundles the webserver, scheduler, triggerer, and database for development purposes. May require `sudo` for Docker access depending on system configuration.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Debugging Cosmos/dbt Partial Parsing Log Messages (Config Issues)\nDESCRIPTION: Shows an example log message from Cosmos or dbt indicating that partial parsing is disabled because the `--vars`, `--profile`, or `--target` configurations have changed between dbt runs (e.g., between parsing and execution). This specifically relates to the requirement for consistent dbt command-line arguments, particularly relevant when Airflow scheduler and worker are on the same node.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/partial-parsing.rst#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[2024-03-14, 17:04:57 GMT] {{subprocess.py:94}} INFO - Unable to do partial parsing because config vars, config profile, or config target have changed\n```\n\n----------------------------------------\n\nTITLE: Setting up directory permissions for Docker Compose development\nDESCRIPTION: Commands to create required directories for Airflow and set proper permissions to prevent errors on Linux systems when using Docker Compose.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p dev/dags dev/logs dev/plugins\nsudo chown 50000:50000 -R dev/dags dev/logs dev/plugins\n```\n\n----------------------------------------\n\nTITLE: Deleting Cloud Run Job Instance - Bash\nDESCRIPTION: This bash snippet deletes a Cloud Run Job instance. It requires the `gcloud` command-line tool to be installed and configured. The `$CLOUD_RUN_JOB_NAME` environment variable should contain the name of the Cloud Run Job to be deleted. The command removes the job instance identified by the provided name.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n# Delete Cloud Run Job instance\n\ngcloud run jobs delete $CLOUD_RUN_JOB_NAME\n```\n\n----------------------------------------\n\nTITLE: Deleting BigQuery Datasets - Bash\nDESCRIPTION: This bash snippet deletes specified BigQuery datasets and all the tables included. It requires the `bq` command-line tool to be installed and configured. The command deletes the main dataset and the `dbt_dev` dataset.  It utilizes the project ID (`$PROJECT_ID`) and the dataset name (`$DATASET_NAME`) defined in the dbt schema.yml file.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n# Delete BigQuery main and custom dataset specified in dbt schema.yml with all tables included\n\nbq rm -r -f -d $PROJECT_ID:$DATASET_NAME\n\nbq rm -r -f -d $PROJECT_ID:dbt_dev\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project using dbt ls with subprocess\nDESCRIPTION: This snippet shows how to explicitly configure DbtDag to use the `dbt ls` command via subprocess.  This is useful if you specifically want to avoid using the `dbtRunner` invocation mode.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.DBT_LS, invocation_mode=InvocationMode.SUBPROCESS\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose environment for Cosmos development\nDESCRIPTION: Command to start a development environment using Docker Compose, building and running all necessary containers.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose -f dev/docker-compose.yaml up -d --build\n```\n\n----------------------------------------\n\nTITLE: Running tests with hatch for specific Python, Airflow and dbt versions\nDESCRIPTION: Command to run tests for a specific combination of Python, Airflow, and dbt-core versions using hatch as the test runner.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhatch run tests.py3.8-2.4-1.9:test-cov:test-cov\n```\n\n----------------------------------------\n\nTITLE: Creating Date Dimension Model with dbt-date - SQL\nDESCRIPTION: Shows how to invoke the get_date_dimension macro to build a date dimension table with useful date-related columns between specified start and end dates. This macro generates a SQL query that is typically materialized as a model in dbt for use in analytics workflows.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.get_date_dimension(\"2015-01-01\", \"2022-12-31\") }}\n```\n\n----------------------------------------\n\nTITLE: Running tests across all supported version combinations\nDESCRIPTION: Command to run tests for all combinations in the version matrix defined in pyproject.toml using hatch.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhatch run tests:test-cov\n```\n\n----------------------------------------\n\nTITLE: Generating Docs with Custom Callback\nDESCRIPTION: This example demonstrates how to generate dbt docs and then run a custom callback function, `upload_docs`, after generation. This snippet utilizes the `DbtDocsOperator`. The callback is designed to upload the documentation to a storage of the users choice. Dependencies include `S3Hook` from `airflow.providers.amazon.aws.hooks.s3` and requires the project directory and profile configuration.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/generating-docs.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.operators import DbtDocsOperator\n\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\n\ndef upload_to_s3(project_dir: str):\n    # Upload the docs to S3\n    hook = S3Hook(aws_conn_id=\"aws_conn_id\")\n\n    for dir, _, files in os.walk(project_dir):\n        for file in files:\n            hook.load_file(\n                filename=os.path.join(dir, file),\n                key=file,\n                bucket_name=\"my-bucket\",\n                replace=True,\n            )\n\n\ndef upload_docs(project_dir):\n    # upload docs to a storage of your choice\n    # you only need to upload the following files:\n    # - f\"{project_dir}/target/index.html\"\n    # - f\"{project_dir}/target/manifest.json\"\n    # - f\"{project_dir}/target/graph.gpickle\"\n    # - f\"{project_dir}/target/catalog.json\"\n    pass\n\n\n# then, in your DAG code:\ngenerate_dbt_docs = DbtDocsOperator(\n    task_id=\"generate_dbt_docs\",\n    project_dir=\"path/to/jaffle_shop\",\n    profile_config=profile_config,\n    # docs-specific arguments\n    callback=upload_docs,\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Postgres container for integration testing\nDESCRIPTION: Command to run a Postgres database container using Docker for integration tests.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name postgres -p 5432:5432 -p 5433:5433 -e POSTGRES_PASSWORD=postgres postgres\n```\n\n----------------------------------------\n\nTITLE: Running integration tests after initial setup\nDESCRIPTION: Command to run integration tests for a specific Python, Airflow, and dbt version after initial setup is complete.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhatch run tests.py3.8-2.4-1.9:test-integration\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Test Suite with Environment Preparation\nDESCRIPTION: This sequence of commands first sources the Airflow environment variable script, activates the Airflow virtual environment, and then executes the 'tests.sh' script to run Airflow tests. Requires both 'env.sh' and 'venv-af3' to be correctly set up, and the existence of the 'tests.sh' script. Inputs: No arguments other than script paths. Outputs: Runs automated test suite for Airflow installation.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsource scripts/airflow3/env.sh\n\nsource \"$(pwd)/scripts/airflow3/venv-af3/bin/activate\"\n\nsh scripts/airflow3/tests.sh\n```\n\n----------------------------------------\n\nTITLE: Serving documentation locally\nDESCRIPTION: Command to run the documentation server locally in a virtual environment with the correct dependencies.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhatch run docs:serve\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Metadata\nDESCRIPTION: This JSON object represents the metadata for a dbt model named 'stg_customers'. It defines the model's properties like its resource type, package name, file path, and configurations (enabled, materialized as 'view', etc.). The object also specifies dependencies on other nodes, specifically the 'raw_customers' seed.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/altered_jaffle_shop/dbt_ls_models_staging.txt#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\"name\": \"stg_customers\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_customers.sql\", \"unique_id\": \"model.jaffle_shop.stg_customers\", \"alias\": \"stg_customers\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_customers\"]}}\n```\n\n----------------------------------------\n\nTITLE: Incrementing project version for a release\nDESCRIPTION: Command to increment the project version for a new release using hatch.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nhatch version minor\n```\n\n----------------------------------------\n\nTITLE: Selecting Models with Union - Python\nDESCRIPTION: This snippet demonstrates the union operation in model selection using the `select` parameter. It utilizes a list of tags, selecting models that have either `tag:include_tag1` or `tag:include_tag2`. The `RenderConfig` and `DbtDag` from `cosmos` are required for this implementation. This allows inclusion of any nodes that satisfy either condition.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, RenderConfig\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        select=[\"tag:include_tag1\", \"tag:include_tag2\"],  # union\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Sample Cosmos ECS Task Definition - JSON\nDESCRIPTION: Defines a sample AWS ECS (Fargate-compatible) task definition for running a Cosmos job. Replace placeholders such as <YOUR_ACCOUNT_ID>, <YOUR_ECR_REPOSITORY_URI>, and adjust environment variables, CPU/memory as needed. Includes configuration for IAM role, network mode, container definitions, environment, and log forwarding to AWS CloudWatch. Must be saved as a JSON file (e.g., cosmos-task-definition.json) with correct values for a working deployment.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"family\": \"cosmos-job\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\n    \"FARGATE\"\n  ],\n  \"cpu\": \"512\",\n  \"memory\": \"1024\",\n  \"executionRoleArn\": \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/ecsTaskExecutionRole\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"cosmos-job\",\n      \"image\": \"<YOUR_ECR_REPOSITORY_URI>/your_image:latest\",\n      \"essential\": true,\n      \"environment\": [\n        { \"name\": \"VAR1\", \"value\": \"value1\" },\n        { \"name\": \"VAR2\", \"value\": \"value2\" }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/cosmos-job\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Showing jinja2 Dependency Conflict Error (Bash)\nDESCRIPTION: Example of a `pip install` error message indicating a dependency conflict between `apache-airflow` and `dbt-core`. The conflict is caused by incompatible version constraints on the `Jinja2` library required by the specified versions of Airflow and dbt.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes-local-conflicts.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n  ERROR: Cannot install apache-airflow==2.2.4 and dbt-core==1.5.0 because these package versions have conflicting dependencies.\n  The conflict is caused by:\n    apache-airflow 2.2.4 depends on jinja2<3.1 and >=2.10.1\n    dbt-core 1.5.0 depends on Jinja2==3.1.2\n```\n\n----------------------------------------\n\nTITLE: Showing pydantic Dependency Conflict Error (Bash)\nDESCRIPTION: Example of a `pip install` error message encountered when attempting to install conflicting versions of `apache-airflow` and `dbt-core`. This specific conflict arises due to incompatible version requirements for the `pydantic` library imposed by different packages or their dependencies.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes-local-conflicts.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n  The conflict is caused by:\n    apache-airflow 2.8.0 depends on pydantic>=2.3.0\n    dbt-semantic-interfaces 0.4.2 depends on pydantic~=1.10\n    apache-airflow 2.8.0 depends on pydantic>=2.3.0\n    dbt-semantic-interfaces 0.4.2.dev0 depends on pydantic~=1.10\n    apache-airflow 2.8.0 depends on pydantic>=2.3.0\n    dbt-semantic-interfaces 0.4.1 depends on pydantic~=1.10\n    apache-airflow 2.8.0 depends on pydantic>=2.3.0\n    dbt-semantic-interfaces 0.4.0 depends on pydantic~=1.10\n```\n\n----------------------------------------\n\nTITLE: dbt Execution Log Initialization\nDESCRIPTION: Initial log lines from a dbt run, indicating the dbt version, registered adapter (exasol), and a summary count of discovered project resources like models, seeds, and tests for the 'jaffle_shop' project.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/jaffle_shop/dbt_ls_models_staging.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n [0m14:26:04  Running with dbt=1.6.9\n [0m14:26:04  Registered adapter: exasol=1.6.2\n [0m14:26:04  Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 366 macros, 0 groups, 0 semantic models\n```\n\n----------------------------------------\n\nTITLE: Parsing dbt project using custom Cosmos parser\nDESCRIPTION: This snippet configures DbtDag to use the Cosmos custom dbt parser. This method doesn't require the dbt executable but has limitations in feature support.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/parsing-methods.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nDbtDag(\n    render_config=RenderConfig(\n        load_method=LoadMode.CUSTOM,\n    )\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Model stg_customers Configuration (JSON)\nDESCRIPTION: JSON object detailing the configuration and metadata for the 'stg_customers' dbt model within the 'jaffle_shop' package. It specifies the resource type ('model'), file path, unique ID, materialization ('view'), and dependencies (depends on 'seed.jaffle_shop.raw_customers').\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/jaffle_shop/dbt_ls_models_staging.txt#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"stg_customers\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_customers.sql\", \"unique_id\": \"model.jaffle_shop.stg_customers\", \"alias\": \"stg_customers\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_customers\"]}}\n```\n\n----------------------------------------\n\nTITLE: Computing ISO Week End Date - SQL\nDESCRIPTION: Uses the iso_week_end macro to compute the Sunday date that ends the ISO week for a given date column or literal. The macro also supports an optional timezone override for accurate date calculations across timezones.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.iso_week_end(\"date_col\") }} as iso_week_end_date\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.iso_week_end(\"date_col\", tz=\"America/New_York\") }} as iso_week_end_date\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Model stg_orders Configuration (JSON)\nDESCRIPTION: JSON object detailing the configuration and metadata for the 'stg_orders' dbt model within the 'jaffle_shop' package. It specifies the resource type ('model'), file path, unique ID, materialization ('view'), and dependencies (depends on 'seed.jaffle_shop.raw_orders').\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/jaffle_shop/dbt_ls_models_staging.txt#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"stg_orders\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_orders.sql\", \"unique_id\": \"model.jaffle_shop.stg_orders\", \"alias\": \"stg_orders\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_orders\"]}}\n```\n\n----------------------------------------\n\nTITLE: Opting Out of Scarf Analytics Using Environment Variables in Bash\nDESCRIPTION: This code snippet shows the environment variables that users or deployments can set to opt out of Scarf telemetry analytics in Astronomer Cosmos. Setting either 'DO_NOT_TRACK' or 'SCARF_NO_ANALYTICS' to 'True' disables telemetry data collection, providing an alternative to the configuration file-based opt-out.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/PRIVACY_NOTICE.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDO_NOT_TRACK=True\nSCARF_NO_ANALYTICS=True\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Model stg_payments Configuration (JSON)\nDESCRIPTION: JSON object detailing the configuration and metadata for the 'stg_payments' dbt model within the 'jaffle_shop' package. It specifies the resource type ('model'), file path, unique ID, materialization ('view'), and dependencies (depends on 'seed.jaffle_shop.raw_payments').\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/jaffle_shop/dbt_ls_models_staging.txt#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"stg_payments\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_payments.sql\", \"unique_id\": \"model.jaffle_shop.stg_payments\", \"alias\": \"stg_payments\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_payments\"]}}\n```\n\n----------------------------------------\n\nTITLE: Showing multiple Dependency Conflict Errors (Bash)\nDESCRIPTION: Example of a complex `pip install` error message demonstrating multiple simultaneous dependency conflicts between `apache-airflow`, `dbt-core`, and several transitive dependencies like `pyyaml`, `connexion`, and `jsonschema`. This illustrates that conflicts can involve various parts of the dependency trees.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes-local-conflicts.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n    ERROR: Cannot install apache-airflow, apache-airflow==2.7.0 and dbt-core==1.4.0 because these package versions have conflicting dependencies.\n\n    The conflict is caused by:\n        dbt-core 1.4.0 depends on pyyaml>=6.0\n        connexion 2.12.0 depends on PyYAML<6 and >=5.1\n        dbt-core 1.4.0 depends on pyyaml>=6.0\n        connexion 2.11.2 depends on PyYAML<6 and >=5.1\n        dbt-core 1.4.0 depends on pyyaml>=6.0\n        connexion 2.11.1 depends on PyYAML<6 and >=5.1\n        dbt-core 1.4.0 depends on pyyaml>=6.0\n        connexion 2.11.0 depends on PyYAML<6 and >=5.1\n        apache-airflow 2.7.0 depends on jsonschema>=4.18.0\n        flask-appbuilder 4.3.3 depends on jsonschema<5 and >=3\n        connexion 2.10.0 depends on jsonschema<4 and >=2.5.1\n```\n\n----------------------------------------\n\nTITLE: Deleting Artifact Registry Repository - Bash\nDESCRIPTION: This bash snippet deletes an Artifact Registry repository, including all images contained within it. It relies on the `gcloud` command-line tool being installed and correctly configured. The script requires the repository name (`$REPO_NAME`) and the region (`$REGION`) to be passed as environment variables to properly identify and delete the repository.  The `--location` flag specifies the region where the repository is located.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n# Delete Artifact Registry repository with all images included\n\ngcloud artifacts repositories delete $REPO_NAME \\\n    --location=$REGION\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Seed raw_customers Configuration (JSON)\nDESCRIPTION: JSON object detailing the configuration and metadata for the 'raw_customers' dbt seed within the 'jaffle_shop' package. It specifies the resource type ('seed'), the source CSV file path ('seeds/raw_customers.csv'), unique ID, and materialization ('seed'). Seeds typically have no node dependencies.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/jaffle_shop/dbt_ls_models_staging.txt#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"raw_customers\", \"resource_type\": \"seed\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"seeds/raw_customers.csv\", \"unique_id\": \"seed.jaffle_shop.raw_customers\", \"alias\": \"raw_customers\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"seed\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"quote_columns\": null, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": []}}\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID for Azure Blob Storage (shell)\nDESCRIPTION: This snippet configures the dbt docs directory and connection ID for accessing dbt docs stored in Azure Blob Storage using environment variables. Setting these variables ensures that Airflow can correctly retrieve and serve the dbt docs from Azure using the provided connection.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nAIRFLOW__COSMOS__DBT_DOCS_DIR=\"wasb://my-container/path/to/docs\"\nAIRFLOW__COSMOS__DBT_DOCS_CONN_ID=\"wasb_default\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Seed Metadata\nDESCRIPTION: This JSON object describes the metadata for a seed named 'raw_customers'. It specifies the resource type, package name, file path, and configuration details such as 'materialized' as 'seed'. The object also contains the relevant configuration for how the seed is loaded within the dbt project.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/altered_jaffle_shop/dbt_ls_models_staging.txt#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\"name\": \"raw_customers\", \"resource_type\": \"seed\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"seeds/raw_customers.csv\", \"unique_id\": \"seed.jaffle_shop.raw_customers\", \"alias\": \"raw_customers\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"seed\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"quote_columns\": null, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": []}}\n```\n\n----------------------------------------\n\nTITLE: Creating GCP IAM Service Account (Bash)\nDESCRIPTION: Creates a new Identity and Access Management (IAM) service account in the configured GCP project using the name defined in the SERVICE_ACCOUNT_NAME environment variable. This account will be used by the Cloud Run Job.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# create a service account\ngcloud iam service-accounts create $SERVICE_ACCOUNT_NAME\n```\n\n----------------------------------------\n\nTITLE: Importing DbtTaskGroup for Cosmos dbt Airflow\nDESCRIPTION: Imports the `DbtTaskGroup` class from the Astronomer Cosmos provider. This class allows grouping related dbt operations (like run, test, seed) for a specific dbt project or subset of models into a reusable TaskGroup within a larger Airflow DAG.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/CHANGELOG.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.providers.dbt.core.dag import DbtTaskGroup\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Zone Variable for dbt-date - YAML\nDESCRIPTION: Defines the required timezone variable in the dbt_project.yml file to specify the default timezone used by the dbt-date macros. The variable \"dbt_date:time_zone\" accepts any valid tz database timezone string such as \"America/Los_Angeles\" or \"America/New_York\" to localize datetime calculations.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  \"dbt_date:time_zone\": \"America/Los_Angeles\"\n```\n\n----------------------------------------\n\nTITLE: Computing ISO Week of Year Number - SQL\nDESCRIPTION: Demonstrates iso_week_of_year macro usage to return the ISO-standard week number (1-53) of a given date. It optionally accepts a timezone parameter to perform date calculations aligned with a specific timezone.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.iso_week_of_year(\"date_col\") }} as iso_week_of_year\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.iso_week_of_year(\"date_col\", tz=\"America/New_York\") }} as iso_week_of_year\n```\n\n----------------------------------------\n\nTITLE: Converting Timezones with dbt-date Macro - SQL\nDESCRIPTION: Provides a cross-database compatible macro, convert_timezone, to convert timestamps between timezones. The macro accepts a timestamp column, optional target timezone, and optional source timezone. If parameters are omitted, it uses the default timezone configuration defined in dbt-date variables.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.convert_timezone(\"my_column\") }}\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.convert_timezone(\"my_column\", \"America/New_York\") }}\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.convert_timezone(\"my_column\", \"America/New_York\", \"UTC\") }}\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.convert_timezone(\"my_column\", source_tz=\"UTC\") }}\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Seed raw_payments Configuration (JSON)\nDESCRIPTION: JSON object detailing the configuration and metadata for the 'raw_payments' dbt seed within the 'jaffle_shop' package. It specifies the resource type ('seed'), the source CSV file path ('seeds/raw_payments.csv'), unique ID, and materialization ('seed'). Seeds typically have no node dependencies.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/jaffle_shop/dbt_ls_models_staging.txt#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"raw_payments\", \"resource_type\": \"seed\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"seeds/raw_payments.csv\", \"unique_id\": \"seed.jaffle_shop.raw_payments\", \"alias\": \"raw_payments\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"seed\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"quote_columns\": null, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": []}}\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Staging Model Metadata for Customers (JSON)\nDESCRIPTION: This JSON snippet details the full configuration for the 'stg_customers' staging model in a dbt project. It specifies the model's name, type, file path, dependencies (linking to the raw customers seed table), and dbt configuration settings such as materialized as view, handling of schema changes, and documentation controls. No user parameters are required, but it presumes a valid dbt project environment and the presence of the required seed source.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/tests/sample/sample_dbt_ls.txt#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n\"name\": \"stg_customers\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_customers.sql\", \"unique_id\": \"model.jaffle_shop.stg_customers\", \"alias\": \"stg_customers\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_customers\"]}}\n\n```\n\n----------------------------------------\n\nTITLE: Determining Day of Week Number with ISO or Non-ISO - SQL\nDESCRIPTION: Details the day_of_week macro which returns the day of the week as a number starting from 1. By default, it uses ISO convention where Monday = 1 and Sunday = 7, but can be configured to use non-ISO where Sunday = 1. Useful for weekday calculations and business logic.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_of_week(\"'2022-03-06'\") }} as day_of_week_iso\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_of_week(\"'2022-03-06'\", isoweek=False) }} as day_of_week\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow and Astronomer Cosmos - Bash\nDESCRIPTION: This snippet provides Bash commands to create a Python virtual environment, activate it, upgrade pip, and install the required Python packages: apache-airflow, astronomer-cosmos with Amazon extras, and optionally aiobotocore[boto3]. Prerequisites are Python 3, pip, and access to install Python packages. The commands must be executed in order inside a shell. aiobotocore[boto3] is needed only for deferred tasks; all others are required for Cosmos Airflow deployments.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install --upgrade pip\npip install apache-airflow\npip install \"astronomer-cosmos[amazon]\"\npip install \"aiobotocore[boto3]\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Metadata\nDESCRIPTION: This JSON object describes the metadata for the 'stg_orders' model. It shares a similar structure to 'stg_customers', defining the model's resource type, package name, file path, and configuration. It also identifies dependencies on 'raw_orders', implying a relationship within the data pipeline.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/altered_jaffle_shop/dbt_ls_models_staging.txt#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\"name\": \"stg_orders\", \"resource_type\": \"model\", \"package_name\": \"jaffle_shop\", \"original_file_path\": \"models/staging/stg_orders.sql\", \"unique_id\": \"model.jaffle_shop.stg_orders\", \"alias\": \"stg_orders\", \"config\": {\"enabled\": true, \"alias\": null, \"schema\": null, \"database\": null, \"tags\": [], \"meta\": {}, \"group\": null, \"materialized\": \"view\", \"incremental_strategy\": null, \"persist_docs\": {}, \"quoting\": {}, \"column_types\": {}, \"full_refresh\": null, \"unique_key\": null, \"on_schema_change\": \"ignore\", \"on_configuration_change\": \"apply\", \"grants\": {}, \"packages\": [], \"docs\": {\"show\": true, \"node_color\": null}, \"contract\": {\"enforced\": false}, \"post-hook\": [], \"pre-hook\": []}, \"tags\": [], \"depends_on\": {\"macros\": [], \"nodes\": [\"seed.jaffle_shop.raw_orders\"]}}\n```\n\n----------------------------------------\n\nTITLE: Extracting Day of Year from Date - SQL\nDESCRIPTION: Uses the day_of_year macro to get the day number within the year (ranging 1 to 365/366) from a date string or date column. This is useful for seasonal analytics and temporal grouping.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_of_year(\"date_col\") }} as day_of_year\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_of_year(\"'2022-02-02'\") }} as day_of_year\n```\n\n----------------------------------------\n\nTITLE: Running a Cosmos ECS Task with Networking - Bash\nDESCRIPTION: Runs a Cosmos job task on ECS using the AWS CLI's run-task command with Fargate launch type. You must replace subnet-12345678, subnet-87654321, and sg-abcdef12 with your own subnet and security group IDs. Parameters specify the ECS cluster, task definition, network configuration (subnets, security groups, assign public IP), and expected to be executed from a network with AWS CLI installed and authenticated.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naws ecs run-task \\\n  --cluster my-cosmos-cluster \\\n  --launch-type FARGATE \\\n  --task-definition cosmos-job \\\n  --network-configuration \"awsvpcConfiguration={subnets=[subnet-12345678,subnet-87654321],securityGroups=[sg-abcdef12],assignPublicIp=ENABLED}\"\n```\n\n----------------------------------------\n\nTITLE: Computing ISO Week Start Date - SQL\nDESCRIPTION: Uses the iso_week_start macro to find the Monday date that starts the ISO week for a specified date, optionally supporting timezone adjustments for accurate boundary calculations based on timezones.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.iso_week_start(\"date_col\") }} as iso_week_start_date\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.iso_week_start(\"date_col\", tz=\"America/New_York\") }} as iso_week_start_date\n```\n\n----------------------------------------\n\nTITLE: Configuring DbtDag Project Path with Manifest - Astronomer Cosmos - Python\nDESCRIPTION: This Python snippet shows how to create a DbtDag Airflow DAG using the Astronomer Cosmos library. It specifies the dbt project directory and an explicit path to a precompiled 'manifest.json' to improve parsing accuracy. Requirements include the Cosmos package and its dependencies. The 'dbt_project_path' and 'manifest_path' parameters point to the respective directories, and the snippet can be adapted to other project paths as needed.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcc.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, ProjectConfig\n\nmy_cosmos_dag = DbtDag(\n    project_config=ProjectConfig(\n        dbt_project_path=\"/usr/local/airflow/dags/my_dbt_project\",\n        manifest_path=\"/usr/local/airflow/dags/my_dbt_project/target/manifest.json\",\n    ),\n    # ...,\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Last Month Name in Short or Long Form - SQL\nDESCRIPTION: Demonstrates the last_month_name macro that returns the name of the previous month relative to the current date or given context, supporting parameters to return abbreviated or full month names and optional timezone specification for localization.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.last_month_name() }} as last_month_short_name\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.last_month_name(short=true) }} as last_month_short_name\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.last_month_name(short=false) }} as last_month_long_name\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.last_month_name(tz=\"America/New_York\") }} as last_month_short_name\n```\n\n----------------------------------------\n\nTITLE: Selecting Descendants and Ancestors - Python\nDESCRIPTION: This snippet illustrates how to select a node along with its descendants and ancestors needed for building the descendants using the `@` operator in `select`. It selects a model `my_model` and includes all descendants, plus any ancestor nodes necessary for constructing those descendants. The libraries `cosmos`, `DbtDag`, and `RenderConfig` are required. It takes a list of selection strings.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, RenderConfig\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        select=[\"@my_model\"],  # selects my_model, all its descendants,\n        # and all ancestors needed to build those descendants\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Datetime Object - dbt_date macro - SQL (dbt)\nDESCRIPTION: This snippet demonstrates the basic usage of the `dbt_date.datetime` macro. It shows how to create a `datetime` object by providing the year, month, day, hour, and minute parameters, reducing boilerplate syntax.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_16\n\nLANGUAGE: SQL (dbt/Jinja)\nCODE:\n```\n{% set datetime_object = dbt_date.datetime(1997, 9, 29, 6, 14) %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom dbt Project Path in Cosmos DAG\nDESCRIPTION: Python code demonstrating how to instantiate a `DbtDag` from `astronomer-cosmos`, specifying a non-default location for the dbt project (e.g., `/usr/local/airflow/dags/my_dbt_project`) using the `dbt_project_path` parameter within the `ProjectConfig` object. This allows flexibility in organizing dbt projects within the Airflow environment if the default `/usr/local/airflow/dags/dbt` is not used.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/mwaa.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, ProjectConfig\n\nmy_cosmos_dag = DbtDag(\n    project_config=ProjectConfig(\n        dbt_project_path=\"/usr/local/airflow/dags/my_dbt_project\",\n    ),\n    # ..., \n)\n```\n\n----------------------------------------\n\nTITLE: Creating Azure PostgreSQL Server using Azure CLI (Bash)\nDESCRIPTION: Provisions an Azure PostgreSQL flexible server using the Azure CLI command `az postgres server create`. Requires Azure CLI to be installed and the user to be logged into their Azure account. Key parameters include resource group (`-g`), server name (`-n`), admin credentials (`-u`, `-p`), location (`-l`), and SKU (`--sku-name`).\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naz postgres server create -l westeurope -g <<<YOUR_RG>>> -n <<<YOUR_DATABASE_NAME>>> -u dbadmin -p <<<YOUR_PASSWORD_HERE>>> --sku-name B_Gen5_1 --ssl-enforcement Enabled\n```\n\n----------------------------------------\n\nTITLE: DbtDag AWS_EKS Execution Mode Example in Python\nDESCRIPTION: This snippet shows how to configure a DbtDag to use the `AWS_EKS` execution mode, which runs dbt commands within an AWS EKS cluster. It includes setting up secrets and configuring operator arguments like the cluster name and container image.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npostgres_password_secret = Secret(\n        deploy_type=\"env\",\n        deploy_target=\"POSTGRES_PASSWORD\",\n        secret=\"postgres-secrets\",\n        key=\"password\",\n    )\n\n    docker_cosmos_dag = DbtDag(\n        # ...\n        execution_config=ExecutionConfig(\n            execution_mode=ExecutionMode.AWS_EKS,\n        ),\n        operator_args={\n            \"image\": \"dbt-jaffle-shop:1.0.0\",\n            \"cluster_name\": CLUSTER_NAME,\n            \"get_logs\": True,\n            \"is_delete_operator_pod\": False,\n            \"secrets\": [postgres_password_secret],\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Selecting Models by Tag - Python\nDESCRIPTION: This snippet demonstrates how to select dbt models based on a specific tag using the `select` parameter within the `RenderConfig`.  It requires the `cosmos` library and `DbtDag` and `RenderConfig` classes. The `select` parameter takes a list of filter values such as ``tag:my_tag``. This will include models with the tag ``my_tag`` when constructing the DAG.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/selecting-excluding.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtDag, RenderConfig\n\njaffle_shop = DbtDag(\n    render_config=RenderConfig(\n        select=[\"tag:my_tag\"],\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Building dbt Project Docker Image using Docker CLI (Bash)\nDESCRIPTION: Builds a Docker image using the `docker build` command, tagging it with a user-specified name and version (e.g., `<YOUR_IMAGE_NAME_HERE>:1.0.0`). It utilizes a specific Dockerfile (`-f Dockerfile.azure_container_instance`) located in the current directory (`.`). Requires Docker daemon to be running and the necessary project files/Dockerfile to be present.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t <<<YOUR_IMAGE_NAME_HERE>>:1.0.0 -f Dockerfile.azure_container_instance .\n```\n\n----------------------------------------\n\nTITLE: Setting Invocation Mode in DbtDag with Python\nDESCRIPTION: This code snippet demonstrates how to configure the invocation mode within a DbtDag using the ExecutionConfig class.  It imports InvocationMode from cosmos.constants and sets the execution_mode to ExecutionMode.LOCAL and the invocation_mode to InvocationMode.DBT_RUNNER.  This requires the cosmos library and dbt to be installed, specifically dbt version 1.5.0 or higher for DBT_RUNNER.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom cosmos.constants import InvocationMode\n\ndag = DbtDag(\n    # ...\n    execution_config=ExecutionConfig(\n        execution_mode=ExecutionMode.LOCAL,\n        invocation_mode=InvocationMode.DBT_RUNNER,\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Building and Tagging Docker Images for ECS - Bash\nDESCRIPTION: This snippet shows how to build a Docker image for the Linux amd64 platform using a custom Dockerfile, then tag it for your ECR repository. Replace <LOCAL_IMAGE_NAME>, <YOUR_LOCAL_IMAGE_NAME>, and <YOUR_ECR_REPOSITORY_URI> with your actual names. Docker must be installed and configured.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -f Dockerfile.aws_ecs . --platform=linux/amd64 -t <LOCAL_IMAGE_NAME>\ndocker tag <YOUR_LOCAL_IMAGE_NAME> <YOUR_ECR_REPOSITORY_URI>\n```\n\n----------------------------------------\n\nTITLE: Generating Docs and Uploading to S3\nDESCRIPTION: This code snippet demonstrates how to use the `DbtDocsS3Operator` to generate dbt documentation and upload it to an Amazon S3 bucket. It specifies the project directory, profile configuration, S3 connection ID, and bucket name. This allows static website hosting of dbt documentation directly from S3.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/generating-docs.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.operators import DbtDocsS3Operator\n\n# then, in your DAG code:\ngenerate_dbt_docs_aws = DbtDocsS3Operator(\n    task_id=\"generate_dbt_docs_aws\",\n    project_dir=\"path/to/jaffle_shop\",\n    profile_config=profile_config,\n    # docs-specific arguments\n    connection_id=\"test_aws\",\n    bucket_name=\"test_bucket\",\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Docs and Uploading to GCS\nDESCRIPTION: This code snippet shows how to use the `DbtDocsGCSOperator` to generate and upload dbt documentation to a Google Cloud Storage bucket.  It requires the project directory, profile configuration, GCS connection, and bucket name. This allows you to host the documentation from Google Cloud Storage.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/generating-docs.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.operators import DbtDocsGCSOperator\n\n# then, in your DAG code:\ngenerate_dbt_docs_aws = DbtDocsGCSOperator(\n    task_id=\"generate_dbt_docs_gcs\",\n    project_dir=\"path/to/jaffle_shop\",\n    profile_config=profile_config,\n    # docs-specific arguments\n    connection_id=\"test_gcs\",\n    bucket_name=\"test_bucket\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Build Environment Variables for M1 Macs (Bash)\nDESCRIPTION: Sets environment variables (`DOCKER_BUILDKIT`, `COMPOSE_DOCKER_CLI_BUILD`, `DOCKER_DEFAULT_PLATFORM`) commonly used to work around Docker build issues on M1 Macs by disabling BuildKit and specifying the target platform as `linux/amd64`. This is typically executed in the shell before running `docker build`.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Behavior Modes in Cosmos with Python\nDESCRIPTION: This code snippet demonstrates how to set the test behavior to 'after all' models using Cosmos's Python classes. By importing relevant classes and setting the 'test_behavior' parameter in RenderConfig, users can control the testing order and execution mode during DAG runs.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/testing-behavior.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos import DbtTaskGroup, RenderConfig\nfrom cosmos.constants import TestBehavior\n\njaffle_shop = DbtTaskGroup(\n    render_config=RenderConfig(\n        test_behavior=TestBehavior.AFTER_ALL,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Granting BigQuery JobUser Role to Service Account (Bash)\nDESCRIPTION: Grants the predefined BigQuery JobUser IAM role (`roles/bigquery.jobUser`) to the previously created service account within the specified GCP project ($PROJECT_ID). Uses environment variables for project ID and service account name ($SERVICE_ACCOUNT_NAME). Requires appropriate IAM permissions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# grant JobUser role\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n--member=\"serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\" \\\n--role=\"roles/bigquery.jobUser\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory and Connection ID for Azure Blob Storage (cfg)\nDESCRIPTION: This snippet configures the dbt docs directory and connection ID for documentation stored in Azure Blob Storage.  `dbt_docs_dir` points to the Azure Blob Storage container and path, and `dbt_docs_conn_id` specifies the Airflow connection configured for Azure. This allows Airflow to access and serve the dbt documentation from Azure Blob Storage using the provided connection.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_7\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ndbt_docs_dir = wasb://my-container/path/to/docs\ndbt_docs_conn_id = wasb_default\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Container Registry using Azure CLI (Bash)\nDESCRIPTION: Creates an Azure Container Registry (ACR) instance using the Azure CLI command `az acr create`. Requires Azure CLI and an active Azure login. Parameters specify the registry name (`--name`), resource group (`--resource-group`), SKU (`--sku Basic`), and enables the admin user (`--admin-enabled`).\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naz acr create --name <<<YOUR_REGISTRY_NAME>>> --resource-group <<<YOUR_RG>>> --sku Basic --admin-enabled\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Artifact Registry Repository (Bash)\nDESCRIPTION: Creates a new Docker repository in Google Artifact Registry using the previously defined environment variables for repository name ($REPO_NAME), location ($REGION), and project ID ($PROJECT_ID). Requires the Artifact Registry API to be enabled and appropriate IAM permissions.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngcloud artifacts repositories create $REPO_NAME \\\n--repository-format=docker \\\n--location=$REGION \\\n--project $PROJECT_ID\n```\n\n----------------------------------------\n\nTITLE: Installing Astronomer Cosmos via requirements.txt - Text\nDESCRIPTION: This snippet demonstrates how to include the 'astronomer-cosmos' package in your Python project's requirements.txt file for installation. No additional dependencies are required beyond standard Python package installation via pip. This entry ensures the Cosmos library is installed in the Composer environment to enable dbt integration.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcc.rst#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nastronomer-cosmos\n```\n\n----------------------------------------\n\nTITLE: DbtDag Azure Container Instance Execution Mode in Python\nDESCRIPTION: This snippet demonstrates configuring a DbtDag to use Azure Container Instances (ACI) as the execution mode. It specifies the `AZURE_CONTAINER_INSTANCE` execution mode and sets the connection IDs for ACI and the Azure Container Registry (ACR), along with the resource group.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocker_cosmos_dag = DbtDag(\n        # ...\n        execution_config=ExecutionConfig(\n            execution_mode=ExecutionMode.AZURE_CONTAINER_INSTANCE\n        ),\n        operator_args={\n            \"ci_conn_id\": \"aci\",\n            \"registry_conn_id\": \"acr\",\n            \"resource_group\": \"my-rg\",\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Copying Example DAGs to Airflow Home using Bash\nDESCRIPTION: Copies the `dags` directory recursively into the location defined by the `$AIRFLOW_HOME` environment variable using the `cp -r` command. This makes the example DAGs available to the Airflow instance. Requires the source `dags` directory to exist and `$AIRFLOW_HOME` to be correctly set.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncp -r dags $AIRFLOW_HOME/\n```\n\n----------------------------------------\n\nTITLE: Exporting Airflow Environment Variables via Bash Script\nDESCRIPTION: This command sources the 'env.sh' script, exporting crucial Apache Airflow environment variables such as AIRFLOW_HOME. It requires the script file at scripts/airflow3/env.sh to exist and to contain correct environment variable exports. Inputs: The script path. Outputs: Updated shell environment with Airflow settings.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/scripts/airflow3/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsource scripts/airflow3/env.sh\n```\n\n----------------------------------------\n\nTITLE: Describing ECS Task Status via AWS CLI - Bash\nDESCRIPTION: Retrieves details and current status of specific ECS tasks using the aws ecs describe-tasks command. Replace <TASK_ID> with the task ID returned by run-task or found in the ECS console. Helpful for debugging and monitoring task execution from the command line.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naws ecs describe-tasks --cluster my-cosmos-cluster --tasks <TASK_ID>\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Docs Directory for Local Storage (cfg)\nDESCRIPTION: This snippet configures the dbt docs directory for documentation stored in the local file system. `dbt_docs_dir` is set to the path where the dbt docs are located. This allows Airflow to serve the documentation directly from the local file system without requiring a cloud storage connection.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/hosting-docs.rst#_snippet_9\n\nLANGUAGE: cfg\nCODE:\n```\n[cosmos]\ndbt_docs_dir = /usr/local/airflow/dags/my_dbt_project/target\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow and Cosmos with ACI Extras using Bash\nDESCRIPTION: Creates a Python virtual environment, activates it, and installs Apache Airflow along with the Astronomer Cosmos package including extras for dbt-postgres and Azure Container Instance support using pip. Requires Python and pip to be installed.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install apache-airflow\npip install \"astronomer-cosmos[dbt-postgres,azure-container-instance]\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry via Cosmos Configuration INI File\nDESCRIPTION: This snippet demonstrates how users can disable telemetry data collection in the Astronomer Cosmos project by setting the 'enable_telemetry' option to 'False' within the '[cosmos]' section of the configuration file. This disables the default data collection performed by the integrated Scarf telemetry service.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/PRIVACY_NOTICE.rst#_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[cosmos] enable_telemetry False\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image to Registry using Docker CLI (Bash)\nDESCRIPTION: Pushes the previously built and tagged Docker image to a container registry using the `docker push` command. Requires Docker and appropriate credentials configured to push to the target registry (e.g., Azure Container Registry). The image name typically includes the registry hostname.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker push <<<YOUR_IMAGE_NAMEHERE>>>:1.0.0\n```\n\n----------------------------------------\n\nTITLE: Creating Datetime Object with Timezone - dbt_date macro - SQL (dbt)\nDESCRIPTION: This snippet illustrates how to use the `dbt_date.datetime` macro while specifying an optional timezone. The `tz` parameter allows overriding the default timezone for the generated `datetime` object.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_17\n\nLANGUAGE: SQL (dbt/Jinja)\nCODE:\n```\n{% set datetime_object = dbt_date.datetime(1997, 9, 29, 6, 14, tz='America/New_York') %}\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Sample for Setting Up Python Virtual Environment\nDESCRIPTION: This Dockerfile snippet shows how to create a virtual environment and install the specified dbt adapter to avoid dependency conflicts between dbt and Airflow within a Docker container.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/astro.rst#_snippet_0\n\nLANGUAGE: docker\nCODE:\n```\nFROM quay.io/astronomer/astro-runtime:11.3.0\n\n# install dbt into a virtual environment\nRUN python -m venv dbt_venv && source dbt_venv/bin/activate && \\\n    pip install --no-cache-dir <your-dbt-adapter> && deactivate\n```\n\n----------------------------------------\n\nTITLE: Importing DBTSeedOperator for Cosmos dbt Airflow\nDESCRIPTION: Imports the operator class specifically designed to execute `dbt seed` commands within an Apache Airflow DAG using the Astronomer Cosmos provider. This operator is used to load data from CSV files into the dbt project's target database.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/CHANGELOG.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.providers.dbt.core.operators import DBTSeedOperator\n```\n\n----------------------------------------\n\nTITLE: Docker Build Configuration for M1 - Bash\nDESCRIPTION: This snippet provides instructions on setting environment variables if the docker build fails on M1 machines. It exports DOCKER_BUILDKIT and COMPOSE_DOCKER_CLI_BUILD to disable buildkit and CLI build, and DOCKER_DEFAULT_PLATFORM is configured to use linux/amd64 architecture.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/docker.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_BUILDKIT=0\n        export COMPOSE_DOCKER_CLI_BUILD=0\n        export DOCKER_DEFAULT_PLATFORM=linux/amd64\n```\n\n----------------------------------------\n\nTITLE: Converting Epoch Timestamp to Timestamp - SQL\nDESCRIPTION: Illustrates from_unixtimestamp macro which converts an epoch integer or bigint into a SQL timestamp type. The macro defaults to interpreting the epoch as seconds but can be configured to milliseconds or other units depending on your data source.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.from_unixtimestamp(\"epoch_column\") }} as timestamp_column\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.from_unixtimestamp(\"epoch_column\", format=\"milliseconds\") }} as timestamp_column\n```\n\n----------------------------------------\n\nTITLE: Importing DBTTestOperator for Cosmos dbt Airflow\nDESCRIPTION: Imports the operator class dedicated to executing `dbt test` commands within an Apache Airflow DAG using the Astronomer Cosmos provider. This operator runs tests defined in the dbt project to ensure data quality and model correctness.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/CHANGELOG.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.providers.dbt.core.operators import DBTTestOperator\n```\n\n----------------------------------------\n\nTITLE: DbtDag Local Execution Mode Example in Python\nDESCRIPTION: This code snippet demonstrates how to configure a DbtDag to use the `local` execution mode. It assumes that dbt is installed and accessible within the Airflow worker's environment.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/execution-modes.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndocker_cosmos_dag = DbtDag(\n      # ...\n      execution_config=ExecutionConfig(\n          execution_mode=ExecutionMode.DOCKER,\n      ),\n      operator_args= {\n          \"image\": \"dbt-jaffle-shop:1.0.0\",\n          \"network_mode\": \"bridge\",\n      },\n  )\n```\n\n----------------------------------------\n\nTITLE: Registering an ECS Task Definition via AWS CLI - Bash\nDESCRIPTION: Registers a local ECS task definition JSON file with AWS ECS using the AWS CLI. Replace cosmos-task-definition.json with your definition filename as needed. Requires AWS CLI permissions for ecs:registerTaskDefinition and correct JSON file contents.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naws ecs register-task-definition --cli-input-json file://cosmos-task-definition.json\n```\n\n----------------------------------------\n\nTITLE: Setting up environment variables for integration tests\nDESCRIPTION: Commands to set required environment variables for running integration tests, including database connection information.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport AIRFLOW_HOME=`pwd`\nexport AIRFLOW_CONN_AIRFLOW_DB=postgres://postgres:postgres@0.0.0.0:5432/postgres\nexport DATABRICKS_HOST=''\nexport DATABRICKS_TOKEN=''\nexport DATABRICKS_WAREHOUSE_ID=''\nexport DATABRICKS_CLUSTER_ID=''\nexport POSTGRES_PORT=5432\nexport POSTGRES_SCHEMA=public\nexport POSTGRES_DB=postgres\nexport POSTGRES_PASSWORD=postgres\nexport POSTGRES_USER=postgres\nexport POSTGRES_HOST=localhost\nhatch run tests.py3.8-2.4-1.9:test-cov:test-integration-setup\nhatch run tests.py3.8-2.4-1.9:test-cov:test-integration\n```\n\n----------------------------------------\n\nTITLE: Extracting Weekday Name from Date - SQL\nDESCRIPTION: Shows usage of the day_name macro which returns the name of the weekday from a date column. It supports a parameter to specify if the short form (e.g., Mon) or long form (e.g., Monday) should be returned.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_name(\"date_col\") }} as day_of_week_short_name\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_name(\"date_col\", short=true) }} as day_of_week_short_name\n```\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.day_name(\"date_col\", short=false) }} as day_of_week_long_name\n```\n\n----------------------------------------\n\nTITLE: Extracting Date Part from a Date - SQL\nDESCRIPTION: Uses the date_part macro to extract specific parts (e.g., day of week) from a date or timestamp column. This macro enhances standard SQL date functions with consistent syntax across supported platforms.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/dev/dags/dbt/simple/dbt_packages/dbt_date/README.md#_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n{{ dbt_date.date_part(\"dayofweek\", \"date_col\") }} as day_of_week\n```\n\n----------------------------------------\n\nTITLE: Setup Postgres Database - Bash\nDESCRIPTION: This command launches a PostgreSQL database using Docker. It specifies the database name, user, password, and port mapping. This PostgreSQL database will be used as the target database for the dbt project in this example.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/docker.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name some-postgres -e POSTGRES_PASSWORD=\"<postgres_password>\" -e POSTGRES_USER=postgres -e POSTGRES_DB=postgres -p5432:5432 -d postgres\n```\n\n----------------------------------------\n\nTITLE: Creating an ECS Cluster via AWS CLI - Bash\nDESCRIPTION: Creates a new ECS cluster named my-cosmos-cluster using the AWS CLI. AWS CLI should be configured with necessary permissions to create ECS clusters in your target AWS account. The command outputs cluster metadata on success.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naws ecs create-cluster --cluster-name my-cosmos-cluster\n```\n\n----------------------------------------\n\nTITLE: Logging into Azure Container Registry using Azure CLI (Bash)\nDESCRIPTION: Authenticates the Docker CLI with Azure Container Registry using the `az acr login` command. This step is often necessary before pushing images to ACR and requires Azure CLI to be installed and logged in.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naz acr login\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Images to Amazon ECR - Bash\nDESCRIPTION: Pushes a locally tagged Docker image to your Amazon ECR repository using the docker push command. Replace <YOUR_ECR_REPOSITORY_URI> with your actual repository URI. Requires Docker to be authenticated against the AWS ECR registry.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/aws-container-run-job.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker push <YOUR_ECR_REPOSITORY_URI>\n```\n\n----------------------------------------\n\nTITLE: Implementing a Cache Cleanup DAG in Python\nDESCRIPTION: Example Python code that creates a DAG to clean up stale Cosmos cache stored in Airflow Variables. It will delete any cache associated with Cosmos that has not been used for the last five days.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/configuration/caching.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nRenderConfig(airflow_vars_to_purge_cache == [\"refresh_cache\"])\n```\n\n----------------------------------------\n\nTITLE: Cloning Example Repository using Git (Bash)\nDESCRIPTION: Clones the `cosmos-example` GitHub repository using the `git clone` command and navigates into the newly created directory using `cd`. Requires Git to be installed.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/azure-container-instance.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/astronomer/cosmos-example.git\ncd cosmos-example\n```\n\n----------------------------------------\n\nTITLE: Setting Default Cloud Run Region in gcloud CLI (Bash)\nDESCRIPTION: Configures the default Google Cloud Run region for subsequent `gcloud run` commands using the previously set REGION environment variable. Requires gcloud SDK.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/getting_started/gcp-cloud-run-job.rst#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ngcloud config set run/region $REGION\n```\n\n----------------------------------------\n\nTITLE: Importing DBTBaseOperator for Cosmos dbt Airflow\nDESCRIPTION: Imports the base operator class for dbt interactions within the Astronomer Cosmos Airflow provider. This class serves as a foundational component for creating more specific dbt operation operators.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/CHANGELOG.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom cosmos.providers.dbt.core.operators import DBTBaseOperator\n```\n\n----------------------------------------\n\nTITLE: Starting Airflow in standalone mode for local development\nDESCRIPTION: Command to run Airflow in standalone mode, which creates a new user if needed and starts all necessary Airflow components (webserver, scheduler, and triggerer).\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairflow standalone\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit hooks\nDESCRIPTION: Command to install pre-commit hooks for running code quality checks before committing code.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Running pre-commit checks manually\nDESCRIPTION: Command to manually run all pre-commit checks on all files in the repository.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Building the project with hatch\nDESCRIPTION: Command to build the astronomer-cosmos project using hatch.\nSOURCE: https://github.com/astronomer/astronomer-cosmos/blob/main/docs/contributing.rst#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nhatch build\n```"
  }
]
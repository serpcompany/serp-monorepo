[
  {
    "owner": "sentient-agi",
    "repo": "opendeepsearch",
    "content": "TITLE: Creating a Custom Reranker in OpenDeepSearch (Python)\nDESCRIPTION: Demonstrates how to create a custom reranker by inheriting from the `BaseSemanticSearcher` class and implementing the `get_embeddings()` method. It requires the `opendeepsearch` library and `torch`. The embedding logic and model initialization must be implemented within the `get_embeddings` function. The output is a PyTorch tensor containing the embeddings.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/src/opendeepsearch/ranking_models/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom opendeepsearch.ranking_models.base_reranker import BaseSemanticSearcher\nimport torch\nclass MyCustomReranker(BaseSemanticSearcher):\n    def init(self):\n        # Initialize your embedding model here\n        super().__init__()\n        self.model = YourEmbeddingModel()\n    def get_embeddings(self, texts: List[str]) -> torch.Tensor:\n        # Implement your embedding logic here\n        pass\n    embeddings = self.model.encode(texts)\n    return torch.tensor(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Using Jina AI Reranker in OpenDeepSearch (Python)\nDESCRIPTION: This code snippet shows how to use the `JinaReranker` class from the `opendeepsearch` library to rerank documents using Jina AI's API. It requires an API key from Jina AI. The code initializes the `JinaReranker` with the API key and then uses the `search` method to rerank a list of documents based on a query. The `search` function returns the top-k most relevant documents.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/src/opendeepsearch/ranking_models/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom opendeepsearch.ranking_models.jina_reranker import JinaReranker\n\n# Initialize with your API key\nreranker = JinaReranker(api_key=\"your_api_key\")  # or set JINA_API_KEY env variable\n\n# Example usage\nquery = \"What is machine learning?\"\ndocuments = [\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Deep learning is a type of machine learning\",\n    \"Natural language processing uses machine learning\"\n]\n\n# Get top 2 most relevant documents\nresults = reranker.search(query, documents, k=2)\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenDeepSearch with ReAct Agent (Python)\nDESCRIPTION: This Python code demonstrates integrating OpenDeepSearch with a ReAct (Reasoning and Acting) agent for complex queries using both a search tool and Wolfram Alpha for calculations. It initializes the required environment variables, the search and Wolfram Alpha tools, and the ReAct agent with a specified model. The ReAct agent processes the provided query and outputs the final result.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom opendeepsearch import OpenDeepSearchTool\nfrom opendeepsearch.wolfram_tool import WolframAlphaTool\nfrom opendeepsearch.prompts import REACT_PROMPT\nfrom smolagents import LiteLLMModel, ToolCallingAgent, Tool\nimport os\n\n# Set environment variables for API keys\nos.environ[\"SERPER_API_KEY\"] = \"your-serper-api-key-here\"\nos.environ[\"JINA_API_KEY\"] = \"your-jina-api-key-here\"\nos.environ[\"WOLFRAM_ALPHA_APP_ID\"] = \"your-wolfram-alpha-app-id-here\"\nos.environ[\"FIREWORKS_API_KEY\"] = \"your-fireworks-api-key-here\"\n\nmodel = LiteLLMModel(\n    \"fireworks_ai/llama-v3p1-70b-instruct\",  # Your Fireworks Deepseek model\n    temperature=0.7\n)\nsearch_agent = OpenDeepSearchTool(model_name=\"fireworks_ai/llama-v3p1-70b-instruct\", reranker=\"jina\") # Set reranker to \"jina\" or \"infinity\"\n\n# Initialize the Wolfram Alpha tool\nwolfram_tool = WolframAlphaTool(app_id=os.environ[\"WOLFRAM_ALPHA_APP_ID\"])\n\n# Initialize the React Agent with search and wolfram tools\nreact_agent = ToolCallingAgent(\n    tools=[search_agent, wolfram_tool],\n    model=model,\n    prompt_templates=REACT_PROMPT # Using REACT_PROMPT as system prompt\n)\n\n# Example query for the React Agent\nquery = \"What is the distance, in metres, between the Colosseum in Rome and the Rialto bridge in Venice\"\nresult = react_agent.run(query)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Using OpenDeepSearch Standalone (Python)\nDESCRIPTION: This Python code demonstrates how to use OpenDeepSearch as a standalone tool. It sets the required environment variables for API keys, initializes the `OpenDeepSearchTool` with a specified model and reranker (Jina), and then performs a search query.  The result of the search is printed to the console.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom opendeepsearch import OpenDeepSearchTool\nimport os\n\n# Set environment variables for API keys\nos.environ[\"SERPER_API_KEY\"] = \"your-serper-api-key-here\"  # If using Serper\n# Or for SearXNG\n# os.environ[\"SEARXNG_INSTANCE_URL\"] = \"https://your-searxng-instance.com\"\n# os.environ[\"SEARXNG_API_KEY\"] = \"your-api-key-here\"  # Optional\n\nos.environ[\"OPENROUTER_API_KEY\"] = \"your-openrouter-api-key-here\"\nos.environ[\"JINA_API_KEY\"] = \"your-jina-api-key-here\"\n\n# Using Serper (default)\nsearch_agent = OpenDeepSearchTool(\n    model_name=\"openrouter/google/gemini-2.0-flash-001\",\n    reranker=\"jina\"\n)\n\n# Or using SearXNG\n# search_agent = OpenDeepSearchTool(\n#     model_name=\"openrouter/google/gemini-2.0-flash-001\",\n#     reranker=\"jina\",\n#     search_provider=\"searxng\",\n#     searxng_instance_url=\"https://your-searxng-instance.com\",\n#     searxng_api_key=\"your-api-key-here\"  # Optional\n# )\n\nif not search_agent.is_initialized:\n    search_agent.setup()\n    \nquery = \"Fastest land animal?\"\nresult = search_agent.forward(query)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Integrating OpenDeepSearch with SmolAgents (Python)\nDESCRIPTION: This Python code demonstrates how to integrate OpenDeepSearch with SmolAgents. It initializes the `OpenDeepSearchTool`, a `LiteLLMModel`, and a `CodeAgent`. The `OpenDeepSearchTool` is passed as a tool to the `CodeAgent`, allowing the agent to use it for search queries. Then, the agent's `run` method is called to perform a search, and the result is printed.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom opendeepsearch import OpenDeepSearchTool\nfrom smolagents import CodeAgent, LiteLLMModel\nimport os\n\n# Set environment variables for API keys\nos.environ[\"SERPER_API_KEY\"] = \"your-serper-api-key-here\"  # If using Serper\n# Or for SearXNG\n# os.environ[\"SEARXNG_INSTANCE_URL\"] = \"https://your-searxng-instance.com\"\n# os.environ[\"SEARXNG_API_KEY\"] = \"your-api-key-here\"  # Optional\n\nos.environ[\"OPENROUTER_API_KEY\"] = \"your-openrouter-api-key-here\"\nos.environ[\"JINA_API_KEY\"] = \"your-jina-api-key-here\"\n\n# Using Serper (default)\nsearch_agent = OpenDeepSearchTool(\n    model_name=\"openrouter/google/gemini-2.0-flash-001\",\n    reranker=\"jina\"\n)\n\n# Or using SearXNG\n# search_agent = OpenDeepSearchTool(\n#     model_name=\"openrouter/google/gemini-2.0-flash-001\",\n#     reranker=\"jina\",\n#     search_provider=\"searxng\",\n#     searxng_instance_url=\"https://your-searxng-instance.com\",\n#     searxng_api_key=\"your-api-key-here\"  # Optional\n# )\n\nmodel = LiteLLMModel(\n    \"openrouter/google/gemini-2.0-flash-001\",\n    temperature=0.2\n)\n\ncode_agent = CodeAgent(tools=[search_agent], model=model)\nquery = \"How long would a cheetah at full speed take to run the length of Pont Alexandre III?\"\nresult = code_agent.run(query)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Running Infinity Reranker via Docker (Bash)\nDESCRIPTION: This bash command starts an Infinity reranker server using Docker. It requires Docker to be installed. The command mounts a volume for caching, specifies the GPU to use, maps port 7997, and sets various parameters for the Infinity server, including the model ID, data type, batch size, device, and engine. Requires an NVIDIA GPU with Compute Capability >= 8.0 and 16-32GB VRAM.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/src/opendeepsearch/ranking_models/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# requires ~16-32GB VRAM NVIDIA Compute Capability >= 8.0\ndocker run \\\n-v $PWD/data:/app/.cache --gpus \"0\" -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68-trt-onnx \\\nv2 --model-id Alibaba-NLP/gte-Qwen2-7B-instruct --revision \"refs/pr/38\" \\\n--dtype bfloat16 --batch-size 8 --device cuda --engine torch --port 7997 \\\n--no-bettertransformer\n```\n\n----------------------------------------\n\nTITLE: Running Gradio Demo with Arguments (Bash)\nDESCRIPTION: This bash command runs the Gradio demo application with specified command-line arguments, which allows customizing the search provider, model, and reranker. It shows examples for both Serper and SearXNG. Replace placeholder values with your actual URLs and API keys.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Using Serper (default)\npython gradio_demo.py --model-name \"openrouter/google/gemini-2.0-flash-001\" --reranker \"jina\"\n\n# Using SearXNG\npython gradio_demo.py --model-name \"openrouter/google/gemini-2.0-flash-001\" --reranker \"jina\" \\\n  --search-provider \"searxng\" --searxng-instance \"https://your-searxng-instance.com\" \\\n  --searxng-api-key \"your-api-key-here\"  # Optional\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenDeepSearch with Model Name (Python)\nDESCRIPTION: This Python code initializes the OpenDeepSearchTool with a specified model name.  The model name should be in the format \"provider/model-name\" (e.g., \"anthropic/claude-3-opus-20240229\").  This will override the environment variables for the model ID. The `search_agent` object is then used to perform search queries.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsearch_agent = OpenDeepSearchTool(model_name=\"provider/model-name\")  # e.g., \"anthropic/claude-3-opus-20240229\", 'huggingface/microsoft/codebert-base', 'openrouter/google/gemini-2.0-flash-001'\n```\n\n----------------------------------------\n\nTITLE: Setting Serper.dev API Key (Bash)\nDESCRIPTION: This bash command sets the Serper.dev API key as an environment variable.  This is necessary for OpenDeepSearch to authenticate with the Serper.dev search engine. Replace 'your-api-key-here' with your actual API key.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport SERPER_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Setting LiteLLM Provider API Key (Bash)\nDESCRIPTION: This bash command sets the API key for a chosen LiteLLM provider (e.g., OpenAI, Anthropic, Google) as an environment variable. Replace `<PROVIDER>` with the specific provider name (e.g., OPENAI, ANTHROPIC).  The API key is required to authenticate with the chosen LLM provider.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport <PROVIDER>_API_KEY='your-api-key-here'  # e.g., OPENAI_API_KEY, ANTHROPIC_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Setting SearXNG Instance URL and API Key (Bash)\nDESCRIPTION: These bash commands set the SearXNG instance URL and API key as environment variables. The URL points to the SearXNG instance. The API key is optional and only needed if the SearXNG instance requires authentication. Replace the placeholder values with your actual URL and API key.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport SEARXNG_INSTANCE_URL='https://your-searxng-instance.com'\nexport SEARXNG_API_KEY='your-api-key-here'  # Optional\n```\n\n----------------------------------------\n\nTITLE: Setting Default LiteLLM Model IDs (Bash)\nDESCRIPTION: These bash commands set default LiteLLM model IDs as environment variables. These IDs specify the models to use for different tasks such as general tasks, search tasks, agent orchestration, and evaluation tasks. If task-specific models are not set, the general default model is used as a fallback.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# General default model (fallback for all tasks)\nexport LITELLM_MODEL_ID='openrouter/google/gemini-2.0-flash-001'\n\n# Task-specific models\nexport LITELLM_SEARCH_MODEL_ID='openrouter/google/gemini-2.0-flash-001'  # For search tasks\nexport LITELLM_ORCHESTRATOR_MODEL_ID='openrouter/google/gemini-2.0-flash-001'  # For agent orchestration\nexport LITELLM_EVAL_MODEL_ID='gpt-4o-mini'  # For evaluation tasks\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI Base URL (Bash)\nDESCRIPTION: This bash command sets a custom base URL for the OpenAI API.  This is useful for using self-hosted endpoints or proxies for the OpenAI API.  If not set, the default OpenAI API endpoint will be used.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_BASE_URL='https://your-custom-openai-endpoint.com'\n```\n\n----------------------------------------\n\nTITLE: Install OpenDeepSearch with pip\nDESCRIPTION: Installs OpenDeepSearch and its dependencies using pip.  The `-e .` flag installs the package in editable mode. `requirements.txt` lists the necessary dependencies.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e . #you can also use: uv pip install -e .\npip install -r requirements.txt #you can also use: uv pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Install OpenDeepSearch with PDM\nDESCRIPTION: These commands install PDM (if not already installed), initialize a PDM project, install OpenDeepSearch and its dependencies, and activate the virtual environment. PDM offers lockfile support and fast dependency resolution.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install PDM if you haven't already\ncurl -sSL https://raw.githubusercontent.com/pdm-project/pdm/main/install-pdm.py | python3 -\n\n# Initialize a new PDM project\npdm init\n\n# Install OpenDeepSearch and its dependencies\npdm install\n\n# Activate the virtual environment\neval \"$(pdm venv activate)\"\n```\n\n----------------------------------------\n\nTITLE: Run Autograding script using Python\nDESCRIPTION: This command executes the `autograde_df.py` script to perform automated grading on DataFrame outputs from a JSONL file. It requires the path to the JSONL file as an argument.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/evals/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython evals/autograde_df.py output/fireworks_ai__accounts__fireworks__models__qwq-32b/codeact/simple_qa_test_set/fireworks_ai__accounts__fireworks__models__qwq-32b__codeact__simple_qa_test_set__trial1.jsonl\n```\n\n----------------------------------------\n\nTITLE: Autograde DataFrame CSV using Python\nDESCRIPTION: This command runs the autograding script for DataFrame outputs using a specified CSV file as input. It takes the input CSV path and the desired output CSV path as arguments.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/evals/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m evals.autograde_dataframe --csv_path <path_to_csv> --output_path <path_to_output_csv>\n```\n\n----------------------------------------\n\nTITLE: Run Task Evaluations using Python\nDESCRIPTION: This command runs evaluations on a dataset with parallel processing. It accepts parameters such as the number of parallel workers, the number of trials, and the paths to CSV files containing evaluation tasks. It stores the results as a DataFrame in the `evals` directory.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/evals/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ./evals/eval_tasks.py --parallel-workers=8 --num-trials=1 --eval-tasks=./evals/datasets/frames_test_set.csv ./evals/datasets/simple_qa_test_set.csv\n```\n\n----------------------------------------\n\nTITLE: Running Gradio Demo (Bash)\nDESCRIPTION: This bash command runs the Gradio demo application for OpenDeepSearch. The `gradio_demo.py` script is executed using Python. This will launch a local web interface for testing search queries.\nSOURCE: https://github.com/sentient-agi/opendeepsearch/blob/main/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython gradio_demo.py\n```"
  }
]
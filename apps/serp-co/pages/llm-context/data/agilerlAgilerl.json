[
  {
    "owner": "agilerl",
    "repo": "agilerl",
    "content": "TITLE: Installing Dependencies for TD3 Lunar Lander Tutorial\nDESCRIPTION: Imports necessary libraries for implementing TD3 with AgileRL, including Gymnasium for the environment, PyTorch for deep learning, and AgileRL components for reinforcement learning and hyperparameter optimization.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Author: Michael Pratt\nimport os\n\nimport imageio\nimport gymnasium as gym\nimport numpy as np\nimport torch\nfrom tqdm import trange\n\nfrom agilerl.algorithms.td3 import TD3\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.training.train_off_policy import train_off_policy\nfrom agilerl.utils.utils import (\n    create_population,\n    make_vect_envs,\n    observation_space_channels_to_first\n)\n```\n\n----------------------------------------\n\nTITLE: Training On-Policy Agents Using AgileRL's Built-in Function\nDESCRIPTION: A code example showing how to train on-policy agents using AgileRL's train_on_policy function. This function handles the training loop, evaluation, evolutionary selection, and mutation processes for a population of agents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.training.train_on_policy import train_on_policy\n\ntrained_pop, pop_fitnesses = train_on_policy(\n    env=env,                              # Gym-style environment\n    env_name=\"LunarLander-v2\",  # Environment name\n    pop=agent_pop,  # Population of agents\n    swap_channels=INIT_HP['CHANNELS_LAST'],  # Swap image channel from last to first\n    max_steps=200000,  # Max number of training steps\n    evo_steps=10000,  # Evolution frequency\n    eval_steps=None,  # Number of steps in evaluation episode\n    eval_loop=1,  # Number of evaluation episodes\n    target=200.,  # Target score for early stopping\n    tournament=tournament,  # Tournament selection object\n    mutation=mutations,  # Mutations object\n    wb=True,  # Weights and Biases tracking\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Population of TD3 Agents with Neural Network Configuration\nDESCRIPTION: Creates a population of TD3 agents for evolutionary hyperparameter optimization. Defines the neural network architecture with a simple MLP for both encoder and head, and sets up the hyperparameter configuration for mutation during evolution.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set-up the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Define the network configuration of a simple mlp with two hidden layers, each with 64 nodes\nnet_config = {\n    \"encoder_config\": {\"hidden_size\": [64, 64]},  # Encoder hidden size\n    \"head_config\": {\"hidden_size\": [64, 64]},  # Head hidden size\n}\n\n# Mutation config for RL hyperparameters\nhp_config = HyperparameterConfig(\n    lr_actor = RLParameter(min=1e-4, max=1e-2),\n    lr_critic = RLParameter(min=1e-4, max=1e-2),\n    learn_step = RLParameter(min=1, max=16, dtype=int),\n    batch_size = RLParameter(\n        min=8, max=512, dtype=int\n        )\n)\n\n# Define a population\npop = create_population(\n    algo=\"TD3\", # Algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=net_config,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    hp_config=hp_config,  # RL hyperparameter configuration\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Training Loop for AgileRL Agents in Python\nDESCRIPTION: This code snippet demonstrates a custom training loop for a population of AgileRL agents, including environment interaction, learning, and evolution steps.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntotal_steps = 0\n\n# TRAINING LOOP\nprint(\"Training...\")\npbar = trange(INIT_HP[\"MAX_STEPS\"], unit=\"step\")\nwhile np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n    pop_episode_scores = []\n    for agent in pop:  # Loop through population\n        state, info = env.reset()  # Reset environment at start of episode\n        scores = np.zeros(num_envs)\n        completed_episode_scores = []\n        steps = 0\n\n        for _ in range(-(INIT_HP[\"EVO_STEPS\"] // -agent.learn_step)):\n\n            states = []\n            actions = []\n            log_probs = []\n            rewards = []\n            dones = []\n            values = []\n\n            done = np.zeros(num_envs)\n\n            learn_steps = 0\n\n            for idx_step in range(-(agent.learn_step // -num_envs)):\n                if INIT_HP[\"CHANNELS_LAST\"]:\n                    state = obs_channels_to_first(state)\n\n                # Get next action from agent\n                action, log_prob, _, value = agent.get_action(state)\n\n                # Clip to action space\n                if isinstance(agent.action_space, spaces.Box):\n                    if agent.actor.squash_output:\n                        clipped_action = agent.actor.scale_action(action)\n                    else:\n                        clipped_action = np.clip(action, agent.action_space.low, agent.action_space.high)\n                else:\n                    clipped_action = action\n\n                # Act in environment\n                next_state, reward, terminated, truncated, info = env.step(action)\n                next_done = np.logical_or(terminated, truncated).astype(np.int8)\n\n                total_steps += num_envs\n                steps += num_envs\n                learn_steps += num_envs\n\n                states.append(state)\n                actions.append(action)\n                log_probs.append(log_prob)\n                rewards.append(reward)\n                dones.append(done)\n                values.append(value)\n\n                state = next_state\n                done = next_done\n                scores += np.array(reward)\n\n                for idx, (d, t) in enumerate(zip(terminated, truncated)):\n                    if d or t:\n                        completed_episode_scores.append(scores[idx])\n                        agent.scores.append(scores[idx])\n                        scores[idx] = 0\n\n            pbar.update(learn_steps // len(pop))\n\n            if INIT_HP[\"CHANNELS_LAST\"]:\n                next_state = obs_channels_to_first(next_state)\n\n            experiences = (\n                states,\n                actions,\n                log_probs,\n                rewards,\n                dones,\n                values,\n                next_state,\n                next_done,\n            )\n            # Learn according to agent's RL algorithm\n            agent.learn(experiences)\n\n        agent.steps[-1] += steps\n        pop_episode_scores.append(completed_episode_scores)\n\n    # Evaluate population\n    fitnesses = [\n        agent.test(\n            env,\n            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n            max_steps=INIT_HP[\"EVAL_STEPS\"],\n            loop=INIT_HP[\"EVAL_LOOP\"],\n        )\n        for agent in pop\n    ]\n    mean_scores = [\n        (\n            np.mean(episode_scores)\n            if len(episode_scores) > 0\n            else \"0 completed episodes\"\n        )\n        for episode_scores in pop_episode_scores\n    ]\n\n    print(f\"--- Global steps {total_steps} ---\")\n    print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n    print(f\"Scores: {mean_scores}\")\n    print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n    print(\n        f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n    )\n\n    # Tournament selection and population mutation\n    elite, pop = tournament.select(pop)\n    pop = mutations.mutation(pop)\n\n    # Update step counter\n    for agent in pop:\n        agent.steps.append(agent.steps[-1])\n\n# Save the trained algorithm\nelite.save_checkpoint(save_path)\n\npbar.close()\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Training NeuralTS Agent on PenDigits Dataset - Python Implementation\nDESCRIPTION: Complete implementation showing how to fetch the PenDigits dataset from UCI repository, convert it to a bandit environment, initialize and train a NeuralTS agent with evolutionary optimization. The code demonstrates setting up the environment, configuring the agent, and running the training process.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/bandits/agilerl_neural_ts_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom ucimlrepo import fetch_ucirepo\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.cross_entropy import CrossEntropyMethod\nfrom agilerl.training.train_bandit import train_bandit\nfrom agilerl.algorithms.neural_ts import NeuralTS\nfrom agilerl.wrappers.learning import BanditEnv\nimport numpy as np\n\n# Fetch data and preprocess\npendigits = fetch_ucirepo(id=81)\n\n# Creation of wrapper class to convert dataset to bandit environment\nenv = BanditEnv(dataset=pendigits.data.features,\n                labels=pendigits.data.targets)\n\n# Configure network\nINPUT_DIM = env.context_dim        # State observation dimension\nACTION_DIM = env.num_actions       # Number of actions agent can take\nCONTEXT_DIM = env.sample_dim       # Dimension of sampled context\nPOP_SIZE = 4                       # Number of agents in population\nGENERATIONS = 5                    # Number of generations for evolution\n\n# Configure agent population\nagent_configs = []\nfor i in range(POP_SIZE):\n    agent_config = {\n        'context_dim': CONTEXT_DIM,\n        'action_dim': ACTION_DIM,\n        'frequency': 1,\n        'layer_dims': [8, 8],\n        'device': 'cpu',\n    }\n    agent_configs.append(agent_config)\n\n# Population of agents\npopulation = []\nfor agent_config in agent_configs:\n    population.append(NeuralTS(**agent_config))\n\n# Configure evolution\nmutations = Mutations()\ncem = CrossEntropyMethod(num_samples=POP_SIZE)\n\n# Training hyperparameters\ntotal_steps = 50         # Number of training steps\nexp_name = 'NeuralTS'   # Name to save agent under\n\n# Train agent\ntrain_bandit(\n    env=env,\n    population=population,\n    mutations=mutations,\n    cem=cem,\n    max_steps=total_steps,\n    evo_epochs=GENERATIONS,\n    evo_loop=1,\n    optimize=True,\n    exp_name=exp_name,)\n```\n\n----------------------------------------\n\nTITLE: Custom Training Loop for Offline RL in Python\nDESCRIPTION: Provides a complete example of a custom training loop for offline reinforcement learning using AgileRL. It includes environment setup, population creation, replay buffer initialization, and the main training loop structure.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/offline_training/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport h5py\nimport numpy as np\nimport torch\nfrom tqdm import trange\n\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import (\n    create_population,\n    make_vect_envs,\n    observation_space_channels_to_first\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNET_CONFIG = {\n    \"encoder_config\": {\"hidden_size\": [32, 32]},  # Encoder hidden size\n    \"head_config\": {\"hidden_size\": [32, 32]},  # Head hidden size\n}\n\nINIT_HP = {\n    \"DOUBLE\": True,  # Use double Q-learning\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"GAMMA\": 0.99,  # Discount factor\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"TAU\": 1e-3,  # For soft update of target network parameters\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n    \"POP_SIZE\": 4,  # Population size\n}\n\n# Create vectorized environment\nnum_envs = 1\nenv = make_vect_envs(\"CartPole-v1\", num_envs=num_envs)  # Create environment\ndataset = h5py.File(\"data/cartpole/cartpole_random_v1.1.0.h5\", \"r\")  # Load dataset\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\npop = create_population(\n    algo=\"CQN\",  # Algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,  # Number of vectorized envs\n    device=device,\n)\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=device,\n)\n\nprint(\"Filling replay buffer with dataset...\")\n# Save transitions to replay buffer\ndataset_length = dataset[\"rewards\"].shape[0]\nfor i in trange(dataset_length - 1):\n    state = dataset[\"observations\"][i]\n    next_state = dataset[\"observations\"][i + 1]\n    if INIT_HP[\"CHANNELS_LAST\"]:\n        state = obs_channels_to_first(state)\n        next_state = obs_channels_to_first(next_state)\n    action = dataset[\"actions\"][i]\n    reward = dataset[\"rewards\"][i]\n    done = bool(dataset[\"terminals\"][i])\n\n    # Save experience to replay buffer\n    transition = Transition(\n        obs=state,\n        action=action,\n        reward=reward,\n        next_obs=next_state,\n        done=done,\n    )\n    transition = transition.unsqueeze(0) # Add vectorized dimension\n    transition.batch_size = [1]\n\n    memory.add(transition.to_tensordict())\n\ntournament = TournamentSelection(\n    tournament_size=2,  # Tournament selection size\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing TD3 Algorithm Components in Python\nDESCRIPTION: This code snippet initializes the components of the TD3 algorithm, including actor and critic networks, target networks, optimizers, and various hyperparameters. It handles custom network configurations and includes error checking for input types.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/base.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nself.batch_size = batch_size\nself.lr_actor = lr_actor\nself.lr_critic = lr_critic\nself.learn_step = learn_step\nself.net_config = net_config\nself.gamma = gamma\nself.tau = tau\nself.wrap = wrap\nself.mut = mut\nself.policy_freq = policy_freq\nself.O_U_noise = O_U_noise\nself.vect_noise_dim = vect_noise_dim\nself.expl_noise = (\n    expl_noise\n    if isinstance(expl_noise, np.ndarray)\n    else expl_noise * np.ones((vect_noise_dim, self.action_dim))\n)\nself.mean_noise = (\n    mean_noise\n    if isinstance(mean_noise, np.ndarray)\n    else mean_noise * np.ones((vect_noise_dim, self.action_dim))\n)\nself.current_noise = np.zeros((vect_noise_dim, self.action_dim))\nself.theta = theta\nself.dt = dt\nself.learn_counter = 0\n\nif actor_network is not None and critic_network is not None:\n    if not isinstance(actor_network, EvolvableModule):\n        raise TypeError(\n            f\"'actor_network' is of type {type(actor_network)}, but must be of type EvolvableModule.\"\n        )\n    if not isinstance(critic_network, EvolvableModule):\n        raise TypeError(\n            f\"'critic_network' is of type {type(critic_network)}, but must be of type EvolvableModule.\"\n        )\n\n    self.actor, self.critic = make_safe_deepcopies(\n        actor_network, critic_network\n    )\n    self.actor_target, self.critic_target = make_safe_deepcopies(\n        actor_network, critic_network\n    )\nelse:\n    net_config = {} if net_config is None else net_config\n    head_config = net_config.get(\"head_config\", None)\n    if head_config is not None:\n        critic_head_config = copy.deepcopy(head_config)\n        critic_head_config[\"output_activation\"] = None\n    else:\n        critic_head_config = MlpNetConfig(hidden_size=[64])\n\n    critic_net_config = copy.deepcopy(net_config)\n    critic_net_config[\"head_config\"] = critic_head_config\n\n    def create_actor():\n        return DeterministicActor(\n            observation_space=observation_space,\n            action_space=action_space,\n            device=device,\n            **net_config,\n        )\n\n    def create_critic():\n        return ContinuousQNetwork(\n            observation_space=observation_space,\n            action_space=action_space,\n            device=device,\n            **critic_net_config,\n        )\n\n    self.actor = create_actor()\n    self.actor_target = create_actor()\n    self.critic = create_critic()\n    self.critic_target = create_critic()\n\nself.actor_target.load_state_dict(self.actor.state_dict())\nself.critic_target.load_state_dict(self.critic.state_dict())\n\n# Optimizers\nself.actor_optimizer = OptimizerWrapper(\n    optim.Adam, networks=self.actor, lr=lr_actor\n)\nself.critic_optimizer = OptimizerWrapper(\n    optim.Adam, networks=self.critic, lr=lr_critic\n)\n\nif self.accelerator is not None and wrap:\n    self.wrap_models()\n\nself.criterion = nn.MSELoss()\n\n# Register network groups for actors and critics\nself.register_network_group(\n    NetworkGroup(eval=self.actor, shared=self.actor_target, policy=True)\n)\nself.register_network_group(\n    NetworkGroup(eval=self.critic, shared=self.critic_target)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Rainbow DQN with Evolution Strategy in Python\nDESCRIPTION: End-to-end implementation of Rainbow DQN with evolutionary training, including environment setup, hyperparameter configuration, network architecture definition, mutation parameters, and training loop configuration. Uses CartPole environment and supports GPU acceleration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_rainbow_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\nfrom agilerl.algorithms.dqn_rainbow import RainbowDQN\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.networks import RainbowQNetwork\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.training.train_off_policy import train_off_policy\nfrom agilerl.utils.utils import make_vect_envs\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create environment\nnum_envs = 16\nenv = make_vect_envs(\"CartPole-v1\", num_envs=num_envs)\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\n\n# Hyperparameters\nINIT_HP = {\n    \"BATCH_SIZE\": 64,  # Batch size\n    \"LR\": 0.0001,  # Learning rate\n    \"GAMMA\": 0.99,  # Discount factor\n    \"MEMORY_SIZE\": 100_000,  # Max memory buffer size\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"TAU\": 0.001,  # For soft update of target parameters\n    \"PRIOR_EPS\": 0.000001,  # Minimum priority for sampling\n    \"NUM_ATOMS\": 51,  # Unit number of support\n    \"V_MIN\": -200.0,  # Minimum value of support\n    \"V_MAX\": 200.0,  # Maximum value of support\n    \"NOISY\": True,  # Add noise directly to the weights of the network\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"LEARNING_DELAY\": 1000,  # Steps before starting learning\n    \"CHANNELS_LAST\": False,  # Use with RGB states\n    \"TARGET_SCORE\": 200.0,  # Target score that will beat the environment\n    \"MAX_STEPS\": 200000,  # Maximum number of steps an agent takes in an environment\n    \"EVO_STEPS\": 10000,  # Evolution frequency\n    \"EVAL_STEPS\": None,  # Number of evaluation steps per episode\n    \"EVAL_LOOP\": 1,  # Number of evaluation episodes\n    \"TOURN_SIZE\": 4,  # Tournament size\n    \"POP_SIZE\": 4,  # Population size\n    \"ELITISM\": True,  # Use elitism in the tournament\n}\n\nMUTATION_PARAMS = {\n    \"NO_MUTATION\": 0.4,  # Probability of no mutation\n    \"ARCHITECTURE\": 0.2,  # Probability of architecture mutation\n    \"NEW_LAYER_PROB\": 0.2,  # Probability of adding a new layer\n    \"PARAMETERS\": 0.2,  # Probability of changing parameters\n    \"ACTIVATION\": 0.2,  # Probability of changing activation function\n    \"RL_HP\": 0.2,  # Probability of changing RL hyperparameters\n    \"MUTATION_SD\": 0.1,  # Standard deviation of the mutation\n    \"RAND_SEED\": 42,  # Random seed\n}\n\n# Actor architecture configuration\nNET_CONFIG = {\n    \"latent_dim\": 32, # latent dimension for observation encodings\n    \"encoder_config\": {\n        \"hidden_size\": [64] # Encoder hidden size\n    },\n    \"head_config\": {\n        \"hidden_size\": [64] # Head hidden size\n    }\n}\n\n# Define the support for the distributional value function and the custom actor\nsupport = torch.linspace(INIT_HP['V_MIN'], INIT_HP['V_MAX'], INIT_HP['NUM_ATOMS'], device=device)\nactor = RainbowQNetwork(\n    observation_space=observation_space,\n    action_space=action_space,\n    support=support,\n    device=device,\n    **NET_CONFIG\n)\n\n# RL hyperparameters configuration for mutation during training\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=6.25e-5, max=1e-2),\n    learn_step = RLParameter(min=1, max=10, dtype=int),\n    batch_size = RLParameter(\n        min=8, max=512, dtype=int\n        )\n)\n\n# Tournament selection\ntournament = TournamentSelection(\n    tournament_size=INIT_HP[\"TOURN_SIZE\"],\n    elitism=INIT_HP[\"ELITISM\"],\n    population_size=INIT_HP[\"POP_SIZE\"],\n    eval_loop=INIT_HP[\"EVAL_LOOP\"],\n)\n\n# Define the mutation parameters\nmutations = Mutations(\n    no_mutation=MUTATION_PARAMS[\"NO_MUTATION\"],\n    architecture=MUTATION_PARAMS[\"ARCHITECTURE\"],\n    new_layer_prob=MUTATION_PARAMS[\"NEW_LAYER_PROB\"],\n    parameters=MUTATION_PARAMS[\"PARAMETERS\"],\n    activation=MUTATION_PARAMS[\"ACTIVATION\"],\n    rl_hp=MUTATION_PARAMS[\"RL_HP\"],\n    mutation_sd=MUTATION_PARAMS[\"MUTATION_SD\"],\n    rand_seed=MUTATION_PARAMS[\"RAND_SEED\"],\n    device=device,\n)\n\n# Define a population of agents\nagent_pop = RainbowDQN.population(\n    size=INIT_HP['POP_SIZE'], # Number of individuals to mutate\n    observation_space=observation_space,\n    action_space=action_space,\n    actor_network=actor,\n    hp_config=hp_config,\n    batch_size=INIT_HP[\"BATCH_SIZE\"],\n    lr=INIT_HP[\"LR\"],\n    learn_step=INIT_HP[\"LEARN_STEP\"],\n    gamma=INIT_HP[\"GAMMA\"],\n    tau=INIT_HP[\"TAU\"],\n    num_atoms=INIT_HP[\"NUM_ATOMS\"],\n    v_min=INIT_HP[\"V_MIN\"],\n    v_max=INIT_HP[\"V_MAX\"],\n    device=device\n)\n\n# Define the memory buffer\nmemory = ReplayBuffer(\n    max_size=INIT_HP['MEMORY_SIZE'],  # Max replay buffer size\n    device=device,\n)\n\n# Train the agent\ntrained_pop, pop_fitnesses = train_off_policy(\n    env,\n    \"CartPole-v1\",\n    \"Rainbow DQN\",\n    agent_pop,\n    memory=memory,\n    INIT_HP=INIT_HP,\n    MUT_P=MUTATION_PARAMS,\n    max_steps=INIT_HP[\"MAX_STEPS\"],\n    evo_steps=INIT_HP[\"EVO_STEPS\"],\n    eval_steps=INIT_HP[\"EVAL_STEPS\"],\n    eval_loop=INIT_HP[\"EVAL_LOOP\"],\n    learning_delay=INIT_HP[\"LEARNING_DELAY\"],\n    target=INIT_HP[\"TARGET_SCORE\"],\n    tournament=tournament,\n    mutation=mutations,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Population for On-Policy Training with PPO in AgileRL\nDESCRIPTION: This snippet demonstrates how to create a population of PPO agents for evolutionary hyperparameter optimization in AgileRL. It includes setting up network configuration, initializing hyperparameters, creating the environment, and constructing the agent population.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom agilerl.utils.utils import (\n    create_population,\n    make_vect_envs,\n    observation_space_channels_to_first\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNET_CONFIG = {\n    \"encoder_config\": {\"hidden_size\": [32, 32]}  # Actor head hidden size\n}\n\nINIT_HP = {\n    \"POP_SIZE\": 6,  # Population size\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"LEARN_STEP\": 128,  # Learning frequency\n    \"GAMMA\": 0.99,  # Discount factor\n    \"GAE_LAMBDA\": 0.95,  # Lambda for general advantage estimation\n    \"ACTION_STD_INIT\": 0.6,  # Initial action standard deviation\n    \"CLIP_COEF\": 0.2,  # Surrogate clipping coefficient\n    \"ENT_COEF\": 0.01,  # Entropy coefficient\n    \"VF_COEF\": 0.5,  # Value function coefficient\n    \"MAX_GRAD_NORM\": 0.5,  # Maximum norm for gradient clipping\n    \"TARGET_KL\": None,  # Target KL divergence threshold\n    \"UPDATE_EPOCHS\": 4,  # Number of policy update epochs\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n}\n\nnum_envs = 16\nenv = make_vect_envs(\"LunarLander-v2\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\npop = create_population(\n    algo=\"PPO\",  # RL algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,  # Number of vectorized envs\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing DDPG Using EvolvableAlgorithm Base Class in Python\nDESCRIPTION: Example implementation of the DDPG algorithm using the EvolvableAlgorithm base class, showing how to define network groups for actor and critic networks with their target networks, and how to wrap optimizers in OptimizerWrapper.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/base.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass DDPG(RLAlgorithm):\n    \"\"\"The DDPG algorithm class. DDPG paper: https://arxiv.org/abs/1509.02971\n\n    :param observation_space: Environment observation space\n    :type observation_space: gym.spaces.Space\n    :param action_space: Environment action space\n    :type action_space: gym.spaces.Space\n    :param O_U_noise: Use Ornstein Uhlenbeck action noise for exploration. If False, uses Gaussian noise. Defaults to True\n    :type O_U_noise: bool, optional\n    :param expl_noise: Scale for Ornstein Uhlenbeck action noise, or standard deviation for Gaussian exploration noise, defaults to 0.1\n    :type expl_noise: Union[float, ArrayLike], optional\n    :param vect_noise_dim: Vectorization dimension of environment for action noise, defaults to 1\n    :type vect_noise_dim: int, optional\n    :param mean_noise: Mean of exploration noise, defaults to 0.0\n    :type mean_noise: float, optional\n    :param theta: Rate of mean reversion in Ornstein Uhlenbeck action noise, defaults to 0.15\n    :type theta: float, optional\n    :param dt: Timestep for Ornstein Uhlenbeck action noise update, defaults to 1e-2\n    :type dt: float, optional\n    :param index: Index to keep track of object instance during tournament selection and mutation, defaults to 0\n    :type index: int, optional\n    :param hp_config: RL hyperparameter mutation configuration, defaults to None, whereby algorithm mutations are disabled.\n    :type hp_config: HyperparameterConfig, optional\n    :param net_config: Encoder configuration, defaults to None\n    :type net_config: Optional[Dict[str, Any]], optional\n    :param head_config: Head configuration, defaults to None\n    :type head_config: Optional[Dict[str, Any]], optional\n    :param batch_size: Size of batched sample from replay buffer for learning, defaults to 64\n    :type batch_size: int, optional\n    :param lr_actor: Learning rate for actor optimizer, defaults to 1e-4\n    :type lr_actor: float, optional\n    :param lr_critic: Learning rate for critic optimizer, defaults to 1e-3\n    :type lr_critic: float, optional\n    :param learn_step: Learning frequency, defaults to 5\n    :type learn_step: int, optional\n    :param gamma: Discount factor, defaults to 0.99\n    :type gamma: float, optional\n    :param tau: For soft update of target network parameters, defaults to 1e-3\n    :type tau: float, optional\n    :param normalize_images: Normalize images flag, defaults to True\n    :type normalize_images: bool, optional\n    :param mut: Most recent mutation to agent, defaults to None\n    :type mut: Optional[str], optional\n    :param policy_freq: Frequency of critic network updates compared to policy network, defaults to 2\n    :type policy_freq: int, optional\n    :param actor_network: Custom actor network, defaults to None\n    :type actor_network: Optional[nn.Module], optional\n    :param critic_network: Custom critic network, defaults to None\n    :type critic_network: Optional[nn.Module], optional\n    :param device: Device for accelerated computing, 'cpu' or 'cuda', defaults to 'cpu'\n    :type device: str, optional\n    :param accelerator: Accelerator for distributed computing, defaults to None\n    :type accelerator: accelerate.Accelerator(), optional\n    :param wrap: Wrap models for distributed training upon creation, defaults to True\n    :type wrap: bool, optional\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: spaces.Space,\n        action_space: spaces.Space,\n        O_U_noise: bool = True,\n        expl_noise: Union[float, ArrayLike] = 0.1,\n        vect_noise_dim: int = 1,\n        mean_noise: float = 0.0,\n        theta: float = 0.15,\n        dt: float = 1e-2,\n        index: int = 0,\n        hp_config: Optional[HyperparameterConfig] = None,\n        net_config: Optional[Dict[str, Any]] = None,\n        batch_size: int = 64,\n        lr_actor: float = 1e-4,\n        lr_critic: float = 1e-3,\n        learn_step: int = 5,\n        gamma: float = 0.99,\n        tau: float = 1e-3,\n        normalize_images: bool = True,\n        mut: Optional[str] = None,\n        policy_freq: int = 2,\n        actor_network: Optional[EvolvableModule] = None,\n        critic_network: Optional[EvolvableModule] = None,\n        device: str = \"cpu\",\n        accelerator: Optional[Any] = None,\n        wrap: bool = True,\n    ) -> None:\n\n        super().__init__(\n            observation_space,\n            action_space,\n            index=index,\n            hp_config=hp_config,\n            device=device,\n            accelerator=accelerator,\n            normalize_images=normalize_images,\n            name=\"DDPG\",\n        )\n\n        assert learn_step >= 1, \"Learn step must be greater than or equal to one.\"\n        assert isinstance(learn_step, int), \"Learn step rate must be an integer.\"\n        assert isinstance(\n            action_space, spaces.Box\n        ), \"DDPG only supports continuous action spaces.\"\n        assert (isinstance(expl_noise, (float, int))) or (\n            isinstance(expl_noise, np.ndarray)\n            and expl_noise.shape == (vect_noise_dim, self.action_dim)\n        ), f\"Exploration action noise rate must be a float, or an array of size {self.action_dim}\"\n        if isinstance(expl_noise, (float, int)):\n            assert (\n                expl_noise >= 0\n            ), \"Exploration noise must be greater than or equal to zero.\"\n        assert isinstance(batch_size, int), \"Batch size must be an integer.\"\n        assert batch_size >= 1, \"Batch size must be greater than or equal to one.\"\n        assert isinstance(lr_actor, float), \"Actor learning rate must be a float.\"\n        assert lr_actor > 0, \"Actor learning rate must be greater than zero.\"\n        assert isinstance(lr_critic, float), \"Critic learning rate must be a float.\"\n        assert lr_critic > 0, \"Critic learning rate must be greater than zero.\"\n        assert isinstance(learn_step, int), \"Learn step rate must be an integer.\"\n        assert learn_step >= 1, \"Learn step must be greater than or equal to one.\"\n        assert isinstance(gamma, (float, int, torch.Tensor)), \"Gamma must be a float.\"\n        assert isinstance(tau, float), \"Tau must be a float.\"\n        assert tau > 0, \"Tau must be greater than zero.\"\n        assert isinstance(policy_freq, int), \"Policy frequency must be an integer.\"\n        assert (\n            policy_freq >= 1\n        ), \"Policy frequency must be greater than or equal to one.\"\n\n        if (actor_network is not None) != (critic_network is not None):  # XOR operation\n            warnings.warn(\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Off-Policy Training Loop\nDESCRIPTION: A comprehensive example combining all the components to create a custom off-policy training loop. This example includes the full setup from environment creation to population initialization, replay buffer setup, and tournament selection and mutation configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/off_policy/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.components.data import Transition\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import create_population, make_vect_envs, observation_space_channels_to_first\nimport numpy as np\nimport torch\nfrom tqdm import trange\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNET_CONFIG = {\n    \"encoder_config\": {\n        \"hidden_size\": [32, 32] # Encoder hidden size\n        },\n    \"head_config\": {\n        \"hidden_size\": [32, 32]  # Head hidden size\n    }\n}\n\nINIT_HP = {\n    \"DOUBLE\": True,  # Use double Q-learning\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"GAMMA\": 0.99,  # Discount factor\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"TAU\": 1e-3,  # For soft update of target network parameters\n    \"CHANNELS_LAST\": False,  # Swap image channels dimension last to first [H, W, C] -> [C, H, W]\n    \"POP_SIZE\": 4,  # Population size\n}\n\n# Initialize vectorized environments\nnum_envs = 16\nenv = make_vect_envs(\"LunarLander-v2\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\npop = create_population(\n    algo=\"DQN\",  # Algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,  # Number of vectorized envs\n    device=device,\n)\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=device,\n)\n\ntournament = TournamentSelection(\n    tournament_size=2,  # Tournament selection size\n    elitism=True,  # Elitism in tournament selection\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    eval_loop=1,  # Evaluate using last N fitness scores\n)\n\nmutations = Mutations(\n    no_mutation=0.4,  # No mutation\n    architecture=0.2,  # Architecture mutation\n    new_layer_prob=0.2,  # New layer mutation\n    parameters=0.2,  # Network parameters mutation\n    activation=0,  # Activation layer mutation\n    rl_hp=0.2,  # Learning HP mutation\n    mutation_sd=0.1,  # Mutation strength  # Network architecture\n    rand_seed=1,  # Random seed\n    device=device,\n)\n\nmax_steps = 200000  # Max steps\nlearning_delay = 1000  # Steps before starting learning\n\n# Exploration params\neps_start = 1.0  # Max exploration\neps_end = 0.1  # Min exploration\n```\n\n----------------------------------------\n\nTITLE: Complete IPPO Training Loop with AsyncPettingZooVecEnv in Python\nDESCRIPTION: Full example training loop for IPPO using a vectorized PettingZoo environment. Demonstrates initialization, episode execution, data collection, and the learning process for the IPPO algorithm.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom pettingzoo.mpe import simple_speaker_listener_v4\nfrom tqdm import trange\n\nfrom agilerl.algorithms import IPPO\nfrom agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\nfrom agilerl.utils.algo_utils import obs_channels_to_first\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_envs = 8\nenv = AsyncPettingZooVecEnv(\n    [\n        lambda: simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n        for _ in range(num_envs)\n    ]\n)\nenv.reset()\n\n# Configure the multi-agent algo input arguments\nobservation_spaces = [env.single_observation_space(agent) for agent in env.agents]\naction_spaces = [env.single_action_space(agent) for agent in env.agents]\nagent_ids = [agent_id for agent_id in env.agents]\n\nchannels_last = False  # Flag to swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n\nagent = IPPO(\n    observation_spaces=observation_spaces,\n    action_spaces=action_spaces,\n    agent_ids=agent_ids,\n    device=device,\n)\n\n# Define training loop parameters\nmax_steps = 100000  # Max steps\nwhile agent.steps[-1] < max_steps:\n    state, info  = env.reset() # Reset environment at start of episode\n    scores = np.zeros((num_envs, len(agent.shared_agent_ids)))\n    completed_episode_scores = []\n    steps = 0\n\n    if channels_last:\n        state = {\n            agent_id: obs_channels_to_first(s)\n            for agent_id, s in state.items()\n        }\n\n    for _ in range(agent.learn_step):\n\n        states = {agent_id: [] for agent_id in agent.agent_ids}\n        actions = {agent_id: [] for agent_id in agent.agent_ids}\n        log_probs = {agent_id: [] for agent_id in agent.agent_ids}\n        rewards = {agent_id: [] for agent_id in agent.agent_ids}\n        dones = {agent_id: [] for agent_id in agent.agent_ids}\n        values = {agent_id: [] for agent_id in agent.agent_ids}\n\n        done = {agent_id: np.zeros(num_envs) for agent_id in agent.agent_ids}\n\n        for idx_step in range(-(agent.learn_step // -num_envs)):\n\n            # Get next action from agent\n            action, log_prob, _, value = agent.get_action(obs=state, infos=info)\n\n            # Clip to action space\n            clipped_action = {}\n            for agent_id, agent_action in action.items():\n                shared_id = agent.get_homo_id(agent_id)\n                actor_idx = agent.shared_agent_ids.index(shared_id)\n                agent_space = agent.action_space[agent_id]\n                if isinstance(agent_space, spaces.Box):\n                    if agent.actors[actor_idx].squash_output:\n                        clipped_agent_action = agent.actors[actor_idx].scale_action(agent_action)\n                    else:\n                        clipped_agent_action = np.clip(agent_action, agent_space.low, agent_space.high)\n                else:\n                    clipped_agent_action = agent_action\n\n                clipped_action[agent_id] = clipped_agent_action\n\n            # Act in environment\n            next_state, reward, termination, truncation, info = env.step(clipped_action)\n            scores += np.array(list(reward.values())).transpose()\n\n            steps += num_envs\n\n            next_done = {}\n            for agent_id in agent.agent_ids:\n                states[agent_id].append(state[agent_id])\n                actions[agent_id].append(action[agent_id])\n                log_probs[agent_id].append(log_prob[agent_id])\n                rewards[agent_id].append(reward[agent_id])\n                dones[agent_id].append(done[agent_id])\n                values[agent_id].append(value[agent_id])\n                next_done[agent_id] = np.logical_or(termination[agent_id], truncation[agent_id]).astype(np.int8)\n\n            if channels_last:\n                next_state = {\n                    agent_id: obs_channels_to_first(s)\n                    for agent_id, s in next_state.items()\n                }\n\n            # Find which agents are \"done\" - i.e. terminated or truncated\n            dones = {\n                agent_id: termination[agent_id] | truncation[agent_id]\n                for agent_id in agent.agent_ids\n            }\n\n            # Calculate scores for completed episodes\n            for idx, agent_dones in enumerate(zip(*dones.values())):\n                if all(agent_dones):\n                    completed_score = list(scores[idx])\n                    completed_episode_scores.append(completed_score)\n                    agent.scores.append(completed_score)\n                    scores[idx].fill(0)\n\n            state = next_state\n            done = next_done\n\n        experiences = (\n            states,\n            actions,\n            log_probs,\n            rewards,\n            dones,\n            values,\n            next_state,\n            next_done,\n        )\n\n        # Learn according to agent's RL algorithm\n        loss = agent.learn(experiences)\n\n    agent.steps[-1] += steps\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Multi-Agent Training Loop in Python\nDESCRIPTION: Shows how to create a custom training loop for multi-agent reinforcement learning using AgileRL components. This comprehensive example includes environment setup, agent population creation, and replay buffer configuration, providing a foundation for custom training implementations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/multi_agent_training/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom pettingzoo.mpe import simple_speaker_listener_v4\nfrom tqdm import trange\n\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import create_population\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the network configuration\nNET_CONFIG = {\n    \"head_config\": {\"hidden_size\": [32, 32]}  # Actor head hidden size\n}\n\n# Define the initial hyperparameters\nINIT_HP = {\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n    \"BATCH_SIZE\": 32,  # Batch size\n    \"O_U_NOISE\": True,  # Ornstein Uhlenbeck action noise\n    \"EXPL_NOISE\": 0.1,  # Action noise scale\n    \"MEAN_NOISE\": 0.0,  # Mean action noise\n    \"THETA\": 0.15,  # Rate of mean reversion in OU noise\n    \"DT\": 0.01,  # Timestep for OU noise\n    \"LR_ACTOR\": 0.001,  # Actor learning rate\n    \"LR_CRITIC\": 0.001,  # Critic learning rate\n    \"GAMMA\": 0.95,  # Discount factor\n    \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n    \"LEARN_STEP\": 100,  # Learning frequency\n    \"TAU\": 0.01,  # For soft update of target parameters\n    \"POLICY_FREQ\": 2,  # Policy frequnecy\n    \"POP_SIZE\": 4,  # Population size\n}\n\nnum_envs = 8\n# Define the simple speaker listener environment as a parallel environment\nenv = AsyncPettingZooVecEnv(\n    [\n        lambda: simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n        for _ in range(num_envs)\n    ]\n)\nenv.reset()\n\n# Configure the multi-agent algo input arguments\nobservation_spaces = [env.single_observation_space(agent) for agent in env.agents]\naction_spaces = [env.single_action_space(agent) for agent in env.agents]\nif INIT_HP[\"CHANNELS_LAST\"]:\n    observation_spaces = [observation_space_channels_to_first(obs) for obs in observation_spaces]\n\n# Append number of agents and agent IDs to the initial hyperparameter dictionary\nINIT_HP[\"AGENT_IDS\"] = env.agents\n\n# Create a population ready for evolutionary hyper-parameter optimisation\npop = create_population(\n    \"MADDPG\",\n    observation_spaces,\n    action_spaces,\n    NET_CONFIG,\n    INIT_HP,\n    population_size=INIT_HP[\"POP_SIZE\"],\n    num_envs=num_envs,\n    device=device,\n)\n\n# Configure the multi-agent replay buffer\nfield_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\nmemory = MultiAgentReplayBuffer(\n    INIT_HP[\"MEMORY_SIZE\"],\n    field_names=field_names,\n    agent_ids=INIT_HP[\"AGENT_IDS\"],\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Training Off-Policy Agent in Python\nDESCRIPTION: Executes the main training loop for off-policy learning using the configured components and hyperparameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.training.train_off_policy import train_off_policy\n\ntrained_pop, pop_fitnesses = train_off_policy(\n    env=env,                                   # Gym-style environment\n    env_name=INIT_HP['ENV_NAME'],              # Environment name\n    algo=INIT_HP['ALGO'],                      # Algorithm\n    pop=agent_pop,                             # Population of agents\n    memory=memory,                             # Replay buffer\n    swap_channels=INIT_HP['CHANNELS_LAST'],    # Swap image channel from last to first\n    max_steps=INIT_HP[\"MAX_STEPS\"],            # Max number of training steps\n    evo_steps=INIT_HP['EVO_STEPS'],            # Evolution frequency\n    eval_steps=INIT_HP[\"EVAL_STEPS\"],          # Number of steps in evaluation episode\n    eval_loop=INIT_HP[\"EVAL_LOOP\"],            # Number of evaluation episodes\n    learning_delay=INIT_HP['LEARNING_DELAY'],  # Steps before starting learning\n    target=INIT_HP['TARGET_SCORE'],            # Target score for early stopping\n    tournament=tournament,                     # Tournament selection object\n    mutation=mutations,                        # Mutations object\n    wb=INIT_HP['WANDB'],                       # Weights and Biases tracking\n)\n```\n\n----------------------------------------\n\nTITLE: Custom On-Policy Training Loop with Evolutionary Hyperparameter Optimization\nDESCRIPTION: This code demonstrates a comprehensive custom implementation for on-policy training with PPO in AgileRL. It includes environment setup, population creation, tournament selection, mutation configuration, and training loop initialization for evolutionary hyperparameter optimization.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom tqdm import trange\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import create_population, make_vect_envs, observation_space_channels_to_first\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNET_CONFIG = {\n    \"encoder_config\": {\n        \"hidden_size\": [32, 32] # Encoder hidden size\n        },\n    \"head_config\": {\n        \"hidden_size\": [32, 32]  # Head hidden size\n    }\n}\n\nINIT_HP = {\n    \"POP_SIZE\": 6,  # Population size\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"LEARN_STEP\": 128,  # Learning frequency\n    \"GAMMA\": 0.99,  # Discount factor\n    \"GAE_LAMBDA\": 0.95,  # Lambda for general advantage estimation\n    \"ACTION_STD_INIT\": 0.6,  # Initial action standard deviation\n    \"CLIP_COEF\": 0.2,  # Surrogate clipping coefficient\n    \"ENT_COEF\": 0.01,  # Entropy coefficient\n    \"VF_COEF\": 0.5,  # Value function coefficient\n    \"MAX_GRAD_NORM\": 0.5,  # Maximum norm for gradient clipping\n    \"TARGET_KL\": None,  # Target KL divergence threshold\n    \"UPDATE_EPOCHS\": 4,  # Number of policy update epochs\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n}\n\nnum_envs = 16\nenv = make_vect_envs(\"LunarLander-v2\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\n# RL hyperparameters configuration for mutation during training\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=1e-4, max=1e-2),\n    batch_size = RLParameter(\n        min=8, max=1024, dtype=int\n        )\n)\n\npop = create_population(\n    algo=\"PPO\",  # RL algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    hp_config=hp_config,  # Hyperparameters configuration\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,  # Number of vectorized envs\n    device=device,\n)\n\ntournament = TournamentSelection(\n    tournament_size=2,  # Tournament selection size\n    elitism=True,  # Elitism in tournament selection\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    eval_loop=1,  # Evaluate using last N fitness scores\n)\n\nmutations = Mutations(\n    no_mutation=0.4,  # No mutation\n    architecture=0.2,  # Architecture mutation\n    new_layer_prob=0.2,  # New layer mutation\n    parameters=0.2,  # Network parameters mutation\n    activation=0,  # Activation layer mutation\n    rl_hp=0.2,  # Learning HP mutation\n    mutation_sd=0.1,  # Mutation strength  # Network architecture\n    rand_seed=1,  # Random seed\n    device=device,\n)\n\nmax_steps = 200000  # Max steps\nevo_steps = 10000  # Evolution frequency\neval_steps = None  # Evaluation steps per episode - go until done\neval_loop = 1  # Number of evaluation episodes\n\ntotal_steps = 0\n\n# TRAINING LOOP\nprint(\"Training...\")\npbar = trange(max_steps, unit=\"step\")\nwhile np.less([agent.steps[-1] for agent in pop], max_steps).all():\n    pop_episode_scores = []\n    for agent in pop:  # Loop through population\n        state, info = env.reset()  # Reset environment at start of episode\n        scores = np.zeros(num_envs)\n        completed_episode_scores = []\n        steps = 0\n\n        for _ in range(-(evo_steps // -agent.learn_step)):\n```\n\n----------------------------------------\n\nTITLE: Training Rainbow DQN Agent in Python using AgileRL\nDESCRIPTION: This snippet shows a comprehensive training loop for a Rainbow DQN agent. It includes environment interaction, experience collection, agent learning, and periodic evaluation. The loop continues until a maximum number of steps is reached, after which the agent is saved.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntotal_steps = 0\nsave_path = \"RainbowDQN.pt\"\n\n# TRAINING LOOP\nprint(\"Training...\")\npbar = trange(INIT_HP[\"MAX_STEPS\"], unit=\"step\")\nwhile rainbow_dqn.steps[-1] < INIT_HP[\"MAX_STEPS\"]:\n    state = env.reset()[0]  # Reset environment at start of episode\n    scores = np.zeros(num_envs)\n    completed_episode_scores = []\n    steps = 0\n    for idx_step in range(INIT_HP[\"EVO_STEPS\"] // num_envs):\n        # Swap channels if channels last is True\n        state = obs_channels_to_first(state) if INIT_HP[\"CHANNELS_LAST\"] else state\n\n        # Get next action from agent\n        action = rainbow_dqn.get_action(state)\n        next_state, reward, terminated, truncated, info = env.step(action)  # Act in environment\n        scores += np.array(reward)\n        steps += num_envs\n        total_steps += num_envs\n\n        # Collect scores for completed episodes\n        for idx, (d, t) in enumerate(zip(terminated, truncated)):\n            if d or t:\n                completed_episode_scores.append(scores[idx])\n                rainbow_dqn.scores.append(scores[idx])\n                scores[idx] = 0\n\n        next_state = obs_channels_to_first(next_state) if INIT_HP[\"CHANNELS_LAST\"] else next_state\n        done = terminated or truncated\n\n        transition = Transition(\n            obs=state,\n            action=action,\n            reward=reward,\n            next_obs=next_state,\n            done=done,\n            batch_size=[num_envs]\n        )\n\n        transition = transition.to_tensordict()\n\n        one_step_transition = n_step_memory.add(transition)\n        if one_step_transition:\n            memory.add(one_step_transition)\n\n        # Update agent beta\n        fraction = min(\n            ((rainbow_dqn.steps[-1] + idx_step + 1) * num_envs / INIT_HP[\"MAX_STEPS\"]), 1.0\n        )\n        rainbow_dqn.beta += fraction * (1.0 - rainbow_dqn.beta)\n\n        # Learn according to learning frequency\n        if len(memory) >= rainbow_dqn.batch_size and memory.counter > INIT_HP[\"LEARNING_DELAY\"]:\n            for _ in range(num_envs // rainbow_dqn.learn_step):\n                # Sample replay buffer\n                # Learn according to agent's RL algorithm\n                experiences = memory.sample(rainbow_dqn.batch_size, rainbow_dqn.beta)\n                n_step_experiences = n_step_memory.sample_from_indices(experiences[6])\n                experiences += n_step_experiences\n                loss, idxs, priorities = rainbow_dqn.learn(experiences, n_step=n_step, per=per)\n                memory.update_priorities(idxs, priorities)\n\n        state = next_state\n        total_steps += num_envs\n        steps += num_envs\n\n    # Evaluate population\n    fitness = rainbow_dqn.test(\n        env,\n        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n        max_steps=INIT_HP[\"EVAL_STEPS\"],\n        loop=INIT_HP[\"EVO_LOOP\"],\n    )\n    mean_score = (\n       np.mean(completed_episode_scores)\n       if len(completed_episode_scores) > 0\n       else \"0 completed episodes\"\n    )\n\n    print(f\"--- Global steps {total_steps} ---\")\n    print(f\"Steps {rainbow_dqn.steps[-1]}\")\n    print(f\"Scores: {\\\"%.2f\\\"%mean_score}\")\n    print(f'Fitness: {\\\"%.2f\\\"%fitness}')\n    print(f'5 fitness avg: {\\\"%.2f\\\"%np.mean(rainbow_dqn.fitness[-5:])}'\n\n    fitness = \"%.2f\" % fitness\n    avg_fitness = \"%.2f\" % np.mean(rainbow_dqn.fitness[-100:])\n    avg_score = \"%.2f\" % np.mean(rainbow_dqn.scores[-100:])\n    num_steps = rainbow_dqn.steps[-1]\n\n    print(\n        f\"\"\"\n        --- Epoch {episode + 1} ---\n        Fitness:\\t\\t{fitness}\n        100 fitness avgs:\\t{avg_fitness}\n        100 score avgs:\\t{avg_score}\n        Steps:\\t\\t{num_steps}\n        \"\"\",\n        end=\"\\r\",\n    )\n\n    rainbow_dqn.steps.append(rainbow_dqn.steps[-1])\n\n# Save the trained algorithm at the end of the training loop\nrainbow_dqn.save_checkpoint(save_path)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using DQN Agent in AgileRL\nDESCRIPTION: Example demonstrating how to create a DQN agent, set up an environment, and implement the basic reinforcement learning loop including action selection, environment interaction, and learning from experiences.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.utils.utils import make_vect_envs, observation_space_channels_to_first\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.algorithms.dqn import DQN\n\n# Create environment and Experience Replay Buffer\nnum_envs = 8\nenv = make_vect_envs('LunarLander-v2', num_envs=num_envs)\nobservation_space = env.observation_space\naction_space = env.action_space\n\nchannels_last = False # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n\nif channels_last:\n    observation_space = observation_space_channels_to_first(observation_space)\n\nmemory = ReplayBuffer(max_size=10000)\nagent = DQN(observation_space, action_space)   # Create DQN agent\n\nstate = env.reset()[0]  # Reset environment at start of episode\nwhile True:\n    if channels_last:\n        state = obs_channels_to_first(state)\n    action = agent.get_action(state, epsilon)    # Get next action from agent\n    next_state, reward, done, _, _ = env.step(action)   # Act in environment\n\n    # Save experience to replay buffer\n    transition = Transition(\n        obs=state,\n        action=action,\n        reward=reward,\n        next_obs=next_state,\n        done=done,\n        batch_size=[num_envs]\n    )\n    memory.add(transition)\n\n    # Learn according to learning frequency\n    if len(memory) >= agent.batch_size:\n        for _ in range(num_envs // agent.learn_step):\n            experiences = memory.sample(agent.batch_size) # Sample replay buffer\n            agent.learn(experiences)    # Learn according to agent's RL algorithm\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Evolving AgileRL Population\nDESCRIPTION: Evaluates the performance of each agent in the population, calculates fitnesses, and performs tournament selection and mutation to evolve the population for the next training iteration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/off_policy/index.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# Evaluate population\nfitnesses = [\n    agent.test(\n        env,\n        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n        max_steps=eval_steps,\n        loop=eval_loop,\n    )\n    for agent in pop\n]\nmean_scores = [\n    (\n        np.mean(episode_scores)\n        if len(episode_scores) > 0\n        else \"0 completed episodes\"\n    )\n    for episode_scores in pop_episode_scores\n]\n\nprint(f\"--- Global steps {total_steps} ---\")\nprint(f\"Steps {[agent.steps[-1] for agent in pop]}\")\nprint(f\"Scores: {mean_scores}\")\nprint(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\nprint(\n    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n)\n\n# Tournament selection and population mutation\nelite, pop = tournament.select(pop)\npop = mutations.mutation(pop)\n\n# Update step counter\nfor agent in pop:\n    agent.steps.append(agent.steps[-1])\n\npbar.close()\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Training Loop with Reinforcement Learning in Python\nDESCRIPTION: A training loop for fine-tuning language models using reinforcement learning with tournament selection and mutation. The code handles agent actions, reward calculation, metrics tracking, and evaluation across multiple GPUs with accelerator support.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# calling env.reset() supplies the first batch of training data\nprompts = env.reset(reset_dataloaders=True)\nfor i in range(max_steps):\n    agent_metrics_dict = {}\n    for agent_idx, agent in enumerate(pop):\n        completion_ids, action_masks = agent.get_action(prompts)\n        completion_lengths = np.mean([x.shape[1] for x in completion_ids])\n\n        # Use the reward function stored in env.step to calculate reward of the each answer from the group\n        next_prompts, rewards = env.step(completion_ids)\n        experiences = (\n            completion_ids,\n            action_masks,\n            rewards,\n        )\n        loss, kl = agent.learn(experiences)\n        metrics = [loss, kl, rewards, completion_lengths]\n        if max_reward is not None:\n            accuracy = (rewards == max_reward).sum() / len(rewards.flatten())\n            metrics.append(accuracy)\n        agg_metrics = [\n            aggregate_metrics_across_gpus(agent, metric) for metric in metrics\n        ]\n        prompts = next_prompts\n        agg_test_metrics = None\n        if (i + 1) % evaluation_interval == 0:\n            test_reward = agent.test(env)\n            test_metrics = [test_reward]\n            if max_reward is not None:\n                test_accuracy = (test_reward == max_reward).sum() / len(\n                    rewards.flatten()\n                )\n                test_metrics.append(test_accuracy)\n            agg_test_metrics = [\n                aggregate_metrics_across_gpus(agent, metric)\n                for metric in test_metrics\n            ]\n            if verbose and (accelerator is None or accelerator.is_main_process):\n                fitness = [str(round(agent.fitness[-1], 2)) for agent in pop]\n                avg_fitness = [\n                    \"%.2f\" % np.mean(agent.fitness[-5:]) for agent in pop\n                ]\n                avg_score = [\"%.2f\" % np.mean(agent.scores[-10:]) for agent in pop]\n                agents = [agent.index for agent in pop]\n                num_steps = [agent.steps[-1] for agent in pop]\n                muts = [agent.mut for agent in pop]\n                print(\n                    f\"\"\"\n                    --- Global Steps {total_steps} ---\n                    Fitness:\\t\\t{fitness}\n                    Score:\\t\\t{mean_scores}\n                    5 fitness avgs:\\t{avg_fitness}\n                    10 score avgs:\\t{avg_score}\n                    Agents:\\t\\t{agents}\n                    Steps:\\t\\t{num_steps}\n                    Mutations:\\t\\t{muts}\n                    \"\"\",\n                    end=\"\\r\",\n                )\n        if accelerator is None or accelerator.is_main_process:\n            metrics_dict = {\n                \"Train/Loss\": agg_metrics[0],\n                \"Train/KL-divergence\": agg_metrics[1],\n                \"Train/Mean reward\": (mean_scores := agg_metrics[2]),\n                \"Train/Average completion length\": int(agg_metrics[3]),\n            }\n            if max_reward is not None:\n                metrics_dict |= {\"Train/Accuracy\": agg_metrics[4]}\n            agent_metrics_dict[f\"agent_{agent_idx}/train_metrics\"] = metrics_dict\n            if agg_test_metrics is not None:\n                test_metrics_dict = {\"Eval/Mean reward\": agg_test_metrics[0]}\n                if max_reward is not None:\n                    test_metrics_dict |= {\"Eval/Accuracy\": agg_test_metrics[1]}\n                agent_metrics_dict[f\"agent_{agent_idx}/test_metrics\"] = (\n                    test_metrics_dict\n                )\n            pbar.update(effective_data_batch_size)\n            agent.steps.append(effective_data_batch_size)\n            agent.scores.append(mean_scores)\n            total_steps += effective_data_batch_size\n\n    if accelerator is not None:\n        accelerator.wait_for_everyone()\n    if tournament and mutation is not None:\n        if (i + 1) % evo_steps == 0:\n            pop = tournament_selection_and_mutation(\n                population=pop,\n                tournament=tournament,\n                mutation=mutations,\n                env_name=env.name,\n                accelerator=None,  # Set as None for LLM finetuning as it does not require the same accelerator handling as standard RL models\n                language_model=True,\n                elite_path=elite_path,\n                save_elite=save_elite\n            )\npbar.close()\n```\n\n----------------------------------------\n\nTITLE: Testing Q-Learning with Single-Agent Probe Environments\nDESCRIPTION: Demonstrates how to test DQN implementations using vector-based probe environments. The code initializes different probe environments, sets up a DQN agent with replay buffer, and validates Q-learning functionality.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/debugging_rl/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom agilerl.algorithms.dqn import DQN\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.utils.probe_envs import check_q_learning_with_probe_env\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nvector_envs = [\n    (ConstantRewardEnv(), 1000),\n    (ObsDependentRewardEnv(), 1000),\n    (DiscountedRewardEnv(), 3000),\n    (FixedObsPolicyEnv(), 1000),\n    (PolicyEnv(), 1000),\n]\n\nfor env, learn_steps in vector_envs:\n    algo_args = {\n        \"observation_space\": env.observation_space,\n        \"action_space\": env.action_space,\n        \"lr\": 1e-2,\n    }\n\n    memory = ReplayBuffer(\n        max_size=1000,  # Max replay buffer size\n        device=device,\n    )\n\n    check_q_learning_with_probe_env(env, DQN, algo_args, memory, learn_steps, device)\n```\n\n----------------------------------------\n\nTITLE: Setting up Mutations for Evolutionary HPO in Python\nDESCRIPTION: This code initializes the Mutations class from AgileRL to enable various mutation strategies for evolutionary hyperparameter optimization. It configures probabilities for different types of mutations, including architecture changes, parameter adjustments, and learning hyperparameter modifications.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/evo_hyperparam_opt/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.hpo.mutation import Mutations\n\nmutations = Mutations(\n    no_mutation=0.4,  # No mutation\n    architecture=0.2,  # Architecture mutation\n    new_layer_prob=0.2,  # New layer mutation\n    parameters=0.2,  # Network parameters mutation\n    activation=0,  # Activation layer mutation\n    rl_hp=0.2,  # Learning HP mutation\n    mutation_sd=0.1,  # Mutation strength  # Network architecture\n    rand_seed=1,  # Random seed\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Training MATD3 Agents in PettingZoo Environment\nDESCRIPTION: This code snippet demonstrates how to train multiple agents using MATD3 in a PettingZoo environment. It includes environment setup, agent initialization, and the training loop.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/matd3.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/PettingZoo/agilerl_matd3.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Training Multi-Agent Systems with Built-in Training Function in Python\nDESCRIPTION: Uses the pre-built train_multi_agent_off_policy function to train multiple agents in a PettingZoo environment. This function handles the evolutionary hyperparameter optimization process, including tournament selection and mutation, while training the population of agents using the specified algorithm.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/multi_agent_training/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.training.train_multi_agent_off_policy import train_multi_agent_off_policy\nimport gymnasium as gym\nimport torch\n\ntrained_pop, pop_fitnesses = train_multi_agent_off_policy(\n    env=env,  # Pettingzoo-style environment\n    env_name='simple_speaker_listener_v4',  # Environment name\n    algo=\"MADDPG\",  # Algorithm\n    pop=pop,  # Population of agents\n    memory=memory,  # Replay buffer\n    INIT_HP=INIT_HP,  # IINIT_HP dictionary\n    net_config=NET_CONFIG,  # Network configuration\n    swap_channels=INIT_HP['CHANNELS_LAST'],  # Swap image channel from last to first\n    max_steps=2000000,  # Max number of training steps\n    evo_steps=10000,  # Evolution frequency\n    eval_steps=None,  # Number of steps in evaluation episode\n    eval_loop=1,  # Number of evaluation episodes\n    learning_delay=1000,  # Steps before starting learning\n    target=200.,  # Target score for early stopping\n    tournament=tournament,  # Tournament selection object\n    mutation=mutations,  # Mutations object\n    wb=False,  # Weights and Biases tracking\n)\n```\n\n----------------------------------------\n\nTITLE: Training PPO Agents with On-Policy Training Function\nDESCRIPTION: Uses the AgileRL train_on_policy function to train the population of PPO agents. This function handles the training loop, evaluation, and evolutionary hyperparameter optimization. It returns the trained population and their fitness scores upon completion.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define a save path for our trained agent\nsave_path = \"PPO_trained_agent.pt\"\n\ntrained_pop, pop_fitnesses = train_on_policy(\n    env=env,\n    env_name=\"Acrobot-v1\",\n    algo=\"PPO\",\n    pop=pop,\n    INIT_HP=INIT_HP,\n    MUT_P=MUT_P,\n    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Training Multiple Agents Using MADDPG in PettingZoo's Space Invaders Environment\nDESCRIPTION: This code demonstrates how to set up and train multiple agents using the MADDPG algorithm on the Space Invaders environment from PettingZoo. It includes environment setup, agent initialization, training process, and model saving.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/maddpg.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/PettingZoo/agilerl_maddpg.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for TD3 with Discrete/Vector Observations in Python\nDESCRIPTION: This snippet shows how to configure the neural network architecture for TD3 with discrete or vector observations, specifying the encoder and head configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/td3.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Training TD3 Agent with AgileRL's train_off_policy Function in Python\nDESCRIPTION: Uses AgileRL's train_off_policy function to train a TD3 agent. This function handles the training and hyperparameter optimization process, including population evolution and evaluation. It returns the trained population and their fitnesses.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrained_pop, pop_fitnesses = train_off_policy(\n    env=env,\n    env_name=\"LunarLanderContinuous-v3\",\n    algo=\"TD3\",\n    pop=pop,\n    memory=memory,\n    INIT_HP=INIT_HP,\n    MUT_P=MUT_P,\n    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n    max_steps=INIT_HP[\"MAX_STEPS\"],\n    evo_steps=INIT_HP[\"EVO_STEPS\"],\n    eval_steps=INIT_HP[\"EVAL_STEPS\"],\n    eval_loop=INIT_HP[\"EVAL_LOOP\"],\n    learning_delay=INIT_HP[\"LEARNING_DELAY\"],\n    target=INIT_HP[\"TARGET_SCORE\"],\n    tournament=tournament,\n    mutation=mutations,\n    wb=False,  # Boolean flag to record run with Weights & Biases\n    save_elite=True,  # Boolean flag to save the elite agent in the population\n    elite_path=\"TD3_trained_agent.pt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up a Rainbow DQN Agent in AgileRL\nDESCRIPTION: A complete example of creating and training a Rainbow DQN agent with AgileRL. The code demonstrates environment setup, replay buffer initialization, agent creation, and the training loop using a vectorized environment.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn_rainbow.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.utils.utils import make_vect_envs, observation_space_channels_to_first\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.algorithms.dqn_rainbow import RainbowDQN\n\n# Create environment and Experience Replay Buffer\nnum_envs = 8\nenv = make_vect_envs('LunarLander-v2', num_envs=num_envs)\nobservation_space = env.observation_space\naction_space = env.action_space\n\nchannels_last = False # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n\nif channels_last:\n    observation_space = observation_space_channels_to_first(observation_space)\n\nmemory = ReplayBuffer(max_size=10000)\n\nagent = RainbowDQN(observation_space, action_space)   # Create agent\n\nstate = env.reset()[0]  # Reset environment at start of episode\nwhile True:\n    if channels_last:\n        state = obs_channels_to_first(state)\n    action = agent.get_action(state, epsilon)    # Get next action from agent\n    next_state, reward, done, _, _ = env.step(action)   # Act in environment\n\n    # Save experience to replay buffer\n    transition = Transition(\n        obs=state,\n        action=action,\n        reward=reward,\n        next_obs=next_state,\n        done=done,\n        batch_size=[num_envs]\n    )\n    transition = transition.to_tensordict()\n    memory.add(transition)\n\n    # Learn according to learning frequency\n    if len(memory) >= agent.batch_size:\n        for _ in range(num_envs // agent.learn_step):\n            experiences = memory.sample(agent.batch_size) # Sample replay buffer\n            agent.learn(experiences)    # Learn according to agent's RL algorithm\n```\n\n----------------------------------------\n\nTITLE: Training Loop for Offline RL with AgileRL in Python\nDESCRIPTION: Demonstrates how to set up and run the training loop for offline reinforcement learning using AgileRL. It includes options for using a pre-defined training function or creating a custom training loop.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/offline_training/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.training.train_offline import train_offline\n\ntrained_pop, pop_fitnesses = train_offline(\n    env=env,  # Gym-style environment\n    env_name=\"CartPole-v1\",  # Environment name\n    dataset=dataset,  # Offline dataset\n    pop=pop,  # Population of agents\n    memory=memory,  # Replay buffer\n    swap_channels=INIT_HP['CHANNELS_LAST'],  # Swap image channel from last to first\n    max_steps=500000,  # Max number of training steps\n    evo_steps=10000,  # Evolution frequency\n    eval_steps=None,  # Evaluation steps\n    eval_loop=1,  # Number of evaluation episodes per agent\n    target=200.,  # Target score for early stopping\n    tournament=tournament,  # Tournament selection object\n    mutation=mutations,  # Mutations object\n    wb=True,  # Weights and Biases tracking\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Fine-tuned Language Model with Transformers and PEFT in Python\nDESCRIPTION: This snippet demonstrates how to load a fine-tuned language model using the Transformers library and PEFT (Parameter-Efficient Fine-Tuning). It loads the base model, tokenizer, and applies the fine-tuned parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-3B\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\nmodel = PeftModel.from_pretrained(base_model, \"path/to/model/directory\")\n```\n\n----------------------------------------\n\nTITLE: Implementing NeuralTS Agent for Contextual Bandits\nDESCRIPTION: Example of creating and training a NeuralTS agent on the Iris dataset by processing contexts, selecting actions, and learning from experiences stored in a replay buffer.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ts.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensordict import TensorDict\n\nfrom agilerl.algorithms.neural_ts import NeuralTS\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.wrappers.learning import BanditEnv\n\n# Fetch data  https://archive.ics.uci.edu/\niris = fetch_ucirepo(id=53)\nfeatures = iris.data.features\ntargets = iris.data.targets\n\n# Create environment\nenv = BanditEnv(features, targets)\ncontext_dim = env.context_dim\naction_dim = env.arms\n\nmemory = ReplayBuffer(max_size=10000)\n\nobservation_space = spaces.Box(low=features.values.min(), high=features.values.max())\naction_space = spaces.Discrete(action_dim)\nbandit = NeuralTS(observation_space, action_space)   # Create NeuralTS agent\n\ncontext = env.reset()  # Reset environment at start of episode\nfor _ in range(500):\n    # Get next action from agent\n    action = agent.get_action(context)\n    next_context, reward = env.step(action)  # Act in environment\n\n    # Save experience to replay buffer\n    transition = TensorDict({\n      \"obs\": context[action],\n      \"reward\": reward,\n      },\n      batch_size=[1]\n    )\n    memory.add(transition)\n\n    # Learn according to learning frequency\n    if len(memory) >= agent.batch_size:\n        for _ in range(agent.learn_step):\n            experiences = memory.sample(agent.batch_size) # Sample replay buffer\n            agent.learn(experiences)    # Learn according to agent's RL algorithm\n\n    context = next_context\n```\n\n----------------------------------------\n\nTITLE: Using the Pre-Built Off-Policy Training Function\nDESCRIPTION: Code demonstrating how to use AgileRL's built-in training function for off-policy learning. This function handles the evolutionary process, including tournament selection and mutation of hyperparameters during training.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/off_policy/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.training.train_off_policy import train_off_policy\n\ntrained_pop, pop_fitnesses = train_off_policy(\n    env=env,  # Gym-style environment\n    env_name=\"LunarLander-v2\",  # Environment name\n    algo=\"DQN\",  # Algorithm\n    pop=pop,  # Population of agents\n    memory=memory,  # Replay buffer\n    swap_channels=INIT_HP[\"CHANNELS_LAST\"],  # Swap image channel from last to first\n    max_steps=200000,  # Max number of training steps\n    evo_steps=10000,  # Evolution frequency\n    eval_steps=None,  # Number of steps in evaluation episode\n    eval_loop=1,  # Number of evaluation episodes\n    learning_delay=1000,  # Steps before starting learning\n    target=200.,  # Target score for early stopping\n    tournament=tournament,  # Tournament selection object\n    mutation=mutations,  # Mutations object\n    wb=False,  # Weights and Biases tracking\n)\n```\n\n----------------------------------------\n\nTITLE: Training Individual Skills with PPO in AgileRL\nDESCRIPTION: Implements a training loop that creates and trains individual skill agents using PPO algorithm. Each skill is trained in a vectorized environment with specific configurations and hyperparameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor skill in skills.keys():\n   env = make_skill_vect_envs(\n         INIT_HP[\"ENV_NAME\"], skills[skill], num_envs=1\n   )\n   observation_space = env.single_observation_space\n   action_space = env.single_action_space\n   if INIT_HP[\"CHANNELS_LAST\"]:\n         observation_space = observation_space_channels_to_first(observation_space)\n\n   pop = create_population(\n         algo=\"PPO\",\n         observation_space=observation_space,\n         action_space=action_space,\n         net_config=NET_CONFIG,\n         INIT_HP=INIT_HP,\n         population_size=INIT_HP[\"POPULATION_SIZE\"],\n         device=device,\n   )\n\n   trained_pop, pop_fitnesses = train_on_policy(\n         env=env,\n         env_name=f\"{INIT_HP['ENV_NAME']}-{skill}\",\n         algo=INIT_HP[\"ALGO\"],\n         pop=pop,\n         swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n         max_steps=INIT_HP[\"MAX_STEPS\"],\n         evo_steps=INIT_HP[\"EVO_STEPS\"],\n         evo_loop=3,\n         target=INIT_HP[\"TARGET_SCORE\"],\n         tournament=None,\n         mutation=None,\n         wb=INIT_HP[\"WANDB\"],\n   )\n\n   filename = f\"PPO_trained_agent_{skill}.pt\"\n   save_path = os.path.join(save_dir, filename)\n   trained_pop[0].save_checkpoint(save_path)\n\n   env.close()\n```\n\n----------------------------------------\n\nTITLE: Using EvolvableSimBa in DDPG for HalfCheetah-v4 Environment\nDESCRIPTION: This code snippet demonstrates how to use EvolvableSimBa architecture in the DDPG algorithm for the HalfCheetah-v4 environment, including environment setup, hyperparameter configuration, and training.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\n\nfrom agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\nfrom agilerl.algorithms import DDPG\nfrom agilerl.wrappers.agent import RSNorm\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.training.train_off_policy import train_off_policy\nfrom agilerl.utils.utils import make_vect_envs, print_hyperparams\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create vectorized environment for HalfCheetah-v4\nenv_name = \"HalfCheetah-v4\"\nnum_envs = 6\nenv = make_vect_envs(env_name, num_envs=num_envs)\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\n\n# Replay buffer for off-policy learning\nmemory_size = 100_000\nmemory = ReplayBuffer(\n    max_size=memory_size,\n    device=device\n)\n\n# Tournament selection and mutations\npopulation_size = 4\ntournament = TournamentSelection(\n    tournament_size=2,\n    elitism=True,\n    population_size=population_size,\n    eval_loop=1\n)\n\n# Hyperarameter mutation probabilities\nmutations = Mutations(\n    no_mutation=0.4, # No mutation\n    architecture=0.2, # Architecture mutation\n    new_layer_prob=0.2, # Mutate layer number (0.2) vs mutate node number (0.8)\n    parameters=0.2, # Mutate parameters with Gaussian noise\n    activation=0.2, # Mutate activation function\n    rl_hp=0.2, # Mutate RL hyperparameters\n    mutation_sd=0.1, # Mutation strength\n    rand_seed=42, # Random seed\n    device=device,\n)\n\n# RL hyperparameters mutation configuration\nhp_config = HyperparameterConfig(\n    lr_actor=RLParameter(min=1e-4, max=1e-2),\n    lr_critic=RLParameter(min=1e-4, max=1e-2),\n    batch_size=RLParameter(min=8, max=512, dtype=int),\n    learn_step=RLParameter(min=1, max=16, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n)\n\n# Architecture of networks in algorithm\nnet_config = {\n    \"latent_dim\": 64, # Latent dimension of evolvable networks\n    \"simba\": True, # Use EvolvableSimBa as encoder for vector space\n\n    # Configuration of EvolvableSimBa encoder\n    \"encoder_config\": {\n        \"hidden_size\": 128, # Hidden size of residual blocks\n        \"num_blocks\": 2, # Number of residual blocks\n        \"min_mlp_nodes\": 64, # Minimum number of nodes for architecture mutations\n        \"max_mlp_nodes\": 500 # Maximum number of nodes for architecture mutations\n    },\n\n    # Configuration of EvolvableMLP head\n    \"head_config\": {\n        \"hidden_size\": [64],\n        \"activation\": \"ReLU\",\n        \"output_activation\": \"Tanh\",\n        \"min_hidden_layers\": 1,\n        \"max_hidden_layers\": 2,\n        \"min_mlp_nodes\": 64,\n        \"max_mlp_nodes\": 500\n    }\n}\n\n# Create population of DDPG agents\nagent_pop = DDPG.population(\n    size=population_size,\n    observation_space=observation_space,\n    action_space=action_space,\n    wrapper_cls=RSNorm, # IMPORTANT: Use RSNorm agent wrapper for input normalization like in paper\n    O_U_noise=True,\n    expl_noise=0.1,\n    vect_noise_dim=num_envs,\n    mean_noise=0.0,\n    theta=0.15,\n    dt=1e-2,\n    hp_config=hp_config,\n    net_config=net_config,\n    batch_size=128,\n    lr_actor=3e-4,\n    lr_critic=3e-4,\n    learn_step=1,\n    gamma=0.99,\n    tau=5e-3,\n    policy_freq=2,\n    device=device\n)\n\ntrained_pop, pop_fitnesses = train_off_policy(\n    env,\n    env_name,\n    \"DDPG\",\n    agent_pop,\n    memory=memory,\n    max_steps=1_000_000,\n    evo_steps=15_000,\n    eval_loop=1,\n    learning_delay=10_000,\n    tournament=tournament,\n    mutation=mutations\n)\n\nprint_hyperparams(trained_pop)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Initializing ReplayBuffer in AgileRL with Python\nDESCRIPTION: This snippet demonstrates how to import and initialize a ReplayBuffer object for storing agent experiences. The buffer is configured with a maximum size of 10000 and assigned to a specific device.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/components/replay_buffer.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Training Bandits Using AgileRL's Built-in Function\nDESCRIPTION: This snippet demonstrates how to use AgileRL's built-in function for training a population of bandit agents. It sets up the training parameters, including the environment, algorithm, population, and evolutionary settings.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/bandits/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.training.train_bandits import train_bandits\n\ntrained_pop, pop_fitnesses = train_bandits(\n    env,  # Bandit environment\n    INIT_HP[\"ENV_NAME\"],  # Environment name\n    \"NeuralUCB\",  # Algorithm\n    agent_pop,  # Population of agents\n    memory=memory,  # Experience replay buffer\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    MUT_P=MUTATION_PARAMS,  # Mutation parameters\n    swap_channels=INIT_HP[\"CHANNELS_LAST\"],  # Swap image channel from last to first\n    max_steps=10000,  # Max number of training steps\n    episode_steps=500,  # Steps in episode\n    evo_steps=500,  # Evolution frequency\n    eval_steps=500,  # Number of steps in evaluation episode,\n    eval_loop=1,  # Number of evaluation episodes\n    target=INIT_HP[\"TARGET_SCORE\"],  # Target score for early stopping\n    tournament=tournament,  # Tournament selection object\n    mutation=mutations,  # Mutations object\n    wb=INIT_HP[\"WANDB\"],  # Weights and Biases tracking\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Agent CNN Critic for RGB Image States in Python\nDESCRIPTION: This code defines a CNN-based critic network for multi-agent systems with RGB image states. It handles separate state and action inputs, combining them after convolutional processing of the state.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass MultiAgentCNNCritic(nn.Module):\n  def __init__(self):\n      super().__init__()\n\n      # Define the convolutional layers\n      self.conv1 = nn.Conv3d(\n          in_channels=4, out_channels=16, kernel_size=(2, 3, 3), stride=4\n      )\n      self.conv2 = nn.Conv3d(\n          in_channels=16, out_channels=32, kernel_size=(1, 3, 3), stride=2\n      )\n\n      # Define the max-pooling layers\n      self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n      # Define fully connected layers\n      self.fc1 = nn.Linear(15208, 256)\n      self.fc2 = nn.Linear(256, 2)\n\n      # Define activation function\n      self.relu = nn.ReLU()\n\n\n  def forward(self, state_tensor, action_tensor):\n      # Forward pass through convolutional layers\n      x = self.relu(self.conv1(state_tensor))\n      x = self.relu(self.conv2(x))\n\n      # Flatten the output for the fully connected layers\n      x = x.view(x.size(0), -1)\n      x = torch.cat([x, action_tensor], dim=1)\n\n      # Forward pass through fully connected layers\n      x = self.relu(self.fc1(x))\n      x = self.fc2(x)\n\n      return x\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters for Rainbow DQN Agent\nDESCRIPTION: This code snippet defines a dictionary of hyperparameters for training a Rainbow DQN agent. It includes parameters for batch size, learning rate, discount factor, memory size, and various algorithm-specific settings like prioritized experience replay and noisy networks.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initial hyperparameters\nINIT_HP = {\n    \"BATCH_SIZE\": 64,  # Batch size\n    \"LR\": 0.0001,  # Learning rate\n    \"GAMMA\": 0.99,  # Discount factor\n    \"MEMORY_SIZE\": 100_000,  # Max memory buffer size\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"N_STEP\": 3,  # Step number to calculate td error\n    \"PER\": True,  # Use prioritized experience replay buffer\n    \"ALPHA\": 0.6,  # Prioritized replay buffer parameter\n    \"BETA\": 0.4,  # Importance sampling coefficient\n    \"TAU\": 0.001,  # For soft update of target parameters\n    \"PRIOR_EPS\": 0.000001,  # Minimum priority for sampling\n    \"NUM_ATOMS\": 51,  # Unit number of support\n    \"V_MIN\": -200.0,  # Minimum value of support\n    \"V_MAX\": 200.0,  # Maximum value of support\n    \"NOISY\": True,  # Add noise directly to the weights of the network\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"LEARNING_DELAY\": 1000,  # Steps before starting learning\n    \"CHANNELS_LAST\": False,  # Use with RGB states\n    \"TARGET_SCORE\": 200.0,  # Target score that will beat the environment\n    \"MAX_STEPS\": 200000,  # Maximum number of steps an agent takes in an environment\n    \"EVO_STEPS\": 10000,  # Evolution frequency\n    \"EVAL_STEPS\": None,  # Number of evaluation steps per episode\n    \"EVAL_LOOP\": 1,  # Number of evaluation episodes\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Evolutionary Reinforcement Learning Training Loop in Python\nDESCRIPTION: This snippet contains the main training loop for the evolutionary reinforcement learning algorithm. It iterates through the population, performs learning steps, evaluates fitness, and applies tournament selection and mutation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/offline_training/index.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# TRAINING LOOP\nprint(\"Training...\")\npbar = trange(max_steps, unit=\"step\")\nwhile np.less([agent.steps[-1] for agent in pop], max_steps).all():\n    for agent in pop:  # Loop through population\n        for idx_step in range(max_steps):\n            experiences = memory.sample(agent.batch_size)  # Sample replay buffer\n            agent.learn(experiences)  # Learn according to agent's RL algorithm\n        total_steps += max_steps\n        agent.steps[-1] += max_steps\n\n    # Evaluate population\n    fitnesses = [\n        agent.test(\n            env,\n            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n            max_steps=eval_steps,\n            loop=eval_loop,\n        )\n        for agent in pop\n    ]\n\n    print(f\"--- Global Steps {total_steps} ---\")\n    print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n    print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n    print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n    print(\n        f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n    )\n\n    # Tournament selection and population mutation\n    elite, pop = tournament.select(pop)\n    pop = mutations.mutation(pop)\n\n    # Update step counter\n    for agent in pop:\n        agent.steps.append(agent.steps[-1])\n\npbar.close()\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Training MATD3 with PettingZoo Environment (Python)\nDESCRIPTION: Complete example of training a MATD3 agent using AgileRL with a PettingZoo environment. Demonstrates environment setup, agent configuration, and the training loop.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/matd3.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom pettingzoo.mpe import simple_speaker_listener_v4\nfrom tqdm import trange\n\nfrom agilerl.algorithms import MATD3\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\nfrom agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_envs = 8\nenv = AsyncPettingZooVecEnv(\n    [\n        lambda: simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n        for _ in range(num_envs)\n    ]\n)\nenv.reset()\n\n# Configure the multi-agent algo input arguments\nobservation_spaces = [env.single_observation_space(agent) for agent in env.agents]\naction_spaces = [env.single_action_space(agent) for agent in env.agents]\n\nchannels_last = False  # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\nn_agents = env.num_agents\nagent_ids = [agent_id for agent_id in env.agents]\nfield_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\nmemory = MultiAgentReplayBuffer(\n    memory_size=1_000_000,\n    field_names=field_names,\n    agent_ids=agent_ids,\n    device=device,\n)\n\nagent = MATD3(\n    observation_spaces=observation_spaces,\n    action_spaces=action_spaces,\n    agent_ids=agent_ids,\n    vect_noise_dim=num_envs,\n    device=device,\n)\n\n# Define training loop parameters\nmax_steps = 100000  # Max steps\ntotal_steps = 0\n\nwhile agent.steps[-1] < max_steps:\n    state, info  = env.reset() # Reset environment at start of episode\n    scores = np.zeros(num_envs)\n    completed_episode_scores = []\n    if channels_last:\n        state = {agent_id: obs_channels_to_first(s) for agent_id, s in state.items()}\n\n    for _ in range(1000):\n        # Get next action from agent\n        cont_actions, discrete_action = agent.get_action(\n            states=state,\n            training=True,\n            infos=info,\n        )\n        if agent.discrete_actions:\n            action = discrete_action\n        else:\n            action = cont_actions\n\n        # Act in environment\n        next_state, reward, termination, truncation, info = env.step(action)\n\n        scores += np.sum(np.array(list(reward.values())).transpose(), axis=-1)\n        total_steps += num_envs\n        steps += num_envs\n\n        # Save experiences to replay buffer\n        if channels_last:\n            next_state = {\n                agent_id: obs_channels_to_first(ns)\n                for agent_id, ns in next_state.items()\n            }\n        memory.save_to_memory(state, cont_actions, reward, next_state, done, is_vectorised=True)\n\n        # Learn according to learning frequency\n        if len(memory) >= agent.batch_size:\n            for _ in range(num_envs // agent.learn_step):\n                experiences = memory.sample(agent.batch_size) # Sample replay buffer\n                agent.learn(experiences) # Learn according to agent's RL algorithm\n\n        # Update the state\n        state = next_state\n\n        # Calculate scores and reset noise for finished episodes\n        reset_noise_indices = []\n        term_array = np.array(list(termination.values())).transpose()\n        trunc_array = np.array(list(truncation.values())).transpose()\n        for idx, (d, t) in enumerate(zip(term_array, trunc_array)):\n            if np.any(d) or np.any(t):\n                completed_episode_scores.append(scores[idx])\n                agent.scores.append(scores[idx])\n                scores[idx] = 0\n                reset_noise_indices.append(idx)\n        agent.reset_action_noise(reset_noise_indices)\n\n    agent.steps[-1] += steps\n```\n\n----------------------------------------\n\nTITLE: Training Loop with Evolutionary Hyperparameter Optimization\nDESCRIPTION: Implements a training loop that combines evolutionary hyperparameter optimization with reinforcement learning for Connect Four. Handles wandb logging, opponent selection, state transformations, and experience collection. Supports both self-play and different opponent difficulties.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nif max_episodes > 0:\n   wandb.init(\n         # set the wandb project where this run will be logged\n         project=\"AgileRL\",\n         name=\"{}-EvoHPO-{}-{}Opposition-CNN-{}\".format(\n            \"connect_four_v3\",\n            INIT_HP[\"ALGO\"],\n            LESSON[\"opponent\"],\n            datetime.now().strftime(\"%m%d%Y%H%M%S\"),\n         ),\n         # track hyperparameters and run metadata\n         config={\n            \"algo\": \"Evo HPO Rainbow DQN\",\n            \"env\": \"connect_four_v3\",\n            \"INIT_HP\": INIT_HP,\n            \"lesson\": LESSON,\n         },\n   )\n\ntotal_steps = 0\ntotal_episodes = 0\npbar = trange(int(max_episodes / episodes_per_epoch))\n\n# Training loop\nfor idx_epi in pbar:\n   turns_per_episode = []\n   train_actions_hist = [0] * action_spaces[0].n\n   for agent in pop:  # Loop through population\n         for episode in range(episodes_per_epoch):\n            env.reset()  # Reset environment at start of episode\n            observation, cumulative_reward, done, truncation, _ = env.last()\n\n            (\n               p1_state,\n               p1_state_flipped,\n               p1_action,\n               p1_next_state,\n               p1_next_state_flipped,\n            ) = (None, None, None, None, None)\n\n            if LESSON[\"opponent\"] == \"self\":\n               # Randomly choose opponent from opponent pool if using self-play\n               opponent = random.choice(opponent_pool)\n            else:\n               # Create opponent of desired difficulty\n               opponent = Opponent(env, difficulty=LESSON[\"opponent\"])\n\n            # Randomly decide whether agent will go first or second\n            opponent_first = random.random() > 0.5\n\n            score = 0\n            turns = 0  # Number of turns counter\n            for idx_step in range(max_steps):\n               # Player 0\"s turn\n               p0_action_mask = observation[\"action_mask\"]\n               p0_state, p0_state_flipped = transform_and_flip(\n                     observation, player=0\n               )\n\n               if opponent_first:\n                     if LESSON[\"opponent\"] == \"self\":\n                        p0_action = opponent.get_action(\n                           p0_state, 0, p0_action_mask\n                        )[0]\n                     elif LESSON[\"opponent\"] == \"random\":\n                        p0_action = opponent.get_action(\n                           p0_action_mask, p1_action, LESSON[\"block_vert_coef\"]\n                        )\n                     else:\n                        p0_action = opponent.get_action(player=0)\n               else:\n                     p0_action = agent.get_action(\n                        p0_state, epsilon, p0_action_mask\n                     )[\n                        0\n                     ]  # Get next action from agent\n                     train_actions_hist[p0_action] += 1\n\n               env.step(p0_action)  # Act in environment\n               observation, cumulative_reward, done, truncation, _ = env.last()\n               p0_next_state, p0_next_state_flipped = transform_and_flip(\n                     observation, player=0\n               )\n               if not opponent_first:\n                     score = cumulative_reward\n               turns += 1\n\n               # Check if game is over (Player 0 win)\n               if done or truncation:\n                     reward = env.reward(done=True, player=0)\n                     transition = Transition(\n                        obs=np.concatenate(\n                           (\n                                 p0_state,\n                                 p1_state,\n                                 p0_state_flipped,\n                                 p1_state_flipped,\n                           )\n                        ),\n                        action=np.array(\n                           [p0_action, p1_action, 6 - p0_action, 6 - p1_action]\n                        ),\n                        reward=np.array(\n                           [\n                                 reward,\n                                 LESSON[\"rewards\"][\"lose\"],\n                                 reward,\n                                 LESSON[\"rewards\"][\"lose\"],\n                           ]\n                        ),\n                        next_obs=np.concatenate(\n                           (\n                                 p0_next_state,\n                                 p1_next_state,\n                                 p0_next_state_flipped,\n                                 p1_next_state_flipped,\n                           )\n                        ),\n                        done=np.array([done, done, done, done]),\n                        batch_size=[4],\n                     )\n                     memory.add(transition.to_tensordict(), is_vectorised=True)\n               else:  # Play continues\n                     if p1_state is not None:\n                        reward = env.reward(done=False, player=1)\n                        transition = Transition(\n                           obs=np.concatenate((p1_state, p1_state_flipped)),\n                           action=np.array([p1_action, 6 - p1_action]),\n                           reward=np.array([reward, reward]),\n                           next_obs=np.concatenate(\n                                 (p1_next_state, p1_next_state_flipped)\n                           ),\n                           done=np.array([done, done]),\n                           batch_size=[2],\n                        )\n                        memory.add(\n                           transition.to_tensordict(), is_vectorised=True\n                        )\n\n                     # Player 1\"s turn\n                     p1_action_mask = observation[\"action_mask\"]\n                     p1_state, p1_state_flipped = transform_and_flip(\n                        observation, player=1\n                     )\n\n                     if not opponent_first:\n                        if LESSON[\"opponent\"] == \"self\":\n                           p1_action = opponent.get_action(\n                                 p1_state, 0, p1_action_mask\n                           )[0]\n                        elif LESSON[\"opponent\"] == \"random\":\n                           p1_action = opponent.get_action(\n                                 p1_action_mask,\n                                 p0_action,\n                                 LESSON[\"block_vert_coef\"],\n                           )\n                        else:\n                           p1_action = opponent.get_action(player=1)\n                     else:\n                        p1_action = agent.get_action(\n                           p1_state, epsilon, p1_action_mask\n                        )[\n                           0\n                        ]  # Get next action from agent\n                        train_actions_hist[p1_action] += 1\n\n                     env.step(p1_action)  # Act in environment\n                     observation, cumulative_reward, done, truncation, _ = (\n                        env.last()\n                     )\n                     p1_next_state, p1_next_state_flipped = transform_and_flip(\n                        observation, player=1\n                     )\n\n                     if opponent_first:\n                        score = cumulative_reward\n                     turns += 1\n\n                     # Check if game is over (Player 1 win)\n                     if done or truncation:\n                        reward = env.reward(done=True, player=1)\n                        transition = Transition(\n                           obs=np.concatenate(\n                                 (\n                                    p0_state,\n                                    p1_state,\n                                    p0_state_flipped,\n                                    p1_state_flipped,\n                                 )\n                           ),\n                           action=np.array(\n                                 [\n                                    p0_action,\n                                    p1_action,\n                                    6 - p0_action,\n                                    6 - p1_action,\n                                 ]\n                           ),\n                           reward=np.array(\n                                 [\n                                    reward,\n                                    LESSON[\"rewards\"][\"lose\"],\n                                    reward,\n                                    LESSON[\"rewards\"][\"lose\"],\n                                 ]\n```\n\n----------------------------------------\n\nTITLE: Training Loop Iteration in Python for Reinforcement Learning\nDESCRIPTION: Executes a single iteration of the training loop, including action selection, environment interaction, experience collection, and state updates. It handles channel swapping, action clipping, and episode completion tracking.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfor idx_step in range(-(agent.learn_step // -num_envs)):\n    if INIT_HP[\"CHANNELS_LAST\"]:\n        state = obs_channels_to_first(state)\n\n    # Get next action from agent\n    action, log_prob, _, value = agent.get_action(state)\n\n    # Clip to action space\n    if isinstance(agent.action_space, spaces.Box):\n        if agent.actor.squash_output:\n            clipped_action = agent.actor.scale_action(action)\n        else:\n            clipped_action = np.clip(action, agent.action_space.low, agent.action_space.high)\n    else:\n        clipped_action = action\n\n    # Act in environment\n    next_state, reward, terminated, truncated, info = env.step(clipped_action)\n    next_done = np.logical_or(terminated, truncated).astype(np.int8)\n\n    total_steps += num_envs\n    steps += num_envs\n    learn_steps += num_envs\n\n    states.append(state)\n    actions.append(action)\n    log_probs.append(log_prob)\n    rewards.append(reward)\n    dones.append(dones)\n    values.append(value)\n\n    state = next_state\n    done = next_done\n    scores += np.array(reward)\n\n    for idx, (d, t) in enumerate(zip(terminated, truncated)):\n        if d or t:\n            completed_episode_scores.append(scores[idx])\n            agent.scores.append(scores[idx])\n            scores[idx] = 0\n\npbar.update(learn_steps // len(pop))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Rainbow DQN Cartpole Tutorial\nDESCRIPTION: This code snippet imports the necessary libraries and modules for training a Rainbow DQN agent on the Cartpole environment using AgileRL. It includes imports for the environment, the Rainbow DQN algorithm, replay buffers, and training utilities.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Author: Michael Pratt\nimport os\n\nimport imageio\nimport gymnasium as gym\nimport numpy as np\nimport torch\nfrom agilerl.algorithms.dqn_rainbow import RainbowDQN\nfrom agilerl.components.replay_buffer import (\n    MultiStepReplayBuffer,\n    PrioritizedReplayBuffer,\n)\nfrom agilerl.training.train_off_policy import train_off_policy\nfrom agilerl.utils.utils import make_vect_envs\nfrom tqdm import trange\n```\n\n----------------------------------------\n\nTITLE: Creating GRPO Agent - Python\nDESCRIPTION: Initialization of the GRPO (Gradient-based Reinforcement Policy Optimization) agent with model and training parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nagent = GRPO(\n    env.observation_space,\n    env.action_space,\n    actor_network=model,\n    pad_token_id=tokenizer.eos_token_id,\n    max_output_tokens=1024,\n    batch_size=1,\n    group_size=12,\n    reduce_memory_peak=True,\n    accelerator=Accelerator()\n)\n```\n\n----------------------------------------\n\nTITLE: Agent Learning Process in Connect Four Training Loop\nDESCRIPTION: This code implements the learning process for the agent, sampling from memory when conditions are met. It checks if enough steps have passed according to the learning frequency and if the memory buffer contains enough samples for a batch.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Learn according to learning frequency\nif (memory.counter % agent.learn_step == 0) and (\n      len(memory) >= agent.batch_size\n):\n      # Sample replay buffer\n      # Learn according to agent\"s RL algorithm\n      experiences = memory.sample(agent.batch_size)\n      agent.learn(experiences)\n\n# Stop episode if any agents have terminated\nif done or truncation:\n      break\n```\n\n----------------------------------------\n\nTITLE: Training Loop Implementation in Python for Neural UCB Reinforcement Learning\nDESCRIPTION: Main training loop that manages multiple agents training on a bandit environment. Includes experience collection, learning from replay buffer, periodic evaluation, logging with wandb, and evolutionary population updates. Uses tournament selection and mutation for population optimization.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/bandits/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    episode_steps = 500  # Steps in episode\n    evo_steps = 500  # Evolution frequency\n    eval_steps = 500  # Evaluation steps per episode\n    eval_loop = 1  # Number of evaluation episodes\n\n    print(\"Training...\")\n\n    wandb.init(\n        # set the wandb project where this run will be logged\n        project=\"AgileRL-Bandits\",\n        name=\"NeuralUCB-{}\".format(datetime.now().strftime(\"%m%d%Y%H%M%S\")),\n        # track hyperparameters and run metadata\n        config=INIT_HP,\n    )\n\n    total_steps = 0\n    evo_count = 0\n\n    # TRAINING LOOP\n    print(\"Training...\")\n    pbar = trange(max_steps, unit=\"step\")\n    while np.less([agent.steps[-1] for agent in pop], max_steps).all():\n        pop_episode_scores = []\n        for agent_idx, agent in enumerate(pop):  # Loop through population\n            score = 0\n            losses = []\n            context = env.reset()  # Reset environment at start of episode\n            for idx_step in range(episode_steps):\n                if INIT_HP[\"CHANNELS_LAST\"]:\n                    context = obs_channels_to_first(context)\n                # Get next action from agent\n                action = agent.get_action(context)\n                next_context, reward = env.step(action)  # Act in environment\n\n                transition = TensorDict(\n                    {\n                        \"obs\": context[action],\n                        \"reward\": reward,\n                    },\n                ).float()\n                transition.batch_size = [1]\n                # Save experience to replay buffer\n                memory.add(transition)\n\n                # Learn according to learning frequency\n                if len(memory) >= agent.batch_size:\n                    for _ in range(agent.learn_step):\n                        # Sample replay buffer\n                        # Learn according to agent's RL algorithm\n                        experiences = memory.sample(agent.batch_size)\n                        loss = agent.learn(experiences)\n                        losses.append(loss)\n\n                context = next_context\n                score += reward\n                agent.regret.append(agent.regret[-1] + 1 - reward)\n\n            agent.scores.append(score)\n            pop_episode_scores.append(score)\n            agent.steps[-1] += episode_steps\n            total_steps += episode_steps\n            pbar.update(episode_steps // len(pop))\n\n            wandb_dict = {\n                \"global_step\": total_steps,\n                \"train/loss\": np.mean(losses),\n                \"train/score\": score,\n                \"train/mean_regret\": np.mean([agent.regret[-1] for agent in pop]),\n            }\n            wandb.log(wandb_dict)\n\n        # Evaluate population\n        fitnesses = [\n            agent.test(\n                env,\n                swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n                max_steps=eval_steps,\n                loop=eval_loop,\n            )\n            for agent in pop\n        ]\n\n        print(f\"--- Global steps {total_steps} ---\")\n        print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n        print(f\"Regret: {[agent.regret[-1] for agent in pop]}\")\n        print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n        print(\n            f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n        )\n\n        if pop[0].steps[-1] // evo_steps > evo_count:\n            # Tournament selection and population mutation\n            elite, pop = tournament.select(pop)\n            pop = mutations.mutation(pop)\n            evo_count += 1\n\n        # Update step counter\n        for agent in pop:\n            agent.steps.append(agent.steps[-1])\n\n    pbar.close()\n    env.close()\n```\n\n----------------------------------------\n\nTITLE: Complete MADDPG Training Example\nDESCRIPTION: Full example of setting up and training a MADDPG agent using PettingZoo environment and AgileRL framework.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/maddpg.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom pettingzoo.mpe import simple_speaker_listener_v4\nfrom tqdm import trange\n\nfrom agilerl.algorithms import MADDPG\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\nfrom agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\nfrom agilerl.utils.algo_utils import obs_channels_to_first\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_envs = 8\nenv = AsyncPettingZooVecEnv(\n    [\n        lambda: simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n        for _ in range(num_envs)\n    ]\n)\nenv.reset()\n\n# Configure the multi-agent algo input arguments\nobservation_spaces = [env.single_observation_space(agent) for agent in env.agents]\naction_spaces = [env.single_action_space(agent) for agent in env.agents]\n\nchannels_last = False  # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\nn_agents = env.num_agents\nagent_ids = [agent_id for agent_id in env.agents]\nfield_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\nmemory = MultiAgentReplayBuffer(\n    memory_size=1_000_000,\n    field_names=field_names,\n    agent_ids=agent_ids,\n    device=device,\n)\n\nagent = MADDPG(\n    observation_spaces=observation_spaces,\n    action_spaces=action_spaces,\n    agent_ids=agent_ids,\n    vect_noise_dim=num_envs,\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters for PPO Agent and Mutation Parameters\nDESCRIPTION: Sets up the initial hyperparameters for the PPO algorithm, including population size, batch size, learning rate, and other training parameters. Also defines mutation parameters that control how evolutionary hyperparameter optimization is performed.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initial hyperparameters\nINIT_HP = {\n    \"POP_SIZE\": 4,  # Population size\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 0.001,  # Learning rate\n    \"LEARN_STEP\": 1024,  # Learning frequency\n    \"GAMMA\": 0.99,  # Discount factor\n    \"GAE_LAMBDA\": 0.95,  # Lambda for general advantage estimation\n    \"ACTION_STD_INIT\": 0.6,  # Initial action standard deviation\n    \"CLIP_COEF\": 0.2,  # Surrogate clipping coefficient\n    \"ENT_COEF\": 0.01,  # Entropy coefficient\n    \"VF_COEF\": 0.5,  # Value function coefficient\n    \"MAX_GRAD_NORM\": 0.5,  # Maximum norm for gradient clipping\n    \"TARGET_KL\": None,  # Target KL divergence threshold\n    \"UPDATE_EPOCHS\": 4,  # Number of policy update epochs\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,  # Use with RGB states\n    \"TARGET_SCORE\": 200.0,  # Target score that will beat the environment\n    \"MAX_STEPS\": 150000,  # Maximum number of steps an agent takes in an environment\n    \"EVO_STEPS\": 10000,  # Evolution frequency\n    \"EVAL_STEPS\": None,  # Number of evaluation steps per episode\n    \"EVAL_LOOP\": 3,  # Number of evaluation episodes\n    \"TOURN_SIZE\": 2,  # Tournament size\n    \"ELITISM\": True,  # Elitism in tournament selection\n}\n\n# Mutation parameters\nMUT_P = {\n    # Mutation probabilities\n    \"NO_MUT\": 0.4,  # No mutation\n    \"ARCH_MUT\": 0.2,  # Architecture mutation\n    \"NEW_LAYER\": 0.2,  # New layer mutation\n    \"PARAMS_MUT\": 0.2,  # Network parameters mutation\n    \"ACT_MUT\": 0.2,  # Activation layer mutation\n    \"RL_HP_MUT\": 0.2,  # Learning HP mutation\n    \"MUT_SD\": 0.1,  # Mutation strength\n    \"RAND_SEED\": 42,  # Random seed\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Hyperparameters in Python\nDESCRIPTION: Defines initial hyperparameters for training including environment settings, algorithm choice, learning parameters, and population evolution settings.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nINIT_HP = {\n    'ENV_NAME': 'LunarLander-v2',   # Gym environment name\n    'ALGO': 'DQN',                  # Algorithm\n    'DOUBLE': True,                 # Use double Q-learning\n    'CHANNELS_LAST': False,         # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    'BATCH_SIZE': 256,              # Batch size\n    'LR': 1e-3,                     # Learning rate\n    'MAX_STEPS': 1_000_000,         # Max no. steps\n    'TARGET_SCORE': 200.,           # Early training stop at avg score of last 100 episodes\n    'GAMMA': 0.99,                  # Discount factor\n    'MEMORY_SIZE': 10000,           # Max memory buffer size\n    'LEARN_STEP': 1,                # Learning frequency\n    'TAU': 1e-3,                    # For soft update of target parameters\n    'TOURN_SIZE': 2,                # Tournament size\n    'ELITISM': True,                # Elitism in tournament selection\n    'POP_SIZE': 6,                  # Population size\n    'EVO_STEPS': 10_000,            # Evolution frequency\n    'EVAL_STEPS': None,             # Evaluation steps\n    'EVAL_LOOP': 1,                 # Evaluation episodes\n    'LEARNING_DELAY': 1000,         # Steps before starting learning\n    'WANDB': True,                  # Log with Weights and Biases\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing AgilRL Skills Curriculum Training\nDESCRIPTION: Complete implementation of a skills curriculum training approach in AgilRL. This code demonstrates how to set up and train a hierarchical policy using the Skills framework.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n../../../tutorials/Skills/agilerl_skills_curriculum.py\n```\n\n----------------------------------------\n\nTITLE: Initializing TournamentSelection for Evolutionary HPO in Python\nDESCRIPTION: This snippet demonstrates how to initialize the TournamentSelection class from AgileRL. It sets up tournament selection parameters for evolutionary hyperparameter optimization, including tournament size, elitism, population size, and evaluation loop.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/evo_hyperparam_opt/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.hpo.tournament import TournamentSelection\n\ntournament = TournamentSelection(\n    tournament_size=2,  # Tournament selection size\n    elitism=True,  # Elitism in tournament selection\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    eval_loop=1,  # Evaluate using last N fitness scores\n)\n```\n\n----------------------------------------\n\nTITLE: Training Selector Agent with Hierarchical Skills\nDESCRIPTION: Implements the main training loop for the selector agent that learns to choose between different skills. Includes policy execution, reward accumulation, and learning updates with experience collection.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwhile np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n   for agent in pop:\n         state = env.reset()[0]\n         score = 0\n\n         states = []\n         actions = []\n         log_probs = []\n         rewards = []\n         dones = []\n         values = []\n\n         done = np.zeros(1)\n\n         for idx_step in range(500):\n            action, log_prob, _, value = agent.get_action(state)\n\n            if isinstance(agent.action_space, spaces.Box):\n                if agent.actor.squash_output:\n                    clipped_action = agent.actor.scale_action(action)\n                else:\n                    clipped_action = np.clip(action, agent.action_space.low, agent.action_space.high)\n            else:\n                clipped_action = action\n\n            skill_agent = trained_skills[action[0]][\"agent\"]\n            skill_duration = trained_skills[action[0]][\"skill_duration\"]\n            reward = 0\n            for skill_step in range(skill_duration):\n               if state[0][6] or state[0][7]:\n                     next_state, skill_reward, termination, truncation, _ = env.step([0])\n               else:\n                     skill_action, _, _, _ = skill_agent.get_action(state)\n                     next_state, skill_reward, termination, truncation, _ = env.step(skill_action)\n               next_done = np.logical_or(termination, truncation).astype(np.int8)\n               reward += skill_reward\n               if np.any(termination) or np.any(truncation):\n                     break\n               state = next_state\n               done = next_done\n            score += reward\n\n            states.append(state)\n            actions.append(action)\n            log_probs.append(log_prob)\n            rewards.append(reward)\n            dones.append(done)\n            values.append(value)\n\n         agent.scores.append(score)\n\n         agent.learn(\n            (\n               states,\n               actions,\n               log_probs,\n               rewards,\n               dones,\n               values,\n               next_state,\n               next_done,\n            )\n         )\n\n         agent.steps[-1] += idx_step + 1\n         total_steps += idx_step + 1\n\n   if (agent.steps[-1]) % INIT_HP[\"EVO_STEPS\"] == 0:\n      mean_scores = np.mean([agent.scores[-20:] for agent in pop], axis=1)\n      if INIT_HP[\"WANDB\"]:\n          wandb.log(\n              {\n                  \"global_step\": total_steps,\n                  \"train/mean_score\": np.mean(mean_scores),\n              }\n          )\n      print(\n          f\"\"\"\n          --- Global Steps {total_steps} ---\n          Score:\\t\\t{mean_scores}\n          \"\"\",\n          end=\"\\r\",\n      )\n\nif INIT_HP[\"WANDB\"]:\n   wandb.finish()\nenv.close()\n\nfilename = \"PPO_trained_agent_selector.pt\"\nsave_path = os.path.join(save_dir, filename)\npop[0].save_checkpoint(save_path)\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-Agent Policy and Q-Learning with MADDPG\nDESCRIPTION: Shows how to test MADDPG implementations using multi-agent probe environments. Validates policy and Q-learning components for multiple agents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/debugging_rl/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom agilerl.algorithms.maddpg import MADDPG\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\nfrom agilerl.utils.probe_envs_ma import check_policy_q_learning_with_probe_env\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nvector_envs = [\n    (ConstantRewardEnv(), 1000),\n    (ObsDependentRewardEnv(), 1000),\n    (DiscountedRewardEnv(), 3000),\n    (FixedObsPolicyEnv(), 1000),\n    (PolicyEnv(), 4000),\n    (MultiPolicyEnv(), 8000),\n]\n\nfor env, learn_steps in vector_envs:\n    algo_args = {\n        \"observation_spaces\": [env.observation_spaces[agent] for agent in env.agents],\n        \"action_spaces\": [env.action_space[agent] for agent in env.agents],\n        \"agent_ids\": env.possible_agents,\n        \"net_config\": {\"head_config\": {\"hidden_size\": [32, 32]}},\n        \"batch_size\": 256,\n    }\n    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n    memory = MultiAgentReplayBuffer(\n        memory_size=10000,  # Max replay buffer size\n        field_names=field_names,  # Field names to store in memory\n        agent_ids=algo_args[\"agent_ids\"],\n        device=device,\n    )\n\n    check_policy_q_learning_with_probe_env(env, MADDPG, algo_args, memory, learn_steps, device)\n```\n\n----------------------------------------\n\nTITLE: Population Evaluation Against Random Opponents in Connect Four\nDESCRIPTION: This code evaluates the population of agents against random or predefined opponents. It tracks various metrics including fitness scores, win rates, action distributions, and average turns per game during the evaluation phase.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Now evolve population if necessary\nif (idx_epi + 1) % evo_epochs == 0:\n      # Evaluate population vs random actions\n      fitnesses = []\n      win_rates = []\n      eval_actions_hist = [0] * action_spaces[0].n  # Eval actions histogram\n      eval_turns = 0  # Eval turns counter\n      for agent in pop:\n         with torch.no_grad():\n            rewards = []\n            for i in range(evo_loop):\n                  env.reset()  # Reset environment at start of episode\n                  observation, cumulative_reward, done, truncation, _ = (\n                     env.last()\n                  )\n\n                  player = -1  # Tracker for which player\"s turn it is\n\n                  # Create opponent of desired difficulty\n                  opponent = Opponent(env, difficulty=LESSON[\"eval_opponent\"])\n\n                  # Randomly decide whether agent will go first or second\n                  if random.random() > 0.5:\n                     opponent_first = False\n                  else:\n                     opponent_first = True\n\n                  score = 0\n\n                  for idx_step in range(max_steps):\n                     action_mask = observation[\"action_mask\"]\n                     if player < 0:\n                        if opponent_first:\n                              if LESSON[\"eval_opponent\"] == \"random\":\n                                 action = opponent.get_action(action_mask)\n                              else:\n                                 action = opponent.get_action(player=0)\n                        else:\n                              state = np.moveaxis(\n                                 observation[\"observation\"], [-1], [-3]\n                              )\n                              state = np.expand_dims(state, 0)\n                              action = agent.get_action(\n                                 state, 0, action_mask\n                              )[\n                                 0\n                              ]  # Get next action from agent\n                              eval_actions_hist[action] += 1\n                     if player > 0:\n                        if not opponent_first:\n                              if LESSON[\"eval_opponent\"] == \"random\":\n                                 action = opponent.get_action(action_mask)\n                              else:\n                                 action = opponent.get_action(player=1)\n                        else:\n                              state = np.moveaxis(\n                                 observation[\"observation\"], [-1], [-3]\n                              )\n                              state[[0, 1], :, :] = state[[1, 0], :, :]\n                              state = np.expand_dims(state, 0)\n                              action = agent.get_action(\n                                 state, 0, action_mask\n                              )[\n                                 0\n                              ]  # Get next action from agent\n                              eval_actions_hist[action] += 1\n\n                     env.step(action)  # Act in environment\n                     observation, cumulative_reward, done, truncation, _ = (\n                        env.last()\n                     )\n\n                     if (player > 0 and opponent_first) or (\n                        player < 0 and not opponent_first\n                     ):\n                        score = cumulative_reward\n\n                     eval_turns += 1\n\n                     if done or truncation:\n                        break\n\n                     player *= -1\n\n                  rewards.append(score)\n         mean_fit = np.mean(rewards)\n         agent.fitness.append(mean_fit)\n         fitnesses.append(mean_fit)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Population for Multi-Agent Reinforcement Learning in Python\nDESCRIPTION: Creates a population of agents for evolutionary hyperparameter optimization in a multi-agent environment. This example sets up a simple speaker listener environment using PettingZoo, configures observation and action spaces, and creates a population of MADDPG agents with defined network configurations and hyperparameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/multi_agent_training/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.utils.utils import create_population\nfrom agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\nfrom pettingzoo.mpe import simple_speaker_listener_v4\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the network configuration\nNET_CONFIG = {\n    \"head_config\": {\"hidden_size\": [32, 32]}  # Actor head hidden size\n}\n\n# Define the initial hyperparameters\nINIT_HP = {\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n    \"BATCH_SIZE\": 32,  # Batch size\n    \"O_U_NOISE\": True,  # Ornstein Uhlenbeck action noise\n    \"EXPL_NOISE\": 0.1,  # Action noise scale\n    \"MEAN_NOISE\": 0.0,  # Mean action noise\n    \"THETA\": 0.15,  # Rate of mean reversion in OU noise\n    \"DT\": 0.01,  # Timestep for OU noise\n    \"LR_ACTOR\": 0.001,  # Actor learning rate\n    \"LR_CRITIC\": 0.001,  # Critic learning rate\n    \"GAMMA\": 0.95,  # Discount factor\n    \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n    \"LEARN_STEP\": 100,  # Learning frequency\n    \"TAU\": 0.01,  # For soft update of target parameters\n    \"POLICY_FREQ\": 2,  # Policy frequnecy\n    \"POP_SIZE\": 4,  # Population size\n}\n\nnum_envs = 8\n# Define the simple speaker listener environment as a parallel environment\nenv = AsyncPettingZooVecEnv(\n    [\n        lambda: simple_speaker_listener_v4.parallel_env(continuous_actions=True)\n        for _ in range(num_envs)\n    ]\n)\nenv.reset()\n\n# Configure the multi-agent algo input arguments\nobservation_spaces = [env.single_observation_space(agent) for agent in env.agents]\naction_spaces = [env.single_action_space(agent) for agent in env.agents]\nif INIT_HP[\"CHANNELS_LAST\"]:\n    observation_spaces = [observation_space_channels_to_first(obs) for obs in observation_spaces]\n\n# Append number of agents and agent IDs to the initial hyperparameter dictionary\nINIT_HP[\"AGENT_IDS\"] = env.agents\n\n# Mutation config for RL hyperparameters\nhp_config = HyperparameterConfig(\n    lr_actor = RLParameter(min=1e-4, max=1e-2),\n    lr_critic = RLParameter(min=1e-4, max=1e-2),\n    batch_size = RLParameter(min=8, max=512, dtype=int),\n    learn_step = RLParameter(\n        min=20, max=200, dtype=int, grow_factor=1.5, shrink_factor=0.75\n        )\n)\n\n# Create a population ready for evolutionary hyper-parameter optimisation\npop = create_population(\n    \"MADDPG\",\n    observation_spaces,\n    action_spaces,\n    NET_CONFIG,\n    INIT_HP,\n    hp_config,\n    population_size=INIT_HP[\"POP_SIZE\"],\n    num_envs=num_envs,\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tournament Selection for Evolution\nDESCRIPTION: Initializes the tournament selection mechanism used for evolutionary hyperparameter optimization. Tournament selection helps determine which agents from the current population survive to the next generation based on their performance in the environment.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntournament = TournamentSelection(\n    INIT_HP[\"TOURN_SIZE\"],\n    INIT_HP[\"ELITISM\"],\n    INIT_HP[\"POP_SIZE\"],\n    INIT_HP[\"EVAL_LOOP\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Training Loop in Python\nDESCRIPTION: Complete example of a distributed training implementation using AgileRL and Accelerate. Includes setup of environment, replay buffer, population management, and training loop with evolutionary strategies.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/distributed_training/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.components.data import ReplayDataset\nfrom agilerl.components.sampler import Sampler\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import create_population, make_vect_envs, observation_space_channels_to_first\nfrom accelerate import Accelerator\nimport numpy as np\nimport os\nfrom torch.utils.data import DataLoader\nfrom tqdm import trange\n\naccelerator = Accelerator()\n\naccelerator.wait_for_everyone()\nif accelerator.is_main_process:\n    print(\"===== AgileRL Online Distributed Demo =====\")\naccelerator.wait_for_everyone()\n\nNET_CONFIG = {\n    \"encoder_config\": {\"hidden_size\": [32, 32]},  # Encoder hidden size\n    \"head_config\": {\"hidden_size\": [32, 32]},  # Head hidden size\n}\n\nINIT_HP = {\n    \"DOUBLE\": True,  # Use double Q-learning in DQN or CQN\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"GAMMA\": 0.99,  # Discount factor\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"TAU\": 1e-3,  # For soft update of target network parameters\n    # Swap image channels dimension last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n    \"POP_SIZE\": 4,  # Population size\n}\n\n# Create vectorized environment\nnum_envs = 8\nenv = make_vect_envs(\"LunarLander-v2\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\n# RL hyperparameter configuration for mutations\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=1e-4, max=1e-2),\n    batch_size = RLParameter(min=8, max=64, dtype=int),\n    learn_step = RLParameter(\n        min=1, max=120, dtype=int, grow_factor=1.5, shrink_factor=0.75\n        )\n)\n\npop = create_population(\n    algo=\"DQN\",  # RL algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,  # No. vectorized envs\n    accelerator=accelerator,  # Accelerator\n)\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=accelerator.device,\n)\n\nreplay_dataset = ReplayDataset(memory, INIT_HP[\"BATCH_SIZE\"])\nreplay_dataloader = DataLoader(replay_dataset, batch_size=None)\nreplay_dataloader = accelerator.prepare(replay_dataloader)\nsampler = Sampler(\n    distributed=True, dataset=replay_dataset, dataloader=replay_dataloader\n)\n\ntournament = TournamentSelection(\n    tournament_size=2,  # Tournament selection size\n    elitism=True,  # Elitism in tournament selection\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    eval_loop=1,  # Evaluate using last N fitness scores\n)\n\nmutations = Mutations(\n    no_mutation=0.4,  # No mutation\n    architecture=0.2,  # Architecture mutation\n    new_layer_prob=0.2,  # New layer mutation\n    parameters=0.2,  # Network parameters mutation\n    activation=0,  # Activation layer mutation\n    rl_hp=0.2,  # Learning HP mutation\n    mutation_sd=0.1,  # Mutation strength  # Network architecture\n    rand_seed=1,  # Random seed\n    accelerator=accelerator, # Accelerator\n)\n\nmax_steps = 200000  # Max steps\nlearning_delay = 1000  # Steps before starting learning\n\n# Exploration params\neps_start = 1.0  # Max exploration\neps_end = 0.1  # Min exploration\neps_decay = 0.995  # Decay per episode\nepsilon = eps_start\n\nevo_steps = 10000  # Evolution frequency\neval_steps = None  # Evaluation steps per episode - go until done\neval_loop = 1  # Number of evaluation episodes\n\ntotal_steps = 0\n\naccel_temp_models_path = \"models/{}\".format(\"LunarLander-v2\")\nif accelerator.is_main_process:\n    if not os.path.exists(accel_temp_models_path):\n        os.makedirs(accel_temp_models_path)\n\nprint(f\"\\nDistributed training on {accelerator.device}...\")\n\n# TRAINING LOOP\nprint(\"Training...\")\npbar = trange(max_steps, unit=\"step\", disable=not accelerator.is_local_main_process)\nwhile np.less([agent.steps[-1] for agent in pop], max_steps).all():\n    accelerator.wait_for_everyone()\n    pop_episode_scores = []\n    for agent in pop:  # Loop through population\n        state, info = env.reset()  # Reset environment at start of episode\n        scores = np.zeros(num_envs)\n        completed_episode_scores, losses = [], []\n        steps = 0\n        epsilon = eps_start\n\n        for idx_step in range(evo_steps):\n            # Get next action from agent\n            action = agent.get_action(state, epsilon)\n            epsilon = max(\n                eps_end, epsilon * eps_decay\n            )  # Decay epsilon for exploration\n\n            # Act in environment\n            next_state, reward, terminated, truncated, info = env.step(action)\n            scores += np.array(reward)\n            steps += num_envs\n            total_steps += num_envs\n\n            # Collect scores for completed episodes\n            for idx, (d, t) in enumerate(zip(terminated, truncated)):\n                if d or t:\n                    completed_episode_scores.append(scores[idx])\n                    agent.scores.append(scores[idx])\n                    scores[idx] = 0\n\n            # Save experience to replay buffer\n            memory.save_to_memory_vect_envs(\n                state, action, reward, next_state, terminated\n            )\n\n            # Learn according to learning frequency\n            if memory.counter > learning_delay and len(memory) >= agent.batch_size:\n                for _ in range(num_envs // agent.learn_step):\n                    # Sample dataloader\n                    experiences = sampler.sample(agent.batch_size)\n                    # Learn according to agent's RL algorithm\n                    agent.learn(experiences)\n\n            state = next_state\n\n        pbar.update(evo_steps // len(pop))\n        agent.steps[-1] += steps\n        pop_episode_scores.append(completed_episode_scores)\n\n    # Reset epsilon start to latest decayed value for next round of population training\n    eps_start = epsilon\n\n    # Evaluate population\n    fitnesses = [\n        agent.test(\n            env,\n            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n            max_steps=eval_steps,\n            loop=eval_loop,\n        )\n        for agent in pop\n    ]\n    mean_scores = [\n        (\n            np.mean(episode_scores)\n            if len(episode_scores) > 0\n            else \"0 completed episodes\"\n        )\n        for episode_scores in pop_episode_scores\n    ]\n\n    if accelerator.is_main_process:\n        print(f\"--- Global steps {total_steps} ---\")\n        print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n        print(f\"Scores: {mean_scores}\")\n        print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n        print(\n            f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n        )\n\n    # Tournament selection and population mutation\n    accelerator.wait_for_everyone()\n    for model in pop:\n        model.unwrap_models()\n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        elite, pop = tournament.select(pop)\n        pop = mutations.mutation(pop)\n        for pop_i, model in enumerate(pop):\n            model.save_checkpoint(f\"{accel_temp_models_path}/DQN_{pop_i}.pt\")\n    accelerator.wait_for_everyone()\n    if not accelerator.is_main_process:\n```\n\n----------------------------------------\n\nTITLE: Instantiating Rainbow DQN Agent for Cartpole\nDESCRIPTION: This code snippet sets up the device for training, defines the network configuration for the Rainbow DQN agent, and creates a hyperparameter configuration for potential mutation during training. It then instantiates a Rainbow DQN agent with the specified parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set-up the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Define the network configuration of a simple mlp with two hidden layers, each with 64 nodes\nnet_config = {\n    \"encoder_config\": {\"hidden_size\": [64, 64]},  # Encoder hidden size\n    \"head_config\": {\"hidden_size\": [64, 64]}  # Head hidden size\n}\n\n# RL hyperparameters configuration for mutation during training\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=6.25e-5, max=1e-2),\n    learn_step = RLParameter(min=1, max=10),\n    batch_size = RLParameter(\n        min=8, max=512, dtype=int\n        )\n)\n\n# Define a Rainbow-DQN agent\nrainbow_dqn = RainbowDQN(\n    observation_space=observation_space,\n    action_space=action_space,\n    net_config=net_config,\n    hp_config=hp_config,\n    batch_size=INIT_HP[\"BATCH_SIZE\"],\n    lr=INIT_HP[\"LR\"],\n    learn_step=INIT_HP[\"LEARN_STEP\"],\n    gamma=INIT_HP[\"GAMMA\"],\n    tau=INIT_HP[\"TAU\"],\n    beta=INIT_HP[\"BETA\"],\n    n_step=INIT_HP[\"N_STEP\"],\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Experience Collection and Agent Learning in Python\nDESCRIPTION: Prepares the collected experiences and triggers the agent's learning process. It handles channel swapping for the next state and packages the experiences into a tuple for the agent to learn from.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nif INIT_HP[\"CHANNELS_LAST\"]:\n    next_state = obs_channels_to_first(next_state)\n\nexperiences = (\n    states,\n    actions,\n    log_probs,\n    rewards,\n    dones,\n    values,\n    next_state,\n    next_done,\n)\n# Learn according to agent's RL algorithm\nagent.learn(experiences)\n\nagent.steps[-1] += steps\npop_episode_scores.append(completed_episode_scores)\n```\n\n----------------------------------------\n\nTITLE: Defining HPO and Mutation Parameters for GRPO Algorithm\nDESCRIPTION: Sets up hyperparameters for GRPO algorithm and mutation parameters that control the evolutionary search. Defines search spaces for learning rate, beta values, and group size with specified upper and lower limits.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMUTATION_PARAMS = {\n        \"NO_MUT\": 0.1,\n        \"RL_HP_MUT\": 0.6,\n        \"MUT_SD\": 0.1,\n        \"RAND_SEED\": 42,\n        \"MIN_LR\": 0.0000001,\n        \"MAX_LR\": 0.00001,\n        \"MIN_BETA\": 0.0001,\n        \"MAX_BETA\": 0.01,\n        \"MIN_GROUP_SIZE\": 4,\n        \"MAX_GROUP_SIZE\": 12\n    }\n\nINIT_HP = {\n    \"ALGO\": \"GRPO\",\n    \"BATCH_SIZE\": 1,\n    \"REDUCE_MEMORY_PEAK\": True,\n    \"BETA\": 0.001,\n    \"LR\": 0.000005,\n    \"CLIP_COEF\": 0.2,\n    \"MAX_GRAD_NORM\": 0.1,\n    \"UPDATE_EPOCHS\": 1,\n    \"GROUP_SIZE\": 8,\n    \"TEMPERATURE\": 0.9,\n    \"CALC_POSITION_EMBEDDINGS\": True,\n    \"MIN_OUTPUT_TOKENS\": None,\n    \"MAX_OUTPUT_TOKENS\": 1024,\n    \"COSINE_lR_SCHEDULER\": None,\n    \"TOURN_SIZE\": 2,\n    \"ELITISM\": True,\n    \"POP_SIZE\": 4,\n    \"EVAL_LOOP\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Opponent Pool for Self-Play\nDESCRIPTION: Initializes a pool of opponent agents for self-play training, allowing for varied and increasingly difficult opponents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nif LESSON[\"opponent\"] == \"self\":\n   # Create initial pool of opponents\n   opponent_pool = deque(maxlen=LESSON[\"opponent_pool_size\"])\n   for _ in range(LESSON[\"opponent_pool_size\"]):\n         opp = copy.deepcopy(pop[0])\n         opp.actor.load_state_dict(pop[0].actor.state_dict())\n         opp.actor.eval()\n         opponent_pool.append(opp)\n```\n\n----------------------------------------\n\nTITLE: Custom Training Loop for TD3 Agent Population in Python\nDESCRIPTION: Implements a custom training loop for a population of TD3 agents. This loop handles environment interactions, experience collection, learning, evaluation, and population evolution through tournament selection and mutation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntotal_steps = 0\n\n# TRAINING LOOP\nprint(\"Training...\")\npbar = trange(INIT_HP[\"MAX_STEPS\"], unit=\"step\")\nwhile np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n    pop_episode_scores = []\n    for agent in pop:  # Loop through population\n        state, info = env.reset()  # Reset environment at start of episode\n        scores = np.zeros(num_envs)\n        completed_episode_scores = []\n        steps = 0\n\n        for idx_step in range(INIT_HP[\"EVO_STEPS\"] // num_envs):\n            # Swap channels if channels last is True\n            state = obs_channels_to_first(state) if INIT_HP[\"CHANNELS_LAST\"] else state\n\n            action = agent.get_action(state)  # Get next action from agent\n\n            # Act in environment\n            next_state, reward, terminated, truncated, info = env.step(action)\n            scores += np.array(reward)\n            steps += num_envs\n            total_steps += num_envs\n\n            # Collect scores for completed episodes\n            reset_noise_indices = []\n            for idx, (d, t) in enumerate(zip(terminated, truncated)):\n                if d or t:\n                    completed_episode_scores.append(scores[idx])\n                    agent.scores.append(scores[idx])\n                    scores[idx] = 0\n                    reset_noise_indices.append(idx)\n\n            # Reset action noise\n            agent.reset_action_noise(reset_noise_indices)\n\n            # Save experience to replay buffer\n            done = terminated or truncated\n            next_state = obs_channels_to_first(next_state) if INIT_HP[\"CHANNELS_LAST\"] else next_state\n            transition = Transition(\n                obs=state,\n                action=action,\n                reward=reward,\n                next_obs=next_state,\n                done=done,\n                batch_size=[num_envs]\n            )\n            transition = transition.to_tensordict()\n            memory.add(transition)\n\n            # Learn according to learning frequency\n            if memory.size > INIT_HP[\"LEARNING_DELAY\"] and len(memory) >= agent.batch_size:\n                for _ in range(num_envs // agent.learn_step):\n                    # Sample replay buffer\n                    experiences = memory.sample(agent.batch_size)\n                    # Learn according to agent's RL algorithm\n                    agent.learn(experiences)\n\n            state = next_state\n\n        pbar.update(INIT_HP[\"EVO_STEPS\"] // len(pop))\n        agent.steps[-1] += steps\n        pop_episode_scores.append(completed_episode_scores)\n\n    # Evaluate population\n    fitnesses = [\n        agent.test(\n            env,\n            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n            INIT_HP[\"MAX_STEPS\"]=INIT_HP[\"EVAL_STEPS\"],\n            loop=INIT_HP[\"EVAL_LOOP\"],\n        )\n        for agent in pop\n    ]\n    mean_scores = [\n        (\n            np.mean(episode_scores)\n            if len(episode_scores) > 0\n            else \"0 completed episodes\"\n        )\n        for episode_scores in pop_episode_scores\n    ]\n\n    print(f\"--- Global steps {total_steps} ---\")\n    print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n    print(f\"Scores: {mean_scores}\")\n    print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n    print(\n        f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n    )\n\n    # Tournament selection and population mutation\n    elite, pop = tournament.select(pop)\n    pop = mutations.mutation(pop)\n\n    # Update step counter\n    for agent in pop:\n        agent.steps.append(agent.steps[-1])\n\n# Save the trained algorithm\nsave_path = \"TD3_trained_agent.pt\"\nelite.save_checkpoint(save_path)\n\npbar.close()\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters for TD3 Algorithm and Mutation Parameters\nDESCRIPTION: Sets up two dictionaries: one for TD3 algorithm hyperparameters including learning rates, batch size, and training settings, and another for mutation parameters that control evolutionary hyperparameter optimization, including mutation probabilities and search space constraints.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Initial hyperparameters\nINIT_HP = {\n    \"ALGO\": \"TD3\",\n    \"POP_SIZE\": 4,  # Population size\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR_ACTOR\": 0.0001,  # Actor learning rate\n    \"LR_CRITIC\": 0.001,  # Critic learning rate\n    \"O_U_NOISE\": True,  # Ornstein-Uhlenbeck action noise\n    \"EXPL_NOISE\": 0.1,  # Action noise scale\n    \"MEAN_NOISE\": 0.0,  # Mean action noise\n    \"THETA\": 0.15,  # Rate of mean reversion in OU noise\n    \"DT\": 0.01,  # Timestep for OU noise\n    \"GAMMA\": 0.99,  # Discount factor\n    \"MEMORY_SIZE\": 100_000,  # Max memory buffer size\n    \"POLICY_FREQ\": 2,  # Policy network update frequency\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"TAU\": 0.005,  # For soft update of target parameters\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,  # Use with RGB states\n    \"EPISODES\": 1000,  # Number of episodes to train for\n    \"EVO_EPOCHS\": 20,  # Evolution frequency, i.e. evolve after every 20 episodes\n    \"TARGET_SCORE\": 200.0,  # Target score that will beat the environment\n    \"EVO_LOOP\": 3,  # Number of evaluation episodes\n    \"MAX_STEPS\": 500,  # Maximum number of steps an agent takes in an environment\n    \"LEARNING_DELAY\": 1000,  # Steps before starting learning\n    \"EVO_STEPS\": 10000,  # Evolution frequency\n    \"EVAL_STEPS\": None,  # Number of evaluation steps per episode\n    \"EVAL_LOOP\": 1,  # Number of evaluation episodes\n    \"TOURN_SIZE\": 2,  # Tournament size\n    \"ELITISM\": True,  # Elitism in tournament selection\n}\n\n# Mutation parameters\nMUT_P = {\n    # Mutation probabilities\n    \"NO_MUT\": 0.4,  # No mutation\n    \"ARCH_MUT\": 0.2,  # Architecture mutation\n    \"NEW_LAYER\": 0.2,  # New layer mutation\n    \"PARAMS_MUT\": 0.2,  # Network parameters mutation\n    \"ACT_MUT\": 0.2,  # Activation layer mutation\n    \"RL_HP_MUT\": 0.2,  # Learning HP mutation\n    \"MUT_SD\": 0.1,  # Mutation strength\n    \"RAND_SEED\": 42,  # Random seed\n    # Define max and min limits for mutating RL hyperparams\n    \"MIN_LR\": 0.0001,\n    \"MAX_LR\": 0.01,\n    \"MIN_BATCH_SIZE\": 8,\n    \"MAX_BATCH_SIZE\": 1024,\n    \"MIN_LEARN_STEP\": 1,\n    \"MAX_LEARN_STEP\": 16,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Reward Functions for LLM Reasoning\nDESCRIPTION: Defines reward functions for accuracy and format, which are used to evaluate the LLM's reasoning outputs. These functions check for correct answers and proper formatting of the model's responses.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef format_reward_func(completions, target, **kwargs):\n    rewards = []\n\n    for completion, gt in zip(completions, target):\n        try:\n            # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n            completion = \"<think>\" + completion\n            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n            match = re.search(regex, completion, re.DOTALL)\n            if match is None or len(match.groups()) != 2:\n                rewards.append(0.0)\n            else:\n                rewards.append(1.0)\n        except Exception:\n            rewards.append(0.0)\n    return rewards\n\n\ndef equation_reward_func(completions, target, nums, **kwargs):\n    rewards = []\n\n    for completion, gt, numbers in zip(completions, target, nums):\n        try:\n            # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n            completion = \"<think>\" + completion\n            answer_tags = re.findall(r\"<answer>([\\s\\S]*?)<\\/answer>\", completion)\n\n            if len(answer_tags) != 1:\n                rewards.append(0.0)\n                continue\n\n            equation = answer_tags[0].strip()\n            used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n\n            if sorted(used_numbers) != sorted(numbers):\n                print(f\"Numbers mismatch: {used_numbers} vs {numbers}\")\n                rewards.append(0.0)\n                continue\n\n            allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n            if not re.match(allowed_pattern, equation):\n                print(f\"Equation format invalid: {equation}\")\n                rewards.append(0.0)\n                continue\n\n            result = eval(equation, {\"__builtins__\": None}, {})\n\n            if abs(float(result) - float(gt)) < 1e-5:\n                rewards.append(1.0)\n            else:\n                print(f\"Result {result} doesn't match target {gt}\")\n                rewards.append(0.0)\n        except Exception as e:\n            print(f\"Equation error: {e}\")\n            rewards.append(0.0)\n    return rewards\n\n\ndef combined_rewards(completion, solution, prompt):\n    reward = (\n        equation_reward_func([completion], [solution], [prompt])[0]\n        + format_reward_func([completion], [solution])[0]\n    )\n\n    print(\n        f\"\"\"\n    ============================================, \\n\n    Completion: {completion}, \\n\n    Numbers: {prompt}, \\n\n    Gospel Answer: {solution.item()} \\n\n    Reward: {reward}\n    \"\"\"\n    )\n    # Save successful countdown  comletions\n    if reward == 2.0:\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Loop Variables in Python\nDESCRIPTION: Sets up initial lists to store states, actions, log probabilities, rewards, done flags, and values for the training loop. It also initializes the 'done' array for multiple environments.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nstates = []\nactions = []\nlog_probs = []\nrewards = []\ndones = []\nvalues = []\n\ndone = np.zeros(num_envs)\n\nlearn_steps = 0\n```\n\n----------------------------------------\n\nTITLE: Training Rainbow DQN Agent Using AgileRL's Off-Policy Training Function\nDESCRIPTION: This code snippet demonstrates how to use AgileRL's train_off_policy function to train a single Rainbow DQN agent on the Cartpole environment. It sets up the training parameters, including the environment, algorithm, population (single agent), memory buffers, and various hyperparameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Define parameters per and n_step\ntrained_pop, pop_fitnesses = train_off_policy(\n    env=env,\n    env_name=\"CartPole-v1\",\n    algo=\"RainbowDQN\",\n    pop=[rainbow_dqn],\n    memory=memory,\n    n_step_memory=n_step_memory,\n    INIT_HP=INIT_HP,\n    swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n    max_steps=INIT_HP[\"MAX_STEPS\"],\n    evo_steps=INIT_HP[\"EVO_STEPS\"],\n    eval_steps=INIT_HP[\"EVAL_STEPS\"],\n    eval_loop=INIT_HP[\"EVAL_LOOP\"],\n    learning_delay=INIT_HP[\"LEARNING_DELAY\"],\n    target=INIT_HP[\"TARGET_SCORE\"],\n    n_step=True,\n    per=True,\n    tournament=None,\n    mutation=None,\n    wb=False,  # Boolean flag to record run with Weights & Biases\n    checkpoint=INIT_HP[\"MAX_STEPS\"],\n    checkpoint_path=\"RainbowDQN.pt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Tournament Selection and Population Mutation in Python\nDESCRIPTION: Performs tournament selection to choose elite agents and applies mutations to the population. It updates the step counter for each agent in the new population.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Tournament selection and population mutation\nelite, pop = tournament.select(pop)\npop = mutations.mutation(pop)\n\n# Update step counter\nfor agent in pop:\n    agent.steps.append(agent.steps[-1])\n\npbar.close()\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Initializing Tournament Selection in Python\nDESCRIPTION: Demonstrates how to initialize tournament selection for evolutionary agent selection. Configuration includes tournament size, elitism flag, population size, and evaluation step parameter.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/hpo/tournament.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.hpo.tournament import TournamentSelection\n\ntournament = TournamentSelection(tournament_size=2, # Tournament selection size\n                                    elitism=True,      # Elitism in tournament selection\n                                    population_size=6, # Population size\n                                    evo_step=1)        # Evaluate using last N fitness scores\n```\n\n----------------------------------------\n\nTITLE: Initializing IPPO with Homogeneous Agents in Python\nDESCRIPTION: Example of initializing IPPO agent with homogeneous agents. Agent IDs with matching prefixes before the last underscore are treated as homogeneous and share actor/critic networks.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nenv.agent_ids = [\"bob_0\", \"bob_1\", \"fred_0\"]\n\nagent = IPPO(\n  observation_spaces=env.observation_spaces,\n  action_spaces=env.action_spaces,\n  agent_ids=env.agent_ids\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Population of RL Agents for Evolutionary HPO\nDESCRIPTION: Code for initializing a population of reinforcement learning agents with configurable network architecture and hyperparameters for evolutionary optimization. It sets up the environment, configures hyperparameter bounds for mutation, and creates the agent population.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/off_policy/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\nfrom agilerl.utils.utils import (\n    create_population,\n    make_vect_envs,\n    observation_space_channels_to_first\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNET_CONFIG = {\n    \"encoder_config\": {\n        \"hidden_size\": [32, 32] # Encoder hidden size\n        },\n    \"head_config\": {\n        \"hidden_size\": [32, 32]  # Head hidden size\n    }\n}\n\nINIT_HP = {\n    \"DOUBLE\": True,  # Use double Q-learning\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"GAMMA\": 0.99,  # Discount factor\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"TAU\": 1e-3,  # For soft update of target network parameters\n    \"CHANNELS_LAST\": False,  # Swap image channels dimension last to first [H, W, C] -> [C, H, W]\n    \"POP_SIZE\": 4,  # Population size\n}\n\n# Initialize vectorized environments\nnum_envs = 16\nenv = make_vect_envs(\"LunarLander-v2\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\n# RL hyperparameter configuration for mutations\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=1e-4, max=1e-2),\n    batch_size = RLParameter(min=8, max=64, dtype=int),\n    learn_step = RLParameter(\n        min=1, max=120, dtype=int, grow_factor=1.5, shrink_factor=0.75\n        )\n)\n\npop = create_population(\n    algo=\"DQN\",  # Algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    hp_config=hp_config,  # Hyperparameter configuration\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,  # Number of vectorized envs\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Experience Replay Buffer for Off-Policy Learning\nDESCRIPTION: Code for initializing a replay buffer to store experiences collected by agents. The replay buffer is a key component for off-policy algorithms as it allows sharing experiences between different agents in the population.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/off_policy/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing AgileRL Training Loop\nDESCRIPTION: Executes the main training loop for the population of agents, including environment interaction, action selection, experience collection, and learning. It also handles epsilon decay for exploration and tracks episode scores.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/off_policy/index.rst#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# TRAINING LOOP\nprint(\"Training...\")\npbar = trange(max_steps, unit=\"step\")\nwhile np.less([agent.steps[-1] for agent in pop], max_steps).all():\n    pop_episode_scores = []\n    for agent in pop:  # Loop through population\n        state, info = env.reset()  # Reset environment at start of episode\n        scores = np.zeros(num_envs)\n        completed_episode_scores = []\n        steps = 0\n        epsilon = eps_start\n\n        for idx_step in range(evo_steps // num_envs):\n            if INIT_HP[\"CHANNELS_LAST\"]:\n                state = obs_channels_to_first(state)\n\n            action = agent.get_action(state, epsilon)  # Get next action from agent\n            epsilon = max(\n                eps_end, epsilon * eps_decay\n            )  # Decay epsilon for exploration\n\n            # Act in environment\n            next_state, reward, terminated, truncated, info = env.step(action)\n            scores += np.array(reward)\n            steps += num_envs\n            total_steps += num_envs\n\n            # Collect scores for completed episodes\n            for idx, (d, t) in enumerate(zip(terminated, truncated)):\n                if d or t:\n                    completed_episode_scores.append(scores[idx])\n                    agent.scores.append(scores[idx])\n                    scores[idx] = 0\n\n            next_state = obs_channels_to_first(next_state) if INIT_HP[\"CHANNELS_LAST\"] else next_state\n\n            # Wrap transition as TensorDict\n            transition = Transition(\n                obs=state,\n                action=action,\n                reward=reward,\n                next_obs=next_state,\n                done=terminated,\n                batch_size=[num_envs]\n            )\n            transition = transition.to_tensordict()\n\n            # Save experience to replay buffer\n            memory.add(transition)\n\n            # Learn according to learning frequency\n            if memory.size > learning_delay and len(memory) >= agent.batch_size:\n                for _ in range(num_envs // agent.learn_step):\n                    experiences = memory.sample(\n                        agent.batch_size\n                    )  # Sample replay buffer\n                    agent.learn(\n                        experiences\n                    )  # Learn according to agent's RL algorithm\n\n            state = next_state\n\n        pbar.update(evo_steps // len(pop))\n        agent.steps[-1] += steps\n        pop_episode_scores.append(completed_episode_scores)\n\n    # Reset epsilon start to latest decayed value for next round of population training\n    eps_start = epsilon\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Vector Observations\nDESCRIPTION: Configuration setup for DDPG neural network with discrete/vector observations, defining encoder and head architectures.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ddpg.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {\n          'hidden_size': [32, 32] # Network encoder hidden size\n      },\n      \"head_config\": {\n          'hidden_size': [32] # Network head hidden size\n      }\n  }\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Creating Agent Population for Bandit Learning\nDESCRIPTION: This snippet demonstrates how to set up the learning environment using a UCI dataset, create a population of bandit agents, and configure hyperparameters for evolutionary optimization. It uses the NeuralUCB algorithm and sets up mutation configurations for reinforcement learning parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/bandits/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.utils.utils import create_population\nfrom agilerl.wrappers.learning import BanditEnv\nimport torch\nfrom ucimlrepo import fetch_ucirepo\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNET_CONFIG = {\n    \"encoder_config\": {\"hidden_size\": [128]},  # Encoder hidden size\n}\n\nINIT_HP = {\n    \"BATCH_SIZE\": 64,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"GAMMA\": 1.0,  # Scaling factor\n    \"LAMBDA\": 1.0,  # Regularization factor\n    \"REG\": 0.000625,  # Loss regularization factor\n    \"LEARN_STEP\": 2,  # Learning frequency\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n    \"POP_SIZE\": 4,  # Population size\n}\n\n# Fetch data  https://archive.ics.uci.edu/\niris = fetch_ucirepo(id=53)\nfeatures = iris.data.features\ntargets = iris.data.targets\n\nenv = BanditEnv(features, targets)  # Create environment\ncontext_dim = env.context_dim\naction_dim = env.arms\n\n# Mutation config for RL hyperparameters\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=6.25e-5, max=1e-2),\n    batch_size = RLParameter(min=8, max=512, dtype=int),\n    learn_step = RLParameter(min=1, max=10, dtype=int, grow_factor=1.5, shrink_factor=0.75)\n)\n\nobs_space = spaces.Box(low=features.values.min(), high=features.values.max())\naction_space = spaces.Discrete(action_dim)\npop = create_population(\n    algo=\"NeuralUCB\",  # Algorithm\n    observation_space=obs_space,  # Observation space\n    action_space=action_space,  # Action space\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    hp_config=hp_config,  # Hyperparameter configuration\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Offline Dataset with CQN in AgileRL\nDESCRIPTION: Example demonstrating how to create a CQN agent, load an offline dataset from an H5 file, populate a replay buffer with transitions, and train the agent with the collected experience.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/cql.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport h5py\n\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.components.data import Transition\nfrom agilerl.algorithms.cqn import CQN\nfrom agilerl.utils.algo_utils import obs_channels_to_first\n\n# Create environment and Experience Replay Buffer, and load dataset\nenv = gym.make('CartPole-v1')\nobservation_space = env.observation_space\naction_space = env.action_space\n\nmemory = ReplayBuffer(max_size=10000)\ndataset = h5py.File('data/cartpole/cartpole_random_v1.1.0.h5', 'r')  # Load dataset\n\n# Save transitions to replay buffer\ndataset_length = dataset['rewards'].shape[0]\nfor i in range(dataset_length-1):\n    state = dataset['observations'][i]\n    next_state = dataset['observations'][i+1]\n    if channels_last:\n        state = obs_channels_to_first(state)\n        next_state = obs_channels_to_first(next_state)\n\n    action = dataset['actions'][i]\n    reward = dataset['rewards'][i]\n    done = bool(dataset['terminals'][i])\n    transition = Transition(\n        obs=state,\n        action=action,\n        reward=reward,\n        next_obs=next_state,\n        done=done,\n    )\n    transition = transition.unsqueeze(0)\n    transition.batch_size = [1]\n    transition = transition.to_tensordict()\n    memory.add(transition)\n\nagent = CQN(observation_space=observation_space, action_space=action_space)   # Create DQN agent\n\nstate = env.reset()[0]  # Reset environment at start of episode\nwhile True:\n    experiences = memory.sample(agent.batch_size)   # Sample replay buffer\n    # Learn according to agent's RL algorithm\n    agent.learn(experiences)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Discrete/Vector Observations in Rainbow DQN\nDESCRIPTION: Code snippet demonstrating how to configure the neural network architecture for discrete or vector observations in Rainbow DQN using a dictionary of parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn_rainbow.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for DQN Connect Four\nDESCRIPTION: Core imports needed for DQN training implementation including AgileRL components, PettingZoo environment, and utility libraries.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport copy\nimport os\nimport random\nfrom collections import deque\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nimport wandb\nimport yaml\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import create_population\nfrom tqdm import tqdm, trange\n\nfrom pettingzoo.classic import connect_four_v3\n```\n\n----------------------------------------\n\nTITLE: Setting Up Experience Replay for Rainbow DQN\nDESCRIPTION: This code snippet initializes the prioritized experience replay buffer and multi-step replay buffer for the Rainbow DQN agent. These specialized buffers are used to store and sample experiences for more efficient learning.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmemory = PrioritizedReplayBuffer(\n    max_size=INIT_HP[\"MEMORY_SIZE\"],\n    alpha=INIT_HP[\"ALPHA\"],\n    device=device,\n)\nn_step_memory = MultiStepReplayBuffer(\n    max_size=INIT_HP[\"MEMORY_SIZE\"],\n    n_step=INIT_HP[\"N_STEP\"],\n    gamma=INIT_HP[\"GAMMA\"],\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Training NeuralUCB Agent on Iris Dataset\nDESCRIPTION: Complete implementation of training a NeuralUCB agent on the Iris dataset. Includes data loading from UCI repository, environment setup, agent initialization, and training loop with visualization of results.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/bandits/agilerl_neural_ucb_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.wrappers.learning import BanditEnv\nfrom agilerl.algorithms.bandits_world import NeuralUCB\nfrom ucimlrepo import fetch_ucirepo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed\nnp.random.seed(42)\n\n# Fetch data from UCI repository\niris = fetch_ucirepo(id=53)\n\n# Create BanditEnv from UCI dataset\nenv = BanditEnv(\n    X=iris.data.features.to_numpy(),\n    y=iris.data.targets.to_numpy(),\n    continuous_actions=False,\n    num_actions=3,\n    sample_all_actions=True\n)\n\n# Initialize NeuralUCB agent\nagent = NeuralUCB(\n    context_size=4,  # The length of observed state/context vector\n    num_actions=3,  # The number of possible actions to take\n    hidden_sizes=[50],  # Architecture of the neural network used\n    lambda_=1,  # Regularization parameter\n    nu=1,  # Controls the exploration rate\n    training_freq=20,  # Frequency of neural network training\n    batch_size=64,  # Training batch size\n    training_epochs=100,  # Training epochs for neural network\n    use_cuda=False,  # Using CUDA for training\n)\n\n# Define parameters\nEPOCHS = 200\n\n# Store metrics\nregret = np.zeros(EPOCHS)\nrewards = np.zeros(EPOCHS)\n\n# Training loop\nfor epoch in range(EPOCHS):\n    print(f\"Starting Epoch {epoch+1}\")\n\n    # Reset environment\n    state = env.reset()\n    reward = 0\n    done = False\n\n    # Play episode\n    while not done:\n        # Get environment action\n        action = agent.getAction(state[0])\n\n        # Step environment\n        next_state, reward, done, _ = env.step(action)\n\n        # Learn from experience\n        agent.learn(state[0], action, reward)\n\n        # Update state\n        state = next_state\n\n    # Store reward and regret\n    rewards[epoch] = reward\n    regret[epoch] = env.episode_regret\n\n# Set up figure and axes\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Plot regret\nax1.plot(np.arange(len(regret)), np.cumsum(regret))\nax1.set_xlabel(\"Episode\")\nax1.set_ylabel(\"Cumulative Regret\")\nax1.set_title(\"NeuralUCB - Iris\")\n\n# Plot reward\nax2.plot(np.arange(len(rewards)), rewards)\nax2.set_xlabel(\"Episode\")\nax2.set_ylabel(\"Reward\")\nax2.set_title(\"NeuralUCB - Iris\")\n\n# Adjust layout and display\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Base Model and Dataset for LLM Reasoning\nDESCRIPTION: Sets up the pretrained model (Qwen2.5-3B) and dataset (Countdown-Tasks-3to4) for the reasoning task. Includes functions to create the model with LoRA configuration and prepare the dataset.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMODEL_PATH = \"Qwen/Qwen2.5-3B\"\nDATASET = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n\ndef create_model(pretrained_model_name_or_path):\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    )\n    peft_config = LoraConfig(\n        r=32,\n        lora_alpha=32,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"down_proj\",\n            \"gate_proj\",\n        ],\n        task_type=\"CAUSAL_LM\",\n        lora_dropout=0.05,\n    )\n    model = get_peft_model(model, peft_config)\n    return model\n\ndef make_dataset(dataset_name: str) -> Tuple[Dataset, Dataset]:\n    raw_dataset = (\n        load_dataset(DATASET, split=\"train\").shuffle(seed=42).select(range(50000))\n    )\n    raw_dataset = raw_dataset.rename_column(\"target\", \"answer\")\n    raw_dataset = raw_dataset.rename_column(\"nums\", \"question\")\n    train_test_split = raw_dataset.train_test_split(test_size=0.1)\n    train_dataset = train_test_split[\"train\"]\n    test_dataset = train_test_split[\"test\"]\n    return train_dataset, test_dataset\n\n# Instantiate the model and the associated tokenizer\nmodel = create_model(pretrained_model_name_or_path=MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ntokenizer.pad_token = tokenizer.eos_token\ntrain_dataset, test_dataset = make_dataset(DATASET)\n```\n\n----------------------------------------\n\nTITLE: Configuring CNN and Network Architecture for IPPO\nDESCRIPTION: Defines the CNN and network configuration for handling dictionary/tuple observations with image, discrete and vector inputs. Includes CNN parameters and encoder configuration with support for vector space processing.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Neural Network Configuration for Vector/Discrete Observations\nDESCRIPTION: Configuration example for network architecture with vector or discrete observations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/maddpg.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n        \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n        \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n    }\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training PPO Agent with Gymnasium Environment\nDESCRIPTION: Complete example of setting up a PPO agent with AgileRL, creating a vectorized environment, and running a training loop including action selection and experience collection.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ppo.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nimport numpy as np\n\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.utils.utils import make_vect_envs, observation_space_channels_first\nfrom agilerl.algorithms.ppo import PPO\n\n# Create environment\nnum_envs = 1\nenv = make_vect_envs('LunarLanderContinuous-v3', num_envs=num_envs)\nobservation_space = env.observation_space\naction_space = env.action_space\n\nchannels_last = False # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n\nif channels_last:\n    observation_space = observation_space_channels_first(observation_space)\n\nagent = PPO(observation_space, action_space)   # Create PPO agent\n\nwhile True:\n    states = []\n    actions = []\n    log_probs = []\n    rewards = []\n    dones = []\n    values = []\n\n    done = np.zeros(num_envs)\n\n    for step in range(agent.learn_step):\n        if channels_last:\n            state = obs_channels_to_first(state)\n\n        # Get next action from agent\n        action, log_prob, _, value = agent.get_action(state)\n\n        # Clip to action space\n        if isinstance(agent.action_space, spaces.Box):\n            if agent.actor.squash_output:\n                clipped_action = agent.actor.scale_action(action)\n            else:\n                clipped_action = np.clip(action, agent.action_space.low, agent.action_space.high)\n        else:\n            clipped_action = action\n\n        next_state, reward, term, trunc, _ = env.step(clipped_action)  # Act in environment\n        next_done = np.logical_or(term, trunc).astype(np.int8)\n\n        states.append(state)\n        actions.append(action)\n        log_probs.append(log_prob)\n        rewards.append(reward)\n        dones.append(done)\n        values.append(value)\n\n        state = next_state\n        done = next_done\n\n    experiences = (\n        states,\n        actions,\n        log_probs,\n        rewards,\n        dones,\n        values,\n        next_state,\n        next_done,\n    )\n    # Learn according to agent's RL algorithm\n    agent.learn(experiences)\n```\n\n----------------------------------------\n\nTITLE: Implementing Landing Skill for LunarLander\nDESCRIPTION: Defines a LandingSkill class to teach the agent to land safely at a target location. Implements reward shaping to encourage precise landing behavior.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LandingSkill(Skill):\n   def __init__(self, env):\n      super().__init__(env)\n\n      self.x_landing = 0\n      self.y_landing = 0\n      self.theta_level = 0\n\n   def skill_reward(self, observation, reward, terminated, truncated, info):\n      if terminated or truncated:\n            return observation, reward, terminated, truncated, info\n\n      x, y, theta = observation[0], observation[1], observation[4]\n      reward, terminated, truncated = 1.0, 0, 0\n\n      # Minimise x distance to landing zone\n      reward -= (abs(self.x_landing - x)) ** 2\n      # Minimise y distance to landing zone\n      reward -= (abs(self.y_landing - y)) ** 2\n      # Minimise tilt angle\n      reward -= abs(self.theta_level - theta)\n\n      return observation, reward, terminated, truncated, info\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Image Observations\nDESCRIPTION: Configuration for NeuralUCB's network architecture when using image observations. Specifies CNN parameters like channels, kernel size and stride along with head hidden layer sizes.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ucb.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Initializing Tournament Selection and Mutations in Python\nDESCRIPTION: This snippet sets up the tournament selection and mutation parameters for the evolutionary algorithm. It defines the population size, elitism, and various mutation probabilities.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/offline_training/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nelitism=True,  # Elitism in tournament selection\npopulation_size=INIT_HP[\"POP_SIZE\"],  # Population size\neval_loop=1,  # Evaluate using last N fitness scores\n)\n\nmutations = Mutations(\n    no_mutation=0.4,  # No mutation\n    architecture=0.2,  # Architecture mutation\n    new_layer_prob=0.2,  # New layer mutation\n    parameters=0.2,  # Network parameters mutation\n    activation=0,  # Activation layer mutation\n    rl_hp=0.2,  # Learning HP mutation\n    mutation_sd=0.1,  # Mutation strength  # Network architecture\n    rand_seed=1,  # Random seed\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Population of PPO Agents for Evolutionary Training\nDESCRIPTION: Creates a population of PPO agents with a defined network architecture (an MLP with two hidden layers of 64 nodes each). Sets up hyperparameter configuration for mutation during training, defining search ranges for learning rate and batch size.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set-up the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Define the network configuration of a simple mlp with two hidden layers, each with 64 nodes\nnet_config = {\"head_config\": {\"hidden_size\": [64, 64]}}\n\n# RL hyperparameters configuration for mutation during training\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=1e-4, max=1e-2),\n    batch_size = RLParameter(\n        min=8, max=1024, dtype=int\n        )\n)\n\n# Define a population\npop = create_population(\n    algo=\"PPO\",  # RL algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=net_config,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameter\n    hp_config=hp_config,  # RL hyperparameter configuration\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Episode Tracking and Self-Play Opponent Pool Updates\nDESCRIPTION: This snippet tracks episode statistics and updates the opponent pool in self-play mode. It implements a mechanism to periodically upgrade the opponent by adding elite agents to the opponent pool based on tournament selection.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntotal_steps += idx_step + 1\ntotal_episodes += 1\nturns_per_episode.append(turns)\n# Save the total episode reward\nagent.scores.append(score)\n\nif LESSON[\"opponent\"] == \"self\":\n   if (total_episodes % LESSON[\"opponent_upgrade\"] == 0) and (\n         (idx_epi + 1) > evo_epochs\n   ):\n         elite_opp, _, _ = tournament._elitism(pop)\n         elite_opp.actor.eval()\n         opponent_pool.append(elite_opp)\n         opp_update_counter += 1\n\n# Update epsilon for exploration\nepsilon = max(eps_end, epsilon * eps_decay)\n```\n\n----------------------------------------\n\nTITLE: Implementing Policy Q-Learning Test Function for Multi-Agent Environments in Python\nDESCRIPTION: A utility function to verify policy-based Q-learning algorithms using probe environments. It trains agents in controlled environments to evaluate whether they correctly learn optimal policies and converge as expected.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef check_policy_q_learning_with_probe_env(\n    agent_class,\n    env_class,\n    env_args,\n    agent_args,\n    target_score=0.9,\n    max_episodes=100,\n    verbose=True,\n):\n    \"\"\"Test if an agent can learn a multi-agent policy using Q-learning.\n\n    This function creates a probe environment and agents, then trains them\n    to see if they can achieve a target score within a maximum number of episodes.\n\n    Args:\n        agent_class (class): The agent class to test\n        env_class (class): The environment class to use\n        env_args (dict): Arguments to pass to the environment constructor\n        agent_args (dict): Arguments to pass to the agent constructor\n        target_score (float, optional): Target average reward to achieve. Defaults to 0.9.\n        max_episodes (int, optional): Maximum number of episodes to train. Defaults to 100.\n        verbose (bool, optional): Whether to print progress. Defaults to True.\n\n    Returns:\n        bool: True if the agent achieved the target score, False otherwise\n    \"\"\"\n    # Create the environment\n    env = env_class(**env_args)\n\n    # Create the agent\n    n_agents = env_args.get(\"n_agents\")\n    agent = agent_class(**agent_args)\n\n    best_score = -float(\"inf\")\n    for episode in range(max_episodes):\n        states = env.reset()\n\n        episode_rewards = [0.0] * n_agents\n        dones = [False] * n_agents\n\n        while not all(dones):\n            # Agent selects actions\n            actions = agent.getAction(states)\n\n            # Execute actions in the environment\n            next_states, rewards, dones, _ = env.step(actions)\n\n            # Update the agent with this experience\n            agent.learn(states, actions, rewards, next_states, dones)\n\n            # Update episode rewards\n            for i in range(n_agents):\n                episode_rewards[i] += rewards[i]\n\n            # Update current states\n            states = next_states\n\n        # Calculate average reward across all agents\n        avg_reward = sum(episode_rewards) / n_agents\n        best_score = max(best_score, avg_reward)\n\n        if verbose and (episode + 1) % 10 == 0:\n            print(\n                f\"Episode {episode+1}/{max_episodes} - Avg Reward: {avg_reward:.3f}, Best: {best_score:.3f}\"\n            )\n\n        # Check if we've reached the target score\n        if avg_reward >= target_score:\n            if verbose:\n                print(\n                    f\"Environment solved in {episode+1} episodes! Avg Reward: {avg_reward:.3f}\"\n                )\n            return True\n\n    if verbose:\n        print(\n            f\"Failed to solve environment in {max_episodes} episodes. Best Avg Reward: {best_score:.3f}\"\n        )\n    return False\n```\n\n----------------------------------------\n\nTITLE: Implementing Test Loop for PPO Agent Inference in Python\nDESCRIPTION: This code demonstrates a test loop for running inference with a trained PPO agent, including environment interaction and reward collection.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntest_env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\nrewards = []\nframes = []\ntesting_eps = 7\nmax_testing_steps = 1000\nwith torch.no_grad():\n    for ep in range(testing_eps):\n        state = test_env.reset()[0]  # Reset environment at start of episode\n        score = 0\n\n        for step in range(max_testing_steps):\n            # If your state is an RGB image\n            if INIT_HP[\"CHANNELS_LAST\"]:\n                state = obs_channels_to_first(state)\n\n            # Get next action from agent\n            action, *_ = ppo.get_action(state)\n            action = action.squeeze()\n\n            # Save the frame for this step and append to frames list\n            frame = test_env.render()\n            frames.append(frame)\n\n            # Take the action in the environment\n            state, reward, terminated, truncated, _ = test_env.step(action)\n\n            # Collect the score\n            score += reward\n\n            # Break if environment 0 is done or truncated\n            if terminated or truncated:\n                break\n\n        # Collect and print episodic reward\n        rewards.append(score)\n        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n        print(\"Episodic Reward: \", rewards[-1])\n\n    test_env.close()\n```\n\n----------------------------------------\n\nTITLE: Setting Network Architecture Configuration in Python\nDESCRIPTION: Defines the neural network architecture configuration including latent dimensions and layer sizes for the encoder and network head.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    'latent_dim': 16\n\n    'encoder_config': {\n      'hidden_size': [32]     # Observation encoder configuration\n    }\n\n    'head_config': {\n      'hidden_size': [32]     # Network head configuration\n    }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing DuelingDistributionalMLP for Rainbow DQN in Python\nDESCRIPTION: A custom implementation of a multi-layer perceptron that calculates state-action values through separate advantage and value networks, outputting a distribution of values for both networks. This class inherits from EvolvableMLP and is designed for use in the Rainbow DQN algorithm, featuring noisy linear layers and support for architecture mutation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_rainbow_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass DuelingDistributionalMLP(EvolvableMLP):\n    \"\"\"A multi-layer perceptron network that calculates state-action values through\n    the use of separate advantage and value networks. It outputs a distribution of values\n    for both of these networks. Used in the Rainbow DQN algorithm.\n\n    :param num_inputs: Number of input features.\n    :type num_inputs: int\n    :param num_outputs: Number of output features.\n    :type num_outputs: int\n    :param hidden_size: List of hidden layer sizes.\n    :type hidden_size: List[int]\n    :param num_atoms: Number of atoms in the distribution.\n    :type num_atoms: int\n    :param support: Support of the distribution.\n    :type support: torch.Tensor\n    :param noise_std: Standard deviation of the noise. Defaults to 0.5.\n    :type noise_std: float, optional\n    :param activation: Activation layer, defaults to 'ReLU'\n    :type activation: str, optional\n    :param output_activation: Output activation layer, defaults to None\n    :type output_activation: str, optional\n    :param min_hidden_layers: Minimum number of hidden layers the network will shrink down to, defaults to 1\n    :type min_hidden_layers: int, optional\n    :param max_hidden_layers: Maximum number of hidden layers the network will expand to, defaults to 3\n    :type max_hidden_layers: int, optional\n    :param min_mlp_nodes: Minimum number of nodes a layer can have within the network, defaults to 64\n    :type min_mlp_nodes: int, optional\n    :param max_mlp_nodes: Maximum number of nodes a layer can have within the network, defaults to 500\n    :type max_mlp_nodes: int, optional\n    :param layer_norm: Normalization between layers, defaults to True\n    :type layer_norm: bool, optional\n    :param output_vanish: Vanish output by multiplying by 0.1, defaults to True\n    :type output_vanish: bool, optional\n    :param init_layers: Initialise network layers, defaults to True\n    :type init_layers: bool, optional\n    :param new_gelu: Use new GELU activation function, defaults to False\n    :type new_gelu: bool, optional\n    :param device: Device for accelerated computing, 'cpu' or 'cuda', defaults to 'cpu'\n    :type device: str, optional\n    \"\"\"\n\n    def __init__(\n        self,\n        num_inputs: int,\n        num_outputs: int,\n        hidden_size: List[int],\n        num_atoms: int,\n        support: torch.Tensor,\n        noise_std: float = 0.5,\n        activation: str = \"ReLU\",\n        output_activation: str = None,\n        min_hidden_layers: int = 1,\n        max_hidden_layers: int = 3,\n        min_mlp_nodes: int = 64,\n        max_mlp_nodes: int = 500,\n        new_gelu: bool = False,\n        device: str = \"cpu\",\n    ) -> None:\n\n        super().__init__(\n            num_inputs,\n            num_atoms,\n            hidden_size,\n            activation,\n            output_activation,\n            min_hidden_layers,\n            max_hidden_layers,\n            min_mlp_nodes,\n            max_mlp_nodes,\n            layer_norm=True,\n            output_vanish=True,\n            init_layers=False,\n            noisy=True,\n            noise_std=noise_std,\n            new_gelu=new_gelu,\n            device=device,\n            name=\"value\",\n        )\n\n        self.num_atoms = num_atoms\n        self.num_actions = num_outputs\n        self.support = support\n\n        self.advantage_net = create_mlp(\n            input_size=num_inputs,\n            output_size=num_outputs * num_atoms,\n            hidden_size=self.hidden_size,\n            output_vanish=self.output_vanish,\n            output_activation=self.output_activation,\n            noisy=self.noisy,\n            init_layers=self.init_layers,\n            layer_norm=self.layer_norm,\n            activation=self.activation,\n            noise_std=self.noise_std,\n            device=self.device,\n            new_gelu=self.new_gelu,\n            name=\"advantage\",\n        )\n\n    @property\n    def net_config(self) -> Dict[str, Any]:\n        net_config = super().net_config.copy()\n        net_config.pop(\"num_atoms\")\n        net_config.pop(\"support\")\n        return net_config\n\n    def forward(\n        self, x: torch.Tensor, q: bool = True, log: bool = False\n    ) -> torch.Tensor:\n        \"\"\"Forward pass of the network.\n\n        :param obs: Input to the network.\n        :type obs: torch.Tensor, dict[str, torch.Tensor], or list[torch.Tensor]\n        :param q: Whether to return Q values. Defaults to True.\n        :type q: bool\n        :param log: Whether to return log probabilities. Defaults to False.\n        :type log: bool\n\n        :return: Output of the network.\n        :rtype: torch.Tensor\n        \"\"\"\n        value: torch.Tensor = self.model(x)\n        advantage: torch.Tensor = self.advantage_net(x)\n\n        batch_size = value.size(0)\n        value = value.view(batch_size, 1, self.num_atoms)\n        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)\n\n        x = value + advantage - advantage.mean(1, keepdim=True)\n        if log:\n            x = F.log_softmax(x.view(-1, self.num_atoms), dim=-1)\n            return x.view(-1, self.num_actions, self.num_atoms)\n\n        x = F.softmax(x.view(-1, self.num_atoms), dim=-1)\n        x = x.view(-1, self.num_actions, self.num_atoms).clamp(min=1e-3)\n        if q:\n```\n\n----------------------------------------\n\nTITLE: Generating State Transitions for Connect Four in Python\nDESCRIPTION: This snippet handles the creation of state transitions for both players in a Connect Four game. It processes game states, actions, rewards, and done flags, creating Transition objects to be added to the memory buffer.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nreward = self.reward(done=True, player=0)\ntransition = Transition(\n      obs=np.concatenate(\n         (p0_state, p1_state, p0_state_flipped, p1_state_flipped)\n      ),\n      action=np.array(\n         [p0_action, p1_action, 6 - p0_action, 6 - p1_action]\n      ),\n      reward=np.array(\n         [\n            reward,\n            LESSON[\"rewards\"][\"lose\"],\n            reward,\n            LESSON[\"rewards\"][\"lose\"],\n         ]\n      ),\n      next_obs=np.concatenate(\n         (\n            p0_next_state,\n            p1_next_state,\n            p0_next_state_flipped,\n            p1_next_state_flipped,\n         )\n      ),\n      done=np.array([done, done, done, done]),\n      batch_size=[4],\n)\nmemory.add(transition.to_tensordict(), is_vectorised=True)\n```\n\n----------------------------------------\n\nTITLE: Creating IPPO Agent Instance\nDESCRIPTION: Demonstrates initialization of an IPPO agent with observation spaces, action spaces, agent IDs, network configuration and device specification.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Create IPPO agent\nagent = IPPO(\n  observation_spaces=observation_spaces,\n  action_spaces=action_spaces,\n  agent_ids=agent_ids,\n  net_config=NET_CONFIG,\n  device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Making CNN Networks Evolvable for Multi-Agent RL in Python\nDESCRIPTION: This snippet demonstrates how to make CNN-based actor and critic networks evolvable using the MakeEvolvable wrapper. It includes handling of input tensors for both actor and critic, considering the differences in their input shapes.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nactor = MultiAgentCNNActor()\nevolvable_actor = MakeEvolvable(network=actor,\n                                input_tensor=torch.randn(1, 4, 1, 210, 160), # (B, C_in, D, H, W) D = 1 as actors are decentralised\n                                device=device)\ncritic = MultiAgentCNNCritic()\nevolvable_critic = MakeEvolvable(network=critic,\n                                 input_tensor=torch.randn(1, 4, 2, 210, 160), # (B, C_in, D, H, W)),\n                                                                              #  D = 2 as critics are centralised and  so we evaluate both agents\n                                 secondary_input_tensor=torch.randn(1,8), # Assuming 2 agents each with action dimensions of 4\n                                 device=device)\n```\n\n----------------------------------------\n\nTITLE: Creating PPO Agent with Custom Network Configuration\nDESCRIPTION: Example of initializing a PPO agent with a custom neural network configuration using the net_config parameter.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ppo.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create PPO agent\nagent = PPO(\n  observation_space=observation_space,\n  action_space=action_space,\n  net_config=NET_CONFIG\n  )\n```\n\n----------------------------------------\n\nTITLE: Metrics Tracking and Population Evolution in Connect Four Training\nDESCRIPTION: This code calculates and displays evaluation metrics after a training epoch, and performs tournament selection and population mutation. It tracks average turns, fitness scores, and updates the progress bar with current performance metrics.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\neval_turns = eval_turns / len(pop) / evo_loop\n\npbar.set_postfix_str(\n   f\"Train Mean Score: {np.mean(agent.scores[-episodes_per_epoch:])} \"\n   f\"Train Mean Turns: {mean_turns} \"\n   f\"Eval Mean Fitness: {np.mean(fitnesses)} \"\n   f\"Eval Best Fitness: {np.max(fitnesses)} \"\n   f\"Eval Mean Turns: {eval_turns} \"\n   f\"Total Steps: {total_steps}\"\n)\npbar.update(0)\n\n# Tournament selection and population mutation\nelite, pop = tournament.select(pop)\npop = mutations.mutation(pop)\n```\n\n----------------------------------------\n\nTITLE: Initializing Population for Offline RL in Python\nDESCRIPTION: Creates a population of agents for offline reinforcement learning using AgileRL. It sets up the environment, loads a dataset, configures hyperparameters, and initializes the agent population.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/offline_training/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.utils.utils import create_population, make_vect_envs, observation_space_channels_to_first\nimport gymnasium as gym\nimport h5py\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nINIT_HP = {\n    \"DOUBLE\": True,  # Use double Q-learning\n    \"BATCH_SIZE\": 128,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"GAMMA\": 0.99,  # Discount factor\n    \"LEARN_STEP\": 1,  # Learning frequency\n    \"TAU\": 1e-3,  # For soft update of target network parameters\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n    \"POP_SIZE\": 4,  # Population size\n}\n\nnum_envs = 1\nenv = make_vect_envs(\"CartPole-v1\", num_envs=num_envs)  # Create environment\ndataset = h5py.File(\"data/cartpole/cartpole_random_v1.1.0.h5\", \"r\")  # Load dataset\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\n# RL hyperparameter configuration for mutations\nhp_config = HyperparameterConfig(\n    lr = RLParameter(min=1e-4, max=1e-2),\n    batch_size = RLParameter(min=8, max=64, dtype=int),\n    learn_step = RLParameter(\n        min=1, max=120, dtype=int, grow_factor=1.5, shrink_factor=0.75\n        )\n)\n\npop = create_population(\n    algo=\"CQN\",  # Algorithm\n    observation_space=observation_space,  # State dimension\n    action_space=action_space,  # Action dimension\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    hp_config=hp_config,  # RL hyperparameters configuration\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    num_envs=num_envs,  # Number of vectorized envs\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Fine-tuned LLM in Python\nDESCRIPTION: Code for running inference with a fine-tuned language model. This snippet shows how to put the model in evaluation mode, tokenize input, generate text with controlled parameters like temperature and top_p sampling, and decode the output.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Put model in evaluation mode\nmodel.eval()\n\n# Tokenize input\ninputs = countdown_chat_template(torch.tensor([33, 19, 27, 5]), # Numbers\n                                torch.tensor([39]),            # Answer\n                                tokenizer)\n\n# Move inputs to the same device as model\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n# Generate text (inference)\nwith torch.no_grad():  # Disable gradient calculation for inference\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=100,  # Control the length of generated text\n        temperature=0.7,     # Control randomness (lower = more deterministic)\n        top_p=0.9,           # Nucleus sampling parameter\n        do_sample=True,      # Use sampling instead of greedy decoding\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n```\n\n----------------------------------------\n\nTITLE: Registering Network Groups for PPO in Python\nDESCRIPTION: Example of registering network groups for the actor and critic networks in PPO algorithm. The actor network is set as the policy.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_algorithms/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Register network groups for mutations\nself.register_network_group(\n    NetworkGroup(\n        eval=self.actor,\n        policy=True\n    )\n)\nself.register_network_group(\n    NetworkGroup(\n        eval=self.critic\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: RainbowQNetwork Class Implementation in Python\nDESCRIPTION: Complete implementation of RainbowQNetwork class that extends EvolvableNetwork to incorporate Rainbow DQN improvements. Includes initialization, network building, and forward pass logic.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_rainbow_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RainbowQNetwork(EvolvableNetwork):\n    \"\"\"RainbowQNetwork is an extension of the QNetwork that incorporates the Rainbow DQN improvements\n    from \"Rainbow: Combining Improvements in Deep Reinforcement Learning\" (Hessel et al., 2017).\n\n    Paper: https://arxiv.org/abs/1710.02298\n\n    :param observation_space: Observation space of the environment.\n    :type observation_space: spaces.Space\n    :param action_space: Action space of the environment\n    :type action_space: DiscreteSpace\n    :param encoder_config: Configuration of the encoder network.\n    :type encoder_config: ConfigType\n    :param support: Support for the distributional value function.\n    :type support: torch.Tensor\n    :param num_atoms: Number of atoms in the distributional value function. Defaults to 51.\n    :type num_atoms: int\n    :param head_config: Configuration of the network MLP head.\n    :type head_config: Optional[ConfigType]\n    :param min_latent_dim: Minimum dimension of the latent space representation. Defaults to 8.\n    :type min_latent_dim: int\n    :param max_latent_dim: Maximum dimension of the latent space representation. Defaults to 128.\n    :type max_latent_dim: int\n    :param n_agents: Number of agents in the environment. Defaults to None, which corresponds to\n        single-agent environments.\n    :type n_agents: Optional[int]\n    :param latent_dim: Dimension of the latent space representation.\n    :type latent_dim: int\n    :param device: Device to use for the network.\n    :type device: str\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: spaces.Space,\n        action_space: spaces.Discrete,\n        support: torch.Tensor,\n        num_atoms: int = 51,\n        noise_std: float = 0.5,\n        encoder_config: Optional[ConfigType] = None,\n        head_config: Optional[ConfigType] = None,\n        min_latent_dim: int = 8,\n        max_latent_dim: int = 128,\n        n_agents: Optional[int] = None,\n        latent_dim: int = 32,\n        device: str = \"cpu\",\n    ):\n```\n\n----------------------------------------\n\nTITLE: Creating NeuralTS Agent with Custom Network Configuration\nDESCRIPTION: Example of initializing a NeuralTS agent with a custom neural network configuration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ts.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nagent = NeuralTS(observation_space, action_space, net_config=NET_CONFIG)   # Create NeuralTS agent\n```\n\n----------------------------------------\n\nTITLE: Loading PPO Agent from Checkpoint in AgileRL\nDESCRIPTION: Example of loading a previously saved PPO agent from a checkpoint file using the static load method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ppo.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.ppo import PPO\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = PPO.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Implementing SimBa Residual Block in PyTorch\nDESCRIPTION: Defines a SimbaResidualBlock class that creates a residual block designed to avoid overfitting in RL by inducing a simplicity bias. It includes layer normalization, linear layers, and skip connections.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass SimbaResidualBlock(nn.Module):\n    \"\"\"Creates a residual block designed to avoid overfitting in RL by inducing\n    a simplicity bias.\n\n    Paper: https://arxiv.org/abs/2410.09754\n\n    :param hidden_size: Hidden size of the residual block\n    :type hidden_size: int\n    :param scale_factor: Scale factor, defaults to 4\n    :type scale_factor: float\n    :param device: Device, defaults to \"cpu\"\n    :type device\n    \"\"\"\n\n    def __init__(\n        self, hidden_size: int, scale_factor: float = 4, device: DeviceType = \"cpu\"\n    ) -> None:\n        super().__init__()\n\n        self.hidden_size = hidden_size\n\n        self.layer_norm = nn.LayerNorm(hidden_size, device=device)\n        self.linear1 = nn.Linear(hidden_size, hidden_size * scale_factor, device=device)\n        self.linear2 = nn.Linear(hidden_size * scale_factor, hidden_size, device=device)\n\n        # initialize weigts using he initialization\n        nn.init.kaiming_uniform_(self.linear1.weight)\n        nn.init.kaiming_uniform_(self.linear2.weight)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        res = x\n        x = self.layer_norm(x)\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        return res + x\n```\n\n----------------------------------------\n\nTITLE: Visualizing Trained MATD3 Agents in PettingZoo Environment\nDESCRIPTION: This code snippet shows how to load a saved MATD3 algorithm, test its performance, and visualize multiple episodes as a gif. It demonstrates the evaluation and rendering process for trained agents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/matd3.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/PettingZoo/render_agilerl_matd3.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Initializing Mutations Class for Neural Network Evolution in Python\nDESCRIPTION: Demonstrates the setup of the Mutations class with configurable mutation probabilities for different aspects of neural networks and RL algorithms. Parameters include probabilities for no mutation, architecture changes, parameter adjustments, activation layer modifications, and learning hyperparameter mutations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/hpo/mutation.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.hpo.mutation import Mutations\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmutations = Mutations(\n              no_mutation=0.4,                      # No mutation\n              architecture=0.2,                     # Architecture mutation\n              new_layer_prob=0.2,                   # New layer mutation\n              parameters=0.2,                       # Network parameters mutation\n              activation=0,                         # Activation layer mutation\n              rl_hp=0.2,                            # Learning HP mutation\n              mutation_sd=0.1,                      # Mutation strength\n              rand_seed=1,                          # Random seed\n              device=device\n            )\n```\n\n----------------------------------------\n\nTITLE: Wrapping Optimizer with OptimizerWrapper in Python\nDESCRIPTION: Example of wrapping an Adam optimizer with OptimizerWrapper, specifying the actor and critic networks it optimizes.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_algorithms/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nself.optimizer = OptimizerWrapper(\n    optim.Adam,\n    networks=[self.actor, self.critic],\n    lr=self.lr\n)\n```\n\n----------------------------------------\n\nTITLE: Forward Pass of EvolvableSimBa Neural Network in Python\nDESCRIPTION: This method performs the forward pass of the neural network. It handles input conversion to torch.Tensor and reshaping before passing through the model.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef forward(self, x: ObservationType) -> torch.Tensor:\n    \"\"\"Returns output of neural network.\n\n    :param x: Neural network input\n    :type x: torch.Tensor\n    :return: Neural network output\n    :rtype: torch.Tensor\n    \"\"\"\n    if not isinstance(x, torch.Tensor):\n        x = torch.tensor(x, dtype=torch.float32, device=self.device)\n\n    if len(x.shape) == 1:\n        x = x.unsqueeze(0)\n\n    return self.model(x)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Population in Python\nDESCRIPTION: Initializes the environment and creates a population of agents using specified configurations and hyperparameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom agilerl.utils.utils import (\n    make_vect_envs,\n    create_population,\n    observation_space_channels_to_first\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_envs = 16\nenv = make_vect_envs(env_name=INIT_HP['ENV_NAME'], num_envs=num_envs)\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP['CHANNELS_LAST']:\n    observation_space = observation_space_channels_to_first(observation_space)\n\nagent_pop = create_population(\n    algo=INIT_HP['ALGO'],                 # Algorithm\n    observation_space=observation_space,  # Observation space\n    action_space=action_space,            # Action space\n    net_config=NET_CONFIG,                # Network configuration\n    INIT_HP=INIT_HP,                      # Initial hyperparameters\n    population_size=INIT_HP['POP_SIZE'],  # Population size\n    num_envs=num_envs,                    # Number of vectorized environments\n    device=device\n)\n```\n\n----------------------------------------\n\nTITLE: Observation Space Transformation for Connect Four\nDESCRIPTION: Transforms observation space from (6,7,2) format to PyTorch's channels-first format and handles player perspective switching.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef transform_and_flip(observation, player):\n   \"\"\"Transforms and flips observation for input to agent's neural network.\n\n   :param observation: Observation to preprocess\n   :type observation: dict[str, np.ndarray]\n   :param player: Player, 0 or 1\n   :type player: int\n   \"\"\"\n   state = observation[\"observation\"]\n   # Pre-process dimensions for PyTorch (N, C, H, W)\n   state = obs_channels_to_first(state)\n   if player == 1:\n      # Swap pieces so that the agent always sees the board from the same perspective\n      state[[0, 1], :, :] = state[[1, 0], :, :]\n\n   state_flipped = np.expand_dims(np.flip(state, 2), 0)\n   state = np.expand_dims(state, 0)\n   return state, state_flipped\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Agent Networks for MADDPG and MATD3 in Python\nDESCRIPTION: This code snippet shows how to set up actor and critic networks for multi-agent algorithms like MADDPG and MATD3. It demonstrates defining networks for multiple agents and creating a population with these custom networks.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# For MADDPG\nevolvable_actors = [actor_network_1, actor_network_2]\nevolvable_critics = [critic_network_1, critic_network_2]\n\n# For MATD3, \"critics\" will be a list of 2 lists as MATD3 uses one more critic than MADDPG\nevolvable_actors = [actor_network_1, actor_network_2]\nevolvable_critics = [[critic_1_network_1, critic_1_network_2],\n                     [critic_2_network_1, critic_2_network_2]]\n\n# Instantiate the populations as follows\nobservation_spaces = [env.single_observation_space(agent) for agent in env.agents]\naction_spaces = [env.single_action_space(agent) for agent in env.agents]\npop = create_population(\n        algo=\"MADDPG\",                                # Algorithm\n        observation_space=observation_spaces,         # Observation space\n        action_space=action_spaces,                   # Action space\n        actor_network=evolvable_actors,               # Custom evolvable actor\n        critic_network=evolvable_critics,             # Custom evolvable critic\n        INIT_HP=INIT_HP,                              # Initial hyperparameters\n        population_size=INIT_HP[\"POPULATION_SIZE\"],   # Population size\n        device=device\n      )\n```\n\n----------------------------------------\n\nTITLE: Implementing Stabilize Skill for LunarLander\nDESCRIPTION: Defines a StabilizeSkill class that inherits from AgileRL's Skill class to teach the agent to minimize movement in all directions. Implements custom reward shaping to encourage stabilization behavior.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass StabilizeSkill(Skill):\n   def __init__(self, env):\n      super().__init__(env)\n\n      self.theta_level = 0\n      self.history = {\"x\": [], \"y\": [], \"theta\": []}\n\n   def skill_reward(self, observation, reward, terminated, truncated, info):\n      if terminated or truncated:\n            reward = -100.0\n            self.history = {\"x\": [], \"y\": [], \"theta\": []}\n            return observation, reward, terminated, truncated, info\n\n      reward, terminated, truncated = 1.0, 0, 0\n      x, y, theta = observation[0], observation[1], observation[4]\n\n      # Ensure there are previous observations to compare with\n      if len(self.history[\"x\"]) == 0:\n            self.history[\"x\"].append(x)\n            self.history[\"y\"].append(y)\n            self.history[\"theta\"].append(theta)\n            return observation, reward, terminated, truncated, info\n\n      # Minimise x movement\n      reward -= (abs(self.history[\"x\"][-1] - x) * 10) ** 2\n      # Minimise y movement\n      reward -= (abs(self.history[\"y\"][-1] - y) * 10) ** 2\n      # Minimise tilt angle\n      reward -= (abs(self.history[\"theta\"][-1] - theta) * 10) ** 2\n\n      self.history[\"x\"].append(x)\n      self.history[\"y\"].append(y)\n      self.history[\"theta\"].append(theta)\n\n      # Reset episode if longer than 300 steps\n      if len(self.history[\"x\"]) > 300:\n            reward = 10.0\n            terminated = True\n            self.history = {\"x\": [], \"y\": [], \"theta\": []}\n            self.env.reset()\n\n      return observation, reward, terminated, truncated, info\n```\n\n----------------------------------------\n\nTITLE: Defining DummyEvolvable Class in Python for AgileRL\nDESCRIPTION: The DummyEvolvable class converts a torch.nn.Module into an evolvable module. It allows pre-trained models to be used in AgileRL algorithms, enabling RL hyperparameter and weight mutations while disabling architecture mutations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/dummy.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: agilerl.modules.dummy.DummyEvolvable\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Rendering and Visualizing Trained MADDPG Agents in Space Invaders\nDESCRIPTION: This code shows how to load a previously trained MADDPG model, test its performance on the Space Invaders environment, and visualize the agents' behavior by rendering episodes as a gif.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/maddpg.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../../tutorials/PettingZoo/render_agilerl_maddpg.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Testing On-Policy Learning with PPO\nDESCRIPTION: Demonstrates validation of PPO implementations using on-policy probe environments. Tests policy and value function learning in continuous action spaces.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/debugging_rl/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom agilerl.algorithms.ppo import PPO\nfrom agilerl.utils.probe_envs import check_policy_on_policy_with_probe_env\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncont_vector_envs = [\n    (ConstantRewardContActionsEnv(), 1000),\n    (ObsDependentRewardContActionsEnv(), 1000),\n    (DiscountedRewardContActionsEnv(), 5000),\n    (FixedObsPolicyContActionsEnv(), 3000),\n    (PolicyContActionsEnv(), 3000),\n]\n\nfor env, learn_steps in cont_vector_envs:\n    algo_args = {\n        \"observation_space\": env.observation_space,\n        \"action_space\": env.action_space,\n        \"lr\": 0.001\n    }\n\n    check_policy_on_policy_with_probe_env(\n        env, PPO, algo_args, memory, learn_steps, device\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Rainbow DQN Agent in Python for Inference\nDESCRIPTION: This snippet demonstrates how to load a previously saved Rainbow DQN agent for inference. It uses the RainbowDQN.load() method to restore the agent's state from a file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrainbow_dqn = RainbowDQN.load(save_path, device=device)\n```\n\n----------------------------------------\n\nTITLE: Basic QNetwork Class Reference\nDESCRIPTION: API documentation for the base QNetwork class used in Q-learning implementations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/networks/q_networks.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.networks.q_networks.QNetwork\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Mutations for Hyperparameter Exploration\nDESCRIPTION: Creates a Mutations object that handles the different types of mutations that can be applied to agents during evolution. This includes mutations to network architecture, parameters, activation functions, and reinforcement learning hyperparameters. The mutation probabilities are specified in the MUT_P dictionary.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmutations = Mutations(\n    no_mutation=MUT_P[\"NO_MUT\"],\n    architecture=MUT_P[\"ARCH_MUT\"],\n    new_layer_prob=MUT_P[\"NEW_LAYER\"],\n    parameters=MUT_P[\"PARAMS_MUT\"],\n    activation=MUT_P[\"ACT_MUT\"],\n    rl_hp=MUT_P[\"RL_HP_MUT\"],\n    mutation_sd=MUT_P[\"MUT_SD\"],\n    rand_seed=MUT_P[\"RAND_SEED\"],\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: LLM Finetuning Example with Python\nDESCRIPTION: Python code snippet demonstrating LLM finetuning implementation. Referenced from the tutorials/LLM_Finetuning/grpo_reasoning.py file as an example of reinforcement learning application in language models.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/llm_finetuning/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n../../tutorials/LLM_Finetuning/grpo_reasoning.py\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for EvolvableCNN\nDESCRIPTION: ReStructuredText documentation snippet that defines the structure and autoclass documentation for the EvolvableCNN class in the agilerl.modules.cnn package.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/cnn.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _cnn:\\n\\nEvolvable Convolutional Neural Network (CNN)\\n============================================\\n\\nParameters\\n------------\\n\\n.. autoclass:: agilerl.modules.cnn.EvolvableCNN\\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Multi-Input Observations\nDESCRIPTION: Configuration setup for DDPG neural network handling multiple input types including images, discrete, and vector observations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ddpg.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tournament Selection for Agent Evolution\nDESCRIPTION: Creates a TournamentSelection object that will select agents for the next generation based on their performance. Tournament selection randomly picks k agents and selects the best performer, repeating until the new population is filled. If elitism is enabled, the best agent is automatically preserved.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntournament = TournamentSelection(\n    INIT_HP[\"TOURN_SIZE\"],\n    INIT_HP[\"ELITISM\"],\n    INIT_HP[\"POP_SIZE\"],\n    INIT_HP[\"EVAL_LOOP\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing TD3 Agent with AgileRL in Python\nDESCRIPTION: This snippet demonstrates how to create and use a TD3 agent with AgileRL, including environment setup, replay buffer initialization, and the main training loop.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/td3.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.utils.utils import make_vect_envs, observation_space_channels_first\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.components.data import Transition\nfrom agilerl.algorithms.td3 import TD3\n\n# Create environment and Experience Replay Buffer\nnum_envs = 1\nenv = make_vect_envs('LunarLanderContinuous-v3', num_envs=num_envs)\nobservation_space = env.observation_space\naction_space = env.action_space\n\nchannels_last = False # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n\nif channels_last:\n    observation_space = observation_space_channels_first(observation_space)\n\nmemory = ReplayBuffer(max_size=10000)\n\nagent = TD3(observation_space, action_space)   # Create TD3 agent\n\nstate = env.reset()[0]  # Reset environment at start of episode\nwhile True:\n    if channels_last:\n        state = obs_channels_to_first(state)\n    action = agent.get_action(state, training=True)    # Get next action from agent\n    next_state, reward, done, _, _ = env.step(action)   # Act in environment\n\n    # Save experience to replay buffer\n    transition = Transition(\n        obs=state,\n        action=action,\n        reward=reward,\n        next_obs=next_state,\n        done=done,\n        batch_size=[num_envs]\n    )\n    transition = transition.to_tensordict()\n    memory.add(transition)\n\n    # Learn according to learning frequency\n    if len(memory) >= agent.batch_size:\n        experiences = memory.sample(agent.batch_size) # Sample replay buffer\n        agent.learn(experiences)    # Learn according to agent's RL algorithm\n```\n\n----------------------------------------\n\nTITLE: Implementing PolicyContActionsEnv for Multi-Agent Continuous Actions in Python\nDESCRIPTION: A multi-agent environment with continuous action spaces for testing policy-based RL algorithms. It provides a simplified environment where agents must learn to output specific continuous action values to maximize rewards.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass PolicyContActionsEnv:\n    \"\"\"Simple multi-agent environment with continuous action space.\n\n    The environment has n_agents agents, each with a state observation of size n_agents.\n    The optimal policy is for each agent i to output action[i] = 1.0 and action[j] = 0.0\n    for all j != i.\n\n    Args:\n        n_agents (int): number of agents in the environment\n        max_steps (int, optional): maximum number of steps per episode. Defaults to 10.\n        shared_reward (bool, optional): whether agents receive shared reward\n            or individual rewards. Defaults to False.\n        return_agent_actions (bool, optional): whether to return previous actions of\n            all agents as part of observation. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_agents,\n        max_steps=10,\n        shared_reward=False,\n        return_agent_actions=False,\n    ):\n        self.n_agents = n_agents\n        self.shared_reward = shared_reward\n        self.return_agent_actions = return_agent_actions\n        self.max_steps = max_steps\n        self.state_size = n_agents\n        self.state_range = (-1.0, 1.0)\n        self.action_size = n_agents\n        self.cont_actions = True\n        self.action_range = (-1.0, 1.0)\n        self.rewards_range = (-float(n_agents), float(n_agents))\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset environment to initial state.\n\n        Returns:\n            list: initial state for each agent\n        \"\"\"\n        self.steps = 0\n        self.dones = [False for _ in range(self.n_agents)]\n        self.states = [\n            np.zeros(self.state_size, dtype=np.float32) for _ in range(self.n_agents)\n        ]\n        self.last_actions = [\n            np.zeros(self.action_size, dtype=np.float32) for _ in range(self.n_agents)\n        ]\n        return self.get_obs()\n\n    def step(self, actions):\n        \"\"\"Take a step in the environment.\n\n        Args:\n            actions (list): actions for each agent\n\n        Returns:\n            tuple: (observations, rewards, dones, infos)\n        \"\"\"\n        rewards = []\n\n        scaled_actions = []\n        for action in actions:\n            if isinstance(action, np.ndarray):\n                action = action.reshape(-1)\n            if len(action) != self.action_size:\n                raise ValueError(\n                    f\"Expected action of size {self.action_size}, got {len(action)}\"\n                )\n            scaled_actions.append(action)\n\n        for i in range(self.n_agents):\n            self.last_actions[i] = scaled_actions[i]\n\n        if self.shared_reward:\n            # Shared reward is sum of all individual rewards\n            reward = 0.0\n            for i in range(self.n_agents):\n                agent_reward = self._get_agent_reward(i, scaled_actions[i])\n                reward += agent_reward\n            rewards = [reward for _ in range(self.n_agents)]\n        else:\n            for i in range(self.n_agents):\n                agent_reward = self._get_agent_reward(i, scaled_actions[i])\n                rewards.append(agent_reward)\n\n        self.steps += 1\n        if self.steps >= self.max_steps:\n            self.dones = [True for _ in range(self.n_agents)]\n\n        infos = [{} for _ in range(self.n_agents)]\n        return self.get_obs(), rewards, self.dones, infos\n\n    def _get_agent_reward(self, agent_idx, action):\n        \"\"\"Calculate reward for an agent based on its action.\n\n        The optimal policy is for each agent i to output action[i] = 1.0 and\n        action[j] = 0.0 for all j != i.\n\n        Args:\n            agent_idx (int): index of the agent\n            action (np.ndarray): action taken by the agent\n\n        Returns:\n            float: reward for the agent\n        \"\"\"\n        reward = 0.0\n        # Positive reward for action[agent_idx] = 1.0\n        # target is 1.0, so reward is higher closer to 1.0\n        reward += 1.0 - abs(action[agent_idx] - 1.0)\n\n        # Negative reward for action[j] != 0.0 where j != agent_idx\n        # target is 0.0, so penalty is higher further from 0.0\n        for j in range(self.n_agents):\n            if j != agent_idx:\n                reward -= abs(action[j])\n\n        return reward\n\n    def get_obs(self):\n        \"\"\"Get observations for all agents.\n\n        Returns:\n            list: observations for each agent\n        \"\"\"\n        if self.return_agent_actions:\n            # Include other agents' last actions in observation\n            obs = []\n            for i in range(self.n_agents):\n                agent_obs = np.zeros(self.state_size + (self.n_agents - 1) * self.action_size)\n                # Add state\n                agent_obs[: self.state_size] = self.states[i]\n                # Add other agents' actions\n                action_idx = self.state_size\n                for j in range(self.n_agents):\n                    if j != i:\n                        agent_obs[action_idx : action_idx + self.action_size] = self.last_actions[\n                            j\n                        ]\n                        action_idx += self.action_size\n                obs.append(agent_obs)\n            return obs\n        else:\n            return self.states\n```\n\n----------------------------------------\n\nTITLE: Implementing NeuralUCB Agent with Iris Dataset\nDESCRIPTION: Example of creating and using a NeuralUCB agent with the Iris dataset. Shows how to set up the environment, create the agent, and perform training with a replay buffer.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ucb.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensordict import TensorDict\n\nfrom agilerl.algorithms.neural_ucb import NeuralUCB\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.wrappers.learning import BanditEnv\n\n# Fetch data  https://archive.ics.uci.edu/\niris = fetch_ucirepo(id=53)\nfeatures = iris.data.features\ntargets = iris.data.targets\n\n# Create environment\nenv = BanditEnv(features, targets)\ncontext_dim = env.context_dim\naction_dim = env.arms\n\nmemory = ReplayBuffer(max_size=10000)\n\nobservation_space = spaces.Box(low=features.values.min(), high=features.values.max())\naction_space = spaces.Discrete(action_dim)\nbandit = NeuralUCB(observation_space, action_space)   # Create NeuralUCB agent\n\ncontext = env.reset()  # Reset environment at start of episode\nfor _ in range(500):\n    # Get next action from agent\n    action = agent.get_action(context)\n    next_context, reward = env.step(action)  # Act in environment\n\n    # Save experience to replay buffer\n    transition = TensorDict({\n      \"obs\": context[action],\n      \"reward\": reward,\n      },\n      batch_size=[1]\n    )\n    memory.add(transition)\n\n    # Learn according to learning frequency\n    if len(memory) >= agent.batch_size:\n        for _ in range(agent.learn_step):\n            experiences = memory.sample(agent.batch_size) # Sample replay buffer\n            agent.learn(experiences)    # Learn according to agent's RL algorithm\n\n\n    context = next_context\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for EvolvableWrapper Class\nDESCRIPTION: ReStructuredText documentation structure for the EvolvableWrapper class which provides a wrapper for making neural network modules evolvable.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/base.rst#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\nEvolvableWrapper\n================\n\nParameters\n------------\n\n.. autoclass:: agilerl.modules.base.EvolvableWrapper\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Creating Population with Custom Networks for TD3 Algorithm in Python\nDESCRIPTION: This snippet demonstrates how to create a population for the TD3 algorithm using custom evolvable actor and critic networks. It shows how to handle multiple critics and set up the population with initial hyperparameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npop = create_population(\n    algo=\"TD3\",                                           # Algorithm\n    observation_space=observation_space,                      # Observation space\n    action_space=action_space,                                # Action space\n    actor_network=evolvable_actor,                            # Custom evolvable actor\n    critic_network=[evolvable_critic_1, evolvable_critic_2],  # Custom evolvable critic\n    INIT_HP=INIT_HP,                                          # Initial hyperparameters\n    population_size=INIT_HP[\"POPULATION_SIZE\"],               # Population size\n    device=device\n)\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Default PettingZoo Environment with AsyncPettingZooVecEnv\nDESCRIPTION: Example demonstrating how to create a vectorized version of a default PettingZoo environment (simple_speaker_listener_v4) and run it for multiple steps. The environment is created with multiple parallel instances and actions are sampled from the action space.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/vector/petting_zoo_async_vector_env.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Default pettingzoo environment\nfrom agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\nfrom pettingzoo.mpe import simple_speaker_listener_v4\n\nnum_envs = 4\nenv = AsyncPettingZooVecEnv(\n      [\n          lambda: simple_speaker_listener_v4.parallel_env()\n          for _ in range(num_envs)\n      ]\n  )\nobservations, infos = vec_env.reset()\nfor step in range(25):\n    actions = {\n        agent: [vec_env.single_action_space(agent).sample() for n in range(num_envs)]\n        for agent in vec_env.agents\n    }\n    observations, rewards, terminations, truncations, infos = vec_env.step(actions)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Reference Declaration\nDESCRIPTION: RST markup defining a documentation reference and class description for the EvolvableMultiInput neural network module.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/multi_input.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _multi_input:\n\nEvolvable Multi-Input Neural Network (Dict / Tuple Observations)\n================================================================\n\nParameters\n------------\n\n.. autoclass:: agilerl.modules.multi_input.EvolvableMultiInput\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Network Architecture Mutation Example in Python\nDESCRIPTION: Example code showing how to instantiate a RainbowQNetwork and inspect available architecture mutations. Demonstrates setup with image observation space and discrete action space.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_rainbow_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom gymnasium import spaces\n\n# Define an image observation space and a discrete action space\nobservation_space = spaces.Box(low=0, high=255, shape=(3, 128, 128), dtype=np.uint8)\naction_space = spaces.Discrete(4)\n\nsupport = torch.linspace(-10, 10, 51)\n\nnetwork = RainbowQNetwork(\n    observation_space=observation_space,\n    action_space=action_space,\n    support=torch.linspace(-10, 10, 51), # Support for the DuelingDistributionalMLP\n    )\n\nprint(network.mutation_methods)\n```\n\n----------------------------------------\n\nTITLE: Loading and Testing Trained TD3 Agent for Inference in Python\nDESCRIPTION: Demonstrates how to load a trained TD3 agent and use it for inference in a test environment. This includes rendering the environment and saving the test episodes as a gif.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntd3 = TD3.load_checkpoint(save_path, device=device)\n\ntest_env = gym.make(\"LunarLanderContinuous-v3\", render_mode=\"rgb_array\")\nrewards = []\nframes = []\ntesting_eps = 7\nmax_testing_steps = 1000\nwith torch.no_grad():\n    for ep in range(testing_eps):\n        state = test_env.reset()[0]  # Reset environment at start of episode\n        score = 0\n\n        for step in range(max_testing_steps):\n            # If your state is an RGB image\n            state = obs_channels_to_first(state) if INIT_HP[\"CHANNELS_LAST\"] else state\n\n            # Get next action from agent\n            action, *_ = td3.get_action(state, training=False)\n\n            # Save the frame for this step and append to frames list\n            frame = test_env.render()\n            frames.append(frame)\n\n            # Take the action in the environment\n            state, reward, terminated, truncated, _ = test_env.step(action)\n\n            # Collect the score\n            score += reward\n\n            # Break if environment 0 is done or truncated\n            if terminated or truncated:\n                print(\"terminated\")\n                break\n\n        # Collect and print episodic reward\n        rewards.append(score)\n        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n        print(\"Episodic Reward: \", rewards[-1])\n\n    print(rewards)\n\n    test_env.close()\n\nframes = frames[::3]\ngif_path = \"./videos/\"\nos.makedirs(gif_path, exist_ok=True)\nimageio.mimwrite(\n    os.path.join(\"./videos/\", \"td3_lunar_lander.gif\"), frames, duration=50, loop=0\n)\nmean_fitness = np.mean(rewards)\n```\n\n----------------------------------------\n\nTITLE: Implementing PolicyContActionsImageEnv for Image-Based Multi-Agent Learning in Python\nDESCRIPTION: An extension of PolicyContActionsEnv that uses image observations instead of vector states. This environment generates simple image representations for each agent to test computer vision-based reinforcement learning algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass PolicyContActionsImageEnv(PolicyContActionsEnv):\n    \"\"\"Image-based version of PolicyContActionsEnv.\n\n    This environment returns image observations instead of vector states.\n    The image has n_agents channels, with agent i having a 1.0 in the center of channel i,\n    and 0.0 elsewhere. This is a simple way to encode the agent's index.\n\n    Args:\n        n_agents (int): number of agents in the environment\n        max_steps (int, optional): maximum number of steps per episode. Defaults to 10.\n        shared_reward (bool, optional): whether agents receive shared reward\n            or individual rewards. Defaults to False.\n        image_size (int, optional): size of observation images. Defaults to 5.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_agents,\n        max_steps=10,\n        shared_reward=False,\n        image_size=5,\n    ):\n        super().__init__(\n            n_agents=n_agents,\n            max_steps=max_steps,\n            shared_reward=shared_reward,\n            return_agent_actions=False,\n        )\n        self.image_size = image_size\n        # Override state size to be image dimensions\n        self.state_size = (self.n_agents, self.image_size, self.image_size)\n\n    def reset(self):\n        \"\"\"Reset environment to initial state.\n\n        Returns:\n            list: initial state for each agent\n        \"\"\"\n        self.steps = 0\n        self.dones = [False for _ in range(self.n_agents)]\n        # Create image observations\n        self.states = self._create_image_observations()\n        self.last_actions = [\n            np.zeros(self.action_size, dtype=np.float32) for _ in range(self.n_agents)\n        ]\n        return self.get_obs()\n\n    def _create_image_observations(self):\n        \"\"\"Create image observations for each agent.\n\n        Returns:\n            list: image observations for each agent\n        \"\"\"\n        observations = []\n        for i in range(self.n_agents):\n            # Create empty image with n_agents channels\n            obs = np.zeros((self.n_agents, self.image_size, self.image_size), dtype=np.float32)\n            # Set center of channel i to 1.0\n            center = self.image_size // 2\n            obs[i, center, center] = 1.0\n            observations.append(obs)\n        return observations\n\n    def get_obs(self):\n        \"\"\"Get observations for all agents.\n\n        Returns:\n            list: observations for each agent\n        \"\"\"\n        return self.states\n```\n\n----------------------------------------\n\nTITLE: Implementing EvolvableSimBa Class in AgileRL\nDESCRIPTION: Defines the EvolvableSimBa class that inherits from EvolvableModule. It sets up the SimBa architecture with evolvable hyperparameters and includes mutation capabilities for optimization during training.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass EvolvableSimBa(EvolvableModule):\n    \"\"\"Evolvable module that implements the architecture presented in 'SimBa: Simplicity\n    Bias for Scaling Up Parameters in Deep Reinforcement Learning'. Designed to avoid\n    overfitting by integrating components that induce a simplicity bias, guiding models toward\n    simple and generalizable solutions.\n\n    Paper: https://arxiv.org/abs/2410.09754\n\n    :param num_inputs: Input layer dimension\n    :type num_inputs: int\n    :param num_outputs: Output layer dimension\n    :type num_outputs: int\n    :param hidden_size: Hidden layer(s) size\n    :type hidden_size: List[int]\n    :param num_blocks: Number of residual blocks that compose the network\n    :type num_blocks: int\n    :param output_activation: Output activation layer, defaults to None\n    :type output_activation: str, optional\n    :param scale_factor: Scale factor for the network, defaults to 4\n    :type scale_factor: int, optional\n    :param min_blocks: Minimum number of residual blocks that compose the network, defaults to 1\n    :type min_blocks: int, optional\n    :param max_blocks: Maximum number of residual blocks that compose the network, defaults to 4\n    :type max_blocks: int, optional\n    :param min_mlp_nodes: Minimum number of nodes a layer can have within the network, defaults to 16\n    :type min_mlp_nodes: int, optional\n    :param max_mlp_nodes: Maximum number of nodes a layer can have within the network, defaults to 500\n    :type max_mlp_nodes: int, optional\n    :param device: Device for accelerated computing, 'cpu' or 'cuda', defaults to 'cpu'\n    :type device: str, optional\n    :param name: Name of the network, defaults to 'mlp'\n    :type name: str, optional\n    \"\"\"\n\n    def __init__(\n        self,\n        num_inputs: int,\n        num_outputs: int,\n        hidden_size: int,\n        num_blocks: int,\n        output_activation: str = None,\n        scale_factor: int = 4,\n        min_blocks: int = 1,\n        max_blocks: int = 4,\n        min_mlp_nodes: int = 16,\n        max_mlp_nodes: int = 500,\n        device: str = \"cpu\",\n        name: str = \"simba\",\n    ) -> None:\n        super().__init__(device=device)\n\n        assert isinstance(scale_factor, int), \"Scale factor must be an integer.\"\n\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.hidden_size = hidden_size\n        self.num_blocks = num_blocks\n        self.output_activation = output_activation\n        self.scale_factor = scale_factor\n        self.min_blocks = min_blocks\n        self.max_blocks = max_blocks\n        self.min_mlp_nodes = min_mlp_nodes\n        self.max_mlp_nodes = max_mlp_nodes\n        self.name = name\n\n        self.model = create_simba(\n            input_size=num_inputs,\n            output_size=num_outputs,\n            hidden_size=hidden_size,\n            num_blocks=num_blocks,\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for GumbelSoftmax Activation Function\nDESCRIPTION: Sphinx RST documentation block that configures autodoc generation for the GumbelSoftmax class from agilerl.modules.custom_components module.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/custom_activation.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.modules.custom_components.GumbelSoftmax\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Dictionary/Tuple Observations in NeuralTS\nDESCRIPTION: Configuration for the neural network architecture when using complex observations like dictionaries or tuples containing combinations of images, discrete, and vector observations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ts.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Documenting NetworkGroup Class for AgileRL Mutations Registry in Python\nDESCRIPTION: Autoclass documentation for the NetworkGroup class in the agilerl.algorithms.core.registry module. This class likely manages groups of neural networks used in reinforcement learning algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/registry.rst#2025-04-19_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: agilerl.algorithms.core.registry.NetworkGroup\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Vector Observations in PPO\nDESCRIPTION: Example of configuring the neural network architecture for PPO when dealing with discrete or vector observations, specifying encoder and head configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ppo.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Custom PettingZoo Environment with AsyncPettingZooVecEnv\nDESCRIPTION: Example showing how to create a vectorized version of a custom PettingZoo environment and run it for multiple steps. The environment is created with multiple parallel instances and actions are sampled from the action space for each agent.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/vector/petting_zoo_async_vector_env.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Custom pettingzoo environment\nfrom agilerl.vector.pz_async_vec_env import AsyncPettingZooVecEnv\n\nnum_envs = 4\nvec_env = AsyncPettingZooVecEnv([lambda: CustomEnv() for _ in range(num_envs)])\nobservations, infos = vec_env.reset()\nfor step in range(25):\n    actions = {\n        agent: [vec_env.single_action_space(agent).sample() for n in range(num_envs)]\n        for agent in vec_env.agents\n    }\n    observations, rewards, terminations, truncations, infos = vec_env.step(actions)\n```\n\n----------------------------------------\n\nTITLE: Implementing DDPG with Gymnasium Environment\nDESCRIPTION: Basic implementation of DDPG agent with Gymnasium environment setup, including replay buffer initialization and training loop logic.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ddpg.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport gymnasium as gym\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.utils.utils import make_vect_envs, observation_space_channels_to_first\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.algorithms.ddpg import DDPG\n\n# Create environment and Experience Replay Buffer\nnum_envs = 1\nenv = make_vect_envs('LunarLanderContinuous-v3', num_envs=num_envs)\nobservation_space = env.observation_space\naction_space = env.action_space\n\nchannels_last = False # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n\nif channels_last:\n    observation_space = observation_space_channels_to_first(observation_space)\n\nmemory = ReplayBuffer(max_size=10000)\n\nagent = DDPG(observation_space, action_space)   # Create DDPG agent\n\nstate = env.reset()[0]  # Reset environment at start of episode\nwhile True:\n    if channels_last:\n        state = obs_channels_to_first(state)\n\n    action = agent.get_action(state, training=True)    # Get next action from agent\n    next_state, reward, done, _, _ = env.step(action)   # Act in environment\n\n    # Save experience to replay buffer\n    next_state = obs_channels_to_first(next_state) if channels_last else next_state\n    transition = Transition(\n        obs=state,\n        action=action,\n        reward=reward,\n        next_obs=next_state,\n        done=done,\n        batch_size=[num_envs]\n    )\n    transition = transition.to_tensordict()\n    memory.add(transition)\n\n    # Learn according to learning frequency\n    if len(memory) >= agent.batch_size:\n        experiences = memory.sample(agent.batch_size) # Sample replay buffer\n        agent.learn(experiences)    # Learn according to agent's RL algorithm\n```\n\n----------------------------------------\n\nTITLE: Adding Transitions to Memory Buffer in Connect Four Game\nDESCRIPTION: This code handles adding game state transitions to the replay memory buffer. It creates transition objects with observations, actions, rewards, next states, and done flags, handling both game completion and ongoing gameplay scenarios.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmemory.add(\n   transition.to_tensordict(), is_vectorised=True\n)\n# Play continues\nreward = env.reward(done=False, player=0)\ntransition = Transition(\n   obs=np.concatenate((p0_state, p0_state_flipped)),\n   action=np.array([p0_action, 6 - p0_action]),\n   reward=np.array([reward, reward]),\n   next_obs=np.concatenate(\n         (p0_next_state, p0_next_state_flipped)\n   ),\n   done=np.array([done, done]),\n   batch_size=[2],\n)\nmemory.add(\n   transition.to_tensordict(), is_vectorised=True\n)\n```\n\n----------------------------------------\n\nTITLE: Documenting HyperparameterConfig Class for AgileRL Mutations Registry in Python\nDESCRIPTION: Autoclass documentation for the HyperparameterConfig class in the agilerl.algorithms.core.registry module. This class likely manages configurations for hyperparameters in reinforcement learning algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/registry.rst#2025-04-19_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: agilerl.algorithms.core.registry.HyperparameterConfig\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for TD3 with Mixed Observations in Python\nDESCRIPTION: This snippet shows how to configure the neural network architecture for TD3 with mixed (dictionary/tuple) observations, including configurations for CNN and MLP components.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/td3.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n\nagent = TD3(observation_space, action_space, net_config=NET_CONFIG)   # Create TD3 agent\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Multi-Input Observations in Rainbow DQN\nDESCRIPTION: Configuration for handling dictionary/tuple observations with any combination of image, discrete, and vector inputs in Rainbow DQN, using nested CNN and MLP configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn_rainbow.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for MADDPG Class in Python\nDESCRIPTION: This code snippet uses Sphinx's autodoc extension to automatically generate documentation for the MADDPG class. It includes all members and inherited members of the class.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/maddpg.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: agilerl.algorithms.maddpg.MADDPG\n  :members:\n  :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Using Agent Masking with the AgileRL Multi-Agent Training Function in Python\nDESCRIPTION: Example of using agent masking in the training loop by passing the info dictionary to the agent's get_action method to control which agents receive new actions.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstate, info = env.reset()  # or: next_state, reward, done, truncation, info = env.step(action)\ncont_actions, discrete_action = agent.get_action(state, infos=info)\nif agent.discrete_actions:\n    action = discrete_action\nelse:\n    action = cont_actions\n```\n\n----------------------------------------\n\nTITLE: Counting Three-in-a-Row Configurations in Connect Four\nDESCRIPTION: This method checks for three pieces in a row with a blank space next to them, or two pieces - blank - piece configurations. It checks horizontally, vertically, and diagonally across the game board.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef check_three_in_row(self, player: int) -> int:\n    \"\"\"Checks if there are three pieces in a row and a blank space next, or two pieces - blank - piece.\n\n    :param player: Player who we are checking, 0 or 1\n    :type player: int\n    \"\"\"\n    board = np.array(self.env.env.board).reshape(6, 7)\n    piece = player + 1\n\n    # Check horizontal locations\n    column_count = 7\n    row_count = 6\n    three_in_row_count = 0\n\n    # Check vertical locations\n    for c in range(column_count):\n          for r in range(row_count - 3):\n             if self.check_winnable(board[r : r + 4, c].tolist(), piece):\n                three_in_row_count += 1\n\n    # Check horizontal locations\n    for r in range(row_count):\n          for c in range(column_count - 3):\n             if self.check_winnable(board[r, c : c + 4].tolist(), piece):\n                three_in_row_count += 1\n\n    # Check positively sloped diagonals\n    for c in range(column_count - 3):\n          for r in range(row_count - 3):\n             if self.check_winnable(\n                [\n                      board[r, c],\n                      board[r + 1, c + 1],\n                      board[r + 2, c + 2],\n                      board[r + 3, c + 3],\n                ],\n                piece,\n             ):\n                three_in_row_count += 1\n\n    # Check negatively sloped diagonals\n    for c in range(column_count - 3):\n          for r in range(3, row_count):\n             if self.check_winnable(\n                [\n                      board[r, c],\n                      board[r - 1, c + 1],\n                      board[r - 2, c + 2],\n                      board[r - 3, c + 3],\n                ],\n                piece,\n             ):\n                three_in_row_count += 1\n\n    return three_in_row_count\n```\n\n----------------------------------------\n\nTITLE: Testing Policy and Q-Learning with DDPG\nDESCRIPTION: Shows how to validate DDPG implementations using continuous action probe environments. Sets up environments and tests both policy and Q-learning components.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/debugging_rl/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom agilerl.algorithms.ddpg import DDPG\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.utils.probe_envs import check_policy_q_learning_with_probe_env\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ncont_vector_envs = [\n    (ConstantRewardContActionsEnv(), 1000),\n    (ObsDependentRewardContActionsEnv(), 1000),\n    (DiscountedRewardContActionsEnv(), 5000),\n    (FixedObsPolicyContActionsEnv(), 3000),\n    (PolicyContActionsEnv(), 3000),\n]\n\nfor env, learn_steps in cont_vector_envs:\n    algo_args = {\n        \"observation_space\": env.observation_space,\n        \"action_space\": env.action_space,\n        \"lr_actor\": 1e-2,\n        \"lr_critic\": 1e-2,\n    }\n\n    memory = ReplayBuffer(\n        max_size=1000,  # Max replay buffer size\n        device=device,\n    )\n\n    check_policy_q_learning_with_probe_env(\n        env, DDPG, algo_args, memory, learn_steps, device\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Image Observations in DQN\nDESCRIPTION: Example showing how to configure the neural network architecture for a DQN agent with image observation spaces, specifying CNN parameters for the encoder and MLP parameters for the head.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Using MakeEvolvable Wrapper to Create Evolvable Networks\nDESCRIPTION: Example of using the MakeEvolvable wrapper to add evolvable functionality to a PyTorch neural network. This involves passing the network and an input tensor to the wrapper to enable architecture mutations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.wrappers.make_evolvable import MakeEvolvable\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\n\nactor = MLPActor(observation_space.shape[0], action_space.n)\nevolvable_actor = MakeEvolvable(\n                    actor,\n                    input_tensor=torch.randn(observation_space.shape[0]),\n                    device=device\n                  )\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Dictionary/Tuple Observations in PPO\nDESCRIPTION: Example of configuring a complex neural network architecture for PPO when dealing with dictionary or tuple observations containing combinations of image, discrete, and vector observations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ppo.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Loading NeuralTS Agent from Checkpoint\nDESCRIPTION: Example of loading a previously saved NeuralTS agent from a checkpoint file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ts.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.neural_ts import NeuralTS\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = NeuralTS.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Multi-Input Observations\nDESCRIPTION: Configuration for NeuralUCB's network architecture when using dictionary or tuple observations with mixed data types (images, vectors, etc.). Defines CNN and MLP configurations for different input types.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ucb.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Calculating Rewards in Connect Four Reinforcement Learning\nDESCRIPTION: This method processes and returns rewards based on the game state and lesson criteria. It checks for different win conditions and assigns rewards accordingly, including a special reward for vertical wins.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef reward(self, done: bool, player: int) -> float:\n    \"\"\"Processes and returns reward from environment according to lesson criteria.\n\n    :param done: Environment has terminated\n    :type done: bool\n    :param player: Player who we are checking, 0 or 1\n    :type player: int\n    \"\"\"\n    if done:\n          reward = (\n             self.lesson[\"rewards\"][\"vertical_win\"]\n             if self.check_vertical_win(player)\n             else self.lesson[\"rewards\"][\"win\"]\n          )\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Fine-tuned Language Model in Python\nDESCRIPTION: This code snippet shows how to perform inference using a fine-tuned language model. It includes setting the model to evaluation mode, tokenizing input, moving inputs to the correct device, generating text, and decoding the output.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Put model in evaluation mode\nmodel.eval()\n\n# Tokenize input\ninputs = countdown_chat_template(torch.tensor([33, 19, 27, 5]), # Numbers\n                                torch.tensor([39]),            # Answer\n                                tokenizer)\n\n# Move inputs to the same device as model\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n# Generate text (inference)\nwith torch.no_grad():  # Disable gradient calculation for inference\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=100,  # Control the length of generated text\n        temperature=0.7,     # Control randomness (lower = more deterministic)\n        top_p=0.9,           # Nucleus sampling parameter\n        do_sample=True,      # Use sampling instead of greedy decoding\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Image Observations in PPO\nDESCRIPTION: Example of configuring the neural network architecture for PPO when dealing with image observations, specifying CNN parameters for the encoder and MLP parameters for the head.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ppo.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Rainbow DQN Agent in AgileRL\nDESCRIPTION: Code snippet showing how to load a previously saved Rainbow DQN agent from a checkpoint file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn_rainbow.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.dqn_rainbow import RainbowDQN\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = RainbowDQN.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Multi-Input Observations in DQN\nDESCRIPTION: Example showing how to configure the neural network architecture for a DQN agent with dictionary/tuple observations containing combinations of image, discrete, and vector observations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Creating CQN Agent with Custom Network Configuration\nDESCRIPTION: Example of creating a CQN agent with a custom neural network configuration passed as a parameter.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/cql.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create CQN agent\nagent = CQN(\n  observation_space=observation_space,\n  action_space=action_space,\n  net_config=NET_CONFIG\n  )\n```\n\n----------------------------------------\n\nTITLE: Loading GRPO Agent from Checkpoint\nDESCRIPTION: Shows how to load a previously saved GRPO agent from a checkpoint file using the static load method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/grpo.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.grpo import GRPO\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = GRPO.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Creating DQN Agent with Custom Network Configuration\nDESCRIPTION: Example showing how to create a DQN agent with a custom neural network configuration by passing the NET_CONFIG parameter during initialization.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create DQN agent\nagent = DQN(\n  observation_space=observation_space,\n  action_space=action_space,\n  net_config=NET_CONFIG\n  )\n```\n\n----------------------------------------\n\nTITLE: Loading Trained Skill Agents\nDESCRIPTION: Loads previously trained skill agents and defines their execution parameters including skill duration. Creates a dictionary mapping skill indices to their respective agents and configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstabilize_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_stabilize.pt\"))\ncenter_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_center.pt\"))\nlanding_agent = PPO.load(os.path.join(save_dir, \"PPO_trained_agent_landing.pt\"))\n\ntrained_skills = {\n   0: {\"skill\": \"stabilize\", \"agent\": stabilize_agent, \"skill_duration\": 40},\n   1: {\"skill\": \"center\", \"agent\": center_agent, \"skill_duration\": 40},\n   2: {\"skill\": \"landing\", \"agent\": landing_agent, \"skill_duration\": 40},\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for PPO Training with AgileRL\nDESCRIPTION: Imports the necessary libraries for training PPO agents, including gymnasium for the environment, torch for neural networks, and AgileRL components for the PPO algorithm, hyperparameter optimization, and training utilities.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Author: Michael Pratt\nimport os\n\nfrom tqdm import trange\nimport imageio\nimport gymnasium as gym\nimport numpy as np\nimport torch\n\nfrom agilerl.algorithms.ppo import PPO\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.training.train_on_policy import train_on_policy\nfrom agilerl.utils.utils import (\n    create_population,\n    make_vect_envs,\n    observation_space_channels_to_first\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Mutations for AgileRL Agents in Python\nDESCRIPTION: Creates a Mutations object with specified probabilities for different types of mutations, including architecture, parameters, activation, and RL hyperparameters. This is used to evolve the population of agents during training.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmutations = Mutations(\n    no_mutation=MUT_P[\"NO_MUT\"],\n    architecture=MUT_P[\"ARCH_MUT\"],\n    new_layer_prob=MUT_P[\"NEW_LAYER\"],\n    parameters=MUT_P[\"PARAMS_MUT\"],\n    activation=MUT_P[\"ACT_MUT\"],\n    rl_hp=MUT_P[\"RL_HP_MUT\"],\n    mutation_sd=MUT_P[\"MUT_SD\"],\n    rand_seed=MUT_P[\"RAND_SEED\"],\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Loop for Rainbow DQN Agent Inference in Python\nDESCRIPTION: This code snippet shows a testing loop for the loaded Rainbow DQN agent. It runs the agent for a specified number of episodes, collects rewards, and captures frames for visualization. The environment used is CartPole-v1 with RGB array rendering.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrewards = []\nframes = []\ntesting_eps = 7\nmax_testing_steps = 1000\ntest_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\nwith torch.no_grad():\n    for ep in range(testing_eps):\n        state = test_env.reset()[0]  # Reset environment at start of episode\n        score = 0\n\n        for step in range(max_testing_steps):\n            # If your state is an RGB image\n            if INIT_HP[\"CHANNELS_LAST\"]:\n                state = obs_channels_to_first(state)\n\n            # Get next action from agent\n            action, *_ = rainbow_dqn.get_action(state, training=False)\n\n            # Save the frame for this step and append to frames list\n            frame = test_env.render()\n            frames.append(frame)\n\n            # Take the action in the environment\n            state, reward, terminated, truncated, _ = test_env.step(action)\n\n            # Collect the score of environment 0\n            score += reward\n\n            # Break if environment 0 is done or truncated\n            if terminated or truncated:\n                break\n\n        # Collect and print episodic reward\n        rewards.append(score)\n        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n        print(\"Episodic Reward: \", rewards[-1])\n\n    test_env.close()\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Networks for Image Observations in IPPO\nDESCRIPTION: Configuration for neural network architecture with image observations, specifying the CNN channel sizes for the encoder.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiPolicyImageEnv for Image-Based Multi-Policy Testing in Python\nDESCRIPTION: An image-based extension of MultiPolicyEnv that generates visual observations for testing multi-agent RL algorithms with computer vision components. The environment encodes states as simple patterns in image channels.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass MultiPolicyImageEnv(MultiPolicyEnv):\n    \"\"\"Image-based version of MultiPolicyEnv.\n\n    This environment returns image observations instead of vector states.\n    The image has n_patterns channels, with channel j having a value equal to the\n    pattern index at position j.\n\n    Args:\n        n_agents (int): number of agents in the environment\n        n_patterns (int): number of different action patterns\n        max_steps (int, optional): maximum number of steps per episode. Defaults to 10.\n        shared_reward (bool, optional): whether agents receive shared reward\n            or individual rewards. Defaults to False.\n        image_size (int, optional): size of observation images. Defaults to 5.\n    \"\"\"\n\n    def __init__(\n        self, n_agents, n_patterns, max_steps=10, shared_reward=False, image_size=5\n    ):\n        super().__init__(\n            n_agents=n_agents,\n            n_patterns=n_patterns,\n            max_steps=max_steps,\n            shared_reward=shared_reward,\n        )\n        self.image_size = image_size\n        # Override state size to be image dimensions\n        self.state_size = (self.n_patterns, self.image_size, self.image_size)\n\n    def reset(self):\n        \"\"\"Reset environment to initial state.\n\n        Returns:\n            list: initial image state for each agent\n        \"\"\"\n        # Reset the underlying environment to get the vector states\n        super().reset()\n        # Convert vector states to image states\n        self.image_states = self._convert_states_to_images()\n        return self.image_states\n\n    def step(self, actions):\n        \"\"\"Take a step in the environment.\n\n        Args:\n            actions (list): discrete actions for each agent\n\n        Returns:\n            tuple: (observations, rewards, dones, infos)\n        \"\"\"\n        # Call the parent step method to update vector states and get rewards/dones\n        _, rewards, dones, infos = super().step(actions)\n        # Convert updated vector states to image states\n        self.image_states = self._convert_states_to_images()\n        return self.image_states, rewards, dones, infos\n\n    def _convert_states_to_images(self):\n        \"\"\"Convert vector states to image observations.\n\n        Returns:\n            list: image observations for each agent\n        \"\"\"\n        image_states = []\n        for i in range(self.n_agents):\n            # Create empty image with n_patterns channels\n            obs = np.zeros(\n                (self.n_patterns, self.image_size, self.image_size), dtype=np.float32\n            )\n            # For each pattern position j, set all pixels in channel j to the pattern index\n            for j in range(self.n_patterns):\n                pattern_idx = int(self.states[i][j])\n                # Normalize the value to be between 0 and 1\n                obs[j, :, :] = pattern_idx / (self.n_patterns - 1)\n            image_states.append(obs)\n        return image_states\n```\n\n----------------------------------------\n\nTITLE: Example Network Configuration for AgileRL 2.0\nDESCRIPTION: This code snippet demonstrates how to configure a network in AgileRL 2.0, defining both an encoder configuration for image observation spaces using EvolvableCNN and a head configuration using EvolvableMLP. This structure supports the new evolvable network architecture system.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/get_started/agilerl2changes.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnet_config = {\n    # For an image observation space we encode observations using EvolvableCNN\n    \"encoder_config\": {\n        \"channel_size\": [32],\n        \"kernel_size\": [3],\n        \"stride_size\": [1],\n    }\n\n    # The head is usually an EvolvableMLP by default\n    \"head_config\": {\n        \"hidden_size\": [64, 64],\n    }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiAgentReplayBuffer in Python with Torch\nDESCRIPTION: Example code showing how to initialize a MultiAgentReplayBuffer object for storing experiences from multiple agents. The buffer is configured with a maximum size, specific field names to track, agent IDs, and a CUDA device for processing.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/components/multi_agent_replay_buffer.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\nimport torch\n\nfield_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\nmemory = MultiAgentReplayBuffer(memory_size=1_000_000,          # Max replay buffer size\n                                field_names=field_names,        # Field names to store in memory\n                                agent_ids=INIT_HP['AGENT_IDS'], # ID for each agent\n                                device=torch.device(\"cuda\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Experience Replay Buffer for TD3 Agents\nDESCRIPTION: Creates an experience replay buffer that allows off-policy TD3 agents to share experiences, enabling efficient learning from the collective exploration of the entire population and reducing the exploration burden on individual agents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Non-Evolvable Models with DummyEvolvable in DQN\nDESCRIPTION: Example of creating a basic PyTorch neural network and integrating it with AgileRL's DQN algorithm using the DummyEvolvable wrapper, which allows using non-evolvable models while still enabling RL hyperparameter and weight mutations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nfrom sgilerl.algorithms import DQN\nfrom agilerl.modules.dummy import DummyEvolvable\n\nclass BasicNetActorDQN(nn.Module):\n  def __init__(self, input_size, hidden_sizes, output_size):\n      super().__init__()\n      layers = []\n\n      # Add input layer\n      layers.append(nn.Linear(input_size, hidden_sizes[0]))\n      layers.append(nn.ReLU())  # Activation function\n\n      # Add hidden layers\n      for i in range(len(hidden_sizes) - 1):\n          layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n          layers.append(nn.ReLU())  # Activation function\n\n      # Add output layer with a sigmoid activation\n      layers.append(nn.Linear(hidden_sizes[-1], output_size))\n\n      # Combine all layers into a sequential model\n      self.model = nn.Sequential(*layers)\n\n  def forward(self, x):\n      return self.model(x)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nactor_kwargs = {\n    \"input_size\": 4,  # Input size\n    \"hidden_sizes\": [64, 64],  # Hidden layer sizes\n    \"output_size\": 2  # Output size\n}\n\nactor = DummyEvolvable(BasicNetActor, actor_kwargs, device=device)\n\n# Use the actor in an algorithm\nobservation_space = ...\naction_space = ...\npopulation = DQN.population(\n    size=4,\n    observation_space=observation_space,\n    action_space=action_space\n    actor_network=actor\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiPolicyEnv for Testing Multi-Policy Agents in Python\nDESCRIPTION: A multi-agent environment designed to test policy-based learning where each agent has a distinct optimal policy. This environment creates agent-specific state/action patterns to verify agents can independently learn optimal behaviors.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MultiPolicyEnv:\n    \"\"\"Multi-agent environment with different optimal policies for each agent.\n\n    Each agent has a different pattern of optimal actions to take based on its state.\n    The state is n_patterns indices between 0 and n_patterns-1, and the optimal action\n    for agent i at index j is (i + j) % n_patterns.\n\n    Args:\n        n_agents (int): number of agents in the environment\n        n_patterns (int): number of different action patterns\n        max_steps (int, optional): maximum number of steps per episode. Defaults to 10.\n        shared_reward (bool, optional): whether agents receive shared reward\n            or individual rewards. Defaults to False.\n    \"\"\"\n\n    def __init__(self, n_agents, n_patterns, max_steps=10, shared_reward=False):\n        self.n_agents = n_agents\n        self.n_patterns = n_patterns\n        self.shared_reward = shared_reward\n        self.max_steps = max_steps\n        self.state_size = n_patterns\n        self.action_size = 1  # Discrete action space with n_patterns possible actions\n        self.cont_actions = False\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset environment to initial state.\n\n        Returns:\n            list: initial state for each agent\n        \"\"\"\n        self.steps = 0\n        self.dones = [False for _ in range(self.n_agents)]\n        # Each agent gets random state indices\n        self.states = []\n        for _ in range(self.n_agents):\n            # Generate n_patterns random indices between 0 and n_patterns-1\n            state = np.zeros(self.state_size, dtype=np.float32)\n            for i in range(self.n_patterns):\n                # One-hot encoding of pattern index at each position\n                pattern_idx = np.random.randint(0, self.n_patterns)\n                state[i] = pattern_idx\n            self.states.append(state)\n        return self.states\n\n    def step(self, actions):\n        \"\"\"Take a step in the environment.\n\n        Args:\n            actions (list): discrete actions for each agent\n\n        Returns:\n            tuple: (observations, rewards, dones, infos)\n        \"\"\"\n        rewards = []\n\n        # Process actions\n        processed_actions = []\n        for action in actions:\n            if isinstance(action, np.ndarray):\n                action = action.reshape(-1)\n                action = action[0]  # Get the discrete action\n            if isinstance(action, (list, tuple)):\n                action = action[0]\n            processed_actions.append(int(action))\n\n        if self.shared_reward:\n            # Shared reward is sum of all individual rewards\n            reward = 0.0\n            for i in range(self.n_agents):\n                agent_reward = self._get_agent_reward(i, processed_actions[i])\n                reward += agent_reward\n            rewards = [reward for _ in range(self.n_agents)]\n        else:\n            for i in range(self.n_agents):\n                agent_reward = self._get_agent_reward(i, processed_actions[i])\n                rewards.append(agent_reward)\n\n        # Update states with new random patterns\n        for i in range(self.n_agents):\n            state = np.zeros(self.state_size, dtype=np.float32)\n            for j in range(self.n_patterns):\n                pattern_idx = np.random.randint(0, self.n_patterns)\n                state[j] = pattern_idx\n            self.states[i] = state\n\n        self.steps += 1\n        if self.steps >= self.max_steps:\n            self.dones = [True for _ in range(self.n_agents)]\n\n        infos = [{} for _ in range(self.n_agents)]\n        return self.states, rewards, self.dones, infos\n\n    def _get_agent_reward(self, agent_idx, action):\n        \"\"\"Calculate reward for an agent based on its action and state.\n\n        The optimal action for agent i at state index j is (i + state[j]) % n_patterns.\n\n        Args:\n            agent_idx (int): index of the agent\n            action (int): action taken by the agent\n\n        Returns:\n            float: reward for the agent\n        \"\"\"\n        reward = 0.0\n        state = self.states[agent_idx]\n\n        # Check action against all pattern positions\n        for j in range(self.n_patterns):\n            pattern_idx = int(state[j])\n            optimal_action = (agent_idx + pattern_idx) % self.n_patterns\n            if action == optimal_action:\n                reward += 1.0\n\n        # Normalize reward between 0 and 1\n        reward /= self.n_patterns\n        return reward\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Image Observations in NeuralTS\nDESCRIPTION: Configuration for the neural network architecture when using image observations with the NeuralTS agent, including CNN parameters for the encoder.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ts.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Agent CNN Actor for RGB Image States in Python\nDESCRIPTION: This snippet defines a CNN-based actor network for multi-agent systems dealing with RGB image states. It includes convolutional layers, max-pooling, and fully connected layers with a Gumbel-Softmax output activation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.networks.custom_activation import GumbelSoftmax\n\nclass MultiAgentCNNActor(nn.Module):\n  def __init__(self):\n  super().__init__()\n    self.conv1 = nn.Conv3d(\n       in_channels=4, out_channels=16, kernel_size=(1, 3, 3), stride=4\n    )\n    self.conv2 = nn.Conv3d(\n          in_channels=16, out_channels=32, kernel_size=(1, 3, 3), stride=2\n    )\n    # Define the max-pooling layers\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    # Define fully connected layers\n    self.fc1 = nn.Linear(15200, 256)\n    self.fc2 = nn.Linear(256, 2)\n\n    # Define activation function\n    self.relu = nn.ReLU()\n\n    # Define output activation\n    self.output_activation = GumbelSoftmax()\n\n  def forward(self, state_tensor):\n      # Forward pass through convolutional layers\n      x = self.relu(self.conv1(state_tensor))\n      x = self.relu(self.conv2(x))\n\n      # Flatten the output for the fully connected layers\n      x = x.view(x.size(0), -1)\n\n      # Forward pass through fully connected layers\n      x = self.relu(self.fc1(x))\n      x = self.output_activation(self.fc2(x))\n\n      return x\n```\n\n----------------------------------------\n\nTITLE: Multi-Agent Probe Environment Classes\nDESCRIPTION: Implementation of multi-agent probe environments that mirror the single-agent variants but adapted for multiple agents. These environments support testing multi-agent RL algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Multi-agent environment classes referenced:\n# ConstantRewardEnv\n# ConstantRewardImageEnv\n# ConstantRewardContActionsEnv\n# ObsDependentRewardEnv\n# ObsDependentRewardImageEnv\n# DiscountedRewardEnv\n# DiscountedRewardImageEnv\n# PolicyEnv\n# PolicyImageEnv\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages for AgileRL Tutorial\nDESCRIPTION: Imports necessary Python packages and classes for implementing hierarchical reinforcement learning with AgileRL, including torch, numpy, wandb for tracking, and AgileRL-specific imports.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nimport wandb\nfrom tqdm import trange\n\nfrom agilerl.algorithms.ppo import PPO\nfrom agilerl.training.train_on_policy import train_on_policy\nfrom agilerl.wrappers.learning import Skill\nfrom agilerl.utils.algo_utils import obs_channels_to_first\nfrom agilerl.utils.utils import (\n   create_population,\n   make_skill_vect_envs,\n   make_vect_envs,\n   observation_space_channels_to_first\n)\n```\n\n----------------------------------------\n\nTITLE: Agent Action Selection with Masking\nDESCRIPTION: Code showing how to handle agent masking in the training loop when getting actions from the agent.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/maddpg.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstate, info = env.reset()  # or: next_state, reward, done, truncation, info = env.step(action)\ncont_actions, discrete_action = agent.get_action(state, infos=info)\nif agent.discrete_actions:\n    action = discrete_action\nelse:\n    action = cont_actions\n```\n\n----------------------------------------\n\nTITLE: Loading and Updating Reinforcement Learning Models in Python\nDESCRIPTION: This code loads checkpoints for each model in the population, wraps the models, and updates step counters for each agent. It uses an accelerator for distributed training and includes a progress bar.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/distributed_training/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfor pop_i, model in enumerate(pop):\n    model.load_checkpoint(f\"{accel_temp_models_path}/DQN_{pop_i}.pt\")\naccelerator.wait_for_everyone()\nfor model in pop:\n    model.wrap_models()\n\n# Update step counter\nfor agent in pop:\n    agent.steps.append(agent.steps[-1])\n\npbar.close()\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Creating Vectorized Cartpole Environment for Rainbow DQN\nDESCRIPTION: This code snippet creates a vectorized Cartpole environment using AgileRL's make_vect_envs function. It initializes multiple environments to run in parallel and extracts the observation and action spaces for the Rainbow DQN agent.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnum_envs = 16\nenv = make_vect_envs(\"CartPole-v1\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP[\"CHANNELS_LAST\"]:\n    # Adjust dimensions for PyTorch API (C, H, W), for envs with RGB image states\n    observation_space = observation_space_channels_to_first(observation_space)\n```\n\n----------------------------------------\n\nTITLE: AgileRL Automated Training Setup - Python\nDESCRIPTION: Using AgileRL's finetune_llm function for automated training with checkpoint saving and monitoring.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfinetune_llm(\n    pop=[agent],\n    env=env,\n    evaluation_interval=10,\n    wb=True,\n    save_elite=True,\n    elite_path=\"path/to/model/directory\",\n    max_reward=2.0,\n    evo_steps=1,\n    accelerator=Accelerator()\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Reward Functions for LLM Reasoning Environment\nDESCRIPTION: Defines three reward functions for evaluating the LLM's outputs: a format reward checking for proper thinking tags, an equation reward verifying mathematical correctness, and a combined reward function that sums both rewards. The environment rewards both correct answers and proper reasoning format.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef format_reward_func(completions, target, **kwargs):\n    rewards = []\n\n    for completion, gt in zip(completions, target):\n        try:\n            # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n            completion = \"<think>\" + completion\n            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n            match = re.search(regex, completion, re.DOTALL)\n            if match is None or len(match.groups()) != 2:\n                rewards.append(0.0)\n            else:\n                rewards.append(1.0)\n        except Exception:\n            rewards.append(0.0)\n    return rewards\n\n\ndef equation_reward_func(completions, target, nums, **kwargs):\n    rewards = []\n\n    for completion, gt, numbers in zip(completions, target, nums):\n        try:\n            # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n            completion = \"<think>\" + completion\n            answer_tags = re.findall(r\"<answer>([\\s\\S]*?)<\\/answer>\", completion)\n\n            if len(answer_tags) != 1:\n                rewards.append(0.0)\n                continue\n\n            equation = answer_tags[0].strip()\n            used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n\n            if sorted(used_numbers) != sorted(numbers):\n                print(f\"Numbers mismatch: {used_numbers} vs {numbers}\")\n                rewards.append(0.0)\n                continue\n\n            allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n            if not re.match(allowed_pattern, equation):\n                print(f\"Equation format invalid: {equation}\")\n                rewards.append(0.0)\n                continue\n\n            result = eval(equation, {\"__builtins__\": None}, {})\n\n            if abs(float(result) - float(gt)) < 1e-5:\n                rewards.append(1.0)\n            else:\n                print(f\"Result {result} doesn't match target {gt}\")\n                rewards.append(0.0)\n        except Exception as e:\n            print(f\"Equation error: {e}\")\n            rewards.append(0.0)\n    return rewards\n\n\ndef combined_rewards(completion, solution, prompt):\n    reward = (\n        equation_reward_func([completion], [solution], [prompt])[0]\n        + format_reward_func([completion], [solution])[0]\n    )\n\n    print(\n        f\"\"\"\n    ============================================, \\n\n    Completion: {completion}, \\n\n    Numbers: {prompt}, \\n\n    Gospel Answer: {solution.item()} \\n\n    Reward: {reward}\n    \"\"\"\n    )\n    # Save successful countdown  comletions\n    if reward == 2.0:\n        with open(\"countdown_completions.txt\", \"a\") as text_file:\n            text_file.write(completion + \"\\n\" + \"=\"*50 + \"\\n\")\n\n\n    return reward\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Vector Observations in NeuralTS\nDESCRIPTION: Configuration for the neural network architecture when using discrete or vector observations with the NeuralTS agent.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ts.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Initializing Base Model and Dataset for LLM Finetuning\nDESCRIPTION: Creates a 1.5B parameter Qwen model with LoRA configuration for efficient finetuning, and loads the Countdown dataset. The code sets up the model with flash attention and prepares train/test splits from the dataset.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nMODEL_PATH = \"Qwen/Qwen2.5-1.5B\"\nDATASET = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n\ndef create_model(pretrained_model_name_or_path):\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    )\n    peft_config = LoraConfig(\n        r=32,\n        lora_alpha=32,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"down_proj\",\n            \"gate_proj\",\n        ],\n        task_type=\"CAUSAL_LM\",\n        lora_dropout=0.05,\n    )\n    model = get_peft_model(model, peft_config)\n    return model\n\ndef make_dataset(dataset_name: str) -> Tuple[Dataset, Dataset]:\n    raw_dataset = (\n        load_dataset(DATASET, split=\"train\").shuffle(seed=42).select(range(50000))\n    )\n    raw_dataset = raw_dataset.rename_column(\"target\", \"answer\")\n    raw_dataset = raw_dataset.rename_column(\"nums\", \"question\")\n    train_test_split = raw_dataset.train_test_split(test_size=0.1)\n    train_dataset = train_test_split[\"train\"]\n    test_dataset = train_test_split[\"test\"]\n    return train_dataset, test_dataset\n\n# Instantiate the model and the associated tokenizer\nmodel = create_model(pretrained_model_name_or_path=MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\ntokenizer.pad_token = tokenizer.eos_token\ntrain_dataset, test_dataset = make_dataset(DATASET)\n```\n\n----------------------------------------\n\nTITLE: Connect Four Game Setup Configuration - Python\nDESCRIPTION: Setup code for initializing the Connect Four environment with CUDA support and curriculum learning configuration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"===== AgileRL Curriculum Learning Demo =====\")\n\nlesson_number = 1\n\n# Load lesson for curriculum\nwith open(f\"./curriculums/connect_four/lesson{lesson_number}.yaml\") as file:\n    LESSON = yaml.safe_load(file)\n\n# Define the network configuration\nNET_CONFIG = {\n    \"encoder_config\": {\n```\n\n----------------------------------------\n\nTITLE: Environment Testing Functions\nDESCRIPTION: Helper functions for testing different RL algorithms with probe environments. Includes functions for testing Q-learning and policy-based methods.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Testing functions referenced:\ncheck_q_learning_with_probe_env\ncheck_policy_q_learning_with_probe_env\ncheck_policy_on_policy_with_probe_env\n```\n\n----------------------------------------\n\nTITLE: Filling Replay Buffer with Random Experiences\nDESCRIPTION: Implements buffer warmup by filling it with random experiences and optionally training agents on these experiences before main training loop.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Perform buffer and agent warmups if desired\nif LESSON[\"buffer_warm_up\"]:\n   warm_up_opponent = Opponent(env, difficulty=LESSON[\"warm_up_opponent\"])\n   memory = env.fill_replay_buffer(\n         memory, warm_up_opponent\n   )  # Fill replay buffer with transitions\n   if LESSON[\"agent_warm_up\"] > 0:\n         print(\"Warming up agents ...\")\n         agent = pop[0]\n\n         # Train on randomly collected samples\n         for epoch in trange(LESSON[\"agent_warm_up\"]):\n            experiences = memory.sample(agent.batch_size)\n            agent.learn(experiences)\n\n         pop = [agent.clone() for _ in pop]\n         elite = agent\n         print(\"Agent population warmed up.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Image Observations in Rainbow DQN\nDESCRIPTION: Code snippet showing the configuration options for a neural network processing image observations in Rainbow DQN, including CNN parameters like channel size, kernel size, and stride size.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn_rainbow.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Template and Data Collation Functions for LLM Training\nDESCRIPTION: Defines functions for creating chat templates and collating training data batches. The countdown_chat_template formats prompts for arithmetic reasoning tasks while custom_collate_fn handles batch preparation with padding.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef countdown_chat_template(q, a, tokenizer):\n    conversation = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant. You first think about the reasoning process in your mind and then provide the user with the answer.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Using each number in this tensor only once {tuple(i.item() for i in q)}, create an equation that equals {a.item()}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\",\n        },\n        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n    ]\n    updated_prompt = tokenizer.apply_chat_template(\n        conversation, tokenize=False, continue_final_message=True\n    )\n    tokenized_prompt = tokenizer(\n        [updated_prompt],\n        return_tensors=\"pt\",\n        padding=True,\n        padding_side=\"left\",\n        return_attention_mask=True,\n    )\n    return tokenized_prompt\n\ndef custom_collate_fn(batch):\n    # Extract answers and questions\n    answers = torch.tensor([item[\"answer\"] for item in batch])\n\n    # For questions of variable length, we need to pad them\n    # First, find the maximum length\n    max_len = max(len(item[\"question\"]) for item in batch)\n\n    # Create padded tensor\n    questions = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, item in enumerate(batch):\n        q_len = len(item[\"question\"])\n        questions[i, :q_len] = torch.tensor(item[\"question\"])\n\n    return {\"answer\": answers, \"question\": questions}\n```\n\n----------------------------------------\n\nTITLE: Configuring MATD3 Neural Network for Discrete/Vector Observations (Python)\nDESCRIPTION: Example of configuring the neural network architecture for MATD3 with discrete or vector observations using a kwargs dictionary.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/matd3.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Saving Test Episodes as GIF in Python\nDESCRIPTION: This snippet shows how to save the rendered frames from the test episodes as a GIF file using the imageio library.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ngif_path = \"./videos/\"\nos.makedirs(gif_path, exist_ok=True)\nimageio.mimwrite(os.path.join(\"./videos/\", \"ppo_acrobot.gif\"), frames, loop=0)\nmean_fitness = np.mean(rewards)\n```\n\n----------------------------------------\n\nTITLE: YAML Config for Connect Four Lesson 1\nDESCRIPTION: Configuration settings for the first curriculum lesson, defining reward shaping parameters and opponent selection.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlesson: 1\nwarm_up_opponent: \"random\"\nvertical_win_reward: 0.8\nthree_in_row_reward: 0.05\nopp_three_in_row_penalty: -0.05\nblock_vert_coef: 0.5\n```\n\n----------------------------------------\n\nTITLE: Rendering Hierarchical Policy in AgilRL\nDESCRIPTION: Code for visualizing a hierarchical policy selector in AgilRL. This allows users to render and observe how the hierarchical policy makes decisions.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n../../../tutorials/Skills/render_agilerl_selector.py\n```\n\n----------------------------------------\n\nTITLE: Single-Agent Probe Environment Classes\nDESCRIPTION: Contains multiple classes for single-agent environments with different reward mechanics including constant rewards, observation-dependent rewards, discounted rewards, and policy-based environments. Each class implements standard gym interface.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/utils/probe_envs.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Classes referenced but code not shown in excerpt:\n# ConstantRewardEnv\n# ConstantRewardImageEnv\n# ConstantRewardContActionsEnv\n# ConstantRewardContActionsImageEnv\n# ObsDependentRewardEnv\n# ObsDependentRewardImageEnv\n# DiscountedRewardEnv\n# DiscountedRewardImageEnv\n# PolicyEnv\n# PolicyImageEnv\n```\n\n----------------------------------------\n\nTITLE: Initializing GRPO Agent with HuggingFace Environment\nDESCRIPTION: Example showing how to initialize a GRPO agent with a HuggingFace environment, including model and tokenizer setup. The configuration includes batch processing and memory optimization settings.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/grpo.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms import GRPO\nfrom agilerl.utils.llm_utils import HuggingFaceGym\n\nmodel = create_model(...)\ntokenizer = create_tokenizer(...)\nenv = HuggingFaceGym(...)\n\nagent = GRPO(\n  env.observation_space,\n  env.action_space,\n  actor_network=model,\n  pad_token_id=tokenizer.eos_token_id,\n  device=\"cuda:0\",\n  batch_size=8,\n  group_size=8,\n  reduce_memory_peak=True,\n)\n```\n\n----------------------------------------\n\nTITLE: YAML Config for Connect Four Lesson 2\nDESCRIPTION: Configuration settings for the second curriculum lesson.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlesson: 2\nwarm_up_opponent: \"random\"\nvertical_win_reward: 0.8\nthree_in_row_reward: 0.05\nopp_three_in_row_penalty: -0.05\nblock_vert_coef: 0.5\n```\n\n----------------------------------------\n\nTITLE: Neural Network Configuration for Image Observations\nDESCRIPTION: Configuration example for network architecture with image observations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/maddpg.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {\n        'channel_size': [32, 32], # CNN channel size\n        'kernel_size': [8, 4],   # CNN kernel size\n        'stride_size': [4, 2],   # CNN stride size\n      },\n      \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n    }\n```\n\n----------------------------------------\n\nTITLE: RainbowQNetwork Class Reference\nDESCRIPTION: API documentation for the RainbowQNetwork class, an enhanced Q-network implementing the Rainbow DQN algorithm features.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/networks/q_networks.rst#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.networks.q_networks.RainbowQNetwork\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Population Evaluation and Fitness Calculation in Python\nDESCRIPTION: Evaluates the population of agents by testing each agent in the environment and calculating fitness scores. It handles channel swapping and reports mean scores and fitnesses for each agent.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/on_policy/index.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfitnesses = [\n    agent.test(\n        env,\n        swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n        max_steps=eval_steps,\n        loop=eval_loop,\n    )\n    for agent in pop\n]\nmean_scores = [\n    (\n        np.mean(episode_scores)\n        if len(episode_scores) > 0\n        else \"0 completed episodes\"\n    )\n    for episode_scores in pop_episode_scores\n]\n\nprint(f\"--- Global steps {total_steps} ---\")\nprint(f\"Steps {[agent.steps[-1] for agent in pop]}\")\nprint(f\"Scores: {mean_scores}\")\nprint(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\nprint(\n    f'5 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-5:]) for agent in pop]}'\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Population with Custom Actor and Critic for PPO\nDESCRIPTION: Example of creating a population of PPO agents with custom evolvable actor and critic networks. Both networks are passed to the create_population function along with other necessary parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npop = create_population(\n        algo=\"PPO\",                                  # Algorithm\n        observation_space=observation_space,         # Observation space\n        action_space=action_space,                   # Action space\n        actor_network=evolvable_actor,               # Custom evolvable actor\n        critic_network=evolvable_critic,             # Custom evolvable critic\n        INIT_HP=INIT_HP,                             # Initial hyperparameters\n        population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n        device=device\n      )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Agent Experience Replay Buffer in Python\nDESCRIPTION: Initializes a MultiAgentReplayBuffer for storing and sampling experiences in multi-agent environments. This buffer enables off-policy algorithms to share memory within agent populations, allowing for more efficient learning from the collective experience of all agents in the environment.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/multi_agent_training/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n\nfield_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\nmemory = MultiAgentReplayBuffer(\n    INIT_HP[\"MEMORY_SIZE\"],\n    field_names=field_names,\n    agent_ids=INIT_HP[\"AGENT_IDS\"],\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Full SimBa Architecture in PyTorch\nDESCRIPTION: Defines a function to create the complete SimBa architecture with multiple residual blocks. It includes input and output linear layers, layer normalization, and optional output activation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef create_simba(\n    input_size: int,\n    output_size: int,\n    hidden_size: int,\n    num_blocks: int,\n    output_activation: Optional[str] = None,\n    scale_factor: float = 4.0,\n    device: DeviceType = \"cpu\",\n    name: str = \"simba\",\n) -> nn.Sequential:\n    \"\"\"Creates a number of SimBa residual blocks.\n\n    Paper: https://arxiv.org/abs/2410.09754.\n\n    :param input_size: Number of input features.\n    :type input_size: int\n    :param output_size: Number of output features.\n    :type output_size: int\n    :param hidden_size: Number of hidden units.\n    :type hidden_size: int\n    :param num_blocks: Number of residual blocks.\n    :type num_blocks: int\n    :param output_activation: Activation function for output layer.\n    :type output_activation: Optional[str]\n    :param scale_factor: Scale factor for the hidden layer.\n    :type scale_factor: float, optional\n    :param device: Device to use. Defaults to \"cpu\".\n    :type device: DeviceType, optional\n    :param name: Name of the network.\n    :type name: str, default \"simba\"\n\n    :return: Residual block.\n    :rtype: nn.Sequential\n    \"\"\"\n    net_dict: Dict[str, nn.Module] = OrderedDict()\n\n    # Initial dense layer\n    net_dict[f\"{name}_linear_layer_input\"] = nn.Linear(\n        input_size, hidden_size, device=device\n    )\n    nn.init.orthogonal_(net_dict[f\"{name}_linear_layer_input\"].weight)\n    for l_no in range(1, num_blocks + 1):\n        net_dict[f\"{name}_residual_block_{str(l_no)}\"] = SimbaResidualBlock(\n            hidden_size, scale_factor=scale_factor, device=device\n        )\n\n    # Final layer norm and output dense\n    net_dict[f\"{name}_layer_norm_output\"] = nn.LayerNorm(hidden_size, device=device)\n    net_dict[f\"{name}_linear_layer_output\"] = nn.Linear(\n        hidden_size, output_size, device=device\n    )\n    nn.init.orthogonal_(net_dict[f\"{name}_linear_layer_output\"].weight)\n\n    net_dict[f\"{name}_activation_output\"] = get_activation(\n        activation_name=output_activation\n    )\n\n    return nn.Sequential(net_dict)\n```\n\n----------------------------------------\n\nTITLE: Initializing Experience Replay Buffer for Bandit Learning\nDESCRIPTION: This code snippet shows how to initialize an Experience Replay Buffer for storing and sampling experiences during bandit training. The buffer is used to efficiently train a population of RL agents by allowing them to learn from shared experiences.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/bandits/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Mutation Parameters\nDESCRIPTION: Sets up mutation parameters for the evolutionary process, defining probabilities for different types of mutations and mutation characteristics.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmutations = Mutations(\n    no_mutation=MUT_P[\"NO_MUT\"],\n    architecture=0,\n    new_layer_prob=0,\n    parameters=0,\n    activation=0,\n    rl_hp=MUT_P[\"RL_HP_MUT\"],\n    mutation_sd=MUT_P[\"MUT_SD\"],\n    rand_seed=MUT_P[\"RAND_SEED\"],\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Training Loop Implementation\nDESCRIPTION: Defines utility functions for distributed training and implements a custom training loop with tensor gathering and metric aggregation across GPUs.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef gather_tensor(tensor: torch.Tensor, agent: GRPO) -> torch.Tensor:\n    \"\"\"Gather tensors from gpus\n\n    :param tensor: Tensor to gather\n    :type tensor: torch.Tensor\n    :param agent: GRPO agent object\n    :type agent: GRPO\n    :return: Stacked tensors\n    :rtype: torch.Tensor\n    \"\"\"\n    # Convert to tensor if it's a scalar\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.tensor(tensor, device=f\"cuda:{agent.local_rank}\")\n\n    if tensor.device != agent.device:\n        tensor = tensor.to(agent.device)\n    # Ensure tensor is on correct device\n    tensor = tensor.detach().clone()\n    # Create a list to store tensors from all processes\n    world_size = dist.get_world_size()\n    gathered_tensors = [torch.zeros_like(tensor) for _ in range(world_size)]\n\n    # Gather the tensor from all processes\n    dist.all_gather(gathered_tensors, tensor)\n    return torch.stack(gathered_tensors)\n\n\ndef aggregate_metrics_across_gpus(agent: GRPO, metric_tensor: torch.Tensor) -> float:\n    \"\"\"Aggregate gathered tensors\n\n    :param agent: GRPO agent\n    :type agent: GRPO\n    :param metric_tensor: Metrics\n    :type metric_tensor: torch.Tensor\n    :return: Mean metric\n    :rtype: float\n    \"\"\"\n    all_metrics = gather_tensor(metric_tensor, agent)\n    avg_metrics = all_metrics.mean().item()\n    return avg_metrics\n\nif accelerator is None or accelerator.is_main_process:\n    print(\"\\nTraining...\")\n\nbar_format = \"{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]\"\nmax_steps = len(env) // effective_data_batch_size\npbar = trange(\n    max_steps,\n    unit=\"step\",\n    bar_format=bar_format,\n    ascii=True,\n    dynamic_ncols=True,\n)\n\ntotal_steps = 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for TD3 with Image Observations in Python\nDESCRIPTION: This snippet demonstrates how to configure the neural network architecture for TD3 with image observations, specifying the CNN encoder and head configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/td3.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Game State Management Methods - Python\nDESCRIPTION: Methods for managing game state and rewards in Connect Four, including checking win conditions and updating game state.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef last(self) -> Tuple[dict, float, bool, bool, dict]:\n    \"\"\"Wrapper around PettingZoo env last method.\"\"\"\n    return self.env.last()\n\ndef step(self, action: int) -> None:\n    \"\"\"Wrapper around PettingZoo env step method.\"\"\"\n    self.env.step(action)\n\ndef reset(self) -> None:\n    \"\"\"Wrapper around PettingZoo env reset method.\"\"\"\n    self.env.reset()\n```\n\n----------------------------------------\n\nTITLE: Implementing Tournament Selection\nDESCRIPTION: Initializes tournament selection mechanism for agent population evolution with specified parameters for tournament size, elitism, and population size.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntournament = TournamentSelection(\n    INIT_HP[\"TOURN_SIZE\"],\n    INIT_HP[\"ELITISM\"],\n    INIT_HP[\"POP_SIZE\"],\n    INIT_HP[\"EVAL_LOOP\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring GRPO Hyperparameters and Population Creation\nDESCRIPTION: Sets up hyperparameter configuration for the GRPO algorithm and creates a population of agents using specified parameters and accelerators.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhp_config = HyperparameterConfig(\n    beta=RLParameter(min=mut_p[\"MIN_BETA\"], max=mut_p[\"MAX_BETA\"]),\n    lr=RLParameter(min=mut_p[\"MIN_LR\"], max=mut_p[\"MAX_LR\"]),\n    group_size=RLParameter(\n        min=mut_p[\"MIN_GROUP_SIZE\"], max=mut_p[\"MAX_GROUP_SIZE\"], dtype=int\n    ),\n)\n\npop = create_population(\n    algo=init_hp[\"ALGO\"],\n    observation_space=env.observation_space,\n    action_space=env.action_space,\n    net_config=None,\n    INIT_HP=init_hp,\n    hp_config=hp_config,\n    population_size=init_hp[\"POP_SIZE\"],\n    accelerator=accelerators,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing PPO Agent Training Parameters in Python\nDESCRIPTION: This snippet shows the initialization of training parameters for a PPO agent, including steps, evolution parameters, and save options.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmax_steps=INIT_HP[\"MAX_STEPS\"],\nevo_steps=INIT_HP[\"EVO_STEPS\"],\neval_steps=INIT_HP[\"EVAL_STEPS\"],\neval_loop=INIT_HP[\"EVAL_LOOP\"],\ntournament=tournament,\nmutation=mutations,\nwb=False,  # Boolean flag to record run with Weights & Biases\nsave_elite=True,  # Boolean flag to save the elite agent in the population\nelite_path=save_path,\n```\n\n----------------------------------------\n\nTITLE: Implementing Center Skill for LunarLander\nDESCRIPTION: Defines a CenterSkill class to teach the agent to move to the center of the environment while maintaining stability. Uses reward shaping to encourage centering behavior.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CenterSkill(Skill):\n   def __init__(self, env):\n      super().__init__(env)\n\n      self.x_center = 0\n      self.history = {\"y\": [], \"theta\": []}\n\n   def skill_reward(self, observation, reward, terminated, truncated, info):\n      if terminated or truncated:\n            reward = -1000.0\n            self.history = {\"y\": [], \"theta\": []}\n            return observation, reward, terminated, truncated, info\n\n      reward, terminated, truncated = 1.0, 0, 0\n      x, y, theta = observation[0], observation[1], observation[4]\n\n      # Ensure there are previous observations to compare with\n      if len(self.history[\"y\"]) == 0:\n            self.history[\"y\"].append(y)\n            self.history[\"theta\"].append(theta)\n            return observation, reward, terminated, truncated, info\n\n      # Minimise x distance to center\n      reward -= abs((self.x_center - x) * 2) ** 2\n      # Minimise y movement\n      reward -= (abs(self.history[\"y\"][-1] - y) * 10) ** 2\n      # Minimise tilt angle\n      reward -= (abs(self.history[\"theta\"][-1] - theta) * 10) ** 2\n\n      self.history[\"y\"].append(y)\n      self.history[\"theta\"].append(theta)\n\n      # Reset episode if longer than 300 steps\n      if len(self.history[\"y\"]) > 300:\n            reward = 10.0\n            terminated = True\n            self.history = {\"y\": [], \"theta\": []}\n            self.env.reset()\n\n      return observation, reward, terminated, truncated, info\n```\n\n----------------------------------------\n\nTITLE: Loading Trained PPO Agent for Inference in Python\nDESCRIPTION: This snippet shows how to load a saved PPO agent for inference using the AgileRL framework.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nppo = PPO.load(save_path, device=device)\n```\n\n----------------------------------------\n\nTITLE: Initializing HuggingFaceGym Environment\nDESCRIPTION: Creates accelerators for distributed training and initializes the HuggingFaceGym environment with necessary parameters including datasets, tokenizer, and reward functions.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Define accelerators for distributed training\naccelerators = [Accelerator() for _ in range(init_hp[\"POP_SIZE\"])]\n\n# Convert the HuggingFace dataset into a Gymnasium environment\nenv = HuggingFaceGym(\n    train_dataset=train_dataset,\n    test_dataset=test_dataset,\n    tokenizer=tokenizer,\n    reward_fn=combined_rewards,\n    apply_chat_template_fn=countdown_chat_template,\n    data_batch_size=8,\n    custom_collate_fn=custom_collate_fn,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Parameters in Python\nDESCRIPTION: This snippet sets up the main training parameters, including maximum steps, evolution frequency, and evaluation steps. It also initializes the total steps counter.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/offline_training/index.rst#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nmax_steps = 200000  # Max steps\n\nevo_steps = 10000  # Evolution frequency\neval_steps = None  # Evaluation steps per episode - go until done\neval_loop = 1  # Number of evaluation episodes\n\ntotal_steps = 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Image Observations in CQN\nDESCRIPTION: Configuration of CNN-based neural network architecture for CQN with image observations, specifying encoder parameters like channel size, kernel size, and stride size.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/cql.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Creating Vectorized Environment for PPO Training\nDESCRIPTION: Sets up a vectorized Acrobot environment using gymnasium, which allows for parallel sampling. It initializes the environment and extracts the observation and action spaces needed for agent creation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_ppo_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnum_envs=8\nenv = make_vect_envs(\"Acrobot-v1\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP[\"CHANNELS_LAST\"]:\n    # Adjust dimensions for PyTorch API (C, H, W), for envs with RGB image states\n    observation_space = observation_space_channels_to_first(observation_space)\n```\n\n----------------------------------------\n\nTITLE: Loading a NeuralUCB Agent from Checkpoint\nDESCRIPTION: Example of how to load a previously saved NeuralUCB agent from a checkpoint file using the static load method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ucb.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.neural_ucb import NeuralUCB\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = NeuralUCB.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Complex Observations in CQN\nDESCRIPTION: Configuration for handling dictionary or tuple observations containing combinations of image, discrete, and vector observations in CQN, using nested CNN and MLP configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/cql.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Setting Up Selector Training Environment\nDESCRIPTION: Initializes the training environment and population for the selector agent. Configures observation and action spaces, and sets up progress tracking with Weights & Biases integration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nenv = make_vect_envs(INIT_HP[\"ENV_NAME\"], num_envs=1)\n\nobservation_space = env.single_observation_space\n\naction_dim = len(trained_skills)\naction_space = spaces.Discrete(action_dim)\n\nif INIT_HP[\"CHANNELS_LAST\"]:\n   observation_space = observation_space_channels_to_first(observation_space)\n\npop = create_population(\n   algo=\"PPO\",\n   observation_space=observation_space,\n   action_space=action_space,\n   net_config=NET_CONFIG,\n   INIT_HP=INIT_HP,\n   population_size=INIT_HP[\"POPULATION_SIZE\"],\n   device=device,\n)\n\nif INIT_HP[\"WANDB\"]:\n   wandb.init(\n         project=\"EvoWrappers\",\n         name=\"{}-EvoHPO-{}-{}\".format(\n            INIT_HP[\"ENV_NAME\"],\n            INIT_HP[\"ALGO\"],\n            datetime.now().strftime(\"%m%d%Y%H%M%S\"),\n         ),\n         config={\n            \"algo\": f\"Evo HPO {INIT_HP['ALGO']}\",\n            \"env\": INIT_HP[\"ENV_NAME\"],\n            \"INIT_HP\": INIT_HP,\n         },\n   )\n\nbar_format = \"{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]\"\npbar = trange(\n  INIT_HP[\"MAX_STEPS\"],\n  unit=\"step\",\n  bar_format=bar_format,\n  ascii=True)\n\ntotal_steps = 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Vector Observations in DQN\nDESCRIPTION: Example showing how to configure the neural network architecture for a DQN agent with discrete or vector observation spaces, specifying encoder and head configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Connect Four Opponent Class Implementation - Python\nDESCRIPTION: Implementation of an opponent class with multiple difficulty levels (random, weak, strong) for Connect Four training and evaluation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass Opponent:\n    \"\"\"Connect 4 opponent to train and/or evaluate against.\n\n    :param env: Environment to learn in\n    :type env: PettingZoo-style environment\n    :param difficulty: Difficulty level of opponent, 'random', 'weak' or 'strong'\n    :type difficulty: str\n    \"\"\"\n\n    def __init__(self, env: ParallelEnv, difficulty: str):\n        self.env = env.env\n        self.difficulty = difficulty\n        if self.difficulty == \"random\":\n            self.get_action = self.random_opponent\n        elif self.difficulty == \"weak\":\n            self.get_action = self.weak_rule_based_opponent\n        else:\n            self.get_action = self.strong_rule_based_opponent\n        self.num_cols = 7\n        self.num_rows = 6\n        self.length = 4\n        self.top = [0] * self.num_cols\n```\n\n----------------------------------------\n\nTITLE: Saving a NeuralUCB Agent Checkpoint\nDESCRIPTION: Example of how to save a NeuralUCB agent to a checkpoint file using the save_checkpoint method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ucb.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.neural_ucb import NeuralUCB\n\nagent = NeuralUCB(observation_space, action_space)   # Create NeuralUCB agent\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Vector Observations in CQN\nDESCRIPTION: Configuration of neural network architecture for CQN with discrete or vector observations, specifying encoder and head configurations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/cql.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Image Observations\nDESCRIPTION: Configuration setup for DDPG neural network with image observations, specifying CNN parameters for the encoder.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ddpg.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Network for Vector Observations\nDESCRIPTION: Configuration for NeuralUCB's network architecture when using discrete or vector observations. Specifies encoder and head hidden layer sizes.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ucb.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Adding and Removing Blocks in EvolvableSimBa Neural Network\nDESCRIPTION: These methods add or remove hidden layers (blocks) in the neural network, with fallback operations if limits are reached.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n@mutation(MutationType.LAYER)\ndef add_block(self) -> None:\n    \"\"\"Adds a hidden layer to neural network. Falls back on add_node if\n    max hidden layers reached.\"\"\"\n    # add layer to hyper params\n    if self.num_blocks < self.max_blocks:  # HARD LIMIT\n        self.num_blocks += 1\n    else:\n        return self.add_node()\n\n@mutation(MutationType.LAYER)\ndef remove_block(self) -> None:\n    \"\"\"Removes a hidden layer from neural network. Falls back on remove_node if\n    min hidden layers reached.\"\"\"\n    if self.num_blocks > self.min_blocks:  # HARD LIMIT\n        self.num_blocks -= 1\n    else:\n        return self.add_node()\n```\n\n----------------------------------------\n\nTITLE: Saving CQN Agent Checkpoint in AgileRL\nDESCRIPTION: Example showing how to save a CQN agent to a checkpoint file for later use.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/cql.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.cqn import CQN\n\n# Create CQN agent\nagent = CQN(observation_space, action_space)\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Weights for Self-Play\nDESCRIPTION: Loads pretrained weights into the agent population and reinitializes optimizers for continued training.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.core.wrappers import OptimizerWrapper\n\nif LESSON[\"pretrained_path\"] is not None:\n   for agent in pop:\n         # Load pretrained checkpoint\n         agent.load_checkpoint(LESSON[\"pretrained_path\"])\n         # Reinit optimizer for new task\n         agent.lr = INIT_HP[\"LR\"]\n         agent.optimizer = OptimizerWrapper(\n            torch.optim.Adam,\n            networks=agent.actor,\n            lr=agent.lr,\n            network_names=agent.optimizer.network_names,\n            lr_name=agent.optimizer.lr_name,\n            optimizer_kwargs={\"capturable\": agent.capturable},\n         )\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom MLP Actor Network for RL\nDESCRIPTION: Example of creating a custom MLP (Multi-Layer Perceptron) actor network in PyTorch that can be used with reinforcement learning algorithms. This network takes an observation as input and outputs action values.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch\n\n\nclass MLPActor(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(MLPActor, self).__init__()\n\n        self.linear_layer_1 = nn.Linear(input_size, 64)\n        self.linear_layer_2 = nn.Linear(64, output_size)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.linear_layer_1(x))\n        x = self.linear_layer_2(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Loading IPPO Agent from Checkpoint\nDESCRIPTION: Shows how to load a previously saved IPPO agent from a checkpoint file using the static load method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms import IPPO\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = IPPO.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Recreating EvolvableSimBa Neural Network in Python\nDESCRIPTION: This method recreates the neural network with updated parameters while preserving existing parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef recreate_network(self) -> None:\n    \"\"\"Recreates neural networks.\n\n    :param shrink_params: Shrink parameters of neural networks, defaults to False\n    :type shrink_params: bool, optional\n    \"\"\"\n    model = create_simba(\n        input_size=self.num_inputs,\n        output_size=self.num_outputs,\n        hidden_size=self.hidden_size,\n        num_blocks=self.num_blocks,\n        output_activation=self.output_activation,\n        scale_factor=self.scale_factor,\n        device=self.device,\n        name=self.name,\n    )\n\n    self.model = EvolvableModule.preserve_parameters(\n        old_net=self.model, new_net=model\n    )\n```\n\n----------------------------------------\n\nTITLE: Custom Training Loop Implementation - Python\nDESCRIPTION: Detailed implementation of a custom training loop with distributed training support, metrics tracking, and checkpoint saving.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef gather_tensor(tensor: torch.Tensor, agent: GRPO) -> torch.Tensor:\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.tensor(tensor, device=f\"cuda:{agent.local_rank}\")\n    if tensor.device != agent.device:\n        tensor = tensor.to(agent.device)\n    tensor = tensor.detach().clone()\n    world_size = dist.get_world_size()\n    gathered_tensors = [torch.zeros_like(tensor) for _ in range(world_size)]\n    dist.all_gather(gathered_tensors, tensor)\n    return torch.stack(gathered_tensors)\n\ndef aggregate_metrics_across_gpus(agent: GRPO, metrics: torch.Tensor):\n    all_metrics = gather_tensor(metrics, agent)\n    avg_metrics = all_metrics.mean().item()\n    return avg_metrics\n\nevaluation_interval = 5\nmax_reward = 2.0\ncheckpoint_path=\"path/to/model/directory\"\n\nif agent.accelerator.is_main_process:\n    print(\"\\nTraining...\")\n\nbar_format = \"{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]\"\nmax_steps = len(env) // env.data_batch_size\nif agent.accelerator.is_main_process:\n    pbar = trange(\n        max_steps,\n        unit=\"step\",\n        bar_format=bar_format,\n        ascii=True,\n        dynamic_ncols=True,\n    )\n\nprompts = env.reset(reset_dataloaders=True)\nfor i in range(max_steps):\n    completion_ids, action_masks = agent.get_action(prompts)\n    next_prompts, rewards = env.step(completion_ids)\n    experiences = (\n        completion_ids,\n        action_masks,\n        rewards,\n    )\n    loss, kl = agent.learn(experiences)\n    metrics = [loss, kl, rewards]\n    if max_reward is not None:\n        accuracy = (rewards == max_reward).sum() / len(rewards.squeeze())\n        metrics.append(accuracy)\n    agg_metrics = [aggregate_metrics_across_gpus(agent, metric) for metric in metrics]\n    prompts = next_prompts\n    if agent.accelerator.is_main_process:\n        metrics = {\n                    \"Loss\": (agg_metrics[0]),\n                    \"KL-divergence\": (agg_metrics[1]),\n                    \"Mean training reward\": (agg_metrics[2]),\n                }\n        if max_reward is not None:\n            metrics |= {\"Accuracy\": (agg_metrics[3])}\n        print(\n            metrics\n        )\n        pbar.update(1)\n        if wb:\n            wandb.log(\n                metrics\n            )\n        if (i + 1) % evaluation_interval == 0:\n            test_reward = agent.test(env)\n            print(f\"Test reward: {test_reward}\")\n            if wb:\n                wandb.log({\"Test reward\": test_reward})\n        if (\n            checkpoint_path is not None\n            and checkpoint_interval is not None\n            and (i + 1) % checkpoint_interval == 0\n        ):\n            if agent.accelerator is not None:\n                unwrapped_model = agent.accelerator.unwrap_model(agent.actor)\n                unwrapped_model.save_pretrained(checkpoint_path)\n                print(f\"Saved checkpoint {save_path}\")\n            else:\n                agent.actor.save_pretrained(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Loading Fine-tuned LLM with transformers and PEFT in Python\nDESCRIPTION: Code to load a fine-tuned language model using the transformers library and PEFT (Parameter-Efficient Fine-Tuning). This snippet initializes a base model, tokenizer, and loads the fine-tuned parameters from a saved directory.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-3B\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\nmodel = PeftModel.from_pretrained(base_model, \"path/to/model/directory\")\n```\n\n----------------------------------------\n\nTITLE: Creating Vectorized Lunar Lander Environment\nDESCRIPTION: Sets up the continuous lunar lander environment from Gymnasium using AgileRL's vectorized environment wrapper, which enables parallel environment execution for more efficient training, and handles observation space configuration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_td3_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create vectorized environment\nnum_envs = 8\nenv = make_vect_envs(\"LunarLanderContinuous-v3\", num_envs=num_envs)  # Create environment\n\nobservation_space = env.single_observation_space\naction_space = env.single_action_space\nif INIT_HP[\"CHANNELS_LAST\"]:\n    # Adjust dimensions for PyTorch API (C, H, W), for envs with RGB image states\n    observation_space = observation_space_channels_to_first(observation_space)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Masking in MATD3 (Python)\nDESCRIPTION: Example of how to implement agent masking in a custom training loop for MATD3. This allows taking actions from agents at different timesteps by providing 'environment defined actions' for masked agents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/matd3.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nenv_defined_actions = {agent: info[agent][\"env_defined_actions\"] for agent in env.agents}\nstate, info = env.reset()  # or: next_state, reward, done, truncation, info = env.step(action)\ncont_actions, discrete_action = agent.get_action(state, env_defined_actions=env_defined_actions)\nif agent.discrete_actions:\n    action = discrete_action\nelse:\n    action = cont_actions\n```\n\n----------------------------------------\n\nTITLE: Saving NeuralTS Agent Checkpoint\nDESCRIPTION: Example of saving a trained NeuralTS agent to a checkpoint file for later use.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ts.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.neural_ts import NeuralTS\n\nagent = NeuralTS(observation_space, action_space)   # Create NeuralTS agent\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Agent Masking Info Dictionary Structure\nDESCRIPTION: Example of structuring the info dictionary for agent masking, showing how to specify environment-defined actions for different agents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/maddpg.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninfo = {'speaker_0': {'env_defined_actions':  None},\n            'listener_0': {'env_defined_actions': np.array([0,0,0,0,0])}\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Template and Data Collation - Python\nDESCRIPTION: Functions to prepare chat prompts and standardize data batches for training. Includes tokenization and padding logic for questions and answers.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef countdown_chat_template(q, a, tokenizer):\n    conversation = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant. You first think about the reasoning process in your mind and then provide the user with the answer.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Using each number in this tensor only once {tuple(i.item() for i in q)}, create an equation that equals {a.item()}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\",\n        },\n        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n    ]\n    updated_prompt = tokenizer.apply_chat_template(\n        conversation, tokenize=False, continue_final_message=True\n    )\n    tokenized_prompt = tokenizer(\n        [updated_prompt],\n        return_tensors=\"pt\",\n        padding=True,\n        padding_side=\"left\",\n        return_attention_mask=True,\n    )\n    return tokenized_prompt\n\ndef custom_collate_fn(batch):\n    answers = torch.tensor([item[\"answer\"] for item in batch])\n    max_len = max(len(item[\"question\"]) for item in batch)\n    questions = torch.zeros(len(batch), max_len, dtype=torch.long)\n    for i, item in enumerate(batch):\n        q_len = len(item[\"question\"])\n        questions[i, :q_len] = torch.tensor(item[\"question\"])\n    return {\"answer\": answers, \"question\": questions}\n```\n\n----------------------------------------\n\nTITLE: Filling Replay Buffer with Offline Data in Python\nDESCRIPTION: Initializes a replay buffer and fills it with transitions from an offline dataset. This prepares the data for use in training the offline reinforcement learning agents.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/offline_training/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.components.data import Transition\n\nmemory = ReplayBuffer(\n    max_size=10000,  # Max replay buffer size\n    device=device,\n)\n\nprint(\"Filling replay buffer with dataset...\")\n# Save transitions to replay buffer\ndataset_length = dataset[\"rewards\"].shape[0]\nfor i in trange(dataset_length - 1):\n    state = dataset[\"observations\"][i]\n    next_state = dataset[\"observations\"][i + 1]\n    if INIT_HP[\"CHANNELS_LAST\"]:\n        state = obs_channels_to_first(state)\n        next_state = obs_channels_to_first(next_state)\n    action = dataset[\"actions\"][i]\n    reward = dataset[\"rewards\"][i]\n    done = bool(dataset[\"terminals\"][i])\n\n    transition = Transition(\n        obs=state,\n        action=action,\n        reward=reward,\n        next_obs=next_state,\n        done=done,\n    )\n    transition = transition.unsqueeze(0) # Add vectorized dimension\n    transition.batch_size = [1]\n\n    # Save experience to replay buffer\n    memory.add(transition.to_tensordict())\n```\n\n----------------------------------------\n\nTITLE: Installing AgileRL Package\nDESCRIPTION: Commands for installing AgileRL either via pip package manager or in development mode from source code.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/get_started/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install agilerl\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/AgileRL/AgileRL.git && cd AgileRL\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Loading MATD3 Agent from Checkpoint (Python)\nDESCRIPTION: Example of loading a saved MATD3 agent from a checkpoint file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/matd3.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.matd3 import MATD3\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = MATD3.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Saving GRPO Agent Checkpoint\nDESCRIPTION: Demonstrates how to save a GRPO agent's state to a checkpoint file. This includes initializing the agent with environment and model parameters and using the save_checkpoint method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/grpo.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.grpo import GRPO\n\nagent = GRPO(\n  env.observation_space,\n  env.action_space,\n  actor_network=model,\n  pad_token_id=tokenizer.eos_token_id,\n)\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for LLM Reasoning with GRPO\nDESCRIPTION: Imports necessary libraries and modules for implementing GRPO-based LLM finetuning, including torch, transformers, datasets, and AgileRL components.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom typing import Tuple\nimport torch\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom agilerl.algorithms import GRPO\nfrom agilerl.training.train_llm import finetune_llm\nfrom agilerl.utils.llm_utils import HuggingFaceGym\n```\n\n----------------------------------------\n\nTITLE: Configuring Mutation Parameters in Python\nDESCRIPTION: Defines mutation parameters for evolutionary optimization including probabilities for different types of mutations and mutation strength.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nMUTATION_PARAMS = {\n    # Relative probabilities\n    'NO_MUT': 0.4,                              # No mutation\n    'ARCH_MUT': 0.2,                            # Architecture mutation\n    'NEW_LAYER': 0.2,                           # New layer mutation\n    'PARAMS_MUT': 0.2,                          # Network parameters mutation\n    'ACT_MUT': 0,                               # Activation layer mutation\n    'RL_HP_MUT': 0.2,                           # Learning HP mutation\n    'MUT_SD': 0.1,                              # Mutation strength\n    'RAND_SEED': 1,                             # Random seed\n}\n```\n\n----------------------------------------\n\nTITLE: Saving MATD3 Agent Checkpoint (Python)\nDESCRIPTION: Example of saving an MATD3 agent checkpoint to a file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/matd3.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.matd3 import MATD3\n\n# Create MATD3 agent\nagent = MATD3(\n  observation_spaces=observation_spaces,\n  action_spaces=action_spaces,\n  agent_ids=agent_ids,\n  net_config=NET_CONFIG\n  )\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Masking with Environment-Defined Actions in Python\nDESCRIPTION: Shows how to implement agent masking where some agents take environment-defined actions while others receive new actions from the IPPO agent. Useful for asynchronous agent action timing.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninfo = {'speaker_0': {'env_defined_actions':  None},\n        'listener_0': {'env_defined_actions': np.array([0,0,0,0,0])}\n```\n\n----------------------------------------\n\nTITLE: Network Recreation Method Implementation in Python\nDESCRIPTION: Method to recreate the network while preserving parameters, including value and advantage networks. Uses super() to handle base network recreation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_rainbow_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef recreate_network(self) -> None:\n    \"\"\"Recreates the network with the same parameters.\"\"\"\n\n    # Recreate value net with the same parameters\n    super().recreate_network()\n\n    advantage_net = create_mlp(\n        input_size=self.num_inputs,\n        output_size=self.num_actions * self.num_atoms,\n        hidden_size=self.hidden_size,\n        output_activation=self.output_activation,\n        output_vanish=self.output_vanish,\n        noisy=self.noisy,\n        init_layers=self.init_layers,\n        layer_norm=self.layer_norm,\n        activation=self.activation,\n        noise_std=self.noise_std,\n        device=self.device,\n        new_gelu=self.new_gelu,\n        name=\"advantage\",\n    )\n\n    self.advantage_net = EvolvableModule.preserve_parameters(\n        self.advantage_net, advantage_net\n    )\n```\n\n----------------------------------------\n\nTITLE: Training Configuration with LLM Finetuning\nDESCRIPTION: Implements the main training configuration using the finetune_llm function with specified parameters for evaluation, saving, and evolution steps.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfinetune_llm(\n    pop=pop,\n    env=env,\n    init_hp=init_hp,\n    evaluation_interval=10,\n    wb=True,\n    save_elite=True,\n    elite_path=\"path/to/model/directory\",\n    max_reward=2.0,\n    evo_steps=10,\n    mutation=mutations,\n    tournament=tournament,\n    accelerator=accelerators[0],\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Saving IPPO Agent Checkpoint\nDESCRIPTION: Example of saving an IPPO agent's state to a checkpoint file using the save_checkpoint method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms import IPPO\n\n# Create IPPO agent\nagent = IPPO(\n  observation_spaces=observation_spaces,\n  action_spaces=action_spaces,\n  agent_ids=agent_ids,\n  net_config=NET_CONFIG,\n  device=device,\n)\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neural Networks for Vector/Discrete Observations in IPPO\nDESCRIPTION: Configuration for neural network architecture with discrete or vector observations, specifying the encoder and head hidden layer sizes.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ippo.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n      \"encoder_config\": {'hidden_size': [32, 32]},  # Network head hidden size\n      \"head_config\": {'hidden_size': [32]}      # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Curriculum Environment Wrapper Implementation\nDESCRIPTION: A wrapper class that modifies the Connect Four environment to enable curriculum learning by adjusting rewards based on lesson configuration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom typing import List, Tuple, Optional\n\nfrom pettingzoo.parallel import ParallelEnv\n\nfrom agilerl.components.data import Transition\nfrom agilerl.components.replay_buffer import ReplayBuffer\n\nclass CurriculumEnv:\n    \"\"\"Wrapper around environment to modify reward for curriculum learning.\n\n    :param env: Environment to learn in\n    :type env: PettingZoo-style environment\n    :param lesson: Lesson settings for curriculum learning\n    :type lesson: dict\n    \"\"\"\n\n    def __init__(self, env: ParallelEnv, lesson: dict):\n        self.env = env\n        self.lesson = lesson\n\n    def fill_replay_buffer(\n        self, memory: ReplayBuffer, opponent: \"Opponent\"\n    ) -> ReplayBuffer:\n        \"\"\"Fill the replay buffer with experiences collected by taking random actions in the environment.\n\n        :param memory: Experience replay buffer\n        :type memory: AgileRL experience replay buffer\n        :param opponent: Opponent to train against\n        :type opponent: Opponent\n        :return: Filled replay buffer\n        :rtype: ReplayBuffer\n        \"\"\"\n        print(\"Filling replay buffer ...\")\n\n        pbar = tqdm(total=memory.max_size)\n        while len(memory) < memory.max_size:\n            # Randomly decide whether random player will go first or second\n            opponent_first = random.random() > 0.5\n\n            mem_full = len(memory)\n            self.reset()  # Reset environment at start of episode\n            observation, reward, done, truncation, _ = self.last()\n\n            (\n                p1_state,\n                p1_state_flipped,\n                p1_action,\n                p1_next_state,\n                p1_next_state_flipped,\n            ) = (None, None, None, None, None)\n            done, truncation = False, False\n\n            while not (done or truncation):\n                # Player 0's turn\n                p0_action_mask = observation[\"action_mask\"]\n                p0_state, p0_state_flipped = transform_and_flip(observation, player=0)\n                if opponent_first:\n                    p0_action = self.env.action_space(\"player_0\").sample(p0_action_mask)\n                else:\n                    if self.lesson[\"warm_up_opponent\"] == \"random\":\n                        p0_action = opponent.get_action(\n                            p0_action_mask, p1_action, self.lesson[\"block_vert_coef\"]\n                        )\n                    else:\n                        p0_action = opponent.get_action(player=0)\n                self.step(p0_action)  # Act in environment\n                observation, env_reward, done, truncation, _ = self.last()\n                p0_next_state, p0_next_state_flipped = transform_and_flip(\n                    observation, player=0\n                )\n\n                if done or truncation:\n```\n\n----------------------------------------\n\nTITLE: Detecting Vertical Wins in Connect Four\nDESCRIPTION: This method checks if a player has achieved a vertical win in Connect Four. It iterates through the game board to find four consecutive pieces of the same player in a vertical line.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef check_vertical_win(self, player: int) -> bool:\n    \"\"\"Checks if a win is vertical.\n\n    :param player: Player who we are checking, 0 or 1\n    :type player: int\n    \"\"\"\n    board = np.array(self.env.env.board).reshape(6, 7)\n    piece = player + 1\n\n    column_count = 7\n    row_count = 6\n\n    # Check vertical locations for win\n    for c in range(column_count):\n          for r in range(row_count - 3):\n             if (\n                board[r][c] == piece\n                and board[r + 1][c] == piece\n                and board[r + 2][c] == piece\n                and board[r + 3][c] == piece\n             ):\n                return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Saving Rainbow DQN Agent in AgileRL\nDESCRIPTION: Code snippet demonstrating how to save a trained Rainbow DQN agent to a checkpoint file for future use.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn_rainbow.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.dqn_rainbow import RainbowDQN\n\nagent = RainbowDQN(observation_space, action_space)   # Create Rainbow DQN agent\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Creating DDPG Agent with Custom Network Configuration\nDESCRIPTION: Example of initializing a DDPG agent with custom neural network configuration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ddpg.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create DDPG agent\nagent = DDPG(\n  observation_space=observation_space,\n  action_space=action_space,\n  net_config=NET_CONFIG\n  )\n```\n\n----------------------------------------\n\nTITLE: Adding and Removing Nodes in EvolvableSimBa Neural Network\nDESCRIPTION: These methods add or remove nodes from the residual blocks of the neural network, respecting specified limits.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n@mutation(MutationType.NODE)\ndef add_node(self, numb_new_nodes: Optional[int] = None) -> Dict[str, int]:\n    \"\"\"Adds nodes to residual blocks of the neural network.\n\n    :param numb_new_nodes: Number of nodes to add, defaults to None\n    :type numb_new_nodes: int, optional\n    \"\"\"\n    if numb_new_nodes is None:\n        numb_new_nodes = np.random.choice([16, 32, 64], 1)[0]\n\n    if self.hidden_size + numb_new_nodes <= self.max_mlp_nodes:  # HARD LIMIT\n        self.hidden_size += numb_new_nodes\n\n    return {\"numb_new_nodes\": numb_new_nodes}\n\n@mutation(MutationType.NODE)\ndef remove_node(self, numb_new_nodes: Optional[int] = None) -> Dict[str, int]:\n    \"\"\"Removes nodes from hidden layer of neural network.\n\n    :param hidden_layer: Depth of hidden layer to remove nodes from, defaults to None\n    :type hidden_layer: int, optional\n    :param numb_new_nodes: Number of nodes to remove from hidden layer, defaults to None\n    :type numb_new_nodes: int, optional\n    \"\"\"\n    if numb_new_nodes is None:\n        numb_new_nodes = np.random.choice([16, 32, 64], 1)[0]\n\n    # HARD LIMIT\n    if self.hidden_size - numb_new_nodes > self.min_mlp_nodes:\n        self.hidden_size -= numb_new_nodes\n\n    return {\"numb_new_nodes\": numb_new_nodes}\n```\n\n----------------------------------------\n\nTITLE: Checking Winnable Positions in Connect Four\nDESCRIPTION: This method checks if a list of four pieces represents a winnable opportunity in Connect Four. It counts the number of player pieces and empty spaces to determine if a win is possible.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef check_winnable(self, lst: List[int], piece: int) -> bool:\n    \"\"\"Checks if four pieces in a row represent a winnable opportunity, e.g. [1, 1, 1, 0] or [2, 0, 2, 2].\n\n    :param lst: List of pieces in row\n    :type lst: List\n    :param piece: Player piece we are checking (1 or 2)\n    :type piece: int\n    \"\"\"\n    return lst.count(piece) == 3 and lst.count(0) == 1\n```\n\n----------------------------------------\n\nTITLE: Styling Tile Layout for Tutorial Navigation in HTML/CSS\nDESCRIPTION: A CSS styling block that creates a responsive grid layout for tutorial navigation tiles. The styling includes hover effects, rounded corners, box shadows, and thumbnail image formatting to create an interactive navigation interface.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n/* CSS styles for tiles with rounded corners, centered titles, and always displayed algorithm list */\n\n/* Style for the container */\n.tiles {\n    display: grid;\n    grid-template-columns: repeat(auto-fill, minmax(150px, 0.25fr));\n    grid-auto-rows: 200px; /* 2 rows */\n    gap: 25px; /* Adjust the gap between tiles */\n    margin-top: 48px;\n    margin-bottom: 48px;\n    width: 100%;\n    align-content: start;\n    /*height: auto;\n}\n\n/* Style for each tile */\n.tile {\n    padding: 0px 0px; ; /* Fixed padding */\n    transition: background-color 0.3s ease; /* Smooth transition */\n    text-decoration: none;\n    width: auto; /* Fixed width */\n    height: auto; /* Fixed height */\n    overflow: hidden; /* Hide overflow content */\n    /* display: flex; /* Use flexbox for content alignment */\n    flex-direction: column; /* Align content vertically */\n    justify-content: center; /* Center content vertically */\n    align-items: flex-start;*/\n    background-color: transparent; /* Dark grey background */\n    border-radius: 7px; /* Rounded corners */\n    box-shadow: 0 4px 8px rgba(0, 150, 150, 0.5);\n    margin-bottom: 0px;\n\n}\n\n.column {\nflex: 1; /* Equal flex distribution */\nwidth: 50%; /* 50% width for each column */\ndisplay: flex;\nflex-direction: column;\n/* Additional styles */\n}\n\n/* Lighter background color on hover */\n.tile:hover {\n    background-color: #48b8b8; /* Lighter grey on hover */\n    color: white;\n}\n\n/* Title styles */\n.tile h2 {\n    font-size: 18px; /* Adjust the font size */\n    text-align: center; /* Center title text */\n    margin-top: 20px;\n    margin-bottom: 20px;\n    padding: 4px 4px 4px 4px;\n\n}\n\n\n/* Learn more link styles */\n.tile a {\n    display: block;\n    margin-bottom: 0px; /* Adjust the margin */\n    text-decoration: none;\n    /*color: white; /* Link color */\n    text-align: center; /* Center link text */\n    padding: 0px 0px;\n}\n\n.tile a:hover {\n    color: white; /* Link color on hover */\n}\n\n.thumbnail-image {\n    width: 100%;\n    height: 60%;\n    object-fit: cover;\n\n}\n```\n\n----------------------------------------\n\nTITLE: Loading CQN Agent from Checkpoint in AgileRL\nDESCRIPTION: Example demonstrating how to load a previously saved CQN agent from a checkpoint file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/cql.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.cqn import CQN\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = CQN.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Creating NeuralUCB Agent with Custom Network Configuration\nDESCRIPTION: Shows how to instantiate a NeuralUCB agent with a custom network configuration.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/neural_ucb.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nagent = NeuralUCB(observation_space, action_space, net_config=NET_CONFIG)   # Create NeuralUCB agent\n```\n\n----------------------------------------\n\nTITLE: Saving DQN Agent Checkpoint\nDESCRIPTION: Example showing how to save a trained DQN agent to a checkpoint file for later use.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.dqn import DQN\n\nagent = DQN(observation_space, action_space)   # Create DQN agent\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Saving TD3 Agent Checkpoint in Python\nDESCRIPTION: This snippet demonstrates how to save a TD3 agent's checkpoint to a specified path using the save_checkpoint method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/td3.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.td3 import TD3\n\nagent = TD3(observation_space, action_space)   # Create TD3 agent\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Custom Bandit Training Loop Implementation\nDESCRIPTION: This extensive code snippet provides a custom implementation of a bandit training loop using AgileRL. It includes environment setup, population creation, replay buffer initialization, and configuration of tournament selection and mutations for evolutionary optimization.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/bandits/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nfrom tensordict import TensorDict\nfrom tqdm import trange\nfrom ucimlrepo import fetch_ucirepo\n\nimport wandb\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.utils.utils import create_population\nfrom agilerl.wrappers.learning import BanditEnv\n\n\nif __name__ == \"__main__\":\nprint(\"===== AgileRL Bandit Demo =====\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNET_CONFIG = {\n    \"hidden_size\": [128],  # Actor hidden size\n}\n\nINIT_HP = {\n    \"BATCH_SIZE\": 64,  # Batch size\n    \"LR\": 1e-3,  # Learning rate\n    \"GAMMA\": 1.0,  # Scaling factor\n    \"LAMBDA\": 1.0,  # Regularization factor\n    \"REG\": 0.000625,  # Loss regularization factor\n    \"LEARN_STEP\": 2,  # Learning frequency\n    # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n    \"CHANNELS_LAST\": False,\n    \"POP_SIZE\": 4,  # Population size\n}\n\n# Fetch data  https://archive.ics.uci.edu/\niris = fetch_ucirepo(id=53)\nfeatures = iris.data.features\ntargets = iris.data.targets\n\nenv = BanditEnv(features, targets)  # Create environment\ncontext_dim = env.context_dim\naction_dim = env.arms\n\nobs_space = spaces.Box(low=features.values.min(), high=features.values.max())\naction_space = spaces.Discrete(action_dim)\npop = create_population(\n    algo=\"NeuralUCB\",  # Algorithm\n    observation_space=obs_space,  # Observation space\n    action_space=action_space,  # Action space\n    net_config=NET_CONFIG,  # Network configuration\n    INIT_HP=INIT_HP,  # Initial hyperparameters\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    device=device,\n)\n\nmemory = ReplayBuffer(max_size=10000, device=device)\n\ntournament = TournamentSelection(\n    tournament_size=2,  # Tournament selection size\n    elitism=True,  # Elitism in tournament selection\n    population_size=INIT_HP[\"POP_SIZE\"],  # Population size\n    eval_loop=1,  # Evaluate using last N fitness scores\n)\nmutations = Mutations(\n    no_mutation=0.4,  # No mutation\n    architecture=0.2,  # Architecture mutation\n    new_layer_prob=0.5,  # New layer mutation\n    parameters=0.2,  # Network parameters mutation\n    activation=0.2,  # Activation layer mutation\n    rl_hp=0.2,  # Learning HP mutation\n    mutation_sd=0.1,  # Mutation strength  # Network architecture\n    rand_seed=1,  # Random seed\n    device=device,\n)\n\nmax_steps = 10000  # Max steps per episode\n```\n\n----------------------------------------\n\nTITLE: Saving Trained Connect Four Agent Model\nDESCRIPTION: This code handles saving the trained elite agent model to the specified file path. It creates the necessary directories if they don't exist and displays a confirmation message when the model is successfully saved.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Save the trained agent\nsave_path = LESSON[\"save_path\"]\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nelite.save_checkpoint(save_path)\nprint(f\"Elite agent saved to '{save_path}'.\")\n\npbar.close()\n```\n\n----------------------------------------\n\nTITLE: Creating Rainbow DQN Agent with Custom Network Configuration\nDESCRIPTION: Example of initializing a Rainbow DQN agent with a custom neural network configuration using the net_config parameter.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn_rainbow.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create agent\nagent = RainbowDQN(\n  observation_space=observation_space,\n  action_space=action_space,\n  net_config=NET_CONFIG\n  )\n```\n\n----------------------------------------\n\nTITLE: Example of Language Model Reasoning Process\nDESCRIPTION: This text snippet showcases an example of the fine-tuned Qwen2.5-3B model's reasoning process. It demonstrates the model's 'aha' moment as it works through different calculations to arrive at the correct solution.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n58 - 1 = 57, then 57 - 25 = 32, and finally 32 + 1 = 33.\nHowever, 33 is not 82. Let's try another combination: 58 + 25 = 83, but we need 82.\nAha! If we use 58 + 1 = 59, then 59 - 25 = 34, and finally 34 - 1 = 33. This doesn't work either.\nHmm... what if we use 58 + 1 = 59, then 59 - 25 = 34, and finally 34 + 1 = 35.\nNope... closer, but not quite. What if we try 58 + 1 = 59, then 59 - 25 = 34, and finally 34 + 25 = 59.\nNope... still not 82.\nAh-ha! One more try: 58 - 1 = 57, then 57 + 25 = 82.</think>\n<answer>(58 - 1) + 25</answer>\n```\n\n----------------------------------------\n\nTITLE: Loading TD3 Agent from Checkpoint in Python\nDESCRIPTION: This snippet shows how to load a previously saved TD3 agent from a checkpoint using the load method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/td3.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.td3 import TD3\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = TD3.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Creating a Population with Custom Evolvable Actor for DQN\nDESCRIPTION: Example of creating a population of DQN agents with a custom evolvable actor network. The evolvable actor is passed to the create_population function along with other necessary parameters.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/custom_architecture/index.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npop = create_population(\n        algo=\"DQN\",                                  # Algorithm\n        observation_space=observation_space,         # Observation space\n        action_space=action_space,                   # Action space\n        actor_network=evolvable_actor,               # Custom evolvable actor\n        INIT_HP=INIT_HP,                             # Initial hyperparameters\n        population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n        device=device\n      )\n```\n\n----------------------------------------\n\nTITLE: Saving PPO Agent Checkpoint in AgileRL\nDESCRIPTION: Example of saving a trained PPO agent to a checkpoint file using the save_checkpoint method.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ppo.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.ppo import PPO\n\nagent = PPO(observation_space, action_space)   # Create PPO agent\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Accelerate Launch Command - Bash\nDESCRIPTION: Command line instruction to launch distributed training using accelerate.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch path/to/training_script\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading DDPG Agent\nDESCRIPTION: Examples showing how to save and load DDPG agent checkpoints for model persistence.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ddpg.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.ddpg import DDPG\n\nagent = DDPG(observation_space, action_space)   # Create DDPG agent\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.ddpg import DDPG\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = DDPG.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Installing AgileRL Package with pip\nDESCRIPTION: Command to install the AgileRL package using pip package manager. This is the recommended method for most users who want to use the library without modifying its source code.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install agilerl\n```\n\n----------------------------------------\n\nTITLE: ContinuousQNetwork Class Reference\nDESCRIPTION: API documentation for the ContinuousQNetwork class, designed for handling continuous action spaces in Q-learning.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/networks/q_networks.rst#2025-04-19_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.networks.q_networks.ContinuousQNetwork\n  :members:\n```\n\n----------------------------------------\n\nTITLE: MADDPG Agent Save and Load Operations\nDESCRIPTION: Examples of saving and loading MADDPG agents using checkpoint functionality.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/maddpg.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.maddpg import MADDPG\n\n# Create MADDPG agent\nagent = MADDPG(\n    observation_spaces=observation_spaces,\n    action_spaces=action_spaces,\n    agent_ids=agent_ids,\n    net_config=NET_CONFIG,\n    device=device,\n)\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent.save_checkpoint(checkpoint_path)\n\n# Loading saved agent\ncheckpoint_path = \"path/to/checkpoint\"\nagent = MADDPG.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Installing AgileRL in Development Mode\nDESCRIPTION: Commands to clone the AgileRL GitHub repository and install it in development mode. This approach is recommended for contributors or users who want to modify the library's source code.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/AgileRL/AgileRL.git && cd AgileRL\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Parameters for AgileRL\nDESCRIPTION: Sets up initial parameters for the training loop, including epsilon decay, evolution steps, and evaluation settings.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/off_policy/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\neps_decay = 0.995  # Decay per episode\nepsilon = eps_start\n\nevo_steps = 10000  # Evolution frequency\neval_steps = None  # Evaluation steps per episode - go until done\neval_loop = 1  # Number of evaluation episodes\n\ntotal_steps = 0\n```\n\n----------------------------------------\n\nTITLE: Retrieving Neural Network Configuration in Python\nDESCRIPTION: This method returns the model configuration as a dictionary, excluding certain attributes like num_inputs, num_outputs, device, and name.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/agilerl_simba_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef net_config(self) -> Dict[str, Any]:\n    \"\"\"Returns model configuration in dictionary.\"\"\"\n    net_config = self.init_dict.copy()\n    for attr in [\"num_inputs\", \"num_outputs\", \"device\", \"name\"]:\n        if attr in net_config:\n            net_config.pop(attr)\n\n    return net_config\n```\n\n----------------------------------------\n\nTITLE: Loading DQN Agent from Checkpoint\nDESCRIPTION: Example showing how to load a previously saved DQN agent from a checkpoint file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/dqn.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.algorithms.dqn import DQN\n\ncheckpoint_path = \"path/to/checkpoint\"\nagent = DQN.load(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Writing Completions to File - Python\nDESCRIPTION: Simple file write operation to save model completions with separator lines.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_finetuning.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"countdown_completions.txt\", \"a\") as text_file:\n    text_file.write(completion + \"\\n\" + \"=\"*50 + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Configuring PPO Training Parameters\nDESCRIPTION: Sets up the initial hyperparameters and configuration for training PPO agents, including network architecture, learning parameters, and training settings.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/skills/index.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n   \"encoder_config\": {\"hidden_size\": [64, 64]}  # Actor encoder hidden size\n}\n\nINIT_HP = {\n   \"ENV_NAME\": \"LunarLander-v3\",\n   \"ALGO\": \"PPO\",\n   \"POPULATION_SIZE\": 1,  # Population size\n   \"BATCH_SIZE\": 128,  # Batch size\n   \"LR\": 1e-3,  # Learning rate\n   \"LEARN_STEP\": 128,  # Learning frequency\n   \"GAMMA\": 0.99,  # Discount factor\n   \"GAE_LAMBDA\": 0.95,  # Lambda for general advantage estimation\n   \"ACTION_STD_INIT\": 0.6,  # Initial action standard deviation\n   \"CLIP_COEF\": 0.2,  # Surrogate clipping coefficient\n   \"ENT_COEF\": 0.01,  # Entropy coefficient\n   \"VF_COEF\": 0.5,  # Value function coefficient\n   \"MAX_GRAD_NORM\": 0.5,  # Maximum norm for gradient clipping\n   \"TARGET_KL\": None,  # Target KL divergence threshold\n   \"TARGET_SCORE\": 2000,\n   \"MAX_STEPS\": 1_000_000,\n   \"EVO_STEPS\": 10_000,\n   \"UPDATE_EPOCHS\": 4,  # Number of policy update epochs\n   # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n   \"CHANNELS_LAST\": False,\n   \"WANDB\": True,\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Directory to save trained agents and skills\nsave_dir = \"./models/PPO\"\nos.makedirs(save_dir, exist_ok=True)\n\nskills = {\n   \"stabilize\": StabilizeSkill,\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting MutationRegistry Class for AgileRL Mutations Registry in Python\nDESCRIPTION: Autoclass documentation for the MutationRegistry class in the agilerl.algorithms.core.registry module. This class likely manages the registry of mutations for reinforcement learning algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/registry.rst#2025-04-19_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: agilerl.algorithms.core.registry.MutationRegistry\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for AgileRL\nDESCRIPTION: A comprehensive list of Python package dependencies with version specifications required for the AgileRL project. The dependencies cover deep learning frameworks, reinforcement learning environments, data processing libraries, and utility packages.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\naccelerate==1.4.0\ndeepspeed==0.16.4\ndill==0.3.7\nfastrand==1.3.0\nflatten_dict==0.4.2\ngoogle-cloud-storage==2.5.0\ngymnasium>=1.0\nh5py==3.8.0\nhydra-core==1.3.2\njax[cpu]==0.4.31\nmatplotlib==3.9.4\nminari[all]==0.5.2\nnumpy==1.26.4\nomegaconf==2.3.0\npandas==2.0.3\npettingzoo==1.23.1\npre-commit==3.4.0\npygame==2.6.0\npymunk==6.2.0\nredis==4.4.4\nscipy==1.12.0\nSuperSuit==3.9.0\ntorch==2.5.1\ntensordict==0.6.*\ntermcolor==1.1.0\ntqdm==4.66.4\nucimlrepo==0.0.3\nwandb==0.17.6\nhuggingface\ndatasets==3.3.2\ntransformers==4.48.1\npeft==0.14.0\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LLM Finetuning with HPO\nDESCRIPTION: Imports necessary libraries for LLM finetuning with hyperparameter optimization, including torch, transformers, datasets, and AgileRL components for evolutionary hyperparameter selection.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/grpo_hpo.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom typing import Tuple\nimport torch\nimport yaml\nfrom accelerate import Accelerator\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom agilerl.algorithms.core.registry import HyperparameterConfig, RLParameter\nfrom agilerl.hpo.mutation import Mutations\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.training.train_llm import finetune_llm\nfrom agilerl.utils.llm_utils import HuggingFaceGym\nfrom agilerl.utils.utils import create_population\n```\n\n----------------------------------------\n\nTITLE: Saving Rainbow DQN Test Episodes as GIF in Python\nDESCRIPTION: This snippet shows how to save the captured frames from the testing episodes as a GIF file. It uses the imageio library to write the frames to a file with a specified duration between frames.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/agilerl_rainbow_dqn_tutorial.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ngif_path = \"./videos/\"\nos.makedirs(gif_path, exist_ok=True)\nimageio.mimwrite(\n    os.path.join(\"./videos/\", \"rainbow_dqn_cartpole.gif\"), frames, duration=10\n)\n```\n\n----------------------------------------\n\nTITLE: Documenting RLParameter Class for AgileRL Mutations Registry in Python\nDESCRIPTION: Autoclass documentation for the RLParameter class in the agilerl.algorithms.core.registry module. This class likely defines parameters for reinforcement learning algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/registry.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: agilerl.algorithms.core.registry.RLParameter\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Training with Accelerate\nDESCRIPTION: Command to launch distributed training using accelerate with a custom config file.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/distributed_training/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate_launch --config_file configs/accelerate/accelerate.yaml demo_online_distributed.py\n```\n\n----------------------------------------\n\nTITLE: Documenting PositionalEncoding Class in RST\nDESCRIPTION: Autodoc directive for the PositionalEncoding class, including all its members.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/bert.rst#2025-04-19_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.modules.bert.PositionalEncoding\n  :members:\n```\n\n----------------------------------------\n\nTITLE: YAML Config for Connect Four Lesson 3\nDESCRIPTION: Configuration settings for the third curriculum lesson.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/dqn.rst#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlesson: 3\nwarm_up_opponent: \"random\"\nvertical_win_reward: 0.8\nthree_in_row_reward: 0.05\nopp_three_in_row_penalty: -0.05\nblock_vert_coef: 0.5\n```\n\n----------------------------------------\n\nTITLE: Documenting Skill Class in AgileRL Learning Wrappers\nDESCRIPTION: reStructuredText documentation for the Skill class from agilerl.wrappers.learning. The documentation references the class and its members using autodoc directives.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/wrappers/learning.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.wrappers.learning.Skill\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Documenting MakeEvolvable Class in AgileRL\nDESCRIPTION: This code snippet uses Sphinx autodoc to generate documentation for the MakeEvolvable class. It includes all members of the class, providing details on its parameters and methods.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/wrappers/make_evolvable.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: agilerl.wrappers.make_evolvable.MakeEvolvable\n  :members:\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for EvolvableModule Class\nDESCRIPTION: ReStructuredText documentation structure for the EvolvableModule class which serves as a base class for evolvable neural network modules.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/base.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _base_module:\n\nEvolvableModule\n===============\n\nParameters\n------------\n\n.. autoclass:: agilerl.modules.base.EvolvableModule\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Documenting PositionalEncoder Class in RST\nDESCRIPTION: Autodoc directive for the PositionalEncoder class, including all its members.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/bert.rst#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.modules.bert.PositionalEncoder\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Referencing ValueNetwork Class in RST Documentation\nDESCRIPTION: A reStructuredText directive that references the ValueNetwork class from the agilerl.networks.value_networks module. The directive includes the :members: option to automatically document all class members.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/networks/value_networks.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.networks.value_networks.ValueNetwork\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Referencing ILQL Module in reStructuredText\nDESCRIPTION: A reStructuredText directive that references the ILQL module in the documentation. It uses Sphinx's autoclass directive to automatically generate documentation from the ILQL class including its members and inherited members.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/ilql.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.algorithms.ilql.ILQL\n  :members:\n  :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Documenting AgentWrapper Class in reStructuredText\nDESCRIPTION: This snippet defines the documentation structure for the AgentWrapper class from the agilerl.wrappers.agent module. It uses autoclass directive to automatically generate member documentation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/wrappers/agent.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: agilerl.wrappers.agent.AgentWrapper\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Documenting PettingZoo Wrapper Class in RST\nDESCRIPTION: ReStructuredText documentation directive for auto-documenting the PettingZooAutoResetParallelWrapper class from the agilerl.wrappers.pettingzoo_wrappers module.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/wrappers/pettingzoo.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.wrappers.pettingzoo_wrappers.PettingZooAutoResetParallelWrapper\n  :members:\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Tutorial Navigation Tiles\nDESCRIPTION: HTML markup defining a tile-based navigation system for tutorials. It creates three clickable tiles with thumbnail images and titles, each linking to a different tutorial for multi-agent reinforcement learning algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"tiles article\">\n   <a href=\"../pettingzoo/dqn.html\" class=\"tile\">\n      <img src=\"../../_images/connect_four_self_opp.gif\" alt=\"Connect4 gif\" class=\"thumbnail-image\">\n      <h2>DQN - Connect4</h2>\n   </a>\n   <a href=\"../pettingzoo/maddpg.html\" class=\"tile online\">\n   <img src=\"../../_images/atari_space_invaders.gif\" alt=\"Space Invaders gif\" class=\"thumbnail-image\">\n      <h2>MADDPG - Space Invaders</h2>\n   </a>\n   <a href=\"../pettingzoo/matd3.html\" class=\"tile online\">\n   <img src=\"../../_images/mpe_looped.gif\" alt=\"Speaker Listener gif\" class=\"thumbnail-image\">\n      <h2>MATD3 - Speaker Listener</h2>\n   </a>\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Documenting BanditEnv Class in AgileRL Learning Wrappers\nDESCRIPTION: reStructuredText documentation for the BanditEnv class from agilerl.wrappers.learning. The documentation references the class and its members using autodoc directives.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/wrappers/learning.rst#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.wrappers.learning.BanditEnv\n  :members:\n```\n\n----------------------------------------\n\nTITLE: HTML Tutorial Navigation Tiles for AgileRL\nDESCRIPTION: HTML markup that creates navigation tiles for AgileRL tutorials. Each tile contains a thumbnail image and a heading, linking to specific tutorial pages for Rainbow DQN and EvolvableSimBa implementations.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/custom_networks/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"tiles article\">\n  <a href=\"../custom_networks/agilerl_rainbow_tutorial.html\" class=\"tile\">\n       <img src=\"../../_static/thumbnails/rainbow_performance.png\" alt=\"Rainbow Performance\" class=\"thumbnail-image\">\n       <h2>Rainbow DQN</h2>\n  </a>\n  <a href=\"../custom_networks/agilerl_simba_tutorial.html\" class=\"tile\">\n       <img src=\"../../_static/thumbnails/simba_thumbnail.png\" alt=\"EvolvableSimBa\" class=\"thumbnail-image\">\n       <h2>EvolvableSimBa</h2>\n  </a>\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Styling Tile Layout for Tutorial Navigation with CSS\nDESCRIPTION: CSS styling for creating an interactive tile-based navigation system for tutorials. The styles define a grid layout with hover effects, rounded corners, and thumbnail images for each tutorial option.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/pettingzoo/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n/* CSS styles for tiles with rounded corners, centered titles, and always displayed algorithm list */\n\n/* Style for the container */\n.tiles {\n    display: grid;\n    grid-template-columns: repeat(auto-fill, minmax(150px, 0.25fr));\n    grid-auto-rows: 200px; /* 2 rows */\n    gap: 25px; /* Adjust the gap between tiles */\n    margin-top: 48px;\n    margin-bottom: 48px;\n    width: 100%;\n    align-content: start;\n    /*height: auto;\n}\n\n/* Style for each tile */\n.tile {\n    padding: 0px 0px; ; /* Fixed padding */\n    transition: background-color 0.3s ease; /* Smooth transition */\n    text-decoration: none;\n    width: auto; /* Fixed width */\n    height: auto; /* Fixed height */\n    overflow: hidden; /* Hide overflow content */\n    /* display: flex; /* Use flexbox for content alignment */\n    flex-direction: column; /* Align content vertically */\n    justify-content: center; /* Center content vertically */\n    align-items: flex-start;*/\n    background-color: transparent; /* Dark grey background */\n    border-radius: 7px; /* Rounded corners */\n    box-shadow: 0 4px 8px rgba(0, 150, 150, 0.5);\n    margin-bottom: 0px;\n\n}\n\n.column {\nflex: 1; /* Equal flex distribution */\nwidth: 50%; /* 50% width for each column */\ndisplay: flex;\nflex-direction: column;\n/* Additional styles */\n}\n\n/* Lighter background color on hover */\n.tile:hover {\n    background-color: #48b8b8; /* Lighter grey on hover */\n    color: white;\n}\n\n/* Title styles */\n.tile h2 {\n    font-size: 18px; /* Adjust the font size */\n    text-align: center; /* Center title text */\n    margin-top: 20px;\n    margin-bottom: 20px;\n    padding: 4px 4px 4px 4px;\n\n}\n\n\n/* Learn more link styles */\n.tile a {\n    display: block;\n    margin-bottom: 0px; /* Adjust the margin */\n    text-decoration: none;\n    /*color: white; /* Link color */\n    text-align: center; /* Center link text */\n    padding: 0px 0px;\n}\n\n.tile a:hover {\n    color: white; /* Link color on hover */\n}\n\n.thumbnail-image {\n    width: 100%;\n    height: 60%;\n    object-fit: cover;\n\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting RSNorm Class in reStructuredText\nDESCRIPTION: This snippet defines the documentation structure for the RSNorm class from the agilerl.wrappers.agent module. It uses autoclass directive to automatically generate member documentation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/wrappers/agent.rst#2025-04-19_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: agilerl.wrappers.agent.RSNorm\n  :members:\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation Reference for EvolvableLSTM\nDESCRIPTION: Sphinx documentation configuration that references the EvolvableLSTM class from the agilerl.modules.lstm module and includes its members in the generated documentation.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/lstm.rst#2025-04-19_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: agilerl.modules.lstm.EvolvableLSTM\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Styling Grid Layout and Tiles with CSS\nDESCRIPTION: CSS styles for creating responsive grid layouts with tiles featuring rounded corners and hover effects. Includes media queries for mobile and desktop views.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/get_started/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n@media (max-width: 750px) {\n      .tiles_2 {\n         display: grid;\n         grid-template-columns: 100%;\n         grid-auto-rows: 0% 50% 50% 0%;\n         gap: 25px;\n         margin-top: 0px;\n         margin-bottom: 58px;\n         width: 100%;\n         align-content: center;\n         height: auto;\n         min-height: 185px;\n      }\n\n      .tiles_3 {\n         display: grid;\n         grid-template-columns: 100%;\n         grid-auto-rows: 33%;\n         gap: 25px;\n         margin-top: 48px;\n         margin-bottom: 72px;\n         width: 100%;\n         align-content: start;\n         height: auto;\n         min-height: 185px;\n      }\n   }\n```\n\n----------------------------------------\n\nTITLE: Creating LLM Finetuning Tutorial Tiles with HTML\nDESCRIPTION: This HTML code creates a container with two tiles, each representing a different LLM finetuning tutorial. The tiles are styled using the CSS defined above and link to their respective tutorial pages.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<div class=\"tiles article\">\n   <a href=\"../llm_finetuning/grpo_finetuning.html\" class=\"tile\">\n      <h2>GRPO - Finetuning</h2>\n   </a>\n   <a href=\"../llm_finetuning/grpo_hpo.html\" class=\"tile online\">\n      <h2>GRPO - Finetuning with HPO</h2>\n   </a>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Tutorial Navigation Grid HTML Structure\nDESCRIPTION: HTML structure implementing a tile-based navigation grid for tutorials. Each tile contains a thumbnail image and title linking to specific tutorials for different reinforcement learning algorithms.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"tiles article\">\n      <a href=\"../gymnasium/agilerl_ppo_tutorial.html\" class=\"tile\">\n         <img src=\"../../_images/agilerl_ppo_acrobot.gif\" alt=\"Acrobot gif\" class=\"thumbnail-image\">\n         <h2>PPO - Acrobot</h2>\n      </a>\n      <a href=\"../gymnasium/agilerl_td3_tutorial.html\" class=\"tile online\">\n      <img src=\"../../_images/agilerl_td3_lunar_lander.gif\" alt=\"Acrobot gif\" class=\"thumbnail-image\">\n         <h2>TD3 - Lunar Lander</h2>\n      </a>\n      <a href=\"../gymnasium/agilerl_rainbow_dqn_tutorial.html\" class=\"tile online\">\n      <img src=\"../../_images/agilerl_rainbow_dqn_cartpole.gif\" alt=\"Acrobot gif\" class=\"thumbnail-image\">\n         <h2>Rainbow DQN - Cart-Pole</h2>\n      </a>\n   </div>\n```\n\n----------------------------------------\n\nTITLE: Documenting TokenEmbedding Class in RST\nDESCRIPTION: Autodoc directive for the TokenEmbedding class, including all its members.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/bert.rst#2025-04-19_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.modules.bert.TokenEmbedding\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Styling LLM Finetuning Tutorial Tiles in CSS\nDESCRIPTION: This CSS code defines the styling for a grid of tiles representing different LLM finetuning tutorials. It includes styles for the container, individual tiles, hover effects, and text formatting.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/llm_finetuning/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: CSS\nCODE:\n```\n/* CSS styles for tiles with rounded corners, centered titles, and always displayed algorithm list */\n\n/* Style for the container */\n.tiles {\n    display: grid;\n    grid-template-columns: repeat(auto-fill, minmax(150px, 0.25fr));\n    grid-auto-rows: 200px; /* 2 rows */\n    gap: 25px; /* Adjust the gap between tiles */\n    margin-top: 48px;\n    margin-bottom: 48px;\n    width: 100%;\n    align-content: start;\n    /*height: auto;\n}\n\n/* Style for each tile */\n\n\n.column {\nflex: 1; /* Equal flex distribution */\nwidth: 50%; /* 50% width for each column */\ndisplay: flex;\nflex-direction: column;\n/* Additional styles */\n}\n\n/* Lighter background color on hover */\n.tile:hover {\n    background-color: #48b8b8; /* Lighter grey on hover */\n    color: white;\n}\n\n\n\n\n/* Learn more link styles */\n.tile a {\n    display: block;\n    margin-bottom: 0px; /* Adjust the margin */\n    text-decoration: none;\n    /*color: white; /* Link color */\n    text-align: center; /* Center link text */\n    padding: 0px 0px;\n}\n\n.tile a:hover {\n    color: white; /* Link color on hover */\n}\n\n.thumbnail-image {\n    width: 100%;\n    height: 60%;\n    object-fit: cover;\n\n}\n/* Style for each tile */\n.tile {\n    padding: 4px 4px;\n    transition: background-color 0.3s ease;\n    text-decoration: none;\n    width: auto;\n    height: auto;\n    overflow: hidden;\n    display: flex; /* Enable flexbox */\n    flex-direction: column;\n    justify-content: center; /* Center vertically */\n    align-items: center; /* Center horizontally */\n    background-color: transparent;\n    border-radius: 7px;\n    box-shadow: 0 4px 8px rgba(0, 150, 150, 0.5);\n    margin-bottom: 0px;\n}\n\n/* Title styles */\n.tile h2 {\n    font-size: 18px;\n    text-align: center;\n    margin: 0; /* Remove margins */\n    padding: 4px;\n    width: 100%; /* Ensure the h2 takes full width */\n}\n```\n\n----------------------------------------\n\nTITLE: Citation Format in BibTeX\nDESCRIPTION: BibTeX citation format for referencing the AgileRL framework in academic work.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_8\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{Ustaran-Anderegg_AgileRL,\nauthor = {Ustaran-Anderegg, Nicholas and Pratt, Michael and Sabal-Bermudez, Jaime},\nlicense = {Apache-2.0},\ntitle = {{AgileRL}},\nurl = {https://github.com/AgileRL/AgileRL}\n}\n```\n\n----------------------------------------\n\nTITLE: Styling Grid Layout for Tutorial Tiles in CSS\nDESCRIPTION: CSS styles defining a responsive grid layout for tutorial tiles with rounded corners, hover effects, and thumbnail images. Includes styling for containers, individual tiles, headings, and links.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/bandits/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n.tiles {\n    display: grid;\n    grid-template-columns: repeat(auto-fill, minmax(150px, 0.25fr));\n    grid-auto-rows: 200px;\n    gap: 25px;\n    margin-top: 48px;\n    margin-bottom: 48px;\n    width: 100%;\n    align-content: start;\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting EvolvableBERT Class in RST\nDESCRIPTION: Autodoc directive for the EvolvableBERT class, including all its members.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/bert.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: agilerl.modules.bert.EvolvableBERT\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Components in Python\nDESCRIPTION: Creates necessary objects for training including replay buffer, tournament selection, and mutation operators.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.hpo.mutation import Mutations\n\nmemory = ReplayBuffer(\n    max_size=INIT_HP['MEMORY_SIZE'],   # Max replay buffer size\n    device=device,\n)\n\ntournament = TournamentSelection(\n    tournament_size=INIT_HP['TOURN_SIZE'], # Tournament selection size\n    elitism=INIT_HP['ELITISM'],            # Elitism in tournament selection\n    population_size=INIT_HP['POP_SIZE'],   # Population size\n    eval_loop=INIT_HP['EVAL_LOOP'],        # Evaluate using last N fitness scores\n)\n\nmutations = Mutations(\n    no_mutation=MUTATION_PARAMS['NO_MUT'],                # No mutation\n    architecture=MUTATION_PARAMS['ARCH_MUT'],             # Architecture mutation\n    new_layer_prob=MUTATION_PARAMS['NEW_LAYER'],          # New layer mutation\n    parameters=MUTATION_PARAMS['PARAMS_MUT'],             # Network parameters mutation\n    activation=MUTATION_PARAMS['ACT_MUT'],                # Activation layer mutation\n    rl_hp=MUTATION_PARAMS['RL_HP_MUT'],                   # Learning HP mutation\n    mutation_sd=MUTATION_PARAMS['MUT_SD'],                # Mutation strength\n    rand_seed=MUTATION_PARAMS['RAND_SEED'],               # Random seed\n    device=device,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MATD3 Neural Network for Mixed Observations (Python)\nDESCRIPTION: Example of configuring the neural network architecture for MATD3 with mixed (image, discrete, and vector) observations using a kwargs dictionary.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/matd3.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCNN_CONFIG = {\n    \"channel_size\": [32, 32], # CNN channel size\n    \"kernel_size\": [8, 4],   # CNN kernel size\n    \"stride_size\": [4, 2],   # CNN stride size\n}\n\nNET_CONFIG = {\n    \"encoder_config\": {\n      \"latent_dim\": 32,\n      # Config for nested EvolvableCNN objects\n      \"cnn_config\": CNN_CONFIG,\n      # Config for nested EvolvableMLP objects\n      \"mlp_config\": {\n          \"hidden_size\": [32, 32]\n      },\n      \"vector_space_mlp\": True # Process vector observations with an MLP\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n\n# Create MATD3 agent\nagent = MATD3(\n  observation_spaces=observation_spaces,\n  action_spaces=action_spaces,\n  agent_ids=agent_ids,\n  net_config=NET_CONFIG\n  )\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for ModuleDict Class\nDESCRIPTION: ReStructuredText documentation structure for the ModuleDict class which implements a dictionary container for evolvable modules.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/modules/base.rst#2025-04-19_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\nModuleDict\n==========\n\nParameters\n------------\n\n.. autoclass:: agilerl.modules.base.ModuleDict\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Tutorial Navigation Grid HTML Structure\nDESCRIPTION: HTML structure for displaying tutorial links in a grid format with thumbnail images and titles. Features two tutorials: NeuralUCB with Iris dataset and NeuralTS with PenDigits dataset.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/bandits/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"tiles article\">\n      <a href=\"../bandits/agilerl_neural_ucb_tutorial.html\" class=\"tile\">\n         <img src=\"../../_static/thumbnails/iris-thumbnail.png\" alt=\"Iris image\" class=\"thumbnail-image\">\n         <h2>NeuralUCB - Iris</h2>\n      </a>\n      <a href=\"../bandits/agilerl_neural_ts_tutorial.html\" class=\"tile\">\n      <img src=\"../../_static/thumbnails/pendigits-thumbnail.png\" alt=\"Pen digits image\" class=\"thumbnail-image\">\n         <h2>NeuralTS - PenDigits</h2>\n      </a>\n   </div>\n```\n\n----------------------------------------\n\nTITLE: Configuring MATD3 Neural Network for Image Observations (Python)\nDESCRIPTION: Example of configuring the neural network architecture for MATD3 with image observations using a kwargs dictionary.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/algorithms/matd3.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nNET_CONFIG = {\n    \"encoder_config\": {\n      'channel_size': [32, 32], # CNN channel size\n      'kernel_size': [8, 4],   # CNN kernel size\n      'stride_size': [4, 2],   # CNN stride size\n    },\n    \"head_config\": {'hidden_size': [32]}  # Network head hidden size\n  }\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for AgileRL Wrappers in RST\nDESCRIPTION: Defines a table of contents (toctree) in reStructuredText format for AgileRL wrapper documentation. The toctree has a maximum depth of 1 and includes links to agent, make_evolvable, learning, and pettingzoo documentation pages.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/api/wrappers/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   agent\n   make_evolvable\n   learning\n   pettingzoo\n```\n\n----------------------------------------\n\nTITLE: Installing AgileRL in Development Mode\nDESCRIPTION: Commands to clone the AgileRL repository from GitHub and install it in development mode, which allows making modifications to the library code while using it.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/AgileRL/AgileRL.git && cd AgileRL\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Python Documentation Dependencies\nDESCRIPTION: Lists required Python packages for documentation generation using Sphinx, including the Furo theme, Sphinx-toolbox utilities, and the not-found page extension.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfuro\nsphinx-toolbox\nsphinx-notfound-page\n```\n\n----------------------------------------\n\nTITLE: Installing AgileRL with pip\nDESCRIPTION: Command to install the AgileRL package using pip package manager. This is the simplest way to get started with the library.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install agilerl\n```\n\n----------------------------------------\n\nTITLE: Styling Grid Layout for Tutorial Tiles in CSS\nDESCRIPTION: CSS styles that create a responsive grid layout of tutorial tiles with rounded corners, hover effects, and image thumbnails. The styles include formatting for tile containers, individual tiles, titles, and thumbnail images.\nSOURCE: https://github.com/agilerl/agilerl/blob/main/docs/tutorials/gymnasium/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n.tiles {\n    display: grid;\n    grid-template-columns: repeat(auto-fill, minmax(150px, 0.25fr));\n    grid-auto-rows: 200px;\n    gap: 25px;\n    margin-top: 48px;\n    margin-bottom: 48px;\n    width: 100%;\n    align-content: start;\n}\n```"
  }
]
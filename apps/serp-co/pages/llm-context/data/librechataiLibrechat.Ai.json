[
  {
    "owner": "librechat-ai",
    "repo": "librechat.ai",
    "content": "TITLE: Configuring LibreChat with Multiple Endpoints in YAML\nDESCRIPTION: This YAML configuration file sets up LibreChat with various endpoints including Anyscale, APIpie, cohere, Fireworks, groq, Mistral AI, OpenRouter, Perplexity, ShuttleAI, and together.ai. It defines API keys, base URLs, model settings, and display labels for each endpoint, as well as options for conversation titles, summarization, and parameter adjustments. This configuration enables LibreChat to interface with multiple AI services for different chat functionalities.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/example.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 1.0.8\n\ncache: true\n\ninterface:\n  # Privacy policy settings\n  privacyPolicy:\n    externalUrl: 'https://librechat.ai/privacy-policy'\n    openNewTab: true\n\n  # Terms of service\n  termsOfService:\n    externalUrl: 'https://librechat.ai/tos'\n    openNewTab: true\n\nregistration:\n  socialLogins: [\"discord\", \"facebook\", \"github\", \"google\", \"openid\"]\n\nendpoints:\n  custom:\n\n    # Anyscale\n    - name: \"Anyscale\"\n      apiKey: \"${ANYSCALE_API_KEY}\"\n      baseURL: \"https://api.endpoints.anyscale.com/v1\"\n      models:\n        default: [\n          \"meta-llama/Llama-2-7b-chat-hf\",\n          ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"meta-llama/Llama-2-7b-chat-hf\"\n      summarize: false\n      summaryModel: \"meta-llama/Llama-2-7b-chat-hf\"\n      forcePrompt: false\n      modelDisplayLabel: \"Anyscale\"\n\n    # APIpie\n    - name: \"APIpie\"\n      apiKey: \"${APIPIE_API_KEY}\"\n      baseURL: \"https://apipie.ai/v1/\"\n      models:\n        default: [\n          \"gpt-4\",\n          \"gpt-4-turbo\",\n          \"gpt-3.5-turbo\",\n          \"claude-3-opus\",\n          \"claude-3-sonnet\",\n          \"claude-3-haiku\",\n          \"llama-3-70b-instruct\",\n          \"llama-3-8b-instruct\",\n          \"gemini-pro-1.5\",\n          \"gemini-pro\",\n          \"mistral-large\",\n          \"mistral-medium\",\n          \"mistral-small\",\n          \"mistral-tiny\",\n          \"mixtral-8x22b\",\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"gpt-3.5-turbo\"\n      dropParams: [\"stream\"]\n\n    #cohere\n    - name: \"cohere\"\n      apiKey: \"${COHERE_API_KEY}\"\n      baseURL: \"https://api.cohere.ai/v1\"\n      models:\n        default: [\"command-r\",\"command-r-plus\",\"command-light\",\"command-light-nightly\",\"command\",\"command-nightly\"]\n        fetch: false\n      modelDisplayLabel: \"cohere\"\n      titleModel: \"command\"\n      dropParams: [\"stop\", \"user\", \"frequency_penalty\", \"presence_penalty\", \"temperature\", \"top_p\"]\n\n    # Fireworks\n    - name: \"Fireworks\"\n      apiKey: \"${FIREWORKS_API_KEY}\"\n      baseURL: \"https://api.fireworks.ai/inference/v1\"\n      models:\n        default: [\n          \"accounts/fireworks/models/mixtral-8x7b-instruct\",\n          ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"accounts/fireworks/models/llama-v2-7b-chat\"\n      summarize: false\n      summaryModel: \"accounts/fireworks/models/llama-v2-7b-chat\"\n      forcePrompt: false\n      modelDisplayLabel: \"Fireworks\"\n      dropParams: [\"user\"]\n  \n    # groq\n    - name: \"groq\"\n      apiKey: \"${GROQ_API_KEY}\"\n      baseURL: \"https://api.groq.com/openai/v1/\"\n      models:\n        default: [\n          \"llama2-70b-4096\",\n          \"llama3-70b-8192\",\n          \"llama3-8b-8192\",\n          \"mixtral-8x7b-32768\",\n          \"gemma-7b-it\",\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"mixtral-8x7b-32768\"\n      modelDisplayLabel: \"groq\"\n\n    # Mistral AI API\n    - name: \"Mistral\"\n      apiKey: \"${MISTRAL_API_KEY}\"\n      baseURL: \"https://api.mistral.ai/v1\"\n      models:\n        default: [\n          \"mistral-tiny\",\n          \"mistral-small\",\n          \"mistral-medium\",\n          \"mistral-large-latest\"\n          ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"mistral-tiny\"\n      modelDisplayLabel: \"Mistral\"\n      dropParams: [\"stop\", \"user\", \"frequency_penalty\", \"presence_penalty\"]\n\n    # OpenRouter.ai\n    - name: \"OpenRouter\"\n      apiKey: \"${OPENROUTER_KEY}\"\n      baseURL: \"https://openrouter.ai/api/v1\"\n      models:\n        default: [\"openai/gpt-3.5-turbo\"]\n        fetch: true\n      titleConvo: true\n      titleModel: \"gpt-3.5-turbo\"\n      summarize: false\n      summaryModel: \"gpt-3.5-turbo\"\n      forcePrompt: false\n      modelDisplayLabel: \"OpenRouter\"\n\n    # Perplexity\n    - name: \"Perplexity\"\n      apiKey: \"${PERPLEXITY_API_KEY}\"\n      baseURL: \"https://api.perplexity.ai/\"\n      models:\n        default: [\n          \"mistral-7b-instruct\",\n          \"sonar-small-chat\",\n          \"sonar-small-online\",\n          \"sonar-medium-chat\",\n          \"sonar-medium-online\"\n          ]\n        fetch: false # fetching list of models is not supported\n      titleConvo: true\n      titleModel: \"sonar-medium-chat\"\n      summarize: false\n      summaryModel: \"sonar-medium-chat\"\n      forcePrompt: false\n      dropParams: [\"stop\", \"frequency_penalty\"]\n      modelDisplayLabel: \"Perplexity\"\n\n    # ShuttleAI API\n    - name: \"ShuttleAI\"\n      apiKey: \"${SHUTTLEAI_API_KEY}\"\n      baseURL: \"https://api.shuttleai.app/v1\"\n      models:\n        default: [\n          \"shuttle-1\", \"shuttle-turbo\"\n          ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"gemini-pro\"\n      summarize: false\n      summaryModel: \"llama-summarize\"\n      forcePrompt: false\n      modelDisplayLabel: \"ShuttleAI\"\n      dropParams: [\"user\"]\n\n    # together.ai\n    - name: \"together.ai\"\n      apiKey: \"${TOGETHERAI_API_KEY}\"\n      baseURL: \"https://api.together.xyz\"\n      models:\n        default: [\n          \"zero-one-ai/Yi-34B-Chat\",\n          \"Austism/chronos-hermes-13b\",\n          \"DiscoResearch/DiscoLM-mixtral-8x7b-v2\",\n          \"Gryphe/MythoMax-L2-13b\",\n          \"lmsys/vicuna-13b-v1.5\",\n          \"lmsys/vicuna-7b-v1.5\",\n          \"lmsys/vicuna-13b-v1.5-16k\",\n          \"codellama/CodeLlama-13b-Instruct-hf\",\n          \"codellama/CodeLlama-34b-Instruct-hf\",\n          \"codellama/CodeLlama-70b-Instruct-hf\",\n          \"codellama/CodeLlama-7b-Instruct-hf\",\n          \"togethercomputer/llama-2-13b-chat\",\n          \"togethercomputer/llama-2-70b-chat\",\n          \"togethercomputer/llama-2-7b-chat\",\n          \"NousResearch/Nous-Capybara-7B-V1p9\",\n          \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n          \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\",\n          \"NousResearch/Nous-Hermes-Llama2-70b\",\n          \"NousResearch/Nous-Hermes-llama-2-7b\",\n          \"NousResearch/Nous-Hermes-Llama2-13b\",\n          \"NousResearch/Nous-Hermes-2-Yi-34B\",\n          \"openchat/openchat-3.5-1210\",\n          \"Open-Orca/Mistral-7B-OpenOrca\",\n          \"togethercomputer/Qwen-7B-Chat\",\n          \"snorkelai/Snorkel-Mistral-PairRM-DPO\",\n          \"togethercomputer/alpaca-7b\",\n          \"togethercomputer/falcon-40b-instruct\",\n          \"togethercomputer/falcon-7b-instruct\",\n          \"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\n          \"togethercomputer/Llama-2-7B-32K-Instruct\",\n          \"togethercomputer/Pythia-Chat-Base-7B-v0.16\",\n          \"togethercomputer/RedPajama-INCITE-Chat-3B-v1\",\n          \"togethercomputer/RedPajama-INCITE-7B-Chat\",\n          \"togethercomputer/StripedHyena-Nous-7B\",\n          \"Undi95/ReMM-SLERP-L2-13B\",\n          \"Undi95/Toppy-M-7B\",\n          \"WizardLM/WizardLM-13B-V1.2\",\n          \"garage-bAInd/Platypus2-70B-instruct\",\n          \"mistralai/Mistral-7B-Instruct-v0.1\",\n          \"mistralai/Mistral-7B-Instruct-v0.2\",\n          \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n          \"teknium/OpenHermes-2-Mistral-7B\",\n          \"teknium/OpenHermes-2p5-Mistral-7B\",\n          \"upstage/SOLAR-10.7B-Instruct-v1.0\"\n          ]\n        fetch: false # fetching list of models is not supported\n      titleConvo: true\n      titleModel: \"togethercomputer/llama-2-7b-chat\"\n      summarize: false\n      summaryModel: \"togethercomputer/llama-2-7b-chat\"\n      forcePrompt: false\n      modelDisplayLabel: \"together.ai\"\n```\n\n----------------------------------------\n\nTITLE: Configuring LibreChat with Comments and Customization in YAML\nDESCRIPTION: This YAML configuration file demonstrates detailed configuration options for LibreChat, including caching settings, interface customization (privacy policy and terms of service URLs), registration settings (social logins and allowed domains), and rate limiting for file uploads. It also provides a commented section for potential Firebase file storage integration, and custom endpoint configurations for Mistral and OpenRouter, showcasing API key usage, base URLs, model handling, and feature toggles like conversation titles and summarization.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/example.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# For more information, see the Configuration Guide:\n# https://docs.librechat.ai/install/configuration/custom_config.html\n\n# Configuration version (required)\nversion: 1.0.9\n\n# Cache settings: Set to true to enable caching\ncache: true\n\n# Custom interface configuration\ninterface:\n  # Privacy policy settings\n  privacyPolicy:\n    externalUrl: 'https://librechat.ai/privacy-policy'\n    openNewTab: true\n\n  # Terms of service\n  termsOfService:\n    externalUrl: 'https://librechat.ai/tos'\n    openNewTab: true\n\n# Example Registration Object Structure (optional)\nregistration:\n  socialLogins: ['github', 'google', 'discord', 'openid', 'facebook']\n  # allowedDomains:\n  # - \"gmail.com\"\n\n# rateLimits:\n#   fileUploads:\n#     ipMax: 100\n#     ipWindowInMinutes: 60  # Rate limit window for file uploads per IP\n#     userMax: 50\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: This snippet shows how to set the `OPENAI_API_KEY` environment variable to either the user's API key or to `user_provided` to allow users to provide their own key. If set to `user_provided`, only the models defined in the `OPENAI_MODELS` variable will be available.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/openai.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=user_provided\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Configuration Example (librechat.yaml)\nDESCRIPTION: This is a comprehensive example of how to configure Azure OpenAI settings in the `librechat.yaml` file for LibreChat. It includes endpoint-level, group-level, and model-level configurations, showcasing the flexibility of the system for managing multiple Azure OpenAI deployments.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    # Endpoint-level configuration\n    titleModel: \"llama-70b-chat\"\n    plugins: true\n    assistants: true\n    groups:\n    # Group-level configuration\n    - group: \"my-resource-westus\"\n      apiKey: \"${WESTUS_API_KEY}\"\n      instanceName: \"my-resource-westus\"\n      version: \"2024-03-01-preview\"\n      # Model-level configuration\n      models:\n        gpt-4-vision-preview:\n          deploymentName: gpt-4-vision-preview\n          version: \"2024-03-01-preview\"\n        gpt-3.5-turbo:\n          deploymentName: gpt-35-turbo\n        gpt-4-1106-preview:\n          deploymentName: gpt-4-1106-preview\n    # Group-level configuration\n    - group: \"mistral-inference\"\n      apiKey: \"${AZURE_MISTRAL_API_KEY}\"\n      baseURL: \"https://Mistral-large-vnpet-serverless.region.inference.ai.azure.com/v1/chat/completions\"\n      serverless: true\n      # Model-level configuration\n      models:\n        mistral-large: true\n    # Group-level configuration\n    - group: \"my-resource-sweden\"\n      apiKey: \"${SWEDEN_API_KEY}\"\n      instanceName: \"my-resource-sweden\"\n      deploymentName: gpt-4-1106-preview\n      version: \"2024-03-01-preview\"\n      assistants: true\n      # Model-level configuration\n      models:\n        gpt-4-turbo: true\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for AWS Cognito in LibreChat\nDESCRIPTION: This snippet shows the environment variables that need to be configured in the `.env` file for LibreChat to use AWS Cognito for authentication. It includes the domain, client ID, client secret, issuer URL, session secret, scope, and callback URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/aws.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nDOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain\nDOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain\n\nOPENID_CLIENT_ID=Your client ID\nOPENID_CLIENT_SECRET=Your client secret\nOPENID_ISSUER=https://cognito-idp.[AWS REGION].amazonaws.com/[USER POOL ID]/.well-known/openid-configuration\nOPENID_SESSION_SECRET=Any random string\nOPENID_SCOPE=openid profile email\nOPENID_CALLBACK_URL=/oauth/openid/callback\n\n# Optional: redirects the user to the end session endpoint after logging out\nOPENID_USE_END_SESSION_ENDPOINT=true\n```\n\n----------------------------------------\n\nTITLE: Setting Assistants API Key (Bash)\nDESCRIPTION: This snippet demonstrates how to set the `ASSISTANTS_API_KEY` environment variable.  The API key is required to authenticate with the OpenAI Assistants API. You can set it directly or to `user_provided` to allow users to provide their own key.  This is a necessary step for enabling the Assistants API functionality within LibreChat.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/assistants.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nASSISTANTS_API_KEY=your-key\n```\n\n----------------------------------------\n\nTITLE: .env example\nDESCRIPTION: Example snippet of the .env file to configure the `CREDS_IV`, `CREDS_KEY`, `JWT_SECRET`, `JWT_REFRESH_SECRET`, and `OPENAI_API_KEY` environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\n# FIND THESE VARIABLES AND REPLACE THEIR DEFAULT VALUES!\n\n# Must be a 16-byte IV (32 characters in hex)\n\nCREDS_IV=e2341419ec3dd3d19b13a1a87fafcbfb\n\n# Must be 32-byte keys (64 characters in hex)\n\nCREDS_KEY=f34be427ebb29de8d88c107a71546019685ed8b241d8f2ed00c3df97ad2566f0\nJWT_SECRET=16f8c0ef4a5d391b26034086c628469d3f9f497f08163ab9b40137092f2909ef\nJWT_REFRESH_SECRET=eaa5191f2914e30b9387fd84e254e4ba6fc51b4654968a9b0803b456a54b8418\n```\n\nLANGUAGE: shell\nCODE:\n```\nOPENAI_API_KEY=sk-yourKey\n```\n\n----------------------------------------\n\nTITLE: Setting the Google API Key in .env\nDESCRIPTION: This snippet shows how to set the Google API key obtained from Google AI Studio in the .env file. This allows all users of the LibreChat instance to use the Gemini models. Alternatively, setting the key to 'user_provided' allows users to provide their own keys from the frontend.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_KEY=mY_SeCreT_w9347w8_kEY\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral in LibreChat via YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the Mistral AI service within a LibreChat environment. It specifies the API key, base URL, available models, conversation title settings, and parameters to drop to ensure compatibility with the Mistral API. The MISTRAL_API_KEY environment variable must be set for the configuration to function correctly.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/mistral.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - name: \"Mistral\"\n      apiKey: \"${MISTRAL_API_KEY}\"\n      baseURL: \"https://api.mistral.ai/v1\"\n      models:\n        default: [\"mistral-tiny\", \"mistral-small\", \"mistral-medium\", \"mistral -large-latest\"]\n        fetch: true\n      titleConvo: true\n      titleModel: \"mistral-tiny\"\n      modelDisplayLabel: \"Mistral\"\n      dropParams: [\"stop\", \"user\", \"frequency_penalty\", \"presence_penalty\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring LiteLLM in LibreChat\nDESCRIPTION: This YAML snippet configures LiteLLM as a model provider in LibreChat. It sets the API key, base URL (assuming a local LiteLLM instance), default models to use, and enables features like automatic title generation. The `baseURL` should be adjusted to point to the correct LiteLLM endpoint, especially when running in a containerized environment.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/litellm.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"LiteLLM\"\n      apiKey: \"sk-from-config-file\"\n      baseURL: \"http://localhost:8000/v1\"\n      # if using LiteLLM example in docker-compose.override.yml.example, use \"http://litellm:8000/v1\"\n      models:\n        default: [\"gpt-3.5-turbo\"]\n        fetch: true\n      titleConvo: true\n      titleModel: \"gpt-3.5-turbo\"\n      summarize: false\n      summaryModel: \"gpt-3.5-turbo\"\n      forcePrompt: false\n      modelDisplayLabel: \"LiteLLM\"\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Model Configuration in librechat.yaml\nDESCRIPTION: This code demonstrates how to configure model-specific settings within the `models` section of a group configuration in the `librechat.yaml` file. It shows how to define deployment names and versions for specific models, or to indicate that a model should use the group-level deployment name and version by setting it to `true`. This configuration is essential for defining the behavior of different models within the Azure OpenAI service.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    # ... (endpoint-level configurations)\n    groups:\n    # ... (group-level configurations)\n    - group: \"example_group\"\n    models:\n     # Model identifiers must match OpenAI Model name (can be a partial match)\n      gpt-4-vision-preview:\n      # Object setting: must include at least \"deploymentName\" and/or \"version\"\n        deploymentName: \"arbitrary-deployment-name\"\n        version: \"2024-02-15-preview\" # version can be any that supports vision\n      # Boolean setting, must be \"true\"\n      gpt-4-turbo: true\n```\n\n----------------------------------------\n\nTITLE: Navigate to LibreChat Directory\nDESCRIPTION: Navigates to the application directory in the terminal.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncd LibreChat\n```\n\n----------------------------------------\n\nTITLE: librechat.yaml example\nDESCRIPTION: Example content for the `librechat.yaml` file, configuring the version and cache settings for the LibreChat application. Required to start deploy-compose.yml\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\n# For more information, see the Configuration Guide:\n# https://docs.librechat.ai/install/configuration/custom_config.html\n\n# Configuration version (required)\nversion: 1.0.5\n# This setting caches the config file for faster loading across app lifecycle\ncache: true\n```\n\n----------------------------------------\n\nTITLE: Configure Keycloak OpenID settings in .env\nDESCRIPTION: This snippet shows the required environment variables that must be configured in the `.env` file to integrate LibreChat with Keycloak using the OpenID Connect protocol. These include the Keycloak issuer URL, client ID, client secret, session secret, callback URL, desired scope, required role (if any), the kind of token in which the role is defined (access or id), and the parameter path for the role inside the token.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/keycloak.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENID_ISSUER=http://localhost:8080/realms/[YourRealmName]\nOPENID_CLIENT_ID=[YourClientID]\nOPENID_CLIENT_SECRET=[YourClientSecret]\nOPENID_SESSION_SECRET=[JustGenerateARandomSessionSecret]\nOPENID_CALLBACK_URL=/oauth/openid/callback\nOPENID_SCOPE=\"openid profile email\"\nOPENID_REQUIRED_ROLE=[YourRequiredRole]\nOPENID_REQUIRED_ROLE_TOKEN_KIND=(access|id) # that means, `access` or `id`\nOPENID_REQUIRED_ROLE_PARAMETER_PATH=\"realm_access.roles\"\n\n# Optional: redirects the user to the end session endpoint after logging out\nOPENID_USE_END_SESSION_ENDPOINT=true\n```\n\n----------------------------------------\n\nTITLE: Anyscale YAML Configuration\nDESCRIPTION: This YAML configuration defines the settings for integrating Anyscale with LibreChat. It specifies the API key (obtained from anyscale.com/credentials), the base URL for the Anyscale API, and a list of default models.  It also configures settings for automatically fetching models, generating conversation titles, summarization, and force prompting. The model display label is set to \"Anyscale\".\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/anyscale.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"Anyscale\"\n      apiKey: \"${ANYSCALE_API_KEY}\"\n      baseURL: \"https://api.endpoints.anyscale.com/v1\"\n      models:\n        default: [\n          \"meta-llama/Llama-2-7b-chat-hf\",\n          ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"meta-llama/Llama-2-7b-chat-hf\"\n      summarize: false\n      summaryModel: \"meta-llama/Llama-2-7b-chat-hf\"\n      forcePrompt: false\n      modelDisplayLabel: \"Anyscale\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Fireworks in LibreChat with YAML\nDESCRIPTION: This YAML snippet configures Fireworks as a custom endpoint in LibreChat. It specifies the API key, base URL, default models, and settings for conversation titles and summaries. The `dropParams` setting removes the `user` parameter from requests to avoid API errors.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/fireworks.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - name: \"Fireworks\"\n      apiKey: \"${FIREWORKS_API_KEY}\"\n      baseURL: \"https://api.fireworks.ai/inference/v1\"\n      models:\n        default: [\n          \"accounts/fireworks/models/mixtral-8x7b-instruct\",\n          ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"accounts/fireworks/models/llama-v2-7b-chat\"\n      summarize: false \n      summaryModel: \"accounts/fireworks/models/llama-v2-7b-chat\"\n      forcePrompt: false\n      modelDisplayLabel: \"Fireworks\"\n      dropParams: [\"user\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Servers in LibreChat using YAML\nDESCRIPTION: This snippet demonstrates how to configure MCP (Model Context Protocol) servers within the LibreChat application using YAML. It shows examples of configuring servers with different connection types (SSE and stdio) and various parameters such as URL, command, arguments, timeouts, and icon paths. The configuration options include defining the connection type, specifying the server address or command to execute, setting timeouts for requests and initialization, and providing additional arguments for the command-line execution.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/config.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nmcpServers:\n  everything:\n    # type: sse # type can optionally be omitted\n    url: http://localhost:3001/sse\n    timeout: 30000\n    initTimeout: 10000\n  puppeteer:\n    type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@modelcontextprotocol/server-puppeteer\"\n    timeout: 30000\n    initTimeout: 10000\n  filesystem:\n    # type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@modelcontextprotocol/server-filesystem\"\n      - /home/user/LibreChat/\n    iconPath: /home/user/LibreChat/client/public/assets/logo.svg\n  mcp-obsidian:\n    command: npx\n    args:\n      - -y\n      - \"mcp-obsidian\"\n      - /path/to/obsidian/vault\n```\n\n----------------------------------------\n\nTITLE: Setting Google API Key to user_provided\nDESCRIPTION: This snippet demonstrates how to configure the Google API key to be provided by the user through the frontend by setting the GOOGLE_KEY environment variable to 'user_provided' in the .env file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_KEY=user_provided\n```\n\n----------------------------------------\n\nTITLE: Enabling Assistants with Azure OpenAI\nDESCRIPTION: This snippet enables the use of Assistants with Azure OpenAI in LibreChat. It sets the `assistants` field to `true` at the `azureOpenAI` endpoint level and then enables it again for a specific group compatible with Azure's Assistants API, specifying the required version. The API key is assumed to be set as an environment variable.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n  # Enable use of Assistants with Azure\n    assistants: true\n```\n\n----------------------------------------\n\nTITLE: Setting OpenWeather API Key in .env\nDESCRIPTION: This snippet demonstrates how to set the OpenWeather API key as an environment variable in the `.env` file. This key is required for the LibreChat plugin to authenticate with the OpenWeather API and retrieve weather data.  Replace `your_api_key_here` with the actual API key obtained from OpenWeather.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/tools/openweather.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENWEATHER_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Configure .env for Google OAuth in LibreChat\nDESCRIPTION: This code snippet demonstrates the configuration of the `.env` file for enabling Google OAuth in LibreChat. It defines the domain for both the client and server, along with the necessary Google client ID, client secret, and callback URL. It assumes that the user has obtained the Google client ID and secret from the Google Cloud Console. The DOMAIN_CLIENT and DOMAIN_SERVER variables should reflect the deployment URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/google.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nDOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain\nDOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain\n\nGOOGLE_CLIENT_ID=your_client_id\nGOOGLE_CLIENT_SECRET=your_client_secret\nGOOGLE_CALLBACK_URL=/oauth/google/callback\n```\n\n----------------------------------------\n\nTITLE: Create User Script Execution (Docker Compose)\nDESCRIPTION: Executes the `create-user` script within the `api` container when using the default `docker-compose.yml` file. This script allows adding users to the database, even when registration is disabled. It requires Docker and Docker Compose to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/index.mdx#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ndocker-compose exec api npm run create-user\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral AI Custom Endpoint\nDESCRIPTION: This snippet configures a custom endpoint for the Mistral AI service within the LibreChat application. It defines the API key, base URL, default models, and settings such as enabling conversation titles, specifying a title model and setting a display label.  It also demonstrates how to drop parameters from the request, which is necessary for Mistral AI to avoid errors. The API key should be set using the environment variable `${MISTRAL_API_KEY}`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/example.mdx#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n- name: 'Mistral' # Unique name for the endpoint\n  # For `apiKey` and `baseURL`, you can use environment variables that you define.\n  # recommended environment variables:\n  apiKey: '${MISTRAL_API_KEY}'\n  baseURL: 'https://api.mistral.ai/v1'\n\n  # Models configuration\n  models:\n    # List of default models to use. At least one value is required.\n    default: ['mistral-tiny', 'mistral-small', 'mistral-medium']\n    # Fetch option: Set to true to fetch models from API.\n    fetch: true # Defaults to false.\n\n  # Optional configurations\n\n  # Title Conversation setting\n  titleConvo: true # Set to true to enable title conversation\n\n  # Title Method: Choose between \"completion\" or \"functions\".\n  # titleMethod: \"completion\"  # Defaults to \"completion\" if omitted.\n\n  # Title Model: Specify the model to use for titles.\n  titleModel: 'mistral-tiny' # Defaults to \"gpt-3.5-turbo\" if omitted.\n\n  # Summarize setting: Set to true to enable summarization.\n  # summarize: false\n\n  # Summary Model: Specify the model to use if summarization is enabled.\n  # summaryModel: \"mistral-tiny\"  # Defaults to \"gpt-3.5-turbo\" if omitted.\n\n  # Force Prompt setting: If true, sends a `prompt` parameter instead of `messages`.\n  # forcePrompt: false\n\n  # The label displayed for the AI model in messages.\n  modelDisplayLabel: 'Mistral' # Default is \"AI\" when not set.\n\n  # Add additional parameters to the request. Default params will be overwritten.\n  # addParams:\n  # safe_prompt: true # This field is specific to Mistral AI: https://docs.mistral.ai/api/\n\n  # Drop Default params parameters from the request. See default params in guide linked below.\n  # NOTE: For Mistral, it is necessary to drop the following parameters or you will encounter a 422 Error:\n  dropParams: ['stop', 'user', 'frequency_penalty', 'presence_penalty']\n```\n\n----------------------------------------\n\nTITLE: Enable Multiple Response Streaming\nDESCRIPTION: Demonstrates how to enable the multiple response streaming feature in LibreChat. Setting `multiConvo` to `true` enables users to stream responses from two AI models simultaneously.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  multiConvo: true\n```\n\n----------------------------------------\n\nTITLE: Local TTS Piper Configuration YAML\nDESCRIPTION: Configures a local Piper instance for Text-to-Speech (TTS).  Specifies the URL, API key, and available voices.  Requires a local Piper instance.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  tts:\n    localai:\n      url: \"http://host.docker.internal:8080/tts\"\n      apiKey: \"EMPTY\"\n      voices: [\n        \"en-us-amy-low.onnx\",\n        \"en-us-danny-low.onnx\",\n        \"en-us-libritts-high.onnx\",\n        \"en-us-ryan-high.onnx\",\n      ]\n      backend: \"piper\"\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Bedrock Region Environment Variable Only\nDESCRIPTION: This snippet shows how to set only the AWS region environment variable when using the default AWS credentials chain, as opposed to explicitly specifying access keys.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/bedrock.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nBEDROCK_AWS_DEFAULT_REGION=us-east-1\n```\n\n----------------------------------------\n\nTITLE: Setting RAG API URL and Embedding Provider to Ollama\nDESCRIPTION: This snippet configures the `.env` file to use Ollama for local embeddings with the RAG API. It sets the `RAG_API_URL`, `EMBEDDINGS_PROVIDER`, `OLLAMA_BASE_URL`, and `EMBEDDINGS_MODEL` environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/rag_api.mdx#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nRAG_API_URL=http://host.docker.internal:8000\nEMBEDDINGS_PROVIDER=ollama\nOLLAMA_BASE_URL=http://host.docker.internal:11434\nEMBEDDINGS_MODEL=nomic-embed-text\n```\n\n----------------------------------------\n\nTITLE: Endpoint-Level Azure OpenAI Configuration (librechat.yaml)\nDESCRIPTION: This snippet demonstrates how to configure endpoint-level settings for Azure OpenAI within the `librechat.yaml` file.  It includes settings for title generation, plugin support, assistant features, summarization, and custom ordering.  It requires a `librechat.yaml` file and a running LibreChat instance to observe the effects.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    titleModel: \"gpt-3.5-turbo-1106\"\n    plugins: true\n    assistants: true\n    summarize: true\n    summaryModel: \"gpt-3.5-turbo-1106\"\n    titleConvo: true\n    titleMethod: \"functions\"\n    groups:\n      # ... (group-level and model-level configurations)\n```\n\n----------------------------------------\n\nTITLE: ShuttleAI Configuration in YAML\nDESCRIPTION: This YAML snippet configures the ShuttleAI integration within LibreChat. It defines the API key, base URL, available models, and settings for various features such as title generation, summarization, and parameter dropping. The `fetch: true` setting enables fetching the list of models from the ShuttleAI API. Ensure to set the SHUTTLEAI_API_KEY environment variable.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/shuttleai.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - name: \"ShuttleAI\"\n      apiKey: \"${SHUTTLEAI_API_KEY}\"\n      baseURL: \"https://api.shuttleai.com/v1\"\n      models:\n        default: [\n          \"shuttle-2.5\", \"shuttle-2.5-mini\"\n          ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"shuttle-2.5-mini\"\n      summarize: false\n      summaryModel: \"shuttle-2.5-mini\"\n      forcePrompt: false\n      modelDisplayLabel: \"ShuttleAI\"\n      dropParams: [\"user\", \"stop\"]\n```\n\n----------------------------------------\n\nTITLE: Configure GitHub Authentication in .env\nDESCRIPTION: This snippet configures the `.env` file with the GitHub Client ID, Client Secret, and Callback URL for LibreChat to authenticate users via GitHub. It also includes optional configurations for GitHub Enterprise users.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/github.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nDOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain\nDOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain\n\nGITHUB_CLIENT_ID=your_client_id\nGITHUB_CLIENT_SECRET=your_client_secret\nGITHUB_CALLBACK_URL=/oauth/github/callback\n\n# GitHub Enterprise (optional)\n# Uncomment and configure the following if you are using GitHub Enterprise for authentication\n# GITHUB_ENTERPRISE_BASE_URL=https://your-ghe-instance.com\n# GITHUB_ENTERPRISE_USER_AGENT=YourEnterpriseAppName\n```\n\n----------------------------------------\n\nTITLE: Configuring Temperature Parameter in YAML\nDESCRIPTION: This example illustrates how to configure the `temperature` parameter in a YAML preset. The temperature controls the randomness of the model's responses. Lower values (e.g., 0.1) make the output more deterministic, while higher values (e.g., 0.9) introduce more randomness and creativity.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_17\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  temperature: 0.7\n```\n\n----------------------------------------\n\nTITLE: Configure Cohere in librechat.yaml\nDESCRIPTION: This YAML snippet configures Cohere as a model provider in LibreChat. It specifies the API key, base URL, supported models, display labels, and parameter adjustments needed for Cohere integration. The `dropParams` field removes parameters typically used by OpenAI to rely on Cohere's default settings.  The API Key should be set as an environment variable `COHERE_API_KEY`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/cohere.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - name: \"cohere\"\n      apiKey: \"${COHERE_API_KEY}\"\n      baseURL: \"https://api.cohere.ai/v1\"\n      models:\n        default: [\"command-r\",\"command-r-plus\",\"command-light\",\"command-light-nightly\",\"command\",\"command-nightly\"]\n        fetch: false\n      modelDisplayLabel: \"cohere\"\n      titleModel: \"command\"\n      dropParams: [\"stop\", \"user\", \"frequency_penalty\", \"presence_penalty\", \"temperature\", \"top_p\"]\n```\n\n----------------------------------------\n\nTITLE: Start LibreChat with Docker Compose (Bash)\nDESCRIPTION: Starts the Docker containers defined in the `deploy-compose.yml` file. This is a manual alternative to the `npm run start:deployed` command.  It brings the LibreChat application online.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose -f ./deploy-compose.yml up\n```\n\n----------------------------------------\n\nTITLE: Configure .env for Authentik Integration with LibreChat\nDESCRIPTION: This code snippet demonstrates how to configure the LibreChat .env file to use Authentik as an OpenID Connect provider. It sets the necessary environment variables for the OpenID issuer, client ID, client secret, session secret, callback URL, scope, button label, image URL, and end session endpoint.  The variables point to the Authentik instance and the LibreChat application configured within Authentik.  Social login must be enabled for this configuration to work.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/authentik.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENID_ISSUER=https://authentik.example.com/application/o/librechat/.well-known/openid-configuration\nOPENID_CLIENT_ID=[YourClientID]\nOPENID_CLIENT_SECRET=[YourClientSecret]\nOPENID_SESSION_SECRET=[JustGenerateARandomSessionSecret]\nOPENID_CALLBACK_URL=/oauth/openid/callback\nOPENID_SCOPE=openid profile email\n# Optional customization below\nOPENID_BUTTON_LABEL=Login with Authentik\nOPENID_IMAGE_URL=https://cdn.jsdelivr.net/gh/selfhst/icons/png/authentik.png\n# Redirects the user to the end session endpoint after logging out\nOPENID_USE_END_SESSION_ENDPOINT=true\n```\n\n----------------------------------------\n\nTITLE: Generate a Master Key for MeiliSearch\nDESCRIPTION: This command generates a master key for MeiliSearch, which is used to secure access to the MeiliSearch instance. This key is required to start MeiliSearch and configure LibreChat's connection to it.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/meilisearch.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./meilisearch --generate-master-key\n```\n\n----------------------------------------\n\nTITLE: Configuring Portkey AI with Configs in LibreChat (YAML)\nDESCRIPTION: This YAML snippet shows how to configure LibreChat to use Portkey AI with Configs.  It specifies the `name`, a dummy `apiKey`, the `baseURL`, and headers including the Portkey API key and the Config identifier. It also defines model settings such as default models, fetching, and display labels. Requires the `PORTKEY_GATEWAY_URL` and `PORTKEY_API_KEY` environment variables, as well as a pre-configured Portkey Config.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/portkey.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"Portkey\"\n      apikey: \"dummy\"\n      baseURL: ${PORTKEY_GATEWAY_URL}\n      headers:\n        x-portkey-api-key: \"${PORTKEY_API_KEY}\"\n        x-portkey-config: \"pc-libre-xxx\"\n      models:\n        default: [\"llama-3.2\"]\n        fetch: true\n      titleConvo: true\n      titleModel: \"current_model\"\n      summarize: false\n      summaryModel: \"current_model\"\n      forcePrompt: false\n      modelDisplayLabel: \"Portkey:Llama\"\n      iconURL: https://images.crunchbase.com/image/upload/c_pad,f_auto,q_auto:eco,dpr_1/rjqy7ghvjoiu4cd1xjbf\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Override Example (YAML)\nDESCRIPTION: This example shows how to mount a local librechat.yaml file into the api service container.  This allows Docker to use your `librechat.yaml` file for custom endpoints and configurations.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/docker_override.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.4'\n\nservices:\n  api:\n    volumes:\n      - ./librechat.yaml:/app/librechat.yaml\n```\n\n----------------------------------------\n\nTITLE: Nginx Configuration for LibreChat with Basic Auth Exception\nDESCRIPTION: This Nginx configuration sets up Basic Authentication for all requests to a LibreChat domain, except for those directed to the `/api/` endpoint, `/manifest.webmanifest`, and `/health` endpoints. This is achieved by using separate `location` blocks with different `auth_basic` settings. The `proxy_pass` directive forwards requests to the LibreChat backend server.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-10-29_basic_auth.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n#https://librechat.domain.com\nserver {\n\tlisten 443 ssl;\n\tlisten [::]:443 ssl;\n\tserver_name librechat.*;\n\tinclude /config/nginx/ssl.conf;\n\n\t#all connections to librechat.domain.com require basic_auth\n\tlocation / {\n\t  auth_basic \"Access Restricted\";\n\t  auth_basic_user_file /config/nginx/.htpasswd;\n\t  include /config/nginx/proxy_params.conf;\n\t  proxy_pass http://127.0.0.1:3080;\n\t}\n\n\t#...except for /api/, which will use LibreChat's own auth system\n\tlocation ~ ^/api/ {\n\t  auth_basic off;\n\t  include /config/nginx/proxy_params.conf;\n\t  proxy_pass http://127.0.0.1:3080;\n\t}\n\n\t#...except for manifest, Manifests are excluded because browsers cannot read them if BASIC authentication is enabled.\n\tlocation /manifest.webmanifest {\n\t  auth_basic off;\n\t  proxy_pass http://127.0.0.1:3080;\n\t}\n\n\t#...except for health check, Avoid the phenomenon of repeatedly requesting BASIC credentials in health checks. Note in particular that Safari has a bug in storing BASIC credentials.\n\tlocation /health {\n\t  auth_basic off;\n\t  proxy_pass http://127.0.0.1:3080;\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Enable Auto Refill in YAML\nDESCRIPTION: This YAML snippet demonstrates enabling automatic refilling of token credits for users. By setting `autoRefillEnabled` to `true`, token credits will be automatically refilled based on the configured interval and amount.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/balance.mdx#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nbalance:\n  autoRefillEnabled: true\n```\n\n----------------------------------------\n\nTITLE: Configure .env for Facebook Authentication in LibreChat\nDESCRIPTION: This snippet shows the required configuration for the `.env` file to enable Facebook authentication. It includes setting the `DOMAIN_CLIENT`, `DOMAIN_SERVER`, `FACEBOOK_CLIENT_ID`, `FACEBOOK_CLIENT_SECRET`, and `FACEBOOK_CALLBACK_URL` environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/facebook.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nDOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain\nDOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain\n\nFACEBOOK_CLIENT_ID=your_app_id\nFACEBOOK_CLIENT_SECRET=your_app_secret\nFACEBOOK_CALLBACK_URL=/oauth/facebook/callback\n```\n\n----------------------------------------\n\nTITLE: Interface Configuration Example\nDESCRIPTION: Illustrates a comprehensive example of the `interface` object configuration, including settings for privacy policy, terms of service, endpoints menu, model selection, and various other UI elements. This example demonstrates how to customize the appearance and behavior of LibreChat's interface.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  privacyPolicy:\n    externalUrl: \"https://example.com/privacy\"\n    openNewTab: true\n  termsOfService:\n    externalUrl: \"https://example.com/terms\"\n    openNewTab: true\n    modalAcceptance: true\n    modalTitle: \"Terms of Service\"\n    modalContent: |-\n      # Terms of Service\n      ## Introduction\n      Welcome to LibreChat!\n  endpointsMenu: true\n  modelSelect: false\n  parameters: true\n  sidePanel: true\n  presets: false\n  prompts: true\n  bookmarks: true\n  multiConvo: true\n  agents: true\n  customWelcome: \"Hey {{user.name}}! Welcome to LibreChat\"\n  runCode: true\n```\n\n----------------------------------------\n\nTITLE: Defining Available OpenAI Models\nDESCRIPTION: This snippet defines the `OPENAI_MODELS` environment variable, which is a comma-separated list of OpenAI models that will be available in LibreChat. This is especially important when `OPENAI_API_KEY` is set to `user_provided`, as only these models will be accessible. When a valid API key is provided and this variable is commented out, LibreChat dynamically fetches available models from the OpenAI API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/openai.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-0301,gpt-3.5-turbo,gpt-4,gpt-4-0613,gpt-4-vision-preview,gpt-3.5-turbo-0613,gpt-3.5-turbo-16k-0613,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview,gpt-3.5-turbo-1106,gpt-3.5-turbo-instruct,gpt-3.5-turbo-instruct-0914,gpt-3.5-turbo-16k\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenRouter Custom Endpoint\nDESCRIPTION: This snippet configures a custom endpoint for the OpenRouter service within the LibreChat application. It includes the API key, base URL, default models, title configuration, and specifies to drop the 'stop' parameter as recommended by OpenRouter. The API key should be set using the environment variable `${OPENROUTER_KEY}`. The fetch option set to true indicates that the model list should be fetched from the OpenRouter API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/example.mdx#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n- name: 'OpenRouter'\n  # For `apiKey` and `baseURL`, you can use environment variables that you define.\n  # recommended environment variables:\n  # Known issue: you should not use `OPENROUTER_API_KEY` as it will then override the `openAI` endpoint to use OpenRouter as well.\n  apiKey: '${OPENROUTER_KEY}'\n  baseURL: 'https://openrouter.ai/api/v1'\n  models:\n    default: ['meta-llama/llama-3-70b-instruct']\n    fetch: true\n  titleConvo: true\n  titleModel: 'meta-llama/llama-3-70b-instruct'\n  # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.\n  dropParams: ['stop']\n  modelDisplayLabel: 'OpenRouter'\n```\n\n----------------------------------------\n\nTITLE: Updating .env with MongoDB Connection String\nDESCRIPTION: This snippet shows how to update the `.env` file in your LibreChat project with the MongoDB connection string. Replace `[hostname]` and `[port]` with the actual values for your MongoDB instance. The `MONGO_URI` variable is used by LibreChat to connect to the database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_community.mdx#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nMONGO_URI=mongodb://[hostname]:[port]\n```\n\n----------------------------------------\n\nTITLE: Google Endpoint Configuration Variables\nDESCRIPTION: These environment variables configure the Google endpoint. `GOOGLE_KEY` sets the Google API key, or allows users to provide their own. `GOOGLE_REVERSE_PROXY` sets the Google reverse proxy URL. `GOOGLE_MODELS` specifies a comma-separated list of allowed models. `GOOGLE_TITLE_MODEL` is deprecated. `GOOGLE_LOC` specifies the Google Cloud location. The `GOOGLE_SAFETY` variables configure safety settings.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_7\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_KEY=user_provided\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_REVERSE_PROXY=\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_MODELS=gemini-1.0-pro,gemini-1.0-pro-001,gemini-1.0-pro-latest,gemini-1.0-pro-vision-latest,gemini-1.5-pro-latest,gemini-pro,gemini-pro-vision\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_MODELS=gemini-1.5-pro-preview-0409,gemini-1.0-pro-vision-001,gemini-pro,gemini-pro-vision,chat-bison,chat-bison-32k,codechat-bison,codechat-bison-32k,text-bison,text-bison-32k,text-unicorn,code-gecko,code-bison,code-bison-32k\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_TITLE_MODEL=gemini-pro\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_LOC=us-central1\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_EXCLUDE_SAFETY_SETTINGS=true\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_SAFETY_SEXUALLY_EXPLICIT=BLOCK_ONLY_HIGH\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_SAFETY_HATE_SPEECH=BLOCK_ONLY_HIGH\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_SAFETY_HARASSMENT=BLOCK_ONLY_HIGH\n```\n\nLANGUAGE: environment\nCODE:\n```\nGOOGLE_SAFETY_DANGEROUS_CONTENT=BLOCK_ONLY_HIGH\n```\n\n----------------------------------------\n\nTITLE: Disable the builder for Agents\nDESCRIPTION: This YAML snippet shows how to disable the builder interface for an agent by setting the `disableBuilder` parameter to `false`. Disabling the builder limits direct manual interaction with the agent and is recommended when using modelSpecs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/agents.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndisableBuilder: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Portkey AI with Virtual Keys in LibreChat (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure LibreChat to use Portkey AI with Virtual Keys. It defines the `name`, `apiKey` (a dummy value is sufficient), `baseURL`, and necessary headers including the Portkey API key and Virtual Key.  The configuration also includes model settings such as default models, fetching capabilities, and display labels. Requires the `PORTKEY_GATEWAY_URL` and `PORTKEY_API_KEY` environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/portkey.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n      - name: \"Portkey\"\n        apiKey: \"dummy\"  \n        baseURL: ${PORTKEY_GATEWAY_URL}\n        headers:\n            x-portkey-api-key: \"${PORTKEY_API_KEY}\"\n            x-portkey-virtual-key: \"PORTKEY_OPENAI_VIRTUAL_KEY\"\n        models:\n            default: [\"gpt-4o-mini\"]\n            fetch: true\n        titleConvo: true\n        titleModel: \"current_model\"\n        summarize: false\n        summaryModel: \"current_model\"\n        forcePrompt: false\n        modelDisplayLabel: \"Portkey:OpenAI\"\n        iconURL: https://images.crunchbase.com/image/upload/c_pad,f_auto,q_auto:eco,dpr_1/rjqy7ghvjoiu4cd1xjbf\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Override for MongoDB URI\nDESCRIPTION: This YAML snippet demonstrates overriding the MongoDB URI in the `docker-compose.override.yml` file for LibreChat when using Docker. It prevents the installation of the included MongoDB instance and utilizes the local MongoDB Community Server.  Replace `user`, `pass`, `host1`, `host2`, `host3`, `27017`, `LibreChat`, and `setname` with the correct values for your setup.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_community.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  api:\n    environment:\n    - MONGO_URI=mongodb://user:pass@host1:27017,host2:27017,host3:27017/LibreChat?authSource=admin&replicaSet=setname\n```\n\n----------------------------------------\n\nTITLE: Configuring AddedEndpoints Property\nDESCRIPTION: This YAML snippet demonstrates how to configure the `addedEndpoints` property to allow specific endpoints to be selectable in the UI alongside the defined model specs.  It requires `interface.modelSelect` to be `true` and lists \"openAI\" and \"google\" as available endpoints.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodelSpecs:\n  # ... other modelSpecs fields\n  addedEndpoints:\n    - openAI\n    - google\n```\n\n----------------------------------------\n\nTITLE: OpenRouter Endpoint Configuration\nDESCRIPTION: This YAML snippet configures LibreChat to use OpenRouter as an endpoint. It specifies the API key (using an environment variable `OPENROUTER_KEY`), the base URL, and settings for model fetching and display. The `dropParams` setting removes the `stop` parameter from requests.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/openrouter.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"OpenRouter\"\n      # For `apiKey` and `baseURL`, you can use environment variables that you define.\n      # recommended environment variables:\n      apiKey: \"${OPENROUTER_KEY}\" # NOT OPENROUTER_API_KEY\n      baseURL: \"https://openrouter.ai/api/v1\"\n      models:\n        default: [\"meta-llama/llama-3-70b-instruct\"]\n        fetch: true\n      titleConvo: true\n      titleModel: \"meta-llama/llama-3-70b-instruct\"\n      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.\n      dropParams: [\"stop\"]\n      modelDisplayLabel: \"OpenRouter\"\n```\n\n----------------------------------------\n\nTITLE: Shared Endpoint Settings Configuration YAML\nDESCRIPTION: This YAML configuration example demonstrates how to set the streamRate and titleModel for various endpoints including OpenAI, Anthropic, Bedrock, Google, Azure OpenAI, and Assistants within LibreChat. It also shows how the `all` setting can override specific endpoint configurations.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/shared_endpoint_settings.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  openAI:\n    streamRate: 25\n    titleModel: \"gpt-4o-mini\"\n  anthropic:\n    streamRate: 25\n    titleModel: \"claude-3-5-haiku-20241022\"\n  bedrock:\n    streamRate: 25\n    titleModel: \"us.amazon.nova-lite-v1:0\"\n  google:\n    streamRate: 1\n    titleModel: \"gemini-2.0-flash-lite\"\n  azureOpenAI:\n    streamRate: 20\n    titleModel: \"gpt-4o-mini\"\n  assistants:\n    streamRate: 30\n  azureAssistants:\n    streamRate: 30\n  # the `all` setting would override all the above values, making them unnecessary to be set\n  all:\n    streamRate: 20\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Bedrock Authentication Environment Variables\nDESCRIPTION: This snippet demonstrates how to set the environment variables required for authenticating with AWS Bedrock. The variables include the AWS region, access key ID, and secret access key. Alternatively, the access keys can be omitted to use the default AWS credentials chain. Note that if you omit access keys, you MUST define the region.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/bedrock.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nBEDROCK_AWS_DEFAULT_REGION=us-east-1\nBEDROCK_AWS_ACCESS_KEY_ID=your_access_key_id\nBEDROCK_AWS_SECRET_ACCESS_KEY=your_secret_access_key\n```\n\n----------------------------------------\n\nTITLE: Configuring LibreChat to Use Amazon S3\nDESCRIPTION: This YAML snippet configures LibreChat to use Amazon S3 for file handling by setting the `fileStrategy` to `s3`. This setting instructs the application to utilize the S3 implementation for storing and retrieving files.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/s3.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 1.0.8\ncache: true\nfileStrategy: \"s3\"\n```\n\n----------------------------------------\n\nTITLE: Set maxRecursionLimit for Agents\nDESCRIPTION: This YAML snippet shows how to set the `maxRecursionLimit` for an agent. The `maxRecursionLimit` defines the upper limit for the `recursionLimit` that can be set from the UI, preventing users from setting excessively high values.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/agents.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmaxRecursionLimit: 100\n```\n\n----------------------------------------\n\nTITLE: Configure Groq in librechat.yaml\nDESCRIPTION: This snippet configures Groq as a language model provider in LibreChat's `librechat.yaml` file. It defines the API key, base URL, available models, and settings for conversation titles. The `apiKey` uses an environment variable `GROQ_API_KEY`. The `baseURL` specifies the Groq API endpoint. The `models` section lists the supported models, with `default` specifying the available models and `fetch: false` indicating that models are not fetched dynamically. `titleConvo: true` enables conversation titles, and `titleModel` sets the model used for generating titles. `modelDisplayLabel` sets the display name for Groq.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/groq.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"groq\"\n      apiKey: \"${GROQ_API_KEY}\"\n      baseURL: \"https://api.groq.com/openai/v1/\"\n      models:\n        default: [\n          \"llama3-70b-8192\",\n          \"llama3-8b-8192\",\n          \"llama2-70b-4096\",\n          \"mixtral-8x7b-32768\",\n          \"gemma-7b-it\",\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"mixtral-8x7b-32768\"\n      modelDisplayLabel: \"groq\"\n```\n\n----------------------------------------\n\nTITLE: Configuring DALLE API (Shell)\nDESCRIPTION: This snippet illustrates how to configure DALLE integration within LibreChat, including API keys, system prompts, and base URLs. It also includes configurations for using DALLE via Azure OpenAI.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.9-breaking-changes.mdx#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# DALLE\n#----------------\n# DALLE_API_KEY=\n# DALLE3_API_KEY=\n# DALLE2_API_KEY=\n# DALLE3_SYSTEM_PROMPT=\n# DALLE2_SYSTEM_PROMPT=\n# DALLE_REVERSE_PROXY=\n# DALLE3_BASEURL=\n# DALLE2_BASEURL=\n\n# DALLE (via Azure OpenAI)\n# Note: requires some of the variables above to be set\n#----------------\n# DALLE3_AZURE_API_VERSION=\n# DALLE2_AZURE_API_VERSION=\n```\n\n----------------------------------------\n\nTITLE: Running Ngrok with Docker\nDESCRIPTION: This command runs ngrok within a Docker container, using the provided authentication token and tunneling HTTP traffic on port 80.  It requires Docker to be installed and configured. Replace <your token> with your actual Ngrok authentication token. The -d flag runs the container in detached mode.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/ngrok.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -it -e NGROK_AUTHTOKEN=<your token> ngrok/ngrok http 80\n```\n\n----------------------------------------\n\nTITLE: Excluding Google Safety Settings in .env\nDESCRIPTION: This snippet shows how to exclude Google safety settings by setting the GOOGLE_EXCLUDE_SAFETY_SETTINGS variable to true in the .env file. This reverts the safety settings to the provider defaults.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_EXCLUDE_SAFETY_SETTINGS=true\n```\n\n----------------------------------------\n\nTITLE: Perplexity YAML Configuration\nDESCRIPTION: This YAML configuration sets up Perplexity as a custom endpoint in LibreChat. It defines the API key, base URL, available models, and various settings such as `titleConvo`, `summarize`, and `dropParams`. The `dropParams` setting is used to handle API restrictions by removing incompatible parameters like `stop` and `frequency_penalty`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/perplexity.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"Perplexity\"\n      apiKey: \"${PERPLEXITY_API_KEY}\"\n      baseURL: \"https://api.perplexity.ai/\"\n      models:\n        default: [\n          \"sonar-deep-research\",\n          \"sonar-reasoning-pro\",\n          \"sonar-reasoning\",\n          \"sonar-pro\",\n          \"sonar\",\n          \"r1-1776\"\n          ]\n        fetch: false # fetching list of models is not supported\n      titleConvo: true\n      titleModel: \"llama-3-sonar-small-32k-chat\"\n      summarize: false\n      summaryModel: \"llama-3-sonar-small-32k-chat\"\n      forcePrompt: false\n      dropParams: [\"stop\", \"frequency_penalty\"]\n      modelDisplayLabel: \"Perplexity\"\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Endpoint Configuration Example\nDESCRIPTION: This YAML snippet demonstrates a basic Azure OpenAI endpoint configuration in LibreChat, showcasing how to define groups with API keys, instance names, versions, and model deployments. It enables `gpt-4-vision-preview`, `gpt-3.5-turbo`, and `gpt-4-turbo` for users based on the defined order. It assumes that the API keys are set as environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    groups:\n      - group: \"my-westus\" # arbitrary name\n        apiKey: \"${WESTUS_API_KEY}\"\n        instanceName: \"actual-instance-name\" # name of the resource group or instance\n        version: \"2023-12-01-preview\"\n        models:\n          gpt-4-vision-preview:\n            deploymentName: gpt-4-vision-preview\n            version: \"2024-02-15-preview\"\n          gpt-3.5-turbo: true\n      - group: \"my-eastus\"\n        apiKey: \"${EASTUS_API_KEY}\"\n        instanceName: \"actual-eastus-instance-name\"\n        deploymentName: gpt-4-turbo\n        version: \"2024-02-15-preview\"\n        models:\n          gpt-4-turbo: true\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables JavaScript\nDESCRIPTION: Illustrates how to access environment variables for sensitive credentials. The code checks if the credential is provided in the `fields` object from the frontend; otherwise, it retrieves the credential from the environment variables. It uses the `getServerURL` method to access an environment variable and throws an error if the variable is missing.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nthis.url = fields.SD_WEBUI_URL || this.getServerURL();\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\n// It's recommended you follow this convention when accessing environment variables.\n  getServerURL() {\n    const url = process.env.SD_WEBUI_URL || '';\n    if (!url) {\n      throw new Error('Missing SD_WEBUI_URL environment variable.');\n    }\n    return url;\n  }\n```\n\n----------------------------------------\n\nTITLE: Install Certbot (CentOS)\nDESCRIPTION: Installs Certbot and its Nginx plugin on CentOS for automated SSL/TLS certificate handling.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install certbot python2-certbot-nginx\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Volume Mount for auth.json\nDESCRIPTION: This snippet shows how to mount the `auth.json` file (Service Account JSON key) as a volume in the `docker-compose.override.yml` file for Docker deployments. This allows the Docker container to access the Google Cloud service account credentials. Requires Docker and Docker Compose.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.4'\n\nservices:\n  api:\n    volumes:\n    - type: bind\n      source: ./api/data/auth.json\n      target: /app/api/data/auth.json\n```\n\n----------------------------------------\n\nTITLE: Setting a Static Prompt Prefix in YAML\nDESCRIPTION: This snippet demonstrates how to set a static text prefix to every prompt sent to the model. It provides a consistent context for model responses, acting as the OpenAI `additional_instructions` field when using the \"assistants\" endpoint. The prefix defines the role or persona for the model.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  promptPrefix: \"As a financial advisor, ...\"\n```\n\n----------------------------------------\n\nTITLE: Update MONGO_URI in Docker Compose (YAML)\nDESCRIPTION: This snippet shows how to update the `MONGO_URI` environment variable in the `docker-compose.override.yml` file to use the newly created user credentials. This allows the LibreChat application to connect to MongoDB with the restricted user.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_31\n\nLANGUAGE: YAML\nCODE:\n```\nenvironment:\n      - MONGO_URI=mongodb://user:userpasswd@mongodb:27017/LibreChat\n```\n\n----------------------------------------\n\nTITLE: Ollama Llama3 Alternative Configuration\nDESCRIPTION: This configuration demonstrates an alternative approach to handling the `llama3` stop sequence issue by omitting the `addParams` in the configuration file and utilizing frontend configuration of conversation parameters/presets. It configures default models and basic settings without hardcoding stop sequences in the YAML.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/ollama.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- name: \"Ollama\"\n    apiKey: \"ollama\"\n    baseURL: \"http://host.docker.internal:11434/v1/\"\n    models:\n    default: [\n        \"llama3:latest\",\n        \"mistral\"\n        ]\n    fetch: false # fetching list of models is not supported\n    titleConvo: true\n    titleModel: \"current_model\"\n    modelDisplayLabel: \"Ollama\"\n```\n\n----------------------------------------\n\nTITLE: Updating MONGO_URI in .env file\nDESCRIPTION: Example of updating the MONGO_URI variable in the .env file with the MongoDB connection string obtained from MongoDB Atlas. This allows the LibreChat application to connect to the online database. The connection string includes the username, password, cluster URL, and database name.  The `<password>` placeholder must be replaced with the actual password, and `&w=majority` should be removed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_atlas.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nMONGO_URI=mongodb+srv://username:password@cluster-url.mongodb.net/LibreChat?retryWrites=true\n```\n\n----------------------------------------\n\nTITLE: Configuring together.ai in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure a service to use together.ai. It includes settings for the API key, base URL, available models, and other configuration options. The API key should be set as an environment variable named TOGETHERAI_API_KEY.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/togetherai.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"together.ai\"\n      apiKey: \"${TOGETHERAI_API_KEY}\"\n      baseURL: \"https://api.together.xyz\"\n      models:\n        default: [\n          \"Austism/chronos-hermes-13b\",\n          \"Gryphe/MythoMax-L2-13b\",\n          \"HuggingFaceH4/zephyr-7b-beta\",\n          \"NousResearch/Hermes-2-Theta-Llama-3-70B\",\n          \"NousResearch/Nous-Capybara-7B-V1p9\",\n          \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\",\n          \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n          \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\",\n          \"NousResearch/Nous-Hermes-2-Yi-34B\",\n          \"NousResearch/Nous-Hermes-Llama2-13b\",\n          \"NousResearch/Nous-Hermes-Llama2-70b\",\n          \"NousResearch/Nous-Hermes-llama-2-7b\",\n          \"Open-Orca/Mistral-7B-OpenOrca\",\n          \"Qwen/Qwen1.5-0.5B-Chat\",\n          \"Qwen/Qwen1.5-1.8B-Chat\",\n          \"Qwen/Qwen1.5-110B-Chat\",\n          \"Qwen/Qwen1.5-14B-Chat\",\n          \"Qwen/Qwen1.5-32B-Chat\",\n          \"Qwen/Qwen1.5-4B-Chat\",\n          \"Qwen/Qwen1.5-72B-Chat\",\n          \"Qwen/Qwen1.5-7B-Chat\",\n          \"Qwen/Qwen2-1.5B\",\n          \"Qwen/Qwen2-1.5B-Instruct\",\n          \"Qwen/Qwen2-72B\",\n          \"Qwen/Qwen2-72B-Instruct\",\n          \"Qwen/Qwen2-7B\",\n          \"Qwen/Qwen2-7B-Instruct\",\n          \"Snowflake/snowflake-arctic-instruct\",\n          \"Undi95/ReMM-SLERP-L2-13B\",\n          \"Undi95/Toppy-M-7B\",\n          \"WizardLM/WizardLM-13B-V1.2\",\n          \"allenai/OLMo-7B-Instruct\",\n          \"carson/ml31405bit\",\n          \"carson/ml3170bit\",\n          \"carson/ml318bit\",\n          \"carson/ml318br\",\n          \"codellama/CodeLlama-13b-Instruct-hf\",\n          \"codellama/CodeLlama-34b-Instruct-hf\",\n          \"codellama/CodeLlama-70b-Instruct-hf\",\n          \"codellama/CodeLlama-7b-Instruct-hf\",\n          \"cognitivecomputations/dolphin-2.5-mixtral-8x7b\",\n          \"databricks/dbrx-instruct\",\n          \"deepseek-ai/deepseek-coder-33b-instruct\",\n          \"deepseek-ai/deepseek-llm-67b-chat\",\n          \"garage-bAInd/Platypus2-70B-instruct\",\n          \"google/gemma-2-27b-it\",\n          \"google/gemma-2-9b-it\",\n          \"google/gemma-2b-it\",\n          \"google/gemma-7b-it\",\n          \"gradientai/Llama-3-70B-Instruct-Gradient-1048k\",\n          \"lmsys/vicuna-13b-v1.3\",\n          \"lmsys/vicuna-13b-v1.5\",\n          \"lmsys/vicuna-13b-v1.5-16k\",\n          \"lmsys/vicuna-7b-v1.3\",\n          \"lmsys/vicuna-7b-v1.5\",\n          \"meta-llama/Llama-2-13b-chat-hf\",\n          \"meta-llama/Llama-2-70b-chat-hf\",\n          \"meta-llama/Llama-2-7b-chat-hf\",\n          \"meta-llama/Llama-3-70b-chat-hf\",\n          \"meta-llama/Llama-3-8b-chat-hf\",\n          \"meta-llama/Meta-Llama-3-70B-Instruct\",\n          \"meta-llama/Meta-Llama-3-70B-Instruct-Lite\",\n          \"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n          \"meta-llama/Meta-Llama-3-8B-Instruct\",\n          \"meta-llama/Meta-Llama-3-8B-Instruct-Lite\",\n          \"meta-llama/Meta-Llama-3-8B-Instruct-Turbo\",\n          \"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n          \"meta-llama/Meta-Llama-3.1-70B-Instruct-Reference\",\n          \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n          \"meta-llama/Meta-Llama-3.1-70B-Reference\",\n          \"meta-llama/Meta-Llama-3.1-8B-Instruct-Reference\",\n          \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n          \"microsoft/WizardLM-2-8x22B\",\n          \"mistralai/Mistral-7B-Instruct-v0.1\",\n          \"mistralai/Mistral-7B-Instruct-v0.2\",\n          \"mistralai/Mistral-7B-Instruct-v0.3\",\n          \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n          \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n          \"openchat/openchat-3.5-1210\",\n          \"snorkelai/Snorkel-Mistral-PairRM-DPO\",\n          \"teknium/OpenHermes-2-Mistral-7B\",\n          \"teknium/OpenHermes-2p5-Mistral-7B\",\n          \"togethercomputer/CodeLlama-13b-Instruct\",\n          \"togethercomputer/CodeLlama-34b-Instruct\",\n          \"togethercomputer/CodeLlama-7b-Instruct\",\n          \"togethercomputer/Koala-13B\",\n          \"togethercomputer/Koala-7B\",\n          \"togethercomputer/Llama-2-7B-32K-Instruct\",\n          \"togethercomputer/Llama-3-8b-chat-hf-int4\",\n          \"togethercomputer/Llama-3-8b-chat-hf-int8\",\n          \"togethercomputer/SOLAR-10.7B-Instruct-v1.0-int4\",\n          \"togethercomputer/StripedHyena-Nous-7B\",\n          \"togethercomputer/alpaca-7b\",\n          \"togethercomputer/guanaco-13b\",\n          \"togethercomputer/guanaco-33b\",\n          \"togethercomputer/guanaco-65b\",\n          \"togethercomputer/guanaco-7b\",\n          \"togethercomputer/llama-2-13b-chat\",\n          \"togethercomputer/llama-2-70b-chat\",\n          \"togethercomputer/llama-2-7b-chat\",\n          \"upstage/SOLAR-10.7B-Instruct-v1.0\",\n          \"zero-one-ai/Yi-34B-Chat\"\n        ]\n        fetch: false # fetching list of models is not supported\n      titleConvo: true\n      titleModel: \"togethercomputer/llama-2-7b-chat\"\n      summarize: false\n      summaryModel: \"togethercomputer/llama-2-7b-chat\"\n      forcePrompt: false\n      modelDisplayLabel: \"together.ai\"\n```\n\n----------------------------------------\n\nTITLE: Cloud STT OpenAI Configuration YAML\nDESCRIPTION: Configures the OpenAI Whisper service for cloud Speech-to-Text (STT). It specifies the API key and the model to use. Ensure that the STT_API_KEY environment variable is set.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  stt:\n    openai:\n      apiKey: '${STT_API_KEY}'\n      model: 'whisper-1'\n```\n\n----------------------------------------\n\nTITLE: Connect to MongoDB Shell as Admin (Bash)\nDESCRIPTION: This command connects to the MongoDB container using the `mongosh` shell with the `adminUser` and `securePassword` credentials. It specifies the `admin` authentication database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_25\n\nLANGUAGE: Bash\nCODE:\n```\ndocker exec -it chat-mongodb mongosh -u adminUser -p securePassword --authenticationDatabase admin\n```\n\n----------------------------------------\n\nTITLE: Configure Capabilities for Agents\nDESCRIPTION: This YAML snippet shows how to configure the `capabilities` for the Agents endpoint. This allows you to specify agent capabilities available to all users. This example defines a set of agent capabilities including execute_code, file_search, actions, tools, artifacts, ocr, and chain.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/agents.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ncapabilities:\n  - \"execute_code\"\n  - \"file_search\"\n  - \"actions\"\n  - \"tools\"\n  - \"artifacts\"\n  - \"ocr\"\n  - \"chain\"\n```\n\n----------------------------------------\n\nTITLE: Firebase Storage Rules Configuration\nDESCRIPTION: This snippet displays the Firebase Storage rules configuration required to allow read and write access to the storage bucket. Specifically, it allows access to files within the `/images/{userId}/{fileName}` path. This configuration is crucial for the application to store and retrieve images.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/firebase.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nrules_version = '2';\n\nservice firebase.storage {\n  match /b/{bucket}/o {\n    match /images/{userId}/{fileName} {\n      allow read, write: if true;\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Default Parameters Configuration in YAML\nDESCRIPTION: This YAML snippet defines the default parameters sent to custom endpoints in LibreChat, mimicking the OpenAI API. It includes settings for the model, temperature, top_p, penalties, user identifier, streaming, and message formatting. The `max_tokens` parameter is intentionally omitted to utilize the maximum available tokens.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/default_params.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n{\n  \"model\": \"your-selected-model\",\n  \"temperature\": 1,\n  \"top_p\": 1,\n  \"presence_penalty\": 0,\n  \"frequency_penalty\": 0,\n  \"user\": \"LibreChat_User_ID\",\n  \"stream\": true,\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"hi how are you\",\n    },\n  ],\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Supported MIME Types for Assistants Endpoint in YAML\nDESCRIPTION: This example shows how to configure a list of regular expressions defining the MIME types permitted for upload for the 'assistants' endpoint using the `supportedMimeTypes` option in YAML. This allows precise control over the types of files that can be uploaded.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nassistants:\n  supportedMimeTypes:\n      - \"image/.*\"\n      - \"application/pdf\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Vultr Cloud Inference in librechat.yaml\nDESCRIPTION: This YAML snippet configures Vultr Cloud Inference as a custom model provider within LibreChat. It defines the API key, base URL, available models (llama2-7b-chat-Q5_K_M.gguf, llama2-13b-chat-Q5_K_M.gguf, mistral-7b-Q5_K_M.gguf, zephyr-7b-beta-Q5_K_M.gguf), sets 'fetch' to true, enables title generation, and specifies the model for title generation (llama2-7b-chat-Q5_K_M.gguf). The apiKey field uses an environment variable VULTRINFERENCE_TOKEN for security.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/vultrcloudinference.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncustom:\n  - name: 'Vultr Cloud Inference'\n    apiKey: '${VULTRINFERENCE_TOKEN}'\n    baseURL: 'https://api.vultrinference.com/v1/chat/completions'\n    models:\n      default: [\n        \"llama2-7b-chat-Q5_K_M.gguf\",\n        \"llama2-13b-chat-Q5_K_M.gguf\",\n        \"mistral-7b-Q5_K_M.gguf\",\n        \"zephyr-7b-beta-Q5_K_M.gguf\",\n      ]\n      fetch: true\n    titleConvo: true\n    titleModel: \"llama2-7b-chat-Q5_K_M.gguf\"\n    modelDisplayLabel: \"Vultr Cloud Inference\"\n```\n\n----------------------------------------\n\nTITLE: Configure Apple MLX in YAML\nDESCRIPTION: This YAML configuration allows LibreChat to interact with an Apple MLX model running on localhost. It specifies the API key, base URL, default model, and additional parameters such as `max_tokens` and `stop` sequences for controlling text generation. Note that fetching the list of available models is not supported, and only one model can be run at a time.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/mlx.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - name: \"MLX\"\n      apiKey: \"mlx\"\n      baseURL: \"http://localhost:8080/v1/\"\n      models:\n        default: [\n          \"Meta-Llama-3-8B-Instruct-4bit\"\n          ]\n        fetch: false # fetching list of models is not supported\n      titleConvo: true\n      titleModel: \"current_model\"\n      summarize: false\n      summaryModel: \"current_model\"\n      forcePrompt: false\n      modelDisplayLabel: \"Apple MLX\"\n      addParams:\n            max_tokens: 2000\n            \"stop\": [\n              \"<|eot_id|>\"\n            ]\n```\n\n----------------------------------------\n\nTITLE: Social Logins Configuration in YAML\nDESCRIPTION: This snippet demonstrates how to configure the `socialLogins` array within the registration object. The order of the providers in the array determines their display order on the login/registration page. Ensure each listed provider is properly configured within the system.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/registration.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsocialLogins: [\"google\", \"facebook\", \"github\", \"discord\", \"openid\"]\n```\n\n----------------------------------------\n\nTITLE: Debugging Environment Variables\nDESCRIPTION: These environment variables control debugging and logging behavior within LibreChat. `DEBUG_LOGGING` enables debug logs, while `DEBUG_CONSOLE` enables verbose console logs. `CONSOLE_JSON` outputs logs in JSON format for cloud deployments. `CONSOLE_JSON_STRING_LENGTH` configures the truncation size for console logs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_0\n\nLANGUAGE: environment\nCODE:\n```\nDEBUG_LOGGING=true\n```\n\nLANGUAGE: environment\nCODE:\n```\nDEBUG_CONSOLE=false\n```\n\nLANGUAGE: environment\nCODE:\n```\nCONSOLE_JSON=false\n```\n\nLANGUAGE: environment\nCODE:\n```\nCONSOLE_JSON_STRING_LENGTH=1000\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Base URL with Azure\nDESCRIPTION: These YAML examples demonstrate how to configure a custom base URL for Azure OpenAI API requests in LibreChat. This allows the use of proxying services or overriding the default base URL handling. `${INSTANCE_NAME}` and `${DEPLOYMENT_NAME}` are placeholders for the instance and deployment names.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# librechat.yaml file, under an Azure group:\nendpoints:\n  azureOpenAI:\n    groups:\n      - group: \"group-with-custom-base-url\"\n      baseURL: \"https://example.azure-api.net/${INSTANCE_NAME}/${DEPLOYMENT_NAME}\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# OR\n      baseURL: \"https://${INSTANCE_NAME}.openai.azure.com/openai/deployments/${DEPLOYMENT_NAME}\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# Cloudflare example\n      baseURL: \"https://gateway.ai.cloudflare.com/v1/ACCOUNT_TAG/GATEWAY/azure-openai/${INSTANCE_NAME}/${DEPLOYMENT_NAME}\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nbaseURL: \"https://gateway.ai.cloudflare.com/v1/ACCOUNT_TAG/GATEWAY/azure-openai/my-secret-instance/my-deployment\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nbaseURL: \"${MY_CUSTOM_BASEURL}\"\n```\n\n----------------------------------------\n\nTITLE: Complete Balance Settings in LibreChat YAML\nDESCRIPTION: This YAML snippet shows the complete balance settings that can be configured in the `librechat.yaml` file. It includes options to enable token credit balances, set the initial balance, configure auto-refill settings with interval and amount, providing a structured way to manage user balances instead of using environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/token_usage.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 1.2.1\n\n# Balance settings\nbalance:\n  enabled: true                # Enable token credit balances for users\n  startBalance: 20000          # Initial tokens credited upon registration\n  autoRefillEnabled: false     # Enable automatic token refills\n  refillIntervalValue: 30      # Numerical value for refill interval\n  refillIntervalUnit: \"days\"   # Time unit for refill interval (days, hours, etc.)\n  refillAmount: 10000          # Tokens added during each refill\n```\n\n----------------------------------------\n\nTITLE: Configuring the 'baseURL' field in YAML\nDESCRIPTION: This snippet demonstrates different ways to configure the `baseURL` field for a custom endpoint. It can be set to a direct URL, an environment variable reference, or 'user_provided' to allow the user to input the base URL. This field is required.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nbaseURL: \"https://api.mistral.ai/v1\"\n```\n\nLANGUAGE: YAML\nCODE:\n```\nbaseURL: \"${MISTRAL_BASE_URL}\"\n```\n\nLANGUAGE: YAML\nCODE:\n```\nbaseURL: \"user_provided\"\n```\n\n----------------------------------------\n\nTITLE: API Key Environment Variables\nDESCRIPTION: These environment variables store API keys for various AI providers.  Each variable corresponds to a specific provider and stores the API key required to access their services.  These keys are necessary for LibreChat to interface with these AI models.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_4\n\nLANGUAGE: environment\nCODE:\n```\n# ANYSCALE_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# APIPIE_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# COHERE_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# FIREWORKS_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# GROQ_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# MISTRAL_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# OPENROUTER_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# PERPLEXITY_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# SHUTTLEAI_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# TOGETHERAI_API_KEY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# DEEPSEEK_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Start MeiliSearch with Master Key\nDESCRIPTION: This command starts the MeiliSearch server, using the provided master key for authentication.  Replace `<your_master_key>` with the actual key generated in the previous step. MeiliSearch will run on the default port 7700.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/meilisearch.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./meilisearch --master-key=<your_master_key>\n```\n\n----------------------------------------\n\nTITLE: Setting Max Output Tokens for Google/Anthropic in YAML\nDESCRIPTION: This snippet shows how to set the `maxOutputTokens` parameter in a YAML preset for Google and Anthropic endpoints. This parameter limits the maximum number of tokens in the response and is equivalent to `max_tokens` for these providers. In this example, it is set to 2048.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_25\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  maxOutputTokens: 2048\n```\n\n----------------------------------------\n\nTITLE: Setting Azure AI Search Environment Variables\nDESCRIPTION: This snippet defines the required environment variables for connecting to Azure AI Search. It includes the service endpoint URL, the index name, and the API key for authentication.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/tools/azure_ai_search.mdx#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nAZURE_AI_SEARCH_SERVICE_ENDPOINT=\"...\"\nAZURE_AI_SEARCH_INDEX_NAME=\"...\"\nAZURE_AI_SEARCH_API_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Strict Function Calling (JSON)\nDESCRIPTION: This JSON snippet demonstrates how to configure strict function calling for actions in the OpenAI Assistants API using the `x-strict` flag within the OpenAPI specification. Setting `x-strict` to `true` enables strict mode for function calls. Note that strict mode has limitations on supported JSON types.  It allows to define what the function call should be in the OpenAPI spec and have the agent respect it.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/assistants.mdx#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"openapi\": \"3.1.0\",\n  \"info\": {\n    \"title\": \"Math.js API\",\n    \"description\": \"API for performing mathematical operations, such as addition, subtraction, etc.\",\n    \"version\": \"1.0.0\"\n  },\n  \"servers\": [\n    {\n      \"url\": \"https://api.mathjs.org/v4\"\n    }\n  ],\n  \"paths\": {\n    \"/\": {\n      \"post\": {\n        \"summary\": \"Evaluate a mathematical expression\",\n        \"description\": \"Sends a mathematical expression in the request body to evaluate.\",\n        \"operationId\": \"math\",\n        \"x-strict\": true,\n        \"parameters\": [\n        \"requestBody\": {\n          \"required\": true,\n          \"content\": {\n            \"application/json\": {\n              \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"expr\": {\n                    \"type\": \"string\",\n                    \"description\": \"The mathematical expression to evaluate (e.g., `2+3`).\"\n                  }\n                },\n                \"required\": [\"expr\"]\n              }\n            }\n          }\n        },\n        \"responses\": {\n          \"200\": {\n            \"description\": \"The result of the evaluated expression.\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"result\": {\n                      \"type\": \"number\",\n                      \"description\": \"The evaluated result of the expression.\"\n                    }\n                  }\n                }\n              }\n            }\n          },\n          \"400\": {\n            \"description\": \"Invalid expression provided.\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"error\": {\n                      \"type\": \"string\",\n                      \"description\": \"Error message describing the invalid expression.\"\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configure allowedProviders for Agents\nDESCRIPTION: This YAML snippet demonstrates how to configure the `allowedProviders` for the Agents endpoint. This parameter specifies a list of endpoint providers that are permitted for use. Only agents configured with these providers can be initialized.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/agents.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nallowedProviders:\n  - openAI\n  - google\n```\n\n----------------------------------------\n\nTITLE: APIpie Configuration in librechat.yaml\nDESCRIPTION: This YAML snippet configures the APIpie integration within LibreChat. It specifies the API key, base URL, default models, and settings for conversation titling, summarization, and parameter dropping.  The API key should be stored in the environment variable APIPIE_API_KEY.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/apipie.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    # APIpie\n    - name: \"APIpie\"\n      apiKey: \"${APIPIE_API_KEY}\"\n      baseURL: \"https://apipie.ai/v1/\"\n      models:\n        default: [\n          \"gpt-4\",\n          \"gpt-4-turbo\",\n          \"gpt-3.5-turbo\",\n          \"claude-3-opus\",\n          \"claude-3-sonnet\",\n          \"claude-3-haiku\",\n          \"llama-3-70b-instruct\",\n          \"llama-3-8b-instruct\",\n          \"gemini-pro-1.5\",\n          \"gemini-pro\",\n          \"mistral-large\",\n          \"mistral-medium\",\n          \"mistral-small\",\n          \"mistral-tiny\",\n          \"mixtral-8x22b\",\n          ]\n        fetch: false\n      titleConvo: true\n      titleModel: \"claude-3-haiku\"\n      summarize: false\n      summaryModel: \"claude-3-haiku\"\n      dropParams: [\"stream\"]\n      modelDisplayLabel: \"APIpie\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Trust Policy for EKS Service Account (IRSA)\nDESCRIPTION: This JSON defines a trust policy for an EKS service account, enabling it to assume a role. The policy allows the EKS OIDC provider to federate access based on the service account and audience, granting permissions through `sts:AssumeRoleWithWebIdentity`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/s3.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::{AWS_ACCOUNT}:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/{EKS_OIDC}\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"oidc.eks.us-east-1.amazonaws.com/id/{EKS_OIDC}:sub\": \"system:serviceaccount:librechat:librechat\",\n          \"oidc.eks.us-east-1.amazonaws.com/id/{EKS_OIDC}:aud\": \"sts.amazonaws.com\"\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configure Mongo Express in Docker Compose (YAML)\nDESCRIPTION: This snippet defines a service for Mongo Express within a Docker Compose override file. It specifies the image, container name, environment variables for connecting to MongoDB, port mapping, and dependency on the MongoDB service. Important: Replace the default username and password with strong credentials.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_mongoexpress.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.4'\n\nservices:\n  mongo-express:\n    image: mongo-express\n    container_name: mongo-express\n    environment:\n      ME_CONFIG_MONGODB_SERVER: mongodb\n      ME_CONFIG_BASICAUTH_USERNAME: admin\n      ME_CONFIG_BASICAUTH_PASSWORD: password\n    ports:\n      - '8081:8081'\n    depends_on:\n      - mongodb\n    restart: always\n```\n\n----------------------------------------\n\nTITLE: Rate Limiting Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure rate limits for file uploads, conversation imports, speech-to-text (STT), and text-to-speech (TTS) operations in LibreChat. It sets maximum requests per IP and user, along with corresponding time windows in minutes.  These limits help prevent abuse and ensure service stability.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/config.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrateLimits:\n  fileUploads:\n    ipMax: 100\n    ipWindowInMinutes: 60\n    userMax: 50\n    userWindowInMinutes: 60\n  conversationsImport:\n    ipMax: 100\n    ipWindowInMinutes: 60\n    userMax: 50\n    userWindowInMinutes: 60\n  stt:\n    ipMax: 100\n    ipWindowInMinutes: 1\n    userMax: 50\n    userWindowInMinutes: 1\n  tts:\n    ipMax: 100\n    ipWindowInMinutes: 1\n    userMax: 50\n    userWindowInMinutes: 1\n```\n\n----------------------------------------\n\nTITLE: Enable ModSecurity in Nginx Configuration\nDESCRIPTION: Enables ModSecurity and specifies the location of the OWASP CRS rules file within the Nginx configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nmodsecurity on;\nmodsecurity_rules_file /usr/share/nginx/modsecurity-crs/nginx-modsecurity.conf;\n```\n\n----------------------------------------\n\nTITLE: Cloud TTS ElevenLabs Configuration YAML\nDESCRIPTION: Configures the ElevenLabs TTS service for cloud Text-to-Speech. It specifies the API key, model, and voices to use. Ensure that the TTS_API_KEY environment variable is set.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  tts:\n    elevenlabs:\n      apiKey: '${TTS_API_KEY}'\n      model: 'eleven_multilingual_v2'\n      voices: ['202898wioas09d2', 'addwqr324tesfsf', '3asdasr3qrq44w', 'adsadsa']\n```\n\n----------------------------------------\n\nTITLE: Configuring 'titleMethod' in YAML\nDESCRIPTION: This snippet shows how to configure the `titleMethod` to be used when `titleConvo` is enabled. It demonstrates setting the method to `completion`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\ntitleMethod: \"completion\"\n```\n\n----------------------------------------\n\nTITLE: Setting RAG API URL in .env for OpenAI Embedding\nDESCRIPTION: This snippet shows how to set the `RAG_API_URL` environment variable in the `.env` file to connect LibreChat with the RAG API using the default OpenAI embedding configuration. Additionally, it includes the optional `RAG_OPENAI_API_KEY` if the OpenAI key is set to 'user_provided'.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/rag_api.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nRAG_API_URL=http://host.docker.internal:8000\nRAG_OPENAI_API_KEY=sk-your-openai-api-key-example\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq Custom Endpoint\nDESCRIPTION: This snippet configures a custom endpoint for the Groq AI service within the LibreChat application. It defines the API key, base URL, models, and settings such as enabling conversation titles, specifying a title model, and setting a display label. The API key should be set using the environment variable `${GROQ_API_KEY}`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/example.mdx#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n- name: 'groq'\n  apiKey: '${GROQ_API_KEY}'\n  baseURL: 'https://api.groq.com/openai/v1/'\n  models:\n    default: [\n      \"llama3-70b-8192\",\n      \"llama3-8b-8192\",\n      \"llama2-70b-4096\",\n      \"mixtral-8x7b-32768\",\n      \"gemma-7b-it\",\n      ]\n    fetch: false\n  titleConvo: true\n  titleModel: 'mixtral-8x7b-32768'\n  modelDisplayLabel: 'groq'\n```\n\n----------------------------------------\n\nTITLE: MongoDB Connection String Example\nDESCRIPTION: Example MongoDB connection string for connecting to a MongoDB Atlas database. It includes the username, password, cluster URL, and database name. The retryWrites parameter is set to true.  The `<password>` placeholder must be replaced with the actual password, and `&w=majority` should be removed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_atlas.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmongodb+srv://username:password@cluster-url.mongodb.net/LibreChat?retryWrites=true\n```\n\n----------------------------------------\n\nTITLE: Setting Max Tokens Limit in YAML\nDESCRIPTION: This example demonstrates how to set the `max_tokens` parameter in a YAML preset. The `max_tokens` parameter limits the length of the model's output. The model will stop generating tokens once it reaches this limit. Google and Anthropic use `maxOutputTokens` or `maxTokens`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_24\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  max_tokens: 4096\n```\n\n----------------------------------------\n\nTITLE: Start all services with final settings (Docker Compose)\nDESCRIPTION: Starts all services defined in the docker-compose.yml and docker-compose.override.yml files, including MongoDB with authentication enabled and the LibreChat application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Setting Google Search API Key and CSE ID in .env\nDESCRIPTION: This snippet shows how to define the Google Search API key and Google Custom Search Engine ID in the `.env` file. These environment variables are essential for authenticating and using the Google Search plugin.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/tools/google_search.mdx#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nGOOGLE_SEARCH_API_KEY=\"....\"\nGOOGLE_CSE_ID=\"....\"\n```\n\n----------------------------------------\n\nTITLE: Start LibreChat with Docker Compose (Bash)\nDESCRIPTION: Deploys LibreChat using Docker Compose with the specified compose file in detached mode. This is the primary command to bring the LibreChat application online.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker-compose -f ./deploy-compose.yml up -d\n```\n\n----------------------------------------\n\nTITLE: Ollama Llama3 Stop Sequence Configuration\nDESCRIPTION: This configuration addresses an issue where the `llama3` model doesn't stop generating text. It adds specific stop sequences via the `addParams` field. This is applicable if the default parameters do not include a `stop` parameter and aims to prevent excessive text generation. It should be configured in the `librechat.yaml` file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/ollama.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncustom:\n  - name: \"Ollama\"\n    apiKey: \"ollama\"\n    baseURL: \"http://host.docker.internal:11434/v1/\"\n    models:\n      default: [\n        \"llama3\"\n      ]\n      fetch: false # fetching list of models is not supported\n    titleConvo: true\n    titleModel: \"current_model\"\n    summarize: false\n    summaryModel: \"current_model\"\n    forcePrompt: false\n    modelDisplayLabel: \"Ollama\"\n    addParams:\n        \"stop\": [\n            \"<|start_header_id|>\",\n            \"<|end_header_id|>\",\n            \"<|eot_id|>\",\n            \"<|reserved_special_token\"\n        ]\n```\n\n----------------------------------------\n\nTITLE: OCR Configuration (librechat.yaml)\nDESCRIPTION: Demonstrates the OCR configuration options within the `librechat.yaml` file. It shows how to specify the Mistral model, API key, base URL, and OCR strategy. The API key can be configured either directly here, or with the environment variable `OCR_API_KEY`. The strategy defaults to `mistral_ocr`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/features/ocr.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# `librechat.yaml`\nocr:\n  mistralModel: \"mistral-ocr-latest\"  # Optional: Specify Mistral model, defaults to \"mistral-ocr-latest\"\n  apiKey: \"your-mistral-api-key\"        # Optional: Defaults to OCR_API_KEY env variable\n  baseURL: \"https://api.mistral.ai/v1\"  # Optional: Defaults to OCR_BASEURL env variable, or Mistral's API if no variable set\n  strategy: \"mistral_ocr\"               # Optional: Defaults to \"mistral_ocr\" (only option currently available)\n```\n\n----------------------------------------\n\nTITLE: Specifying Assistant ID in YAML\nDESCRIPTION: This snippet shows how to set the ID of an assistant for the `assistants` or `azureAssistants` endpoints. Similar to agent configurations, model options are typically excluded in favor of the assistant's configuration. This example sets the `assistant_id` to a specific value.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  assistant_id: \"asst_someUniqueId\"\n```\n\n----------------------------------------\n\nTITLE: MongoDB Configuration File (mongodb.conf)\nDESCRIPTION: This YAML configuration file configures MongoDB to listen on port 27017 and bind to all IP addresses (0.0.0.0), allowing remote access. Ensure the file path is correct when starting the mongod process.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_community.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Configure Azure OpenAI Model with Object\nDESCRIPTION: This snippet shows how to configure an Azure OpenAI model with a specific deployment name and version using an object configuration. This is useful when needing granular control over model deployment settings.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_config.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nmodels:\n  gpt-4-vision-preview:\n    deploymentName: \"gpt-4-vision-preview\"\n    version: \"2024-02-15-preview\"\n```\n\n----------------------------------------\n\nTITLE: ElevenLabs Additional Parameters Configuration YAML\nDESCRIPTION: Configures additional parameters specific to ElevenLabs TTS service. These include voice settings like similarity_boost, stability, and style, as well as pronunciation dictionary locators.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nvoice_settings:\n  similarity_boost: '' # number\n  stability: '' # number\n  style: '' # number\n  use_speaker_boost: # boolean\npronunciation_dictionary_locators: [''] # list of strings (array)\n```\n\n----------------------------------------\n\nTITLE: Advanced Docker Compose Override Example (YAML)\nDESCRIPTION: This example shows how to locally build the image for the `api` service, use the LibreChat config file, and use an older Mongo that doesn't requires AVX support. It defines the api service with a build context and target, and specifies a different image for the mongodb service.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/docker_override.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.4'\n\nservices:\n  api:\n    volumes:\n      - ./librechat.yaml:/app/librechat.yaml\n    image: librechat\n    build:\n      context: .\n      target: node\n\n  mongodb:\n    image: mongo:4.4.18\n```\n\n----------------------------------------\n\nTITLE: Enable Message Summarization in LibreChat\nDESCRIPTION: This snippet shows how to enable message summarization in LibreChat by setting the `OPENAI_SUMMARIZE` environment variable to `true`. This feature is optional and experimental, and it may affect response time when a summary is being generated. It helps to save tokens in long conversations.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.0.mdx#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\n`OPENAI_SUMMARIZE=true`\n```\n\n----------------------------------------\n\nTITLE: Install Nginx (Ubuntu)\nDESCRIPTION: Installs Nginx on Ubuntu, providing the necessary web server software.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install nginx\n```\n\n----------------------------------------\n\nTITLE: Configure OpenID Authentication\nDESCRIPTION: This snippet shows how to configure OpenID authentication using environment variables like `OPENID_REQUIRED_ROLE`, `OPENID_REQUIRED_ROLE_TOKEN_KIND`, and `OPENID_REQUIRED_ROLE_PARAMETER_PATH`. These variables are used to define the required role and how to extract it from the token.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nOPENID_REQUIRED_ROLE=\nOPENID_REQUIRED_ROLE_TOKEN_KIND=\nOPENID_REQUIRED_ROLE_PARAMETER_PATH=\n```\n\n----------------------------------------\n\nTITLE: Configuring Endpoint Property in Preset\nDESCRIPTION: This YAML snippet demonstrates how to configure the `endpoint` property within the `preset` object. This specifies the endpoint the model communicates with to execute operations, and its value must match the defined custom endpoint name exactly if using a custom endpoint.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npreset:\n  endpoint: \"openAI\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Available AWS Bedrock Models\nDESCRIPTION: This snippet demonstrates how to specify a list of AWS Bedrock models to make available in LibreChat via the BEDROCK_AWS_MODELS environment variable. If this variable is omitted, all known, supported models will be included automatically.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/bedrock.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nBEDROCK_AWS_MODELS=anthropic.claude-3-5-sonnet-20240620-v1:0,meta.llama3-1-8b-instruct-v1:0\n```\n\n----------------------------------------\n\nTITLE: Allowed Domains Configuration in YAML\nDESCRIPTION: This snippet shows how to configure the `allowedDomains` array within the registration object. It specifies the list of email domains that are permitted for user registration. Users with email domains not listed in this array will be restricted from registering.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/registration.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nallowedDomains:\n  - \"gmail.com\"\n  - \"protonmail.com\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto-Generated Titles with Azure\nDESCRIPTION: These snippets demonstrates how to enable auto-generated titles for conversations when using Azure OpenAI. `titleConvo` needs to be set to `true` to activate the feature, and `titleModel` specifies the model used for generating the titles. Using `current_model` as the `titleModel` will use the model currently in use.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n# Example Azure OpenAI Object Structure\nendpoints:\n  azureOpenAI:\n    titleConvo: true # <------- Set this\n    groups:\n    # omitted for brevity\n```\n\nLANGUAGE: titleModel\nCODE:\n```\ntitleModel: \"gpt-3.5-turbo\"\n```\n\nLANGUAGE: titleModel\nCODE:\n```\ntitleModel: \"current_model\"\n```\n\n----------------------------------------\n\nTITLE: Install OWASP CRS (Ubuntu)\nDESCRIPTION: Installs the ModSecurity Core Rule Set (CRS) for Nginx on Ubuntu to enhance web application firewall capabilities.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install nginx-modsecurity-crs\n```\n\n----------------------------------------\n\nTITLE: Custom Endpoint Object Structure in YAML\nDESCRIPTION: This YAML snippet demonstrates the overall structure of a custom endpoint object within the `custom` array of the `endpoints` configuration. It includes example configurations for name, apiKey, baseURL, models, titleConvo, titleModel, modelDisplayLabel, and dropParams. It showcases how to define an endpoint using the Mistral AI API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nendpoints:\n    custom:\n        # Example using Mistral AI API\n    - name: \"Mistral\"\n        apiKey: \"${YOUR_ENV_VAR_KEY}\"\n        baseURL: \"https://api.mistral.ai/v1\"\n        models: \n        default: [\"mistral-tiny\", \"mistral-small\", \"mistral-medium\", \"mistral-large-latest\"]\n        titleConvo: true\n        titleModel: \"mistral-tiny\" \n        modelDisplayLabel: \"Mistral\"\n        # addParams:\n        #   safe_prompt: true # Mistral specific value for moderating messages\n        # NOTE: For Mistral, it is necessary to drop the following parameters or you will encounter a 422 Error:\n        dropParams: [\"stop\", \"user\", \"frequency_penalty\", \"presence_penalty\"]\n```\n\n----------------------------------------\n\nTITLE: Configuration Path Environment Variable\nDESCRIPTION: This environment variable specifies an alternative location for the LibreChat configuration file (`librechat.yaml`). The path can be absolute, relative, or a URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_2\n\nLANGUAGE: environment\nCODE:\n```\n# CONFIG_PATH=https://raw.githubusercontent.com/danny-avila/LibreChat/main/librechat.example.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Google Models in .env (Generative Language API)\nDESCRIPTION: This snippet shows how to set the list of available Google models for the Generative Language API in the .env file. These models can then be used with LibreChat. Note that the models list is not fetched dynamically.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_MODELS=gemini-1.5-flash-latest,gemini-1.0-pro,gemini-1.0-pro-001,gemini-1.0-pro-latest,gemini-1.0-pro-vision-latest,gemini-1.5-pro-latest,gemini-pro,gemini-pro-vision\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Override Configuration\nDESCRIPTION: This YAML snippet defines the Docker Compose override configuration to add the Ollama service to an existing LibreChat stack. It configures the Ollama image, GPU resources, port mappings, and volume mounts for persistent storage of models.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-02_ollama.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nversion: '3.4'\n\nservices:\n# USE LIBRECHAT CONFIG FILE\n  api:\n    volumes:\n    - type: bind\n      source: ./librechat.yaml\n      target: /app/librechat.yaml\n\n# ADD OLLAMA\n  ollama:\n    image: ollama/ollama:latest\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [compute, utility]\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ./ollama:/root/.ollama\n```\n\n----------------------------------------\n\nTITLE: Capabilities Configuration YAML\nDESCRIPTION: This YAML snippet shows how to define the assistant capabilities available to all users using the `capabilities` option. This allows for restricting which capabilities are enabled for assistants.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\ncapabilities:\n  - \"code_interpreter\"\n  - \"retrieval\"\n  - \"actions\"\n  - \"tools\"\n  - \"image_vision\"\n```\n\n----------------------------------------\n\nTITLE: Optional Azure AI Search Environment Variables\nDESCRIPTION: These optional environment variables allow customization of the search query. `AZURE_AI_SEARCH_API_VERSION` specifies the API version, `AZURE_AI_SEARCH_SEARCH_OPTION_QUERY_TYPE` determines the query type, `AZURE_AI_SEARCH_SEARCH_OPTION_TOP` sets the number of search results to return, and `AZURE_AI_SEARCH_SEARCH_OPTION_SELECT` limits the fields retrieved from each result.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/tools/azure_ai_search.mdx#_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nAZURE_AI_SEARCH_API_VERSION=2023-10-01-Preview\nAZURE_AI_SEARCH_SEARCH_OPTION_QUERY_TYPE=simple\nAZURE_AI_SEARCH_SEARCH_OPTION_TOP=3\nAZURE_AI_SEARCH_SEARCH_OPTION_SELECT=field1, field2, field3\n```\n\n----------------------------------------\n\nTITLE: Implementing the _call Method in a Tool (JavaScript)\nDESCRIPTION: This snippet demonstrates how to implement the `_call` method within a `Tool` class. The `_call` method contains the core functionality of the tool and is executed when the language model decides to use it. It accepts an `input` parameter and returns a result. Returning a string representing an error is preferred over throwing an error to allow the LLM to handle it.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nclass StableDiffusionAPI extends Tool {\n  ...\n  async _call(input) {\n    // Your tool's functionality goes here\n    ...\n    return this.result;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example docker-compose.override.yml file\nDESCRIPTION: Provides an example docker-compose.override.yml file that configures the API service to use the newly created MongoDB user and enables authentication for MongoDB. It also sets up mongo-express for managing the database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\nversion: '3.4'\n\nservices:\n  api:\n    volumes:\n      - ./librechat.yaml:/app/librechat.yaml\n    environment:\n      - MONGO_URI=mongodb://user:userpasswd@mongodb:27017/LibreChat\n  mongodb:\n    command: mongod --auth\n  mongo-express:\n    image: mongo-express\n    container_name: mongo-express\n    environment:\n      ME_CONFIG_MONGODB_SERVER: mongodb\n      ME_CONFIG_BASICAUTH_USERNAME: admin\n      ME_CONFIG_BASICAUTH_PASSWORD: password\n      ME_CONFIG_MONGODB_URL: 'mongodb://adminUser:securePassword@mongodb:27017'\n      ME_CONFIG_MONGODB_ADMINUSERNAME: adminUser\n      ME_CONFIG_MONGODB_ADMINPASSWORD: securePassword\n    ports:\n      - '8081:8081'\n    depends_on:\n      - mongodb\n    restart: always\n```\n\n----------------------------------------\n\nTITLE: Create new admin user (MongoDB)\nDESCRIPTION: Creates a new administrative user in the 'admin' database with specified username, password, and roles. The roles grant the user administrative privileges across all databases.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ndb.createUser({ user: \"adminUser\", pwd: \"securePassword\", roles: [\"userAdminAnyDatabase\", \"readWriteAnyDatabase\"] })\n```\n\n----------------------------------------\n\nTITLE: Configuring File Handling Endpoints in YAML\nDESCRIPTION: This example shows how to configure file handling settings for various endpoints using YAML. It defines file limits, size limits, supported MIME types, and disabled status for the 'assistants', 'openAI', 'default', and 'YourCustomEndpointName' endpoints. The example also includes the configuration of global server file size limit and avatar size limit.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nfileConfig:\n  endpoints:\n    assistants:\n      fileLimit: 5\n      fileSizeLimit: 10\n      totalSizeLimit: 50\n      supportedMimeTypes:\n        - \"image/.*\"\n        - \"application/pdf\"\n    openAI:\n      disabled: true\n    default:\n      totalSizeLimit: 20\n    YourCustomEndpointName:\n      fileLimit: 5\n      fileSizeLimit: 1000\n      supportedMimeTypes:\n        - \"image/.*\"\n  serverFileSizeLimit: 1000\n  avatarSizeLimit: 2\n```\n\n----------------------------------------\n\nTITLE: CORS Configuration for Firebase Storage (JSON)\nDESCRIPTION: This JSON snippet defines the CORS configuration for the Firebase Storage bucket, allowing requests from the specified origin (e.g., \"https://ai.example.com\") with specific methods (GET, POST, DELETE, PUT). This configuration is essential for addressing Cross-Origin Resource Sharing (CORS) issues when accessing the storage from a web application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/firebase.mdx#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"origin\": [\"https://ai.example.com\"],\n    \"method\": [\"GET\", \"POST\", \"DELETE\", \"PUT\"],\n    \"maxAgeSeconds\": 3600\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Reload Nginx configuration\nDESCRIPTION: Reloads the Nginx service to apply the changes made to the configuration file without interrupting existing connections.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl reload nginx\n```\n\n----------------------------------------\n\nTITLE: MongoDB URI Configuration\nDESCRIPTION: This example shows how to format the MongoDB URI for use with LibreChat. The URI includes the server's IP address, port, and database name, ensuring LibreChat can connect to the MongoDB instance. The database name can be customized, and LibreChat will create it if it doesn't already exist.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmongodb://192.168.1.100:27017/librechat\n```\n\n----------------------------------------\n\nTITLE: Generate Authelia Client Secret (Bash)\nDESCRIPTION: This command generates a client secret for Authelia using the `authelia crypto hash generate pbkdf2` command. The secret is used for secure communication between LibreChat and Authelia. It requires the Authelia Docker image to be available.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/authelia.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm authelia/authelia:latest authelia crypto hash generate pbkdf2 --variant sha512 --random --random.length 72 --random.charset rfc3986\n```\n\n----------------------------------------\n\nTITLE: Timeout Configuration YAML\nDESCRIPTION: This YAML snippet demonstrates how to set the maximum runtime for an assistant using the `timeoutMs` option. The timeout is specified in milliseconds and helps manage system load by preventing long-running operations.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\ntimeoutMs: 10000\n```\n\n----------------------------------------\n\nTITLE: Setting Model Name in YAML\nDESCRIPTION: This snippet demonstrates how to set the model name for a preset. The `model` parameter is supported by all endpoints except `agents`. The value of the `model` parameter should match a configured model under the chosen endpoint. In this example, the model name is set to 'gpt-4-turbo'.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_16\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  model: \"gpt-4-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Traefik and LibreChat Docker Compose Configuration (YAML)\nDESCRIPTION: Configures Traefik as a reverse proxy for the LibreChat API service, enabling HTTPS with automatic SSL/TLS certificate management. This configuration defines Traefik's image, ports, volumes, networks, and command-line arguments to set up the proxy and Let's Encrypt integration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/traefik.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3'\n\nservices:\n    api:\n      labels:\n        - \"traefik.enable=true\"\n        - \"traefik.http.routers.librechat.rule=Host(`your.domain.name`)\"\n        - \"traefik.http.routers.librechat.entrypoints=websecure\"\n        - \"traefik.http.routers.librechat.tls.certresolver=leresolver\"\n        - \"traefik.http.services.librechat.loadbalancer.server.port=3080\"\n      networks:\n        - librechat_default\n      volumes:\n        - ./librechat.yaml:/app/librechat.yaml\n  \n    traefik:\n      image: traefik:v3.0\n      ports:\n        - \"80:80\"\n        - \"443:443\"\n      volumes:\n        - \"/var/run/docker.sock:/var/run/docker.sock:ro\"\n        - \"./letsencrypt:/letsencrypt\"\n      networks:\n        - librechat_default\n      command:\n        - \"--log.level=DEBUG\"\n        - \"--api.insecure=true\"\n        - \"--providers.docker=true\"\n        - \"--providers.docker.exposedbydefault=false\"\n        - \"--entrypoints.web.address=:80\"\n        - \"--entrypoints.websecure.address=:443\"\n        - \"--certificatesresolvers.leresolver.acme.tlschallenge=true\"\n        - \"--certificatesresolvers.leresolver.acme.email=your@email.com\"\n        - \"--certificatesresolvers.leresolver.acme.storage=/letsencrypt/acme.json\"\n\n# other configs here #\n\n# NOTE: This needs to be at the bottom of your docker-compose.override.yml\nnetworks:\n  librechat_default:\n    external: true\n```\n\n----------------------------------------\n\nTITLE: LibreChat Environment Variables (.env)\nDESCRIPTION: These environment variables are added to the `.env` file in your LibreChat project folder to enable and configure Authelia integration. Replace `ACTUAL_GENERATED_SECRET_HERE`, `ANY_RANDOM_STRING`, and `https://auth.example.com/.well-known/openid-configuration` with the correct values. `OPENID_USE_END_SESSION_ENDPOINT` is optional.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/authelia.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nALLOW_SOCIAL_LOGIN=true\nOPENID_BUTTON_LABEL='Log in with Authelia'\nOPENID_ISSUER=https://auth.example.com/.well-known/openid-configuration\nOPENID_CLIENT_ID=librechat\nOPENID_CLIENT_SECRET=ACTUAL_GENERATED_SECRET_HERE\nOPENID_SESSION_SECRET=ANY_RANDOM_STRING\nOPENID_CALLBACK_URL=/oauth/openid/callback\nOPENID_SCOPE=\"openid profile email\"\nOPENID_IMAGE_URL=https://www.authelia.com/images/branding/logo-cropped.png\n# Optional: redirects the user to the end session endpoint after logging out\nOPENID_USE_END_SESSION_ENDPOINT=true\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Models (Shell)\nDESCRIPTION: This snippet provides an example of how to configure the allowed OpenAI models in LibreChat. Multiple models are listed as comma-separated values.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.9-breaking-changes.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# OPENAI_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-0301,gpt-3.5-turbo,gpt-4,gpt-4-0613,gpt-4-vision-preview,gpt-3.5-turbo-0613,gpt-3.5-turbo-16k-0613,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview,gpt-3.5-turbo-1106,gpt-3.5-turbo-instruct,gpt-3.5-turbo-instruct-0914,gpt-3.5-turbo-16k\n```\n\n----------------------------------------\n\nTITLE: Starting LibreChat after Update\nDESCRIPTION: This command starts the LibreChat application using Docker Compose after an update.  It recreates the containers with the latest code and images.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: BingAI Endpoint Configuration Variables\nDESCRIPTION: These environment variables configure the BingAI endpoint. `BINGAI_TOKEN` sets the Bing access token, or allows users to provide their own. `BINGAI_HOST` specifies the Bing host URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_6\n\nLANGUAGE: environment\nCODE:\n```\nBINGAI_TOKEN=user_provided\n```\n\nLANGUAGE: environment\nCODE:\n```\n# BINGAI_HOST=https://cn.bing.com\n```\n\n----------------------------------------\n\nTITLE: Update Docker Compose volumes for SSL certificates\nDESCRIPTION: Updates the `deploy-compose.yml` file to mount the SSL certificate files from the host machine to the Docker container. This allows the Nginx container to access the certificates.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nclient:\n  # ...\n  volumes:\n    - ./client/nginx.conf:/etc/nginx/conf.d/default.conf\n    - /etc/letsencrypt/live/<your-domain>:/etc/letsencrypt/live/<your-domain>\n    - /etc/letsencrypt/archive/<your-domain>:/etc/letsencrypt/archive/<your-domain>\n    - /etc/letsencrypt/options-ssl-nginx.conf:/etc/letsencrypt/options-ssl-nginx.conf\n    - /etc/letsencrypt/ssl-dhparams.pem:/etc/letsencrypt/ssl-dhparams.pem\n```\n\n----------------------------------------\n\nTITLE: Start Docker Containers (Bash)\nDESCRIPTION: Uses the `docker-compose up -d` command to start the Traefik and LibreChat containers in detached mode. This command reads the docker-compose.yml and docker-compose.override.yml files to define and start the services.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/traefik.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for LiteLLM\nDESCRIPTION: This snippet shows how to configure the `docker-compose.override.yml` file to integrate LiteLLM with LibreChat. It sets up volumes for the LiteLLM configuration file and Google application credentials, maps the necessary ports, and defines the command to start the LiteLLM proxy server.  This is a minimal working example.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_litellm.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napi:\n  volumes:\n  - type: bind\n    source: ./librechat.yaml\n    target: /app/librechat.yaml\n\n\nlitellm:\n    image: ghcr.io/berriai/litellm:main-latest\n    volumes:\n      - ./litellm/litellm-config.yaml:/app/config.yaml\n      # NOTE: For Google - required auth \"GOOGLE_APPLICATION_CREDENTIALS\" envronment and volume mount\n      # This also means you need to add the `application_default_credentaials.json` file within ~/litellm\n      - ./litellm/application_default_credentials.json:/app/application_default_credentials.json\n    ports:\n      - \"4000:8000\"\n    command: [ \"--config\", \"/app/config.yaml\", \"--port\", \"8000\", \"--num_workers\", \"8\" ]\n    For Google - see above about required auth \"GOOGLE_APPLICATION_CREDENTIALS\" envronment and volume mount\n    environment:\n      GOOGLE_APPLICATION_CREDENTIALS: /app/application_default_credentials.json\n```\n\n----------------------------------------\n\nTITLE: Enabling File Resending in YAML\nDESCRIPTION: This snippet enables the resending of files in scenarios where persistent sessions are not maintained. Setting `resendFiles` to `true` ensures that files are resent to the model if the session is not persistent. This is useful for applications that require file-based context across multiple requests.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  resendFiles: true\n```\n\n----------------------------------------\n\nTITLE: Adding Ngrok Authtoken on Mac\nDESCRIPTION: This command configures the ngrok client on macOS with the provided authentication token. Replace <your token> with your actual Ngrok authentication token. It authenticates your client and allows you to create tunnels.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/ngrok.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nngrok authtoken <your token>\n```\n\n----------------------------------------\n\nTITLE: Defining Anthropic Models\nDESCRIPTION: This snippet shows how to define the Anthropic models available for use by setting the `ANTHROPIC_MODELS` environment variable. The models are specified as a comma-separated list. Ensure the specified models are supported by your Anthropic API key. Available models include claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307, claude-2.1, claude-2, claude-1.2, claude-1, claude-1-100k, claude-instant-1, claude-instant-1-100k.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/anthropic.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nANTHROPIC_MODELS=claude-3-opus-20240229,claude-3-sonnet-20240229,claude-3-haiku-20240307,claude-2.1,claude-2,claude-1.2,claude-1,claude-1-100k,claude-instant-1,claude-instant-1-100k\n```\n\n----------------------------------------\n\nTITLE: Restarting Docker Containers (Bash)\nDESCRIPTION: This command restarts the Docker containers after applying the configuration changes. This ensures that the application uses the updated configuration file. This command stops and restarts the containers defined in the docker-compose.yml file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/setup.mdx#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Setting File Size Limit for Custom Endpoint in YAML\nDESCRIPTION: This example demonstrates setting the maximum size allowed for each individual file, specified in megabytes (MB), for a custom endpoint ('YourCustomEndpointName') using the `fileSizeLimit` option in YAML. This limit ensures that no single file exceeds the specified size.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nYourCustomEndpointName:\n  fileSizeLimit: 1000\n```\n\n----------------------------------------\n\nTITLE: Configure NeurochainAI integration (YAML)\nDESCRIPTION: This YAML snippet configures the NeurochainAI integration within LibreChat. It defines the service name, API key (which needs to be replaced with the actual generated key), base URL, and model settings. The `fetch: true` setting likely enables fetching the model list dynamically. Other options control conversation titles, summarization, and prompt behavior. The `iconURL` points to the service's icon for display in LibreChat.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/neurochain.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"NeurochainAI\"\n      apiKey: \"<=generated api key=>\"\n      baseURL: \"https://ncmb.neurochain.io/v1/\"\n      models:\n        default: [\n          \"Mistral-7B-OpenOrca-GPTQ\"\n        ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"current_model\"\n      summarize: false\n      summaryModel: \"current_model\"\n      forcePrompt: false\n      modelDisplayLabel: \"NeurochainAI\"\n      iconURL: \"https://raw.githubusercontent.com/LibreChat-AI/librechat-config-yaml/refs/heads/main/icons/NeurochainAI.png\"\n```\n\n----------------------------------------\n\nTITLE: Disable Builder Configuration YAML\nDESCRIPTION: This YAML snippet demonstrates how to disable the builder interface for assistants using the `disableBuilder` option. Setting this to `true` restricts direct manual interaction with the assistant.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\ndisableBuilder: false\n```\n\n----------------------------------------\n\nTITLE: Private Assistants Configuration YAML\nDESCRIPTION: This YAML snippet shows how to control assistant privacy using the `privateAssistants` option. When set to `true`, assistants are only accessible to the user who created them.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\nprivateAssistants: false\n```\n\n----------------------------------------\n\nTITLE: Agents Endpoint Configuration with Custom Capabilities\nDESCRIPTION: This YAML snippet showcases an example configuration of the `agents` endpoint with custom capabilities. The builder interface is disabled, and only specific capabilities (`execute_code`, `actions`, `artifacts`, and `ocr`) are enabled.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/agents.mdx#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  agents:\n    disableBuilder: false\n    capabilities:\n      - \"execute_code\"\n      - \"actions\"\n      - \"artifacts\"\n      - \"ocr\"\n```\n\n----------------------------------------\n\nTITLE: Override Docker Compose Configuration for LibreChat\nDESCRIPTION: This snippet overrides the default Docker Compose configuration for the API service to run in development mode. This disables the secure cookie setting, preventing automatic logout on local networks accessed via HTTP. The `docker-compose.override.yml` file must be created in the same directory as `docker-compose.yml`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-05-16_unsecured_http.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nservices:\n  api:\n    command: npm run backend:dev\n```\n\n----------------------------------------\n\nTITLE: Switch to admin database (MongoDB)\nDESCRIPTION: Switches the current database context within the MongoDB shell to the 'admin' database. This database is used to create and manage administrative users.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nuse admin\n```\n\n----------------------------------------\n\nTITLE: Configuring the 'apiKey' field in YAML\nDESCRIPTION: This snippet demonstrates different ways to configure the `apiKey` field for a custom endpoint. It can be set to an environment variable using `${YOUR_VARIABLE}`, a direct API key value, or 'user_provided' to allow the user to input the key. This field is required.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiKey: \"${MISTRAL_API_KEY}\"\n```\n\nLANGUAGE: YAML\nCODE:\n```\napiKey: \"your_api_key\"\n```\n\nLANGUAGE: YAML\nCODE:\n```\napiKey: \"user_provided\"\n```\n\n----------------------------------------\n\nTITLE: Enable Balance Tracking in YAML\nDESCRIPTION: This YAML snippet shows how to enable token credit tracking and balance management for users in LibreChat. By setting `enabled` to `true`, the application will start monitoring token usage.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/balance.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nbalance:\n  enabled: true\n```\n\n----------------------------------------\n\nTITLE: Enabling Plugins with Azure OpenAI\nDESCRIPTION: This code snippet shows how to enable Plugins functionality when using Azure OpenAI with LibreChat. It sets the `plugins` field to `true` within the `azureOpenAI` endpoint configuration. This configuration uses the primary model selected from the frontend for Plugin use.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# Example Azure OpenAI Object Structure\nendpoints:\n  azureOpenAI:\n    plugins: true # <------- Set this\n    groups:\n    # omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Create librechat.yaml file\nDESCRIPTION: Creates and opens the `librechat.yaml` file using the nano text editor to configure LibreChat settings. This is required for customization of the app's settings and endpoints.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nnano librechat.yaml\n```\n\n----------------------------------------\n\nTITLE: Wolfram Alpha AppID Configuration in .env\nDESCRIPTION: Shows how to configure the Wolfram Alpha AppID directly in the `.env` file, bypassing the prompt in LibreChat. This is an alternative configuration method for administrators.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/tools/wolfram.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nWOLFRAM_APP_ID=your_app_id\n```\n\n----------------------------------------\n\nTITLE: Run Docker Compose\nDESCRIPTION: This snippet shows the command to run the LibreChat application using Docker Compose. This command starts all the services defined in the `docker-compose.yml` and `docker-compose.override.yml` files. It assumes that Docker Desktop or Docker Engine is running.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/quick_start/custom_endpoints.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Retrieval Models Configuration YAML\nDESCRIPTION: This YAML snippet demonstrates how to specify the models that support retrieval for the assistants endpoint using the `retrievalModels` option. This allows for customizing which models are used for retrieval capabilities.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\nretrievalModels:\n  - \"gpt-4-turbo-preview\"\n```\n\n----------------------------------------\n\nTITLE: Configuring 'models/default' in YAML\nDESCRIPTION: This snippet shows how to configure the `default` array within the `models` object, specifying the default models to use. This is an array of strings.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  - \"mistral-tiny\"\n  - \"mistral-small\"\n  - \"mistral-medium\"\n```\n\n----------------------------------------\n\nTITLE: Anthropic Endpoint Configuration Variables\nDESCRIPTION: These environment variables configure the Anthropic endpoint. `ANTHROPIC_API_KEY` sets the API key, or allows users to provide their own. `ANTHROPIC_MODELS` specifies a comma-separated list of allowed models. `ANTHROPIC_REVERSE_PROXY` sets a reverse proxy for the endpoint. `ANTHROPIC_TITLE_MODEL` is deprecated and should be replaced with the `titleModel` setting in `librechat.yaml`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_5\n\nLANGUAGE: environment\nCODE:\n```\nDefaults to an empty string.\n```\n\nLANGUAGE: environment\nCODE:\n```\n# ANTHROPIC_MODELS=claude-3-opus-20240229,claude-3-sonnet-20240229,claude-3-haiku-20240307,claude-2.1,claude-2,claude-1.2,claude-1,claude-1-100k,claude-instant-1,claude-instant-1-100k\n```\n\nLANGUAGE: environment\nCODE:\n```\n# ANTHROPIC_REVERSE_PROXY=\n```\n\nLANGUAGE: environment\nCODE:\n```\n# ANTHROPIC_TITLE_MODEL=claude-3-haiku-20240307\n```\n\n----------------------------------------\n\nTITLE: Switch to LibreChat database (MongoDB)\nDESCRIPTION: Switches the current database context within the MongoDB shell to the 'LibreChat' database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nuse LibreChat\n```\n\n----------------------------------------\n\nTITLE: Fetch and order LLM models from APIpie\nDESCRIPTION: This Python script fetches a list of available LLM models from the APIpie API, extracts their IDs, sorts them, and saves the ordered list to a 'models.txt' file. The file is formatted as a JSON array of strings, ready to be included in a YAML configuration file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/apipie.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport requests\n\ndef fetch_and_order_models():\n    # API endpoint\n    url = \"https://apipie.ai/models\"\n\n    # headers as per request example\n    headers = {\"Accept\": \"application/json\"}\n\n    # request parameters\n    params = {\"type\": \"llm\"}\n\n    # make request\n    response = requests.get(url, headers=headers, params=params)\n\n    # parse JSON response\n    data = response.json()\n\n    # extract an ordered list of unique model IDs\n    model_ids = sorted(set([model[\"id\"] for model in data]))\n\n    # write result to a text file\n    with open(\"models.txt\", \"w\") as file:\n        json.dump(model_ids, file, indent=2)\n\n# execute the function\nif __name__ == \"__main__\":\n    fetch_and_order_models()\n```\n\n----------------------------------------\n\nTITLE: Authelia OIDC Configuration (YAML)\nDESCRIPTION: This YAML snippet configures the OIDC client settings within Authelia's `configuration.yml` file. It specifies the client ID, client name, secret, redirect URIs, scopes, and other parameters required for LibreChat to authenticate users via Authelia. Ensure to replace `GENERATED_SECRET_KEY_HERE` and `LIBRECHAT.URL` with actual values.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/authelia.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n  - client_id: 'librechat'\n    client_name: 'LibreChat'\n    client_secret: '$pbkdf2-GENERATED_SECRET_KEY_HERE'\n    public: false\n    authorization_policy: 'two_factor'\n    redirect_uris:\n      - 'https://LIBRECHAT.URL/oauth/openid/callback'\n    scopes:\n      - 'openid'\n      - 'profile'\n      - 'email'\n    userinfo_signing_algorithm: 'none'\n```\n\n----------------------------------------\n\nTITLE: Modifying webui-user.sh for API access (Linux/macOS)\nDESCRIPTION: This snippet demonstrates how to modify the `webui-user.sh` file to enable the API for Stable Diffusion WebUI. It adds the `--api` argument to the command-line arguments, enabling external access to the Stable Diffusion service. This is required for the LibreChat plugin to communicate with Stable Diffusion.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/tools/stable_diffusion.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport COMMANDLINE_ARGS=\"--api\"\n\n#!/bin/bash\n#########################################################\n# Uncomment and change the variables below to your need:#\n#########################################################\n\n# ...rest\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Configuration for Load Balancing and Streaming\nDESCRIPTION: This snippet shows a `litellm-config.yml` example for LiteLLM that demonstrates rate limiting, load balancing across Azure deployments, and streaming with Ollama. It specifies different deployments of `gpt-3.5-turbo` with varying rate limits and configures streaming for `mixtral` and `mistral` models served by Ollama.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_litellm.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: azure/gpt-turbo-small-eu\n      api_base: https://my-endpoint-europe-berri-992.openai.azure.com/\n      api_key: \n      rpm: 6      # Rate limit for this deployment: in requests per minute (rpm)\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: azure/gpt-turbo-small-ca\n      api_base: https://my-endpoint-canada-berri992.openai.azure.com/\n      api_key: \n      rpm: 6\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: azure/gpt-turbo-large\n      api_base: https://openai-france-1234.openai.azure.com/\n      api_key: \n      rpm: 1440\n  - model_name: mixtral\n    litellm_params:\n      model: openai/mixtral:8x7b-instruct-v0.1-q5_K_M      # use openai/* for ollama's openai api compatibility\n      api_base: http://ollama:11434/v1\n      stream: True\n  - model_name: mistral\n    litellm_params:\n      model: openai/mistral                                # use openai/* for ollama's openai api compatibility\n      api_base: http://ollama:11434/v1\n      stream: True\nlitellm_settings:\n  success_callback: [\"langfuse\"]\n  cache: True\n  cache_params:\n    type: \"redis\"\n    supported_call_types: [\"acompletion\", \"completion\", \"embedding\", \"aembedding\"]\ngeneral_settings:\n  master_key: sk_live_SetToRandomValue\n```\n\n----------------------------------------\n\nTITLE: Setting RAG API URL and Embedding Provider to Hugging Face\nDESCRIPTION: This snippet shows how to configure the `.env` file to use Hugging Face embeddings with the RAG API. It sets the `RAG_API_URL`, `EMBEDDINGS_PROVIDER`, and `HF_TOKEN` environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/rag_api.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nRAG_API_URL=http://host.docker.internal:8000\nEMBEDDINGS_PROVIDER=huggingface\nHF_TOKEN=hf-xxxxxxxxxxxxxxxxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: LibreChat Configuration for LiteLLM Proxy\nDESCRIPTION: This snippet shows how to configure the `librechat.yaml` file to integrate LibreChat with the LiteLLM proxy server. It defines a custom endpoint with the base URL of the LiteLLM proxy, configures model fetching, and sets other relevant parameters like `titleConvo` and `modelDisplayLabel`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_litellm.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncustom:\n    - name: \"Lite LLM\"\n      # A place holder - otherwise it becomes the default (OpenAI) key\n      # Provide the key instead in each \"model\" block within \"litellm/litellm-config.yaml\"\n      apiKey: \"sk-from-config-file\"\n      # See the required changes above in \"Start LiteLLM Proxy Server\" step.\n      baseURL: \"http://host.docker.internal:4000\"\n      # A \"default\" model to start new users with. The \"fetch\" will pull the rest of the available models from LiteLLM\n      # More or less this is \"irrelevant\", you can pick any model. Just pick one you have defined in LiteLLM.\n      models:\n        default: [\"gpt-3.5-turbo\"]\n        fetch: true\n      titleConvo: true\n      titleModel: \"gpt-3.5-turbo\"\n      summarize: false\n      summaryModel: \"gpt-3.5-turbo\"\n      forcePrompt: false\n      modelDisplayLabel: \"Lite LLM\"\n```\n\n----------------------------------------\n\nTITLE: Model Specifications Configuration\nDESCRIPTION: This YAML snippet demonstrates a complete example of the `modelSpecs` configuration, including `enforce`, `prioritize`, and a model specification within the `list`.  It showcases a configuration for generating meeting notes from Teams recording transcripts with a specified endpoint, model, and prompt prefix.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodelSpecs:\n  enforce: true\n  prioritize: true\n  list:\n    - name: \"meeting-notes-gpt4\"\n      label: \"Meeting Notes Assistant (GPT4)\"\n      default: true\n      description: \"Generate meeting notes by simply pasting in the transcript from a Teams recording.\"\n      iconURL: \"https://example.com/icon.png\"\n      preset:\n        endpoint: \"azureOpenAI\"\n        model: \"gpt-4-turbo-1106-preview\"\n        maxContextTokens: 128000 # Maximum context tokens\n        max_tokens: 4096 # Maximum output tokens\n        temperature: 0.2\n        modelLabel: \"Meeting Summarizer\"\n        greeting: |-\n          This assistant creates meeting notes based on transcripts of Teams recordings.\n          To start, simply paste the transcript into the chat box.\n        promptPrefix: |-\n          Based on the transcript, create coherent meeting minutes for a business meeting. Include the following sections:\n          - Date and Attendees\n          - Agenda\n          - Minutes\n          - Action Items\n\n          Focus on what items were discussed and/or resolved. List any open action items.\n          The format should be a bulleted list of high level topics in chronological order, and then one or more concise sentences explaining the details.\n          Each high level topic should have at least two sub topics listed, but add as many as necessary to support the high level topic. \n\n          - Do not start items with the same opening words.\n\n          Take a deep breath and be sure to think step by step.\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Models (Shell)\nDESCRIPTION: This snippet shows the configuration for specifying which models can be used with plugins in LibreChat, using a comma-separated list.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.9-breaking-changes.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# PLUGIN_MODELS=gpt-4,gpt-4-turbo-preview,gpt-4-0125-preview,gpt-4-1106-preview,gpt-4-0613,gpt-3.5-turbo,gpt-3.5-turbo-0125,gpt-3.5-turbo-1106,gpt-3.5-turbo-0613\n```\n\n----------------------------------------\n\nTITLE: Configuring Top P Sampling in YAML\nDESCRIPTION: This snippet demonstrates how to set the `top_p` parameter in a YAML preset. The `top_p` parameter, also known as nucleus sampling, controls the randomness of token selection. Lower values (e.g., 0.1) result in more focused and deterministic output, while higher values (e.g., 0.9) increase the randomness.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_21\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  top_p: 0.9\n```\n\n----------------------------------------\n\nTITLE: Adding Ngrok Authtoken on Windows/Linux\nDESCRIPTION: This command adds the ngrok authentication token to the configuration. It's used to authenticate your ngrok client with your ngrok account. Replace <your token> with your actual Ngrok authentication token.  This command needs to be executed in the ngrok terminal or command line interface.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/ngrok.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nngrok config add-authtoken <your token>\n```\n\n----------------------------------------\n\nTITLE: Example .env Configuration (sh)\nDESCRIPTION: Shows a snippet of the `.env` file with the configuration for host, MongoDB URI, MeiliSearch host and analytics settings. These values are critical for correct container networking and service discovery.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nHOST=0.0.0.0\nMONGO_URI=mongodb://librechat-mongodb:27017/LibreChat\nMEILI_HOST=http://librechat-meilisearch:7700\nMEILI_NO_ANALYTICS=true\n```\n\n----------------------------------------\n\nTITLE: Configuring 'models/fetch' in YAML\nDESCRIPTION: This snippet shows how to configure the `fetch` boolean within the `models` object. Setting it to `true` attempts to fetch the model list from the API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\nfetch: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Sandpack Bundler URL with Environment Variable\nDESCRIPTION: This code snippet shows how to configure LibreChat to use a self-hosted Sandpack bundler by setting the `SANDPACK_BUNDLER_URL` environment variable in the `.env` file. This allows for enhanced privacy, security, and control over code execution.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/features/artifacts.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# `.env` file\nSANDPACK_BUNDLER_URL=http://your-bundler-url\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key\nDESCRIPTION: This snippet demonstrates how to set the Anthropic API key environment variable in a `.env` file. It can be set to the actual API key or to `user_provided` to allow users to input their own keys. The environment variable `ANTHROPIC_API_KEY` needs to be set properly for the Anthropic integration to function.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/anthropic.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nANTHROPIC_API_KEY=user_provided\n```\n\n----------------------------------------\n\nTITLE: Setting Global Server File Size Limit in YAML\nDESCRIPTION: This example shows how to set the global maximum file size limit for any file uploaded to the server using the `serverFileSizeLimit` option in YAML. The value is specified in megabytes (MB) and acts as an overarching limit for all endpoints.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nfileConfig:\n  serverFileSizeLimit: 1000\n```\n\n----------------------------------------\n\nTITLE: Setting Total Size Limit for Assistants Endpoint in YAML\nDESCRIPTION: This example demonstrates setting the total maximum size allowed for all files in a single request, specified in megabytes (MB), for the 'assistants' endpoint using the `totalSizeLimit` option in YAML. This setting is crucial for preventing excessive bandwidth and storage usage.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nassistants:\n  totalSizeLimit: 50\n```\n\n----------------------------------------\n\nTITLE: Configure LibreChat .env for MeiliSearch\nDESCRIPTION: These environment variables configure LibreChat to connect to the MeiliSearch instance. `SEARCH=true` enables the search feature.  `MEILI_HOST` specifies the MeiliSearch server address and port.  `MEILI_MASTER_KEY` provides the authentication key. `MEILI_NO_ANALYTICS=true` disables analytics collection.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/meilisearch.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nSEARCH=true\nMEILI_NO_ANALYTICS=true\nMEILI_HOST=http://localhost:7700\nMEILI_MASTER_KEY=<your_master_key>\n```\n\n----------------------------------------\n\nTITLE: Local TTS Coqui Configuration YAML\nDESCRIPTION: Configures a local Coqui instance for Text-to-Speech (TTS). Specifies the URL and available voices. Requires a local Coqui instance.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  tts:\n    localai:\n      url: 'http://localhost:8080/v1/audio/synthesize'\n      voices: ['tts_models/en/ljspeech/glow-tts', 'tts_models/en/ljspeech/tacotron2', 'tts_models/en/ljspeech/waveglow']\n      backend: 'coqui'\n```\n\n----------------------------------------\n\nTITLE: Change Docker Image in Compose File (YAML)\nDESCRIPTION: Example of how to change the docker image used in the deploy-compose.yml file. This example shows switching from the dev to stable api image.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_42\n\nLANGUAGE: yaml\nCODE:\n```\nimage: ghcr.io/danny-avila/librechat-api:latest\n```\n\n----------------------------------------\n\nTITLE: Specifying Assistants Models (Bash)\nDESCRIPTION: This snippet shows how to define the allowed models for the Assistants API using the `ASSISTANTS_MODELS` environment variable. If not specified, the models list will be fetched from OpenAI, but only compatible models will be shown. This allows for control over which models are available for use with the Assistants API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/assistants.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nASSISTANTS_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-16k-0613,gpt-3.5-turbo-16k,gpt-3.5-turbo,gpt-4,gpt-4-0314,gpt-4-32k-0314,gpt-4-0613,gpt-3.5-turbo-0613,gpt-3.5-turbo-1106,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview\n```\n\n----------------------------------------\n\nTITLE: Make Docker Compose executable\nDESCRIPTION: Changes the permissions of the Docker Compose binary to make it executable.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nsudo chmod +x /usr/local/bin/docker-compose\n```\n\n----------------------------------------\n\nTITLE: Configure Custom Endpoints in librechat.yaml\nDESCRIPTION: This YAML configuration demonstrates how to configure custom endpoints for OpenRouter and Ollama within the `librechat.yaml` file. It defines the `version`, `cache` settings, and specifies custom endpoints with their `name`, `apiKey`, `baseURL`, supported `models`, `titleConvo`, `titleModel`, `summarize`, `summaryModel`, `forcePrompt`, and `modelDisplayLabel`. The configuration also shows how to fetch a list of models from an endpoint.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/quick_start/custom_endpoints.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 1.1.4\ncache: true\nendpoints:\n  custom:\n    - name: \"OpenRouter\"\n      apiKey: \"${OPENROUTER_KEY}\"\n      baseURL: \"https://openrouter.ai/api/v1\"\n      models:\n        default: [\"gpt-3.5-turbo\"]\n        fetch: true\n      titleConvo: true\n      titleModel: \"current_model\"\n      summarize: false\n      summaryModel: \"current_model\"\n      forcePrompt: false\n      modelDisplayLabel: \"OpenRouter\"\n    - name: \"Ollama\"\n      apiKey: \"ollama\"\n      baseURL: \"http://host.docker.internal:11434/v1/\"\n      models:\n        default: [\n          \"llama3:latest\",\n          \"command-r\",\n          \"mixtral\",\n          \"phi3\"\n          ]\n        fetch: true # fetching list of models is not supported\n      titleConvo: true\n      titleModel: \"current_model\"\n```\n\n----------------------------------------\n\nTITLE: Setting Google Models in .env (Vertex AI)\nDESCRIPTION: This snippet configures the .env file to define a list of available Google models to be used via Vertex AI. This allows the user to specify which models will be available for use with LibreChat.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_MODELS=gemini-1.5-flash-preview-0514,gemini-1.5-pro-preview-0514,gemini-1.0-pro-vision-001,gemini-1.0-pro-002,gemini-1.0-pro-001,gemini-pro-vision,gemini-1.0-pro\n```\n\n----------------------------------------\n\nTITLE: Enable Model Selection\nDESCRIPTION: Demonstrates how to enable the model selection feature in the LibreChat interface. Setting `modelSelect` to `true` allows users to choose different models within the application. This is required if using `modelSpecs.addedEndpoints`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  modelSelect: true\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Override for RAG API with Hugging Face\nDESCRIPTION: This `docker-compose.override.yml` snippet updates the `rag_api` service to use the `ghcr.io/danny-avila/librechat-rag-api-dev:latest` image, enabling Hugging Face embeddings.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/rag_api.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.4'\n\nservices:\n  rag_api:\n    image: ghcr.io/danny-avila/librechat-rag-api-dev:latest\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Region for Bedrock in YAML\nDESCRIPTION: This snippet shows how to specify the AWS region for Amazon Bedrock endpoints using the `region` parameter in a YAML preset. It is required to target the correct AWS region. This is necessary when using the Bedrock model.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_27\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  region: \"us-east-1\"\n```\n\n----------------------------------------\n\nTITLE: Prometheus Scrape Configuration for LibreChat Metrics\nDESCRIPTION: This snippet shows how to configure Prometheus to scrape metrics from the librechat_exporter. It defines a job named 'librechat' and specifies the target address where the exporter is running.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/metrics.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- job_name: librechat\n  static_configs:\n    - targets:\n        - 'librechat.example.com:8000'\n```\n\n----------------------------------------\n\nTITLE: Enable Agents\nDESCRIPTION: Illustrates how to enable the Agents feature in LibreChat. Setting `agents` to `true` makes the Agents functionality available to users.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  agents: true\n```\n\n----------------------------------------\n\nTITLE: Add Docker Repository GPG Key\nDESCRIPTION: Retrieves the GPG key from the Docker repository and adds it to the system keyring to verify the authenticity of the packages.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n```\n\n----------------------------------------\n\nTITLE: Edit Nginx Configuration (Bash)\nDESCRIPTION: Opens the `nginx.conf` file located in the `client/` directory using the `nano` text editor.  This allows modification of the Nginx configuration for custom domains or advanced settings. Requires the `nano` editor to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\nnano client/nginx.conf\n```\n\n----------------------------------------\n\nTITLE: Balance Configuration Example in YAML\nDESCRIPTION: This YAML snippet demonstrates a complete example of configuring token credit balances for users in LibreChat. It includes settings for enabling/disabling balance tracking, initial token allocation, and automatic refill parameters.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/balance.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nbalance:\n  enabled: false\n  startBalance: 20000\n  autoRefillEnabled: false\n  refillIntervalValue: 30\n  refillIntervalUnit: \"days\"\n  refillAmount: 10000\n```\n\n----------------------------------------\n\nTITLE: Setting File Limit for Assistants Endpoint in YAML\nDESCRIPTION: This example demonstrates setting the maximum number of files allowed in a single upload request for the 'assistants' endpoint, using the `fileLimit` option in YAML.  This setting helps control the volume of uploads and manage server load.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nassistants:\n  fileLimit: 5\n```\n\n----------------------------------------\n\nTITLE: Configuring 'titleModel' in YAML\nDESCRIPTION: This snippet shows how to configure the `titleModel`, specifying which model should be used for title generation. It demonstrates setting the model to 'mistral-tiny' and 'current_model'.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\ntitleModel: \"mistral-tiny\"\n```\n\nLANGUAGE: YAML\nCODE:\n```\ntitleModel: \"current_model\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Bedrock Endpoint in librechat.yaml\nDESCRIPTION: This snippet shows how to configure the Bedrock endpoint within the `librechat.yaml` file. It includes options for setting available regions, stream rate, and the model used for generating conversation titles.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/bedrock.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  bedrock:\n    availableRegions:\n      - \"us-east-1\"\n      - \"us-west-2\"\n    streamRate: 35\n    titleModel: 'anthropic.claude-3-haiku-20240307-v1:0'\n```\n\n----------------------------------------\n\nTITLE: Cloud STT Azure Whisper Configuration YAML\nDESCRIPTION: Configures the Azure OpenAI Whisper service for cloud Speech-to-Text (STT). It specifies the instance name, API key, deployment name, and API version. Ensure that the STT_API_KEY environment variable is set, and the instanceName, deploymentName, and apiVersion are correctly configured for your Azure OpenAI instance.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  stt:\n    azureOpenAI:\n      instanceName: 'instanceName'\n      apiKey: '${STT_API_KEY}'\n      deploymentName: 'deploymentName'\n      apiVersion: 'apiVersion'\n```\n\n----------------------------------------\n\nTITLE: Configure Serverless Endpoint in librechat.yaml\nDESCRIPTION: This YAML configuration shows how to add a serverless inference endpoint to your LibreChat configuration file (`librechat.yaml`) for models hosted on Azure AI Studio. It includes setting the API key, base URL, API version, and model deployment name. The `serverless: true` flag indicates that the endpoint is configured for serverless inference.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    groups:\n    - group: \"serverless-example\"\n      apiKey: \"${LLAMA318B_API_KEY}\"  # arbitrary env var name\n      baseURL: \"https://example.services.ai.azure.com/models/\"\n      version: \"2024-05-01-preview\" # Optional: specify API version\n      serverless: true\n      models:\n        # Must match the deployment name of the model\n        Meta-Llama-3.1-8B-Instruct: true\n```\n\n----------------------------------------\n\nTITLE: Update apt packages after adding Docker repo\nDESCRIPTION: Updates the package lists to include the newly added Docker repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Group Configuration in librechat.yaml\nDESCRIPTION: This code snippet illustrates a group-level configuration example within the `librechat.yaml` file for Azure OpenAI integration. It showcases how to define a group with settings for API key, instance name, deployment details, custom base URL, additional headers, and advanced parameters, and includes the configuration for models within that group. The configuration also specifies parameters to add, drop and whether the prompt should be forced.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    # ... (endpoint-level configurations)\n    groups:\n      - group: \"my-resource-group\"\n        apiKey: \"${AZURE_API_KEY}\"\n        instanceName: \"my-instance\"\n        deploymentName: \"gpt-35-turbo\"\n        version: \"2023-03-15-preview\"\n        baseURL: \"https://my-instance.openai.azure.com/\"\n        additionalHeaders:\n          CustomHeader: \"HeaderValue\"\n        addParams:\n          max_tokens: 2048\n          temperature: 0.7\n        dropParams:\n          - \"frequency_penalty\"\n          - \"presence_penalty\"\n        forcePrompt: false\n        models:\n        # ... (model-level configurations)\n```\n\n----------------------------------------\n\nTITLE: MongoDB Connection String Format\nDESCRIPTION: This shows the standard MongoDB connection string format, with placeholders for the hostname and port. The hostname is the address where MongoDB is running and the port is typically 27017.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_community.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmongodb://[hostname]:[port]\n```\n\n----------------------------------------\n\nTITLE: Create Admin User\nDESCRIPTION: Creates a new administrator user with specified credentials and roles. This user has administrative privileges across all databases.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_19\n\nLANGUAGE: Bash\nCODE:\n```\ndb.createUser({ user: \"adminUser\", pwd: \"securePassword\", roles: [\"userAdminAnyDatabase\", \"readWriteAnyDatabase\"] })\n```\n\n----------------------------------------\n\nTITLE: Adjusting Presence Penalty in YAML\nDESCRIPTION: This example demonstrates setting the `presence_penalty` parameter in a YAML preset. This parameter penalizes the model for repeating topics, encouraging it to explore new and diverse subjects in its responses. This is a common parameter for OpenAI-like APIs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_18\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  presence_penalty: 0.3\n```\n\n----------------------------------------\n\nTITLE: Create LibreChat Network (bash)\nDESCRIPTION: Creates a network named `librechat` using podman. This network allows containers to communicate with each other using their container names as hostnames.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npodman network create librechat\n```\n\n----------------------------------------\n\nTITLE: ElevenLabs Compatible TTS Configuration YAML\nDESCRIPTION: Configures an ElevenLabs compatible TTS service. It specifies the URL, API key, model, and voices to use. This is an example for configuring TTS that is compatible with the ElevenLabs API, allowing the use of alternative services while maintaining a consistent configuration style.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  tts:\n    elevenlabs:\n      url: 'http://host.docker.internal:8080/v1/audio/synthesize'\n      apiKey: '${TTS_API_KEY}'\n      model: 'eleven_multilingual_v2'\n      voices: ['202898wioas09d2', 'addwqr324tesfsf', '3asdasr3qrq44w', 'adsadsa']\n```\n\n----------------------------------------\n\nTITLE: Enabling Prompt Cache for Anthropic in YAML\nDESCRIPTION: This example shows how to enable or disable Anthropic's built-in prompt caching feature using the `promptCache` parameter in a YAML preset.  It improves efficiency by reusing responses for similar prompts.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_26\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  promptCache: true\n```\n\n----------------------------------------\n\nTITLE: Create .env File\nDESCRIPTION: Creates a `.env` file by copying the `.env.example` file. This allows you to configure environment variables for your LibreChat instance.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/npm.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Update LibreChat (NPM)\nDESCRIPTION: Runs the `update:deployed` script defined in `package.json` using NPM. This likely automates the process of pulling the latest code and rebuilding/redeploying the Docker containers. Requires `npm` to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nnpm run update:deployed\n```\n\n----------------------------------------\n\nTITLE: Ollama Configuration Example 1 in LibreChat\nDESCRIPTION: This configuration sets up Ollama integration with LibreChat, specifying the API key, base URL, and a list of default models.  It configures settings such as automatic conversation title generation and dynamic model usage. The baseURL may need adjustment for docker environments. This snippet demonstrates a general setup for various Ollama models.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/ollama.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncustom:\n  - name: \"Ollama\"\n    apiKey: \"ollama\"\n    # use 'host.docker.internal' instead of localhost if running LibreChat in a docker container\n    baseURL: \"http://localhost:11434/v1/chat/completions\"\n    models:\n      default: [\n        \"llama2\",\n        \"mistral\",\n        \"codellama\",\n        \"dolphin-mixtral\",\n        \"mistral-openorca\"\n        ]\n      # fetching list of models is supported but the `name` field must start\n      # with `ollama` (case-insensitive), as it does in this example.\n      fetch: true\n    titleConvo: true\n    titleModel: \"current_model\"\n    summarize: false\n    summaryModel: \"current_model\"\n    forcePrompt: false\n    modelDisplayLabel: \"Ollama\"\n```\n\n----------------------------------------\n\nTITLE: Create a New Git Branch (Bash)\nDESCRIPTION: Creates a new Git branch with the specified name. Used for isolating changes before committing them.  It switches to the new branch.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b <branchname>\n```\n\n----------------------------------------\n\nTITLE: Local STT Whisper Configuration YAML\nDESCRIPTION: Configures the OpenAI Whisper service for local Speech-to-Text (STT). It specifies the URL of the Whisper instance and the model to use. Requires a local Whisper instance running.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  stt:\n    openai:\n      url: 'http://host.docker.internal:8080/v1/audio/transcriptions'\n      model: 'whisper'\n```\n\n----------------------------------------\n\nTITLE: Configuring Prioritize Property\nDESCRIPTION: This YAML snippet shows how to set the `prioritize` property within the `modelSpecs` configuration.  Setting `prioritize` to `false` specifies that model specifications should not take priority over the default configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodelSpecs:\n  prioritize: false\n```\n\n----------------------------------------\n\nTITLE: Setting Avatar Size Limit in YAML\nDESCRIPTION: This example shows how to configure the maximum size allowed for avatar images using the `avatarSizeLimit` option in YAML.  The value is specified in megabytes (MB) and is tailored for user avatar uploads.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nfileConfig:\n  avatarSizeLimit: 2\n```\n\n----------------------------------------\n\nTITLE: sse MCP Server Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration for an `sse` MCP server named `everything`. It specifies the URL to connect to the server using the Server-Sent Events (SSE) protocol.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/mcp_servers.mdx#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\neverything:\n  url: http://localhost:3001/sse\n```\n\n----------------------------------------\n\nTITLE: Defining Complex Prompt with Multiline YAML\nDESCRIPTION: This YAML snippet showcases setting a complex, multiline prompt prefix for the model.  It instructs the model to create meeting minutes from a transcript, including sections for date, attendees, agenda, minutes, and action items. This defines the detailed instructions and formatting guidelines for generating the minutes.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  promptPrefix: |\n    Based on the transcript, create coherent meeting minutes for a business meeting. Include the following sections:\n    - Date and Attendees\n    - Agenda\n    - Minutes\n    - Action Items\n\n    Focus on what items were discussed and/or resolved. List any open action items.\n    The format should be a bulleted list of high level topics in chronological order, and then one or more concise sentences explaining the details.\n    Each high level topic should have at least two sub topics listed, but add as many as necessary to support the high level topic. \n\n    - Do not start items with the same opening words.\n\n    Take a deep breath and be sure to think step by step.\n```\n\n----------------------------------------\n\nTITLE: Create User Script Execution (Local Development)\nDESCRIPTION: Executes the `create-user` script in a local development environment, allowing you to add users when registration is disabled. It assumes you are running the command from the project's root directory and that Node.js and npm are installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/index.mdx#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nnpm run create-user\n```\n\n----------------------------------------\n\nTITLE: Start Nginx service\nDESCRIPTION: Starts the Nginx service, making the web server accessible.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl start nginx\n```\n\n----------------------------------------\n\nTITLE: Set Refill Interval Value in YAML\nDESCRIPTION: This YAML snippet specifies the numerical value for the interval at which token credits are automatically refilled.  For instance, a value of 30 would typically represent a 30-day interval, depending on the unit.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/balance.mdx#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nbalance:\n  refillIntervalValue: 30\n```\n\n----------------------------------------\n\nTITLE: Deepseek Configuration in librechat.yaml\nDESCRIPTION: This YAML configuration block defines how LibreChat interacts with the Deepseek API. It specifies the API key (taken from the DEEPSEEK_API_KEY environment variable), the base URL for API requests, and the supported models. It also configures title generation using the 'deepseek-chat' model and sets a display label for Deepseek.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/deepseek.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: \"Deepseek\"\n      apiKey: \"${DEEPSEEK_API_KEY}\"\n      baseURL: \"https://api.deepseek.com/v1\"\n      models:\n        default: [\"deepseek-chat\", \"deepseek-coder\", \"deepseek-reasoner\"]\n        fetch: false\n      titleConvo: true\n      titleModel: \"deepseek-chat\"\n      modelDisplayLabel: \"Deepseek\"\n```\n\n----------------------------------------\n\nTITLE: Verify Sudo Permissions\nDESCRIPTION: This command verifies that the user has been successfully added to the sudo group.  It displays the members of the sudo group.  If the username is listed, the user has sudo permissions.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/digitalocean.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngetent group sudo | cut -d: -f4\n```\n\n----------------------------------------\n\nTITLE: Importing and Integrating a Tool in handleTools.js (JavaScript)\nDESCRIPTION: This snippet shows how to import a tool module into `handleTools.js` and integrate it into the `toolConstructors` object. The `toolConstructors` object maps a tool's name (pluginKey) to its constructor function, enabling the tool to be loaded and used by the LibreChat system. The plugin key should match the 'name' property defined in the tool class.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n/* api\\app\\langchain\\tools\\handleTools.js */\nconst StableDiffusionAPI = require('./StableDiffusion');\n...\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst loadTools = async ({ user, model, tools = [], options = {} }) => {\n  const toolConstructors = {\n    calculator: Calculator,\n    google: GoogleSearchAPI,\n    wolfram: WolframAlphaAPI,\n    'dall-e': OpenAICreateImage,\n    'stable-diffusion': StableDiffusionAPI // <----- Newly Added. Note: the key is the 'name' provided in the class. \n    // We will now refer to this name as the `pluginKey`\n  };\n```\n\n----------------------------------------\n\nTITLE: Configuring 'summaryModel' in YAML\nDESCRIPTION: This snippet demonstrates how to configure the model to use for summarization if the `summarize` field is set to `true`. It demonstrates setting the model to 'mistral-tiny'.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\nsummaryModel: \"mistral-tiny\"\n```\n\n----------------------------------------\n\nTITLE: Configuring 'forcePrompt' in YAML\nDESCRIPTION: This snippet shows how to configure `forcePrompt`. If set to true, the `prompt` parameter is sent instead of `messages`, enabling the `/completions` endpoint.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\nforcePrompt: false\n```\n\n----------------------------------------\n\nTITLE: Agents Endpoint Configuration Example\nDESCRIPTION: This YAML snippet demonstrates the basic structure for configuring the `agents` endpoint, including setting `recursionLimit`, `maxRecursionLimit`, and `disableBuilder`. It shows how to define the agent's behavior and restrictions.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/agents.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  agents:\n    recursionLimit: 50\n    maxRecursionLimit: 100\n    disableBuilder: false\n    # (optional) Agent Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.\n    # capabilities: [\"execute_code\", \"file_search\", \"actions\", \"tools\", \"artifacts\", \"ocr\", \"chain\"]\n```\n\n----------------------------------------\n\nTITLE: Update Anthropic Models Configuration\nDESCRIPTION: This snippet shows how to configure the available Anthropic models using the `ANTHROPIC_MODELS` environment variable. The value is a comma-separated list of Anthropic model names.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# ANTHROPIC_MODELS=claude-3-opus-20240229,claude-3-sonnet-20240229,claude-3-haiku-20240307,claude-2.1,claude-2,claude-1.2,claude-1,claude-1-100k,claude-instant-1,claude-instant-1-100k\n```\n\n----------------------------------------\n\nTITLE: Adding Plugin to manifest.json (JSON)\nDESCRIPTION: This snippet shows how to add a plugin's configuration to the `manifest.json` file. This configuration includes the plugin's name, pluginKey, description, icon URL, and authentication settings. The `pluginKey` must match the class name of the Tool class, and `authField` properties must match process.env variable names.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n  {\n    \"name\": \"Calculator\",\n    \"pluginKey\": \"calculator\",\n    \"description\": \"Perform simple and complex mathematical calculations.\",\n    \"icon\": \"https://i.imgur.com/RHsSG5h.png\",\n    \"isAuthRequired\": \"false\",\n    \"authConfig\": []\n  },\n  {\n    \"name\": \"Stable Diffusion\",\n    \"pluginKey\": \"stable-diffusion\",\n    \"description\": \"Generate photo-realistic images given any text input.\",\n    \"icon\": \"https://i.imgur.com/Yr466dp.png\",\n    \"authConfig\": [\n      {\n        \"authField\": \"SD_WEBUI_URL\",\n        \"label\": \"Your Stable Diffusion WebUI API URL\",\n        \"description\": \"You need to provide the URL of your Stable Diffusion WebUI API. For instructions on how to obtain this, see <a href='url'>Our Docs</a>.\"\n      }\n    ]\n  }\n```\n\nLANGUAGE: json\nCODE:\n```\n  [\n  {\n    \"name\": \"Google\",\n    \"pluginKey\": \"google\",\n    \"description\": \"Use Google Search to find information about the weather, news, sports, and more.\",\n    \"icon\": \"https://i.imgur.com/SMmVkNB.png\",\n    \"authConfig\": [\n      {\n        \"authField\": \"GOOGLE_CSE_ID\",\n        \"label\": \"Google CSE ID\",\n        \"description\": \"This is your Google Custom Search Engine ID. For instructions on how to obtain this, see <a href='https://github.com/danny-avila/LibreChat/blob/main/docs/features/plugins/google_search.md'>Our Docs</a>.\"\n      },\n      {\n        \"authField\": \"GOOGLE_SEARCH_API_KEY\",\n        \"label\": \"Google API Key\",\n        \"description\": \"This is your Google Custom Search API Key. For instructions on how to obtain this, see <a href='https://github.com/danny-avila/LibreChat/blob/main/docs/features/plugins/google_search.md'>Our Docs</a>.\"\n      }\n    ]\n  },\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Crypto Keys for LibreChat with Replit Javascript\nDESCRIPTION: This snippet is used to generate the necessary cryptographic keys for LibreChat, including `CREDS_KEY`, `CREDS_IV`, `JWT_SECRET`, and `JWT_REFRESH_SECRET`. It can be run on Replit after creating an account. The generated keys are then used in the LibreChat Docker Compose configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/* This is a placeholder.  The actual Replit code is not provided in the source text. */\n// The Replit code would generate CREDS_KEY, CREDS_IV, JWT_SECRET and JWT_REFRESH_SECRET\n// Example:\n// const CREDS_KEY = 'some_random_key';\n// const CREDS_IV = 'some_random_iv';\n// const JWT_SECRET = 'some_jwt_secret';\n// const JWT_REFRESH_SECRET = 'some_jwt_refresh_secret';\n\n// console.log('CREDS_KEY:', CREDS_KEY);\n// console.log('CREDS_IV:', CREDS_IV);\n// console.log('JWT_SECRET:', JWT_SECRET);\n// console.log('JWT_REFRESH_SECRET:', JWT_REFRESH_SECRET);\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration for Azure Blob Storage\nDESCRIPTION: This snippet shows how to configure environment variables in the .env file to connect LibreChat to Azure Blob Storage. It includes options for using either a connection string or Managed Identity. It also sets whether blobs should be publicly accessible and defines the container name. You must choose either the connection string or the account name for managed identity, not both.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/azure.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Option A: Using a Connection String\nAZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=yourAccountName;AccountKey=yourAccountKey;EndpointSuffix=core.windows.net\n\n# Option B: Using Managed Identity (do not set the connection string if using Managed Identity)\nAZURE_STORAGE_ACCOUNT_NAME=yourAccountName\n\nAZURE_STORAGE_PUBLIC_ACCESS=false\nAZURE_CONTAINER_NAME=files\n```\n\n----------------------------------------\n\nTITLE: Pull Latest Git Changes (Bash)\nDESCRIPTION: Pulls the latest changes from the Git repository.  Assumes the current directory is a Git repository. This updates the local code to the newest version.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\ngit pull \n```\n\n----------------------------------------\n\nTITLE: Pull Latest Project Changes\nDESCRIPTION: Pulls the latest changes from the LibreChat repository.  This command updates the local repository with the most recent version of the code from the remote repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/npm.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit pull\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Override for Mounting Configuration File (YAML)\nDESCRIPTION: This snippet configures Docker Compose to mount the `librechat.yaml` file into the `api` service container. This ensures that the application uses the specified configuration file.  The source is the relative path to the `librechat.yaml` file on the host, and the target is the path within the container.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/setup.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nversion: '3.4'\n\nservices:\n  api:\n    volumes:\n      - type: bind\n        source: ./librechat.yaml\n        target: /app/librechat.yaml\n```\n\n----------------------------------------\n\nTITLE: Removing a User by Username\nDESCRIPTION: This snippet shows how to remove a user from the 'users' table based on their username. Replace `'poop'` with the desired username.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndb.users.remove({ username: 'poop' })\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for AWS Credentials\nDESCRIPTION: This Bash snippet demonstrates setting environment variables required for configuring AWS credentials when not using IRSA. It sets the AWS access key ID, secret access key, region, bucket name, and optional endpoint URL, which the LibreChat application utilizes to interact with the S3 bucket.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/s3.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nAWS_ACCESS_KEY_ID=your_access_key_id\nAWS_SECRET_ACCESS_KEY=your_secret_access_key\nAWS_REGION=your_selected_region\nAWS_BUCKET_NAME=your_bucket_name\nAWS_ENDPOINT_URL=your_endpoint_url\n```\n\n----------------------------------------\n\nTITLE: Executing Commands in Ollama Container\nDESCRIPTION: This bash command allows you to access the Ollama container's shell for executing commands directly within the environment. It's used for managing models, running inference, and other administrative tasks.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-02_ollama.mdx#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ndocker exec -it ollama /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Actions Object Structure Example YAML\nDESCRIPTION: Defines the structure for the actions object in YAML format, showcasing the 'allowedDomains' property. It specifies a list of domains that agents and assistants are permitted to interact with. This configuration is used to restrict actions to specified domains for security and control.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/actions.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n# Example Actions Object Structure\nactions:\n  allowedDomains:\n    - \"swapi.dev\"\n    - \"librechat.ai\"\n    - \"google.com\"\n```\n\n----------------------------------------\n\nTITLE: Configuring 'modelDisplayLabel' in YAML\nDESCRIPTION: This snippet shows how to configure `modelDisplayLabel`, which sets the label displayed next to the icon for the AI model in messages. It demonstrates setting the label to \"Mistral\".\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_14\n\nLANGUAGE: YAML\nCODE:\n```\nmodelDisplayLabel: \"Mistral\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Assistants API (Shell)\nDESCRIPTION: This snippet illustrates how to configure the Assistants API in LibreChat using environment variables, including the API key and allowed models. Note the use of a fallback value.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.9-breaking-changes.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nASSISTANTS_API_KEY=user_provided\n# ASSISTANTS_BASE_URL=\n# ASSISTANTS_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-16k-0613,gpt-3.5-turbo-16k,gpt-3.5-turbo,gpt-4,gpt-4-0314,gpt-4-32k-0314,gpt-4-0613,gpt-3.5-turbo-0613,gpt-3.5-turbo-1106,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview\n```\n\n----------------------------------------\n\nTITLE: Starting Ngrok Tunnel on Windows/Linux\nDESCRIPTION: This command starts an ngrok tunnel, forwarding HTTP traffic to port 3080 on your local machine. Ensure LibreChat or the desired service is running on port 3080 before running this command. This will expose your local server to the internet via a public URL provided by ngrok.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/ngrok.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nngrok http 3080\n```\n\n----------------------------------------\n\nTITLE: WolframAlphaAPI Tool Implementation (JavaScript)\nDESCRIPTION: This code provides a complete implementation example of the `WolframAlphaAPI` tool, demonstrating how to fetch data from an external API using `axios`, handle errors, and format the API request. The `_call` method is responsible for making the API call and returning the result or an error message.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst axios = require('axios');\nconst { Tool } = require('langchain/tools');\n\nclass WolframAlphaAPI extends Tool {\n  constructor(fields) {\n    super();\n    this.name = 'wolfram';\n    this.apiKey = fields.WOLFRAM_APP_ID || this.getAppId();\n    this.description = `Access computation, math, curated knowledge & real-time data through wolframAlpha...`;\n  }\n\n  async fetchRawText(url) {\n    try {\n      const response = await axios.get(url, { responseType: 'text' });\n      return response.data;\n    } catch (error) {\n      console.error(`Error fetching raw text: ${error}`);\n      throw error\n\n    }\n  }\n\n  getAppId() {\n    const appId = process.env.WOLFRAM_APP_ID || '';\n    if (!appId) {\n      throw new Error('Missing WOLFRAM_APP_ID environment variable.');\n    }\n    return appId;\n  }\n\n  createWolframAlphaURL(query) {\n    const formattedQuery = query.replaceAll(/`/g, '').replaceAll(/\\n/g, ' ');\n    const baseURL = 'https://www.wolframalpha.com/api/v1/llm-api';\n    const encodedQuery = encodeURIComponent(formattedQuery);\n    const appId = this.apiKey || this.getAppId();\n    const url = `${baseURL}?input=${encodedQuery}&appid=${appId}`;\n    return url;\n  }\n\n  async _call(input) {\n    try {\n      const url = this.createWolframAlphaURL(input);\n      const response = await this.fetchRawText(url);\n      return response;\n    } catch (error) {\n      if (error.response && error.response.data) {\n        console.log('Error data:', error.response.data);\n        return error.response.data;\n      } else {\n        console.log(`Error querying Wolfram Alpha`, error.message);\n        return 'There was an error querying Wolfram Alpha.';\n      }\n    }\n  }\n}\n\nmodule.exports = WolframAlphaAPI;\n```\n\n----------------------------------------\n\nTITLE: Start LibreChat Backend\nDESCRIPTION: Starts the LibreChat backend server. This command initiates the backend process, making the application accessible.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/npm.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm run backend\n```\n\n----------------------------------------\n\nTITLE: Stream Rate Configuration YAML\nDESCRIPTION: This YAML configuration shows how to set the rate of processing each new token in milliseconds using the `streamRate` key.  This can help stabilize concurrent requests and improve frontend stream rendering.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/aws_bedrock.mdx#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nstreamRate: 35\n```\n\n----------------------------------------\n\nTITLE: Setting Vertex AI to User Provided Key\nDESCRIPTION: This configures both the Vertex AI Service Account JSON key file and the Generative Language API key to be provided from the frontend.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Note: this configures both the Vertex AI Service Account JSON key file\n# and the Generative Language API key to be provided from the frontend.\nGOOGLE_KEY=user_provided\n```\n\n----------------------------------------\n\nTITLE: Configure LibreChat YAML Path\nDESCRIPTION: This snippet shows how to configure the path to the `librechat.yaml` configuration file using the `CONFIG_PATH` environment variable. It allows specifying an absolute path, a relative path, or a URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n#===============#\n# Configuration #\n#===============#\n# Use an absolute path, a relative path, or a URL\n\n# CONFIG_PATH=\"/alternative/path/to/librechat.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Databricks LibreChat Configuration\nDESCRIPTION: This YAML snippet configures LibreChat to use a Databricks serving endpoint. It specifies the API key, base URL, default model, and settings for title generation. The `directEndpoint` and `titleMessageRole` options are required for Databricks.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/databricks.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: 'Databricks'\n      apiKey: '${DATABRICKS_API_KEY}'\n      baseURL: 'https://your_databricks_serving_endpoint_url_here_ending_with/invocations'\n      models:\n        default: [\n          \"databricks-meta-llama-3-70b-instruct\",\n        ]\n        fetch: false\n      titleConvo: true\n      titleModel: 'current_model'\n      directEndpoint: true # required\n      titleMessageRole: 'user' # required\n```\n\n----------------------------------------\n\nTITLE: Enable Run Code Button\nDESCRIPTION: Illustrates how to enable the 'Run Code' button for Markdown code blocks in LibreChat. Setting `runCode` to `true` allows users to execute code directly from the chat interface using the Code Interpreter API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  runCode: true\n```\n\n----------------------------------------\n\nTITLE: Registration Object Structure Example in YAML\nDESCRIPTION: This example shows the overall structure of the registration object, defining both `socialLogins` and `allowedDomains`. The `socialLogins` array specifies available social login providers, while `allowedDomains` restricts registration to users with specific email domains.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/registration.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nregistration:\n  socialLogins: [\"google\", \"facebook\", \"github\", \"discord\", \"openid\"]\n  allowedDomains:\n    - \"gmail.com\"\n    - \"protonmail.com\"\n```\n\n----------------------------------------\n\nTITLE: Install Docker CE\nDESCRIPTION: Installs the Docker Community Edition (docker-ce) package from the configured repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install docker-ce\n```\n\n----------------------------------------\n\nTITLE: Starting Ngrok Tunnel on Mac\nDESCRIPTION: This command starts an ngrok tunnel on macOS, directing HTTP traffic from the public ngrok URL to port 3080 on your local machine. Ensure the ngrok executable has execute permissions and that your local server is running on port 3080. This exposes your local service to the internet.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/ngrok.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./ngrok http 3080\n```\n\n----------------------------------------\n\nTITLE: Configuring Enforce Property\nDESCRIPTION: This YAML snippet demonstrates how to set the `enforce` property within the `modelSpecs` configuration. When set to `true`, it ensures that the model specifications strictly override other configuration settings, potentially leading to conflicts if not managed carefully.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodelSpecs:\n  enforce: true\n```\n\n----------------------------------------\n\nTITLE: Setting Detailed Tool Description JavaScript\nDESCRIPTION: Demonstrates how to add a more detailed description for the tool using the `description_for_model` property, particularly when using Functions. The `description` property is generalized to optimize tokens, while `description_for_model` contains more explicit instructions.  It highlights the format and guidelines for creating visual content with Stable Diffusion.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// ...\n    this.description_for_model = `// Generate images and visuals using text with 'stable-diffusion'.\n// Guidelines:\n// - ALWAYS use {{\"prompt\": \"7+ detailed keywords\", \"negative_prompt\": \"7+ detailed keywords\"}} structure for queries.\n// - Visually describe the moods, details, structures, styles, and/or proportions of the image. Remember, the focus is on visual attributes.\n// - Craft your input by \"showing\" and not \"telling\" the imagery. Think in terms of what you'd want to see in a photograph or a painting.\n// - Here's an example for generating a realistic portrait photo of a man:\n// \"prompt\":\"photo of a man in black clothes, half body, high detailed skin, coastline, overcast weather, wind, waves, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"\n// \"negative_prompt\":\"semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, out of frame, low quality, ugly, mutation, deformed\"\n// - Generate images only once per human query unless explicitly requested by the user`;\n    this.description = 'You can generate images using text with \\'stable-diffusion\\'. This tool is exclusively for visual content.';\n// ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Endpoints (Shell)\nDESCRIPTION: This snippet shows an example configuration for defining the available endpoints for LibreChat. The endpoints are listed as a comma-separated string.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.9-breaking-changes.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# ENDPOINTS=openAI,assistants,azureOpenAI,bingAI,chatGPTBrowser,google,gptPlugins,anthropic\n```\n\n----------------------------------------\n\nTITLE: Configuring 'headers' in YAML\nDESCRIPTION: This snippet shows how to configure additional headers for requests using the `headers` field, including the use of environment variables. The example sets the 'x-api-key' and 'Content-Type' headers.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_17\n\nLANGUAGE: YAML\nCODE:\n```\nheaders:\n  x-api-key: \"${ENVIRONMENT_VARIABLE}\"\n  Content-Type: \"application/json\"\n```\n\n----------------------------------------\n\nTITLE: Rendering YAMLChecker Component in JSX\nDESCRIPTION: This snippet renders the YAMLChecker component within the JSX structure. This will display the YAML checker user interface on the page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/toolkit/yaml_checker.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<YAMLChecker />\n```\n\n----------------------------------------\n\nTITLE: Set OpenRouter API Key\nDESCRIPTION: This snippet demonstrates how to set the `OPENROUTER_KEY` environment variable in the `.env` file. This variable is used to authenticate with the OpenRouter API, as specified in the `librechat.yaml` configuration. The `OPENROUTER_KEY` must be replaced with the actual API key obtained from OpenRouter.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/quick_start/custom_endpoints.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOPENROUTER_KEY=your_openrouter_api_key\n```\n\n----------------------------------------\n\nTITLE: OCR Strategy Configuration\nDESCRIPTION: Sets the OCR strategy to be used.  Available options are `mistral_ocr` for using Mistral's OCR capabilities and `custom_ocr` for using a custom OCR service defined by the baseURL. The default strategy is `mistral_ocr`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/ocr.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nocr:\n  strategy: \"custom_ocr\"\n```\n\n----------------------------------------\n\nTITLE: Add Docker Repository to APT Sources\nDESCRIPTION: Adds the official Docker repository to the APT sources list. This allows the system to install and update Docker packages from the official Docker repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n```\n\n----------------------------------------\n\nTITLE: Install and Boot MeiliSearch Container (bash)\nDESCRIPTION: Installs and boots the MeiliSearch container, named `librechat-meilisearch`, leveraging the `.env` file for configuration and setting up a volume for persistent data.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npodman run \\\n  --name=\"librechat-meilisearch\" \\\n  --network=librechat \\\n  --env-file=\"./.env\" \\\n  -v \"librechat-meilisearch-data:/meili_data\" \\\n  --detach \\\n  docker.io/getmeili/meilisearch:v1.0;\n```\n\n----------------------------------------\n\nTITLE: Stop all running containers\nDESCRIPTION: Stops all running Docker containers defined in the docker-compose configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: OpenAI Compatible TTS Configuration YAML\nDESCRIPTION: Configures an OpenAI compatible TTS service. It specifies the URL, API key, model, and voices to use. This is an example for configuring TTS that is compatible with the OpenAI API, allowing the use of alternative services while maintaining a consistent configuration style.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  tts:\n    openai:\n      url: 'http://host.docker.internal:8080/v1/audio/synthesize'\n      apiKey: '${TTS_API_KEY}'\n      model: 'tts-1'\n      voices: ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']\n```\n\n----------------------------------------\n\nTITLE: Cloud TTS OpenAI Configuration YAML\nDESCRIPTION: Configures the OpenAI TTS service for cloud Text-to-Speech. It specifies the API key, model, and voices to use. Ensure that the TTS_API_KEY environment variable is set.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  tts:\n    openai:\n      apiKey: '${TTS_API_KEY}'\n      model: 'tts-1'\n      voices: ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']\n```\n\n----------------------------------------\n\nTITLE: Set Refill Amount in YAML\nDESCRIPTION: This YAML snippet specifies the number of tokens to be added to the user's balance during each automatic refill. This determines the amount of token credits added at each refill interval.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/balance.mdx#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\nbalance:\n  refillAmount: 10000\n```\n\n----------------------------------------\n\nTITLE: Start MongoDB container (Docker Compose)\nDESCRIPTION: Starts the MongoDB container in detached mode. This allows MongoDB to run in the background while you continue working in the terminal.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose up -d mongodb\n```\n\n----------------------------------------\n\nTITLE: Setting Stop Tokens in YAML\nDESCRIPTION: This example shows how to set stop tokens for the model using the `stop` parameter in a YAML preset. Stop tokens instruct the model to end its response when any of the specified tokens are encountered. In this example, the model will stop when it generates either \"END\" or \"STOP\".\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_20\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  stop:\n    - \"END\"\n    - \"STOP\"\n```\n\n----------------------------------------\n\nTITLE: Adjusting Frequency Penalty in YAML\nDESCRIPTION: This snippet configures the `frequency_penalty` parameter in a YAML preset. This parameter penalizes repeated tokens, reducing redundancy in the model's output. It helps to generate more concise and varied responses and is used by OpenAI-like APIs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_19\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  frequency_penalty: 0.5\n```\n\n----------------------------------------\n\nTITLE: Setting Image Detail Level in YAML\nDESCRIPTION: This example sets the level of detail required for image analysis tasks. It configures the `imageDetail` parameter, which is applicable to models with vision capabilities (OpenAI spec). Supported values are 'low', 'auto', and 'high', defining the image resolution used in the analysis.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  imageDetail: \"high\"\n```\n\n----------------------------------------\n\nTITLE: Check Docker status\nDESCRIPTION: Checks the status of the Docker service using systemctl to ensure it is running.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl status docker\n```\n\n----------------------------------------\n\nTITLE: Start LibreChat Container (bash)\nDESCRIPTION: Starts the main LibreChat container using the locally built image. Exposes port 3080 and uses the `.env` file for configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npodman run \\\n  --name=\"librechat\" \\\n  --network=librechat \\\n  --env-file=\"./.env\" \\\n  -p 3080:3080 \\\n  --detach \\\n  librechat:local;\n```\n\n----------------------------------------\n\nTITLE: Backup Librechat MongoDB Data (bash)\nDESCRIPTION: Backs up the LibreChat MongoDB data volume to a tar archive, including the date in the filename.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npodman volume export librechat-mongodb-data --output \"librechat-mongodb-backup-$(date +\"%d-%m-%Y\").tar\"\n```\n\n----------------------------------------\n\nTITLE: Nginx reverse proxy configuration\nDESCRIPTION: Configures Nginx as a reverse proxy, forwarding client requests to the LibreChat application running on localhost at the specified port.  The configuration also sets necessary headers for WebSocket connections.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_8\n\nLANGUAGE: nginx\nCODE:\n```\nserver {\n    listen 80;\n    server_name your_domain.com;\n\n    location / {\n        proxy_pass http://localhost:app_port;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: MCP Server with Custom Icon Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration for an MCP server named `filesystem`, using a custom icon. It shows how to configure a `stdio` server with arguments and set the `iconPath` to a custom image.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/mcp_servers.mdx#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nfilesystem:\n  command: npx\n  args:\n    - -y\n    - \"@modelcontextprotocol/server-filesystem\"\n    - /home/user/LibreChat/\n  iconPath: /home/user/LibreChat/client/public/assets/logo.svg\n```\n\n----------------------------------------\n\nTITLE: Setting Max Tokens for Bedrock in YAML\nDESCRIPTION: This snippet sets the maximum number of output tokens for Amazon Bedrock endpoints using the `maxTokens` parameter in a YAML preset. It serves the same function as `max_tokens` for other endpoints. In this example, the maximum number of tokens is set to 1024.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_28\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  maxTokens: 1024\n```\n\n----------------------------------------\n\nTITLE: Listing Databases in MongoDB\nDESCRIPTION: This snippet demonstrates how to list all databases available within the MongoDB instance after entering the MongoDB shell.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nshow dbs\n```\n\n----------------------------------------\n\nTITLE: Set Initial User Balance in YAML\nDESCRIPTION: This YAML snippet configures the initial number of tokens credited to a user upon registration. The `startBalance` parameter defines the token amount a new user receives.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/balance.mdx#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nbalance:\n  startBalance: 20000\n```\n\n----------------------------------------\n\nTITLE: Edit .env file\nDESCRIPTION: Opens the `.env` file in the nano text editor to configure environment variables, including secret keys, API keys, and other settings.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nnano .env\n```\n\n----------------------------------------\n\nTITLE: Configuring ModelLabel Property in Preset\nDESCRIPTION: This YAML snippet demonstrates how to configure the `modelLabel` property within the `preset` object. The label is used to identify the model in user interfaces or logs and provides a human-readable name for the model.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\npreset:\n  modelLabel: \"Customer Support Bot\"\n```\n\n----------------------------------------\n\nTITLE: Setting Balances via CLI in LibreChat\nDESCRIPTION: These bash snippets provide commands for manually setting token balances for users in LibreChat, covering both local development and Docker setups. The snippets demonstrate how to execute the `set-balance` script using npm or Docker commands and how to specify the user's email and the desired token balance.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/token_usage.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Local Development\nnpm run set-balance\n\n# Docker (default setup)\ndocker-compose exec api npm run set-balance\n\n# Docker (deployment setup)\ndocker exec -it LibreChat-API /bin/sh -c \"cd .. && npm run set-balance\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Local Development\nnpm run set-balance danny@librechat.ai 1000\n\n# Docker (default setup)\ndocker-compose exec api npm run set-balance danny@librechat.ai 1000\n\n# Docker (deployment setup)\ndocker exec -it LibreChat-API /bin/sh -c \"cd .. && npm run set-balance danny@librechat.ai 1000\"\n```\n\n----------------------------------------\n\nTITLE: Check Docker Status (Bash)\nDESCRIPTION: Confirms that Docker is running by displaying Docker information.  Useful for verifying that the Docker service started successfully.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ndocker info\n```\n\n----------------------------------------\n\nTITLE: Creating a Policy with S3 Permissions for LibreChat\nDESCRIPTION: This JSON defines an IAM policy that grants necessary S3 permissions for LibreChat. Specifically, it enables the application to put, get, list, and delete objects within a specified S3 bucket.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/s3.mdx#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObjectAcl\",\n        \"s3:GetObject\",\n        \"s3:ListBucket\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-example-librechat-bucket/*\",\n        \"arn:aws:s3:::my-example-librechat-bucket\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting Plugin in index.js (JavaScript)\nDESCRIPTION: This snippet demonstrates how to export the plugin in `index.js` under `api/app/clients/tools` to make it available for compilation.  The plugin is added to `module.exports` along with declaring the plugin as const.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst StructuredSD = require('./structured/StableDiffusion');\nconst StableDiffusionAPI = require('./StableDiffusion');\n...\nmodule.exports = {\n  ...\n  StableDiffusionAPI,\n  StructuredSD,\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Create Docker Compose Override\nDESCRIPTION: This snippet shows how to create or edit a `docker-compose.override.yml` file to mount the `librechat.yaml` configuration file into the API service within the Docker container. This allows you to customize the LibreChat configuration without modifying the base Docker Compose file. It ensures the custom `librechat.yaml` is used by the LibreChat API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/quick_start/custom_endpoints.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  api:\n    volumes:\n    - type: bind\n      source: ./librechat.yaml\n      target: /app/librechat.yaml\n```\n\n----------------------------------------\n\nTITLE: Overriding Assistant Instructions in YAML\nDESCRIPTION: This snippet demonstrates how to override an assistant's default instructions using the `instructions` parameter. This setting is distinct from `promptPrefix`, which is used for additional instructions.  This is useful if specific behavior is needed for a single run, instead of globally.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_14\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  instructions: \"Please handle customer queries regarding order status.\"\n```\n\n----------------------------------------\n\nTITLE: Shut down container initially (Docker Compose)\nDESCRIPTION: Stops all running containers defined in the docker-compose.yml and docker-compose.override.yml files. This is a prerequisite to modifying the MongoDB configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: websocket MCP Server Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration for a `websocket` MCP server named `myWebSocketServer`. It specifies the URL to connect to the server using the WebSocket protocol.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/mcp_servers.mdx#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nmyWebSocketServer:\n  url: ws://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Cloning the LibreChat Repository\nDESCRIPTION: This command clones the LibreChat repository from GitHub to your local machine. It is the first step in the installation process and downloads all the necessary files.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/danny-avila/LibreChat.git\n```\n\n----------------------------------------\n\nTITLE: Exit MongoDB shell (Mongosh)\nDESCRIPTION: Exits the MongoDB shell, returning you to the host terminal.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nexit\n```\n\n----------------------------------------\n\nTITLE: Stop Docker Container (NPM)\nDESCRIPTION: Runs the `stop:deployed` script using NPM.  This script is designed to stop the running docker containers. Requires `npm` to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nnpm run stop:deployed\n```\n\n----------------------------------------\n\nTITLE: Disable Compression in LibreChat (.env)\nDESCRIPTION: Sets the DISABLE_COMPRESSION environment variable to true in the LibreChat .env file. This prevents LibreChat from compressing static files when Traefik is handling compression, avoiding redundant processing and improving performance.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/traefik.mdx#_snippet_2\n\nLANGUAGE: dotenv\nCODE:\n```\n# .env file\nDISABLE_COMPRESSION=true\n```\n\n----------------------------------------\n\nTITLE: Render React Components (JSX)\nDESCRIPTION: This snippet renders the `ChangelogHeader` and `Content` components. `<ChangelogHeader />` displays the header, and `<Content />` renders the breaking changes content. The Content component likely parses and renders the MDX content from the specified file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.6.10-breaking-changes.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Delete User Script Execution (Docker Compose)\nDESCRIPTION: Executes the `delete-user` script within the `api` container when using the default `docker-compose.yml` file. This script deletes a user based on their email address. Replace `email@domain.com` with the actual email. Requires Docker and Docker Compose.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/index.mdx#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\ndocker-compose exec api npm run delete-user email@domain.com\n```\n\n----------------------------------------\n\nTITLE: Clone LibreChat Repository\nDESCRIPTION: Clones the LibreChat repository from GitHub to your local machine. This downloads the source code necessary for installation.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/npm.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/danny-avila/LibreChat.git\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Configuration for Multiple Providers\nDESCRIPTION: This snippet provides an example of `litellm-config.yml` to configure LiteLLM proxy with OpenAI, Azure OpenAI, and GCP models. It defines a `model_list` with various models, including API keys and endpoint details.  Note that the keys and project names are placeholders.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_litellm.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  # https://litellm.vercel.app/docs/proxy/quick_start\n\n  # MS Azure\n  - model_name: azure-gpt-4-turbo-preview\n    litellm_params:\n      model: azure/gpt-4-turbo-preview\n      api_base: https://tenant-name.openai.azure.com/\n      api_key: ...\n\n  - model_name: azure-gpt-3.5-turbo\n    litellm_params:\n      model: azure/gpt-35-turbo\n      api_base: https://tenant-name.openai.azure.com/\n      api_key: ...\n\n  - model_name: azure-gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_base: https://tenant-name.openai.azure.com/\n      api_key: ...\n\n  - model_name: azure-gpt-4o\n    litellm_params:\n      model: azure/gpt-4o\n      api_base: https://tenant-name.openai.azure.com/\n      api_key: ...\n\n  - model_name: azure-gpt-3.5-turbo-16k\n    litellm_params:\n      model: azure/gpt-35-turbo-16k\n      api_base: https://tenant-name.openai.azure.com/\n      api_key: ...\n\n  - model_name: azure-gpt-4-32k\n    litellm_params:\n      model: azure/gpt-4-32k\n      api_base: https://tenant-name.openai.azure.com/\n      api_key: ...\n\n\n  # OpenAI\n  - model_name: gpt-4-turbo\n    litellm_params:\n      model: gpt-4-turbo\n      api_key: ...\n\n  - model_name: old-gpt-4-turbo-preview\n    litellm_params:\n      model: gpt-4-turbo-preview\n      api_key: ...\n\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: gpt-3.5-turbo\n      api_key: ...\n\n  - model_name: gpt-4\n    litellm_params:\n      model: gpt-4\n      api_key: ...\n\n  - model_name: gpt-4o\n    litellm_params:\n      model: gpt-4o\n      api_key: ...\n\n  - model_name: gpt-3.5-turbo-16k\n    litellm_params:\n      model: gpt-3.5-turbo-16k\n      api_key: ...\n\n  - model_name: gpt-4-32k\n    litellm_params:\n      model: gpt-4-32k\n      api_key: ...\n\n  - model_name: gpt-4-vision-preview\n    litellm_params:\n      model: gpt-4-vision-preview\n      api_key: ...\n\n\n  # Google Vertex\n  # NOTE: For Google - see above about required auth \"GOOGLE_APPLICATION_CREDENTIALS\" environment and volume mount\n  - model_name: google-chat-bison\n    litellm_params:\n      model: vertex_ai/chat-bison\n      vertex_project: gcp-proj-name\n      vertex_location: us-central1\n\n  - model_name: google-chat-bison-32k\n    litellm_params:\n      model: vertex_ai/chat-bison-32k\n      vertex_project: gcp-proj-name\n      vertex_location: us-central1\n\n  - model_name: google-gemini-pro-1.0\n    litellm_params:\n      model: vertex_ai/gemini-pro\n      vertex_project: gcp-proj-name\n      vertex_location: us-central1\n\n   - model_name: google-gemini-pro-1.5-preview\n    litellm_params:\n      model: vertex_ai/gemini-1.5-pro-preview-0514\n      vertex_project: gcp-proj-name\n      vertex_location: us-central1\n\n  - model_name: google-gemini-flash-1.5-preview\n    litellm_params:\n      model: vertex_ai/gemini-1.5-flash-preview-0514\n      vertex_project: gcp-proj-name\n      vertex_location: us-central1\n\n# NOTE: It may be a good idea to comment out \"success_callback\", \"cache\", \"cache_params\" (both lines under) when you first start until this works!\nlitellm_settings:\n  success_callback: [\"langfuse\"]\n  cache: True\n  cache_params:\n    type: \"redis\"\n    supported_call_types: [\"acompletion\", \"completion\", \"embedding\", \"aembedding\"]\ngeneral_settings:\n  master_key: sk_live_SetToRandomValue\n```\n\n----------------------------------------\n\nTITLE: Configuring Assistants-Compatible Group\nDESCRIPTION: This configuration shows how to configure a group to be compatible with Azure OpenAI's Assistants API. It sets `assistants` to `true` for the group and specifies a version compatible with the Assistants API.  It emphasizes the importance of region and model compatibility as per Azure documentation.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    assistants: true\n    groups:\n      - group: \"my-sweden-group\"\n        apiKey: \"${SWEDEN_API_KEY}\"\n        instanceName: \"actual-instance-name\"\n      # Mark this group as assistants compatible\n        assistants: true\n      # version must be \"2024-02-15-preview\" or later\n        version: \"2024-03-01-preview\"\n        models:\n          # ... (model-level configuration)\n```\n\n----------------------------------------\n\nTITLE: Stop LibreChat MeiliSearch Container (bash)\nDESCRIPTION: Stops the running LibreChat MeiliSearch container.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npodman stop librechat-meilisearch\n```\n\n----------------------------------------\n\nTITLE: Backup Librechat Meilisearch Data (bash)\nDESCRIPTION: Backs up the LibreChat MeiliSearch data volume to a tar archive, including the date in the filename.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npodman volume export librechat-meilisearch-data --output \"librechat-meilisearch-backup-$(date +\"%d-%m-%Y\").tar\"\n```\n\n----------------------------------------\n\nTITLE: Setting the CONFIG_PATH Environment Variable (.env)\nDESCRIPTION: This snippet shows how to set the `CONFIG_PATH` environment variable in a `.env` file. This variable is used to specify a custom path to the `librechat.yaml` configuration file, allowing users to use a remote or alternative configuration location.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nCONFIG_PATH=\"/alternative/path/to/librechat.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Rebase Deployed Application (NPM)\nDESCRIPTION: Runs the `rebase:deployed` script using NPM. This is likely used to rebase a custom branch onto the latest changes.  Requires `npm` to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\nnpm run rebase:deployed\n```\n\n----------------------------------------\n\nTITLE: Create Kubernetes Secret for LibreChat\nDESCRIPTION: This command creates a Kubernetes secret named `librechat` containing sensitive environment variables required by LibreChat, such as `CREDS_KEY`, `CREDS_IV`, `MONGO_URI`, `JWT_SECRET`, and `JWT_REFRESH_SECRET`.  The secret is created using the `kubectl create secret generic` command and the `--from-literal` flag for each variable. You must replace the placeholder values with your own secure values.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/helm_chart.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic librechat \\\n--from-literal=CREDS_KEY=0963cc1e5e5df9554c8dd32435d0eb2b1a8b6edde6596178d5c5418ade897673 \\\n--from-literal=CREDS_IV=46d727a066d5d8c4ebc94305d028fecc \\\n--from-literal=MONGO_URI=mongodb+srv://<user>:<password>@<mongodb-url> \\\n--from-literal=JWT_SECRET=83e5c1f0e037e4f027dbdb332d54ca1bd1f12af6798700c207ed817ebd7c544b \\ --from-literal=JWT_REFRESH_SECRET=83e5c1f0c037e4f027dbab332d54ca1bd1f12af6798700c207ed817ebd7c544\n```\n\n----------------------------------------\n\nTITLE: OCR Mistral Model Configuration\nDESCRIPTION: Sets the specific Mistral model to be used for OCR processing when using the `mistral_ocr` strategy.  This is an optional setting; if not provided, a default Mistral model may be used. The mistralModel value should be a string representing the model name.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/ocr.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nocr:\n  mistralModel: \"mistral-ocr-latest\"\n```\n\n----------------------------------------\n\nTITLE: Deprecated Azure OpenAI Configuration\nDESCRIPTION: This snippet highlights the deprecation of direct environment variable configuration for Azure OpenAI. It indicates that the preferred method is now the `librechat.yaml` configuration file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n#============#\n# Azure      #\n#============#\n\n\n# Note: these variables are DEPRECATED\n# Use the `librechat.yaml` configuration for `azureOpenAI` instead\n# You may also continue to use them if you opt out of using the `librechat.yaml` configuration\n\n# AZURE_OPENAI_DEFAULT_MODEL=gpt-3.5-turbo # Deprecated\n# AZURE_OPENAI_MODELS=gpt-3.5-turbo,gpt-4 # Deprecated\n# AZURE_USE_MODEL_AS_DEPLOYMENT_NAME=TRUE # Deprecated\n# AZURE_API_KEY= # Deprecated\n# AZURE_OPENAI_API_INSTANCE_NAME= # Deprecated\n# AZURE_OPENAI_API_DEPLOYMENT_NAME= # Deprecated\n# AZURE_OPENAI_API_VERSION= # Deprecated\n# AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME= # Deprecated\n# AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME= # Deprecated\n# PLUGINS_USE_AZURE=\"true\" # Deprecated\n## v0.6.10+ (-dev build)\n```\n\n----------------------------------------\n\nTITLE: stdio MCP Server Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration for a `stdio` MCP server named `puppeteer`. It shows how to define the command to execute (`npx`), command line arguments, timeout settings, environment variables, and stderr handling.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/mcp_servers.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\npuppeteer:\n  type: stdio\n  command: npx\n  args:\n    - -y\n    - \"@modelcontextprotocol/server-puppeteer\"\n  timeout: 30000\n  initTimeout: 10000\n  env:\n    NODE_ENV: \"production\"\n  stderr: inherit\n```\n\n----------------------------------------\n\nTITLE: Create User Script Execution (Deploy Compose)\nDESCRIPTION: Executes the `create-user` script within the `LibreChat-API` container when using the `deploy-compose.yml` file (as per the Ubuntu Docker Guide).  This script adds users directly to the database, bypassing registration. Requires Docker and knowledge of container names.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/index.mdx#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ndocker exec -it LibreChat-API /bin/sh -c \"cd .. && npm run create-user\"\n```\n\n----------------------------------------\n\nTITLE: Enable JSON Logging for LibreChat\nDESCRIPTION: This snippet shows how to enable JSON logging in LibreChat for cloud deployments using the `CONSOLE_JSON` environment variable. Setting it to `true` configures the application to output logs in JSON format.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n#===============#\n# JSON Logging  #\n#===============#\n\n# Use when process console logs in cloud deployment like GCP/AWS\nCONSOLE_JSON=false\n```\n\n----------------------------------------\n\nTITLE: Start MongoDB container in detached mode\nDESCRIPTION: Starts the MongoDB container in detached mode, which means it runs in the background.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose up -d mongodb\n```\n\n----------------------------------------\n\nTITLE: Reverting Changes - Interactive Rebase\nDESCRIPTION: Starts an interactive rebase session for the last 'N' commits.  During the interactive rebase, replace 'pick' with 'drop' for each commit that needs to be removed. This is used to selectively undo changes in the feature branch.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ngit rebase -i HEAD~N\n```\n\n----------------------------------------\n\nTITLE: Specify Docker Volume Mapping\nDESCRIPTION: This command shows how to add a volume mapping when creating the container using Docker/Podman. This will map a `librechat.yaml` configuration file from the host system to the container.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n-v \"./librechat.yaml:/app/librechat.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Project Changes with Git\nDESCRIPTION: This command pulls the latest changes from the LibreChat repository using Git. This ensures that you have the most up-to-date code and configuration files.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit pull\n```\n\n----------------------------------------\n\nTITLE: Rename Google API Key\nDESCRIPTION: This snippet highlights the renaming of the environment variable for the Google Search API key from `GOOGLE_API_KEY` to `GOOGLE_SEARCH_API_KEY` to avoid conflicts with the Google Generative AI library.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nGOOGLE_SEARCH_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Define version for Azure OpenAI Model\nDESCRIPTION: This snippet shows how to define the version for an Azure OpenAI model. The version specifies the version of the model to be used.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_config.mdx#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nversion: \"2024-02-15-preview\"\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Up Command\nDESCRIPTION: This command starts the Docker containers in detached mode, which includes LibreChat and the RAG API, based on the configurations defined in the `docker-compose.yml` and any override files.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/rag_api.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Change directory to LibreChat\nDESCRIPTION: Changes the current directory to the cloned LibreChat project directory.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# enter the project directory\ncd LibreChat/\n```\n\n----------------------------------------\n\nTITLE: Make Installation Script Executable (bash)\nDESCRIPTION: Makes the installation script executable, allowing it to be run directly.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x ./install.sh\n```\n\n----------------------------------------\n\nTITLE: Restart MongoDB container with authentication (Docker Compose)\nDESCRIPTION: Restarts the MongoDB container with authentication enabled. This enforces authentication for all connections to the database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose up -d mongodb\n```\n\n----------------------------------------\n\nTITLE: Shut down container after setup (Docker Compose)\nDESCRIPTION: Stops the MongoDB container after the initial admin user setup.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Create user in LibreChat database (MongoDB)\nDESCRIPTION: Creates a user within the 'LibreChat' database with read and write roles specific to that database. This user will be used by the LibreChat application to access the database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\ndb.createUser({ user: 'user', pwd: 'userpasswd', roles: [ { role: \"readWrite\", db: \"LibreChat\" } ] });\n```\n\n----------------------------------------\n\nTITLE: Install npm Dependencies\nDESCRIPTION: Installs the npm dependencies required for LibreChat. This command ensures that all necessary packages are installed based on the `package.json` file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/npm.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm ci\n```\n\n----------------------------------------\n\nTITLE: Clone LibreChat Repository (bash)\nDESCRIPTION: Clones the LibreChat repository from GitHub into the target directory. This is the first step in setting up the LibreChat environment.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/danny-avila/LibreChat\n```\n\n----------------------------------------\n\nTITLE: Configuring xAI in librechat.yaml\nDESCRIPTION: This YAML snippet configures the xAI integration within LibreChat. It sets the API key, base URL, default model, and parameters for conversation title generation. The `apiKey` field is set using an environment variable `${XAI_API_KEY}`. Title is generated using completion method using the `grok-beta` model. Summarization is turned off.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/xai.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - name: \"xai\"\n      apiKey: \"${XAI_API_KEY}\"\n      baseURL: \"https://api.x.ai/v1\"\n      models:\n        default: [\"grok-beta\"]\n        fetch: false\n      titleConvo: true\n      titleMethod: \"completion\"\n      titleModel: \"grok-beta\"\n      summarize: false\n      summaryModel: \"grok-beta\"\n      forcePrompt: false\n      modelDisplayLabel: \"Grok\"\n```\n\n----------------------------------------\n\nTITLE: Set Assistants API Key to User Provided\nDESCRIPTION: This snippet shows how the `ASSISTANTS_API_KEY` is now defaulted to `user_provided`, implying that users need to explicitly provide their API key.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nASSISTANTS_API_KEY=user_provided\n```\n\n----------------------------------------\n\nTITLE: Setting Assistants Base URL (Bash)\nDESCRIPTION: This snippet demonstrates how to configure an alternative base URL for the Assistants API using the `ASSISTANTS_BASE_URL` environment variable. This is similar to `OPENAI_REVERSE_PROXY` and allows for directing API requests to a different endpoint, useful for testing or custom setups.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/assistants.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nASSISTANTS_BASE_URL=http://your-alt-baseURL:3080/\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Override for RAG API with Ollama\nDESCRIPTION: This `docker-compose.override.yml` snippet updates the `rag_api` service to use the `ghcr.io/danny-avila/librechat-rag-api-dev:latest` image for Ollama embeddings.  It also includes an optional `extra_hosts` configuration for Linux environments.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/rag_api.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.4'\n\nservices:\n  rag_api:\n    image: ghcr.io/danny-avila/librechat-rag-api-dev:latest\n    # If running on Linux\n    # extra_hosts:\n    #   - \"host.docker.internal:host-gateway\"\n```\n\n----------------------------------------\n\nTITLE: Navigating to the LibreChat Directory\nDESCRIPTION: This command changes the current directory in the terminal to the LibreChat directory. This is necessary to execute subsequent commands within the project's root.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd LibreChat\n```\n\n----------------------------------------\n\nTITLE: Render Changelog Components\nDESCRIPTION: Renders the imported `ChangelogHeader` and `Content` components. `ChangelogHeader` probably displays the title and date, while `Content` renders the breaking changes from the specified Markdown file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.6.9-breaking-changes.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Enabling 'summarize' in YAML\nDESCRIPTION: This snippet demonstrates how to enable summarization by setting the `summarize` field to `false`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_11\n\nLANGUAGE: YAML\nCODE:\n```\nsummarize: false\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Array with a Code Icon in TSX\nDESCRIPTION: This snippet demonstrates how to render an array of cards, specifically one card containing a code icon, using the Cards and Cards.Card components in a TSX environment. It imports the Code component from the '@/components/svg/' path to be used as the icon for the card.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_16\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Code } from '@/components/svg/'\n;<Cards>\n  <Cards.Card icon={<Code />} title=\"Code\" href=\"/\" />\n</Cards>\n```\n\n----------------------------------------\n\nTITLE: Install Podman Container as Systemd Service (bash)\nDESCRIPTION: This script installs a podman container as a systemd service, enabling it to start automatically on system boot.  It takes the container name as an argument.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n# Install podman container as systemd container\nset -e\nname=\"$1\";\npodman generate systemd --name \"$name\" > ~/.config/systemd/user/container-$name.service\nsystemctl --user enable --now container-$name;\n```\n\n----------------------------------------\n\nTITLE: Setting the directEndpoint option in YAML\nDESCRIPTION: This code snippet shows how to set the `directEndpoint` option to `true` in a YAML file. When `directEndpoint` is set to `true`, the configured `baseURL` is treated as the completions endpoint.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_18\n\nLANGUAGE: YAML\nCODE:\n```\ndirectEndpoint: true\n```\n\n----------------------------------------\n\nTITLE: Configure Azure Entra Environment Variables in .env\nDESCRIPTION: This snippet shows the environment variables that need to be set in the .env file for Azure Entra authentication to work with LibreChat. It includes the client ID, client secret, issuer URL, and other OpenID Connect related settings.  The user must replace placeholder values with their actual Azure Entra configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/azure.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nDOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain\nDOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain\n\n# enable social login or else OpenID button will not appear on login page\nALLOW_SOCIAL_LOGIN=true\n\nOPENID_CLIENT_ID=Your Application (client) ID\nOPENID_CLIENT_SECRET=Your client secret\nOPENID_ISSUER=https://login.microsoftonline.com/Your Directory (tenant ID)/v2.0/\nOPENID_SESSION_SECRET=Any random string\nOPENID_SCOPE=openid profile email #DO NOT CHANGE THIS\nOPENID_CALLBACK_URL=/oauth/openid/callback # this should be the same for everyone\n\nOPENID_REQUIRED_ROLE_TOKEN_KIND=id\n\n# If you want to restrict access by groups\nOPENID_REQUIRED_ROLE_PARAMETER_PATH=\"roles\"\nOPENID_REQUIRED_ROLE=\"Your Group Name\"\n\n# Optional: redirects the user to the end session endpoint after logging out\nOPENID_USE_END_SESSION_ENDPOINT=true\n```\n\n----------------------------------------\n\nTITLE: Switch to New User\nDESCRIPTION: This command switches the current user to the newly created user. Replace <yourusername> with the actual username. This command ensures you are operating under the non-root account for security.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/digitalocean.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsu - <yourusername>\n```\n\n----------------------------------------\n\nTITLE: Log into MongoDB shell with credentials (Mongosh)\nDESCRIPTION: Opens the MongoDB shell, authenticating as the newly created admin user against the 'admin' database.  This allows you to perform administrative tasks.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\ndocker exec -it chat-mongodb mongosh -u adminUser -p securePassword --authenticationDatabase admin\n```\n\n----------------------------------------\n\nTITLE: Title Model Configuration YAML\nDESCRIPTION: This YAML configuration snippet shows how to specify the model used for generating conversation titles.  The `titleModel` key sets the model to use, such as `anthropic.claude-3-haiku-20240307-v1:0`, or it can be set to 'current_model' to use the chat model.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/aws_bedrock.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\ntitleModel: \"anthropic.claude-3-haiku-20240307-v1:0\"\n```\n\n----------------------------------------\n\nTITLE: Limiting Maximum Context Tokens in YAML\nDESCRIPTION: This snippet demonstrates setting a limit on the maximum number of context tokens provided to the model. The `maxContextTokens` parameter restricts the context size, which can be useful for managing resource usage and preventing excessive token consumption. Here it is set to 4096.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_11\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  maxContextTokens: 4096\n```\n\n----------------------------------------\n\nTITLE: Import React components (JSX)\nDESCRIPTION: This snippet imports the `ChangelogHeader` and `Content` components from their respective modules. These components are likely used to render the header and the main body of the changelog page. This allows for modularity and reuse of components.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.6.10-breaking-changes.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.6.10-breaking-changes.mdx'\n```\n\n----------------------------------------\n\nTITLE: Configuring 'dropParams' in YAML\nDESCRIPTION: This snippet demonstrates how to remove default parameters from requests using the `dropParams` field. This is useful for APIs that do not accept certain parameters. It shows an example where 'stop', 'user', 'frequency_penalty', and 'presence_penalty' are dropped.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_16\n\nLANGUAGE: YAML\nCODE:\n```\ndropParams:\n  - \"stop\"\n  - \"user\"\n  - \"frequency_penalty\"\n  - \"presence_penalty\"\n```\n\n----------------------------------------\n\nTITLE: Specify Multiple Compose Files (Bash)\nDESCRIPTION: Shows how to specify multiple Docker Compose files, including a main configuration file (deploy-compose.yml) and an override file (docker-compose.override.yml). The order of the files matters; settings in later files override those in earlier ones.  This also shows the pull command.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/docker_override.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose -f deploy-compose.yml -f docker-compose.override.yml pull\ndocker compose -f deploy-compose.yml -f docker-compose.override.yml up\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Registration in Shell\nDESCRIPTION: This command sets the environment variable ALLOW_REGISTRATION to true, enabling user registration for the application. This can be omitted or set to false to close registration. The variable is used to control whether new users can register with the application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.5.0.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nALLOW_REGISTRATION=true\n```\n\n----------------------------------------\n\nTITLE: Install LibreChat Helm Chart\nDESCRIPTION: This command installs the LibreChat Helm chart with a specified deployment name. The example demonstrates setting the `config.envSecrets.secretRef` value to reference the Kubernetes secret containing the environment variables, assuming you created a single secret containing all configuration data. This requires Helm to be installed and configured.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/helm_chart.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <deployment-name> helmchart\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install librechat helmchart --set config.envSecrets.secretRef=librechat\n```\n\n----------------------------------------\n\nTITLE: Import AuthorProfile Component (Next.js)\nDESCRIPTION: This snippet demonstrates how to import the `AuthorProfile` component into a Next.js component.  The component is located in the `@/components/Author/AuthorProfile` directory. This import makes the component available for use in the current file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/anon.mdx#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport AuthorProfile from '@/components/Author/AuthorProfile'\n```\n\n----------------------------------------\n\nTITLE: Stop LibreChat MongoDB Container (bash)\nDESCRIPTION: Stops the running LibreChat MongoDB container.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npodman stop librechat-mongodb\n```\n\n----------------------------------------\n\nTITLE: Translation with Multiple Interpolations - Example\nDESCRIPTION: This example illustrates translating a string with multiple interpolations ({{0}} and {{1}}) from English to German, ensuring both placeholders are preserved and positioned correctly in the translated text.\nEnglish: {{0}} of {{1}} item(s) selected\nGerman: {{0}} von {{1}} Datei(en) ausgewhlt\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n{{0}} of {{1}} item(s) selected\n```\n\nLANGUAGE: text\nCODE:\n```\n{{0}} von {{1}} Datei(en) ausgewhlt\n```\n\n----------------------------------------\n\nTITLE: Creating a Button with onClick Event in TSX\nDESCRIPTION: This snippet demonstrates creating a button component in TSX that opens a URL in a new tab or current tab when clicked. It also includes an example of a button that displays an alert message when clicked. The buttons use inline styling and an external class.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_18\n\nLANGUAGE: tsx\nCODE:\n```\n<Button onClick={() => window.open(\"https://librechat.ai\", '_blank')}> librechat.ai</Button>\n\n{' '}\n\n<Button onClick={() => window.open('https://librechat.ai', '_self')}> librechat.ai</Button>\n\n  <Button className=\"my-extra-class\" onClick={() => alert('This is an Alert')}>\n     Alert Button\n  </Button>\n```\n\n----------------------------------------\n\nTITLE: Rebuild LibreChat Image\nDESCRIPTION: This command rebuilds the LibreChat image using the Dockerfile located at `./LibreChat/Dockerfile`. The `--tag` option sets the tag for the new image to `librechat:local`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\npodman build --tag \"librechat:local\" --file ./LibreChat/Dockerfile\n```\n\n----------------------------------------\n\nTITLE: Render Components (JSX)\nDESCRIPTION: Renders the imported `ChangelogHeader` and `Content` components. `ChangelogHeader` likely provides a title and introductory information, while `Content` provides the detailed changelog body.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.6.9.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Creating a .env File\nDESCRIPTION: This command copies the .env.example file to create a new .env file. The .env file will contain configuration settings for LibreChat. Users can modify this file to customize the application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Importing YAMLChecker Component in React\nDESCRIPTION: This code snippet imports the YAMLChecker component from the specified path.  This component is assumed to be a React component that provides YAML validation functionality.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/toolkit/yaml_checker.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport YAMLChecker from '@/components/tools/yamlChecker'\n```\n\n----------------------------------------\n\nTITLE: Shut down the running container\nDESCRIPTION: Shuts down the currently running Docker container.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_22\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Import Components - React\nDESCRIPTION: This snippet imports various React components from the '@/components/CardIcons' module. These components are likely used to render cards with icons for different sections like Quick Start, Custom Endpoints, Logo, ToolKit, Changelog, Roadmap, Features, and OurAuthors.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/index.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport {\n  QuickStartLocal,\n  CustomEndpoints,\n  Logo,\n  ToolKit,\n  Changelog,\n  Roadmap,\n  Features,\n  OurAuthors,\n} from '@/components/CardIcons'\n```\n\n----------------------------------------\n\nTITLE: Custom Tool Initialization (JavaScript)\nDESCRIPTION: This snippet shows an example of how to define custom constructors for tools requiring more advanced initialization. The `customConstructors` object maps a tool's name to an asynchronous function that creates and returns an instance of the tool, allowing for dynamic configuration and dependency injection.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst customConstructors = {\n    browser: async () => {\n      let openAIApiKey = process.env.OPENAI_API_KEY;\n      if (!openAIApiKey) {\n        openAIApiKey = await getUserPluginAuthValue(user, 'OPENAI_API_KEY');\n      }\n      return new WebBrowser({ model, embeddings: new OpenAIEmbeddings({ openAIApiKey }) });\n    },\n  // ...\n    plugins: async () => {\n      return [\n        new HttpRequestTool(),\n        await AIPluginTool.fromPluginUrl(\n          \"https://www.klarna.com/.well-known/ai-plugin.json\", new ChatOpenAI({ openAIApiKey: options.openAIApiKey, temperature: 0 })\n        ),\n      ]\n    }\n  };\n```\n\n----------------------------------------\n\nTITLE: Listing Tables in LibreChat Database\nDESCRIPTION: This snippet demonstrates how to list all tables (collections) within the selected 'librechat' database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nshow tables\n```\n\n----------------------------------------\n\nTITLE: Verify Git, Node.js, and npm versions\nDESCRIPTION: Checks the installed versions of Git, Node.js, and npm to confirm that they were installed successfully.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ngit --version\nnode -v\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Rendering Changelog Components\nDESCRIPTION: Renders the `ChangelogHeader` component and the `Content` component within the changelog page. `ChangelogHeader` likely handles the title and other header related information, while `Content` displays the details of the release.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.7.7.mdx#_snippet_1\n\nLANGUAGE: MDX\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Implementing Features Section with Feature Component in TSX\nDESCRIPTION: This code demonstrates how to implement a features section using the Features and Feature components in TSX.  It showcases the customization of the Feature component with props such as href, medium, large, lightOnly, and centered to achieve different layouts and functionalities within the Features section. It imports Feature and Features from '@/components/features'.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_19\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Feature, Features } from '@/components/features'\n;<Features>\n  <Feature href=\"https://example.com\">This is a feature with a link.</Feature>\n  <Feature medium lightOnly>\n    This is a medium \"lightOnly\" feature occupying two spaces.\n  </Feature>\n  <Feature large centered>\n    This is a large feature with \"centered\" attribute.\n  </Feature>\n  <Feature large>![](/images/banner.png)</Feature>\n</Features>\n```\n\n----------------------------------------\n\nTITLE: Specifying Agent ID in YAML\nDESCRIPTION: This example shows how to specify the ID of an agent to use when the `agents` endpoint is selected. The `agent_id` parameter identifies which assistant is to be used. Model options should be excluded, deferring to the agent's UI-defined configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  agent_id: \"agent_someUniqueId\"\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Override Configuration (YAML)\nDESCRIPTION: This code snippet shows an example `docker-compose.override.yml` file. It overrides the default docker-compose configuration to build the LibreChat image from the current directory using the `node` target.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.10-breaking-changes.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nversion: '3.4'\n\nservices:\n  api:\n  image: librechat\n  build:\n    context: .\n    target: node\n```\n\n----------------------------------------\n\nTITLE: Updating LibreChat with npm\nDESCRIPTION: This command updates the LibreChat project by running `npm run update` from the project directory. It is intended to perform a clean installation. Requires git and npm to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.5.9.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run update\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pnpm\nDESCRIPTION: This command installs the project dependencies using `pnpm`. Ensure Node.js and `pnpm` are installed on your system.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Importing Components in JavaScript\nDESCRIPTION: This snippet imports the ChangelogHeader and Content components from their respective modules. These components are used to render the changelog header and the changelog content, respectively.  This enables the reuse of React components for structuring the changelog page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/config_v1.01.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/config_v1.01.mdx'\n```\n\n----------------------------------------\n\nTITLE: Setting Upstream Remote\nDESCRIPTION: These commands set up the upstream remote to keep your fork up-to-date with the original LibreChat.ai repository. The first command adds the remote, and the second fetches the changes.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngit remote add upstream https://github.com/LibreChat-AI/librechat.ai.git\ngit fetch upstream\n```\n\n----------------------------------------\n\nTITLE: Callout Component Usage (Non-Collapsible)\nDESCRIPTION: This code demonstrates usage of callout component with 'success' type, title and emoji. The \"collapsible\" property is omitted to create a static (non-collapsible) component.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_13\n\nLANGUAGE: tsx\nCODE:\n```\n<Callout type=\"success\" title=\"title\" emoji=''>\n  - The type comes with a color scheme and default emoji\n  - \"title\" is optional\n  - \"emoji\" is optional\n  - \"collapsible\" is only added when the collapsible feature is wanted\n</Callout>\n```\n\n----------------------------------------\n\nTITLE: Copy env.example to .env (bash)\nDESCRIPTION: Copies the `.env.example` file from the LibreChat directory to a `.env` file in the current directory. This allows for easy configuration of environment variables.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp ./LibreChat/.env.example .env\n```\n\n----------------------------------------\n\nTITLE: Installing pnpm globally\nDESCRIPTION: This command installs `pnpm` globally using `npm`. This is required if `pnpm` is not already installed on your system.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nnpm install -g pnpm\n```\n\n----------------------------------------\n\nTITLE: Installing LibreChat Dependencies\nDESCRIPTION: Installs the dependencies for the LibreChat project using npm.  It is crucial for setting up the development environment and ensuring that all required packages are available. This command should be executed in the project's root directory.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nnpm ci\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Class with Credentials in JavaScript\nDESCRIPTION: Defines a class for the plugin that extends the `Tool` class. The constructor sets the `name`, `url`, and `description` properties. It also demonstrates how to handle credentials either from the fields parameter or from a method that retrieves them from the process environment.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nclass StableDiffusionAPI extends Tool {\n  constructor(fields) {\n    super();\n    this.name = 'stable-diffusion';\n    this.url = fields.SD_WEBUI_URL || this.getServerURL(); // <--- important!\n    this.description = `You can generate images with 'stable-diffusion'. This tool is exclusively for visual content...`;\n  }\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring topP for Google/Anthropic in YAML\nDESCRIPTION: This snippet demonstrates how to set the `topP` parameter in a YAML preset specifically for Google and Anthropic endpoints.  This parameter functions similarly to `top_p` but uses a different name.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_22\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  topP: 0.8\n```\n\n----------------------------------------\n\nTITLE: Cloud TTS Azure OpenAI Configuration YAML\nDESCRIPTION: Configures the Azure OpenAI TTS service for cloud Text-to-Speech. It specifies the instance name, API key, deployment name, API version, model, and voices. Ensure that the TTS_API_KEY environment variable is set, and the instanceName, deploymentName, and apiVersion are correctly configured for your Azure OpenAI instance.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  tts:\n    azureOpenAI:\n      instanceName: ''\n      apiKey: '${TTS_API_KEY}'\n      deploymentName: ''\n      apiVersion: ''\n      model: 'tts-1'\n      voices: ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']\n```\n\n----------------------------------------\n\nTITLE: Import Changelog Components (JSX)\nDESCRIPTION: This snippet imports the `ChangelogHeader` component and the MDX content file for the LibreChat v0.5.8 changelog. It's a standard import statement in a JSX/JavaScript file used to include reusable components and external content.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.8.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.5.8.mdx'\n```\n\n----------------------------------------\n\nTITLE: Importing React Components for Changelog\nDESCRIPTION: Imports necessary React components, `ChangelogHeader` and `Content`, to render the changelog page. `ChangelogHeader` is imported from the '@/components/changelog/ChangelogHeader' path and `Content` from '@/components/changelog/content/v0.7.7.mdx'.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.7.7.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.7.7.mdx'\n```\n\n----------------------------------------\n\nTITLE: Configuring Top K Sampling in YAML\nDESCRIPTION: This snippet configures the `topK` parameter in a YAML preset, relevant for Google and Anthropic models. The `topK` parameter limits the next token selection to the top K most likely tokens, influencing the diversity and creativity of the output. The example sets `topK` to 40.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_23\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  topK: 40\n```\n\n----------------------------------------\n\nTITLE: Clone LibreChat repository\nDESCRIPTION: Clones the LibreChat repository from GitHub to the specified directory.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# clone down the repository\ngit clone https://github.com/danny-avila/LibreChat.git\n```\n\n----------------------------------------\n\nTITLE: Render AuthorProfile Component with authorId (JSX)\nDESCRIPTION: This JSX snippet shows how to render the `AuthorProfile` component and pass the `authorId` prop. The `authorId` is set to \"anon\", which is presumably an identifier for a specific author's profile. This rendering call triggers the component to fetch and display the corresponding author data.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/anon.mdx#_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<AuthorProfile authorId=\"anon\" />\n```\n\n----------------------------------------\n\nTITLE: OptionTable Component Definition\nDESCRIPTION: This snippet demonstrates how to define an OptionTable component with a list of options. The options array contains the properties needed for the table.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_14\n\nLANGUAGE: mdx\nCODE:\n```\n<OptionTable\n  options={[\n    ['EXAMPLE', 'boolean', 'This is an example.','EXAMPLE=true'],\n  ]}\n/>\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Configuration for Meilisearch Integration\nDESCRIPTION: This Dockerfile snippet configures the LibreChat instance to connect to a Meilisearch instance. It defines environment variables for enabling search, disabling analytics, and specifying the Meilisearch host URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/huggingface.mdx#_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nENV SEARCH=true\nENV MEILI_NO_ANALYTICS=true\nENV MEILI_HOST=<YOUR_MEILISEARCH_SPACE_URL>\n```\n\n----------------------------------------\n\nTITLE: Apply Docker Compose Changes (Bash)\nDESCRIPTION: Applies the Docker Compose configuration changes defined in docker-compose.yml and docker-compose.override.yml. The `-d` flag runs the containers in detached mode.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/docker_override.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Callout with Warning Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'warning' type. It includes a title, collapsible functionality, and placeholder text, commonly used to display warnings or potential issues.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_10\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"warning\" title=\"warning\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Render Changelog Components (MDX)\nDESCRIPTION: This snippet renders the imported `ChangelogHeader` component and the imported `Content` component, which is expected to be an MDX file. This pattern allows for structured content authoring using Markdown with JSX components, creating a dynamic changelog page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.8.mdx#_snippet_1\n\nLANGUAGE: MDX\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Import Pricing Component (JSX)\nDESCRIPTION: This snippet imports the `Pricing` component from the specified path.  This allows it to be rendered on the current page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/pricing.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport Pricing from '@/components/home/Pricing'\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Tabs with Nextra\nDESCRIPTION: This snippet demonstrates how to create nested tabs using the `<Tabs>` and `<Tabs.Tab>` components from Nextra. The outer `<Tabs>` component has two tabs: 'A' and 'B'. Inside each of these tabs, there are nested tabs ('1a', '2a' and '1b', '2b' respectively) displaying content.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_21\n\nLANGUAGE: tsx\nCODE:\n```\n<Tabs items={['A', 'B']}>\n  <Tabs.Tab>\n    <Tabs items={['1a', '2a']}>\n      <Tabs.Tab>\n        **Content for 1a**\n      </Tabs.Tab>\n      <Tabs.Tab>\n        **Content for 2a**\n      </Tabs.Tab>\n    </Tabs>\n  </Tabs.Tab>\n  <Tabs.Tab>\n    <Tabs items={['1b', '2b']}>\n      <Tabs.Tab>\n        **Content for 1b**\n      </Tabs.Tab>\n      <Tabs.Tab>\n        **Content for 2b**\n      </Tabs.Tab>\n    </Tabs>\n  </Tabs.Tab>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Importing BlogIndex and Link in Next.js\nDESCRIPTION: This snippet imports the BlogIndex component from the '@/components/blog/BlogIndex' path and the Link component from 'next/link'. These imports are necessary for rendering the blog index and creating links within the page, respectively. BlogIndex likely contains the logic for fetching and displaying blog posts.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { BlogIndex } from '@/components/blog/BlogIndex'\nimport Link from 'next/link'\n```\n\n----------------------------------------\n\nTITLE: Exit the Mongosh/Container Terminal\nDESCRIPTION: Exits the MongoDB shell and returns to the container's terminal.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_21\n\nLANGUAGE: Bash\nCODE:\n```\nexit\n```\n\n----------------------------------------\n\nTITLE: Import React Components\nDESCRIPTION: Imports React components for rendering a changelog header and the changelog content itself. `ChangelogHeader` is used for the header section, and `Content` imports the actual changelog from an MDX file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.7.0-breaking-changes.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.7.0-breaking-changes.mdx'\n```\n\n----------------------------------------\n\nTITLE: Creating a New Branch\nDESCRIPTION: This command creates a new branch for your blog post. Replace `your-branch-name` with a descriptive name for your branch (e.g., `add-new-post`).\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ngit checkout -b your-branch-name\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Model Configuration YAML\nDESCRIPTION: This YAML snippet configures a Hugging Face endpoint in LibreChat, specifying the API key, base URL, available models, and other settings like dropping 'top_p' parameters and setting the display label. The configuration includes a list of default models.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/huggingface.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   - name: 'HuggingFace'\n      apiKey: '${HUGGINGFACE_TOKEN}'\n      baseURL: 'https://api-inference.huggingface.co/v1'\n      models:\n        default: [\n          \"codellama/CodeLlama-34b-Instruct-hf\",\n          \"google/gemma-1.1-2b-it\",\n          \"google/gemma-1.1-7b-it\",\n          \"HuggingFaceH4/starchat2-15b-v0.1\",\n          \"HuggingFaceH4/zephyr-7b-beta\",\n          \"meta-llama/Meta-Llama-3-8B-Instruct\",\n          \"microsoft/Phi-3-mini-4k-instruct\",\n          \"mistralai/Mistral-7B-Instruct-v0.1\",\n          \"mistralai/Mistral-7B-Instruct-v0.2\",\n          \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n          \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n        ]\n        fetch: true\n      titleConvo: true\n      titleModel: \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n      dropParams: [\"top_p\"]\n      modelDisplayLabel: \"HuggingFace\"\n```\n\n----------------------------------------\n\nTITLE: Override Values with a YAML file\nDESCRIPTION: This is an example of YAML configuration for setting environment variables from separate Kubernetes secrets. It will be saved in a separate file. It maps each environment variable to a specific secret and key within that secret. This approach allows for more fine-grained control over secret management.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/helm_chart.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n  envSecrets:\n    secretKeyRef:\n    - name: CREDS_KEY\n      secretName: librechat-creds-iv\n      secretKey: CREDS_KEY\n    <...>\n```\n\n----------------------------------------\n\nTITLE: Create Individual Kubernetes Secrets\nDESCRIPTION: This command creates a separate Kubernetes secret for each sensitive environment variable. This provides more fine-grained control over the secrets. This example shows creating a secret for `CREDS_KEY`. This command needs `kubectl` installed and configured to connect to the Kubernetes cluster.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/helm_chart.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic librechat-creds-key \\\n--from-literal=CREDS_KEY=0963cc1e5e5df9554c8dd32435d0eb2b1a8b6edde6596178d5c5418ade897673\n```\n\n----------------------------------------\n\nTITLE: Importing React components\nDESCRIPTION: Imports the necessary React components for rendering the changelog content. `ChangelogHeader` is assumed to be a component that displays the header information of the changelog, and `Content` is a component that renders the Markdown content from the `v0.5.9.mdx` file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.9.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.5.9.mdx'\n```\n\n----------------------------------------\n\nTITLE: Disable Prompts\nDESCRIPTION: Illustrates how to disable prompt-related features for all users in LibreChat. Setting `prompts` to `false` prevents users from creating, editing, or using custom prompts.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  prompts: false\n```\n\n----------------------------------------\n\nTITLE: Changelog/Blog Header Example in Markdown\nDESCRIPTION: This markdown snippet demonstrates the structure and metadata required for creating a changelog or blog header in LibreChat. It includes fields for the date, title, description, author ID, and OG image. The `ChangelogHeader` component is imported and rendered.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/README.md#_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n---\ndate: 2024-04-01\ntitle: LibreChat v0.7.0\ndescription: The v0.7.0 release of LibreChat\nauthorid: danny\nogImage: /images/changelog/2024-04-01-v0.7.0.png\n---\n\nimport { ChangelogHeader } from \"@/components/changelog/ChangelogHeader\";\n\n<ChangelogHeader />\n```\n\n----------------------------------------\n\nTITLE: Enable MongoDB Authentication in Docker Compose (YAML)\nDESCRIPTION: This snippet configures the `docker-compose.override.yml` file to enable authentication for the MongoDB container by adding the `--auth` flag to the `mongod` command.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_23\n\nLANGUAGE: YAML\nCODE:\n```\nversion: '3.4'\n\nservices:\n  api:\n    volumes:\n      - ./librechat.yaml:/app/librechat.yaml # Optional for using the librechat config file.\n  mongodb:\n    command: mongod --auth # <--- Add this to enable authentication\n```\n\n----------------------------------------\n\nTITLE: Enable Presets\nDESCRIPTION: Demonstrates how to enable presets in the LibreChat interface. When `presets` is set to `true`, pre-configured settings or operations are available to the user, enhancing the user experience.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  presets: true\n```\n\n----------------------------------------\n\nTITLE: Grant Roles to Admin User\nDESCRIPTION: Grants additional roles to the admin user, specifically for use with mongo-express. These roles provide necessary permissions for managing the database through mongo-express.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_20\n\nLANGUAGE: Bash\nCODE:\n```\ndb.grantRolesToUser(\"adminUser\", [\"clusterAdmin\", \"readAnyDatabase\"])\n```\n\n----------------------------------------\n\nTITLE: Enabling 'titleConvo' in YAML\nDESCRIPTION: This snippet shows how to enable conversation title generation by setting `titleConvo` to `true`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\ntitleConvo: true\n```\n\n----------------------------------------\n\nTITLE: Removing a Document from a Table\nDESCRIPTION: This snippet demonstrates how to remove a document from a table based on specified criteria. Replace `[table]` with the actual table name and `[criteria]` with the selection criteria. The criteria should be a JSON-like object.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ndb.[table].remove({ [criteria] })\n```\n\n----------------------------------------\n\nTITLE: Embedding Video within Nextra Tabs\nDESCRIPTION: This code snippet shows how to embed a video player within a Nextra Tab component. The `<video>` element is used to display the video, with attributes like `muted`, `autoPlay`, `playsInline`, and `controls` configured for playback. The source of the video is defined using the `<source>` element.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_22\n\nLANGUAGE: tsx\nCODE:\n```\n<video\n      muted\n      autoPlay\n      playsInline\n      controls\n    >\n      <source src=\"/videos/example.mp4\" />\n    </video>\n```\n\n----------------------------------------\n\nTITLE: Render Changelog Header (JSX)\nDESCRIPTION: Renders the ChangelogHeader component. This component likely displays the title and date of the changelog.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.6.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<ChangelogHeader />\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Metadata for Author Profile\nDESCRIPTION: This YAML snippet defines metadata for an author profile, including the title, author ID, subtitle, name, biography, Open Graph image, and social media links. This data is used to populate the author's profile page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/rubent.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n---\ntitle: RubenT\nauthorid: rubent\nsubtitle: Collaborator for LibreChat\nname: 'RubenT'\nbio: I'm a self-taught programmer\nogImage: '/images/people/rubent.png'\nsocials:\n  GitHub: https://github.com/rubentalstra\n  LinkedIn: https://www.linkedin.com/in/rubentalstra/\n---\n```\n\n----------------------------------------\n\nTITLE: Launch Docker Containers (Shell)\nDESCRIPTION: This command is used to start the Docker containers defined in the docker-compose.yml and docker-compose.override.yml files. The `-d` flag runs the containers in detached mode (in the background).  It assumes you are in the directory containing the `docker-compose.yml` file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_mongoexpress.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Check MongoDB Connection Status (Bash)\nDESCRIPTION: This command verifies the authentication status by running the `connectionStatus` command in the MongoDB shell. It returns information about authenticated users and their roles.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_26\n\nLANGUAGE: Bash\nCODE:\n```\ndb.runCommand({ connectionStatus: 1 })\n```\n\n----------------------------------------\n\nTITLE: Author Profile MDX Template\nDESCRIPTION: This is the MDX template for creating an author profile. It includes metadata for the author's name, bio, and social media links, as well as an import statement and component to render the profile.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_6\n\nLANGUAGE: mdx\nCODE:\n```\n---\ntitle: Your Name Profile\nauthorid: your-authorid\nsubtitle: Job Title or Short Description\nname: Your Name\nbio: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla litora ridiculus magna et etiam mi. Dolor etiam id elit morbi ipsum mauris. Non dapibus urna platea elementum fusce vulputate.\nogImage: '/images/people/your-image.png'\nsocials:\n  GitHub: https://github.librechat.ai\n  LinkedIn: https://linkedin.librechat.ai/\n  X: https://x.com/LibreChatAI\n  Discord: https://discord.librechat.ai\n---\n\nimport AuthorProfile from '@/components/Author/AuthorProfile'\n\n<AuthorProfile authorId=\"your-authorid\" />\n```\n\n----------------------------------------\n\nTITLE: Configuring Greeting Property in Preset\nDESCRIPTION: This YAML snippet demonstrates how to configure the `greeting` property within the `preset` object.  It sets a predefined message that is visible in the UI before a new chat is started, providing instructions or a friendly welcome to the user.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npreset:\n  greeting: \"This assistant creates meeting notes based on transcripts of Teams recordings. To start, simply paste the transcript into the chat box.\"\n```\n\n----------------------------------------\n\nTITLE: User Permission Environment Variables\nDESCRIPTION: These environment variables configure user and group IDs for the Docker container, which helps resolve permission issues. `UID` specifies the user ID, and `GID` specifies the group ID.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_1\n\nLANGUAGE: environment\nCODE:\n```\n# UID=1000\n```\n\nLANGUAGE: environment\nCODE:\n```\n# GID=1000\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Turkish)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Turkish language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_22\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'tr'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"TR Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (English)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the English language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'en'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"EN Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Pull Latest Docker Image (Bash)\nDESCRIPTION: Pulls the latest version of the LibreChat Docker image specified in the `deploy-compose.yml` file.  This ensures that the Docker image is up-to-date.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose -f ./deploy-compose.yml pull\n```\n\n----------------------------------------\n\nTITLE: Token Value Calculation\nDESCRIPTION: These math snippets show how to calculate the token value based on raw amount of tokens and rate and the USD spend.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/token_usage.mdx#_snippet_4\n\nLANGUAGE: math\nCODE:\n```\n\\text{Token Value} = (\\text{Raw Amount of Tokens}) \\times (\\text{Rate})\n```\n\nLANGUAGE: math\nCODE:\n```\n137 \\times 1.5 = 205.5\n```\n\nLANGUAGE: math\nCODE:\n```\n\\frac{\\text{Token Value}}{1,000,000} = \\left(\\frac{\\text{Raw Amount of Tokens} \\times \\text{Rate}}{1,000,000}\\right)\n```\n\nLANGUAGE: math\nCODE:\n```\n\\frac{205.5}{1,000,000} = \\$0.0002055 \\text{ USD}\n```\n\n----------------------------------------\n\nTITLE: Available Regions Configuration YAML\nDESCRIPTION: This YAML configuration demonstrates how to specify the AWS regions that are available for Bedrock using the `availableRegions` key.  Users will see a dropdown to select from the defined regions.  If no regions are specified, the default region is used.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/aws_bedrock.mdx#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\navailableRegions:\n  - \"us-east-1\"\n  - \"us-west-2\"\n```\n\n----------------------------------------\n\nTITLE: Carousel Component Usage in TSX\nDESCRIPTION: This example demonstrates the usage of the `Carousel` component in a React (TSX) context. It showcases the configuration of the carousel with autoplay, animation duration, bullet navigation, and the number of visible slides. The code snippet includes placeholder image sources that should be replaced with actual image URLs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n<Carousel autoplay animationDuration=\"1500\" showBullets perView=\"3\">\n  <img src=\"slide1.jpg\" alt=\"Slide 1\" />\n  <img src=\"slide2.jpg\" alt=\"Slide 2\" />\n  <img src=\"slide3.jpg\" alt=\"Slide 3\" />\n</Carousel>\n```\n\n----------------------------------------\n\nTITLE: Configure Custom Language Grammar for Shiki in Nextra\nDESCRIPTION: This code snippet demonstrates how to override the `getHighlighter` function in `mdxOptions.rehypePrettyCodeOptions` to include a custom language grammar for syntax highlighting in Nextra, using Shiki. It adds a custom language definition ('my-lang') to the `langs` array, specifying its scope name, aliases, and path to the grammar file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/syntax_highlighting.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { BUNDLED_LANGUAGES } from 'shiki'\n\nnextra({\n  // ... other options\n  mdxOptions: {\n    rehypePrettyCodeOptions: {\n      getHighlighter: (options) =>\n        getHighlighter({\n          ...options,\n          langs: [\n            ...BUNDLED_LANGUAGES,\n            // custom grammar options, see the Shiki documentation for how to provide these options\n            {\n              id: 'my-lang',\n              scopeName: 'source.my-lang',\n              aliases: ['mylang'], // Along with id, aliases will be included in the allowed names you can use when writing markdown.\n              path: '@/public/syntax/grammar.tmLanguage.json',\n            },\n          ],\n        }),\n    },\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring 'models/userIdQuery' in YAML\nDESCRIPTION: This snippet demonstrates how to configure the `userIdQuery` boolean within the `models` object. When set to true, it includes the LibreChat user ID as a query parameter in the API request for models.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\nuserIdQuery: true\n```\n\n----------------------------------------\n\nTITLE: Cloning Git Repository\nDESCRIPTION: This command clones the LibreChat.ai repository from GitHub to your local machine. Replace `<your-username>` with your GitHub username.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/<your-username>/librechat.ai.git\n```\n\n----------------------------------------\n\nTITLE: General Endpoint Configuration Variables\nDESCRIPTION: These environment variables configure general endpoint settings. `ENDPOINTS` specifies a comma-separated list of available endpoints. `PROXY` sets a proxy for all endpoints. `TITLE_CONVO` enables titling for all endpoints.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/dotenv.mdx#_snippet_3\n\nLANGUAGE: environment\nCODE:\n```\n# ENDPOINTS=openAI,agents,assistants,gptPlugins,azureOpenAI,google,anthropic,bingAI,custom\n```\n\nLANGUAGE: environment\nCODE:\n```\nPROXY=\n```\n\nLANGUAGE: environment\nCODE:\n```\nTITLE_CONVO=true\n```\n\n----------------------------------------\n\nTITLE: Speech Tab Configuration YAML\nDESCRIPTION: Defines the structure for configuring speech settings within a speechTab menu, encompassing conversation mode, advanced mode, and detailed STT/TTS settings. This configuration allows for customizable default user settings.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/stt_tts.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nspeech:\n  speechTab:\n    conversationMode: true\n    advancedMode: false\n    speechToText:\n      engineSTT: \"external\"\n      languageSTT: \"English (US)\"\n      autoTranscribeAudio: true\n      decibelValue: -45\n      autoSendText: 0\n    textToSpeech:\n      engineTTS: \"external\"\n      voice: \"alloy\"\n      languageTTS: \"en\"\n      automaticPlayback: true\n      playbackRate: 1.0\n      cacheTTS: true\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Hungarian)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Hungarian language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_9\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'hu'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"HU Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Callout with Abstract Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'abstract' type. It includes a title, collapsible functionality, and placeholder text. This is for displaying abstract or summary information.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_5\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"abstract\" title=\"abstract\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter: Berry Author Profile\nDESCRIPTION: This markdown frontmatter defines the author profile information for 'Berry', including name, subtitle, bio, OG image, and social media links. This data is likely used to dynamically populate author profiles within the LibreChat application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/berry.mdx#_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n--- \ntitle: Berry\nauthorid: berry\nname: 'Berry'\nsubtitle: Collaborator for LibreChat\nbio: '// TODO: Become a software engineer !self_taught++; // Debugging life, coding solutions'\nogImage: '/images/people/berry.png'\nsocials:\n  GitHub: https://github.com/Berry-13\n  LinkedIn: https://www.linkedin.com/in/marco-beretta-berry/\n  X: https://x.com/berry13000\n  email: mailto:berry@librechat.ai\n---\n```\n\n----------------------------------------\n\nTITLE: Import React Components\nDESCRIPTION: Imports the necessary React components for rendering the changelog header and the content from a Markdown file.  These components likely handle the presentation and formatting of the changelog information.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.6.9-breaking-changes.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.6.9-breaking-changes.mdx'\n```\n\n----------------------------------------\n\nTITLE: Import and Render AuthorProfile Component (JSX)\nDESCRIPTION: This code snippet imports the `AuthorProfile` component from '@/components/Author/AuthorProfile' and renders it with the `authorId` prop set to \"danny\". This displays Danny Avila's profile on the page, using the data defined in the YAML frontmatter.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/danny.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\nimport AuthorProfile from '@/components/Author/AuthorProfile'\n\n<AuthorProfile authorId=\"danny\" />\n```\n\n----------------------------------------\n\nTITLE: MCP Servers Object Example in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure of the `mcpServers` object, showcasing various MCP server configurations including `everything`, `puppeteer`, `filesystem`, and `mcp-obsidian`. It defines the connection types, commands, arguments, and other optional settings for each server.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/mcp_servers.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nmcpServers:\n  everything:\n    # type: sse # type can optionally be omitted\n    url: http://localhost:3001/sse\n  puppeteer:\n    type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@modelcontextprotocol/server-puppeteer\"\n  filesystem:\n    # type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@modelcontextprotocol/server-filesystem\"\n      - /home/user/LibreChat/\n    iconPath: /home/user/LibreChat/client/public/assets/logo.svg\n  mcp-obsidian:\n    command: npx\n    args:\n      - -y\n      - \"mcp-obsidian\"\n      - /path/to/obsidian/vault\n```\n\n----------------------------------------\n\nTITLE: Install Docker dependencies\nDESCRIPTION: Installs the necessary dependencies for Docker, including packages for HTTPS transport, certificate authority, curl, software properties, GPG, and LSB release.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Korean)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Korean language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_14\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ko'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"KO Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Vietnamese)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Vietnamese language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_23\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'vi'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"VI Badge\" />\n```\n\n----------------------------------------\n\nTITLE: OCR Configuration (Environment Variables)\nDESCRIPTION: Shows the basic environment variables needed to configure OCR with the Mistral OCR API in LibreChat. The `OCR_API_KEY` environment variable is required to provide the API key for the Mistral OCR service.  `OCR_BASEURL` can optionally be set, defaulting to Mistral's API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/features/ocr.mdx#_snippet_0\n\nLANGUAGE: dotenv\nCODE:\n```\n# `.env`\nOCR_API_KEY=your-mistral-api-key\n# OCR_BASEURL=https://api.mistral.ai/v1 # this is the default value\n```\n\n----------------------------------------\n\nTITLE: Configuring LibreChat Metrics Exporter Environment Variables\nDESCRIPTION: This snippet configures environment variables for the librechat_exporter within the Docker Compose file. It sets the MongoDB URI to connect to the database and configures the logging level to 'info'.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/metrics.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    environment:\n      - MONGODB_URI=mongodb://mongodb:27017/\n      - LOGGING_LEVEL=info\n```\n\n----------------------------------------\n\nTITLE: Navigate to LibreChat Directory\nDESCRIPTION: Changes the current directory in the terminal to the LibreChat directory that was just cloned. This is necessary for executing subsequent commands within the project directory.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/npm.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd LibreChat\n```\n\n----------------------------------------\n\nTITLE: Import Components (JSX)\nDESCRIPTION: Imports the ChangelogHeader and Content components from their respective modules. These components are used to structure and display the changelog content.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.6.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.5.6.mdx'\n```\n\n----------------------------------------\n\nTITLE: Poll Interval Configuration YAML\nDESCRIPTION: This YAML snippet shows how to configure the polling interval for checking run updates using the `pollIntervalMs` option. This parameter specifies the interval in milliseconds.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\npollIntervalMs: 2500\n```\n\n----------------------------------------\n\nTITLE: Rendering AuthorProfile Component with authorId prop\nDESCRIPTION: This line renders the `AuthorProfile` component, passing the `authorId` prop with the value \"berry\". This prop is used to fetch and display the author's information based on the provided ID.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/berry.mdx#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n<AuthorProfile authorId=\"berry\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Russian)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Russian language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_19\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ru'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"RU Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Simplified Chinese)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Simplified Chinese language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_24\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'zh-Hans'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"ZH-HANS Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Importing React Components\nDESCRIPTION: This code snippet imports necessary React components, `ChangelogHeader` and `Content`, from local modules to build the changelog page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/config_v1.06.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/config_v1.06.mdx'\n```\n\n----------------------------------------\n\nTITLE: Rendering ChangelogHeader component\nDESCRIPTION: Renders the `ChangelogHeader` component, which likely displays the title and date of the changelog. This component is responsible for showing the initial header of the changelog page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.9.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<ChangelogHeader />\n```\n\n----------------------------------------\n\nTITLE: Reverting Changes - Force Push\nDESCRIPTION: Force pushes the changes to the remote repository after completing the interactive rebase. This is necessary to update the remote branch with the reverted commits. Use caution when force pushing, as it can overwrite remote history.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ngit push --force origin feature-branch-name\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Docker Image\nDESCRIPTION: This command pulls the latest LibreChat Docker image. This ensures that you have the most up-to-date image for running the application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose pull\n```\n\n----------------------------------------\n\nTITLE: Cloning LibreChat repository using Git\nDESCRIPTION: This command clones the LibreChat repository from GitHub to your local machine using Git. It requires Git to be installed and configured on your system. It downloads the entire project to the current directory, or the directory specified in the command.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/quick_start/local_setup.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/danny-avila/LibreChat.git\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Arabic)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Arabic language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ar'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"AR Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Retrieving CORS Configuration (gsutil)\nDESCRIPTION: This shell command uses the `gsutil` tool to retrieve and display the current CORS configuration for the specified Firebase Storage bucket. `<your-cloud-storage-bucket>` must be replaced with the name of the actual bucket. The `gsutil` tool must be installed and configured to interact with the Firebase project.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/firebase.mdx#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngsutil cors get gs://<your-cloud-storage-bucket>\n```\n\n----------------------------------------\n\nTITLE: Example successful connection (Bash)\nDESCRIPTION: This snippet shows example successful connection logs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_32\n\nLANGUAGE: Bash\nCODE:\n```\nLibreChat         | 2024-02-04 20:59:43 info: Server listening on all interfaces at port 3080. Use http://localhost:3080 to access it\nchat-mongodb      | {\"t\":{\"$date\":\"2024-02-04T20:59:53.880+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":22943,   \"ctx\":\"listener\",\"msg\":\"Connection accepted\",\"attr\":{\"remote\":\"192.168.160.4:58114\",\"uuid\":{\"uuid\":{\"$uuid\":\"027bdc7b-a3f4-429a-80ee-36cd172058ec\"}},\"connectionId\":17,\"connectionCount\":10}}\n```\n\n----------------------------------------\n\nTITLE: Add User to Sudo Group\nDESCRIPTION: This command adds the specified user to the sudo group, granting them administrative privileges. Replace <yourusername> with the actual username. This is essential for performing administrative tasks.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/digitalocean.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nusermod -aG sudo <yourusername>\n```\n\n----------------------------------------\n\nTITLE: FileTree Component Usage\nDESCRIPTION: This snippet demonstrates using the FileTree component to display a file system structure. It includes folders and files with optional `defaultOpen` for folders and `active` for files.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_15\n\nLANGUAGE: tsx\nCODE:\n```\n<FileTree>\n  <FileTree.Folder name=\"src\" defaultOpen>\n    <FileTree.File name=\"main.js\" active />\n    <FileTree.Folder name=\"components\">\n      <FileTree.File name=\"Header.js\" />\n      <FileTree.File name=\"Footer.js\" />\n    </FileTree.Folder>\n  </FileTree.Folder>\n  <FileTree.File name=\"package.json\" />\n</FileTree>\n```\n\n----------------------------------------\n\nTITLE: Disable Telemetry\nDESCRIPTION: Disables telemetry collection in MongoDB, preventing anonymous usage data from being sent to MongoDB.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_17\n\nLANGUAGE: Bash\nCODE:\n```\ndisableTelemetry()\n```\n\n----------------------------------------\n\nTITLE: LibreChat YAML Configuration for Firebase\nDESCRIPTION: This snippet demonstrates the configuration required in the `librechat.yaml` file to enable Firebase as the file storage strategy. By setting `fileStrategy` to \"firebase\", the application utilizes Firebase Storage for file management. The snippet includes the `version` and `cache` settings as well.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/firebase.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 1.0.8\ncache: true\nfileStrategy: \"firebase\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Tabs Component in TSX\nDESCRIPTION: This snippet shows how to implement a simple tab component using the Tabs and Tabs.Tab components in TSX. The Tabs component is configured with three tabs: React, Vue, and Angular. Each tab displays a brief description of the corresponding framework.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_20\n\nLANGUAGE: tsx\nCODE:\n```\n<Tabs items={['React', 'Vue', 'Angular']}>\n  <Tabs.Tab>\n    **React** is a JavaScript library for building user interfaces.\n  </Tabs.Tab>\n  <Tabs.Tab>\n    **Vue** is a progressive JavaScript framework for building user interfaces.\n  </Tabs.Tab>\n  <Tabs.Tab>\n    **Angular** is a platform for building mobile and desktop web applications.\n  </Tabs.Tab>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: End Shell Session (Bash)\nDESCRIPTION: This command exits the current shell session.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_28\n\nLANGUAGE: Bash\nCODE:\n```\nexit\n```\n\n----------------------------------------\n\nTITLE: Attach LibreChat to specific network (bash)\nDESCRIPTION: Attaches the LibreChat container to a specific network, allowing it to communicate with other services on that network. This snippet shows how to attach to multiple networks, including a load balancer network.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n--network=librechat \\\n```\n\nLANGUAGE: bash\nCODE:\n```\n--network=mybalancernetwork \\\n```\n\n----------------------------------------\n\nTITLE: Restart Docker Compose\nDESCRIPTION: This snippet shows the command to restart the LibreChat application using Docker Compose.  This command stops and then starts all the services defined in the `docker-compose.yml` and `docker-compose.override.yml` files.  It's useful for applying configuration changes without completely shutting down the application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/quick_start/custom_endpoints.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose restart\n```\n\n----------------------------------------\n\nTITLE: Check Active Docker Containers (Bash)\nDESCRIPTION: Lists the currently running Docker containers. This command can verify that the LibreChat containers are running.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: LibreChat Environment Variables Configuration\nDESCRIPTION: This snippet shows the default environment variable settings within the `values.yaml` file for the LibreChat Helm chart. These settings can be overridden to configure the application's behavior. It demonstrates the base configuration without sensitive data, which will need to be configured separately using Kubernetes Secrets.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/helm_chart.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  env:\n    # Full list of possible values\n    # https://github.com/danny-avila/LibreChat/blob/main/.env.example\n    ALLOW_EMAIL_LOGIN: \"true\"\n    ALLOW_REGISTRATION: \"true\"\n    ALLOW_SOCIAL_LOGIN: \"false\"\n    ALLOW_SOCIAL_REGISTRATION: \"false\"\n    APP_TITLE: \"Librechat\"\n    CUSTOM_FOOTER: \"Provided with \"\n    DEBUG_CONSOLE: \"true\"\n    DEBUG_LOGGING: \"true\"\n    DEBUG_OPENAI: \"true\"\n    DEBUG_PLUGINS: \"true\"\n    DOMAIN_CLIENT: \"\"\n    DOMAIN_SERVER: \"\"\n    ENDPOINTS: \"openAI,azureOpenAI,bingAI,chatGPTBrowser,google,gptPlugins,anthropic\"\n    SEARCH: false \n```\n\n----------------------------------------\n\nTITLE: OCR Configuration with Mistral OCR\nDESCRIPTION: Configures OCR using the Mistral OCR service. Specifies the Mistral model and API key.  Environment variables OCR_API_KEY and OCR_BASEURL can be used instead of specifying apiKey and baseURL in the configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/ocr.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nocr:\n  mistralModel: \"mistral-ocr-latest\"\n  apiKey: \"your-mistral-api-key\"\n  strategy: \"mistral_ocr\"\n```\n\n----------------------------------------\n\nTITLE: Configure Discord Credentials in .env File\nDESCRIPTION: This snippet shows how to configure the `.env` file with the Discord Client ID, Client Secret, and Callback URL for LibreChat authentication.  The `DOMAIN_CLIENT` and `DOMAIN_SERVER` variables are also set, based on whether a custom domain is used or not. Make sure to replace the placeholder values with your actual Discord application credentials.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/discord.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nDOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain\nDOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain\n\nDISCORD_CLIENT_ID=your_client_id\nDISCORD_CLIENT_SECRET=your_client_secret\nDISCORD_CALLBACK_URL=/oauth/discord/callback\n```\n\n----------------------------------------\n\nTITLE: Launching MLX Model Server\nDESCRIPTION: This command launches the MLX model server, allowing you to run a specified large language model. Replace `<author>/<model_id>` with the actual model identifier from Hugging Face (e.g., `mlx-community/Meta-Llama-3-8B-Instruct-4bit`). This assumes you have the `mlx_lm` package installed and configured.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-05-01_mlx.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmlx_lm.server --model <author>/<model_id>\n```\n\n----------------------------------------\n\nTITLE: Finding All Documents in a Table\nDESCRIPTION: This snippet shows how to find and display all documents within a specified table (collection) in the MongoDB database. Replace `[table]` with the actual table name (e.g., `users`, `messages`).\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndb.[table].find()\n```\n\n----------------------------------------\n\nTITLE: Switch to LibreChat Database (Bash)\nDESCRIPTION: This command switches the current database context in the MongoDB shell to the `LibreChat` database. This database is the default for LibreChat unless otherwise configured.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_27\n\nLANGUAGE: Bash\nCODE:\n```\nuse LibreChat\n```\n\n----------------------------------------\n\nTITLE: Callout with Example Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'example' type. It includes a title, collapsible functionality, and placeholder text. This shows how to use the Callout component to highlight an example.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_3\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"example\" title=\"example\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Install Helm Chart with Values Override\nDESCRIPTION: This command installs the LibreChat Helm chart, using a custom `values.yaml` override file to configure the application. This override allows you to customize settings, such as referencing individual Kubernetes secrets for sensitive configuration values. You need to have helm installed for this command to work.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/helm_chart.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install librechat helmchart --values <values-override-filel>\n```\n\n----------------------------------------\n\nTITLE: Copy .env.example to .env\nDESCRIPTION: Copies the `.env.example` file to `.env` to create the environment configuration file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n# Copies the example file as your global env file\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Render Changelog Content (JSX)\nDESCRIPTION: Renders the Content component. This component is responsible for displaying the main body of the changelog, which is likely written in Markdown and processed by MDX.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.6.mdx#_snippet_2\n\nLANGUAGE: JSX\nCODE:\n```\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Allowed Domains Configuration YAML\nDESCRIPTION: Demonstrates how to configure the 'allowedDomains' property within the actions object using YAML.  It lists example domains such as 'swapi.dev', 'librechat.ai', and 'google.com' which would be permitted for agent/assistant actions. This configuration restricts actions to the specified domains.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/actions.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nallowedDomains:\n  - \"swapi.dev\"\n  - \"librechat.ai\"\n  - \"google.com\"\n```\n\n----------------------------------------\n\nTITLE: Start Docker Service (Bash)\nDESCRIPTION: Starts the Docker service using systemctl. This ensures that Docker is running before attempting to deploy LibreChat.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl start docker\n```\n\n----------------------------------------\n\nTITLE: Callout Component Usage (Collapsible)\nDESCRIPTION: This code snippet demonstrates how to use the Callout component with the 'success' type, title, emoji, and 'collapsible' property. The 'collapsible' property is used to create collapsible callout components. Other properties like title and emoji can also be added.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_12\n\nLANGUAGE: tsx\nCODE:\n```\n<Callout type=\"success\" title=\"title\" emoji='' collapsible>\n  - The type comes with a color scheme and default emoji\n  - \"title\" is optional\n  - \"emoji\" is optional\n  - \"collapsible\" is only added when the collapsible feature is wanted\n</Callout>\n```\n\n----------------------------------------\n\nTITLE: Enable Bookmarks\nDESCRIPTION: Shows how to enable bookmarks-related features for all users in LibreChat. Setting `bookmarks` to `true` allows users to create, manage and access bookmarks.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  bookmarks: true\n```\n\n----------------------------------------\n\nTITLE: Import Home Component (JSX)\nDESCRIPTION: This snippet imports the 'Home' component from the '../components/home' module. This is a prerequisite for rendering the home page content.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/index.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { Home } from '../components/home'\n```\n\n----------------------------------------\n\nTITLE: Displaying Steps in Nextra Tabs\nDESCRIPTION: This example demonstrates the usage of `<Steps>` component (likely from Nextra or a related library) within Nextra Tabs to represent sequential steps. The steps are defined as a string within the `<Steps>` component.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_23\n\nLANGUAGE: tsx\nCODE:\n```\n<Steps>- Step 1: example - Step 2: example - Step 3: example - Step 4: example</Steps>\n```\n\n----------------------------------------\n\nTITLE: OCR Base URL Configuration\nDESCRIPTION: Sets the base URL for the custom OCR service API. This is only required when using a custom OCR service with the `custom_ocr` strategy. The baseURL should point to the root endpoint of the OCR service's API.  The baseURL can also be set using the `OCR_BASEURL` environment variable.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/ocr.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nocr:\n  baseURL: \"https://your-ocr-service.com/api\"\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Error Model List YAML\nDESCRIPTION: This YAML snippet lists Hugging Face models that resulted in errors when tested. It categorizes errors such as requiring a Pro subscription or template not found errors for different models. This can be used for troubleshooting.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/huggingface.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n      models:\n        default: [\n          \"CohereForAI/c4ai-command-r-plus\", # Model requires a Pro subscription\n          \"HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\", # Model requires a Pro subscription\n          \"meta-llama/Llama-2-7b-hf\", # Model requires a Pro subscription\n          \"meta-llama/Meta-Llama-3-70B-Instruct\", # Model requires a Pro subscription\n          \"meta-llama/Llama-2-13b-chat-hf\", # Model requires a Pro subscription\n          \"meta-llama/Llama-2-13b-hf\", # Model requires a Pro subscription\n          \"meta-llama/Llama-2-70b-chat-hf\", # Model requires a Pro subscription\n          \"meta-llama/Llama-2-7b-chat-hf\", # Model requires a Pro subscription\n          \"------\",\n          \"bigcode/octocoder\", # template not found\n          \"bigcode/santacoder\", # template not found\n          \"bigcode/starcoder2-15b\", # template not found\n          \"bigcode/starcoder2-3b\", # template not found \n          \"codellama/CodeLlama-13b-hf\", # template not found\n          \"codellama/CodeLlama-7b-hf\", # template not found\n          \"google/gemma-2b\", # template not found\n          \"google/gemma-7b\", # template not found\n          \"HuggingFaceH4/starchat-beta\", # template not found\n          \"HuggingFaceM4/idefics-80b-instruct\", # template not found\n          \"HuggingFaceM4/idefics-9b-instruct\", # template not found\n          \"HuggingFaceM4/idefics2-8b\", # template not found\n          \"kashif/stack-llama-2\", # template not found\n          \"lvwerra/starcoderbase-gsm8k\", # template not found\n          \"tiiuae/falcon-7b\", # template not found\n          \"timdettmers/guanaco-33b-merged\", # template not found\n          \"------\",\n          \"bigscience/bloom\", # 404 status code (no body)\n          \"------\",\n          \"google/gemma-2b-it\", # stream` is not supported for this model / unknown error\n          \"------\",\n          \"google/gemma-7b-it\", # AI Response error likely caused by Google censor/filter\n          \"------\",\n          \"bigcode/starcoder\", # Service Unavailable\n          \"google/flan-t5-xxl\", # Service Unavailable\n          \"HuggingFaceH4/zephyr-7b-alpha\", # Service Unavailable\n          \"mistralai/Mistral-7B-v0.1\", # Service Unavailable\n          \"OpenAssistant/oasst-sft-1-pythia-12b\", # Service Unavailable\n          \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", # Service Unavailable\n        ]\n```\n\n----------------------------------------\n\nTITLE: Import AuthorProfile Component\nDESCRIPTION: This line imports the `AuthorProfile` component from the specified path. This component is responsible for rendering the author profile based on the provided author ID.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/berry.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport AuthorProfile from '@/components/Author/AuthorProfile'\n```\n\n----------------------------------------\n\nTITLE: Configure Apple OAuth2 in .env\nDESCRIPTION: This snippet demonstrates how to configure the `.env` file for Apple OAuth2 authentication in LibreChat. It includes the client ID, team ID, key ID, path to the private key, and callback URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/apple.mdx#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nDOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain\nDOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain\n\n# Apple\nAPPLE_CLIENT_ID=com.yourdomain.librechat.services\nAPPLE_TEAM_ID=YOUR_TEAM_ID\nAPPLE_KEY_ID=YOUR_KEY_ID\nAPPLE_PRIVATE_KEY_PATH=/path/to/AuthKey.p8 # Absolute path to your downloaded .p8 file\nAPPLE_CALLBACK_URL=/oauth/apple/callback\n```\n\n----------------------------------------\n\nTITLE: Uninstall LibreChat Helm Chart\nDESCRIPTION: This command uninstalls the deployed LibreChat Helm chart from the Kubernetes cluster, removing all associated resources. Replace `<deployment-name>` with the name of your LibreChat deployment. This requires Helm to be installed and configured.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/helm_chart.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm uninstall <deployment-name>\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm uninstall librechat\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Indonesian)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Indonesian language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_10\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'id'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"ID Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Starting LibreChat with Docker Compose\nDESCRIPTION: This command starts the LibreChat application using Docker Compose. It reads the docker-compose.yml file in the project directory and starts all the services defined in the file. The -d flag runs the containers in detached mode, meaning they run in the background.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/quick_start/local_setup.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Setting Configuration Path via Environment Variable (Bash)\nDESCRIPTION: This snippet shows how to set an alternative filepath for the `librechat.yaml` file using the `CONFIG_PATH` environment variable. This is useful when you want to keep your configuration file outside the default project root.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/setup.mdx#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nCONFIG_PATH=\"/alternative/path/to/librechat.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Create MongoDB User with readWrite Role (MongoDB Shell)\nDESCRIPTION: This command creates a new user with the username 'user' and password 'userpasswd' in the 'LibreChat' database. The user is granted the `readWrite` role, limiting their access to this specific database.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_30\n\nLANGUAGE: Javascript\nCODE:\n```\ndb.createUser({ user: 'user', pwd: 'userpasswd', roles: [ { role: \"readWrite\", db: \"LibreChat\" } ] });\n```\n\n----------------------------------------\n\nTITLE: Firebase Configuration in .env\nDESCRIPTION: This snippet shows the required Firebase configuration variables that need to be set in the `.env` file. It includes the API key, auth domain, project ID, storage bucket, messaging sender ID, and app ID, which are essential for connecting the LibreChat application to the Firebase project.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/firebase.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFIREBASE_API_KEY=api_key #apiKey\nFIREBASE_AUTH_DOMAIN=auth_domain #authDomain\nFIREBASE_PROJECT_ID=project_id #projectId\nFIREBASE_STORAGE_BUCKET=storage_bucket #storageBucket\nFIREBASE_MESSAGING_SENDER_ID=messaging_sender_id #messagingSenderId\nFIREBASE_APP_ID=1:your_app_id #appId\n```\n\n----------------------------------------\n\nTITLE: Configure Known Endpoints in LibreChat YAML\nDESCRIPTION: This snippet shows placeholders for various API keys related to known endpoints, intended to be configured within the `librechat.yaml` file. These keys are used for integrations with services like Groq, ShuttleAI, and Mistral.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n#===================================#\n# Known Endpoints - librechat.yaml  #\n#===================================#\n# https://docs.librechat.ai/install/configuration/ai_endpoints.html\n\n# GROQ_API_KEY=\n# SHUTTLEAI_KEY=\n# OPENROUTER_KEY=\n# MISTRAL_API_KEY=\n# ANYSCALE_API_KEY=\n# FIREWORKS_API_KEY=\n# PERPLEXITY_API_KEY=\n# TOGETHERAI_API_KEY=\n# DEEPSEEK_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Creating .env file for API tests\nDESCRIPTION: This command copies the .env.example file to .env in the /api directory. This .env file is needed to run the API unit tests.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/testing.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example ./api/.env\n```\n\n----------------------------------------\n\nTITLE: Open MongoDB shell (Mongosh)\nDESCRIPTION: Opens the MongoDB shell (mongosh) inside the 'chat-mongodb' container. This allows you to interact with the MongoDB instance directly.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker exec -it chat-mongodb mongosh\n```\n\n----------------------------------------\n\nTITLE: Define deploymentName for Azure OpenAI Model\nDESCRIPTION: This snippet shows how to define the deploymentName for an Azure OpenAI model. The deploymentName identifies the specific deployment of the model within Azure and must match the actual name of your deployment on Azure.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_config.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\ndeploymentName: \"gpt-4-vision-preview\"\n```\n\n----------------------------------------\n\nTITLE: Install Git, Node.js, and npm\nDESCRIPTION: Installs Git for version control, Node.js for server-side JavaScript runtime, and npm for package management.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install git nodejs npm\n```\n\n----------------------------------------\n\nTITLE: Reboot the system\nDESCRIPTION: Reboots the system to apply changes, particularly after adding the user to the docker group.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo reboot\n```\n\n----------------------------------------\n\nTITLE: Configure Git User Name (Bash)\nDESCRIPTION: Sets the global Git user name.  Required for making commits to a Git repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global user.name \"Your Name\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Retrieval Capability for Azure Assistants\nDESCRIPTION: This configuration demonstrates how to disable the `retrieval` capability for Azure Assistants in LibreChat. This is recommended to avoid errors while retrieval is not yet supported through Azure OpenAI. All other capabilities except retrieval are enabled by default.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/azure.mdx#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nendpoints:\n  azureOpenAI:\n    # ...rest\n\n  azureAssistants:\n  # \"retrieval\" omitted.\n    capabilities: [\"code_interpreter\", \"actions\", \"tools\"]\n```\n\n----------------------------------------\n\nTITLE: Render React Components (JSX)\nDESCRIPTION: Renders the imported React components `ChangelogHeader` and `Content`. `ChangelogHeader` likely displays the title and date, while `Content` displays the actual changelog details written in MDX format.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.7.0-breaking-changes.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Stop Docker Compose (Bash)\nDESCRIPTION: Stops the Docker containers defined in the `deploy-compose.yml` file. This is a manual alternative to the `npm run stop:deployed` command.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose -f ./deploy-compose.yml down\n```\n\n----------------------------------------\n\nTITLE: Appending Current Datetime to Instructions in YAML\nDESCRIPTION: This snippet shows how to append the current date and time to the `additional_instructions` within the `promptPrefix` setting for each run. The `append_current_datetime` parameter, when set to `true`, automatically adds a timestamp, providing a context-aware instruction. This does not overwrite the `promptPrefix` value.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx#_snippet_15\n\nLANGUAGE: YAML\nCODE:\n```\npreset:\n  append_current_datetime: true\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Traditional Chinese)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Traditional Chinese language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_25\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'zh-Hant'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"ZH-HANT Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Modifying webui-user.bat for API access (Windows)\nDESCRIPTION: This snippet demonstrates how to modify the `webui-user.bat` file to enable the API for Stable Diffusion WebUI.  It adds the `--api` argument to the command-line arguments, enabling external access to the Stable Diffusion service.  This is required for the LibreChat plugin to communicate with Stable Diffusion.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/tools/stable_diffusion.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@echo off\n\nset PYTHON=\nset GIT=\nset VENV_DIR=\nset COMMANDLINE_ARGS=--api\n\ncall webui.bat\n```\n\n----------------------------------------\n\nTITLE: Setting Google Safety Settings in .env\nDESCRIPTION: This snippet shows how to configure the Google safety settings in the .env file. These settings allow the user to control the level of filtering for sexually explicit content, hate speech, harassment, and dangerous content.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/pre_configured_ai/google.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_SAFETY_SEXUALLY_EXPLICIT=BLOCK_ONLY_HIGH\nGOOGLE_SAFETY_HATE_SPEECH=BLOCK_ONLY_HIGH\nGOOGLE_SAFETY_HARASSMENT=BLOCK_ONLY_HIGH\nGOOGLE_SAFETY_DANGEROUS_CONTENT=BLOCK_ONLY_HIGH\n```\n\n----------------------------------------\n\nTITLE: URL Encoding Example\nDESCRIPTION: This example demonstrates how to use JavaScript's `encodeURIComponent()` function to properly encode special characters in a prompt for use in a URL.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/features/url_query.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst prompt = \"Write a function: def hello()\";\nconst encodedPrompt = encodeURIComponent(prompt);\nconst url = `/c/new?prompt=${encodedPrompt}`;\nconsole.log(url);\n```\n\n----------------------------------------\n\nTITLE: Commit Git Changes (Bash)\nDESCRIPTION: Commits the staged changes to the Git repository with the specified message. Saves the changes to the local repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -m \"edited nginx.conf\"\n```\n\n----------------------------------------\n\nTITLE: Run Docker Container with Specific Configuration\nDESCRIPTION: This command shows how to run a Docker/Podman container with specific configurations, setting the name, network, environment variables, and port mapping, running detached.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name=\"librechat\" --network=librechat --env-file=\"./.env\" -p 3080:3080 --detach librechat:local\n```\n\n----------------------------------------\n\nTITLE: Install Certbot (Ubuntu)\nDESCRIPTION: Installs Certbot and its Nginx plugin on Ubuntu to automate SSL/TLS certificate management.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install certbot python3-certbot-nginx\n```\n\n----------------------------------------\n\nTITLE: Configuring the titleMessageRole in YAML\nDESCRIPTION: This code snippet demonstrates how to configure the `titleMessageRole` option in a YAML file. This option specifies the role value to use in the message payload for title generation. The default value is `\"system\"`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_19\n\nLANGUAGE: YAML\nCODE:\n```\ntitleMessageRole: \"user\"\n```\n\n----------------------------------------\n\nTITLE: Building Frontend\nDESCRIPTION: Builds the frontend of the LibreChat application. This step is necessary to compile the frontend code and prepare it for development or deployment. It should be executed after installing the dependencies.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nnpm run frontend\n```\n\n----------------------------------------\n\nTITLE: Start Docker Container (NPM)\nDESCRIPTION: Runs the `start:deployed` script using NPM.  This script is designed to start the docker containers. Requires `npm` to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nnpm run start:deployed\n```\n\n----------------------------------------\n\nTITLE: Bedrock Endpoint Configuration YAML\nDESCRIPTION: This YAML configuration block demonstrates the overall structure for configuring the AWS Bedrock endpoint. It defines the model used for generating conversation titles (`titleModel`), the stream rate for token processing (`streamRate`), and the AWS regions that are available (`availableRegions`).\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/aws_bedrock.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nendpoints:\n  bedrock:\n    titleModel: \"anthropic.claude-3-haiku-20240307-v1:0\"\n    streamRate: 35\n    availableRegions:\n      - \"us-east-1\"\n      - \"us-west-2\"\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Estonian)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Estonian language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_4\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'et'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"ET Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Creating a Pull Request\nDESCRIPTION: Updates the local main branch, merges it into the feature branch, and pushes the changes to the remote repository before creating a pull request on GitHub.  This integrates the latest changes from the main branch into the feature branch, resolving any conflicts before submitting the PR.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ngit checkout main\ngit pull origin main\ngit checkout feature-branch-name\ngit merge main\n# Resolve conflicts if any\ngit push origin feature-branch-name\n# Open PR on GitHub\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (French)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the French language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_7\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'fr'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"FR Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Dutch)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Dutch language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_15\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'nl'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"NL Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Finnish)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Finnish language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_6\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'fi'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"FI Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Set Refill Interval Unit in YAML\nDESCRIPTION: This YAML snippet defines the time unit for the refill interval (e.g., \"days\", \"hours\"). It indicates the unit of time for `refillIntervalValue`.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/balance.mdx#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nbalance:\n  refillIntervalUnit: \"days\"\n```\n\n----------------------------------------\n\nTITLE: Assistants Endpoint Configuration Example YAML\nDESCRIPTION: This is an example YAML configuration for the assistants endpoint. It includes settings for disabling the builder interface, specifying supported assistant IDs, and configuring polling interval and timeout values. It also shows how to specify retrieval models and assistant capabilities.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nendpoints:\n  # azureAssistants: # <-- Azure-specific configuration has the same structure as `assistants`\n    #  pollIntervalMs: 500\n    #  timeoutMs: 10000\n\n  assistants:\n    disableBuilder: false\n    # Use either `supportedIds` or `excludedIds` but not both\n    supportedIds: [\"asst_supportedAssistantId1\", \"asst_supportedAssistantId2\"]\n    # excludedIds: [\"asst_excludedAssistantId\"]\n    # `privateAssistants` do not work with `supportedIds` or `excludedIds`\n    # privateAssistants: false\n    # (optional) Models that support retrieval, will default to latest known OpenAI models that support the feature\n    # retrievalModels: [\"gpt-4-turbo-preview\"]\n    # (optional) Assistant Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.\n    # capabilities: [\"code_interpreter\", \"retrieval\", \"actions\", \"tools\", \"image_vision\"]\n```\n\n----------------------------------------\n\nTITLE: Update apt packages\nDESCRIPTION: Updates the package lists for upgrades and new package installations.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Custom Welcome Message\nDESCRIPTION: Shows how to define a custom welcome message for the chat interface in LibreChat. This allows for a personalized greeting experience.  The `{{user.name}}` parameter is used to dynamically insert the user's name.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n    customWelcome: \"Hey {{user.name}}! Welcome to LibreChat\"\n```\n\n----------------------------------------\n\nTITLE: Rebase deployed instance\nDESCRIPTION: Rebases a deployed instance of LibreChat to incorporate recent changes.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm run rebase:deployed\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Japanese)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Japanese language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_12\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ja'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"JA Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Georgian)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Georgian language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_13\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ka'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"KA Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Disable MeiliSearch Sync in Multi-node Setup\nDESCRIPTION: This environment variable, when set to `true`, prevents database documents from syncing redundantly across multiple LibreChat instances in a node cluster, optimizing resource consumption. This is useful when only one instance should be responsible for indexing.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/meilisearch.mdx#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nMEILI_NO_SYNC=true\n```\n\n----------------------------------------\n\nTITLE: Configure LibreChat to use Azure Blob Storage\nDESCRIPTION: This snippet demonstrates how to configure LibreChat to use Azure Blob Storage for file handling by setting the `fileStrategy` property to `azure` in the `librechat.yaml` configuration file.  This configuration tells LibreChat to use the implemented Azure Blob Storage strategy. Requires a `librechat.yaml` file in the root directory.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/azure.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 1.0.8\ncache: true\nfileStrategy: \"azure\"\n```\n\n----------------------------------------\n\nTITLE: Reverting Changes - Pull from Feature Branch\nDESCRIPTION: Updates the local repository with the latest changes from the specified feature branch before reverting any commits. This ensures that the local repository is up-to-date with the remote branch before starting the rebase process.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ngit pull origin feature-branch-name\n```\n\n----------------------------------------\n\nTITLE: Opening Project in VS Code\nDESCRIPTION: Opens the current directory (LibreChat) in VS Code. This command assumes that VS Code is installed and accessible from the command line.  It provides a convenient way to start working on the project using the VS Code editor.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncode .\n```\n\n----------------------------------------\n\nTITLE: Embedding images using next/image in MDX\nDESCRIPTION: This MDX snippet demonstrates how to embed images with light and dark theme support using Next.js's Image component.  It imports the Image component and then uses it to render images from a GitHub URL, setting width, height, and styling options. The images are wrapped in divs to control layout and theme-specific display.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/index.mdx#_snippet_2\n\nLANGUAGE: mdx\nCODE:\n```\nimport Image from 'next/image'\n\n<div style={{padding: \"20px\", display: \"flex\", justifyContent: \"center\", alignItems: \"center\", flexDirection: \"column\"}}>\n  <div className=\"image-light-theme\">\n    <Image src=\"https://github.com/danny-avila/LibreChat/assets/32828263/cf0f3231-287a-407f-bd4d-3d5bad94e893\" alt=\"ipad-light\" width={1024} height={512} style={{borderRadius: \"5px\"}} />\n  </div>\n\n  <div className=\"image-dark-theme\">\n    <Image src=\"https://github.com/danny-avila/LibreChat/assets/32828263/a03ee02d-5099-4220-95b0-bfa2d3b00b4d\" alt=\"ipad-dark\" width={1024} height={512} style={{borderRadius: \"5px\"}} />\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Verify Docker Compose version\nDESCRIPTION: Checks the installed version of Docker Compose to confirm the installation was successful.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose -v\n# output should be: Docker Compose version v2.20.2\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Thai)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Thai language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_21\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'th'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"TH Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Disabling File Uploads for OpenAI Endpoint in YAML\nDESCRIPTION: This example shows how to disable file uploads for a specific endpoint, in this case 'openAI', using the `disabled` option in YAML. Setting `disabled` to `true` prevents any file uploads to the specified endpoint.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nopenAI:\n  disabled: true\n```\n\n----------------------------------------\n\nTITLE: Starting LibreChat with Docker Compose\nDESCRIPTION: This command starts the LibreChat application using Docker Compose. The `-d` flag runs the containers in detached mode, meaning they will run in the background.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Portuguese)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Portuguese (Portugal) language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_17\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'pt-PT'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"PT-PT Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Enable Azure OpenAI Model with Default Group Configuration\nDESCRIPTION: This snippet shows how to enable an Azure OpenAI model using a boolean value, which utilizes the default group configuration for deployment name and version. This is a simpler method when specific deployment settings are not required.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/model_config.mdx#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nmodels:\n  gpt-4-turbo: true\n```\n\n----------------------------------------\n\nTITLE: Deploying LibreChat Metrics Exporter with Docker Compose\nDESCRIPTION: This snippet shows how to add the librechat_exporter to a Docker Compose configuration. It defines the container, specifies dependencies on MongoDB, maps ports, and sets a restart policy.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/metrics.mdx#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  metrics:\n    image: ghcr.io/virtuos/librechat_exporter:main\n    depends_on:\n      - mongodb\n    ports:\n      - \"8000:8000\"\n    restart: unless-stopped\n```\n\n----------------------------------------\n\nTITLE: Import and Render Terms of Service Component (JSX)\nDESCRIPTION: This code snippet imports the `TermsOfServices` component from the specified path and renders it. The component likely contains the actual terms of service content. No dependencies are explicitly shown beyond the imported component.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/tos.mdx#_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { TermsOfServices } from '@/components/policies'\n\n<TermsOfServices />\n```\n\n----------------------------------------\n\nTITLE: Start MongoDB Container (bash)\nDESCRIPTION: Starts a MongoDB container named `librechat-mongodb` using the `docker.io/mongo` image.  It configures network and volume settings for persistent data storage.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npodman run \\\n  --name=\"librechat-mongodb\" \\\n  --network=librechat \\\n  -v \"librechat-mongodb-data:/data/db\" \\\n  --detach \\\n  docker.io/mongo \\\n  mongod --noauth\n```\n\n----------------------------------------\n\nTITLE: Add user to docker group\nDESCRIPTION: Adds the current user to the 'docker' group, granting them permission to execute Docker commands without using sudo.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo usermod -aG docker $USER\n```\n\n----------------------------------------\n\nTITLE: Import CredsGenerator component\nDESCRIPTION: This snippet imports the CredsGenerator component from the specified path. The CredsGenerator component is used for generating credentials within the application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/toolkit/creds_generator.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport CredsGenerator from '@/components/tools/CredentialsGeneratorBox'\n```\n\n----------------------------------------\n\nTITLE: Download the latest Docker Compose version\nDESCRIPTION: Downloads the latest version of Docker Compose from the GitHub releases page using curl.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nsudo curl -L https://github.com/docker/compose/releases/download/v2.26.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n```\n\n----------------------------------------\n\nTITLE: Add Tavily and Traversaal API Keys\nDESCRIPTION: This snippet shows the addition of environment variables for the Tavily and Traversaal APIs, `TAVILY_API_KEY` and `TRAVERSAAL_API_KEY` respectively, for integrating with these services.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n# Tavily\n#-----------------\nTAVILY_API_KEY=\n\n# Traversaal\n#-----------------\nTRAVERSAAL_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: OCR API Key Configuration\nDESCRIPTION: Sets the API key to be used for the OCR service. This key is required to authenticate with the OCR service, whether it's Mistral's or a custom service. The API key can also be set using the `OCR_API_KEY` environment variable.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/ocr.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nocr:\n  apiKey: \"your-ocr-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Set recursionLimit for Agents\nDESCRIPTION: This YAML snippet shows how to set the `recursionLimit` for an agent. The `recursionLimit` controls the default number of steps an agent can take in a run. Setting a reasonable value is crucial to prevent infinite loops.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/agents.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nrecursionLimit: 50\n```\n\n----------------------------------------\n\nTITLE: Delete User Script Execution (Local Development)\nDESCRIPTION: Executes the `delete-user` script locally, deleting a user by their email address.  Assumes command execution from project root with Node.js and npm installed.  Replace `email@domain.com` with the email of the user to delete.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/index.mdx#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nnpm run delete-user email@domain.com\n```\n\n----------------------------------------\n\nTITLE: Listing Balances via CLI in LibreChat\nDESCRIPTION: These bash snippets show the commands to list all user balances in LibreChat via the command line, covering different environments like local development and Docker setups. The snippets use `npm run list-balances` within the appropriate context (local, docker-compose, or docker exec) to display user balances.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/token_usage.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Local Development\nnpm run list-balances\n\n# Docker (default setup)\ndocker-compose exec api npm run list-balances\n\n# Docker (deployment setup)\ndocker exec -it LibreChat-API /bin/sh -c \"cd .. && npm run list-balances\"\n```\n\n----------------------------------------\n\nTITLE: Shut down container after user creation (Docker Compose)\nDESCRIPTION: Stops the MongoDB container after the LibreChat user has been created.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Recreate Liberchat Container\nDESCRIPTION: This command recreates the LibreChat container using the newly built image. It sets the container name, network, environment variables, and port mapping. The `--detach` flag runs the container in the background.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\npodman run --name=\"librechat\" --network=librechat --env-file=\"./.env\" -p 3080:3080 --detach librechat:local\n```\n\n----------------------------------------\n\nTITLE: OCR Configuration with Custom OCR\nDESCRIPTION: Configures OCR using a custom OCR service. Specifies the API key, base URL, and sets the strategy to `custom_ocr`. The baseURL should point to the API endpoint of the custom OCR service.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/ocr.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nocr:\n  apiKey: \"your-custom-ocr-api-key\"\n  baseURL: \"https://your-custom-ocr-service.com/api\"\n  strategy: \"custom_ocr\"\n```\n\n----------------------------------------\n\nTITLE: Make MeiliSearch Binary Executable (Linux/macOS)\nDESCRIPTION: This command sets the execute permission for the MeiliSearch binary, allowing it to be run from the terminal in Linux and macOS environments. It's a prerequisite for starting the MeiliSearch server.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/meilisearch.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nchmod +x meilisearch\n```\n\n----------------------------------------\n\nTITLE: Configuring the 'iconURL' field in YAML\nDESCRIPTION: This snippet shows how to configure the `iconURL` field for a custom endpoint, specifying the URL of the icon to use. The default value is an empty string.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\niconURL: https://github.com/danny-avila/LibreChat/raw/main/docs/assets/LibreChat.svg\n```\n\n----------------------------------------\n\nTITLE: Enable Side Panel\nDESCRIPTION: Shows how to enable the side panel in the LibreChat interface. Setting `sidePanel` to `true` makes the side panel visible, providing additional navigation or information within the application. This is enabled by default.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  sidePanel: true\n```\n\n----------------------------------------\n\nTITLE: Removed ChatGPT Browser Endpoint Configuration\nDESCRIPTION: This snippet indicates the removal of the `chatGPTBrowser` endpoint from the list of available endpoints. The `ENDPOINTS` variable no longer includes this option.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# ENDPOINTS=openAI,assistants,azureOpenAI,bingAI,google,gptPlugins,anthropic\n```\n\n----------------------------------------\n\nTITLE: Stop Docker Compose Services (Bash)\nDESCRIPTION: This command stops all the services defined in the `docker-compose.yml` and `docker-compose.override.yml` files.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_29\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Install Librechat MongoDB (bash)\nDESCRIPTION: Installs the LibreChat MongoDB container as a systemd service using the provided installation script.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n./install.sh librechat-mongodb\n```\n\n----------------------------------------\n\nTITLE: Install Librechat Application (bash)\nDESCRIPTION: Installs the main LibreChat application container as a systemd service using the provided installation script.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n./install.sh librechat\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Italian)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Italian language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_11\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'it'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"IT Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Start MongoDB Container (Bash)\nDESCRIPTION: This command starts the MongoDB container using Docker Compose. It assumes that the `docker-compose.override.yml` file has been configured to enable authentication.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_24\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose up -d mongodb\n```\n\n----------------------------------------\n\nTITLE: Configure Illegal Model Request Score\nDESCRIPTION: This snippet shows how to configure the score for illegal model requests using the `ILLEGAL_MODEL_REQ_SCORE` environment variable. It's likely related to moderation features.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nILLEGAL_MODEL_REQ_SCORE=5\n```\n\n----------------------------------------\n\nTITLE: Build LibreChat Base Container Image (bash)\nDESCRIPTION: Builds the base container image for LibreChat using the Dockerfile located within the cloned repository. It tags the image as \"librechat:local\".\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npodman build --tag \"librechat:local\" --file ./LibreChat/Dockerfile\n```\n\n----------------------------------------\n\nTITLE: Stop Systemd Service\nDESCRIPTION: This command stops the systemd service responsible for running the LibreChat container. It ensures that the existing container is stopped before proceeding with the update.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl --user stop container-librechat\n```\n\n----------------------------------------\n\nTITLE: Supported IDs Configuration YAML\nDESCRIPTION: This YAML snippet shows how to define a list of supported assistant IDs using the `supportedIds` option.  Only assistants with these IDs will be allowed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nsupportedIds:\n  - \"asst_supportedAssistantId1\"\n  - \"asst_supportedAssistantId2\"\n```\n\n----------------------------------------\n\nTITLE: Copy Example Override File (Bash)\nDESCRIPTION: Copies the example override file to create a docker-compose.override.yml file. This allows you to modify the default Docker Compose configuration without altering the original docker-compose.yml file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/docker_override.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncp docker-compose.override.yml.example docker-compose.override.yml\n```\n\n----------------------------------------\n\nTITLE: Stage Git Changes (Bash)\nDESCRIPTION: Stages all modified and new files in the current directory for commit. This prepares the changes to be saved in the Git repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\n```\n\n----------------------------------------\n\nTITLE: Removed ChatGPT Configuration\nDESCRIPTION: This snippet shows the removal of the ChatGPT-specific environment variables, indicating that direct configuration of ChatGPT is no longer supported.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.7.0-breaking-changes.mdx#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n#============#\n# ChatGPT    #\n#============#\n\nCHATGPT_TOKEN=\nCHATGPT_MODELS=text-davinci-002-render-sha\n# CHATGPT_REVERSE_PROXY=\n```\n\n----------------------------------------\n\nTITLE: Accessing MongoDB Shell\nDESCRIPTION: This snippet shows how to access the MongoDB shell within the Docker container to interact with the database. It assumes you have access to the Docker container's console.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmongosh\n```\n\n----------------------------------------\n\nTITLE: Configuring the 'name' field in YAML\nDESCRIPTION: This snippet demonstrates how to define the `name` field for a custom endpoint. The `name` serves as a unique identifier and will be displayed as the title in the Endpoints Selector. This field is required.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nname: \"Mistral\"\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Brazilian Portuguese)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Brazilian Portuguese language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_18\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'pt-BR'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"PT-BR Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Disable Endpoints Menu\nDESCRIPTION: Shows how to disable the endpoints menu in the LibreChat interface. Setting `endpointsMenu` to `false` hides the endpoint selection options from the user interface. Note that this setting is deprecated.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  endpointsMenu: false\n```\n\n----------------------------------------\n\nTITLE: Update Git Repository\nDESCRIPTION: This command updates the LibreChat git repository to the latest version by pulling changes from the remote repository. It assumes that the current directory is the parent directory containing the LibreChat repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n(cd LibreChat && git pull)\n```\n\n----------------------------------------\n\nTITLE: Restart LibreChat with Docker Compose\nDESCRIPTION: This snippet shows the command to restart LibreChat using Docker Compose after updating the .env file. This ensures the new configuration is loaded.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/OAuth2-OIDC/apple.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Edit Docker Compose File (Bash)\nDESCRIPTION: Opens the `deploy-compose.yml` file in the `nano` text editor for modification. Allows changes to the Docker Compose configuration.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\nnano deploy-compose.yml\n```\n\n----------------------------------------\n\nTITLE: Switch to Admin Database\nDESCRIPTION: Switches the context to the 'admin' database within the MongoDB shell.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_18\n\nLANGUAGE: Bash\nCODE:\n```\nuse admin\n```\n\n----------------------------------------\n\nTITLE: Stop deployed instance\nDESCRIPTION: Stops a currently running deployed instance of LibreChat.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm run stop:deployed\n```\n\n----------------------------------------\n\nTITLE: Callout with Note Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'note' type. It includes a title, collapsible functionality, and placeholder text. This is a specific usage for displaying notes or important information.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_4\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"note\" title=\"note\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Restart Systemd Service After Stopping Container\nDESCRIPTION: This command stops the LibreChat container and restarts the systemd service. It is used to ensure that the updated container is running correctly.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\npodman stop librechat && systemctl --user start container-librechat\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Container(s)\nDESCRIPTION: This command stops the running LibreChat containers. This needs to be done before pulling the latest changes to avoid conflicts.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/docker.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Stop Docker Instance (NPM)\nDESCRIPTION: Stops the running LibreChat instance using an NPM script. Requires `npm` to be installed.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nnpm run stop:deployed\n```\n\n----------------------------------------\n\nTITLE: Authentication Flow Sequence Diagram\nDESCRIPTION: This Mermaid diagram illustrates the authentication flow, depicting the interactions between the client, server, Passport, and database during login, access token usage, refresh token usage, and session management.  It shows how access and refresh tokens are generated, stored, and used to maintain user sessions.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/index.mdx#_snippet_6\n\nLANGUAGE: Mermaid\nCODE:\n```\nsequenceDiagram\n    Client->>Server: Login request with credentials\n    Server->>Passport: Use authentication strategy (e.g., 'local', 'google', etc.)\n    Passport-->>Server: User object or false/error\n    Note over Server: If valid user...\n    Server->>Server: Generate access and refresh tokens\n    Server->>Database: Store hashed refresh token\n    Server-->>Client: Access token and refresh token\n    Client->>Client: Store access token in HTTP Header and refresh token in HttpOnly cookie\n    Client->>Server: Request with access token from HTTP Header\n    Server-->>Client: Requested data\n    Note over Client,Server: Access token expires\n    Client->>Server: Request with expired access token\n    Server-->>Client: Unauthorized\n    Client->>Server: Request with refresh token from HttpOnly cookie\n    Server->>Database: Retrieve hashed refresh token\n    Server->>Server: Compare hash of provided refresh token with stored hash\n    Note over Server: If hashes match...\n    Server-->>Client: New access token and refresh token\n    Client->>Server: Retry request with new access token\n    Server-->>Client: Requested data\n```\n\n----------------------------------------\n\nTITLE: Translation with Single Interpolation - Example\nDESCRIPTION: This example demonstrates translating a string with a single interpolation ({{0}}) from English to German, ensuring the placeholder remains intact and is placed appropriately in the translated text.\nEnglish: Talked to {{0}}\nGerman: Mit {{0}} gesprochen\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_26\n\nLANGUAGE: text\nCODE:\n```\nTalked to {{0}}\n```\n\nLANGUAGE: text\nCODE:\n```\nMit {{0}} gesprochen\n```\n\n----------------------------------------\n\nTITLE: Connect to the MongoDB shell\nDESCRIPTION: Executes the `mongosh` command within the `chat-mongodb` container to connect to the MongoDB shell.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_auth.mdx#_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\ndocker exec -it chat-mongodb mongosh\n```\n\n----------------------------------------\n\nTITLE: Adding Balances via CLI in LibreChat\nDESCRIPTION: These bash snippets provide commands for manually adding token balances to users in LibreChat, covering both local development and Docker setups. The snippets demonstrate how to execute the `add-balance` script using npm or Docker commands and how to specify the user's email and the amount of tokens to add.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/token_usage.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Local Development\nnpm run add-balance\n\n# Docker (default setup)\ndocker-compose exec api npm run add-balance\n\n# Docker (deployment setup)\ndocker exec -it LibreChat-API /bin/sh -c \"cd .. && npm run add-balance\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Local Development\nnpm run add-balance danny@librechat.ai 1000\n\n# Docker (default setup)\ndocker-compose exec api npm run add-balance danny@librechat.ai 1000\n\n# Docker (deployment setup)\ndocker exec -it LibreChat-API /bin/sh -c \"cd .. && npm run add-balance danny@librechat.ai 1000\"\n```\n\n----------------------------------------\n\nTITLE: Edit .env file (again)\nDESCRIPTION: Shows how to edit .env file again using nano.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n# if editing the .env file\nnano .env\n```\n\n----------------------------------------\n\nTITLE: Configuring Birthday Icon (Shell)\nDESCRIPTION: This code snippet configures whether a birthday icon is shown in LibreChat.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/components/changelog/content/v0.6.9-breaking-changes.mdx#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# SHOW_BIRTHDAY_ICON=true\n```\n\n----------------------------------------\n\nTITLE: Callout with Default Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'default' type. It includes a title, collapsible functionality, and placeholder text. This is a basic example showing the standard appearance of the Callout component.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_2\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"default\" title=\"default\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Callout with Tip Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'tip' type. It includes a title, collapsible functionality, and placeholder text, typically used to provide helpful tips.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_7\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"tip\" title=\"tip\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Remote\nDESCRIPTION: Pushes the local commits to the remote repository on the specified branch. This shares the changes with the remote repository, making them available to other contributors and for deployment.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ngit push origin feature-branch-name\n```\n\n----------------------------------------\n\nTITLE: Fetching together.ai Models with Python\nDESCRIPTION: This Python script fetches available chat models from the together.ai API, sorts them, and saves them to a JSON file. It requires the 'requests' library and an API key. The output is a JSON file named 'models_togetherai.json' containing a sorted list of model IDs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/ai_endpoints/togetherai.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nimport requests\n\n# API key\napi_key = \"\"\n\n# API endpoint\nurl = \"https://api.together.xyz/v1/models\"\n\n# headers\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n}\n\n# make request\nresponse = requests.get(url, headers=headers)\n\n# parse JSON response\ndata = response.json()\n\n# extract an ordered list of unique model IDs\nmodel_ids = sorted(\n    [\n        model['id']\n        for model in data\n        if model['type'] == 'chat'\n    ]\n)\n\n# write result to a text file\nwith open(\"models_togetherai.json\", \"w\") as file:\n    json.dump(model_ids, file, indent=2)\n```\n\n----------------------------------------\n\nTITLE: Install Librechat Meilisearch (bash)\nDESCRIPTION: Installs the LibreChat Meilisearch container as a systemd service using the provided installation script.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n./install.sh librechat-meilisearch\n```\n\n----------------------------------------\n\nTITLE: Blog Post MDX Metadata and Header Template\nDESCRIPTION: This is the MDX template for a blog post, including metadata such as title, date, description, tags, and author ID. It also includes an import statement and component for the blog header.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_7\n\nLANGUAGE: mdx\nCODE:\n```\n---\n\ntitle: Your Awesome Title\ndate: 2024/01/01\ndescription: \"Short description of the blog post\"\ntags: \n  - tag1 \n  - tag2\nauthorid: your-authorid\nogImage: /images/blog/2024-01-01_your_awesome_title.png\n\n---\n\nimport { BlogHeader } from '@/components/blog/BlogHeader'\n\n<BlogHeader />\n\n# Title \nLorem ipsum...\n```\n\n----------------------------------------\n\nTITLE: Callout with Question Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'question' type. It includes a title, collapsible functionality, and placeholder text, typically used to pose questions or request information.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_9\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"question\" title=\"question\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Reverting Changes - Review Commit History\nDESCRIPTION: Displays the commit history for the current branch. Review the commit history to determine which commits need to be reverted. This is essential to identify the specific commits to be removed during the interactive rebase process.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ngit log\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Polish)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Polish language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_16\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'pl'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"PL Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Rendering a Single Card with Code Icon in TSX\nDESCRIPTION: This snippet shows how to render a single card with a code icon using the Cards.Card component in TSX.  It imports the Code component from '@/components/svg/' and uses it as the icon for the card, which is linked to the root path.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_17\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Code } from '@/components/svg/'\n;<Cards.Card icon={<Code />} title=\"Code\" href=\"/\" />\n```\n\n----------------------------------------\n\nTITLE: Callout with Error Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'error' type. It includes a title, collapsible functionality, and placeholder text, typically used to display errors or critical failures.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_11\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"error\" title=\"error\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n```\n\n----------------------------------------\n\nTITLE: Defining Helper Methods JavaScript\nDESCRIPTION: Shows an example of defining helper methods within the tool class.  The provided example demonstrates the `replaceNewLinesWithSpaces` method, which replaces newline characters with spaces.  This highlights the flexibility of the Tool class for handling input pre-processing and data manipulation.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nclass StableDiffusionAPI extends Tool {\n  ...\n  replaceNewLinesWithSpaces(inputString) {\n    return inputString.replace(/\\r\\n|\\r|\\n/g, ' ');\n  }\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Changing Directory\nDESCRIPTION: This command changes the current directory to the newly cloned `librechat.ai` directory.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-04-17_blog_guide.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd librechat.ai\n```\n\n----------------------------------------\n\nTITLE: Rendering React Components\nDESCRIPTION: This snippet renders the ChangelogHeader and Content components. The ChangelogHeader component is rendered using the self-closing tag syntax, and the Content component is also rendered using the self-closing tag syntax. This allows for easy rendering of React components within the page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/config_v1.01.mdx#_snippet_1\n\nLANGUAGE: MDX\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Set Custom Theme for Shiki in Nextra\nDESCRIPTION: This code snippet shows how to set a custom theme for syntax highlighting in Nextra using Shiki, within the `rehypePrettyCodeOptions` of the `mdxOptions`. It reads a VSCode theme from a JSON file and assigns it to the `theme` option, allowing for customized visual styling of code blocks.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/syntax_highlighting.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nnextra({\n  // ... other options\n  mdxOptions: {\n    rehypePrettyCodeOptions: {\n      // VSCode theme or built-in Shiki theme, see Shiki documentation for more information\n      theme: JSON.parse(\n        readFileSync('./public/syntax/arctis_light.json', 'utf8')\n      )\n    }\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Committing Changes with Git\nDESCRIPTION: Stages all changes and creates a new commit with the provided message. This marks a logical checkpoint in development. A clear commit message explaining the changes is important.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ngit add .\ngit commit -m \"Add login functionality\"\n```\n\n----------------------------------------\n\nTITLE: Delete User Script Execution (Deploy Compose)\nDESCRIPTION: Executes the `delete-user` script within the `LibreChat-API` container when using the `deploy-compose.yml` file.  Deletes a user from the database using their email. Replace `email@domain.com` with the correct email address.  Docker and container knowledge are required.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/authentication/index.mdx#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ndocker exec -it LibreChat-API /bin/sh -c \"cd .. && npm run delete-user email@domain.com\"\n```\n\n----------------------------------------\n\nTITLE: Install Nginx (CentOS)\nDESCRIPTION: Installs Nginx on CentOS, enabling its web server functionality.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install nginx\n```\n\n----------------------------------------\n\nTITLE: Selecting the LibreChat Database\nDESCRIPTION: This snippet shows how to select the 'librechat' database within the MongoDB shell to perform operations on it.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nuse librechat\n```\n\n----------------------------------------\n\nTITLE: Rendering Content component\nDESCRIPTION: Renders the `Content` component, which displays the Markdown content from the `v0.5.9.mdx` file. This component handles the display of the main changelog body, including any updates, features, or fixes.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.5.9.mdx#_snippet_2\n\nLANGUAGE: JSX\nCODE:\n```\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Start application with Docker Compose\nDESCRIPTION: Starts the LibreChat application using Docker Compose, building and running the necessary containers in detached mode.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker-compose -f ./deploy-compose.yml up -d\n```\n\n----------------------------------------\n\nTITLE: Callout with Info Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'info' type. It includes a title, collapsible functionality, and placeholder text, used to display informative content.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_6\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"info\" title=\"info\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Rendering React Components\nDESCRIPTION: This snippet renders the `ChangelogHeader` and `Content` React components within the page. The `Content` component likely displays the details of the Config v1.0.6 changelog from a markdown file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/config_v1.06.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n<ChangelogHeader />\n\n---\n\n<Content />\n```\n\n----------------------------------------\n\nTITLE: Applying CORS Configuration (gsutil)\nDESCRIPTION: This shell command uses the `gsutil` tool to apply the CORS configuration defined in the `cors.json` file to the specified Firebase Storage bucket.  `<your-cloud-storage-bucket>` must be replaced with the name of the actual bucket. The `gsutil` tool must be installed and configured to interact with the Firebase project.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/cdn/firebase.mdx#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngsutil cors set cors.json gs://<your-cloud-storage-bucket>\n```\n\n----------------------------------------\n\nTITLE: Check Nginx configuration\nDESCRIPTION: Verifies the Nginx configuration file for syntax errors before applying the changes.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nsudo nginx -t\n```\n\n----------------------------------------\n\nTITLE: Render Home Component (JSX)\nDESCRIPTION: This snippet renders the imported 'Home' component. It is responsible for displaying the main content of the LibreChat home page.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/index.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<Home />\n```\n\n----------------------------------------\n\nTITLE: Rendering Author Profile Component\nDESCRIPTION: This JavaScript/JSX snippet imports the `AuthorProfile` component from '@/components/Author/AuthorProfile' and renders it, passing the authorId 'rubent' as a prop. This displays the author's profile based on the data fetched or provided to the `AuthorProfile` component.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/rubent.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport AuthorProfile from '@/components/Author/AuthorProfile'\n\n<AuthorProfile authorId=\"rubent\" />\n```\n\n----------------------------------------\n\nTITLE: Document Metadata example in YAML format\nDESCRIPTION: This YAML snippet demonstrates the required metadata format for LibreChat documentation files. It includes fields for the document title, description (used in social cards and search engine results), and an optional ogImage for social media sharing. The ogImage path should be relative to the /public/images directory.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/index.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Document Title\ndescription: This description will be used in social cards and search engine results.\nogImage: /images/docs/<category>/image.png (optional)\n---\n```\n\n----------------------------------------\n\nTITLE: Render SubscribeForm Component in JSX\nDESCRIPTION: This snippet renders the `SubscribeForm` component within a `div` with the class name 'text-center py-20'. This centers the form horizontally and adds padding on the top and bottom. The component handles the form's presentation and submission logic.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/subscribe.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<div className=\"text-center py-20\">\n\n<SubscribeForm />\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Rendering Blog Index with JSX\nDESCRIPTION: This JSX code renders the blog page layout. It includes a heading, some spacing, and the BlogIndex component. The container class 'md:container' provides responsive styling. The flex classes center the content horizontally and vertically. The BlogIndex component is responsible for displaying the actual blog posts.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<div className=\"md:container\">\n  <div className=\"flex flex-col items-center content-center text-center my-10\">\n    <h1 className=\"font-bold text-4xl lg:text-5xl xl:text-5xl\">LibreChat's Blog</h1>\n    <div className=\"mb-8\"></div>\n  </div>\n  <BlogIndex />\n</div>\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (German)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the German language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'de'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"DE Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Persian)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Persian language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_5\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'fa'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"FA Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Import Components (JSX)\nDESCRIPTION: Imports the `ChangelogHeader` and `Content` components from specified modules. These components are used to structure and display the changelog information.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/changelog/v0.6.9.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\nimport { ChangelogHeader } from '@/components/changelog/ChangelogHeader'\nimport Content from '@/components/changelog/content/v0.6.9.mdx'\n```\n\n----------------------------------------\n\nTITLE: Bring Down Docker Environment (Shell)\nDESCRIPTION: This command stops and removes the Docker containers, networks, volumes, and images created by `docker compose up`. This is used to remove the Mongo Express container.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_mongoexpress.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Stop LibreChat Container (bash)\nDESCRIPTION: Stops a running LibreChat container.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npodman stop librechat\n```\n\n----------------------------------------\n\nTITLE: Importing LangChain Tool Module JavaScript\nDESCRIPTION: Imports the `Tool` class from the `langchain/tools` module. This is the first step in creating a custom plugin for LibreChat, as plugins are essentially extensions of this class. It's crucial for defining the structure and behavior of the plugin.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst { Tool } = require('langchain/tools');\n// ... whatever else you need\n```\n\n----------------------------------------\n\nTITLE: Change Docker Image in Compose File (YAML)\nDESCRIPTION: Example of the previous docker image used in the deploy-compose.yml file. Used for context.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_43\n\nLANGUAGE: yaml\nCODE:\n```\nimage: ghcr.io/danny-avila/librechat-dev-api:latest\n```\n\n----------------------------------------\n\nTITLE: Cloning a LibreChat branch using Git\nDESCRIPTION: This command clones a specific branch from a forked LibreChat repository to a local machine. Replace `branch-name` with the name of your branch and `username` with your GitHub username.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/index.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone -b branch-name https://github.com/username/librechat.ai.git\n```\n\n----------------------------------------\n\nTITLE: Setting the ASSISTANTS_API_KEY Variable in Docker Compose\nDESCRIPTION: Instructions to set the ASSISTANTS_API_KEY environment variable within the Docker Compose file to enable OpenAI Assistants functionality in LibreChat. The variable should be added as a new 'Variable' config type in the Docker Compose settings.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2024-03-22_unraid_guide.mdx#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n- **Config Type**: Variable\n- **Name**: Assistants API Key\n- **Key**: ASSISTANTS_API_KEY\n- **Value**: [Your OpenAI API Key (yes we're entering it again)]\n- **Default Value**: [blank]\n- **Description**: [blank]\n- **Display**: Always\n- **Required**: No\n- **Password Mask**: Yes\n```\n\n----------------------------------------\n\nTITLE: Starting MongoDB Server (mongod) with dbpath\nDESCRIPTION: This command starts the MongoDB server (mongod) and specifies the path to the data directory where MongoDB will store its data files. Replace `/path/to/data/directory` with the actual path to your data directory. This is a prerequisite to running the LibreChat application.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_community.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./mongod --dbpath=/path/to/data/directory\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Swedish)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Swedish language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_20\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'sv'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"SV Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Import SubscribeForm Component\nDESCRIPTION: This snippet imports the `SubscribeForm` component from the '@/components/Newsletter/SubscribeForm' path. This component is responsible for rendering and handling the newsletter subscription form.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/subscribe.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport SubscribeForm from '@/components/Newsletter/SubscribeForm'\n```\n\n----------------------------------------\n\nTITLE: Exporting a Tool Module (JavaScript)\nDESCRIPTION: This snippet illustrates how to export a tool module using `module.exports`. This allows the tool to be imported and used in other parts of the application, such as `handleTools.js`. The exported module is typically the class definition of the tool.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/tools_and_plugins.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n// Export\nmodule.exports = StableDiffusionAPI;\n```\n\n----------------------------------------\n\nTITLE: Render CredsGenerator component\nDESCRIPTION: This snippet renders the CredsGenerator component. It's assumed to be part of a React component's render method or a similar context where JSX elements can be returned.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/toolkit/creds_generator.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<CredsGenerator />\n```\n\n----------------------------------------\n\nTITLE: Render Pricing Component (JSX)\nDESCRIPTION: This JSX snippet renders the `Pricing` component and passes the `isPricingPage` prop set to `true`. This prop likely modifies the behavior or appearance of the `Pricing` component to suit the pricing page context.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/pricing.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<Pricing isPricingPage />\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter for Author Profile\nDESCRIPTION: This YAML block defines the data for Danny Avila's author profile, including his name, title, bio, social media links, and an image.  It is used to populate the `AuthorProfile` component.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/authors/danny.mdx#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n---\ntitle: Danny\nauthorid: danny\nsubtitle: Founder and Maintainer of LibreChat\nname: 'Danny Avila'\nbio: Danny Avila is a software engineer passionate about AI and web development. His expertise in automating processes and data analysis has been applied across many roles, from operations to accounting. Danny's work demonstrates a commitment to meaningful technology and community-driven innovation.\nogImage: '/images/people/danny.webp'\nsocials:\n  GitHub: https://github.com/danny-avila\n  LinkedIn: https://www.linkedin.com/in/danny-avila/\n  X: https://x.com/lgtm_hbu\n  email: mailto:danny@librechat.ai\n---\n```\n\n----------------------------------------\n\nTITLE: Restart Docker Environment Without Mongo Express (Shell)\nDESCRIPTION: This command is used to restart the Docker containers defined in the docker-compose.yml file, excluding the Mongo Express container that was removed in the previous steps. The `-d` flag runs the containers in detached mode (in the background).\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-11-30_mongoexpress.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Cloning LibreChat Repository\nDESCRIPTION: Clones the specified branch of the LibreChat repository from GitHub to the local machine. It requires the branch name and the GitHub username to be replaced with appropriate values. This command is used to get the latest changes from a forked repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone -b branch-name https://github.com/username/LibreChat.git\n```\n\n----------------------------------------\n\nTITLE: Configuring 'addParams' in YAML\nDESCRIPTION: This snippet demonstrates how to add additional parameters to requests using the `addParams` field. This is useful for specifying API-specific options.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx#_snippet_15\n\nLANGUAGE: YAML\nCODE:\n```\naddParams:\n  safe_prompt: true\n  max_tokens: 2048\n```\n\n----------------------------------------\n\nTITLE: Excluded IDs Configuration YAML\nDESCRIPTION: This YAML snippet demonstrates how to specify a list of excluded assistant IDs using the `excludedIds` option. Assistants with these IDs will be blocked.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nexcludedIds:\n  - \"asst_excludedAssistantId1\"\n  - \"asst_excludedAssistantId2\"\n```\n\n----------------------------------------\n\nTITLE: Create a New User in Ubuntu\nDESCRIPTION: This command adds a new user to the Ubuntu system.  The user will be prompted to create a password and provide additional user information.  Replace <yourusername> with the desired username.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/digitalocean.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nadduser <yourusername>\n```\n\n----------------------------------------\n\nTITLE: Starting MongoDB Server (mongod) with config file\nDESCRIPTION: This command starts the MongoDB server using a configuration file. The `--config` option specifies the path to the configuration file, which contains settings such as the port and bind IP.  Replace `/path/to/mongodb/config/mongodb.conf` with the actual path to your configuration file.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/mongodb/mongodb_community.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n./mongod --config /path/to/mongodb/config/mongodb.conf\n```\n\n----------------------------------------\n\nTITLE: Disable LibreChat compression via .env\nDESCRIPTION: Disables static file compression in LibreChat by setting the DISABLE_COMPRESSION environment variable to true in the .env file. This allows Nginx to handle compression instead.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/nginx.mdx#_snippet_15\n\nLANGUAGE: dotenv\nCODE:\n```\n# .env file\nDISABLE_COMPRESSION=true\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Hebrew)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Hebrew language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_8\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'he'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"HE Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Fetching Translation Percentage with Locize Badge (Spanish)\nDESCRIPTION: This snippet generates a badge using shields.io and Locize to display the translation percentage for the Spanish language. The badge dynamically fetches the translation progress from the Locize API.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/translation/index.mdx#_snippet_3\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'es'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated\" alt=\"ES Badge\" />\n```\n\n----------------------------------------\n\nTITLE: Build Frontend\nDESCRIPTION: Builds the frontend of the LibreChat application. This compiles the frontend code into static assets that can be served by the backend.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/local/npm.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm run frontend\n```\n\n----------------------------------------\n\nTITLE: Destroy Local Image\nDESCRIPTION: This command removes the local LibreChat image using podman. The `-f` flag forces the removal, even if the image is in use.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npodman rmi -f librechat:local\n```\n\n----------------------------------------\n\nTITLE: Remove LibreChat Container\nDESCRIPTION: This command removes the existing LibreChat container using podman. The `-f` flag forces the removal, even if the container is running.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\npodman rm -f librechat\n```\n\n----------------------------------------\n\nTITLE: Configure Git User Email (Bash)\nDESCRIPTION: Sets the global Git user email address.  Required for making commits to a Git repository.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/remote/docker_linux.mdx#_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global user.email \"you@example.com\"\n```\n\n----------------------------------------\n\nTITLE: Disable Parameter Configuration\nDESCRIPTION: Illustrates how to hide parameter configuration options in the LibreChat interface. Setting `parameters` to `false` removes the parameter adjustment options from the user interface.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/configuration/librechat_yaml/object_structure/interface.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ninterface:\n  parameters: false\n```\n\n----------------------------------------\n\nTITLE: Enable linger for the current user (bash)\nDESCRIPTION: Enables lingering for the current user using loginctl. This allows user-based services to continue running after the user logs out, necessary for rootless containers.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/blog/2023-08-18_podman.mdx#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nloginctl enable-linger $(whoami)\n```\n\n----------------------------------------\n\nTITLE: Callout with Success Type\nDESCRIPTION: This snippet demonstrates a Callout component with the 'success' type. It includes a title, collapsible functionality, and placeholder text, commonly used to indicate successful operations.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_8\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"success\" title=\"success\" collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. \n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Callout with Unsupported Type\nDESCRIPTION: This snippet demonstrates a Callout component with an 'other' type, representing an undefined or unsupported state. It includes a title, a skull emoji, collapsible functionality, and placeholder text. This example is useful for showcasing how the component handles unexpected type inputs.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/documentation/examples.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<Callout type=\"other\" title=\"undefined or unsupported type\" emoji=''collapsible>\n  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. \n  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet.\n  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.\n\nDolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.\nVestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.\nInteger euismod mi luctus proin habitant interdum proin.\n\nLitora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.\nNetus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.\nMetus imperdiet fusce id rhoncus urna ridiculus sem.\n\n  </Callout>\n```\n\n----------------------------------------\n\nTITLE: Navigating to LibreChat Folder\nDESCRIPTION: Changes the current directory to the LibreChat folder. This is necessary to execute commands within the project's root directory. It assumes the directory exists in the current location.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/docs/development/get_started.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd LibreChat\n```\n\n----------------------------------------\n\nTITLE: Render UnsubscribeForm Component (JSX)\nDESCRIPTION: This JSX snippet renders the UnsubscribeForm component within a centered div. The UnsubscribeForm component is imported from '@/components/Newsletter/UnsubscribeForm'. The outer div provides basic layout and styling for the form.\nSOURCE: https://github.com/librechat-ai/librechat.ai/blob/main/pages/unsubscribe.mdx#_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<div className=\"text-center py-20\">\n\nimport UnsubscribeForm from '@/components/Newsletter/UnsubscribeForm'\n\n<UnsubscribeForm />\n\n</div>\n```"
  }
]
[
  {
    "owner": "langchain-ai",
    "repo": "langmem",
    "content": "TITLE: Storing and Searching Preferences with Agent Memory Tools in Python\nDESCRIPTION: This snippet illustrates storing a user preference and querying it back using the configured agent memory tools. Inputs include user messages to store and search data, and a config containing the user ID. Both operations run through the same user-specific namespace, ensuring isolated memory for each user.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/memory_tools.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Use the agent\nconfig = {\"configurable\": {\"user_id\": \"user-1\"}}\n\n# Store a preference\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember I prefer dark mode\"}]},\n    config=config,\n)\n\n# Search preferences\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What are my preferences?\"}]},\n    config=config,\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Semantic Memories using LangMem Collections (Python)\nDESCRIPTION: This snippet demonstrates how to use LangMem to extract and store semantic memories (facts and knowledge) from a conversation using the 'Collection' pattern. It initializes a memory manager with instructions and enables inserts, then processes a conversation to generate a list of `ExtractedMemory` objects, simulating adding new facts to a collection store. This approach is suitable for accumulating potentially unbounded knowledge.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/concepts/conceptual_guide.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_memory_manager\n\n# highlight-next-line\nmanager = create_memory_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    instructions=\"Extract all noteworthy facts, events, and relationships. Indicate their importance.\",\n    # highlight-next-line\n    enable_inserts=True,\n)\n\n# Process a conversation to extract semantic memories\nconversation = [\n    {\"role\": \"user\", \"content\": \"I work at Acme Corp in the ML team\"},\n    {\"role\": \"assistant\", \"content\": \"I'll remember that. What kind of ML work do you do?\"},\n    {\"role\": \"user\", \"content\": \"Mostly NLP and large language models\"}\n]\n\nmemories = manager.invoke({\"messages\": conversation})\n# Example memories:\n# [\n#     ExtractedMemory(\n#         id=\"27e96a9d-8e53-4031-865e-5ec50c1f7ad5\",\n#         content=Memory(\n#             content=\"[IMPORTANT] User prefers to be called Lex (short for Alex) and appreciates\"\n#             \" casual, witty communication style with relevant emojis.\"\n#         ),\n#     ),\n#     ExtractedMemory(\n#         id=\"e2f6b646-cdf1-4be1-bb40-0fd91d25d00f\",\n#         content=Memory(\n#             content=\"[BACKGROUND] Lex is proficient in Python programming and specializes in developing\"\n#             \" AI systems with a focus on making them sound more natural and less corporate.\"\n#         ),\n#     ),\n#     ExtractedMemory(\n#         id=\"c1e03ebb-a393-4e8d-8eb7-b928d8bed510\",\n#         content=Memory(\n#             content=\"[HOBBY] Lex is a competitive speedcuber (someone who solves Rubik's cubes competitively),\"\n#             \" showing an interest in both technical and recreational puzzle-solving.\"\n#         ),\n#     ),\n#     ExtractedMemory(\n#         id=\"ee7fc6e4-0118-425f-8704-6b3145881ff7\",\n#         content=Memory(\n#             content=\"[PERSONALITY] Based on communication style and interests, Lex appears to value authenticity,\"\n#             \" creativity, and technical excellence while maintaining a fun, approachable demeanor.\"\n#         ),\n#     ),\n# ]\n```\n\n----------------------------------------\n\nTITLE: Interacting with the LangMem Agent in Python\nDESCRIPTION: Demonstrates how to use the previously created LangGraph agent. It shows invoking the agent with user messages and a configuration dictionary specifying the `thread_id`. The example illustrates how the agent initially lacks knowledge, then stores a user preference (dark mode) via the `manage_memory` tool when prompted, and finally retrieves this stored preference in a different conversation thread (`thread-b`), showcasing memory persistence and retrieval across threads.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/hot_path_quickstart.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"configurable\": {\"thread_id\": \"thread-a\"}}\n\n# Use the agent. The agent hasn't saved any memories,\n# so it doesn't know about us\nresponse = agent.invoke(\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Know which display mode I prefer?\"}\n        ]\n    },\n    config=config,\n)\nprint(response[\"messages\"][-1].content)\n# Output: \"I don't seem to have any stored memories about your display mode preferences...\"\n\nagent.invoke(\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"dark. Remember that.\"}\n        ]\n    },\n    # We will continue the conversation (thread-a) by using the config with\n    # the same thread_id\n    config=config,\n)\n\n# New thread = new conversation!\n# highlight-next-line\nnew_config = {\"configurable\": {\"thread_id\": \"thread-b\"}}\n# The agent will only be able to recall\n# whatever it explicitly saved using the manage_memories tool\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hey there. Do you remember me? What are my preferences?\"}]},\n    # highlight-next-line\n    config=new_config,\n)\nprint(response[\"messages\"][-1].content)\n# Output: \"Based on my memory search, I can see that you've previously indicated a preference for dark display mode...\"\n```\n\n----------------------------------------\n\nTITLE: Managing User Preferences using LangMem Profiles (Python)\nDESCRIPTION: This snippet illustrates using LangMem to manage structured, task-specific information like user preferences using the 'Profile' pattern. It defines a Pydantic model (`UserProfile`) for the profile schema, initializes a memory manager with the schema and `enable_inserts=False`, and processes a conversation. The manager extracts information according to the schema, intended to update a single profile document rather than creating new memories.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/concepts/conceptual_guide.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_memory_manager\nfrom pydantic import BaseModel\n\n\nclass UserProfile(BaseModel):\n    \"\"\"Save the user's preferences.\"\"\"\n    name: str\n    preferred_name: str\n    response_style_preference: str\n    special_skills: list[str]\n    other_preferences: list[str]\n\n\nmanager = create_memory_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    schemas=[UserProfile],\n    instructions=\"Extract user preferences and settings\",\n    enable_inserts=False,\n)\n\n# Extract user preferences from a conversation\nconversation = [\n    {\"role\": \"user\", \"content\": \"Hi! I'm Alex but please call me Lex. I'm a wizard at Python and love making AI systems that don't sound like boring corporate robots ðŸ¤–\"},\n    {\"role\": \"assistant\", \"content\": \"Nice to meet you, Lex! Love the anti-corporate-robot stance. How would you like me to communicate with you?\"},\n    {\"role\": \"user\", \"content\": \"Keep it casual and witty - and maybe throw in some relevant emojis when it feels right âœ¨ Also, besides AI, I do competitive speedcubing!\"},\n]\n\nprofile = manager.invoke({\"messages\": conversation})[0]\nprint(profile)\n# Example profile:\n# ExtractedMemory(\n#     id=\"6f555d97-387e-4af6-a23f-a66b4e809b0e\",\n#     content=UserProfile(\n#         name=\"Alex\",\n#         preferred_name=\"Lex\",\n#         response_style_preference=\"casual and witty with appropriate emojis\",\n#         special_skills=[\n#             \"Python programming\",\n#             \"AI development\",\n#             \"competitive speedcubing\",\n#         ],\n#         other_preferences=[\n#             \"prefers informal communication\",\n#             \"dislikes corporate-style interactions\",\n#         ],\n#     ),\n# )\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Manager Agent with LangGraph (Python)\nDESCRIPTION: This snippet demonstrates setting up a memory management agent using LangGraph. It initializes an in-memory store with embedding configurations, sets up chat models, defines a prompt to provide context to the agent, creates a ReAct agent with `create_manage_memory_tool` and `create_search_memory_tool`, and integrates the agent into a simple LangGraph application to process messages and manage memories in the background.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\n\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\n# Set up store and checkpointer\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)\nmy_llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n\ndef prompt(state):\n    \"\"\"Prepare messages with context from existing memories.\"\"\"\n    memories = store.search(\n        (\"memories\",),\n        query=state[\"messages\"][-1].content,\n    )\n    system_msg = f\"\"\"You are a memory manager. Extract and manage all important knowledge, rules, and events using the provided tools.\n    \n\nExisting memories:\n<memories>\n{memories}\n</memories>\n\nUse the manage_memory tool to update and contextualize existing memories, create new ones, or delete old ones that are no longer valid.\nYou can also expand your search of existing memories to augment using the search tool.\"\"\"\n    return [{\"role\": \"system\", \"content\": system_msg}, *state[\"messages\"]]\n\n\n# Create the memory extraction agent\nmanager = create_react_agent(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    prompt=prompt,\n    tools=[\n        # Agent can create/update/delete memories\n        create_manage_memory_tool(namespace=(\"memories\",)),\n        create_search_memory_tool(namespace=(\"memories\",)),\n    ],\n)\n\n\n# Run extraction in background\n@entrypoint(store=store)\ndef app(messages: list):\n    response = my_llm.invoke(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\",\n            },\n            *messages,\n        ]\n    )\n\n    # Extract and store triples (Uses store from @entrypoint context)\n    manager.invoke({\"messages\": messages})\n    return response\n\n\napp.invoke(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": \"Alice manages the ML team and mentors Bob, who is also on the team.\",\n        }\n    ]\n)\n\nprint(store.search((\"memories\",)))\n```\n\n----------------------------------------\n\nTITLE: Creating a LangGraph Agent with Memory Tools in Python\nDESCRIPTION: This Python snippet illustrates how to create a LangGraph agent equipped with long-term memory management capabilities using LangMem tools. It imports relevant modules, configures an in-memory vector store (`InMemoryStore`) with embedding dimensions and model, and initializes an agent with memory management and search tools, both scoped under the \"memories\" namespace. This setup enables the agent to store, retrieve, and adapt based on conversational memories during interactions. The in-memory store is ephemeral; for persistent storage, a database-backed store is recommended.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import core components (1)\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\n# Set up storage (2)\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n) \n\n# Create an agent with memory capabilities (3)\nagent = create_react_agent(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    tools=[\n        # Memory tools use LangGraph's BaseStore for persistence (4)\n        create_manage_memory_tool(namespace=(\"memories\",)),\n        create_search_memory_tool(namespace=(\"memories\",)),\n    ],\n    store=store,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Memory Tools to the Agent\nDESCRIPTION: Enhances the agent with memory capabilities by initializing a vector store with embedding capabilities and adding memory management and search tools.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\"\n    }\n)\n\nnamespace = (\"agent_memories\",)\nmemory_tools = [\n    create_manage_memory_tool(namespace),\n    create_search_memory_tool(namespace)\n]\ncheckpointer = InMemorySaver()\nagent = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", tools=memory_tools, store=store, checkpointer=checkpointer)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent with Configured Memory Tools in Python\nDESCRIPTION: Shows how to instantiate an agent with integrated manage and search memory tools, both configured to share a runtime namespace that includes a user ID placeholder. Dependencies include LangGraph, LangMem, and the previously initialized store. The memory tools leverage the namespace to scope memory operations for individual users. Both tools are accessible in the agent, supporting consistent user-specific data management.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/memory_tools.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Create agent with memory tools\nagent = create_react_agent(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    tools=[\n        # Configure memory tools with runtime namespace (1)\n        create_manage_memory_tool(namespace=(\"memories\", \"{user_id}\")),\n        create_search_memory_tool(namespace=(\"memories\", \"{user_id}\")),\n    ],\n    store=store,\n)\n```\n\n----------------------------------------\n\nTITLE: Interacting with a LangGraph Agent to Store and Retrieve Memories in Python\nDESCRIPTION: This snippet demonstrates how to use the created LangGraph agent to store a new memory and then retrieve information from it via conversational prompts. The agent automatically determines relevant information to save and searches stored memories when queried. The inputs are JSON-like message structures representing user messages. Outputs include stored memories retrievable through natural language queries maintaining conversational context.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Store a new memory (1)\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember that I prefer dark mode.\"}]}\n)\n\n# Retrieve the stored memory (2)\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What are my lighting preferences?\"}]}\n)\nprint(response[\"messages\"][-1].content)\n# Output: \"You've told me that you prefer dark mode.\"\n```\n\n----------------------------------------\n\nTITLE: Updating Stored Memories\nDESCRIPTION: Demonstrates how the agent can update existing memories with new information, in this case modifying a training plan to accommodate an injury.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nchat(agent, \"That may be tricky. I just sprained my ankle. Could you update my plan to include more cross training? Be sure to update the existing key of our plan\", thread_2)\n```\n\n----------------------------------------\n\nTITLE: Extracting Triples from First Conversation using Manager - Python\nDESCRIPTION: This snippet demonstrates invoking the memory manager to extract semantic triples from a user's message in a conversation. The manager processes the input message, outputs a list of ExtractedMemory objects containing the extracted facts as per the Triple schema, and prints their representations. Requires prior configuration of the memory manager and Triple schema as shown previously. Inputs are lists of chat messages; outputs are structured memory objects.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# First conversation - extract triples\nconversation1 = [\n    {\"role\": \"user\", \"content\": \"Alice manages the ML team and mentors Bob, who is also on the team.\"}\n]\nmemories = manager.invoke({\"messages\": conversation1})\nprint(\"After first conversation:\")\nfor m in memories:\n    print(m)\n# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))\n# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))\n# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))\n```\n\n----------------------------------------\n\nTITLE: Integrating Summarization in a ReAct Agent with Tool Calling in Python\nDESCRIPTION: This code shows how to implement context summarization in a ReAct-style agent that uses tools. The example places the summarization node before the LLM node and after tool execution to ensure the context remains manageable across multiple conversation turns and tool calls.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/summarization.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import AnyMessage\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph, START, END, MessagesState\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langmem.short_term import SummarizationNode, RunningSummary\n\nclass State(MessagesState):\n    context: dict[str, Any]\n\ndef search(query: str):\n    \"\"\"Search the web for realtim information like weather forecasts.\"\"\"\n    return \"The weather is sunny in New York, with a high of 104 degrees.\"\n\ntools = [search]\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nsummarization_model = model.bind(max_tokens=128)\n\nsummarization_node = SummarizationNode(\n    token_counter=model.get_num_tokens_from_messages,\n    model=summarization_model,\n    max_tokens=256,\n    max_summary_tokens=128,\n)\n\nclass LLMInputState(TypedDict):\n    summarized_messages: list[AnyMessage]\n    context: dict[str, Any]\n\ndef call_model(state: LLMInputState):\n    response = model.bind_tools(tools).invoke(state[\"summarized_messages\"])\n    return {\"messages\": [response]}\n\n# Define a router that determines whether to execute tools or exit\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return END\n    else:\n        return \"tools\"\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(State)\n# highlight-next-line\nbuilder.add_node(\"summarize_node\", summarization_node)\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"tools\", ToolNode(tools))\nbuilder.set_entry_point(\"summarize_node\")\nbuilder.add_edge(\"summarize_node\", \"call_model\")\nbuilder.add_conditional_edges(\"call_model\", should_continue, path_map=[\"tools\", END])\n# instead of returning to LLM after executing tools, we first return to the summarization node\n# highlight-next-line\nbuilder.add_edge(\"tools\", \"summarize_node\")\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, i am bob\"}, config)\ngraph.invoke({\"messages\": \"what's the weather in nyc this weekend\"}, config)\ngraph.invoke({\"messages\": \"what's new on broadway?\"}, config)\n```\n\n----------------------------------------\n\nTITLE: Defining Episode Schema and Initializing Manager Without Storage - LangMem Python\nDESCRIPTION: Defines a Pydantic BaseModel, `Episode`, to structure extracted episodic memories, capturing observation, thoughts, action, and result. Initializes a LangMem memory manager using `create_memory_manager`, specifying the language model (`anthropic:claude-3-5-sonnet-latest`), the custom `Episode` schema, and instructions for extraction without persistent storage within the manager itself. Enables insertion functionality.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_episodic_memories.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_memory_manager\nfrom pydantic import BaseModel, Field\n\n\nclass Episode(BaseModel):  # (1)!\n    \"\"\"Write the episode from the perspective of the agent within it. Use the benefit of hindsight to record the memory, saving the agent's key internal thought process so it can learn over time.\"\"\"\n\n    observation: str = Field(..., description=\"The context and setup - what happened\")\n    thoughts: str = Field(\n        ...,\n        description=\"Internal reasoning process and observations of the agent in the episode that let it arrive\"\n        ' at the correct action and result. \"I ...\"',\n    )\n    action: str = Field(\n        ...,\n        description=\"What was done, how, and in what format. (Include whatever is salient to the success of the action). I ..\",\n    )\n    result: str = Field(\n        ...,\n        description=\"Outcome and retrospective. What did you do well? What could you do better next time? I ...\",\n    )\n\nmanager = create_memory_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    schemas=[Episode],  # (2)!\n    instructions=\"Extract examples of successful explanations, capturing the full chain of reasoning. Be concise in your explanations and precise in the logic of your reasoning.\",\n    enable_inserts=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Agent with Memory Tools using OpenAI API\nDESCRIPTION: Complete implementation of a custom agent that uses LangMem's memory tools with OpenAI's GPT model. Includes tool execution, agent loop, and memory store configuration.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/use_tools_in_custom_agent.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List\n\nfrom langgraph.store.memory import InMemoryStore\nfrom openai import OpenAI\n\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\n\ndef execute_tool(tools_by_name: Dict[str, Any], tool_call: Dict[str, Any]) -> str:\n    \"\"\"Execute a tool call and return the result\"\"\"\n    tool_name = tool_call[\"function\"][\"name\"]\n\n    if tool_name not in tools_by_name:\n        return f\"Error: Tool {tool_name} not found\"\n\n    tool = tools_by_name[tool_name]\n    try:\n        result = tool.invoke(tool_call[\"function\"][\"arguments\"])\n        return str(result)\n    except Exception as e:\n        return f\"Error executing {tool_name}: {str(e)}\"\n\n\ndef run_agent(tools: List[Any], user_input: str, max_steps: int = 5) -> str:\n    \"\"\"Run a simple agent loop that can use tools\"\"\"\n    # Setup\n    client = OpenAI()\n    tools_by_name = {tool.name: tool for tool in tools}\n\n    # Convert tools to OpenAI's format\n    openai_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.tool_call_schema.model_json_schema(),\n            },\n        }\n        for tool in tools\n    ]\n\n    messages = [{\"role\": \"user\", \"content\": user_input}]\n\n    # REACT loop\n    for step in range(max_steps):\n        # Get next action\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=messages,\n            tools=openai_tools if step < max_steps - 1 else [],\n            tool_choice=\"auto\",\n        )\n        message = response.choices[0].message\n        tool_calls = message.tool_calls\n\n        if not tool_calls:\n            # No more tools to call, return the final response\n            return message.content\n\n        messages.append(\n            {\"role\": \"assistant\", \"content\": message.content, \"tool_calls\": tool_calls}\n        )\n\n        for tool_call in tool_calls:\n            tool_result = execute_tool(tools_by_name, tool_call.model_dump())\n            messages.append(\n                {\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call.id,\n                    \"content\": tool_result,\n                }\n            )\n\n    return \"Reached maximum number of steps\"\n\n\n# Set up memory store and tools\nstore = InMemoryStore(\n            index={\n                \"dims\": 1536,\n                \"embed\": \"openai:text-embedding-3-small\",\n            }\n        )\nmemory_tools = [\n    create_manage_memory_tool(namespace=\"memories\", store=store),\n    create_search_memory_tool(namespace=\"memories\", store=store),\n]\n\n# Run the agent\nresult = run_agent(\n    tools=memory_tools,\n    user_input=\"Remember that I like cherry pie. Then remember that I dislike rocky road.\",\n)\nprint(result)\n# I've remembered that you like cherry pie and that you dislike rocky road...\nprint(store.search((\"memories\",)))\n\n# [\n#     Item(\n#         namespace=[\"memories\"],\n#         key=\"6d3d82d9-724c-47af-aa2f-1d1e917f8bc2\",\n#         value={\"content\": '{\"action\":\"create\",\"content\":\"likes cherry pie\"}'},\n#         created_at=\"2025-02-07T23:49:46.056925+00:00\",\n#         updated_at=\"2025-02-07T23:49:46.056928+00:00\",\n#         score=None,\n#     ),\n#     Item(\n#         namespace=[\"memories\"],\n#         key=\"cf40797f-b00a-41eb-ab96-e6aeadb468e3\",\n#         value={\"content\": '{\"action\":\"create\",\"content\":\"dislikes rocky road\"}'},\n#         created_at=\"2025-02-07T23:49:47.353574+00:00\",\n#         updated_at=\"2025-02-07T23:49:47.353579+00:00\",\n#         score=None,\n#     ),\n# ]\n```\n\n----------------------------------------\n\nTITLE: Building and Running a Crew of Knowledge Agents with Shared Memory Tools in Python\nDESCRIPTION: This complete example creates a shared in-memory vector store to maintain conversational knowledge and defines two CrewAI agents: a \"Knowledge Learner\" tasked with storing new information, and a \"Knowledge Teacher\" tasked with using stored memories to respond to questions. Both agents use LangMem's memory management and search tools. Tasks are created representing learning and teaching actions executed sequentially within Crews that run these tasks. Finally, it demonstrates searching the shared storeâ€™s contents. Dependencies include crewai, langmem, and langgraph.store.memory, with an emphasis on vector embeddings and function-calling LLM configurations. This pattern supports collaborative AI workflows that build and leverage persistent shared knowledge.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/use_tools_in_crewai.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crewai import Agent, Crew, Task\nfrom langgraph.store.memory import InMemoryStore\n\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\n# Set up shared store\nstore = InMemoryStore(\n            index={\n                \"dims\": 1536,\n                \"embed\": \"openai:text-embedding-3-small\",\n            }\n        )\n\n# Create base tools\nbase_tools = [\n    create_manage_memory_tool(namespace=\"memories\", store=store),\n    create_search_memory_tool(namespace=\"memories\", store=store),\n]\n\n# Create agents\nlearner = Agent(\n    role=\"Knowledge Learner\",\n    goal=\"Learn and store new information in the knowledge base\",\n    backstory=\"Your name is Hannah and you are a little too into Demon Slayers.\",\n    tools=base_tools,\n    function_calling_llm=\"gpt-4o-mini\",\n)\n\nteacher = Agent(\n    role=\"Knowledge Teacher\",\n    goal=\"Use stored knowledge to answer questions\",\n    tools=base_tools,\n    backstory=\"You were born on the Nile in the midst of the great pestilence..\",\n    function_calling_llm=\"gpt-4o-mini\",\n)\n\n# Create tasks\nlearn_task = Task(\n    description=\"Save some of your favorite Demon Slayer quotes in memory.\",\n    agent=learner,\n    expected_output=\"Response:\",\n)\n\n\n# Create and run crew\ncrew = Crew(agents=[learner, teacher], tasks=[learn_task])\nresult = crew.kickoff()\n\nteach_task = Task(\n    description=\"Search your memories for information about your teammates.\",\n    agent=teacher,\n    expected_output=\"Response:\",\n)\ncrew = Crew(agents=[learner, teacher], tasks=[teach_task])\nresult = crew.kickoff()\nprint(store.search((\"memories\",)))\n# Output:\n# [\n#     Item(\n#         namespace=[\"memories\"],\n#         key=\"bd61f87e-d591-45b7-b950-3f9318604ea3\",\n#         value={\n#             \"content\": '\"The bond between Nezuko and I can never be severed. I will always protect her.\" - Tanjiro Kamado\\n\"You have to find your own path, you have to find your own way to live!\" - Kanao Tsuyuri\\n\"Itâ€™s not the face that makes someone a monster; itâ€™s the choices they make with their lives.\" - Giyu Tomioka\\n\"Give me strength! I want to be strong enough to face my own failures!\" - Zenitsu Agatsuma\\n\"Never give up! Never stop fighting until your last breath!\" - Giyu Tomioka'\n#         },\n#         created_at=\"2025-02-07T22:24:37.736962+00:00\",\n#         updated_at=\"2025-02-07T22:24:37.736969+00:00\",\n#         score=None,\n#     )\n# ]\n```\n\n----------------------------------------\n\nTITLE: Invoking LangGraph App and Searching Stored Episode - LangMem Python\nDESCRIPTION: Invokes the defined LangGraph `app` with a sample conversation. This execution triggers the app's logic: searching for similar stored episodes, using them in the system message for the LLM, generating a response, and then invoking the memory manager to extract and store the current interaction as a new episode in the `InMemoryStore`. Finally, it demonstrates searching the store to confirm that the new episode related to 'Trees' has been successfully stored.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_episodic_memories.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp.invoke(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": \"What's a binary tree? I work with family trees if that helps\",\n        },\n    ],\n)\nprint(store.search((\"memories\", \"episodes\"), query=\"Trees\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Eager Memory Retrieval\nDESCRIPTION: Creates an advanced setup with a custom prompt function that proactively retrieves relevant memories before the agent processes a query, improving efficiency.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\nfrom langgraph.config import get_store\n\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\"\n    }\n)\n\nnamespace = (\"agent_memories\",)\nmemory_tools = [\n    create_manage_memory_tool(namespace),\n    create_search_memory_tool(namespace)\n]\ncheckpointer = InMemorySaver()\n\ndef prompt(state):\n    # Search over memories based on the messages\n    store = get_store()\n    items = store.search(namespace, query=state[\"messages\"][-1].content)\n    memories = \"\\n\\n\".join(str(item) for item in items)\n    system_msg = {\"role\": \"system\", \"content\": f\"## Memories:\\n\\n{memories}\"}\n    return [system_msg] + state[\"messages\"]\n    \nagent = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", prompt=prompt, tools=memory_tools, store=store, checkpointer=checkpointer)\n```\n\n----------------------------------------\n\nTITLE: Defining a Chat App Endpoint with Memory Extraction and Storage - Python\nDESCRIPTION: This snippet defines an application endpoint using the entrypoint decorator to provide chat context and automatically manage store access. It combines response generation via a chat LLM and semantic triple extraction/storage. Inputs include user messages; the output is a chat response, with extracted triples committed to the configured store. Dependencies are langgraph.func.entrypoint, langmem, and a properly configured store and manager.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define app with store context\n@entrypoint(store=store) # (1)!\ndef app(messages: list):\n    response = my_llm.invoke(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\",\n            },\n            *messages\n        ]\n    )\n\n    # Extract and store triples (Uses store from @entrypoint context)\n    manager.invoke({\"messages\": messages}) \n    return response\n```\n\n----------------------------------------\n\nTITLE: Configuring Langmem Tools for Org and User-Specific Namespaces in Python\nDESCRIPTION: Illustrates various patterns for configuring dynamic namespaces in Langmem tools. Examples include organizing memories by organization (`{org_id}`), by user within an organization (`{org_id}`, `{user_id}`), and by feature/category alongside user (`agent_smith`, `memories`, `{user_id}`, `preferences`). Each example shows creating the tool (`create_manage_memory_tool` or `create_search_memory_tool`), integrating it with a LangGraph agent, and invoking the agent with the necessary `configurable` values (e.g., `org_id`, `user_id`) provided in the `config` dictionary.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/dynamically_configure_namespaces.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Organization-level\ntool = create_manage_memory_tool(\n    namespace=(\"memories\", \"{org_id}\")\n)\napp = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", tools=[tool])\napp.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"I'm questioning the new company health plan..\"}]},\n    config={\"configurable\": {\"org_id\": \"acme\"}}\n)\n\n# User within organization\ntool = create_manage_memory_tool(\n    namespace=(\"memories\", \"{org_id}\", \"{user_id}\")\n)\n# If you wanted to, you could let the agent\n# search over all users within an organization\ntool = create_search_memory_tool(\n    namespace=(\"memories\", \"{org_id}\")\n)\napp = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", tools=[tool])\napp.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's our policy on dogs at work?\"}]},\n    config={\"configurable\": {\"org_id\": \"acme\", \"user_id\": \"alice\"}}\n)\n\n# You could also organize memories by type or category if you prefer \ntool = create_manage_memory_tool(\n    namespace=(\"agent_smith\", \"memories\", \"{user_id}\", \"preferences\")\n)\napp = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", tools=[tool])\napp.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"I like dolphins\"}]},\n    config={\"configurable\": {\"user_id\": \"alice\"}}\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating with LangGraph Memory Store\nDESCRIPTION: Demonstrates how to integrate `create_memory_store_manager` with LangGraph's `InMemoryStore` to maintain user profiles across conversations. It sets up a store, defines a manager with a namespace for user profiles, and creates a `chat` function to personalize responses and update profiles.  The namespace isolates profiles by user ID.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/manage_user_profile.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.config import get_config\nfrom langmem import create_memory_store_manager\n\n# Set up store and models (1)\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)\nmy_llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n# Create profile manager (2)\nmanager = create_memory_store_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    namespace=(\"users\", \"{user_id}\", \"profile\"),  # Isolate profiles by user\n    schemas=[UserProfile],\n    enable_inserts=False,  # Update existing profile only\n)\n\n@entrypoint(store=store)\ndef chat(messages: list):\n    # Get user's profile for personalization\n    configurable = get_config()[\"configurable\"]\n    results = store.search(\n        (\"users\", configurable[\"user_id\"], \"profile\")\n    )\n    profile = None\n    if results:\n        profile = f\"\"\"<User Profile>:\n\n{results[0].value}\n</User Profile>\n\"\"\"\n    \n    # Use profile in system message\n    response = my_llm.invoke([\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"You are a helpful assistant.{profile}\"\"\"\n        },\n        *messages\n    ])\n\n    # Update profile with any new information\n    manager.invoke({\"messages\": messages})\n    return response\n\n# Example usage\nawait chat.ainvoke(\n    [{\"role\": \"user\", \"content\": \"I'm Alice from California\"}],\n    config={\"configurable\": {\"user_id\": \"user-123\"}}\n)\n\nawait chat.ainvoke(\n    [{\"role\": \"user\", \"content\": \"I just passed the N1 exam!\"}],\n    config={\"configurable\": {\"user_id\": \"user-123\"}}\n)\n\nprint(store.search((\"users\", \"user-123\", \"profile\")))\n```\n\n----------------------------------------\n\nTITLE: Initializing Episodic Memory Manager\nDESCRIPTION: This snippet initializes a memory manager for extracting episodic memories from conversations. It defines an `Episode` class using `pydantic` to structure the memory, specifying the observation, thoughts, action, and result of each episode. The code then calls `create_memory_manager` to set up the manager with a specified language model, episode schema, and instructions for extracting episodes. The `enable_inserts` parameter is set to `True`, allowing the memory manager to insert new memories.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/concepts/conceptual_guide.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom langmem import create_memory_manager\n\nclass Episode(BaseModel):\n    \"\"\"An episode captures how to handle a specific situation, including the reasoning process\n    and what made it successful.\"\"\"\n    \n    observation: str = Field(\n        ..., \n        description=\"The situation and relevant context\"\n    )\n    thoughts: str = Field(\n        ...,\n        description=\"Key considerations and reasoning process\"\n    )\n    action: str = Field(\n        ...,\n        description=\"What was done in response\"\n    )\n    result: str = Field(\n        ...,\n        description=\"What happened and why it worked\"\n    )\n\n# highlight-next-line\nmanager = create_memory_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    schemas=[Episode],\n    instructions=\"Extract examples of successful interactions. Include the context, thought process, and why the approach worked.\",\n    enable_inserts=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating and Deleting Triples after New Conversation - Python\nDESCRIPTION: Invokes the memory manager a second time with new user input and existing memories to update triples. With deletes enabled, outdated triples may return RemoveDoc objects to indicate removal, and new triples are extracted for updated facts. Inputs are the new conversation and prior extracted memories; outputs are updated lists of ExtractedMemory objects. This approach allows users to define what 'removal' means operationally.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Second conversation - update and add triples\nconversation2 = [\n    {\"role\": \"user\", \"content\": \"Bob now leads the ML team and the NLP project.\"}\n]\nupdate = manager.invoke({\"messages\": conversation2, \"existing\": memories})\nprint(\"After second conversation:\")\nfor m in update:\n    print(m)\n# ExtractedMemory(id='65fd9b68-77a7-4ea7-ae55-66e1dd603046', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))\n# ExtractedMemory(id='7f8be100-5687-4410-b82a-fa1cc8d304c0', content=Triple(subject='Bob', predicate='leads', object='ML_team', context=None))\n# ExtractedMemory(id='f4c09154-2557-4e68-8145-8ccd8afd6798', content=Triple(subject='Bob', predicate='leads', object='NLP_project', context=None))\n# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))\n# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))\n# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))\nexisting = [m for m in update if isinstance(m.content, Triple)]\n```\n\n----------------------------------------\n\nTITLE: Initializing Store, Manager With Storage, and LangGraph App - LangMem Python\nDESCRIPTION: Sets up an `InMemoryStore` from LangGraph to serve as a vector store for similarity search. Initializes a LangMem memory manager using `create_memory_store_manager`, configuring it with the LLM model, a namespace for storage, the `Episode` schema, instructions for extraction, and enabled inserts. Initializes a chat model (`llm`) for generating responses. Defines a LangGraph `@entrypoint` function (`app`) that takes messages and orchestrates the memory search, response generation, and memory storage process.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_episodic_memories.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint\nfrom langgraph.store.memory import InMemoryStore\nfrom langmem import create_memory_store_manager\n\n# Set up vector store for similarity search\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)\n\n# Configure memory manager with storage\nmanager = create_memory_store_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    namespace=(\"memories\", \"episodes\"),\n    schemas=[Episode],\n    instructions=\"Extract exceptional examples of noteworthy problem-solving scenarios, including what made them effective.\",\n    enable_inserts=True,\n)\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n\n@entrypoint(store=store)\ndef app(messages: list):\n    # Step 1: Find similar past episodes\n    similar = store.search(\n        (\"memories\", \"episodes\"),\n        query=messages[-1][\"content\"],\n        limit=1,\n    )\n\n    # Step 2: Build system message with relevant experience\n    system_message = \"You are a helpful assistant.\"\n    if similar:\n        system_message += \"\\n\\n### EPISODIC MEMORY:\"\n        for i, item in enumerate(similar, start=1):\n            episode = item.value[\"content\"]\n            system_message += f\"\"\"\n            \nEpisode {i}:\nWhen: {episode['observation']}\nThought: {episode['thoughts']}\nDid: {episode['action']}\nResult: {episode['result']}\n        \"\"\"\n\n    # Step 3: Generate response using past experience\n    response = llm.invoke([{\"role\": \"system\", \"content\": system_message}, *messages])\n\n    # Step 4: Store this interaction if successful\n    manager.invoke({\"messages\": messages})\n    return response\n```\n\n----------------------------------------\n\nTITLE: Configuring Semantic Triple Extraction with LangMem Memory Manager - Python\nDESCRIPTION: This snippet defines a custom Triple schema using pydantic's BaseModel, then configures LangMem's memory manager to extract and update user information from conversations as triples. Required dependencies are langmem and pydantic. Key parameters include schemas for memory structure, LLM provider string, custom extraction instructions, and toggles for enabling inserts and deletes. Inputs are chat-like messages; outputs are extracted structured triples encapsulated as memory objects. Memory context fields help disambiguate facts for reliable retrieval.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_memory_manager # (1)!\nfrom pydantic import BaseModel\n\nclass Triple(BaseModel): # (2)!\n    \"\"\"Store all new facts, preferences, and relationships as triples.\"\"\"\n    subject: str\n    predicate: str\n    object: str\n    context: str | None = None\n\n# Configure extraction\nmanager = create_memory_manager(  \n    \"anthropic:claude-3-5-sonnet-latest\",\n    schemas=[Triple], \n    instructions=\"Extract user preferences and any other useful information\",\n    enable_inserts=True,\n    enable_deletes=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Compiling a LangGraph Supervisor Workflow (Python)\nDESCRIPTION: Uses `langgraph_supervisor.create_supervisor` to create a workflow that manages the previously defined `email_agent` and `social_media_agent`. It configures the supervisor with a specific Anthropic model and a high-level prompt defining its role. The workflow is then compiled into a runnable application (`app`) linked to the `InMemoryStore`.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph_supervisor import create_supervisor\n\n# Create supervisor workflow\nworkflow = create_supervisor(\n    [email_agent, social_media_agent],\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    prompt=(\n        \"You are a team supervisor managing email and tweet assistants to help with correspondance.\"\n    )\n)\n\n# Compile and run\napp = workflow.compile(store=store)\n```\n\n----------------------------------------\n\nTITLE: Searching LangMem Store (Python)\nDESCRIPTION: Demonstrates how to retrieve items (memories) from the initialized memory store within a specific namespace. This allows verifying the memories extracted by the background memory manager. Requires the `store` object initialized using LangMem's `create_memory_store_manager`.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/background_quickstart.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# (in case our memory manager is still running)\nprint(store.search((\"memories\",)))\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Triple Extraction with LangGraph InMemoryStore - Python\nDESCRIPTION: This snippet sets up an in-memory vector store and memory manager using LangGraph and LangMem, configuring storage of extracted triples in a persistent namespace. Required dependencies are langgraph, langmem, langchain, and pydantic. The code establishes a storage backend, memory manager with schemas and namespace, and initializes a chat LLM model. Inputs include embeddings configuration and schemas; output is a ready-to-use triple extraction pipeline with persistent storage, suitable for development environments.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint\nfrom langgraph.store.memory import InMemoryStore\nfrom langmem import create_memory_store_manager\n\n# Set up store and models\nstore = InMemoryStore(  # (1)!\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)\nmanager = create_memory_store_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    namespace=(\"chat\", \"{user_id}\", \"triples\"),  # (2)!\n    schemas=[Triple],\n    instructions=\"Extract all user information and events as triples.\",\n    enable_inserts=True,\n    enable_deletes=True,\n)\nmy_llm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing Prompts using Gradient Optimizer in Python\nDESCRIPTION: Demonstrates initializing and using the 'gradient' optimizer from `langmem`. This optimizer utilizes multiple LLM calls (configurable via `max/min_reflection_steps`) to iteratively propose and apply improvements to a base prompt based on the provided conversation `trajectories`. The example uses the 'anthropic:claude-3-5-sonnet-latest' model. Requires the `langmem` library.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/optimize_memory_prompt.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptimizer = create_prompt_optimizer(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    kind=\"gradient\",  # 2-10 LLM calls\n    config={\n        \"max_reflection_steps\": 3,  # Max improvement cycles\n        \"min_reflection_steps\": 1   # Min improvement cycles\n    }\n)\nupdated = optimizer.invoke(\n    {\"trajectories\": trajectories, \"prompt\": \"You are a planetary science expert\"}\n)\nprint(updated)\n```\n\n----------------------------------------\n\nTITLE: Extracting Memories for a Specific User with Runtime Namespace Substitution - Python\nDESCRIPTION: Explains how to extract facts for a specific user by providing a user ID in the config parameter, triggering substitution in the namespace. Inputs are messages and a config dictionary with a user_id; output is adding structured triples for the targeted user namespace in the underlying store. Dependencies include an already-configured manager and store.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Extract memories for User A\nmanager.invoke(\n    messages=[{\"role\": \"user\", \"content\": \"I prefer dark mode\"}],\n    config={\"configurable\": {\"user_id\": \"user-a\"}}  # (1)!\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up Multi-Agent System with Separate Instructions (Python)\nDESCRIPTION: Initializes a new `InMemoryStore` for a multi-agent setup. Stores distinct instructions for an 'email_agent' and a 'twitter_agent'. Defines tools (`draft_email`, `tweet`) and corresponding prompt functions (`prompt_email`, `prompt_social_media`) for each agent, retrieving instructions from the store based on agent-specific keys. Creates two separate ReAct agents (`email_agent`, `social_media_agent`) using these components and the shared store.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.config import get_store\n\nstore = InMemoryStore()\n\nstore.put((\"instructions\",), key=\"email_agent\", value={\"prompt\": \"Write good emails. Repeat your draft content to the user after submitting.\"})\nstore.put((\"instructions\",), key=\"twitter_agent\", value={\"prompt\": \"Write fire tweets. Repeat the tweet content to the user upon submission.\"})\n\n## Email agent\ndef draft_email(to: str, subject: str, body: str):\n    \"\"\"Submit an email draft.\"\"\"\n    return \"Draft saved succesfully.\"\n\ndef prompt_email(state):\n    item = store.get((\"instructions\",), key=\"email_agent\")\n    instructions = item.value[\"prompt\"]\n    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\"}\n    return [sys_prompt] + state['messages']\n\nemail_agent = create_react_agent(\n    \"anthropic:claude-3-5-sonnet-latest\", \n    prompt=prompt_email, \n    tools=[draft_email], \n    store=store,\n    name=\"email_assistant\",\n)\n\n## Tweet\n\ndef tweet(to: str, subject: str, body: str):\n    \"\"\"Poast a tweet.\"\"\"\n    return \"Legendary.\"\n\ndef prompt_social_media(state):\n    item = store.get((\"instructions\",), key=\"twitter_agent\")\n    instructions = item.value[\"prompt\"]\n    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\"}\n    return [sys_prompt] + state['messages']\n\nsocial_media_agent = create_react_agent(\n    \"anthropic:claude-3-5-sonnet-latest\", \n    prompt=prompt_social_media, \n    tools=[tweet], \n    store=store,\n    name=\"social_media_agent\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Invoking Multi-Prompt Optimization in Python\nDESCRIPTION: This Python snippet demonstrates initializing the Langmem multi-prompt optimizer (`create_multi_prompt_optimizer`) with a specified model ('anthropic:claude-3-5-sonnet-latest') and configuration (gradient kind, max reflection steps). It defines example conversation trajectories (with and without explicit feedback) and initial prompts for different roles (researcher, writer). Finally, it invokes the optimizer with these trajectories and prompts to generate updated prompt recommendations based on inferred performance gradients.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/optimize_compound_system.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_multi_prompt_optimizer\n\n# Example team: researcher finds information, writer creates reports\nconversations = [\n    (\n        [\n            {\"role\": \"user\", \"content\": \"Research quantum computing advances\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Found several papers on quantum supremacy...\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Recent quantum computing developments show...\",\n            },\n            {\"role\": \"user\", \"content\": \"The report is missing implementation details\"},\n        ],\n        # No explicit feedback provided but the optimizer can infer from the conversation\n        None,\n    ),\n    (\n        [\n            {\"role\": \"user\", \"content\": \"Analyze new ML models\"},\n            {\"role\": \"assistant\", \"content\": \"Key findings on architecture: ...\"},\n            {\"role\": \"assistant\", \"content\": \"Based on the research, these models...\"},\n            {\"role\": \"user\", \"content\": \"Great report, very thorough\"},\n        ],\n        # Numeric score for the team as a whole\n        {\"score\": 0.95},\n    ),\n]\n\n# Define prompts for each role\nprompts = [\n    {\n        \"name\": \"researcher\",\n        \"prompt\": \"You analyze technical papers and extract key findings\",\n    },\n    {\"name\": \"writer\", \"prompt\": \"You write clear reports based on research findings\"},\n]\n\n# Create optimizer\noptimizer = create_multi_prompt_optimizer(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    kind=\"gradient\",  # Best for team dynamics\n    config={\"max_reflection_steps\": 3},\n)\n\n# Update all prompts based on team performance\nupdated = optimizer.invoke({\"trajectories\": conversations, \"prompts\": prompts})\nprint(updated)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Prompts using Metaprompt Optimizer in Python\nDESCRIPTION: Initializes and uses the 'metaprompt' optimizer from the `langmem` library to refine a system prompt. It processes a list of conversation `trajectories`, which include user/assistant interactions and optional feedback or revisions, along with an initial prompt. The example uses the 'anthropic:claude-3-5-sonnet-latest' model and configures the optimizer with specific reflection steps. Requires the `langmem` library.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/optimize_memory_prompt.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_prompt_optimizer\n\n# Multiple conversations showing what to improve\ntrajectories = [\n    # Conversation with no annotations (just the conversation)\n    (\n        [\n            {\"role\": \"user\", \"content\": \"Tell me about Mars\"},\n            {\"role\": \"assistant\", \"content\": \"Mars is the fourth planet...\"},\n            {\"role\": \"user\", \"content\": \"I wanted more about its moons\"},\n        ],\n        None,\n    ),\n    (\n        [\n            {\"role\": \"user\", \"content\": \"What are Mars' moons?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Mars has two moons: Phobos and Deimos...\",\n            },\n        ],\n        {\n            \"score\": 0.9,\n            \"comment\": \"Should include more details and recommended follow-up questions\",\n        },\n    ),\n    # Annotations can be of different types, such as edits/revisions!\n    (\n        [\n            {\"role\": \"user\", \"content\": \"Compare Mars and Earth\"},\n            {\"role\": \"assistant\", \"content\": \"Mars and Earth have many differences...\"},\n        ],\n        {\"revised\": \"Earth and Mars have many similarities and differences...\"},\n    ),\n]\n\noptimizer = create_prompt_optimizer(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    kind=\"metaprompt\",\n    config={\"max_reflection_steps\": 1, \"min_reflection_steps\": 0},\n)\nupdated = optimizer.invoke(\n    {\"trajectories\": trajectories, \"prompt\": \"You are a planetary science expert\"}\n)\nprint(updated)\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Tools for Shared and Personal Namespace Access in Python\nDESCRIPTION: Shows how to prepare separate tool configurations for two agents, each allowed to write to their own sub-namespace while sharing read access to the overarching team namespace. This pattern enables personalized contributions and team-wide searches. It is useful for collaborative settings where both private and shared memories coexist.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/memory_tools.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nagent_a_tools = [\n    # Write to agent-specific namespace\n    create_manage_memory_tool(namespace=(\"memories\", \"team_a\", \"agent_a\")),\n    # Read from shared team namespace\n    create_search_memory_tool(namespace=(\"memories\", \"team_a\"))\n]\n\n# Agents with different prompts sharing read access\nagent_a = create_react_agent(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    tools=agent_a_tools,\n    store=store,\n    prompt=\"You are a research assistant\"\n)\n\n# Create tools for agent B with different write space\nagent_b_tools = [\n    create_manage_memory_tool(namespace=(\"memories\", \"team_a\", \"agent_b\")),\n    create_search_memory_tool(namespace=(\"memories\", \"team_a\"))\n]\nagent_b = create_react_agent(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    tools=agent_b_tools,\n    store=store,\n    prompt=\"You are a report writer.\"\n)\n\nagent_b.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}]})\n```\n\n----------------------------------------\n\nTITLE: Testing Eager Memory Agent with Initial Query\nDESCRIPTION: Tests the eager memory retrieval agent with an initial query about half marathon training, demonstrating proactive memory usage.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nthread_1 = \"thread-1\"\nchat(agent, \"Hi there, I'm training for a half marathon in 2 months - could you propose a daily training plan to prepare?\", thread_1, None)\n```\n\n----------------------------------------\n\nTITLE: Initializing Langmem Tool with Dynamic User Namespace in Python\nDESCRIPTION: Demonstrates creating a `create_manage_memory_tool` with a dynamic namespace tuple `(\"memories\", \"{user_id}\")`. It shows how to integrate this tool with a LangGraph agent (`create_react_agent`) and invoke the agent, passing the actual `user_id` value ('user-123') at runtime via the `config` dictionary's `configurable` field to store memories specifically for that user.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/dynamically_configure_namespaces.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.prebuilt import create_react_agent\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\n# Create tool with {user_id} template\ntool = create_manage_memory_tool(namespace=(\"memories\", \"{user_id}\"))\n# Agent just sees that it has memory. It doesn't know where it's stored.\napp = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", tools=[tool])\n# Use with different users\napp.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"I like dolphins\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user-123\"}}\n)  # Stores in (\"memories\", \"user-123\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Different Namespace Patterns for Memory Organization in Python\nDESCRIPTION: Provides examples of configuring memory tool namespaces for various access patterns: personal, team-shared, and project-specific. This enables flexible memory scoping and sharing strategies, with each namespace representing a different memory organization concept. Key parameter is 'namespace', which controls memory separation by user, team, or project.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/memory_tools.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Personal memories\nnamespace=(\"memories\", \"user-123\")\n\n# Shared team knowledge\nnamespace=(\"memories\", \"team-product\")\n\n# Project-specific memories\nnamespace=(\"memories\", \"project-x\")\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Schema Types for Extraction - Text\nDESCRIPTION: Shows how to configure the memory manager to extract multiple types of structured memories by providing a list of schemas. Each schema (such as Triple, Preference, Relationship) can have unique extraction rules and distinct storage patterns. The input is a Python list of schema classes; the output is a configuration that enables parallel extraction of several memory types at once. No code execution is shown.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nschemas=[Triple, Preference, Relationship]\n```\n\n----------------------------------------\n\nTITLE: Invoking Agent for User-Specific Memory Management in Python\nDESCRIPTION: Demonstrates how to use the configured agent to store and search user memories by invoking it with different user IDs in the namespace. Inputs include a user message and a configuration object specifying the user ID. The agent uses the namespace pattern (\"memories\", \"user-a\") or (\"memories\", \"user-b\") to keep memories isolated by user.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/memory_tools.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Example 1: Store and search User A's memories\nresponse_a = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember my favorite color is blue\"}]},\n    config={\"configurable\": {\"user_id\": \"user-a\"}}\n)  # Both tools use namespace (\"memories\", \"user-a\")\n\n# Example 2: Store and search User B's memories\nresponse_b = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Remember I prefer dark mode\"}]},\n    config={\"configurable\": {\"user_id\": \"user-b\"}}\n)  # Both tools use namespace (\"memories\", \"user-b\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing Agent Prompt with LangMem (Python)\nDESCRIPTION: Retrieves the current agent prompt from the `InMemoryStore`. Defines feedback based on a previous interaction (`result`). Invokes the `optimizer` with the current prompt, the interaction trajectory (messages and feedback), to generate an improved prompt.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncurrent_prompt = store.get((\"instructions\",), key=\"agent_instructions\").value[\"prompt\"]\nfeedback = {\"request\": \"Always sign off from 'William'; for meeting requests, offer to schedule on Zoom or Google Meet\"}\n\noptimizer_result = optimizer.invoke({\"prompt\": current_prompt, \"trajectories\": [(result[\"messages\"], feedback)]})\n```\n\n----------------------------------------\n\nTITLE: Initializing LangMem Memory Tools with CrewAI Agents in Python\nDESCRIPTION: This Python snippet demonstrates setting up an in-memory vector store for LangMem using the InMemoryStore class with specific embedding dimensions and an embedding model. It then creates two memory tools: one for managing memories and another for searching within them, both namespaced as \"memories\". These tools are attached to a CrewAI Agent configured with a role, a goal, and a descriptive backstory. The agent is set to verbose mode for detailed operation logging. Dependencies include crewai, langmem, and langgraph.store.memory modules. The snippet highlights how to create persistent knowledge capabilities for AI agents during conversations.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/use_tools_in_crewai.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crewai import Agent\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\nfrom langgraph.store.memory import InMemoryStore\n\n# Set up memory store\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)  # (1)!\n\n# Create memory tools\nmemory_tools = [\n    create_manage_memory_tool(namespace=\"memories\", store=store),\n    create_search_memory_tool(namespace=\"memories\", store=store),\n]\n\n# Create an agent with memory tools\nknowledge_agent = Agent(\n    role='Knowledge Manager',\n    goal='Build and maintain a knowledge base',\n    backstory=\"\"\"You are a knowledge management expert who excels at\n    organizing and storing important information for future use.\"\"\",\n    tools=memory_tools,\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing In-Memory Store for Memory Tools in Python\nDESCRIPTION: This snippet demonstrates how to create an in-memory store instance configured for use with memory tools in LangGraph and LangMem. The store uses an OpenAI embedding model with dimension 1536. For production, persistent stores like AsyncPostgresStore are recommended. This in-memory store does not persist data across restarts, making it suitable for development only.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/memory_tools.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\n# Set up store and memory saver\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n) # (1)!\n```\n\n----------------------------------------\n\nTITLE: Restarting Thread 1 with Memory-Enabled Agent\nDESCRIPTION: Initiates a new conversation with the memory-enabled agent, allowing it to store information about the half marathon training plan.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nthread_1 = \"thread-1\"\nchat(agent, \"Hi there, I'm training for a half marathon in 2 months - could you propose a daily training plan to prepare?\", thread_1)\n```\n\n----------------------------------------\n\nTITLE: Organizing Memory Namespaces with Tuple Structure in LangMem (Python)\nDESCRIPTION: This Python snippet demonstrates creating a hierarchical namespace for memories by combining organization, runtime-configurable user identifiers, and contextual labels in a tuple. It leverages string template variables (such as \"{user_id}\") that are intended to be filled at runtime from configuration objects, supporting flexible and dynamic segmentation. The expected input is a tuple with static and templated strings, and there are no external dependencies except general Python data structures; actual namespace resolution requires integration with LangMem's configuration utilities (e.g., RunnableConfig). The output is a tuple that forms the namespace for uniquely identifying and organizing memory records.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/concepts/conceptual_guide.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# Organize memories by organization -> configurable user -> context\nnamespace = (\"acme_corp\", \"{user_id}\", \"code_assistant\")\n```\n\n----------------------------------------\n\nTITLE: Creating a LangGraph ReAct Agent with Stored Instructions (Python)\nDESCRIPTION: Defines a tool function `draft_email` and a custom `prompt` function that retrieves agent instructions from the `InMemoryStore`. It then creates a ReAct agent using `langgraph.prebuilt.create_react_agent`, configuring it with an Anthropic model, the custom prompt function, the tool, and the shared memory store.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.config import get_store\n\ndef draft_email(to: str, subject: str, body: str):\n    \"\"\"Submit an email draft.\"\"\"\n    return \"Draft saved succesfully.\"\n\ndef prompt(state):\n    item = store.get((\"instructions\",), key=\"agent_instructions\")\n    instructions = item.value[\"prompt\"]\n    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\"}\n    return [sys_prompt] + state['messages']\n\nagent = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", prompt=prompt, tools=[draft_email], store=store)\n```\n\n----------------------------------------\n\nTITLE: Triggering App with Sequential Conversations and Checking Stored Triples - Python\nDESCRIPTION: Demonstrates sequentially invoking the app for a user across multiple conversational turns, then fetching all stored triples for that user's namespace. Inputs are chat messages and config parameters specifying the user ID; the output upon searching the store is a sequence of structured triple objects per user. Assumes prior app and store setup.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# First conversation\napp.invoke(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": \"Alice manages the ML team and mentors Bob, who is also on the team.\",\n        },\n    ],\n    config={\"configurable\": {\"user_id\": \"user123\"}},\n)\n\n# Second conversation\napp.invoke(\n    [\n        {\"role\": \"user\", \"content\": \"Bob now leads the ML team and the NLP project.\"},\n    ],\n    config={\"configurable\": {\"user_id\": \"user123\"}},\n)\n\n# Third conversation\napp.invoke(\n    [\n        {\"role\": \"user\", \"content\": \"Alice left the company.\"},\n    ],\n    config={\"configurable\": {\"user_id\": \"user123\"}},\n)\n\n# Check stored triples\nfor item in store.search((\"chat\", \"user123\")):\n    print(item.namespace, item.value)\n\n# Output:\n# ('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'is_member_of', 'object': 'ML_team', 'context': None}}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Agent with Memory Tools using Anthropic API\nDESCRIPTION: Complete implementation of a custom agent that uses LangMem's memory tools with Anthropic's Claude model. Includes tool execution, agent loop, and memory store setup.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/use_tools_in_custom_agent.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport anthropic\nfrom typing import List, Dict, Any\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\nfrom langgraph.store.memory import InMemoryStore\n\n\ndef execute_tool(tools_by_name: Dict[str, Any], tool_call: Dict[str, Any]) -> str:\n    \"\"\"Execute a tool call and return the result\"\"\"\n    tool_name = tool_call.name\n\n    if tool_name not in tools_by_name:\n        return f\"Error: Tool {tool_name} not found\"\n\n    tool = tools_by_name[tool_name]\n    try:\n        result = tool.invoke(tool_call.input)\n        return str(result)\n    except Exception as e:\n        return f\"Error executing {tool_name}: {str(e)}\"\n\n\ndef run_agent(tools: List[Any], user_input: str, max_steps: int = 5) -> str:\n    \"\"\"Run a simple agent loop that can use tools\"\"\"\n    # Setup\n    client = anthropic.Anthropic()\n    tools_by_name = {tool.name: tool for tool in tools}\n\n    # Convert tools to Anthropic's format\n    anthropic_tools = [\n        {\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"input_schema\": tool.tool_call_schema.model_json_schema(),\n        }\n        for tool in tools\n    ]\n\n    messages = [{\"role\": \"user\", \"content\": user_input}]\n\n    # REACT loop\n    for step in range(max_steps):\n        # Get next action from Claude\n        tools = anthropic_tools if step < max_steps - 1 else []\n        response = client.messages.create(\n            model=\"claude-3-5-sonnet-latest\",\n            max_tokens=1024,\n            temperature=0.7,\n            tools=tools,\n            messages=messages,\n        )\n        tool_calls = [\n            content for content in response.content if content.type == \"tool_use\"\n        ]\n        if not tool_calls:\n            # No more tools to call, return the final response\n            return \"\".join(\n                [block.text for block in response.content if block.type == \"text\"]\n            )\n        messages.append({\"role\": \"assistant\", \"content\": response.content})\n        for tool_call in tool_calls:\n            tool_result = execute_tool(tools_by_name, tool_call)\n\n            # Add the tool call and result to the conversation\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"tool_result\",\n                            \"tool_use_id\": tool_call.id,\n                            \"content\": tool_result,\n                        }\n                    ],\n                },\n            )\n\n    return \"Reached maximum number of steps\"\n\n\n# Set up memory store and tools\nstore = InMemoryStore(\n            index={\n                \"dims\": 1536,\n                \"embed\": \"openai:text-embedding-3-small\",\n            }\n        )  # (1)!\nmemory_tools = [\n    create_manage_memory_tool(namespace=\"memories\", store=store),\n    create_search_memory_tool(namespace=\"memories\", store=store),\n]\n\n# Run the agent\nresult = run_agent(\n    tools=memory_tools,\n    user_input=\"Remember that I like cherry pie. Then remember that I dislike rocky road.\",\n)\nprint(result)\n# I've created both memories. I'll remember that you like cherry pie and dislike rocky road ice cream...\nprint(store.search((\"memories\",)))\n# [\n#     Item(\n#         namespace=[\"memories\"],\n#         key=\"79d6d323-c6ec-408a-ae75-bda1fcbebd6f\",\n#         value={\"content\": \"User likes cherry pie\"},\n#         created_at=\"2025-02-07T23:26:00.975678+00:00\",\n#         updated_at=\"2025-02-07T23:26:00.975682+00:00\",\n#         score=None,\n#     ),\n#     Item(\n#         namespace=[\"memories\"],\n#         key=\"72705ea8-babf-4ddd-bf0f-7426dd0e4f35\",\n#         value={\"content\": \"User dislikes rocky road\"},\n#         created_at=\"2025-02-07T23:26:02.995210+00:00\",\n#         updated_at=\"2025-02-07T23:26:02.995215+00:00\",\n#         score=None,\n#     ),\n# ]\n```\n\n----------------------------------------\n\nTITLE: Optimizing Multiple Agent Prompts with LangMem (Python)\nDESCRIPTION: Retrieves the current prompts for both the email and tweet agents from the `InMemoryStore`. Formats each prompt into a dictionary containing its name, the prompt text, and criteria for when it should be updated. Invokes the `optimizer` with the list of prompt dictionaries and the trajectory (messages and feedback) from the last supervisor run to generate potentially updated prompts for relevant agents.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import Prompt\n\nemail_prompt = store.get((\"instructions\",), key=\"email_agent\").value['prompt']\ntweet_prompt = store.get((\"instructions\",), key=\"twitter_agent\").value['prompt']\n\nemail_prompt = {\n    \"name\": \"email_prompt\",\n    \"prompt\": email_prompt,\n    \"when_to_update\": \"Only if feedback is provided indicating email writing performance needs improved.\"\n}\ntweet_prompt = {\n    \"name\": \"tweet_prompt\",\n    \"prompt\": tweet_prompt,\n    \"when_to_update\": \"Only if tweet writing generation needs improvement.\"\n}\n\n\noptimizer_result = optimizer.invoke({\"prompts\": [tweet_prompt, email_prompt], \"trajectories\": [(result[\"messages\"], feedback)]})\n```\n\n----------------------------------------\n\nTITLE: Optimizing Prompts using Prompt Memory Optimizer in Python\nDESCRIPTION: Illustrates initializing and using the 'prompt_memory' optimizer from `langmem`. This approach uses a single LLM call with a specialized metaprompt to infer prompt updates directly from the provided conversation `trajectories` and the initial prompt. The example employs the 'anthropic:claude-3-5-sonnet-latest' model. Requires the `langmem` library.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/optimize_memory_prompt.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptimizer = create_prompt_optimizer(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    kind=\"prompt_memory\",  # 1 LLM call\n)\nupdated = optimizer.invoke(\n    {\"trajectories\": trajectories, \"prompt\": \"You are a planetary science expert\"}\n)\nprint(updated)\n```\n\n----------------------------------------\n\nTITLE: Invoking the LangGraph Agent and Viewing Output (Python)\nDESCRIPTION: Invokes the previously created agent with a user message requesting an email draft. It then accesses and prints the agent's response message (typically the second message in the result list for ReAct agents) in a formatted way using `pretty_print()`.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]}\n)\nresult['messages'][1].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Using SummarizationNode for Separation of Concerns in Python\nDESCRIPTION: This example demonstrates how to use the dedicated SummarizationNode to handle context summarization as a separate step before LLM invocation. It shows a cleaner architecture with better separation between summarization and model calling.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/summarization.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langmem.short_term import SummarizationNode, RunningSummary\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nsummarization_model = model.bind(max_tokens=128)\n\n\n# We will keep track of our running summary in the `context` field\n# (expected by the `SummarizationNode`)\nclass State(MessagesState):\n    # highlight-next-line\n    context: dict[str, Any]\n\n\n# Define private state that will be used only for filtering\n# the inputs to call_model node\nclass LLMInputState(TypedDict):\n    summarized_messages: list[AnyMessage]\n    context: dict[str, Any]\n\n# SummarizationNode uses `summarize_messages` under the hood and\n# automatically handles existing summary propagation that we had\n# to manually do in the above example \n# highlight-next-line\nsummarization_node = SummarizationNode(\n    token_counter=model.get_num_tokens_from_messages,\n    model=summarization_model,\n    max_tokens=256,\n    max_summary_tokens=128,\n)\n\n# The model-calling node now is simply a single LLM invocation\n# IMPORTANT: we're passing a private input state here to isolate the summarization\n# highlight-next-line\ndef call_model(state: LLMInputState):\n    response = model.invoke(state[\"summarized_messages\"])\n    return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(State)\nbuilder.add_node(call_model)\n# highlight-next-line\nbuilder.add_node(\"summarize\", summarization_node)\nbuilder.add_edge(START, \"summarize\")\nbuilder.add_edge(\"summarize\", \"call_model\")\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\ngraph.invoke({\"messages\": \"what's my name?\"}, config)\n```\n\n----------------------------------------\n\nTITLE: Initializing Prompt Optimizer\nDESCRIPTION: This code snippet initializes a prompt optimizer. It imports the `create_prompt_optimizer` function from `langmem`.  The `optimizer` is configured with a language model and the `kind` of optimizer, which is set to \"metaprompt\", allowing for rule and behavior updates. The `config` parameter includes `max_reflection_steps` for refining prompts.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/concepts/conceptual_guide.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_prompt_optimizer\n\n# highlight-next-line\noptimizer = create_prompt_optimizer(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    kind=\"metaprompt\",\n    config={\"max_reflection_steps\": 3}\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Agent Instructions in Store (Python)\nDESCRIPTION: Updates the agent's instructions in the `InMemoryStore` by putting the `optimizer_result` (the new prompt) back into the store at the path `(\"instructions\",)` with the key `agent_instructions`.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstore.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": optimizer_result})\n```\n\n----------------------------------------\n\nTITLE: Invoking Manager to Extract Episode Without Storage - LangMem Python\nDESCRIPTION: Provides a sample conversation history between a user and an assistant. Calls the `invoke` method of the previously initialized memory manager (`manager`) with the conversation messages. The manager processes this input according to its configuration and attempts to extract an `Episode` object, which is then printed.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_episodic_memories.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's a binary tree? I work with family trees if that helps\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"A binary tree is like a family tree, but each parent has at most 2 children. Here's a simple example:\\n   Bob\\n  /  \\\\\\nAmy  Carl\\n\\nJust like in family trees, we call Bob the 'parent' and Amy and Carl the 'children'.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Oh that makes sense! So in a binary search tree, would it be like organizing a family by age?\",\n    },\n]\n\nepisodes = manager.invoke({\"messages\": conversation})\nprint(episodes[0])\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Optimized Prompt (Python)\nDESCRIPTION: Invokes the agent again with the same initial user request after the prompt has been updated in the store via the optimizer. It then prints the agent's response to observe the effect of the optimized instructions.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]}\n)\nresult['messages'][1].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Invoking the Supervisor Workflow (Python)\nDESCRIPTION: Invokes the compiled supervisor application (`app`) with a user message intended for the email agent. The supervisor determines which agent should handle the request.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresult = app.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]},\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Basic Agent Infrastructure\nDESCRIPTION: Initializes the core components needed for a conversational agent including a memory saver and store for maintaining conversation state.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.store.memory import InMemoryStore\n\ncheckpointer = InMemorySaver()\nstore = InMemoryStore()\n```\n\n----------------------------------------\n\nTITLE: Optimizing Prompt with Feedback\nDESCRIPTION: This code snippet shows how to use the prompt optimizer to refine a prompt based on user interaction and feedback. It defines an initial prompt and a `trajectory`, which is a series of interactions between a user and an assistant. Each interaction includes the role and the content of the message. The `optimizer.invoke()` method is called with the `trajectories` and the initial `prompt`, which returns the optimized prompt based on the user feedback. The output, showing the optimized prompt, which now contains more detailed instructions that focus on giving the user practical examples and adapting the explanation style based on the user's preferences.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/concepts/conceptual_guide.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"You are a helpful assistant.\"\ntrajectory = [\n    {\"role\": \"user\", \"content\": \"Explain inheritance in Python\"},\n    {\"role\": \"assistant\", \"content\": \"Here's a detailed theoretical explanation...\"},\n    {\"role\": \"user\", \"content\": \"Show me a practical example instead\"},\n]\noptimized = optimizer.invoke({\n    \"trajectories\": [(trajectory, {\"user_score\": 0})], \n    \"prompt\": prompt\n})\nprint(optimized)\n# You are a helpful assistant with expertise in explaining technical concepts clearly and practically. When explaining programming concepts:\n\n# 1. Start with a brief, practical explanation supported by a concrete code example\n# 2. If the user requests more theoretical details, provide them after the practical example\n# 3. Always include working code examples for programming-related questions\n# 4. Pay close attention to user preferences - if they ask for a specific approach (like practical examples or theory), adapt your response accordingly\n# 5. Use simple, clear language and break down complex concepts into digestible parts\n\n# When users ask follow-up questions or request a different approach, immediately adjust your explanation style to match their preferences. If they ask for practical examples, provide them. If they ask for theory, explain the concepts in depth.\n```\n\n----------------------------------------\n\nTITLE: Defining Namespace Patterns for Memory Isolation - Python\nDESCRIPTION: Demonstrates how to organize extracted memories by using tuple-based namespace patterns, allowing isolation by user, team, or domain. Inputs are tuples representing different namespace hierarchies; outputs are used to separate and manage different categories of facts in storage. This facilitates fine-grained access, organization, and sharing of knowledge within the application.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# User-specific memories\n(\"chat\", \"user_123\", \"triples\")\n\n# Team-shared knowledge\n(\"chat\", \"team_x\", \"triples\")\n\n# Domain-specific extraction\n(\"chat\", \"user_123\", \"preferences\")\n```\n\n----------------------------------------\n\nTITLE: Deleting Triples Based on Entity Change in Conversation - Python\nDESCRIPTION: Processes a third conversation where an entity leaves, triggering removal of the corresponding triples. The manager returns RemoveDoc objects for deleted memories while retaining or updating the rest. Input is a list of new messages with updated context and the filtered set of existing triples; output is a list of current ExtractedMemory and RemoveDoc objects. This demonstrates iterative fact management over the course of multiple conversational turns.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/extract_semantic_memories.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Delete triples about an entity\nconversation3 = [\n    {\"role\": \"user\", \"content\": \"Alice left the company.\"}\n]\nfinal = manager.invoke({\"messages\": conversation3, \"existing\": existing})\nprint(\"After third conversation:\")\nfor m in final:\n    print(m)\n# ExtractedMemory(id='7ca76217-66a4-4041-ba3d-46a03ea58c1b', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))\n# ExtractedMemory(id='35b443c7-49e2-4007-8624-f1d6bcb6dc69', content=RemoveDoc(json_doc_id='0214f151-b0c5-40c4-b621-db36b845956c'))\n# ExtractedMemory(id='65fd9b68-77a7-4ea7-ae55-66e1dd603046', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))\n# ExtractedMemory(id='7f8be100-5687-4410-b82a-fa1cc8d304c0', content=Triple(subject='Bob', predicate='leads', object='ML_team', context=None))\n# ExtractedMemory(id='f4c09154-2557-4e68-8145-8ccd8afd6798', content=Triple(subject='Bob', predicate='leads', object='NLP_project', context=None))\n# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))\n# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))\n# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))\n```\n\n----------------------------------------\n\nTITLE: Testing User-Specific Memory Recall\nDESCRIPTION: Tests the agent's ability to recall user-specific information by asking about sports preferences within User B's context.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nchat(agent, \n     \"Do you remember me liking any sports?\",\n     thread_1,\n     user_id2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Background Memory Processing (Python)\nDESCRIPTION: Sets up an in-memory store, initializes an LLM, and creates a background memory manager. It defines a simple LangGraph workflow where the LLM responds to a message, and the memory manager is then invoked asynchronously with the conversation history to extract memories. Requires langchain, langgraph, and langmem libraries, and a configured LLM API key.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/background_quickstart.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint\nfrom langgraph.store.memory import InMemoryStore\n\nfrom langmem import ReflectionExecutor, create_memory_store_manager\n\nstore = InMemoryStore( # (1)!\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)  \nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n# Create memory manager Runnable to extract memories from conversations\nmemory_manager = create_memory_store_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    # Store memories in the \"memories\" namespace (aka directory)\n    namespace=(\"memories\",),  # (2)!\n)\n\n@entrypoint(store=store)  # Create a LangGraph workflow\nasync def chat(message: str):\n    response = llm.invoke(message)\n\n    # memory_manager extracts memories from conversation history\n    # We'll provide it in OpenAI's message format\n    to_process = {\"messages\": [{\"role\": \"user\", \"content\": message}] + [response]}\n    await memory_manager.ainvoke(to_process)  # (3)!\n    return response.content\n# Run conversation as normal\nresponse = await chat.ainvoke(\n    \"I like dogs. My dog's name is Fido.\",\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Optimized Prompt on New Task (Python)\nDESCRIPTION: Invokes the agent with a different user request after the prompt optimization. This tests the generalizability of the updated instructions. The agent's response is printed.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\" : \"Let roger@langchain.dev know that the release should be later by 4:00 PM.\"}]}\n)\nresult['messages'][1].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Creating a LangMem Prompt Optimizer (Python)\nDESCRIPTION: Initializes a prompt optimizer using `langmem.create_prompt_optimizer`, specifying the Anthropic model to be used for the optimization process. This optimizer will refine agent prompts based on examples.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_prompt_optimizer\n\noptimizer = create_prompt_optimizer(\"anthropic:claude-3-5-sonnet-latest\")\n```\n\n----------------------------------------\n\nTITLE: Installing LangMem Library (Bash)\nDESCRIPTION: Installs or upgrades the LangMem library using pip. This is the first step to use the library's features for background memory processing. Requires Python and pip to be installed.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/background_quickstart.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U langmem\n```\n\n----------------------------------------\n\nTITLE: Defining User Profile with Pydantic\nDESCRIPTION: Defines a `UserProfile` class using Pydantic's `BaseModel`.  This model represents a user's profile and includes optional fields for name, language, and timezone. It ensures type safety and guides the model on what information is important for extraction.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/manage_user_profile.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing import Optional\n\n\n# Define profile structure\nclass UserProfile(BaseModel):\n    \"\"\"Represents the full representation of a user.\"\"\"\n    name: Optional[str] = None\n    language: Optional[str] = None\n    timezone: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM API Key (Bash)\nDESCRIPTION: Configures the environment variable required for authenticating with an LLM provider like Anthropic. Replace the placeholder with your actual API key for the chosen provider. This key is used by LangMem and LangGraph components to interact with the LLM.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/background_quickstart.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"sk-...\"  # Or another supported LLM provider\n```\n\n----------------------------------------\n\nTITLE: Example Output of Optimized Prompts in Python\nDESCRIPTION: This snippet shows an example Python list representing the output from the Langmem multi-prompt optimizer. It contains dictionaries for each role ('researcher', 'writer'), including the role 'name' and the updated 'prompt' string. The 'researcher' prompt has been significantly enhanced with detailed instructions based on the optimization process, reflecting feedback inferred from the conversations, while the 'writer' prompt remains unchanged in this example.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/optimize_compound_system.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[\n    {\n        'name': 'researcher',\n        'prompt': '''You analyze technical papers and extract key findings. For each analysis, include:\n1. High-level overview of the main contributions\n2. Technical implementation details and methodologies\n3. Architectural components and design choices\n4. Experimental results and performance metrics\n5. Practical implications and limitations\n\nEnsure your analysis maintains technical depth while remaining accessible. When discussing implementation details, include specific technical parameters, algorithms, and methodologies used. Structure your response to clearly separate these components.'''\n    },\n    {\n        'name': 'writer',\n        'prompt': 'You write clear reports based on research findings'\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Creating a LangGraph Agent with Manual Memory Management in Python\nDESCRIPTION: Defines a Python setup for a LangGraph agent that uses LangMem for explicit memory management. It initializes an `InMemoryStore` for storing memories with vector indexing, sets up a `MemorySaver` for checkpointing conversation state, and defines a custom `prompt` function to fetch relevant memories using `store.search`. The `create_react_agent` function integrates the LLM, the custom prompt, and the `create_manage_memory_tool` which allows the agent to consciously save, update, or delete memories within a specified namespace.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/hot_path_quickstart.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.utils.config import get_store \nfrom langmem import (\n    # Lets agent create, update, and delete memories (1)\n    create_manage_memory_tool,\n)\n\n\ndef prompt(state):\n    \"\"\"Prepare the messages for the LLM.\"\"\"\n    # Get store from configured contextvar; (5)\n    store = get_store() # Same as that provided to `create_react_agent`\n    memories = store.search(\n        # Search within the same namespace as the one\n        # we've configured for the agent\n        (\"memories\",),\n        query=state[\"messages\"][-1].content,\n    )\n    system_msg = f\"\"\"You are a helpful assistant.\n\n## Memories\n<memories>\n{memories}\n</memories>\n\"\"\"\n    return [{\"role\": \"system\", \"content\": system_msg}, *state[\"messages\"]]\n\n\nstore = InMemoryStore(\n    index={ # Store extracted memories (4)\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n) \ncheckpointer = MemorySaver() # Checkpoint graph state (2)\n\nagent = create_react_agent( \n    \"anthropic:claude-3-5-sonnet-latest\",\n    prompt=prompt,\n    tools=[ # Add memory tools (3)\n        # The agent can call \"manage_memory\" to\n        # create, update, and delete memories by ID\n        # Namespaces add scope to memories. To\n        # scope memories per-user, do (\"memories\", \"{user_id}\"): (6)\n        create_manage_memory_tool(namespace=(\"memories\",)),\n    ],\n    # Our memories will be stored in this provided BaseStore instance\n    store=store,\n    # And the graph \"state\" will be checkpointed after each node\n    # completes executing for tracking the chat history and durable execution\n    checkpointer=checkpointer, \n)\n```\n\n----------------------------------------\n\nTITLE: Starting a Conversation with User B\nDESCRIPTION: Initiates a conversation with a second user (John) who is learning chess, demonstrating multi-user memory separation.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nthread_1 = \"thread-2\"\nuser_id2 = \"User-B\"\nchat(agent, \n     \"Hi I'm John, I'm learning chess - could you help me become great??\",\n     thread_1,\n     user_id2)\n```\n\n----------------------------------------\n\nTITLE: Installing LangMem and Configuring Environment\nDESCRIPTION: Installs the LangMem Python package using pip and sets the environment variable for the Anthropic API key. Replace the placeholder with your actual key. This setup is necessary for using LangMem and interacting with the chosen LLM provider.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/hot_path_quickstart.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U langmem\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"sk-...\"  # Or another supported LLM provider\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Agent without Memory\nDESCRIPTION: Creates a ReAct agent using Claude 3.5 Sonnet model with no tools, configured with the previously defined store and checkpointer.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nagent = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", tools=[], store=store, checkpointer=checkpointer)\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Chatbot with Direct Message Summarization in Python\nDESCRIPTION: A basic implementation of a chatbot that uses summarize_messages to directly manage context length within the LLM call node. The example shows how to track running summaries in the graph state and apply summarization before each LLM invocation.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/summarization.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langmem.short_term import summarize_messages, RunningSummary\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\n# NOTE: we're also setting max output tokens for the summary\n# this should match max_summary_tokens in `summarize_messages` for better\n# token budget estimates\n# highlight-next-line\nsummarization_model = model.bind(max_tokens=128)\n\n# We will keep track of our running summary in the graph state\nclass SummaryState(MessagesState):\n    summary: RunningSummary | None\n\n# Define the node that will be calling the LLM\ndef call_model(state: SummaryState) -> SummaryState:\n    # We will attempt to summarize messages before the LLM is called\n    # If the messages in state[\"messages\"] fit into max tokens budget,\n    # we will simply return those messages. Otherwise, we will summarize\n    # and return [summary_message] + remaining_messages\n    # highlight-next-line\n    summarization_result = summarize_messages(\n        state[\"messages\"],\n        # IMPORTANT: Pass running summary, if any. This is what\n        # allows summarize_messages to avoid re-summarizing the same\n        # messages on every conversation turn\n        # highlight-next-line\n        running_summary=state.get(\"summary\"),\n        # by default this is using approximate token counting,\n        # but you can also use LLM-specific one, like below\n        # highlight-next-line\n        token_counter=model.get_num_tokens_from_messages,\n        model=summarization_model, \n        max_tokens=256,\n        max_summary_tokens=128\n    )\n    response = model.invoke(summarization_result.messages)\n    state_update = {\"messages\": [response]}\n    # If we generated a summary, add it as a state update and overwrite\n    # the previously generated summary, if any\n    if summarization_result.running_summary:\n        state_update[\"summary\"] = summarization_result.running_summary\n    return state_update\n\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(SummaryState)\nbuilder.add_node(call_model)\nbuilder.add_edge(START, \"call_model\")\n# It's important to compile the graph with a checkpointer,\n# otherwise the graph won't remember previous conversation turns\n# highlight-next-line\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\ngraph.invoke({\"messages\": \"what's my name?\"}, config)\n```\n\n----------------------------------------\n\nTITLE: Follow-up Conversation in Thread 1\nDESCRIPTION: Shows a follow-up message in the same conversation thread, demonstrating continuation of the conversation context.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nchat(agent, \"Nice! Wish me luck!\", thread_1)\n```\n\n----------------------------------------\n\nTITLE: Creating a LangMem Multi-Prompt Optimizer (Python)\nDESCRIPTION: Initializes a multi-prompt optimizer using `langmem.create_multi_prompt_optimizer`, specifying the Anthropic model. Defines feedback relevant to the email agent's task based on the previous supervisor interaction.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_multi_prompt_optimizer\n\nfeedback = {\"request\": \"Always sign off emails from 'William'; for meeting requests, offer to schedule on Zoom or Google Meet\"}\n\noptimizer = create_multi_prompt_optimizer(\"anthropic:claude-3-5-sonnet-latest\")\n```\n\n----------------------------------------\n\nTITLE: Installing LangMem\nDESCRIPTION: Command to install the LangMem package using pip.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/use_tools_in_custom_agent.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U langmem\n```\n\n----------------------------------------\n\nTITLE: Viewing Supervisor Result After Multi-Optimization (Python)\nDESCRIPTION: Prints the specific message (index 3) from the latest supervisor workflow invocation result to observe the final output generated with the newly optimized email agent prompt.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nresult[\"messages\"][3].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Installing LangMem and CrewAI Packages Using Bash\nDESCRIPTION: This snippet shows the command to install the LangMem and CrewAI Python packages using pip. These packages provide tools and agents for memory management and AI interactions. The command requires a Python environment with pip installed and the ability to access the Python Package Index. The output is the successful installation of the latest versions.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/use_tools_in_crewai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U crewai langmem\n```\n\n----------------------------------------\n\nTITLE: Displaying Multi-Prompt Optimization Result (Python)\nDESCRIPTION: Prints the result from the multi-prompt optimizer, which contains the optimized prompts (or original prompts if no update was deemed necessary based on the feedback and criteria).\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\noptimizer_result\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Semantic Memory\nDESCRIPTION: Instructions for installing the necessary packages (langmem and langgraph) using pip to implement semantic memory functionality.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# %pip install -U langmem langgraph\n```\n\n----------------------------------------\n\nTITLE: Installing LangGraph Supervisor Dependency (Bash)\nDESCRIPTION: Installs or updates the `langgraph-supervisor` package using pip, which is required for creating multi-agent workflows with a supervising agent. The `%%capture` magic command suppresses output.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n%%capture cap --no-stderr\n%pip install -U langgraph-supervisor\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Requirements with Poetry\nDESCRIPTION: Installs the necessary requirements for building documentation using Poetry package manager with the test extra.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npoetry install --with test\n```\n\n----------------------------------------\n\nTITLE: Displaying the Optimized Prompt (Python)\nDESCRIPTION: Prints the result obtained from the prompt optimizer invocation, showing the newly suggested agent prompt.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(optimizer_result)\n```\n\n----------------------------------------\n\nTITLE: Executing Notebooks for CI\nDESCRIPTION: Two-step process for executing notebooks that mimics the GitHub Actions workflow. First prepares notebooks by adding VCR cassette context managers, then executes them.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython docs/_scripts/prepare_notebooks_for_ci.py\n./docs/_scripts/execute_notebooks.sh\n```\n\n----------------------------------------\n\nTITLE: Viewing Supervisor Workflow Result (Python)\nDESCRIPTION: Accesses and prints a specific message (index 3) from the result of the supervisor workflow invocation. This message likely contains the final output or action taken by one of the managed agents.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresult[\"messages\"][3].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Removing Existing VCR Cassettes for a Notebook\nDESCRIPTION: Command to delete existing VCR cassettes for a specific notebook when updating its content, allowing for fresh recordings of API requests.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nrm docs/cassettes/<notebook_name>*\n```\n\n----------------------------------------\n\nTITLE: Initializing InMemoryStore for Agent Instructions (Python)\nDESCRIPTION: Creates an instance of `InMemoryStore` from `langgraph.store.memory` and stores initial instructions for an agent under the key `agent_instructions` within the tuple path `(\"instructions\",)`. This demonstrates storing procedural memory.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore()\nstore.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": \"Write good emails.\"})\n```\n\n----------------------------------------\n\nTITLE: Installing LangMem and LangGraph Dependencies (Bash)\nDESCRIPTION: Installs or updates the necessary Python packages `langmem` and `langgraph` using pip. The `%%capture` magic command suppresses output in a Jupyter-like environment.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n%%capture cap --no-stderr\n%pip install -U langmem langgraph\n```\n\n----------------------------------------\n\nTITLE: Implementing User-Specific Memories\nDESCRIPTION: Sets up a system for user-specific memories by creating a parameterized namespace that incorporates a user ID, allowing distinct memory spaces for different users.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_manage_memory_tool, create_search_memory_tool\n\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\"\n    }\n)\n\nnamespace = (\"agent_memories\", \"{user_id}\")\nmemory_tools = [\n    create_manage_memory_tool(namespace),\n    create_search_memory_tool(namespace)\n]\ncheckpointer = InMemorySaver()\n\nagent = create_react_agent(\"anthropic:claude-3-5-sonnet-latest\", tools=memory_tools, store=store, checkpointer=checkpointer)\n```\n\n----------------------------------------\n\nTITLE: Enhanced Chat Function with User ID Support\nDESCRIPTION: Updates the chat function to include a user ID parameter, enabling the agent to maintain separate memories for different users during conversations.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef chat(agent, txt, thread_id, user_id):\n    result_state = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": txt}]}, \n                                config={\"configurable\": {\"thread_id\": thread_id, \"user_id\": user_id}})\n    return result_state[\"messages\"][-1].content\n```\n\n----------------------------------------\n\nTITLE: Saving and Displaying Memories with Eager Retrieval\nDESCRIPTION: Demonstrates the agent storing detailed memories with the eager retrieval mechanism and displaying the content to the user.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(chat(agent, \"Nice! Wish me luck! Please note down the detailed memories for me :)\", thread_1, None))\n```\n\n----------------------------------------\n\nTITLE: Testing Memory Recall with Eager Retrieval\nDESCRIPTION: Tests the agent's ability to recall previously stored training plan information using the eager memory retrieval approach.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nthread_2 = \"thread-2\"\nchat(agent, \"What I'm supposed to do for my training this week? It's week 3...\", thread_2, None)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chat Function\nDESCRIPTION: Defines a function to handle chat interactions with the agent. It takes input text and a thread ID, invokes the agent, and returns the agent's response.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef chat(agent, txt, thread_id):\n    result_state = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": txt}]}, config={\"configurable\": {\"thread_id\": thread_id}})\n    return result_state[\"messages\"][-1].content\n```\n\n----------------------------------------\n\nTITLE: Starting a New Conversation in Thread 2\nDESCRIPTION: Illustrates starting a new conversation thread with the agent, showing how without memory the agent has no context from the previous thread.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nthread_2 = \"thread-2\"\nchat(agent, \"Nice! Oh thank you! It'll be hard.\", thread_2)\n```\n\n----------------------------------------\n\nTITLE: Testing Memory Retrieval in Thread 2\nDESCRIPTION: Tests the agent's ability to retrieve previously stored memories about the training plan when asked in a different conversation thread.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nthread_2 = \"thread-2\"\nchat(agent, \"Remember what I'm supposed to do for my training this week? It's week 3...\", thread_2)\n```\n\n----------------------------------------\n\nTITLE: Initial Conversation in Thread 1\nDESCRIPTION: Demonstrates the agent responding to an initial query about a half marathon training plan in a specific conversation thread.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nthread_1 = \"thread-1\"\nchat(agent, \"Hi there, I'm training for a half marathon in 2 months - could you propose a daily training plan to prepare?\", thread_1)\n```\n\n----------------------------------------\n\nTITLE: Starting a Conversation with User A\nDESCRIPTION: Initiates a conversation with the first user (Will) who is training for a half marathon, storing this information in their specific memory space.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nthread_1 = \"thread-1\"\nuser_id = \"User-A\"\nchat(agent, \n     \"Hi I'm Will, I'm training for a half marathon in 2 months - could you propose a daily training plan to prepare and help me stay honest??\",\n     thread_1,\n     user_id)\n```\n\n----------------------------------------\n\nTITLE: Saving Detailed Memories in Thread 1\nDESCRIPTION: Demonstrates explicitly requesting the agent to note down detailed memories for later retrieval, showcasing memory management functionality.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(chat(agent, \"Nice! Wish me luck! Please note down the detailed memories for me :)\", thread_1))\n```\n\n----------------------------------------\n\nTITLE: Creating Memory Manager in Python\nDESCRIPTION: This snippet describes the process of creating a memory manager using the provided handler and options. It likely involves initializing memory management components crucial for storing and retrieving context in AI workflows. Dependencies include the langmem module and associated classes like create_memory_manager.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/reference/memory.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nhandler: python\noptions:\n  members:\n    - create_memory_manager\n    - create_memory_store_manager\n    - MemoryStoreManager\n```\n\n----------------------------------------\n\nTITLE: Extracting Episodes from Conversation\nDESCRIPTION: This code snippet demonstrates how to extract episodes from a conversation using a pre-configured memory manager. It defines a sample conversation as a list of dictionaries, each representing a user or assistant message. The `manager.invoke()` function is then called with the conversation to process and extract relevant episodes, storing them in the `episodes` variable.  This is then followed by a example extracted memory for illustration, but is not executable code.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/concepts/conceptual_guide.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Extract episode(s)\nepisodes = manager.invoke({\"messages\": conversation})\n# Example episode:\n# [\n#     ExtractedMemory(\n#         id=\"f9194af3-a63f-4d8a-98e9-16c66e649844\",\n#         content=Episode(\n#             observation=\"User struggled debugging a recursive \"\n#                         \"function for longest path in binary \"\n#                         \"tree, unclear on logic.\",\n#             thoughts=\"Used explorer in treehouse village \"\n#                      \"metaphor to explain recursion:\\n\"\n#                      \"- Houses = Nodes\\n\"\n#                      \"- Bridges = Edges\\n\"\n#                      \"- Explorer's path = Traversal\",\n#             action=\"Reframed problem using metaphor, \"\n#                    \"outlined steps:\\n\"\n#                    \"1. Check left path\\n\"\n#                    \"2. Check right path\\n\"\n#                    \"3. Add 1 for current position\\n\"\n#                    \"Highlighted common bugs\",\n#             result=\"Metaphor helped user understand logic. \"\n#                    \"Worked because it:\\n\"\n#                    \"1. Made concepts tangible\\n\"\n#                    \"2. Created mental model\\n\"\n#                    \"3. Showed key steps\\n\"\n#                    \"4. Pointed to likely bugs\",\n#         ),\n#     )\n# ]\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Provider API Key in Bash\nDESCRIPTION: This snippet shows how to set an environment variable storing the API key for an LLM provider, such as Anthropic. This key is required by LangMem agents for accessing the LLM API to enable memory and conversational capabilities. The key should be kept secure and replaced with a valid credential.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"sk-...\"  # Or another supported LLM provider\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory Manager and Extracting Profile\nDESCRIPTION: Initializes a `MemoryManager` with a specified language model (`anthropic:claude-3-5-sonnet-latest`) and configures it to extract user profile information. It demonstrates how to invoke the manager with a conversation to extract or update the user's profile and prints the extracted memory.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/manage_user_profile.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langmem import create_memory_manager\nfrom pydantic import BaseModel\nfrom typing import Optional\n\n\n# Define profile structure\nclass UserProfile(BaseModel):\n    \"\"\"Represents the full representation of a user.\"\"\"\n    name: Optional[str] = None\n    language: Optional[str] = None\n    timezone: Optional[str] = None\n\n\n# Configure extraction\nmanager = create_memory_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    schemas=[UserProfile], # (optional) customize schema (1)\n    instructions=\"Extract user profile information\",\n    enable_inserts=False,  # Profiles update in-place (2)\n)\n\n# First conversation\nconversation1 = [{\"role\": \"user\", \"content\": \"I'm Alice from California\"}]\nmemories = manager.invoke({\"messages\": conversation1})\nprint(memories[0])\n# ExtractedMemory(id='profile-1', content=UserProfile(\n#    name='Alice',\n#    language=None,\n#    timezone='America/Los_Angeles'\n# ))\n\n# Second conversation updates existing profile\nconversation2 = [{\"role\": \"user\", \"content\": \"I speak Spanish too!\"}]\nupdate = manager.invoke({\"messages\": conversation2, \"existing\": memories})\nprint(update[0])\n# ExtractedMemory(id='profile-1', content=UserProfile(\n#    name='Alice',\n#    language='Spanish',  # Updated\n#    timezone='America/Los_Angeles'\n# ))\n```\n\n----------------------------------------\n\nTITLE: Implementing Delayed Memory Processing with ReflectionExecutor in Python\nDESCRIPTION: This code snippet demonstrates how to use ReflectionExecutor to debounce memory processing tasks in a chat application. It shows the setup of a memory manager, wrapping it with ReflectionExecutor to handle deferred execution, and scheduling memory processing after a specified delay with support for task cancellation if new messages arrive. Dependencies include langchain, langgraph, and langmem modules. The main function â€˜chatâ€™ invokes the language model, formats the message, and submits a delayed processing task to optimize memory handling during active conversations.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/docs/guides/delayed_processing.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint\nfrom langgraph.store.memory import InMemoryStore\nfrom langmem import ReflectionExecutor, create_memory_store_manager\n\n# Create memory manager to extract memories from conversations (1)\nmemory_manager = create_memory_store_manager(\n    \"anthropic:claude-3-5-sonnet-latest\",\n    namespace=(\"memories\",),\n)\n# Wrap memory_manager to handle deferred background processing (2)\nexecutor = ReflectionExecutor(memory_manager)\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)\n\n@entrypoint(store=store)\ndef chat(message: str):\n    response = llm.invoke(message)\n    # Format conversation for memory processing\n    # Must follow OpenAI's message format\n    to_process = {\"messages\": [{\"role\": \"user\", \"content\": message}] + [response]}\n    \n    # Wait 30 minutes before processing\n    # If new messages arrive before then:\n    # 1. Cancel pending processing task\n    # 2. Reschedule with new messages included\n    delay = 0.5 # In practice would choose longer (30-60 min)\n    # depending on app context.\n    executor.submit(to_process, after_seconds=delay)\n    return response.content\n```\n\n----------------------------------------\n\nTITLE: Updating Email Agent Prompt After Multi-Optimization (Python)\nDESCRIPTION: Updates the email agent's instructions in the `InMemoryStore` using the optimized prompt returned by the multi-prompt optimizer (`optimizer_result[1]['prompt']`, assuming the email prompt was the second item in the input list).\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nstore.put((\"instructions\",), key=\"email_agent\", value={\"prompt\": optimizer_result[1]['prompt']})\n```\n\n----------------------------------------\n\nTITLE: Installing LangMem Python Package with pip\nDESCRIPTION: This Bash snippet demonstrates how to install the LangMem Python package using pip. It is a prerequisite for using LangMem's functionalities to manage agent memory. Ensure Python and pip are installed before running this command.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U langmem\n```\n\n----------------------------------------\n\nTITLE: Executing Notebooks Without Installation Cells\nDESCRIPTION: Executes notebooks while commenting out the pip install cells, useful for development environments where dependencies are already installed.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython docs/_scripts/prepare_notebooks_for_ci.py --comment-install-cells\n./docs/_scripts/execute_notebooks.sh\n```\n\n----------------------------------------\n\nTITLE: Executing a Single Notebook with VCR Cassette Recording\nDESCRIPTION: Command for executing a single notebook with Jupyter after preparing it with VCR cassettes to record API requests for future replay.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\njupyter execute <path_to_notebook>\n```\n\n----------------------------------------\n\nTITLE: Serving Documentation Locally\nDESCRIPTION: Runs the documentation server locally using the make command, which executes the serve-docs target defined in the project's Makefile.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/docs/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake serve-docs\n```\n\n----------------------------------------\n\nTITLE: Listing All Stored Memories\nDESCRIPTION: Retrieves and displays all memories stored in the agent_memories namespace, showing how information is organized in the memory store.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/semantic_memory.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nitems = store.search((\"agent_memories\",))\nfor item in items:\n    print(item.namespace, item.value)\n```\n\n----------------------------------------\n\nTITLE: Testing Supervisor Workflow with Optimized Email Prompt (Python)\nDESCRIPTION: Invokes the supervisor application again with the same email drafting request after the email agent's prompt has been updated by the multi-prompt optimizer. This tests the effect of the targeted optimization within the multi-agent system.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/intro_videos/procedural_memory.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nresult = app.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\" :\"Draft an email to joe@langchain.dev saying that we want to schedule a followup meeting for thursday at noon.\"}]},\n)\n```\n\n----------------------------------------\n\nTITLE: Executing LangMem Custom Store Example in Bash\nDESCRIPTION: Command-line instructions to create and activate a Python virtual environment, synchronize project dependencies using the 'uv' tool, export the OpenAI API key environment variable, and run the 'custom_store_example.py' script. This facilitates setting up the environment necessary for executing LangMem standalone memory management examples.\nSOURCE: https://github.com/langchain-ai/langmem/blob/main/examples/standalone_examples/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv venv\nsource .venv/bin/activate\nuv sync\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/standalone_examples\nuv run custom_store_example.py\n```"
  }
]
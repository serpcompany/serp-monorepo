[
  {
    "owner": "terraform-aws-modules",
    "repo": "terraform-aws-eks",
    "content": "TITLE: Creating EKS Managed Node Group - terraform-aws-modules (HCL)\nDESCRIPTION: This code sets up an EKS cluster with managed node group(s) using the terraform-aws-modules/eks/aws module, specifying instance types, AMI types, and scaling options. Required dependencies are Terraform and a configured AWS account, with VPC and subnet resources already provisioned. Major parameters manage node group defaults, control plane subnets, cluster versions, public endpoint access, and tags. Adjust instance types, subnet IDs, and cluster names as needed for your setup; limits may apply on instance sizes and regional availability.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/README.md#_snippet_2\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 20.0\"\n\n  cluster_name    = \"my-cluster\"\n  cluster_version = \"1.31\"\n\n  bootstrap_self_managed_addons = false\n  cluster_addons = {\n    coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni                = {}\n  }\n\n  # Optional\n  cluster_endpoint_public_access = true\n\n  # Optional: Adds the current caller identity as an administrator via cluster access entry\n  enable_cluster_creator_admin_permissions = true\n\n  vpc_id                   = \"vpc-1234556abcdef\"\n  subnet_ids               = [\"subnet-abcde012\", \"subnet-bcde012a\", \"subnet-fghi345a\"]\n  control_plane_subnet_ids = [\"subnet-xyzde987\", \"subnet-slkjf456\", \"subnet-qeiru789\"]\n\n  # EKS Managed Node Group(s)\n  eks_managed_node_group_defaults = {\n    instance_types = [\"m6i.large\", \"m5.large\", \"m5n.large\", \"m5zn.large\"]\n  }\n\n  eks_managed_node_groups = {\n    example = {\n      # Starting on 1.30, AL2023 is the default AMI type for EKS managed node groups\n      ami_type       = \"AL2023_x86_64_STANDARD\"\n      instance_types = [\"m5.xlarge\"]\n\n      min_size     = 2\n      max_size     = 10\n      desired_size = 2\n    }\n  }\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Instance Refresh for Terraform Self-Managed Node Groups\nDESCRIPTION: This HCL configuration demonstrates how to disable the default security group creation (`create_security_group = false`) and configure instance refresh settings (`instance_refresh`) for self-managed node groups within the terraform-aws-eks module. This configuration is crucial during the upgrade to v19.x to ensure nodes are automatically replaced with the updated launch template configuration, maintaining a minimum healthy percentage during the rolling update.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\ncreate_security_group = false\ninstance_refresh = {\n  strategy = \"Rolling\"\n  preferences = {\n    min_healthy_percentage = 66\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Karpenter Module with All Resources in Terraform\nDESCRIPTION: Example showing how to use the Karpenter module to create all required AWS resources including IAM roles, Pod Identity association, and SQS queue with EventBridge rules. This also demonstrates how to attach additional IAM policies to the Karpenter node IAM role.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/karpenter/README.md#_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"eks\" {\n  source = \"terraform-aws-modules/eks/aws\"\n\n  ...\n}\n\nmodule \"karpenter\" {\n  source = \"terraform-aws-modules/eks/aws//modules/karpenter\"\n\n  cluster_name = module.eks.cluster_name\n\n  # Attach additional IAM policies to the Karpenter node IAM role\n  node_iam_role_additional_policies = {\n    AmazonSSMManagedInstanceCore = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n  }\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Default instance refresh configuration for self-managed node groups (HCL)\nDESCRIPTION: This configuration block demonstrates the updated default value for `instance_refresh` in self-managed node groups.  It sets the strategy to 'Rolling' and defines preferences for the minimum healthy percentage during instance replacement. It is written in HashiCorp Configuration Language (HCL).\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\n{\n      strategy = \"Rolling\"\n      preferences = {\n        min_healthy_percentage = 66\n      }\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Managed Node Groups with Default Configurations in Terraform\nDESCRIPTION: Configuration that demonstrates the use of default configurations for EKS managed node groups. This example creates four node groups with different configurations, showing how defaults can be overridden at various levels.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_7\n\nLANGUAGE: HCL\nCODE:\n```\n  eks_managed_node_group_defaults = {\n    ami_type               = \"AL2_x86_64\"\n    disk_size              = 50\n    instance_types         = [\"m6i.large\", \"m5.large\", \"m5n.large\", \"m5zn.large\"]\n  }\n\n  eks_managed_node_groups = {\n    # Uses module default configurations overridden by configuration above\n    default = {}\n\n    # This further overrides the instance types used\n    compute = {\n      instance_types = [\"c5.large\", \"c6i.large\", \"c6d.large\"]\n    }\n\n    # This further overrides the instance types and disk size used\n    persistent = {\n      disk_size = 1024\n      instance_types = [\"r5.xlarge\", \"r6i.xlarge\", \"r5b.xlarge\"]\n    }\n\n    # This overrides the OS used\n    bottlerocket = {\n      ami_type = \"BOTTLEROCKET_x86_64\"\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Terraform EKS Managed Node Group Module Usage Configuration\nDESCRIPTION: This code snippet defines a Terraform module block configuring an EKS managed node group. It specifies parameters such as cluster info, scaling parameters, instance types, labels, taints, and tags, including optional setup for remote access and custom launch templates. It sets up nodes to join an EKS cluster with desired settings for scaling and security.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/eks-managed-node-group/README.md#_snippet_0\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks_managed_node_group\" {\n  source = \"terraform-aws-modules/eks/aws//modules/eks-managed-node-group\"\n\n  name            = \"separate-eks-mng\"\n  cluster_name    = \"my-cluster\"\n  cluster_version = \"1.31\"\n\n  subnet_ids = [\"subnet-abcde012\", \"subnet-bcde012a\", \"subnet-fghi345a\"]\n\n  // The following variables are necessary if you decide to use the module outside of the parent EKS module context.\n  // Without it, the security groups of the nodes are empty and thus won't join the cluster.\n  cluster_primary_security_group_id = module.eks.cluster_primary_security_group_id\n  vpc_security_group_ids            = [module.eks.node_security_group_id]\n\n  // Note: `disk_size`, and `remote_access` can only be set when using the EKS managed node group default launch template\n  // This module defaults to providing a custom launch template to allow for custom security groups, tag propagation, etc.\n  // use_custom_launch_template = false\n  // disk_size = 50\n  //\n  //  # Remote access cannot be specified with a launch template\n  //  remote_access = {\n  //    ec2_ssh_key               = module.key_pair.key_pair_name\n  //    source_security_group_ids = [aws_security_group.remote_access.id]\n  //  }\n\n  min_size     = 1\n  max_size     = 10\n  desired_size = 1\n\n  instance_types = [\"t3.large\"]\n  capacity_type  = \"SPOT\"\n\n  labels = {\n    Environment = \"test\"\n    GithubRepo  = \"terraform-aws-eks\"\n    GithubOrg   = \"terraform-aws-modules\"\n  }\n\n  taints = {\n    dedicated = {\n      key    = \"dedicated\"\n      value  = \"gpuGroup\"\n      effect = \"NO_SCHEDULE\"\n    }\n  }\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Module-Provided Bootstrap User Data in Self Managed Node Group using Terraform (HCL)\nDESCRIPTION: This snippet configures variables to opt-in to the module-supplied bootstrap user data template in a self-managed EKS node group context, primarily when using an AMI derived from the AWS EKS Optimized AMI. The variables allow specification of pre- and post-bootstrap scripts and extra bootstrap arguments. Dependencies are the terraform-aws-eks module and a compatible AMI. The output will be EC2 nodes with the specified customization injected at bootstrap.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/user_data.md#_snippet_3\n\nLANGUAGE: hcl\nCODE:\n```\nenable_bootstrap_user_data = true # to opt in to using the module supplied bootstrap user data template\npre_bootstrap_user_data    = \"...\"\nbootstrap_extra_args       = \"...\"\npost_bootstrap_user_data   = \"...\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom User Data Template in Self Managed Node Group using Terraform (HCL)\nDESCRIPTION: This snippet demonstrates setting the user_data_template_path variable to inject a fully custom user data script for self-managed EKS node provisioning, along with supporting variables for extra configuration. Required dependencies include the terraform-aws-eks module and a suitable script file accessible at the given path. The snippet's inputs are the custom template path and optional script content. The output is an EC2 node bootstrapped according to the user-supplied template.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/user_data.md#_snippet_4\n\nLANGUAGE: hcl\nCODE:\n```\nuser_data_template_path  = \"./your/user_data.sh\" # user supplied bootstrap user data template\npre_bootstrap_user_data  = \"...\"\nbootstrap_extra_args     = \"...\"\npost_bootstrap_user_data = \"...\"\n```\n\n----------------------------------------\n\nTITLE: Terraform Module Configuration for AWS EKS Cluster\nDESCRIPTION: This snippet demonstrates defining a Terraform module for creating an Amazon EKS cluster with specified parameters such as name, version, node groups, and networking configurations. It depends on the terraform-aws-eks module and outputs relevant cluster details.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-no-op.txt#_snippet_0\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  source          = \"terraform-aws-modules/eks/aws\"\n  version         = \"<version>\"\n  cluster_name    = \"my-eks-cluster\"\n  cluster_version = \"1.26\"\n  subnets         = var.subnets\n  vpc_id          = var.vpc_id\n  node_groups     = {\n    default = {\n      desired_capacity = 3\n      max_capacity     = 5\n      min_capacity     = 1\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Migrating Terraform State for EKS IAM Role - Bash\nDESCRIPTION: This Bash command migrates the Terraform state for the EKS IAM role resource to adapt to module changes between v17.x and v18.x of terraform-aws-eks. It uses 'terraform state mv' to rename the state path, which is required because the resource address has changed. Prerequisites: Terraform CLI installed, and your workspace initialized. Run this before applying the upgraded configuration to avoid resource recreation. Replace 'module.eks.aws_iam_role.cluster[0]' and 'module.eks.aws_iam_role.this[0]' with the actual addresses if they differ in your configuration.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-18.0.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nterraform state mv 'module.eks.aws_iam_role.cluster[0]' 'module.eks.aws_iam_role.this[0]'\n\n```\n\n----------------------------------------\n\nTITLE: Configuring aws-auth ConfigMap in Terraform AWS EKS Module (HCL)\nDESCRIPTION: This snippet demonstrates the usage of the terraform-aws-modules/eks/aws//modules/aws-auth submodule to manage the aws-auth ConfigMap in an Amazon EKS cluster via Terraform. Dependencies include a Terraform configuration with version >= 1.3.2 and the kubernetes provider >= 2.20. Key input parameters are aws_auth_roles (list of IAM roles to map), aws_auth_users (list of IAM users to map), aws_auth_accounts (AWS accounts to map), and manage_aws_auth_configmap (flag to enable management). The module expects lists of maps for roles and users with fields rolearn/userarn, username, and groups. Outputs are not produced. This is designed for scenarios where EKS RBAC integration with AWS IAM is required and supports self-managed and managed node groups.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/aws-auth/README.md#_snippet_0\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws//modules/aws-auth\"\n  version = \"~> 20.0\"\n\n  manage_aws_auth_configmap = true\n\n  aws_auth_roles = [\n    {\n      rolearn  = \"arn:aws:iam::66666666666:role/role1\"\n      username = \"role1\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  aws_auth_users = [\n    {\n      userarn  = \"arn:aws:iam::66666666666:user/user1\"\n      username = \"user1\"\n      groups   = [\"system:masters\"]\n    },\n    {\n      userarn  = \"arn:aws:iam::66666666666:user/user2\"\n      username = \"user2\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  aws_auth_accounts = [\n    \"777777777777\",\n    \"888888888888\",\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Cluster Access Entry - terraform-aws-modules (HCL)\nDESCRIPTION: This snippet configures explicit 'access_entries' for EKS cluster access management using Terraform. It specifies an IAM principal ARN and a policy association that limits access scope (e.g., to a Kubernetes namespace). The code is intended for clusters with CAM support enabled via authentication_mode = \"API_AND_CONFIG_MAP\"; ensure policy ARNs and principal ARNs are correct for your environment. No changes are needed for default managed/Fargate profiles; applies to additional roles or legacy clusters migrating to access entries.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/README.md#_snippet_3\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 20.0\"\n\n  # Truncated for brevity ...\n\n  access_entries = {\n    # One access entry with a policy associated\n    example = {\n      principal_arn = \"arn:aws:iam::123456789012:role/something\"\n\n      policy_associations = {\n        example = {\n          policy_arn = \"arn:aws:eks::aws:cluster-access-policy/AmazonEKSViewPolicy\"\n          access_scope = {\n            namespaces = [\"default\"]\n            type       = \"namespace\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl and Deploying Sample Application to EKS\nDESCRIPTION: These Bash commands configure kubectl to interact with the newly provisioned EKS cluster and then deploy a sample application using a 'deployment.yaml' file. It retrieves the cluster name dynamically using Terraform output.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/eks-auto-mode/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naws eks update-kubeconfig --name $(terraform output -raw cluster_name)\nkubectl apply -f deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Terraform module for EKS with existing node group names in HCL\nDESCRIPTION: This HCL snippet demonstrates defining the `module` block for `terraform-aws-modules/eks/aws` version 17.0.0, explicitly setting the `name` argument within `node_groups` to match the existing node groups. This ensures that during upgrade, node groups are preserved instead of being recreated. The snippet highlights setting cluster parameters and node group names to maintain continuity across upgrades.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-17.0.md#_snippet_1\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"17.0.0\"\n\n  cluster_name    = \"test-eks-mwIwsvui\"\n  cluster_version = \"1.20\"\n  # ...\n\n  node_groups = {\n    example = {\n      name = \"test-eks-mwIwsvui-example-sincere-squid\"\n\n      # ...\n    }\n  }\n  # ...\n}\n```\n\n----------------------------------------\n\nTITLE: Generic Terraform State Move for EKS Managed Node Group Policies\nDESCRIPTION: This shell command template shows how to use `terraform state mv` to update state for `aws_iam_role_policy_attachment` resources on EKS Managed Node Group roles during the v18.x to v19.x upgrade. Replace `<NODE_GROUP_KEY>` with the key from the `eks_managed_node_groups` map, `<POLICY_ARN>` with the policy ARN, and `<POLICY_MAP_KEY>` with the key from the `iam_role_additional_policies` map.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nterraform state mv 'module.eks.module.eks_managed_node_group[\"<NODE_GROUP_KEY>\"].aws_iam_role_policy_attachment.this[\"<POLICY_ARN>\"]' 'module.eks.module.eks_managed_node_group[\"<NODE_GROUP_KEY>\"].aws_iam_role_policy_attachment.additional[\"<POLICY_MAP_KEY>\"]'\n```\n\n----------------------------------------\n\nTITLE: Defining EKS Cluster with Managed Nodes and Fargate (v18.x) - Terraform\nDESCRIPTION: This Terraform module configures an EKS cluster using version 18.x of the terraform-aws-modules/eks/aws module. It defines the cluster name, version, VPC, subnets, managed node groups, self-managed node groups, and Fargate profiles. The configuration includes settings for cluster access, node group capacity, instance types, and Fargate namespace selectors. Dependencies include the VPC module and aws_security_group resource.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-18.0.md#_snippet_3\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"cluster_after\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 18.0\"\n\n  cluster_name                    = local.name\n  cluster_version                 = local.cluster_version\n  cluster_endpoint_private_access = true\n  cluster_endpoint_public_access  = true\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_group_defaults = {\n    ami_type  = \"AL2_x86_64\"\n    disk_size = 50\n  }\n\n  eks_managed_node_groups = {\n    node_group = {\n      min_size     = 1\n      max_size     = 10\n      desired_size = 1\n\n      instance_types = [\"t3.large\"]\n      capacity_type  = \"SPOT\"\n\n      update_config = {\n        max_unavailable_percentage = 50\n      }\n\n      labels = {\n        Environment = \"test\"\n        GithubRepo  = \"terraform-aws-eks\"\n        GithubOrg   = \"terraform-aws-modules\"\n      }\n\n      taints = [\n        {\n          key    = \"dedicated\"\n          value  = \"gpuGroup\"\n          effect = \"NO_SCHEDULE\"\n        }\n      ]\n\n      tags = {\n        ExtraTag = \"example\"\n      }\n    }\n  }\n\n  self_managed_node_group_defaults = {\n    vpc_security_group_ids = [aws_security_group.additional.id]\n  }\n\n  self_managed_node_groups = {\n    worker_group = {\n      name = \"worker-group\"\n\n      min_size      = 1\n      max_size      = 5\n      desired_size  = 2\n      instance_type = \"m4.large\"\n\n      bootstrap_extra_args = \"--kubelet-extra-args '--node-labels=node.kubernetes.io/lifecycle=spot'\"\n\n      block_device_mappings = {\n        xvda = {\n          device_name = \"/dev/xvda\"\n          ebs = {\n            delete_on_termination = true\n            encrypted             = false\n            volume_size           = 100\n            volume_type           = \"gp2\"\n          }\n\n        }\n      }\n\n      use_mixed_instances_policy = true\n      mixed_instances_policy = {\n        instances_distribution = {\n          spot_instance_pools = 4\n        }\n\n        override = [\n          { instance_type = \"m5.large\" },\n          { instance_type = \"m5a.large\" },\n          { instance_type = \"m5d.large\" },\n          { instance_type = \"m5ad.large\" },\n        ]\n      }\n    }\n  }\n\n  # Fargate\n  fargate_profiles = {\n    default = {\n      name = \"default\"\n\n      selectors = [\n        {\n          namespace = \"kube-system\"\n          labels = {\n            k8s-app = \"kube-dns\"\n          }\n        },\n        {\n          namespace = \"default\"\n        }\n      ]\n\n      tags = {\n        Owner = \"test\"\n      }\n\n      timeouts = {\n        create = \"20m\"\n        delete = \"20m\"\n      }\n    }\n  }\n\n  tags = {\n    Environment = \"test\"\n    GithubRepo  = \"terraform-aws-eks\"\n    GithubOrg   = \"terraform-aws-modules\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Attaching IAM Role Policy to Fargate Profile (v17.x) - Terraform\nDESCRIPTION: This Terraform resource attaches an IAM role policy to a Fargate profile using version 17.x of the EKS module. It relies on the `module.eks.fargate_iam_role_name` output to determine the role name and `aws_iam_policy.default.arn` for the policy ARN.  Dependencies include the EKS module and aws_iam_policy resource.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-18.0.md#_snippet_4\n\nLANGUAGE: hcl\nCODE:\n```\nresource \"aws_iam_role_policy_attachment\" \"default\" {\n  role       = module.eks.fargate_iam_role_name\n  policy_arn = aws_iam_policy.default.arn\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Module-Provided Bootstrap User Data in EKS Managed Node Group using Terraform (HCL)\nDESCRIPTION: This snippet shows how to enable and configure the module-supplied bootstrap user data template when launching EKS Managed Node Groups with a custom AMI that is a derivative of the AWS EKS Optimized AMI. Required dependencies are the terraform-aws-eks module and an appropriate custom AMI. The snippet sets variables to opt-in to the module-provided template, add custom scripts before and after bootstrapping, and provide extra arguments. Inputs include booleans and strings for each variable. Outputs are node instances bootstrapped with the customized flow.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/user_data.md#_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\nenable_bootstrap_user_data = true # to opt in to using the module supplied bootstrap user data template\npre_bootstrap_user_data    = \"...\"\nbootstrap_extra_args       = \"...\"\npost_bootstrap_user_data   = \"...\"\n```\n\n----------------------------------------\n\nTITLE: Recommended next steps after upgrade to avoid node group recreation conflicts\nDESCRIPTION: This HCL snippet suggests removing the explicit `name` argument from `node_groups` and instead using `node_group_name_prefix` to allow the module to generate names dynamically. This practice facilitates create-before-destroy lifecycle during updates, reducing the risk of collision and unintentional node group recreation. It encourages creating new node groups post-upgrade for optimal management.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-17.0.md#_snippet_3\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  # ...\n\n  node_group_name_prefix = \"test-eks-\"  # Remove 'name' argument for dynamic naming\n  # ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Karpenter Module with Existing Node IAM Role in Terraform\nDESCRIPTION: Example demonstrating how to configure the Karpenter module to reuse an existing Node IAM role from an EKS managed node group. This approach creates only the controller IAM role, SQS queue, and EventBridge rules while using the existing node group's IAM role and access entry.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/karpenter/README.md#_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"eks\" {\n  source = \"terraform-aws-modules/eks\"\n\n  # Shown just for connection between cluster and Karpenter sub-module below\n  eks_managed_node_groups = {\n    initial = {\n      instance_types = [\"t3.medium\"]\n\n      min_size     = 1\n      max_size     = 3\n      desired_size = 1\n    }\n  }\n  ...\n}\n\nmodule \"karpenter\" {\n  source = \"terraform-aws-modules/eks/aws//modules/karpenter\"\n\n  cluster_name = module.eks.cluster_name\n\n  create_node_iam_role = false\n  node_iam_role_arn    = module.eks.eks_managed_node_groups[\"initial\"].iam_role_arn\n\n  # Since the node group role will already have an access entry\n  create_access_entry = false\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying and Verifying Karpenter Setup with kubectl Commands in Bash\nDESCRIPTION: This snippet provides a set of kubectl commands for verifying the EKS cluster connectivity, deploying Karpenter NodeClass and example workloads, and monitoring Karpenter's controller logs. Before running these commands, updating the kubeconfig for the target EKS cluster is required. It validates that pods are scheduled on Karpenter-managed nodes versus managed node groups, which is essential for resource scaling verification.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/karpenter/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# First, make sure you have updated your local kubeconfig\naws eks --region eu-west-1 update-kubeconfig --name ex-karpenter\n\n# Second, deploy the Karpenter NodeClass/NodePool\nkubectl apply -f karpenter.yaml\n\n# Second, deploy the example deployment\nkubectl apply -f inflate.yaml\n\n# You can watch Karpenter's controller logs with\nkubectl logs -f -n kube-system -l app.kubernetes.io/name=karpenter -c controller\n```\n\n----------------------------------------\n\nTITLE: Example HCL Configuration for Additional IAM Policies (v19.x)\nDESCRIPTION: This HCL snippet shows an example of how to define additional IAM policies for the cluster IAM role using the new map structure (`iam_role_additional_policies`) introduced in v19.x of the Terraform EKS module. It uses a map key (`additional`) to associate the policy ARN, replacing the previous list-based approach.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_3\n\nLANGUAGE: hcl\nCODE:\n```\n  ...\n  # This is demonstrating the cluster IAM role additional policies\n  iam_role_additional_policies = {\n    additional = aws_iam_policy.additional.arn\n  }\n  ...\n```\n\n----------------------------------------\n\nTITLE: Updating Karpenter Sub-Module for EKS with New IAM Role Resources (HCL)\nDESCRIPTION: This code block shows the migration of the Karpenter sub-module in Terraform from version 19.21 to 20.0. Key changes include updating the version, enabling IRSA, specifying instance profile creation, and setting names/descriptions for the IAM resources. The configuration assumes the eks module is defined and outputs 'cluster_name'. This snippet requires Terraform 0.13+ and the appropriate permissions to create AWS IAM resources and EKS integrations.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-20.0.md#_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n module \"eks_karpenter\" {\n   source  = \"terraform-aws-modules/eks/aws//modules/karpenter\"\n-  version = \"~> 19.21\"\n+  version = \"~> 20.0\"\n\n# If you wish to maintain the current default behavior of v19.x\n+  enable_irsa             = true\n+  create_instance_profile = true\n\n# To avoid any resource re-creation\n+  iam_role_name          = \"KarpenterIRSA-${module.eks.cluster_name}\"\n+  iam_role_description   = \"Karpenter IAM role for service account\"\n+  iam_policy_name        = \"KarpenterIRSA-${module.eks.cluster_name}\"\n+  iam_policy_description = \"Karpenter IAM role for service account\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Terraform Commands for EKS Cluster Provisioning in Bash\nDESCRIPTION: This snippet shows the basic Terraform CLI commands needed to initialize the working directory, generate the execution plan, and apply the configuration automatically to provision the Amazon EKS clusters with managed node groups. It assumes Terraform is installed and configured. The expected input is the Terraform configuration files in the current directory, and the output is the infrastructure being provisioned on AWS. Users are cautioned about potential costs and advised to run 'terraform destroy' to clean up resources. This snippet requires bash shell environment.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/eks-managed-node-group/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n$ terraform plan\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: EKS Hybrid Node Initialization and Application\nDESCRIPTION: This script initializes Terraform, applies the configuration to provision the remote node VPC, key pair, and generates a key file. It then builds a custom AMI using Packer and subsequently applies the Terraform configuration to create the hybrid node infrastructure. Finally, it executes a script to join the node to the EKS cluster.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/eks-hybrid-nodes/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nterraform init\nterraform apply -target=module.remote_node_vpc -target=local_file.key_pem -target=module.key_pair --auto-approve\ncd ami && packer build -var 'ssh_keypair_name=hybrid-node' -var 'ssh_private_key_file=../key.pem' . && cd -\nterraform apply --auto-approve\n./join.sh\n```\n\n----------------------------------------\n\nTITLE: Extending AWS EKS Cluster and Node Security Group Rules with Terraform HCL\nDESCRIPTION: This snippet demonstrates how to add custom inbound and outbound rules to the cluster security group and node group security groups within an AWS EKS cluster managed via terraform-aws-eks. It uses Terraform HCL syntax to specify additional egress rules for ephemeral TCP ports on the cluster security group and node-to-node ingress and egress rules on the node security group. Dependencies include the terraform-aws-eks module and a configured AWS provider. The key parameters define protocol types, port ranges, traffic directions (ingress/egress), and rule targets (self or node security groups). This enables users to tailor network access policies, facilitating secure and functional node communication and cluster operations.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/network_connectivity.md#_snippet_0\n\nLANGUAGE: HCL\nCODE:\n```\n  ...\n  # Extend cluster security group rules\n  cluster_security_group_additional_rules = {\n    egress_nodes_ephemeral_ports_tcp = {\n      description                = \"To node 1025-65535\"\n      protocol                   = \"tcp\"\n      from_port                  = 1025\n      to_port                    = 65535\n      type                       = \"egress\"\n      source_node_security_group = true\n    }\n  }\n\n  # Extend node-to-node security group rules\n  node_security_group_additional_rules = {\n    ingress_self_all = {\n      description = \"Node to node all ports/protocols\"\n      protocol    = \"-1\"\n      from_port   = 0\n      to_port     = 0\n      type        = \"ingress\"\n      self        = true\n    }\n    egress_all = {\n      description      = \"Node all egress\"\n      protocol         = \"-1\"\n      from_port        = 0\n      to_port          = 0\n      type             = \"egress\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n      ipv6_cidr_blocks = [\"::/0\"]\n    }\n  }\n  ...\n```\n\n----------------------------------------\n\nTITLE: Preserving EKS Cluster State during Upgrade - Terraform HCL\nDESCRIPTION: This snippet provides Terraform configuration variables to preserve the state of your EKS cluster control plane during an upgrade from v17.x to v18.x of the terraform-aws-eks module. It assumes the 'create_iam_role' variable is set to true (the default). You should set the 'prefix_separator', 'iam_role_name', 'cluster_security_group_name', and 'cluster_security_group_description' variables as shown. Required dependency: terraform-aws-modules/terraform-aws-eks v18.x. Set $CLUSTER_NAME as a variable or literal with your actual EKS cluster name. This ensures resource naming is compatible with the new version, minimizing state changes.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-18.0.md#_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\nprefix_separator                   = \"\"\niam_role_name                      = $CLUSTER_NAME\ncluster_security_group_name        = $CLUSTER_NAME\ncluster_security_group_description = \"EKS cluster security group.\"\n\n```\n\n----------------------------------------\n\nTITLE: Attaching IAM Role Policy to All Fargate Profiles (v18.x) - Terraform\nDESCRIPTION: This Terraform resource uses a `for_each` loop to attach an IAM role policy to all Fargate profiles defined in version 18.x of the EKS module. It relies on the `module.eks.fargate_profiles` output and the `each.value.iam_role_name` attribute for determining each role name.  Dependencies include the EKS module and aws_iam_policy resource.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-18.0.md#_snippet_6\n\nLANGUAGE: hcl\nCODE:\n```\n# Attach the policy to all Fargate profiles\nresource \"aws_iam_role_policy_attachment\" \"default\" {\n  for_each = module.eks.fargate_profiles\n\n  role       = each.value.iam_role_name\n  policy_arn = aws_iam_policy.default.arn\n}\n```\n\n----------------------------------------\n\nTITLE: Provisioning EKS Resources with Terraform (Bash)\nDESCRIPTION: Provides the standard sequence of Terraform CLI commands to initialize the working directory, preview the execution plan, and apply the configuration to create AWS EKS resources defined in the module. It also notes the command to destroy the created resources.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/eks-managed-node-group/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n$ terraform plan\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Managed Node Group with Custom AMI and Bootstrap User Data in Terraform\nDESCRIPTION: Configuration for an EKS managed node group that uses a custom AMI with custom bootstrap user data. This example enables the module's bootstrap template and shows how to add pre and post bootstrap user data scripts.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_3\n\nLANGUAGE: HCL\nCODE:\n```\n  eks_managed_node_groups = {\n    custom_ami = {\n      ami_id = \"ami-0caf35bc73450c396\"\n\n      # By default, EKS managed node groups will not append bootstrap script;\n      # this adds it back in using the default template provided by the module\n      # Note: this assumes the AMI provided is an EKS optimized AMI derivative\n      enable_bootstrap_user_data = true\n\n      pre_bootstrap_user_data = <<-EOT\n        export FOO=bar\n      EOT\n\n      # Because we have full control over the user data supplied, we can also run additional\n      # scripts/configuration changes after the bootstrap script has been run\n      post_bootstrap_user_data = <<-EOT\n        echo \"you are free little kubelet!\"\n      EOT\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Defining EKS Node Kubelet Configuration in YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes NodeConfig resource (apiVersion: node.eks.aws/v1alpha1, kind: NodeConfig) to customize Kubelet settings for EKS nodes. It sets the `shutdownGracePeriod` to 30 seconds and enables the `DisableKubeletCloudCredentialProviders` feature gate within the Kubelet configuration. This configuration is typically applied to manage node behavior within an EKS cluster, often provisioned via Terraform.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-additional.txt#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n---\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  kubelet:\n    config:\n      shutdownGracePeriod: 30s\n      featureGates:\n        DisableKubeletCloudCredentialProviders: true\n```\n\n----------------------------------------\n\nTITLE: Configuring aws-auth Module in Terraform AWS EKS (HCL)\nDESCRIPTION: This snippet demonstrates configuring the aws-auth sub-module for EKS using HCL in Terraform v20.0+. The aws_auth_roles and aws_auth_users values, previously managed in the main EKS module, are now passed to the aws-auth sub-module. Dependencies include the terraform-aws-modules/eks/aws module, version ~> 20.0, and managing aws-auth entries via a dedicated resource block. You must provide IAM role and user ARNs for access control, and set manage_aws_auth_configmap to true if you want Terraform to manage the config map.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-20.0.md#_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"eks_aws_auth\" {\n  source  = \"terraform-aws-modules/eks/aws//modules/aws-auth\"\n  version = \"~> 20.0\"\n\n  manage_aws_auth_configmap = true\n\n  aws_auth_roles = [\n    {\n      rolearn  = \"arn:aws:iam::66666666666:role/role1\"\n      username = \"role1\"\n      groups   = [\"custom-role-group\"]\n    },\n  ]\n\n  aws_auth_users = [\n    {\n      userarn  = \"arn:aws:iam::66666666666:user/user1\"\n      username = \"user1\"\n      groups   = [\"custom-users-group\"]\n    },\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Terraform Configuration Example for Creating an EKS Fargate Profile\nDESCRIPTION: This snippet demonstrates the basic usage of the Terraform module to create an EKS Fargate profile named 'separate-fargate-profile' associated with a cluster 'my-cluster'. It specifies subnet IDs and pod selectors, along with tagging options. Dependencies include Terraform AWS provider version >= 5.95 and Terraform version >= 1.3.2.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/fargate-profile/README.md#_snippet_0\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"fargate_profile\" {\n  source = \"terraform-aws-modules/eks/aws//modules/fargate-profile\"\n  \n  name         = \"separate-fargate-profile\"\n  cluster_name = \"my-cluster\"\n  \n  subnet_ids = [\"subnet-abcde012\", \"subnet-bcde012a\", \"subnet-fghi345a\"]\n  selectors = [{\n    namespace = \"kube-system\"\n  }]\n  \n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generic Terraform State Move for Cluster IAM Role Policies\nDESCRIPTION: This shell command provides the template for using `terraform state mv` to update the state for `aws_iam_role_policy_attachment` resources associated with the main EKS cluster role during the v18.x to v19.x upgrade. Replace `<POLICY_ARN>` with the policy ARN and `<POLICY_MAP_KEY>` with the corresponding key from the `iam_role_additional_policies` map.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nterraform state mv 'module.eks.aws_iam_role_policy_attachment.this[\"<POLICY_ARN>\"]' 'module.eks.aws_iam_role_policy_attachment.additional[\"<POLICY_MAP_KEY>\"]'\n```\n\n----------------------------------------\n\nTITLE: Attaching IAM Role Policy to Fargate Profile (v18.x) - Terraform\nDESCRIPTION: This Terraform resource attaches an IAM role policy to a specific Fargate profile using version 18.x of the EKS module. It relies on the `module.eks.fargate_profiles[\"example\"].iam_role_name` output to determine the role name and `aws_iam_policy.default.arn` for the policy ARN. The example attaches to the \"example\" profile. Dependencies include the EKS module and aws_iam_policy resource.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-18.0.md#_snippet_5\n\nLANGUAGE: hcl\nCODE:\n```\n# Attach the policy to an \"example\" Fargate profile\nresource \"aws_iam_role_policy_attachment\" \"default\" {\n  role       = module.eks.fargate_profiles[\"example\"].iam_role_name\n  policy_arn = aws_iam_policy.default.arn\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Kubelet Settings with NodeConfig (YAML)\nDESCRIPTION: Defines a `NodeConfig` resource (v1alpha1) to customize Kubelet settings for EKS nodes. Sets the `shutdownGracePeriod` to 30 seconds and disables cloud credential providers via `featureGates`. This configuration is applied to nodes managed by EKS.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-custom-ami.txt#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  kubelet:\n    config:\n      shutdownGracePeriod: 30s\n      featureGates:\n        DisableKubeletCloudCredentialProviders: true\n```\n\n----------------------------------------\n\nTITLE: Node Configuration for AWS EKS Kubernetes Node - Disable Kubelet Feature Gate (YAML)\nDESCRIPTION: This YAML snippet defines a NodeConfig resource to disable the 'DisableKubeletCloudCredentialProviders' feature gate in Kubernetes by setting it to true in the kubelet configuration. It ensures that node credential providers are not disabled, affecting node authentication behavior. Requires Kubernetes API version 'node.eks.aws/v1alpha1' and dependencies on the AWS EKS custom resource definitions.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/self-mng-bootstrap.txt#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n---\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  kubelet:\n    config:\n      shutdownGracePeriod: 30s\n      featureGates:\n        DisableKubeletCloudCredentialProviders: true\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Managed Node Group without Custom Launch Template in Terraform\nDESCRIPTION: Basic configuration for an EKS managed node group that uses the default launch template provided by AWS EKS managed node group service instead of a custom one. This is done by setting use_custom_launch_template to false.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_0\n\nLANGUAGE: HCL\nCODE:\n```\n  eks_managed_node_groups = {\n    default = {\n      use_custom_launch_template = false\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Example Terraform State Move Command for IAM Policy Attachment\nDESCRIPTION: This shell command demonstrates how to use `terraform state mv` to migrate the state of an `aws_iam_role_policy_attachment` resource when upgrading the EKS module to v19.x. It moves the state corresponding to the example HCL, from the old list-based index `[\"arn:aws:iam::111111111111:policy/ex-complete-additional\"]` to the new map-based key `[\"additional\"]`.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nterraform state mv 'module.eks.aws_iam_role_policy_attachment.this[\"arn:aws:iam::111111111111:policy/ex-complete-additional\"]' 'module.eks.aws_iam_role_policy_attachment.additional[\"additional\"]'\n```\n\n----------------------------------------\n\nTITLE: Removing aws-auth ConfigMap from Terraform State (Shell)\nDESCRIPTION: This shell snippet removes aws-auth Kubernetes ConfigMap resources from the Terraform state (not AWS itself), preparing for migration or removal in EKS clusters. It's essential when transitioning authentication modes and Terraform management style with aws-auth configmap changes. These commands require Terraform 1.0+ installed and come with the risk of orphaning resources if not re-created elsewhere, so ensure backups and plan changes carefully.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-20.0.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nterraform state rm 'module.eks.kubernetes_config_map_v1_data.aws_auth[0]'\nterraform state rm 'module.eks.kubernetes_config_map.aws_auth[0]' # include if Terraform created the original configmap\n```\n\n----------------------------------------\n\nTITLE: Terraform Configuration: Initialize, Plan, Apply\nDESCRIPTION: This snippet provides the commands to initialize, plan, and apply a Terraform configuration. It is the primary set of actions to create the resources defined in the configuration and applies any changes to the infrastructure.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n$ terraform plan\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Initializing and Applying Terraform Configuration for AWS EKS with Karpenter in Bash\nDESCRIPTION: This snippet shows the sequence of Terraform CLI commands required to initialize the working directory, create an execution plan, and apply the configuration automatically. It provisions the EKS cluster and Karpenter resources as defined in the Terraform files. The commands assume Terraform 1.3.2 or later and AWS provider compatibility.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/karpenter/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n$ terraform plan\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Configuring Self Managed Node Group with Default EKS AMI in Terraform\nDESCRIPTION: Basic configuration for a self-managed node group that uses the latest AWS EKS Optimized AMI for the specified Kubernetes version. This example shows how the Kubernetes version specified at the cluster level affects the AMI selection.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_5\n\nLANGUAGE: HCL\nCODE:\n```\n  cluster_version = \"1.31\"\n\n  # This self managed node group will use the latest AWS EKS Optimized AMI for Kubernetes 1.27\n  self_managed_node_groups = {\n    default = {}\n  }\n```\n\n----------------------------------------\n\nTITLE: Diff of Terraform EKS Module Configuration (v18.x vs v19.x)\nDESCRIPTION: This diff illustrates the configuration changes required when upgrading the `terraform-aws-modules/eks/aws` module from version `~> 18.0` to `~> 19.0`. Key changes include updated defaults (e.g., `cluster_endpoint_private_access`, `resolve_conflicts`), restructuring of map/list inputs (e.g., `iam_role_additional_policies`, `cluster_encryption_config`), new parameters (`preserve`, `most_recent` for addons, `timeouts`), and removal/modification of security group and node group parameters.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n module \"eks\" {\n   source  = \"terraform-aws-modules/eks/aws\"\n-  version = \"~> 18.0\"\n+  version = \"~> 19.0\"\n\n  cluster_name                    = local.name\n+ cluster_endpoint_public_access  = true\n- cluster_endpoint_private_access = true # now the default\n\n  cluster_addons = {\n-   resolve_conflicts = \"OVERWRITE\" # now the default\n+   preserve          = true\n+   most_recent       = true\n\n+   timeouts = {\n+     create = \"25m\"\n+     delete = \"10m\"\n    }\n    kube-proxy = {}\n    vpc-cni = {\n-     resolve_conflicts = \"OVERWRITE\" # now the default\n    }\n  }\n\n  # Encryption key\n  create_kms_key = true\n- cluster_encryption_config = [{\n-   resources = [\"secrets\"]\n- }]\n+ cluster_encryption_config = {\n+   resources = [\"secrets\"]\n+ }\n  kms_key_deletion_window_in_days = 7\n  enable_kms_key_rotation         = true\n\n- iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+ iam_role_additional_policies = {\n+   additional = aws_iam_policy.additional.arn\n+ }\n\n  vpc_id                   = module.vpc.vpc_id\n  subnet_ids               = module.vpc.private_subnets\n  control_plane_subnet_ids = module.vpc.intra_subnets\n\n  # Extend node-to-node security group rules\n- node_security_group_ntp_ipv4_cidr_block = [\"169.254.169.123/32\"] # now the default\n  node_security_group_additional_rules = {\n-    ingress_self_ephemeral = {\n-      description = \"Node to node ephemeral ports\"\n-      protocol    = \"tcp\"\n-      from_port   = 0\n-      to_port     = 0\n-      type        = \"ingress\"\n-      self        = true\n-    }\n-    egress_all = {\n-      description      = \"Node all egress\"\n-      protocol         = \"-1\"\n-      from_port        = 0\n-      to_port          = 0\n-      type             = \"egress\"\n-      cidr_blocks      = [\"0.0.0.0/0\"]\n-      ipv6_cidr_blocks = [\"::/0\"]\n-    }\n  }\n\n  # Self-Managed Node Group(s)\n  self_managed_node_group_defaults = {\n    vpc_security_group_ids = [aws_security_group.additional.id]\n-   iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+   iam_role_additional_policies = {\n+     additional = aws_iam_policy.additional.arn\n+   }\n  }\n\n  self_managed_node_groups = {\n    spot = {\n      instance_type = \"m5.large\"\n      instance_market_options = {\n        market_type = \"spot\"\n      }\n\n      pre_bootstrap_user_data = <<-EOT\n        echo \"foo\"\n        export FOO=bar\n      EOT\n\n      bootstrap_extra_args = \"--kubelet-extra-args '--node-labels=node.kubernetes.io/lifecycle=spot'\"\n\n      post_bootstrap_user_data = <<-EOT\n        cd /tmp\n        sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\n        sudo systemctl enable amazon-ssm-agent\n        sudo systemctl start amazon-ssm-agent\n      EOT\n\n-     create_security_group          = true\n-     security_group_name            = \"eks-managed-node-group-complete-example\"\n-     security_group_use_name_prefix = false\n-     security_group_description     = \"EKS managed node group complete example security group\"\n-     security_group_rules = {}\n-     security_group_tags = {}\n    }\n  }\n\n  # EKS Managed Node Group(s)\n  eks_managed_node_group_defaults = {\n    ami_type       = \"AL2_x86_64\"\n    instance_types = [\"m6i.large\", \"m5.large\", \"m5n.large\", \"m5zn.large\"]\n\n    attach_cluster_primary_security_group = true\n    vpc_security_group_ids                = [aws_security_group.additional.id]\n-   iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+   iam_role_additional_policies = {\n+     additional = aws_iam_policy.additional.arn\n+   }\n  }\n\n  eks_managed_node_groups = {\n    blue = {}\n    green = {\n      min_size     = 1\n      max_size     = 10\n      desired_size = 1\n\n      instance_types = [\"t3.large\"]\n      capacity_type  = \"SPOT\"\n      labels = {\n        Environment = \"test\"\n        GithubRepo  = \"terraform-aws-eks\"\n        GithubOrg   = \"terraform-aws-modules\"\n      }\n\n      taints = {\n        dedicated = {\n          key    = \"dedicated\"\n          value  = \"gpuGroup\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n\n      update_config = {\n        max_unavailable_percentage = 33 # or set `max_unavailable`\n      }\n\n-     create_security_group          = true\n-     security_group_name            = \"eks-managed-node-group-complete-example\"\n-     security_group_use_name_prefix = false\n-     security_group_description     = \"EKS managed node group complete example security group\"\n-     security_group_rules = {}\n-     security_group_tags = {}\n\n      tags = {\n        ExtraTag = \"example\"\n      }\n    }\n  }\n\n  # Fargate Profile(s)\n  fargate_profile_defaults = {\n-   iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+   iam_role_additional_policies = {\n+     additional = aws_iam_policy.additional.arn\n+   }\n  }\n\n  fargate_profiles = {\n    default = {\n      name = \"default\"\n      selectors = [\n        {\n          namespace = \"kube-system\"\n          labels = {\n            k8s-app = \"kube-dns\"\n          }\n        },\n        {\n          namespace = \"default\"\n        }\n      ]\n\n      tags = {\n        Owner = \"test\"\n      }\n\n      timeouts = {\n        create = \"20m\"\n        delete = \"20m\"\n      }\n    }\n  }\n\n  # OIDC Identity provider\n  cluster_identity_providers = {\n    cognito = {\n      client_id      = \"702vqsrjicklgb7c5b7b50i1gc\"\n      issuer_url     = \"https://cognito-idp.us-west-2.amazonaws.com/us-west-2_re1u6bpRA\"\n      username_claim = \"email\"\n      groups_claim   = \"cognito:groups\"\n      groups_prefix  = \"gid:\"\n    }\n  }\n\n  # aws-auth configmap\n  manage_aws_auth_configmap = true\n\n  aws_auth_node_iam_role_arns_non_windows = [\n    module.eks_managed_node_group.iam_role_arn,\n    module.self_managed_node_group.iam_role_arn,\n  ]\n  aws_auth_fargate_profile_pod_execution_role_arns = [\n    module.fargate_profile.fargate_profile_pod_execution_role_arn\n  ]\n\n  aws_auth_roles = [\n    {\n      rolearn  = \"arn:aws:iam::66666666666:role/role1\"\n      username = \"role1\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  aws_auth_users = [\n    {\n      userarn  = \"arn:aws:iam::66666666666:user/user1\"\n      username = \"user1\"\n      groups   = [\"system:masters\"]\n    },\n    {\n      userarn  = \"arn:aws:iam::66666666666:user/user2\"\n      username = \"user2\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  aws_auth_accounts = [\n    \"777777777777\",\n    \"888888888888\",\n  ]\n\n  tags = local.tags\n}\n```\n\n----------------------------------------\n\nTITLE: Using Custom User Data Template in EKS Managed Node Group using Terraform (HCL)\nDESCRIPTION: This snippet enables users to specify a custom bootstrap user data template for an EKS Managed Node Group using the user_data_template_path variable. It takes the path to a user-supplied template file, along with optional variables to inject additional scripts and arguments. Dependencies include the terraform-aws-eks module and access to the external script file. Inputs are the path to the template, as well as optional script and argument strings. The output is an EC2 instance initialized using the full custom content from the provided template.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/user_data.md#_snippet_2\n\nLANGUAGE: hcl\nCODE:\n```\nuser_data_template_path  = \"./your/user_data.sh\" # user supplied bootstrap user data template\npre_bootstrap_user_data  = \"...\"\nbootstrap_extra_args     = \"...\"\npost_bootstrap_user_data = \"...\"\n```\n\n----------------------------------------\n\nTITLE: EKS Hybrid Node Role Configuration with SSM in Terraform\nDESCRIPTION: This configuration defines two Terraform modules: one for EKS cluster configuration and another for configuring the IAM role for hybrid nodes using SSM. It specifies the source for both modules and sets the principal ARN of the hybrid node role in the EKS access entries. The hybrid node role module is configured with a name and tags.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/hybrid-node-role/README.md#_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"eks\" {\n  source = \"terraform-aws-modules/eks/aws\"\n\n  ...\n  access_entries = {\n    hybrid-node-role = {\n      principal_arn = module.eks_hybrid_node_role.arn\n      type          = \"HYBRID_LINUX\"\n    }\n  }\n}\n\nmodule \"eks_hybrid_node_role\" {\n  source = \"terraform-aws-modules/eks/aws//modules/hybrid-node-role\"\n\n  name = \"hybrid\"\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving existing node group name using Terraform state show command\nDESCRIPTION: This shell snippet extracts the current node group name from the Terraform state for an EKS node group resource. It requires running `terraform state show` on the specific module resource and then grepping the node_group_name. This helps in configuring the new module version to reuse existing node group names, avoiding recreation issues.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-17.0.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n~ terraform state show 'module.eks.module.node_groups.aws_eks_node_group.workers[\"example\"]' | grep node_group_name\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Cluster Details with NodeConfig (YAML)\nDESCRIPTION: Defines a `NodeConfig` resource (v1alpha1) specifying EKS cluster connection details for nodes. Includes the cluster `name`, `apiServerEndpoint`, base64 encoded `certificateAuthority`, and the cluster `cidr`. This allows nodes to securely connect to the EKS control plane.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-custom-ami.txt#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  cluster:\n    name: ex-user-data\n    apiServerEndpoint: https://012345678903AB2BAE5D1E0BFE0E2B50.gr7.us-east-1.eks.amazonaws.com\n    certificateAuthority: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKbXFqQ1VqNGdGR2w3ZW5PeWthWnZ2RjROOTVOUEZCM2o0cGhVZUsrWGFtN2ZSQnZya0d6OGxKZmZEZWF2b2plTwpQK2xOZFlqdHZncmxCUEpYdHZIZmFzTzYxVzdIZmdWQ2EvamdRM2w3RmkvL1dpQmxFOG9oWUZkdWpjc0s1SXM2CnNkbk5KTTNYUWN2TysrSitkV09NT2ZlNzlsSWdncmdQLzgvRU9CYkw3eUY1aU1hS3lsb1RHL1V3TlhPUWt3ZUcKblBNcjdiUmdkQ1NCZTlXYXowOGdGRmlxV2FOditsTDhsODBTdFZLcWVNVlUxbjQyejVwOVpQRTd4T2l6L0xTNQpYV2lXWkVkT3pMN0xBWGVCS2gzdkhnczFxMkI2d1BKZnZnS1NzWllQRGFpZTloT1NNOUJkNFNPY3JrZTRYSVBOCkVvcXVhMlYrUDRlTWJEQzhMUkVWRDdCdVZDdWdMTldWOTBoL3VJUy9WU2VOcEdUOGVScE5DakszSjc2aFlsWm8KWjNGRG5QWUY0MWpWTHhiOXF0U1ROdEp6amYwWXBEYnFWci9xZzNmQWlxbVorMzd3YWM1eHlqMDZ4cmlaRUgzZgpUM002d2lCUEVHYVlGeWN5TmNYTk5aYW9DWDJVL0N1d2JsUHAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQ==\n    cidr: 192.168.0.0/16\n```\n\n----------------------------------------\n\nTITLE: Provisioning AWS EKS Cluster with Terraform\nDESCRIPTION: This snippet provides the standard command sequence to deploy the AWS EKS infrastructure defined by the Terraform configuration files in this directory. It initializes the Terraform working directory, generates an execution plan, and applies the plan to provision the resources, automatically approving the changes. Requires the Terraform CLI installed and configured with AWS credentials.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/self-managed-node-group/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n$ terraform plan\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Provisioning AWS EKS Self-Managed Node Group with Terraform Bash\nDESCRIPTION: This snippet provides the standard command sequence for provisioning infrastructure using Terraform. It covers initializing the backend and modules, planning the changes, and applying the configuration automatically. It also mentions the command to destroy the created resources.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/self-managed-node-group/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n$ terraform plan\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Initializing and Applying Terraform Configuration for EKS Auto Mode\nDESCRIPTION: These Bash commands initialize the Terraform working directory, generate an execution plan, and apply the configuration changes to provision the AWS EKS cluster and related resources. The '--auto-approve' flag skips interactive approval of the plan.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/eks-auto-mode/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nterraform init\nterraform plan\nterraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Displaying All Pods with Their Assigned Nodes Using kubectl in sh\nDESCRIPTION: This snippet uses kubectl to list all pods across all namespaces and outputs a custom table showing pod names and the nodes they are running on. It helps in examining pod distribution, ensuring that workloads are scheduled appropriately on Karpenter provisioned nodes.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/karpenter/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName\n```\n\n----------------------------------------\n\nTITLE: Updating EKS Module Version Constraint in Terraform\nDESCRIPTION: Illustrates the required change in the Terraform module block to upgrade the `terraform-aws-eks` module source from version constraint `~> 19.21` to `~> 20.0`.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-20.0.md#_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\n module \"eks\" {\n   source  = \"terraform-aws-modules/eks/aws\"\n-  version = \"~> 19.21\"\n+  version = \"~> 20.0\"\n\n```\n\n----------------------------------------\n\nTITLE: Terraform Initialization\nDESCRIPTION: This snippet demonstrates how to initialize, plan, and apply a Terraform configuration. It showcases the basic commands required to provision infrastructure defined in Terraform files, using `terraform init` to initialize the working directory, `terraform plan` to preview the changes, and `terraform apply` to apply the configuration.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/fast-addons/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n$ terraform plan\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Shell Script Output Statement\nDESCRIPTION: This shell script simply outputs the message 'All done' to indicate completion of a process. It has no dependencies and is intended for basic scripting purposes, typically used at the end of automation workflows or setup scripts.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/self-mng-bootstrap.txt#_snippet_2\n\nLANGUAGE: us-ascii\nCODE:\n```\necho \"All done\"\n```\n\n----------------------------------------\n\nTITLE: Terraform Plan\nDESCRIPTION: Creates an execution plan that outlines the changes Terraform will make to the infrastructure. This allows for a preview of the actions before applying them, ensuring no unexpected modifications occur.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/eks-hybrid-nodes/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform plan\n```\n\n----------------------------------------\n\nTITLE: EKS Hybrid Node Role Configuration with IAM Roles Anywhere in Terraform\nDESCRIPTION: This configuration defines two Terraform modules: one for EKS cluster configuration and another for configuring the IAM role for hybrid nodes using IAM Roles Anywhere (IRA). The IRA setup requires enabling the `enable_ira` flag and providing the trust anchor details, including the certificate data. The hybrid node role module is configured with a name, the IRA configuration and tags.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/hybrid-node-role/README.md#_snippet_1\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"eks\" {\n  source = \"terraform-aws-modules/eks/aws\"\n\n  ...\n  access_entries = {\n    hybrid-node-role = {\n      principal_arn = module.eks_hybrid_node_role.arn\n      type          = \"HYBRID_LINUX\"\n    }\n  }\n}\n\nmodule \"eks_hybrid_node_role\" {\n  source = \"terraform-aws-modules/eks/aws//modules/hybrid-node-role\"\n\n  name = \"hybrid-ira\"\n\n  enable_ira = true\n\n  ira_trust_anchor_source_type           = \"CERTIFICATE_BUNDLE\"\n  ira_trust_anchor_x509_certificate_data = <<-EOT\n    MIIFMzCCAxugAwIBAgIRAMnVXU7ncv/+Cl16eJbZ9hswDQYJKoZIhvcNAQELBQAw\n    ...\n    MGx/BMRkrNUVcg3xA0lhECo/olodCkmZo5/mjybbjFQwJzDSKFoW\n  EOT\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Nodes Registered with Karpenter in sh\nDESCRIPTION: Runs a kubectl command to retrieve Kubernetes nodes along with the karpenter.sh/registered label, which indicates nodes provisioned and managed by Karpenter. This information helps verify successful node registration and ready status in the cluster context.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/karpenter/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get nodes -L karpenter.sh/registered\n```\n\n----------------------------------------\n\nTITLE: Deleting Example Deployment to Clean Up Cluster Resources in Bash\nDESCRIPTION: Executes a kubectl command to delete the example deployment named 'inflate' from the cluster, necessary before tearing down resources managed outside Terraform, specifically those created by Karpenter. It ensures safe removal of workloads before destroying infrastructure.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/karpenter/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete deployment inflate\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Cluster Connection for Node in YAML\nDESCRIPTION: NodeConfig resource that sets up the cluster connection details for an EKS node, including cluster name, API server endpoint, certificate authority, and network CIDR. It also configures containerd to avoid discarding unpacked layers.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/self-mng-custom-template.txt#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  cluster:\n    name: ex-user-data\n    apiServerEndpoint: https://012345678903AB2BAE5D1E0BFE0E2B50.gr7.us-east-1.eks.amazonaws.com\n    certificateAuthority: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKbXFqQ1VqNGdGR2w3ZW5PeWthWnZ2RjROOTVOUEZCM2o0cGhVZUsrWGFtN2ZSQnZya0d6OGxKZmZEZWF2b2plTwpQK2xOZFlqdHZncmxCUEpYdHZIZmFzTzYxVzdIZmdWQ2EvamdRM2w3RmkvL1dpQmxFOG9oWUZkdWpjc0s1SXM2CnNkbk5KTTNYUWN2TysrSitkV09NT2ZlNzlsSWdncmdQLzgvRU9CYkw3eUY1aU1hS3lsb1RHL1V3TlhPUWt3ZUcKblBNcjdiUmdkQ1NCZTlXYXowOGdGRmlxV2FOditsTDhsODBTdFZLcWVNVlUxbjQyejVwOVpQRTd4T2l6L0xTNQpYV2lXWkVkT3pMN0xBWGVCS2gzdkhnczFxMkI2d1BKZnZnS1NzWllQRGFpZTloT1NNOUJkNFNPY3JrZTRYSVBOCkVvcXVhMlYrUDRlTWJEQzhMUkVWRDdCdVZDdWdMTldWOTBoL3VJUy9WU2VOcEdUOGVScE5DakszSjc2aFlsWm8KWjNGRG5QWUY0MWpWTHhiOXF0U1ROdEp6amYwWXBEYnFWci9xZzNmQWlxbVorMzd3YWM1eHlqMDZ4cmlaRUgzZgpUM002d2lCUEVHYVlGeWN5TmNYTk5aYW9DWDJVL0N1d2JsUHAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQ==\n    cidr: 192.168.0.0/16\n  containerd:\n    config: |\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n      discard_unpacked_layers = false\n```\n\n----------------------------------------\n\nTITLE: Destroying Terraform-Managed AWS Resources After Karpenter Clean-Up in Bash\nDESCRIPTION: Runs the terraform destroy command with auto-approval to remove all AWS resources created by Terraform for the EKS cluster and associated infrastructure. It should only be executed after deleting Karpenter-managed resources to avoid orphaned cloud resources and unintended costs.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/karpenter/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nterraform destroy --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Provisioning Hybrid Node EKS Cluster Resources - terraform-aws-modules (HCL)\nDESCRIPTION: This example illustrates configuring an EKS cluster with hybrid node support and custom networking using Terraform modules. Dependencies include VPC/subnet setup, the hybrid node role sub-module, and required CIDR assignments. The code defines local IP ranges, hybrid IAM roles, complex security group rules, optional compute config, and hybrid access entries, enabling hybrid Linux nodes and integration with remote networks. All parameterized resources must be adjusted to match your environment; ensure IAM, VPC, and subnet IDs are valid.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/README.md#_snippet_1\n\nLANGUAGE: HCL\nCODE:\n```\nlocals {\n  # RFC 1918 IP ranges supported\n  remote_network_cidr = \"172.16.0.0/16\"\n  remote_node_cidr    = cidrsubnet(local.remote_network_cidr, 2, 0)\n  remote_pod_cidr     = cidrsubnet(local.remote_network_cidr, 2, 1)\n}\n\n# SSM and IAM Roles Anywhere supported - SSM is default\nmodule \"eks_hybrid_node_role\" {\n  source  = \"terraform-aws-modules/eks/aws//modules/hybrid-node-role\"\n  version = \"~> 20.31\"\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 20.31\"\n\n  cluster_name    = \"example\"\n  cluster_version = \"1.31\"\n\n  cluster_addons = {\n    coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n  }\n\n  # Optional\n  cluster_endpoint_public_access = true\n\n  # Optional: Adds the current caller identity as an administrator via cluster access entry\n  enable_cluster_creator_admin_permissions = true\n\n  create_node_security_group = false\n  cluster_security_group_additional_rules = {\n    hybrid-all = {\n      cidr_blocks = [local.remote_network_cidr]\n      description = \"Allow all traffic from remote node/pod network\"\n      from_port   = 0\n      to_port     = 0\n      protocol    = \"all\"\n      type        = \"ingress\"\n    }\n  }\n\n  # Optional\n  cluster_compute_config = {\n    enabled    = true\n    node_pools = [\"system\"]\n  }\n\n  access_entries = {\n    hybrid-node-role = {\n      principal_arn = module.eks_hybrid_node_role.arn\n      type          = \"HYBRID_LINUX\"\n    }\n  }\n\n  vpc_id     = \"vpc-1234556abcdef\"\n  subnet_ids = [\"subnet-abcde012\", \"subnet-bcde012a\", \"subnet-fghi345a\"]\n\n  cluster_remote_network_config = {\n    remote_node_networks = {\n      cidrs = [local.remote_node_cidr]\n    }\n    # Required if running webhooks on Hybrid nodes\n    remote_pod_networks = {\n      cidrs = [local.remote_pod_cidr]\n    }\n  }\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving EKS Add-on Configuration Schema Using AWS CLI\nDESCRIPTION: AWS CLI command to retrieve the configuration schema for a specific EKS add-on. This command allows users to query available configuration options for any EKS add-on by specifying the add-on name and version.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/faq.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\naws eks describe-addon-configuration --addon-name <value> --addon-version <value> --query 'configurationSchema' --output text | jq\n```\n\n----------------------------------------\n\nTITLE: Configuring Self Managed Node Group with Bottlerocket OS in Terraform\nDESCRIPTION: Configuration for a self-managed node group that uses Bottlerocket OS by specifying the BOTTLEROCKET_x86_64 AMI type and providing a Bottlerocket OS AMI ID. This demonstrates how to use alternative operating systems with self-managed nodes.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_6\n\nLANGUAGE: HCL\nCODE:\n```\n  cluster_version = \"1.31\"\n\n  self_managed_node_groups = {\n    bottlerocket = {\n      ami_id   = data.aws_ami.bottlerocket_ami.id\n      ami_type = \"BOTTLEROCKET_x86_64\"\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring an EKS Self Managed Node Group using Terraform\nDESCRIPTION: This HCL code snippet demonstrates how to use the `self_managed_node_group` Terraform module to provision a self-managed node group for an existing AWS EKS cluster. It defines parameters such as the node group name (`name`), target cluster details (`cluster_name`, `cluster_version`, `cluster_endpoint`, `cluster_auth_base64`), networking configuration (`subnet_ids`, `vpc_security_group_ids`), scaling settings (`min_size`, `max_size`, `desired_size`), instance details (`launch_template_name`, `instance_type`), and tags. It requires an existing EKS cluster and associated security group IDs, which are typically obtained from the parent EKS module's outputs.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/self-managed-node-group/README.md#_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"self_managed_node_group\" {\n  source = \"terraform-aws-modules/eks/aws//modules/self-managed-node-group\"\n\n  name                = \"separate-self-mng\"\n  cluster_name        = \"my-cluster\"\n  cluster_version     = \"1.31\"\n  cluster_endpoint    = \"https://012345678903AB2BAE5D1E0BFE0E2B50.gr7.us-east-1.eks.amazonaws.com\"\n  cluster_auth_base64 = \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKbXFqQ1VqNGdGR2w3ZW5PeWthWnZ2RjROOTVOUEZCM2o0cGhVZUsrWGFtN2ZSQnZya0d6OGxKZmZEZWF2b2plTwpQK2xOZFlqdHZncmxCUEpYdHZIZmFzTzYxVzdIZmdWQ2EvamdRM2w3RmkvL1dpQmxFOG9oWUZkdWpjc0s1SXM2CnNkbk5KTTNYUWN2TysrSitkV09NT2ZlNzlsSWdncmdQLzgvRU9CYkw3eUY1aU1hS3lsb1RHL1V3TlhPUWt3ZUcKblBNcjdiUmdkQ1NCZTlXYXowOGdGRmlxV2FOditsTDhsODBTdFZLcWVNVlUxbjQyejVwOVpQRTd4T2l6L0xTNQpYV2lXWkVkT3pMN0xBWGVCS2gzdkhnczFxMkI2d1BKZnZnS1NzWllQRGFpZTloT1NNOUJkNFNPY3JrZTRYSVBOCkVvcXVhMlYrUDRlTWJEQzhMUkVWRDdCdVZDdWdMTldWOTBoL3VJUy9WU2VOcEdUOGVScE5DakszSjc2aFlsWm8KWjNGRG5QWUY0MWpWTHhiOXF0U1ROdEp6amYwWXBEYnFWci9xZzNmQWlxbVorMzd3YWM1eHlqMDZ4cmlaRUgzZgpUM002d2lCUEVHYVlGeWN5TmNYTk5aYW9DWDJVL0N1d2JsUHAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQ==\"\n\n  subnet_ids = [\"subnet-abcde012\", \"subnet-bcde012a\", \"subnet-fghi345a\"]\n\n  // The following variables are necessary if you decide to use the module outside of the parent EKS module context.\n  // Without it, the security groups of the nodes are empty and thus won't join the cluster.\n  vpc_security_group_ids = [\n    module.eks.cluster_primary_security_group_id,\n    module.eks.cluster_security_group_id,\n  ]\n\n  min_size     = 1\n  max_size     = 10\n  desired_size = 1\n\n  launch_template_name   = \"separate-self-mng\"\n  instance_type          = \"m5.large\"\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining EKS Cluster with Managed Nodes and Fargate (v17.x) - Terraform\nDESCRIPTION: This Terraform module configures an EKS cluster using version 17.x of the terraform-aws-modules/eks/aws module. It defines the cluster name, version, VPC, subnets, managed node groups, worker groups, and Fargate profiles. The configuration includes settings for cluster access, node group capacity, instance types, and Fargate namespace selectors. Dependencies include the VPC module and aws_security_group resource.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-18.0.md#_snippet_2\n\nLANGUAGE: hcl\nCODE:\n```\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 17.0\"\n\n  cluster_name                    = local.name\n  cluster_version                 = local.cluster_version\n  cluster_endpoint_private_access = true\n  cluster_endpoint_public_access  = true\n\n  vpc_id  = module.vpc.vpc_id\n  subnets = module.vpc.private_subnets\n\n  # Managed Node Groups\n  node_groups_defaults = {\n    ami_type  = \"AL2_x86_64\"\n    disk_size = 50\n  }\n\n  node_groups = {\n    node_group = {\n      min_capacity     = 1\n      max_capacity     = 10\n      desired_capacity = 1\n\n      instance_types = [\"t3.large\"]\n      capacity_type  = \"SPOT\"\n\n      update_config = {\n        max_unavailable_percentage = 50\n      }\n\n      k8s_labels = {\n        Environment = \"test\"\n        GithubRepo  = \"terraform-aws-eks\"\n        GithubOrg   = \"terraform-aws-modules\"\n      }\n\n      taints = [\n        {\n          key    = \"dedicated\"\n          value  = \"gpuGroup\"\n          effect = \"NO_SCHEDULE\"\n        }\n      ]\n\n      additional_tags = {\n        ExtraTag = \"example\"\n      }\n    }\n  }\n\n  # Worker groups\n  worker_additional_security_group_ids = [aws_security_group.additional.id]\n\n  worker_groups_launch_template = [\n    {\n      name                    = \"worker-group\"\n      override_instance_types = [\"m5.large\", \"m5a.large\", \"m5d.large\", \"m5ad.large\"]\n      spot_instance_pools     = 4\n      asg_max_size            = 5\n      asg_desired_capacity    = 2\n      kubelet_extra_args      = \"--node-labels=node.kubernetes.io/lifecycle=spot\"\n      public_ip               = true\n    },\n  ]\n\n  # Fargate\n  fargate_profiles = {\n    default = {\n      name = \"default\"\n      selectors = [\n        {\n          namespace = \"kube-system\"\n          labels = {\n            k8s-app = \"kube-dns\"\n          }\n        },\n        {\n          namespace = \"default\"\n        }\n      ]\n\n      tags = {\n        Owner = \"test\"\n      }\n\n      timeouts = {\n        create = \"20m\"\n        delete = \"20m\"\n      }\n    }\n  }\n\n  tags = {\n    Environment = \"test\"\n    GithubRepo  = \"terraform-aws-eks\"\n    GithubOrg   = \"terraform-aws-modules\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining EKS Cluster Configuration in NodeConfig (YAML)\nDESCRIPTION: This YAML snippet defines a NodeConfig object specifying the EKS cluster name, API server endpoint, certificate authority, and CIDR block. It also includes a containerd configuration with `discard_unpacked_layers = false`.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-custom-template.txt#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  cluster:\n    name: ex-user-data\n    apiServerEndpoint: https://012345678903AB2BAE5D1E0BFE0E2B50.gr7.us-east-1.eks.amazonaws.com\n    certificateAuthority: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKbXFqQ1VqNGdGR2w3ZW5PeWthWnZ2RjROOTVOUEZCM2o0cGhVZUsrWGFtN2ZSQnZya0d6OGxKZmZEZWF2b2plTwpQK2xOZFlqdHZncmxCUEpYdHZIZmFzTzYxVzdIZmdWQ2EvamdRM2w3RmkvL1dpQmxFOG9oWUZkdWpjc0s1SXM2CnNkbk5KTTNYUWN2TysrSitkV09NT2ZlNzlsSWdncmdQLzgvRU9CYkw3eUY1aU1hS3lsb1RHL1V3TlhPUWt3ZUcKblBNcjdiUmdkQ1NCZTlXYXowOGdGRmlxV2FOditsTDhsODBTdFZLcWVNVlUxbjQyejVwOVpQRTd4T2l6L0xTNQpYV2lXWkVkT3pMN0xBWGVCS2gzdkhnczFxMkI2d1BKZnZnS1NzWllQRGFpZTloT1NNOUJkNFNPY3JrZTRYSVBOCkVvcXVhMlYrUDRlTWJEQzhMUkVWRDdCdVZDdWdMTldWOTBoL3VJUy9WU2VOcEdUOGVScE5DakszSjc2aFlsWm8KWjNGRG5QWUY0MWpWTHhiOXF0U1ROdEp6amYwWXBEYnFWci9xZzNmQWlxbVorMzd3YWM1eHlqMDZ4cmlaRUgzZgpUM002d2lCUEVHYVlGeWN5TmNYTk5aYW9DWDJVL0N1d2JsUHAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQ==\n    cidr: 192.168.0.0/16\n  containerd:\n    config: |\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n      discard_unpacked_layers = false\n```\n\n----------------------------------------\n\nTITLE: Terraform Apply with Auto-Approval\nDESCRIPTION: Applies the changes defined in the Terraform configuration to the infrastructure. The `--auto-approve` flag bypasses the interactive approval prompt, automatically applying the changes.  Use with caution.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/eks-hybrid-nodes/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform apply --auto-approve\n```\n\n----------------------------------------\n\nTITLE: Terraform Destroy\nDESCRIPTION: Destroys all the resources managed by the Terraform configuration. This command is used to remove the deployed infrastructure and prevent incurring further costs. Run this when the resources are no longer needed.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/eks-hybrid-nodes/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform destroy\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Managed Node Group with Bottlerocket OS in Terraform\nDESCRIPTION: Configuration for an EKS managed node group that uses Bottlerocket OS by specifying the BOTTLEROCKET_x86_64 AMI type. This example disables the custom launch template to use the AWS-provided one.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_1\n\nLANGUAGE: HCL\nCODE:\n```\n  eks_managed_node_groups = {\n    bottlerocket_default = {\n      use_custom_launch_template = false\n\n      ami_type = \"BOTTLEROCKET_x86_64\"\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: CoreDNS Add-on Configuration Schema JSON Structure\nDESCRIPTION: The JSON schema defining all available configuration options for the CoreDNS add-on. This schema details configurable parameters including affinity settings, resource requirements, replica count, and pod configurations for customizing the CoreDNS deployment.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/faq.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"$ref\": \"#/definitions/Coredns\",\n  \"$schema\": \"http://json-schema.org/draft-06/schema#\",\n  \"definitions\": {\n    \"Coredns\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"affinity\": {\n          \"default\": {\n            \"affinity\": {\n              \"nodeAffinity\": {\n                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n                  \"nodeSelectorTerms\": [\n                    {\n                      \"matchExpressions\": [\n                        {\n                          \"key\": \"kubernetes.io/os\",\n                          \"operator\": \"In\",\n                          \"values\": [\n                            \"linux\"\n                          ]\n                        },\n                        {\n                          \"key\": \"kubernetes.io/arch\",\n                          \"operator\": \"In\",\n                          \"values\": [\n                            \"amd64\",\n                            \"arm64\"\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                }\n              },\n              \"podAntiAffinity\": {\n                \"preferredDuringSchedulingIgnoredDuringExecution\": [\n                  {\n                    \"podAffinityTerm\": {\n                      \"labelSelector\": {\n                        \"matchExpressions\": [\n                          {\n                            \"key\": \"k8s-app\",\n                            \"operator\": \"In\",\n                            \"values\": [\n                              \"kube-dns\"\n                            ]\n                          }\n                        ]\n                      },\n                      \"topologyKey\": \"kubernetes.io/hostname\"\n                    },\n                    \"weight\": 100\n                  }\n                ]\n              }\n            }\n          },\n          \"description\": \"Affinity of the coredns pods\",\n          \"type\": [\n            \"object\",\n            \"null\"\n          ]\n        },\n        \"computeType\": {\n          \"type\": \"string\"\n        },\n        \"corefile\": {\n          \"description\": \"Entire corefile contents to use with installation\",\n          \"type\": \"string\"\n        },\n        \"nodeSelector\": {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        \"podAnnotations\": {\n          \"properties\": {},\n          \"title\": \"The podAnnotations Schema\",\n          \"type\": \"object\"\n        },\n        \"podDisruptionBudget\": {\n          \"description\": \"podDisruptionBudget configurations\",\n          \"enabled\": {\n            \"default\": true,\n            \"description\": \"the option to enable managed PDB\",\n            \"type\": \"boolean\"\n          },\n          \"maxUnavailable\": {\n            \"anyOf\": [\n              {\n                \"pattern\": \".*%$\",\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"integer\"\n              }\n            ],\n            \"default\": 1,\n            \"description\": \"minAvailable value for managed PDB, can be either string or integer; if it's string, should end with %\"\n          },\n          \"minAvailable\": {\n            \"anyOf\": [\n              {\n                \"pattern\": \".*%$\",\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"integer\"\n              }\n            ],\n            \"description\": \"maxUnavailable value for managed PDB, can be either string or integer; if it's string, should end with %\"\n          },\n          \"type\": \"object\"\n        },\n        \"podLabels\": {\n          \"properties\": {},\n          \"title\": \"The podLabels Schema\",\n          \"type\": \"object\"\n        },\n        \"replicaCount\": {\n          \"type\": \"integer\"\n        },\n        \"resources\": {\n          \"$ref\": \"#/definitions/Resources\"\n        },\n        \"tolerations\": {\n          \"default\": [\n            {\n              \"key\": \"CriticalAddonsOnly\",\n              \"operator\": \"Exists\"\n            },\n            {\n              \"effect\": \"NoSchedule\",\n              \"key\": \"node-role.kubernetes.io/control-plane\"\n            }\n          ],\n          \"description\": \"Tolerations of the coredns pod\",\n          \"items\": {\n            \"type\": \"object\"\n          },\n          \"type\": \"array\"\n        },\n        \"topologySpreadConstraints\": {\n          \"description\": \"The coredns pod topology spread constraints\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"Coredns\",\n      \"type\": \"object\"\n    },\n    \"Limits\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"cpu\": {\n          \"type\": \"string\"\n        },\n        \"memory\": {\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"Limits\",\n      \"type\": \"object\"\n    },\n    \"Resources\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"limits\": {\n          \"$ref\": \"#/definitions/Limits\"\n        },\n        \"requests\": {\n          \"$ref\": \"#/definitions/Limits\"\n        }\n      },\n      \"title\": \"Resources\",\n      \"type\": \"object\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing a Simple Shell Command\nDESCRIPTION: A basic shell script that prints 'All done' to standard output. This is often included in multipart user data scripts to signal the completion of a setup or configuration sequence on an EC2 instance, such as an EKS node.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-custom-ami.txt#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\necho \"All done\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Parameters for EKS Node in YAML\nDESCRIPTION: This YAML snippet defines a NodeConfig object that configures kubelet parameters for an AWS EKS node. It sets the shutdown grace period to 30 seconds and disables Kubelet Cloud Credential Providers.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-custom-template.txt#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  kubelet:\n    config:\n      shutdownGracePeriod: 30s\n      featureGates:\n        DisableKubeletCloudCredentialProviders: true\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Cluster in Auto Mode - terraform-aws-modules (HCL)\nDESCRIPTION: This snippet demonstrates setting up a basic EKS cluster using the terraform-aws-modules/eks/aws module with recommended parameters. It requires Terraform and AWS credentials configured, and the module sourced from the Terraform Registry. Key variables include cluster name, version, VPC ID, and subnet IDs, with options for public endpoint access, administrative permissions, and compute resource configuration. The output is a managed EKS cluster with specified tags and networking setup; sensitive values like IDs should be supplied for your environment.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/README.md#_snippet_0\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 20.31\"\n\n  cluster_name    = \"example\"\n  cluster_version = \"1.31\"\n\n  # Optional\n  cluster_endpoint_public_access = true\n\n  # Optional: Adds the current caller identity as an administrator via cluster access entry\n  enable_cluster_creator_admin_permissions = true\n\n  cluster_compute_config = {\n    enabled    = true\n    node_pools = [\"general-purpose\"]\n  }\n\n  vpc_id     = \"vpc-1234556abcdef\"\n  subnet_ids = [\"subnet-abcde012\", \"subnet-bcde012a\", \"subnet-fghi345a\"]\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Terraform Initialization\nDESCRIPTION: Initializes the Terraform working directory by downloading the necessary plugins and modules specified in the configuration files. This command prepares Terraform to create, modify, or destroy infrastructure.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/eks-hybrid-nodes/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ terraform init\n```\n\n----------------------------------------\n\nTITLE: Generic Terraform State Move for Fargate Profile Policies\nDESCRIPTION: This shell command template guides the use of `terraform state mv` for updating the state of `aws_iam_role_policy_attachment` resources related to Fargate Profile roles when upgrading from v18.x to v19.x. Replace `<FARGATE_PROFILE_KEY>` with the key from `fargate_profiles`, `<POLICY_ARN>` with the policy ARN, and `<POLICY_MAP_KEY>` with the key from `iam_role_additional_policies` accordingly.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nterraform state mv 'module.eks.module.fargate_profile[\"<FARGATE_PROFILE_KEY>\"].aws_iam_role_policy_attachment.this[\"<POLICY_ARN>\"]' 'module.eks.module.fargate_profile[\"<FARGATE_PROFILE_KEY>\"].aws_iam_role_policy_attachment.additional[\"<POLICY_MAP_KEY>\"]'\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Managed Node Group with Custom Bottlerocket AMI in Terraform\nDESCRIPTION: Configuration for an EKS managed node group that uses a custom Bottlerocket AMI with custom bootstrap settings. This example shows how to add kernel settings, node labels, and node taints to the Bottlerocket configuration.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_4\n\nLANGUAGE: HCL\nCODE:\n```\n  eks_managed_node_groups = {\n    bottlerocket_custom_ami = {\n      ami_id   = \"ami-0ff61e0bcfc81dc94\"\n      ami_type = \"BOTTLEROCKET_x86_64\"\n\n      # use module user data template to bootstrap\n      enable_bootstrap_user_data = true\n      # this will get added to the template\n      bootstrap_extra_args = <<-EOT\n        # extra args added\n        [settings.kernel]\n        lockdown = \"integrity\"\n\n        [settings.kubernetes.node-labels]\n        \"label1\" = \"foo\"\n        \"label2\" = \"bar\"\n\n        [settings.kubernetes.node-taints]\n        \"dedicated\" = \"experimental:PreferNoSchedule\"\n        \"special\" = \"true:NoSchedule\"\n      EOT\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Setting USE_MAX_PODS Environment Variable - Shell\nDESCRIPTION: This shell script sets the `USE_MAX_PODS` environment variable to `false`. This variable likely controls the configuration of the maximum number of pods allowed in a Kubernetes cluster managed by the EKS module. There are no dependencies for this snippet. The output of this command is setting of the environment variable for the subsequent processes.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2/eks-mng-additional.txt#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nexport USE_MAX_PODS=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Pre-Bootstrap User Data in EKS Managed Node Group using Terraform (HCL)\nDESCRIPTION: This snippet demonstrates how to supply additional custom user data that is prepended to the bootstrap user data generated by the AWS EKS Managed Node Group service. No special dependencies are required other than the terraform-aws-eks module. The variable \"pre_bootstrap_user_data\" accepts a string containing the user data script or content to be injected. The expected output is the EC2 node receiving the user-supplied user data followed by the standard EKS bootstrap, and it is valid only if you do not provide a custom AMI ID.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/user_data.md#_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\npre_bootstrap_user_data = \"...\"\n```\n\n----------------------------------------\n\nTITLE: Complete AWS EKS Node Configuration with Cluster Access Details (YAML)\nDESCRIPTION: This YAML snippet defines a NodeConfig resource for AWS EKS including cluster connection details such as cluster name, API server endpoint, and CA certificate. It establishes cluster access parameters for the node, specifying network CIDR and secure communication settings. It depends on the same custom resource version as the first snippet and requires the node.eks.aws API.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/self-mng-bootstrap.txt#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n---\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  cluster:\n    name: ex-user-data\n    apiServerEndpoint: https://012345678903AB2BAE5D1E0BFE0E2B50.gr7.us-east-1.eks.amazonaws.com\n    certificateAuthority: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKbXFqQ1VqNGdGR2w3ZW5PeWthWnZ2RjROOTVOUEZCM2o0cGhVZUsrWGFtN2ZSQnZya0d6OGxKZmZEZWF2b2plTwpQK2xOZFlqdHZncmxCUEpYdHZIZmFzTzYxVzdIZmdWQ2EvamdRM2w3RmkvL1dpQmxFOG9oWUZkdWpjc0s1SXM2CnNkbk5KTTNYUWN2TysrSitkV09NT2ZlNzlsSWdncmdQLzgvRU9CYkw3eUY1aU1hS3lsb1RHL1V3TlhPUWt3ZUcKblBNcjdiUmdkQ1NCZTlXYXowOGdGRmlxV2FOditsTDhsODBTdFZLcWVNVlUxbjQyejVwOVpQRTd4T2l6L0xTNQpYV2lXWkVkT3pMN0xBWGVCS2gzdkhnczFxMkI2d1BKZnZnS1NzWllQRGFpZTloT1NNOUJkNFNPY3JrZTRYSVBOCkVvcXVhMlYrUDRlTWJEQzhMUkVWRDdCdVZDdWdMTldWOTBoL3VJUy9WU2VOcEdUOGVScE5DakszSjc2aFlsWm8KWjNGRG5QWUY0MWpWTHhiOXF0U1ROdEp6amYwWXBEYnFWci9xZzNmQWlxbVorMzd3YWM1eHlqMDZ4cmlaRUgzZgpUM002d2lCUEVHYVlGeWN5TmNYTk5aYW9DWDJVL0N1d2JsUHAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQ==\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Managed Node Group with Bottlerocket OS and Custom User Data in Terraform\nDESCRIPTION: Configuration for an EKS managed node group with Bottlerocket OS that includes custom bootstrap arguments in TOML format. This example shows how to add kernel settings to the Bottlerocket configuration.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/compute_resources.md#_snippet_2\n\nLANGUAGE: HCL\nCODE:\n```\n  eks_managed_node_groups = {\n    bottlerocket_prepend_userdata = {\n      ami_type = \"BOTTLEROCKET_x86_64\"\n\n      bootstrap_extra_args = <<-EOT\n        # extra args added\n        [settings.kernel]\n        lockdown = \"integrity\"\n      EOT\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Generic Terraform State Move for Self-Managed Node Group Policies\nDESCRIPTION: Use this shell command template with `terraform state mv` to update state for `aws_iam_role_policy_attachment` resources on Self-Managed Node Group roles during the v18.x to v19.x upgrade. Substitute `<NODE_GROUP_KEY>` with the key from `self_managed_node_groups`, `<POLICY_ARN>` with the policy ARN, and `<POLICY_MAP_KEY>` with the key from `iam_role_additional_policies`.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-19.0.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nterraform state mv 'module.eks.module.self_managed_node_group[\"<NODE_GROUP_KEY>\"].aws_iam_role_policy_attachment.this[\"<POLICY_ARN>\"]' 'module.eks.module.self_managed_node_group[\"<NODE_GROUP_KEY>\"].aws_iam_role_policy_attachment.additional[\"<POLICY_MAP_KEY>\"]'\n```\n\n----------------------------------------\n\nTITLE: Enabling EFA Support in Terraform AWS EKS Module Using HCL\nDESCRIPTION: This Terraform HCL snippet demonstrates how to enable EFA support at both the EKS cluster and EKS managed node group levels using the terraform-aws-eks module. The configuration sets `enable_efa_support = true` at the cluster level to add required security group ingress and egress rules, and within the managed node group to expose all supported EFA interfaces on the launch template and create a clustered placement group. Parameters include specifying an NVIDIA GPU AMI with pre-installed EFA components, defining high-performance instance types such as `p5.48xlarge`, and adjusting node group sizing to meet EFA requirements (minimum two nodes). Additionally, a cloud-init configuration snippet mounts instance store volumes in RAID-0 to optimize storage for Kubernetes components. Dependencies include using the custom launch template provided by the module, specifying compatible instance types, and deploying the aws-efa-k8s-device-plugin Helm chart to enable EFA devices within pods. This snippet assumes knowledge of Terraform module usage and basic EKS cluster/node group concepts.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/README.md#_snippet_4\n\nLANGUAGE: HCL\nCODE:\n```\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 20.0\"\n\n  # Truncated for brevity ...\n\n  # Adds the EFA required security group rules to the shared\n  # security group created for the node group(s)\n  enable_efa_support = true\n\n  eks_managed_node_groups = {\n    example = {\n      # The EKS AL2023 NVIDIA AMI provides all of the necessary components\n      # for accelerated workloads w/ EFA\n      ami_type       = \"AL2023_x86_64_NVIDIA\"\n      instance_types = [\"p5.48xlarge\"]\n\n      # Exposes all EFA interfaces on the launch template created by the node group(s)\n      # This would expose all 32 EFA interfaces for the p5.48xlarge instance type\n      enable_efa_support = true\n\n      # Mount instance store volumes in RAID-0 for kubelet and containerd\n      # https://github.com/awslabs/amazon-eks-ami/blob/master/doc/USER_GUIDE.md#raid-0-for-kubelet-and-containerd-raid0\n      cloudinit_pre_nodeadm = [\n        {\n          content_type = \"application/node.eks.aws\"\n          content      = <<-EOT\n            ---\n            apiVersion: node.eks.aws/v1alpha1\n            kind: NodeConfig\n            spec:\n              instance:\n                localStorage:\n                  strategy: RAID0\n          EOT\n        }\n      ]\n\n      # EFA should only be enabled when connecting 2 or more nodes\n      # Do not use EFA on a single node workload\n      min_size     = 2\n      max_size     = 10\n      desired_size = 2\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Settings for EKS Node in YAML\nDESCRIPTION: NodeConfig resource that configures kubelet settings for an EKS node, enabling shutdown grace period and disabling cloud credential providers via feature gates.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/self-mng-custom-template.txt#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: node.eks.aws/v1alpha1\nkind: NodeConfig\nspec:\n  kubelet:\n    config:\n      shutdownGracePeriod: 30s\n      featureGates:\n        DisableKubeletCloudCredentialProviders: true\n```\n\n----------------------------------------\n\nTITLE: Retrieving CoreDNS Add-on Configuration Schema Example\nDESCRIPTION: A practical example of retrieving configuration schema for CoreDNS add-on version v1.11.1-eksbuild.8. This command demonstrates how to get the specific configuration options available for the CoreDNS add-on.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/faq.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\naws eks describe-addon-configuration --addon-name coredns --addon-version v1.11.1-eksbuild.8 --query 'configurationSchema' --output text | jq\n```\n\n----------------------------------------\n\nTITLE: Simple Shell Script for User Data\nDESCRIPTION: This shell script snippet simply echoes \"All done\" to the console.  It's a basic example of user data that can be executed during the initialization of an AWS instance.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/eks-mng-custom-template.txt#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\necho \"All done\"\n```\n\n----------------------------------------\n\nTITLE: Terraform plan output showing destruction of random_pet resource during upgrade\nDESCRIPTION: This shell snippet depicts the output of `terraform plan` indicating that only the `random_pet` resource associated with the node group will be destroyed. It confirms that the existing node group names are reused appropriately, and only the replaceable resources are scheduled for destruction, maintaining cluster integrity. The output details the resource references and destruction actions.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/UPGRADE-17.0.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nTerraform will perform the following actions:\n\n  # module.eks.module.node_groups.random_pet.node_groups[\"example\"] will be destroyed\n  - resource \"random_pet\" \"node_groups\" {\n      - id        = \"sincere-squid\" -> null\n      - keepers   = {\n          - \"ami_type\"                  = \"AL2_x86_64\"\n          - \"capacity_type\"             = \"SPOT\"\n          - \"disk_size\"                 = \"50\"\n          - \"iam_role_arn\"              = \"arn:aws:iam::123456789123:role/test-eks-mwIwsvui20210527220853611600000009\"\n          - \"instance_types\"            = \"t3.large\"\n          - \"key_name\"                  = \"\"\n          - \"node_group_name\"           = \"test-eks-mwIwsvui-example\"\n          - \"source_security_group_ids\" = \"\"\n          - \"subnet_ids\"                = \"subnet-xxxxxxxxxxxx|subnet-xxxxxxxxxxxx|subnet-xxxxxxxxxxxx\"\n        } -> null\n      - length    = 2 -> null\n      - separator = \"-\" -> null\n    }\n\nPlan: 0 to add, 0 to change, 1 to destroy.\n```\n\n----------------------------------------\n\nTITLE: Default Tag Specifications for Launched Resources\nDESCRIPTION: This snippet defines the default value for the `tag_specifications` input variable. It specifies that tags defined in the `tags` map should be applied to EC2 instances, EBS volumes, and network interfaces created by the Autoscaling Group by default.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/self-managed-node-group/README.md#_snippet_2\n\nLANGUAGE: Terraform\nCODE:\n```\n[\n  \"instance\",\n  \"volume\",\n  \"network-interface\"\n]\n```\n\n----------------------------------------\n\nTITLE: Input Variable Descriptions for Terraform AWS EKS Module\nDESCRIPTION: This section defines input variables for configuring an Amazon EKS cluster and associated resources using Terraform. Each variable includes a description, type, default value, and whether it is required, enabling flexible and customizable deployment of the EKS infrastructure.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/modules/self-managed-node-group/README.md#_snippet_1\n\nLANGUAGE: HCL\nCODE:\n```\n| variable definitions for each input parameter, detailing their purpose, data type, default values, and necessity |\n\n```\n\n----------------------------------------\n\nTITLE: Simple Completion Shell Script for EKS Node\nDESCRIPTION: A basic shell script that outputs a confirmation message when execution is complete. This script would run after the node configuration is applied.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/tests/user-data/rendered/al2023/self-mng-custom-template.txt#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\necho \"All done\"\n```\n\n----------------------------------------\n\nTITLE: Serving MkDocs Documentation Locally Using Bash\nDESCRIPTION: This snippet demonstrates how to launch the documentation site for the terraform-aws-eks project using MkDocs in a Bash shell. It requires Python 3.x, MkDocs, and the pip packages mkdocs-material, mkdocs-include-markdown-plugin, and mkdocs-awesome-pages-plugin to be installed. Running this command from the repository root starts a local server, typically available at http://127.0.0.1:8000/terraform-aws-eks/.\nSOURCE: https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/local.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdocs serve\n\n```"
  }
]
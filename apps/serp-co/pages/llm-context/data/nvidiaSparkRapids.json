[
  {
    "owner": "nvidia",
    "repo": "spark-rapids",
    "content": "TITLE: Extracting Performance and Acquisition Data from Raw Mortgage DataFrames in PySpark\nDESCRIPTION: Defines two functions for extracting and transforming data from raw mortgage dataframes. The extract_perf_columns function processes loan performance data with date formatting, while extract_acq_columns handles loan acquisition data and adds ranking to identify first records for each loan.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef extract_perf_columns(rawDf):\n    perfDf = rawDf.select(\n      col(\"loan_id\"),\n      date_format(to_date(col(\"monthly_reporting_period\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"monthly_reporting_period\"),\n      upper(col(\"servicer\")).alias(\"servicer\"),\n      col(\"interest_rate\"),\n      col(\"current_actual_upb\"),\n      col(\"loan_age\"),\n      col(\"remaining_months_to_legal_maturity\"),\n      col(\"adj_remaining_months_to_maturity\"),\n      date_format(to_date(col(\"maturity_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"maturity_date\"),\n      col(\"msa\"),\n      col(\"current_loan_delinquency_status\"),\n      col(\"mod_flag\"),\n      col(\"zero_balance_code\"),\n      date_format(to_date(col(\"zero_balance_effective_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"zero_balance_effective_date\"),\n      date_format(to_date(col(\"last_paid_installment_date\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"last_paid_installment_date\"),\n      date_format(to_date(col(\"foreclosed_after\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"foreclosed_after\"),\n      date_format(to_date(col(\"disposition_date\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"disposition_date\"),\n      col(\"foreclosure_costs\"),\n      col(\"prop_preservation_and_repair_costs\"),\n      col(\"asset_recovery_costs\"),\n      col(\"misc_holding_expenses\"),\n      col(\"holding_taxes\"),\n      col(\"net_sale_proceeds\"),\n      col(\"credit_enhancement_proceeds\"),\n      col(\"repurchase_make_whole_proceeds\"),\n      col(\"other_foreclosure_proceeds\"),\n      col(\"non_interest_bearing_upb\"),\n      col(\"principal_forgiveness_upb\"),\n      col(\"repurchase_make_whole_proceeds_flag\"),\n      col(\"foreclosure_principal_write_off_amount\"),\n      col(\"servicing_activity_indicator\"),\n      col('quarter')\n    )\n\n    return perfDf.select(\"*\").filter(\"current_actual_upb != 0.0\")\n\ndef extract_acq_columns(rawDf):\n    acqDf = rawDf.select(\n      col(\"loan_id\"),\n      col(\"orig_channel\"),\n      upper(col(\"seller_name\")).alias(\"seller_name\"),\n      col(\"orig_interest_rate\"),\n      col(\"orig_upb\"),\n      col(\"orig_loan_term\"),\n      date_format(to_date(col(\"orig_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"orig_date\"),\n      date_format(to_date(col(\"first_pay_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"first_pay_date\"),\n      col(\"orig_ltv\"),\n      col(\"orig_cltv\"),\n      col(\"num_borrowers\"),\n      col(\"dti\"),\n      col(\"borrower_credit_score\"),\n      col(\"first_home_buyer\"),\n      col(\"loan_purpose\"),\n      col(\"property_type\"),\n      col(\"num_units\"),\n      col(\"occupancy_status\"),\n      col(\"property_state\"),\n      col(\"zip\"),\n      col(\"mortgage_insurance_percent\"),\n      col(\"product_type\"),\n      col(\"coborrow_credit_score\"),\n      col(\"mortgage_insurance_type\"),\n      col(\"relocation_mortgage_indicator\"),\n      dense_rank().over(Window.partitionBy(\"loan_id\").orderBy(to_date(col(\"monthly_reporting_period\"),\"MMyyyy\"))).alias(\"rank\"),\n      col('quarter')\n      )\n\n    return acqDf.select(\"*\").filter(col(\"rank\")==1)\n```\n\n----------------------------------------\n\nTITLE: Processing Mortgage Data with PySpark\nDESCRIPTION: Main processing script that runs a mortgage data function, saves the results as Parquet, and optionally creates train/eval datasets. The script times the execution and displays the query execution plan before stopping the Spark session.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# run main function to process data\nout = run_mortgage(spark, perf, acq)\n\n# save processed data\nout.write.parquet(output_path, mode='overwrite')\n\n# save processed data\nif save_train_eval_dataset:\n    etlDf = spark.read.parquet(output_path)\n\n    # split 80% for training, 20% for test\n    splits = etlDf.randomSplit([0.8, 0.2])\n\n    splits[0].write.format('parquet').save(output_path_train, mode=\"overwrite\")\n    splits[1].write.format('parquet').save(output_path_eval, mode=\"overwrite\")\n\n# print explain and time\nprint(out.explain())\nend = time.time()\nprint(end - start)\nspark.stop()\n```\n\n----------------------------------------\n\nTITLE: Defining CSV Schema for Mortgage Performance Data\nDESCRIPTION: Creates a comprehensive schema definition for the mortgage data CSV files using PySpark's StructType. The schema specifies column names, data types, and structure for the Fannie Mae loan performance dataset, capturing over 100 financial and performance metrics for each loan.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# File schema\n_csv_raw_schema = StructType([\n      StructField(\"reference_pool_id\", StringType()),\n      StructField(\"loan_id\", LongType()),\n      StructField(\"monthly_reporting_period\", StringType()),\n      StructField(\"orig_channel\", StringType()),\n      StructField(\"seller_name\", StringType()),\n      StructField(\"servicer\", StringType()),\n      StructField(\"master_servicer\", StringType()),\n      StructField(\"orig_interest_rate\", DoubleType()),\n      StructField(\"interest_rate\", DoubleType()),\n      StructField(\"orig_upb\", DoubleType()),\n      StructField(\"upb_at_issuance\", StringType()),\n      StructField(\"current_actual_upb\", DoubleType()),\n      StructField(\"orig_loan_term\", IntegerType()),\n      StructField(\"orig_date\", StringType()),\n      StructField(\"first_pay_date\", StringType()),    \n      StructField(\"loan_age\", DoubleType()),\n      StructField(\"remaining_months_to_legal_maturity\", DoubleType()),\n      StructField(\"adj_remaining_months_to_maturity\", DoubleType()),\n      StructField(\"maturity_date\", StringType()),\n      StructField(\"orig_ltv\", DoubleType()),\n      StructField(\"orig_cltv\", DoubleType()),\n      StructField(\"num_borrowers\", DoubleType()),\n      StructField(\"dti\", DoubleType()),\n      StructField(\"borrower_credit_score\", DoubleType()),\n      StructField(\"coborrow_credit_score\", DoubleType()),\n      StructField(\"first_home_buyer\", StringType()),\n      StructField(\"loan_purpose\", StringType()),\n      StructField(\"property_type\", StringType()),\n      StructField(\"num_units\", IntegerType()),\n      StructField(\"occupancy_status\", StringType()),\n      StructField(\"property_state\", StringType()),\n      StructField(\"msa\", DoubleType()),\n      StructField(\"zip\", IntegerType()),\n      StructField(\"mortgage_insurance_percent\", DoubleType()),\n      StructField(\"product_type\", StringType()),\n      StructField(\"prepayment_penalty_indicator\", StringType()),\n      StructField(\"interest_only_loan_indicator\", StringType()),\n      StructField(\"interest_only_first_principal_and_interest_payment_date\", StringType()),\n      StructField(\"months_to_amortization\", StringType()),\n      StructField(\"current_loan_delinquency_status\", IntegerType()),\n      StructField(\"loan_payment_history\", StringType()),\n      StructField(\"mod_flag\", StringType()),\n      StructField(\"mortgage_insurance_cancellation_indicator\", StringType()),\n      StructField(\"zero_balance_code\", StringType()),\n      StructField(\"zero_balance_effective_date\", StringType()),\n      StructField(\"upb_at_the_time_of_removal\", StringType()),\n      StructField(\"repurchase_date\", StringType()),\n      StructField(\"scheduled_principal_current\", StringType()),\n      StructField(\"total_principal_current\", StringType()),\n      StructField(\"unscheduled_principal_current\", StringType()),\n      StructField(\"last_paid_installment_date\", StringType()),\n      StructField(\"foreclosed_after\", StringType()),\n      StructField(\"disposition_date\", StringType()),\n      StructField(\"foreclosure_costs\", DoubleType()),\n      StructField(\"prop_preservation_and_repair_costs\", DoubleType()),\n      StructField(\"asset_recovery_costs\", DoubleType()),\n      StructField(\"misc_holding_expenses\", DoubleType()),\n      StructField(\"holding_taxes\", DoubleType()),\n      StructField(\"net_sale_proceeds\", DoubleType()),\n      StructField(\"credit_enhancement_proceeds\", DoubleType()),\n      StructField(\"repurchase_make_whole_proceeds\", StringType()),\n      StructField(\"other_foreclosure_proceeds\", DoubleType()),\n      StructField(\"non_interest_bearing_upb\", DoubleType()),\n      StructField(\"principal_forgiveness_upb\", StringType()),\n      StructField(\"original_list_start_date\", StringType()),\n      StructField(\"original_list_price\", StringType()),\n      StructField(\"current_list_start_date\", StringType()),\n      StructField(\"current_list_price\", StringType()),\n      StructField(\"borrower_credit_score_at_issuance\", StringType()),\n      StructField(\"co-borrower_credit_score_at_issuance\", StringType()),\n      StructField(\"borrower_credit_score_current\", StringType()),\n      StructField(\"co-Borrower_credit_score_current\", StringType()),\n      StructField(\"mortgage_insurance_type\", DoubleType()),\n      StructField(\"servicing_activity_indicator\", StringType()),\n      StructField(\"current_period_modification_loss_amount\", StringType()),\n      StructField(\"cumulative_modification_loss_amount\", StringType()),\n      StructField(\"current_period_credit_event_net_gain_or_loss\", StringType()),\n      StructField(\"cumulative_credit_event_net_gain_or_loss\", StringType()),\n      StructField(\"homeready_program_indicator\", StringType()),\n      StructField(\"foreclosure_principal_write_off_amount\", StringType()),\n      StructField(\"relocation_mortgage_indicator\", StringType()),\n      StructField(\"zero_balance_code_change_date\", StringType()),\n      StructField(\"loan_holdback_indicator\", StringType()),\n      StructField(\"loan_holdback_effective_date\", StringType()),\n      StructField(\"delinquent_accrued_interest\", StringType()),\n      StructField(\"property_valuation_method\", StringType()),\n      StructField(\"high_balance_loan_indicator\", StringType()),\n      StructField(\"arm_initial_fixed-rate_period_lt_5_yr_indicator\", StringType()),\n      StructField(\"arm_product_type\", StringType()),\n      StructField(\"initial_fixed-rate_period\", StringType()),\n      StructField(\"interest_rate_adjustment_frequency\", StringType()),\n      StructField(\"next_interest_rate_adjustment_date\", StringType()),\n      StructField(\"next_payment_change_date\", StringType()),\n      StructField(\"index\", StringType()),\n      StructField(\"arm_cap_structure\", StringType()),\n      StructField(\"initial_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"periodic_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"lifetime_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"mortgage_margin\", StringType()),\n      StructField(\"arm_balloon_indicator\", StringType()),\n      StructField(\"arm_plan_number\", StringType()),\n      StructField(\"borrower_assistance_plan\", StringType()),\n      StructField(\"hltv_refinance_option_indicator\", StringType()),\n      StructField(\"deal_name\", StringType()),\n      StructField(\"repurchase_make_whole_proceeds_flag\", StringType()),\n      StructField(\"alternative_delinquency_resolution\", StringType()),\n      StructField(\"alternative_delinquency_resolution_count\", StringType()),\n      StructField(\"total_deferral_amount\", StringType())\n      ])\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark RAPIDS with GPU Acceleration\nDESCRIPTION: Sets up Spark configuration for GPU-accelerated data processing using NVIDIA Spark RAPIDS. Configurations include memory allocation, GPU resource management, and performance optimization parameters specific to GPU-based data processing.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%configure -f\n{\n    \"driverMemory\": \"4000M\",\n    \"driverCores\": 2,\n    \"executorMemory\": \"4000M\",\n    \"conf\": {\"spark.sql.adaptive.enabled\": \"false\", \"spark.dynamicAllocation.enabled\": \"false\", \"spark.executor.instances\":2, \"spark.executor.cores\":2, \"spark.rapids.sql.explain\":\"ALL\", \"spark.task.cpus\":\"1\", \"spark.rapids.sql.concurrentGpuTasks\":\"2\", \"spark.rapids.memory.pinnedPool.size\":\"2G\", \"spark.executor.memoryOverhead\":\"2G\", \"spark.executor.extraJavaOptions\":\"-Dai.rapids.cudf.prefer-pinned=true\", \"spark.sql.files.maxPartitionBytes\":\"512m\", \"spark.executor.resource.gpu.amount\":\"1\", \"spark.task.resource.gpu.amount\":\"0.5\", \"spark.plugins\":\"com.nvidia.spark.SQLPlugin\", \"spark.rapids.sql.batchSizeBytes\":\"512M\", \"spark.rapids.sql.reader.batchSizeBytes\":\"768M\", \"spark.sql.cache.serializer\" : \"com.nvidia.spark.ParquetCachedBatchSerializer\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Main ETL Function for Mortgage Data Processing with PySpark\nDESCRIPTION: The main function that orchestrates the entire ETL process by calling other helper functions in sequence. It parses dates, creates delinquency features, cleans acquisition data, joins datasets, and converts categorical variables to numerical values.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef run_mortgage(spark, perf, acq):\n    parsed_perf = _parse_dates(perf)\n    perf_deliqency = _create_perf_deliquency(spark, parsed_perf)\n    cleaned_acq = _create_acquisition(spark, acq)\n    clean_df = perf_deliqency.join(cleaned_acq, [\"loan_id\", \"quarter\"], \"inner\").drop(\"quarter\")\n    casted_clean_df = _cast_string_columns_to_numeric(spark, clean_df)\\\n                    .select(all_col_names)\\\n                    .withColumn(label_col_name, when(col(label_col_name) > 0, 1).otherwise(0))\\\n                    .fillna(float(0))\n    return casted_clean_df\n```\n\n----------------------------------------\n\nTITLE: SQL String Functions Support Matrix\nDESCRIPTION: Documents support for SQL string manipulation functions including translate(), trim(), ltrim(), rtrim(), to_json(), substr(), substring_index() and arithmetic operations. Details data type support and limitations for each operation.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_71\n\nLANGUAGE: SQL\nCODE:\n```\ntranslate(input_str, from_str, to_str)\ntrim(src [, trimStr])\nltrim(src [, trimStr])\nrtrim(src [, trimStr])\nto_json(struct)\nsubstr(str, pos [, len])\nsubstring_index(str, delim, count)\nsubtract(lhs, rhs)\n```\n\n----------------------------------------\n\nTITLE: Running Mortgage ETL Process with PySpark\nDESCRIPTION: This main function orchestrates the ETL process for performance and acquisition data, transforming them using predefined sub-functions. After extracting, transforming, and joining datasets, it returns a fully processed DataFrame suitable for analysis. Dependencies include all sub-functions for specific ETL operations and an initialized PySpark session.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef run_mortgage(spark, perf, acq):\n    parsed_perf = _parse_dates(perf)\n    perf_deliqency = _create_perf_deliquency(spark, parsed_perf)\n    cleaned_acq = _create_acquisition(spark, acq)\n    clean_df = perf_deliqency.join(cleaned_acq, [\"loan_id\", \"quarter\"], \"inner\").drop(\"quarter\")\n    casted_clean_df = _cast_string_columns_to_numeric(spark, clean_df)\\\n                    .select(all_col_names)\\\n                    .withColumn(label_col_name, when(col(label_col_name) > 0, 1).otherwise(0))\\\n                    .fillna(float(0))\n    return casted_clean_df\n```\n\n----------------------------------------\n\nTITLE: Adding RAPIDS Accelerator For Apache Spark Dependency in Maven Projects\nDESCRIPTION: Maven dependency configuration for external projects that need to develop functionality on top of RAPIDS Accelerator For Apache Spark. This uses the provided scope to indicate that the dependency will be provided at runtime.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/README.md#2025-04-19_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependency>\n    <groupId>com.nvidia</groupId>\n    <artifactId>rapids-4-spark_2.12</artifactId>\n    <version>25.06.0-SNAPSHOT</version>\n    <scope>provided</scope>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Reading and Preparing Raw Mortgage Data with PySpark\nDESCRIPTION: Code to read raw CSV mortgage data files, convert them to Parquet format for better performance, and extract acquisition and performance data for further processing.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nrawDf = read_raw_csv(spark, orig_raw_path)\nrawDf.write.parquet(output_csv2parquet, mode='overwrite')\nrawDf = spark.read.parquet(output_csv2parquet)\n\nacq = extract_acq_columns(rawDf)\nperf = extract_perf_columns(rawDf)\n```\n\n----------------------------------------\n\nTITLE: Creating Delinquency Data Frame with PySpark\nDESCRIPTION: This function creates a delinquency data frame from performance data by aggregating delinquency statuses. It utilizes PySpark DataFrame operations to group data by loan ID and quarter, and computes various delinquency-related columns. The function requires PySpark and a pre-existing performance data frame as inputs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _create_perf_deliquency(spark, perf):\n    aggDF = perf.select(\n            col(\"quarter\"),\n            col(\"loan_id\"),\n            col(\"current_loan_delinquency_status\"),\n            when(col(\"current_loan_delinquency_status\") >= 1, col(\"monthly_reporting_period\")).alias(\"delinquency_30\"),\n            when(col(\"current_loan_delinquency_status\") >= 3, col(\"monthly_reporting_period\")).alias(\"delinquency_90\"),\n            when(col(\"current_loan_delinquency_status\") >= 6, col(\"monthly_reporting_period\")).alias(\"delinquency_180\")) \\\n            .groupBy(\"quarter\", \"loan_id\") \\\n            .agg(\n                max(\"current_loan_delinquency_status\").alias(\"delinquency_12\"),\n                min(\"delinquency_30\").alias(\"delinquency_30\"),\n                min(\"delinquency_90\").alias(\"delinquency_90\"),\n                min(\"delinquency_180\").alias(\"delinquency_180\")) \\\n            .select(\n                col(\"quarter\"),\n                col(\"loan_id\"),\n                (col(\"delinquency_12\") >= 1).alias(\"ever_30\"),\n                (col(\"delinquency_12\") >= 3).alias(\"ever_90\"),\n                (col(\"delinquency_12\") >= 6).alias(\"ever_180\"),\n                col(\"delinquency_30\"),\n                col(\"delinquency_90\"),\n                col(\"delinquency_180\"))\n    joinedDf = perf \\\n            .withColumnRenamed(\"monthly_reporting_period\", \"timestamp\") \\\n            .withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\") \\\n            .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\") \\\n            .withColumnRenamed(\"current_loan_delinquency_status\", \"delinquency_12\") \\\n            .withColumnRenamed(\"current_actual_upb\", \"upb_12\") \\\n            .select(\"quarter\", \"loan_id\", \"timestamp\", \"delinquency_12\", \"upb_12\", \"timestamp_month\", \"timestamp_year\") \\\n            .join(aggDF, [\"loan_id\", \"quarter\"], \"left_outer\")\n\n    # calculate the 12 month delinquency and upb values\n    months = 12\n    monthArray = [lit(x) for x in range(0, 12)]\n    # explode on a small amount of data is actually slightly more efficient than a cross join\n    testDf = joinedDf \\\n            .withColumn(\"month_y\", explode(array(monthArray))) \\\n            .select(\n                    col(\"quarter\"),\n                    floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000) / months).alias(\"josh_mody\"),\n                    floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000 - col(\"month_y\")) / months).alias(\"josh_mody_n\"),\n                    col(\"ever_30\"),\n                    col(\"ever_90\"),\n                    col(\"ever_180\"),\n                    col(\"delinquency_30\"),\n                    col(\"delinquency_90\"),\n                    col(\"delinquency_180\"),\n                    col(\"loan_id\"),\n                    col(\"month_y\"),\n                    col(\"delinquency_12\"),\n                    col(\"upb_12\")) \\\n            .groupBy(\"quarter\", \"loan_id\", \"josh_mody_n\", \"ever_30\", \"ever_90\", \"ever_180\", \"delinquency_30\", \"delinquency_90\", \"delinquency_180\", \"month_y\") \\\n            .agg(max(\"delinquency_12\").alias(\"delinquency_12\"), min(\"upb_12\").alias(\"upb_12\")) \\\n            .withColumn(\"timestamp_year\", floor((lit(24000) + (col(\"josh_mody_n\") * lit(months)) + (col(\"month_y\") - 1)) / lit(12))) \\\n            .selectExpr('*', 'pmod(24000 + (josh_mody_n * {}) + month_y, 12) as timestamp_month_tmp'.format(months)) \\\n            .withColumn(\"timestamp_month\", when(col(\"timestamp_month_tmp\") == lit(0), lit(12)).otherwise(col(\"timestamp_month_tmp\"))) \\\n            .withColumn(\"delinquency_12\", ((col(\"delinquency_12\") > 3).cast(\"int\") + (col(\"upb_12\") == 0).cast(\"int\")).alias(\"delinquency_12\")) \\\n            .drop(\"timestamp_month_tmp\", \"josh_mody_n\", \"month_y\")\n\n    return perf.withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\") \\\n            .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\") \\\n            .join(testDf, [\"quarter\", \"loan_id\", \"timestamp_year\", \"timestamp_month\"], \"left\") \\\n            .drop(\"timestamp_year\", \"timestamp_month\")\n```\n\n----------------------------------------\n\nTITLE: HyperLogLog Aggregation Implementation\nDESCRIPTION: Implementation of approx_count_distinct aggregate function supporting primitive types and timestamps (UTC only). Has limitations for complex types and special temporal types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_73\n\nLANGUAGE: SQL\nCODE:\n```\napprox_count_distinct\n```\n\n----------------------------------------\n\nTITLE: Feature Extraction with Spark SQL and Windowing in Python\nDESCRIPTION: This Python code extracts acquisition-related columns from a Spark DataFrame. It uses functions like `col`, `upper`, `alias`, `date_format`, `to_date`, and `dense_rank` to transform and select the columns. `dense_rank` is used with a `Window` specification to assign a rank based on `monthly_reporting_period` within each `loan_id` partition, and then filters the DataFrame to keep only the rows with rank 1.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef extract_acq_columns(rawDf):\n    acqDf = rawDf.select(\n      col(\"loan_id\"),\n      col(\"orig_channel\"),\n      upper(col(\"seller_name\")).alias(\"seller_name\"),\n      col(\"orig_interest_rate\"),\n      col(\"orig_upb\"),\n      col(\"orig_loan_term\"),\n      date_format(to_date(col(\"orig_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"orig_date\"),\n      date_format(to_date(col(\"first_pay_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"first_pay_date\"),\n      col(\"orig_ltv\"),\n      col(\"orig_cltv\"),\n      col(\"num_borrowers\"),\n      col(\"dti\"),\n      col(\"borrower_credit_score\"),\n      col(\"first_home_buyer\"),\n      col(\"loan_purpose\"),\n      col(\"property_type\"),\n      col(\"num_units\"),\n      col(\"occupancy_status\"),\n      col(\"property_state\"),\n      col(\"zip\"),\n      col(\"mortgage_insurance_percent\"),\n      col(\"product_type\"),\n      col(\"coborrow_credit_score\"),\n      col(\"mortgage_insurance_type\"),\n      col(\"relocation_mortgage_indicator\"),\n      dense_rank().over(Window.partitionBy(\"loan_id\").orderBy(to_date(col(\"monthly_reporting_period\"),\"MMyyyy\"))).alias(\"rank\"),\n      col('quarter')\n      )\n\n    return acqDf.select(\"*\").filter(col(\"rank\")==1)\n```\n\n----------------------------------------\n\nTITLE: Reading Raw CSV Mortgage Data Files in PySpark\nDESCRIPTION: Defines a function to read raw CSV mortgage data files with specific options like delimiter, header settings, and null value handling. The function also adds a quarter column based on the filename.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef read_raw_csv(spark, path):\n    return spark.read.format('csv') \\\n            .option('nullValue', '') \\\n            .option('header', False) \\\n            .option('delimiter', '|') \\\n            .schema(_csv_raw_schema) \\\n            .load(path) \\\n            .withColumn('quarter', _get_quarter_from_csv_file_name())\n```\n\n----------------------------------------\n\nTITLE: Configuring RAPIDS SQL Expression Acceleration\nDESCRIPTION: Configuration options for enabling RAPIDS acceleration of various SQL expressions in Apache Spark. Each option corresponds to a specific SQL function or operator.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_6\n\nLANGUAGE: Properties\nCODE:\n```\nspark.rapids.sql.expression.Lower=true\nspark.rapids.sql.expression.MakeDecimal=true\nspark.rapids.sql.expression.MapConcat=true\nspark.rapids.sql.expression.MapEntries=true\nspark.rapids.sql.expression.MapFilter=true\nspark.rapids.sql.expression.MapFromArrays=true\nspark.rapids.sql.expression.MapKeys=true\nspark.rapids.sql.expression.MapValues=true\nspark.rapids.sql.expression.Md5=true\nspark.rapids.sql.expression.MicrosToTimestamp=true\nspark.rapids.sql.expression.MillisToTimestamp=true\nspark.rapids.sql.expression.Minute=true\nspark.rapids.sql.expression.MonotonicallyIncreasingID=true\nspark.rapids.sql.expression.Month=true\nspark.rapids.sql.expression.MonthsBetween=true\nspark.rapids.sql.expression.Multiply=true\nspark.rapids.sql.expression.Murmur3Hash=true\nspark.rapids.sql.expression.NaNvl=true\nspark.rapids.sql.expression.NamedLambdaVariable=true\nspark.rapids.sql.expression.Not=true\nspark.rapids.sql.expression.NthValue=true\nspark.rapids.sql.expression.OctetLength=true\nspark.rapids.sql.expression.Or=true\nspark.rapids.sql.expression.ParseUrl=true\nspark.rapids.sql.expression.PercentRank=true\nspark.rapids.sql.expression.Pmod=true\nspark.rapids.sql.expression.PosExplode=true\nspark.rapids.sql.expression.Pow=true\nspark.rapids.sql.expression.PreciseTimestampConversion=true\nspark.rapids.sql.expression.PromotePrecision=true\nspark.rapids.sql.expression.PythonUDF=true\nspark.rapids.sql.expression.Quarter=true\nspark.rapids.sql.expression.RLike=true\nspark.rapids.sql.expression.RaiseError=true\nspark.rapids.sql.expression.Rand=true\nspark.rapids.sql.expression.Rank=true\nspark.rapids.sql.expression.RegExpExtract=true\nspark.rapids.sql.expression.RegExpExtractAll=true\nspark.rapids.sql.expression.RegExpReplace=true\nspark.rapids.sql.expression.Remainder=true\nspark.rapids.sql.expression.ReplicateRows=true\nspark.rapids.sql.expression.Reverse=true\nspark.rapids.sql.expression.Rint=true\nspark.rapids.sql.expression.Round=true\nspark.rapids.sql.expression.RowNumber=true\nspark.rapids.sql.expression.ScalaUDF=true\nspark.rapids.sql.expression.Second=true\nspark.rapids.sql.expression.SecondsToTimestamp=true\nspark.rapids.sql.expression.Sequence=true\nspark.rapids.sql.expression.ShiftLeft=true\nspark.rapids.sql.expression.ShiftRight=true\nspark.rapids.sql.expression.ShiftRightUnsigned=true\nspark.rapids.sql.expression.Signum=true\nspark.rapids.sql.expression.Sin=true\nspark.rapids.sql.expression.Sinh=true\nspark.rapids.sql.expression.Size=true\nspark.rapids.sql.expression.Slice=true\nspark.rapids.sql.expression.SortArray=true\nspark.rapids.sql.expression.SortOrder=true\nspark.rapids.sql.expression.SparkPartitionID=true\nspark.rapids.sql.expression.SpecifiedWindowFrame=true\nspark.rapids.sql.expression.Sqrt=true\nspark.rapids.sql.expression.Stack=true\n```\n\n----------------------------------------\n\nTITLE: Creating Delinquency Features from Mortgage Performance Data with PySpark\nDESCRIPTION: Function that generates delinquency features from mortgage performance data, including ever_30, ever_90, and ever_180 flags indicating if a loan has ever been delinquent for 30, 90, or 180 days. The function performs complex time-based aggregations and transformations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _create_perf_deliquency(spark, perf):\n    aggDF = perf.select(\n            col(\"quarter\"),\n            col(\"loan_id\"),\n            col(\"current_loan_delinquency_status\"),\n            when(col(\"current_loan_delinquency_status\") >= 1, col(\"monthly_reporting_period\")).alias(\"delinquency_30\"),\n            when(col(\"current_loan_delinquency_status\") >= 3, col(\"monthly_reporting_period\")).alias(\"delinquency_90\"),\n            when(col(\"current_loan_delinquency_status\") >= 6, col(\"monthly_reporting_period\")).alias(\"delinquency_180\")) \\\n            .groupBy(\"quarter\", \"loan_id\") \\\n            .agg(\n                max(\"current_loan_delinquency_status\").alias(\"delinquency_12\"),\n                min(\"delinquency_30\").alias(\"delinquency_30\"),\n                min(\"delinquency_90\").alias(\"delinquency_90\"),\n                min(\"delinquency_180\").alias(\"delinquency_180\")) \\\n            .select(\n                col(\"quarter\"),\n                col(\"loan_id\"),\n                (col(\"delinquency_12\") >= 1).alias(\"ever_30\"),\n                (col(\"delinquency_12\") >= 3).alias(\"ever_90\"),\n                (col(\"delinquency_12\") >= 6).alias(\"ever_180\"),\n                col(\"delinquency_30\"),\n                col(\"delinquency_90\"),\n                col(\"delinquency_180\"))\n    joinedDf = perf \\\n            .withColumnRenamed(\"monthly_reporting_period\", \"timestamp\") \\\n            .withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\") \\\n            .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\") \\\n            .withColumnRenamed(\"current_loan_delinquency_status\", \"delinquency_12\") \\\n            .withColumnRenamed(\"current_actual_upb\", \"upb_12\") \\\n            .select(\"quarter\", \"loan_id\", \"timestamp\", \"delinquency_12\", \"upb_12\", \"timestamp_month\", \"timestamp_year\") \\\n            .join(aggDF, [\"loan_id\", \"quarter\"], \"left_outer\")\n\n    # calculate the 12 month delinquency and upb values\n    months = 12\n    monthArray = [lit(x) for x in range(0, 12)]\n    # explode on a small amount of data is actually slightly more efficient than a cross join\n    testDf = joinedDf \\\n            .withColumn(\"month_y\", explode(array(monthArray))) \\\n            .select(\n                    col(\"quarter\"),\n                    floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000) / months).alias(\"josh_mody\"),\n                    floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000 - col(\"month_y\")) / months).alias(\"josh_mody_n\"),\n                    col(\"ever_30\"),\n                    col(\"ever_90\"),\n                    col(\"ever_180\"),\n                    col(\"delinquency_30\"),\n                    col(\"delinquency_90\"),\n                    col(\"delinquency_180\"),\n                    col(\"loan_id\"),\n                    col(\"month_y\"),\n                    col(\"delinquency_12\"),\n                    col(\"upb_12\")) \\\n            .groupBy(\"quarter\", \"loan_id\", \"josh_mody_n\", \"ever_30\", \"ever_90\", \"ever_180\", \"delinquency_30\", \"delinquency_90\", \"delinquency_180\", \"month_y\") \\\n            .agg(max(\"delinquency_12\").alias(\"delinquency_12\"), min(\"upb_12\").alias(\"upb_12\")) \\\n            .withColumn(\"timestamp_year\", floor((lit(24000) + (col(\"josh_mody_n\") * lit(months)) + (col(\"month_y\") - 1)) / lit(12))) \\\n            .selectExpr('*', 'pmod(24000 + (josh_mody_n * {}) + month_y, 12) as timestamp_month_tmp'.format(months)) \\\n            .withColumn(\"timestamp_month\", when(col(\"timestamp_month_tmp\") == lit(0), lit(12)).otherwise(col(\"timestamp_month_tmp\"))) \\\n            .withColumn(\"delinquency_12\", ((col(\"delinquency_12\") > 3).cast(\"int\") + (col(\"upb_12\") == 0).cast(\"int\")).alias(\"delinquency_12\")) \\\n            .drop(\"timestamp_month_tmp\", \"josh_mody_n\", \"month_y\")\n\n    return perf.withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\") \\\n            .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\") \\\n            .join(testDf, [\"quarter\", \"loan_id\", \"timestamp_year\", \"timestamp_month\"], \"left\") \\\n            .drop(\"timestamp_year\", \"timestamp_month\")\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Configuration at Startup (Scala)\nDESCRIPTION: Example of how to set RAPIDS Accelerator configuration options when starting a Spark shell. This snippet demonstrates setting the Spark plugin and concurrent GPU tasks.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/configs.md#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n${SPARK_HOME}/bin/spark-shell --jars rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar \\\n--conf spark.plugins=com.nvidia.spark.SQLPlugin \\\n--conf spark.rapids.sql.concurrentGpuTasks=2\n```\n\n----------------------------------------\n\nTITLE: Cleaning Mortgage Acquisition Data with PySpark\nDESCRIPTION: Function that cleans acquisition data by standardizing seller names using a name mapping table and converting date columns from string format to date format.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _create_acquisition(spark, acq):\n    nameMapping = spark.createDataFrame(_name_mapping, [\"from_seller_name\", \"to_seller_name\"])\n    return acq.join(nameMapping, col(\"seller_name\") == col(\"from_seller_name\"), \"left\") \\\n      .drop(\"from_seller_name\") \\\n      .withColumn(\"old_name\", col(\"seller_name\")) \\\n      .withColumn(\"seller_name\", coalesce(col(\"to_seller_name\"), col(\"seller_name\"))) \\\n      .drop(\"to_seller_name\") \\\n      .withColumn(\"orig_date\", to_date(col(\"orig_date\"), \"MM/yyyy\")) \\\n      .withColumn(\"first_pay_date\", to_date(col(\"first_pay_date\"), \"MM/yyyy\"))\n```\n\n----------------------------------------\n\nTITLE: Sort Array Function\nDESCRIPTION: Sorts an array in ascending or descending order. Has limitations on certain data types including binary, calendar, nested arrays/maps, and custom types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_70\n\nLANGUAGE: SQL\nCODE:\n```\nsort_array(array, [ascendingOrder])\n```\n\n----------------------------------------\n\nTITLE: Defining CSV Schema for Mortgage Data Processing\nDESCRIPTION: Defines a comprehensive schema for the mortgage data CSV files using PySpark's StructType and StructField. The schema includes over 100 fields that capture various aspects of mortgage loans, such as loan identifiers, interest rates, borrower details, property information, and payment history.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# File schema\n_csv_raw_schema = StructType([\n      StructField(\"reference_pool_id\", StringType()),\n      StructField(\"loan_id\", LongType()),\n      StructField(\"monthly_reporting_period\", StringType()),\n      StructField(\"orig_channel\", StringType()),\n      StructField(\"seller_name\", StringType()),\n      StructField(\"servicer\", StringType()),\n      StructField(\"master_servicer\", StringType()),\n      StructField(\"orig_interest_rate\", DoubleType()),\n      StructField(\"interest_rate\", DoubleType()),\n      StructField(\"orig_upb\", DoubleType()),\n      StructField(\"upb_at_issuance\", StringType()),\n      StructField(\"current_actual_upb\", DoubleType()),\n      StructField(\"orig_loan_term\", IntegerType()),\n      StructField(\"orig_date\", StringType()),\n      StructField(\"first_pay_date\", StringType()),    \n      StructField(\"loan_age\", DoubleType()),\n      StructField(\"remaining_months_to_legal_maturity\", DoubleType()),\n      StructField(\"adj_remaining_months_to_maturity\", DoubleType()),\n      StructField(\"maturity_date\", StringType()),\n      StructField(\"orig_ltv\", DoubleType()),\n      StructField(\"orig_cltv\", DoubleType()),\n      StructField(\"num_borrowers\", DoubleType()),\n      StructField(\"dti\", DoubleType()),\n      StructField(\"borrower_credit_score\", DoubleType()),\n      StructField(\"coborrow_credit_score\", DoubleType()),\n      StructField(\"first_home_buyer\", StringType()),\n      StructField(\"loan_purpose\", StringType()),\n      StructField(\"property_type\", StringType()),\n      StructField(\"num_units\", IntegerType()),\n      StructField(\"occupancy_status\", StringType()),\n      StructField(\"property_state\", StringType()),\n      StructField(\"msa\", DoubleType()),\n      StructField(\"zip\", IntegerType()),\n      StructField(\"mortgage_insurance_percent\", DoubleType()),\n      StructField(\"product_type\", StringType()),\n      StructField(\"prepayment_penalty_indicator\", StringType()),\n      StructField(\"interest_only_loan_indicator\", StringType()),\n      StructField(\"interest_only_first_principal_and_interest_payment_date\", StringType()),\n      StructField(\"months_to_amortization\", StringType()),\n      StructField(\"current_loan_delinquency_status\", IntegerType()),\n      StructField(\"loan_payment_history\", StringType()),\n      StructField(\"mod_flag\", StringType()),\n      StructField(\"mortgage_insurance_cancellation_indicator\", StringType()),\n      StructField(\"zero_balance_code\", StringType()),\n      StructField(\"zero_balance_effective_date\", StringType()),\n      StructField(\"upb_at_the_time_of_removal\", StringType()),\n      StructField(\"repurchase_date\", StringType()),\n      StructField(\"scheduled_principal_current\", StringType()),\n      StructField(\"total_principal_current\", StringType()),\n      StructField(\"unscheduled_principal_current\", StringType()),\n      StructField(\"last_paid_installment_date\", StringType()),\n      StructField(\"foreclosed_after\", StringType()),\n      StructField(\"disposition_date\", StringType()),\n      StructField(\"foreclosure_costs\", DoubleType()),\n      StructField(\"prop_preservation_and_repair_costs\", DoubleType()),\n      StructField(\"asset_recovery_costs\", DoubleType()),\n      StructField(\"misc_holding_expenses\", DoubleType()),\n      StructField(\"holding_taxes\", DoubleType()),\n      StructField(\"net_sale_proceeds\", DoubleType()),\n      StructField(\"credit_enhancement_proceeds\", DoubleType()),\n      StructField(\"repurchase_make_whole_proceeds\", StringType()),\n      StructField(\"other_foreclosure_proceeds\", DoubleType()),\n      StructField(\"non_interest_bearing_upb\", DoubleType()),\n      StructField(\"principal_forgiveness_upb\", StringType()),\n      StructField(\"original_list_start_date\", StringType()),\n      StructField(\"original_list_price\", StringType()),\n      StructField(\"current_list_start_date\", StringType()),\n      StructField(\"current_list_price\", StringType()),\n      StructField(\"borrower_credit_score_at_issuance\", StringType()),\n      StructField(\"co-borrower_credit_score_at_issuance\", StringType()),\n      StructField(\"borrower_credit_score_current\", StringType()),\n      StructField(\"co-Borrower_credit_score_current\", StringType()),\n      StructField(\"mortgage_insurance_type\", DoubleType()),\n      StructField(\"servicing_activity_indicator\", StringType()),\n      StructField(\"current_period_modification_loss_amount\", StringType()),\n      StructField(\"cumulative_modification_loss_amount\", StringType()),\n      StructField(\"current_period_credit_event_net_gain_or_loss\", StringType()),\n      StructField(\"cumulative_credit_event_net_gain_or_loss\", StringType()),\n      StructField(\"homeready_program_indicator\", StringType()),\n      StructField(\"foreclosure_principal_write_off_amount\", StringType()),\n      StructField(\"relocation_mortgage_indicator\", StringType()),\n      StructField(\"zero_balance_code_change_date\", StringType()),\n      StructField(\"loan_holdback_indicator\", StringType()),\n      StructField(\"loan_holdback_effective_date\", StringType()),\n      StructField(\"delinquent_accrued_interest\", StringType()),\n      StructField(\"property_valuation_method\", StringType()),\n      StructField(\"high_balance_loan_indicator\", StringType()),\n      StructField(\"arm_initial_fixed-rate_period_lt_5_yr_indicator\", StringType()),\n      StructField(\"arm_product_type\", StringType()),\n      StructField(\"initial_fixed-rate_period\", StringType()),\n      StructField(\"interest_rate_adjustment_frequency\", StringType()),\n      StructField(\"next_interest_rate_adjustment_date\", StringType()),\n      StructField(\"next_payment_change_date\", StringType()),\n      StructField(\"index\", StringType()),\n      StructField(\"arm_cap_structure\", StringType()),\n      StructField(\"initial_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"periodic_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"lifetime_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"mortgage_margin\", StringType()),\n      StructField(\"arm_balloon_indicator\", StringType()),\n      StructField(\"arm_plan_number\", StringType()),\n      StructField(\"borrower_assistance_plan\", StringType()),\n      StructField(\"hltv_refinance_option_indicator\", StringType()),\n      StructField(\"deal_name\", StringType()),\n      StructField(\"repurchase_make_whole_proceeds_flag\", StringType()),\n      StructField(\"alternative_delinquency_resolution\", StringType()),\n      StructField(\"alternative_delinquency_resolution_count\", StringType()),\n      StructField(\"total_deferral_amount\", StringType())\n      ])\n```\n\n----------------------------------------\n\nTITLE: Defining Utility Functions for CSV File Processing in PySpark\nDESCRIPTION: These functions extract the quarter from the input CSV file name and read raw CSV data into a PySpark DataFrame. They handle file naming conventions and set options for CSV parsing.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef _get_quarter_from_csv_file_name():\n    return substring_index(substring_index(input_file_name(), '.', 1), '/', -1)\n\ndef read_raw_csv(spark, path):\n    return spark.read.format('csv') \\\n            .option('nullValue', '') \\\n            .option('header', False) \\\n            .option('delimiter', '|') \\\n            .schema(_csv_raw_schema) \\\n            .load(path) \\\n            .withColumn('quarter', _get_quarter_from_csv_file_name())\n```\n\n----------------------------------------\n\nTITLE: Generating Column Dictionary for Categorical Data Encoding with PySpark\nDESCRIPTION: Function that generates a dictionary mapping for categorical columns, where each unique value in a column is assigned a numeric ID. This dictionary is used for converting string columns to numeric values.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _gen_dictionary(etl_df, col_names):\n    cnt_table = etl_df.select(posexplode(array([col(i) for i in col_names])))\\\n                    .withColumnRenamed(\"pos\", \"column_id\")\\\n                    .withColumnRenamed(\"col\", \"data\")\\\n                    .filter(\"data is not null\")\\\n                    .groupBy(\"column_id\", \"data\")\\\n                    .count()\n    windowed = Window.partitionBy(\"column_id\").orderBy(desc(\"count\"))\n    return cnt_table.withColumn(\"id\", row_number().over(windowed)).drop(\"count\")\n```\n\n----------------------------------------\n\nTITLE: Example of Multiple Pandas UDFs in PySpark\nDESCRIPTION: This code snippet demonstrates the use of multiple Pandas UDFs (MapInPandas) in a PySpark DataFrame operation. It shows how multiple Python processes can be launched for a single Spark task.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf_1 = df_0.mapInPandas(udf_1, schema_1)\ndf_2 = df_1.mapInPandas(udf_2, schema_2)\ndf_3 = df_2.mapInPandas(udf_3, schema_3)\ndf_3.explain(True)\n```\n\n----------------------------------------\n\nTITLE: Extracting Performance and Acquisition Columns from Raw Mortgage Data in PySpark\nDESCRIPTION: These functions extract and transform performance and acquisition data from raw DataFrames. They include date formatting, column renaming, and filtering operations to prepare the data for analysis.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef extract_perf_columns(rawDf):\n    perfDf = rawDf.select(\n      col(\"loan_id\"),\n      date_format(to_date(col(\"monthly_reporting_period\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"monthly_reporting_period\"),\n      upper(col(\"servicer\")).alias(\"servicer\"),\n      col(\"interest_rate\"),\n      col(\"current_actual_upb\"),\n      col(\"loan_age\"),\n      col(\"remaining_months_to_legal_maturity\"),\n      col(\"adj_remaining_months_to_maturity\"),\n      date_format(to_date(col(\"maturity_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"maturity_date\"),\n      col(\"msa\"),\n      col(\"current_loan_delinquency_status\"),\n      col(\"mod_flag\"),\n      col(\"zero_balance_code\"),\n      date_format(to_date(col(\"zero_balance_effective_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"zero_balance_effective_date\"),\n      date_format(to_date(col(\"last_paid_installment_date\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"last_paid_installment_date\"),\n      date_format(to_date(col(\"foreclosed_after\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"foreclosed_after\"),\n      date_format(to_date(col(\"disposition_date\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"disposition_date\"),\n      col(\"foreclosure_costs\"),\n      col(\"prop_preservation_and_repair_costs\"),\n      col(\"asset_recovery_costs\"),\n      col(\"misc_holding_expenses\"),\n      col(\"holding_taxes\"),\n      col(\"net_sale_proceeds\"),\n      col(\"credit_enhancement_proceeds\"),\n      col(\"repurchase_make_whole_proceeds\"),\n      col(\"other_foreclosure_proceeds\"),\n      col(\"non_interest_bearing_upb\"),\n      col(\"principal_forgiveness_upb\"),\n      col(\"repurchase_make_whole_proceeds_flag\"),\n      col(\"foreclosure_principal_write_off_amount\"),\n      col(\"servicing_activity_indicator\"),\n      col('quarter')\n    )\n\n    return perfDf.select(\"*\").filter(\"current_actual_upb != 0.0\")\n\ndef extract_acq_columns(rawDf):\n    acqDf = rawDf.select(\n      col(\"loan_id\"),\n      col(\"orig_channel\"),\n      upper(col(\"seller_name\")).alias(\"seller_name\"),\n      col(\"orig_interest_rate\"),\n      col(\"orig_upb\"),\n      col(\"orig_loan_term\"),\n      date_format(to_date(col(\"orig_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"orig_date\"),\n      date_format(to_date(col(\"first_pay_date\"),\"MMyyyy\"), \"MM/yyyy\").alias(\"first_pay_date\"),\n      col(\"orig_ltv\"),\n      col(\"orig_cltv\"),\n      col(\"num_borrowers\"),\n      col(\"dti\"),\n      col(\"borrower_credit_score\"),\n      col(\"first_home_buyer\"),\n      col(\"loan_purpose\"),\n      col(\"property_type\"),\n      col(\"num_units\"),\n      col(\"occupancy_status\"),\n      col(\"property_state\"),\n      col(\"zip\"),\n      col(\"mortgage_insurance_percent\"),\n      col(\"product_type\"),\n      col(\"coborrow_credit_score\"),\n      col(\"mortgage_insurance_type\"),\n      col(\"relocation_mortgage_indicator\"),\n      dense_rank().over(Window.partitionBy(\"loan_id\").orderBy(to_date(col(\"monthly_reporting_period\"),\"MMyyyy\"))).alias(\"rank\"),\n      col('quarter')\n      )\n\n    return acqDf.select(\"*\").filter(col(\"rank\")==1)\n```\n\n----------------------------------------\n\nTITLE: Using regexp_replace Function in Spark SQL\nDESCRIPTION: Perform string replacement using a regular expression pattern. The function takes a string, a regex pattern, a replacement string, and an optional position parameter. Only a position value of 1 is supported in this implementation.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_59\n\nLANGUAGE: SQL\nCODE:\n```\nregexp_replace(str, regex, rep, pos)\n```\n\n----------------------------------------\n\nTITLE: Casting String Columns to Numeric with PySpark\nDESCRIPTION: This function converts specified string columns in the input DataFrame to numeric columns using a lookup dictionary. It caches the dictionary DataFrame and performs left joins to replace string values with their corresponding numeric values. Dependencies include PySpark, and dictionaries generated from _gen_dictionary.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _cast_string_columns_to_numeric(spark, input_df):\n    cached_dict_df = _gen_dictionary(input_df, cate_col_names).cache()\n    output_df = input_df\n    #  Generate the final table with all columns being numeric.\n    for col_pos, col_name in enumerate(cate_col_names):\n        col_dict_df = cached_dict_df.filter(col(\"column_id\") == col_pos)\\\n                                    .drop(\"column_id\")\\\n                                    .withColumnRenamed(\"data\", col_name)\n        \n        output_df = output_df.join(broadcast(col_dict_df), col_name, \"left\")\\\n                        .drop(col_name)\\\n                        .withColumnRenamed(\"id\", col_name)\n    return output_df\n```\n\n----------------------------------------\n\nTITLE: Parsing Dates with PySpark\nDESCRIPTION: This function parses date fields in the performance data set, converting them to appropriate date formats using PySpark's built-in date functions. Required dependencies include PySpark DataFrame functions and the presence of specified date columns in the input data. The parsed dates enable downstream processing by extracting year, month, and day components.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _parse_dates(perf):\n    return perf \\\n            .withColumn('monthly_reporting_period', to_date(col('monthly_reporting_period'), 'MM/dd/yyyy')) \\\n            .withColumn('monthly_reporting_period_month', month(col('monthly_reporting_period'))) \\\n            .withColumn('monthly_reporting_period_year', year(col('monthly_reporting_period'))) \\\n            .withColumn('monthly_reporting_period_day', dayofmonth(col('monthly_reporting_period'))) \\\n            .withColumn('last_paid_installment_date', to_date(col('last_paid_installment_date'),\n                       'MM/dd/yyyy')) \\\n            .withColumn('foreclosed_after', to_date(col('foreclosed_after'), 'MM/dd/yyyy')) \\\n            .withColumn('disposition_date', to_date(col('disposition_date'), 'MM/dd/yyyy')) \\\n            .withColumn('maturity_date', to_date(col('maturity_date'), 'MM/yyyy')) \\\n            .withColumn('zero_balance_effective_date', to_date(col('zero_balance_effective_date'), 'MM/yyyy'))\n```\n\n----------------------------------------\n\nTITLE: Generating Column Dictionary with PySpark\nDESCRIPTION: The function generates a dictionary mapping for specified column names, converting categorical string values to numeric identifiers. It filters non-null values and utilizes the row_number window function to assign numeric identifiers. Dependencies include PySpark and a DataFrame containing the target columns.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _gen_dictionary(etl_df, col_names):\n    cnt_table = etl_df.select(posexplode(array([col(i) for i in col_names])))\\\n                    .withColumnRenamed(\"pos\", \"column_id\")\\\n                    .withColumnRenamed(\"col\", \"data\")\\\n                    .filter(\"data is not null\")\\\n                    .groupBy(\"column_id\", \"data\")\\\n                    .count()\n    windowed = Window.partitionBy(\"column_id\").orderBy(desc(\"count\"))\n    return cnt_table.withColumn(\"id\", row_number().over(windowed)).drop(\"count\")\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Configuration at Runtime (Scala)\nDESCRIPTION: Example of how to set RAPIDS Accelerator configuration options at runtime within a Spark session. This snippet shows how to set the number of concurrent GPU tasks.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/configs.md#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nscala> spark.conf.set(\"spark.rapids.sql.concurrentGpuTasks\", 2)\n```\n\n----------------------------------------\n\nTITLE: Mortgage Delinquency Analysis with PySpark\nDESCRIPTION: Complex function to calculate various delinquency metrics for mortgage performance data using PySpark operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _create_perf_deliquency(spark, perf):\n    aggDF = perf.select(\n            col(\"quarter\"),\n            col(\"loan_id\"),\n            col(\"current_loan_delinquency_status\"),\n            when(col(\"current_loan_delinquency_status\") >= 1, col(\"monthly_reporting_period\")).alias(\"delinquency_30\"),\n            when(col(\"current_loan_delinquency_status\") >= 3, col(\"monthly_reporting_period\")).alias(\"delinquency_90\"),\n            when(col(\"current_loan_delinquency_status\") >= 6, col(\"monthly_reporting_period\")).alias(\"delinquency_180\")) \\\n                    .groupBy(\"quarter\", \"loan_id\") \\\n                    .agg(\n                            max(\"current_loan_delinquency_status\").alias(\"delinquency_12\"),\n                            min(\"delinquency_30\").alias(\"delinquency_30\"),\n                            min(\"delinquency_90\").alias(\"delinquency_90\"),\n                            min(\"delinquency_180\").alias(\"delinquency_180\")) \\\n                                    .select(\n                                            col(\"quarter\"),\n                                            col(\"loan_id\"),\n                                            (col(\"delinquency_12\") >= 1).alias(\"ever_30\"),\n                                            (col(\"delinquency_12\") >= 3).alias(\"ever_90\"),\n                                            (col(\"delinquency_12\") >= 6).alias(\"ever_180\"),\n                                            col(\"delinquency_30\"),\n                                            col(\"delinquency_90\"),\n                                            col(\"delinquency_180\"))\n```\n\n----------------------------------------\n\nTITLE: Converting Categorical Columns to Numeric Values with PySpark\nDESCRIPTION: Function that transforms categorical string columns into numeric values using the dictionary mapping generated by _gen_dictionary. This enables machine learning algorithms to process the data.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _cast_string_columns_to_numeric(spark, input_df):\n    cached_dict_df = _gen_dictionary(input_df, cate_col_names).cache()\n    output_df = input_df\n    #  Generate the final table with all columns being numeric.\n    for col_pos, col_name in enumerate(cate_col_names):\n        col_dict_df = cached_dict_df.filter(col(\"column_id\") == col_pos)\\\n                                    .drop(\"column_id\")\\\n                                    .withColumnRenamed(\"data\", col_name)\n        \n        output_df = output_df.join(broadcast(col_dict_df), col_name, \"left\")\\\n                        .drop(col_name)\\\n                        .withColumnRenamed(\"id\", col_name)\n    return output_df\n```\n\n----------------------------------------\n\nTITLE: Using least Function in Spark SQL\nDESCRIPTION: The least function returns the smallest value among all parameters, skipping null values. It supports most scalar data types but not complex types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_40\n\nLANGUAGE: SQL\nCODE:\n```\nleast(param1, param2, ...)\n```\n\n----------------------------------------\n\nTITLE: Defining Hive Generic UDF in Spark RAPIDS\nDESCRIPTION: This snippet defines a Hive Generic UDF within the Spark RAPIDS framework. The UDF can implement a RAPIDS accelerated interface, allowing users to gain better performance in Spark applications using GPUs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.expression.HiveGenericUDF\"></a>spark.rapids.sql.expression.HiveGenericUDF| |Hive Generic UDF, the UDF can choose to implement a RAPIDS accelerated interface to get better performance|true|None|\n```\n\n----------------------------------------\n\nTITLE: Mortgage Data Processing with Spark in Python\nDESCRIPTION: This Python code processes mortgage data using Apache Spark. It reads Parquet files, configures Spark settings, calls the `run_mortgage` function (not defined in the snippet), and writes the resulting train and test DataFrames to Parquet format on S3. The code also measures the execution time of the operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Now lets actually process the data\\n\",\nstart = time.time()\nspark.conf.set('spark.sql.files.maxPartitionBytes', '1G')\nspark.conf.set('spark.sql.shuffle.partitions', '160')\nperf = spark.read.parquet(tmp_perf_path)\nacq = spark.read.parquet(tmp_acq_path)\ntrain_out, test_out = run_mortgage(spark, perf, acq)\ntrain_out.write.parquet(train_path, mode='overwrite')\nend = time.time()\nprint(end - start)\ntest_out.write.parquet(test_path, mode='overwrite')\nend = time.time()\nprint(end - start)\n```\n\n----------------------------------------\n\nTITLE: Data Transformation with Spark SQL Functions in Python\nDESCRIPTION: This Python code snippet uses Spark SQL functions to transform and select columns from a DataFrame.  It uses functions such as `date_format`, `to_date`, `col`, and `alias` to manipulate date columns and rename them. It also filters data where 'current_actual_upb' is not equal to 0.0.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndate_format(to_date(col(\"last_paid_installment_date\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"last_paid_installment_date\"),\n      date_format(to_date(col(\"foreclosed_after\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"foreclosed_after\"),\n      date_format(to_date(col(\"disposition_date\"),\"MMyyyy\"), \"MM/dd/yyyy\").alias(\"disposition_date\"),\n      col(\"foreclosure_costs\"),\n      col(\"prop_preservation_and_repair_costs\"),\n      col(\"asset_recovery_costs\"),\n      col(\"misc_holding_expenses\"),\n      col(\"holding_taxes\"),\n      col(\"net_sale_proceeds\"),\n      col(\"credit_enhancement_proceeds\"),\n      col(\"repurchase_make_whole_proceeds\"),\n      col(\"other_foreclosure_proceeds\"),\n      col(\"non_interest_bearing_upb\"),\n      col(\"principal_forgiveness_upb\"),\n      col(\"repurchase_make_whole_proceeds_flag\"),\n      col(\"foreclosure_principal_write_off_amount\"),\n      col(\"servicing_activity_indicator\"),\n      col('quarter')\n    )\n\n    return perfDf.select(\"*\").filter(\"current_actual_upb != 0.0\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Quarter Information from CSV Filenames\nDESCRIPTION: Defines a function to extract the quarter information from CSV filenames using PySpark's SQL string manipulation functions. This enables data to be organized chronologically based on the financial reporting quarter.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _get_quarter_from_csv_file_name():\n    return substring_index(substring_index(input_file_name(), '.', 1), '/', -1)\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance on Test Data\nDESCRIPTION: Transforms the test dataset using the trained model, displays predictions, and calculates accuracy using MulticlassClassificationEvaluator.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprintln(\"\\n------ Transforming ------\")\nval (results, _) = Benchmark.time(\"transform\") {\n  val ret = xgbClassificationModel.transform(transSet).cache()\n  ret.foreachPartition((_: Iterator[_]) => ())\n  ret\n}\nresults.select(\"orig_channel\", labelColName,\"rawPrediction\",\"probability\",\"prediction\").show(10)\n\nprintln(\"\\n------Accuracy of Evaluation------\")\nval evaluator = new MulticlassClassificationEvaluator().setLabelCol(labelColName)\nval accuracy = evaluator.evaluate(results)\nprintln(accuracy)\n```\n\n----------------------------------------\n\nTITLE: Using lead Window Function in Spark SQL\nDESCRIPTION: The lead function returns N entries ahead of the current one in window operations. It supports various data types with limited timezone support for TIMESTAMP types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_39\n\nLANGUAGE: SQL\nCODE:\n```\nlead(column, offset, default)\n```\n\n----------------------------------------\n\nTITLE: Parsing Dates in Mortgage Performance Data with PySpark\nDESCRIPTION: Function that transforms date columns in the performance dataframe from string format to date format, and extracts additional date components like month, year, and day for analysis purposes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _parse_dates(perf):\n    return perf \\\n            .withColumn('monthly_reporting_period', to_date(col('monthly_reporting_period'), 'MM/dd/yyyy')) \\\n            .withColumn('monthly_reporting_period_month', month(col('monthly_reporting_period'))) \\\n            .withColumn('monthly_reporting_period_year', year(col('monthly_reporting_period'))) \\\n            .withColumn('monthly_reporting_period_day', dayofmonth(col('monthly_reporting_period'))) \\\n            .withColumn('last_paid_installment_date', to_date(col('last_paid_installment_date'), 'MM/dd/yyyy')) \\\n            .withColumn('foreclosed_after', to_date(col('foreclosed_after'), 'MM/dd/yyyy')) \\\n            .withColumn('disposition_date', to_date(col('disposition_date'), 'MM/dd/yyyy')) \\\n            .withColumn('maturity_date', to_date(col('maturity_date'), 'MM/yyyy')) \\\n            .withColumn('zero_balance_effective_date', to_date(col('zero_balance_effective_date'), 'MM/yyyy'))\n```\n\n----------------------------------------\n\nTITLE: Defining Schema and Parameters for XGBoost Mortgage Model\nDESCRIPTION: Sets up the data schema for the mortgage dataset with 27 columns (26 features and 1 label), extracts feature names, and defines common XGBoost training parameters.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nval labelColName = \"delinquency_12\"\nval schema = StructType(List(\n  StructField(\"orig_channel\", DoubleType),\n  StructField(\"first_home_buyer\", DoubleType),\n  StructField(\"loan_purpose\", DoubleType),\n  StructField(\"property_type\", DoubleType),\n  StructField(\"occupancy_status\", DoubleType),\n  StructField(\"property_state\", DoubleType),\n  StructField(\"product_type\", DoubleType),\n  StructField(\"relocation_mortgage_indicator\", DoubleType),\n  StructField(\"seller_name\", DoubleType),\n  StructField(\"mod_flag\", DoubleType),\n  StructField(\"orig_interest_rate\", DoubleType),\n  StructField(\"orig_upb\", DoubleType),\n  StructField(\"orig_loan_term\", IntegerType),\n  StructField(\"orig_ltv\", DoubleType),\n  StructField(\"orig_cltv\", DoubleType),\n  StructField(\"num_borrowers\", DoubleType),\n  StructField(\"dti\", DoubleType),\n  StructField(\"borrower_credit_score\", DoubleType),\n  StructField(\"num_units\", IntegerType),\n  StructField(\"zip\", IntegerType),\n  StructField(\"mortgage_insurance_percent\", DoubleType),\n  StructField(\"current_loan_delinquency_status\", IntegerType),\n  StructField(\"current_actual_upb\", DoubleType),\n  StructField(\"interest_rate\", DoubleType),\n  StructField(\"loan_age\", DoubleType),\n  StructField(\"msa\", DoubleType),\n  StructField(\"non_interest_bearing_upb\", DoubleType),\n  StructField(labelColName, IntegerType)))\n\nval featureNames = schema.filter(_.name != labelColName).map(_.name).toArray\n\nval commParamMap = Map(\n  \"objective\" -> \"binary:logistic\",\n  \"num_round\" -> 100)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading XGBoost Model for Inference\nDESCRIPTION: Demonstrates how to save the trained model to disk, load it back into memory, and use it for making predictions on the test dataset.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nxgbClassificationModel.write.overwrite.save(dataRoot + \"/model/\")\n\nval modelFromDisk = XGBoostClassificationModel.load(dataRoot + \"/model/\")\n\nval (results2, _) = Benchmark.time(\"transform2\") {\n  modelFromDisk.transform(transSet)\n}\nresults2.show(10)\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Reader Type for RAPIDS Accelerator\nDESCRIPTION: Sets the ORC reader type for optimized GPU-accelerated reading. Options include PERFILE, COALESCING, and MULTITHREADED, with AUTO as the default to select the best option based on the environment.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nspark.rapids.sql.format.orc.reader.type=AUTO\n```\n\n----------------------------------------\n\nTITLE: Hash Partitioning in Spark RAPIDS\nDESCRIPTION: This snippet describes the hash-based partitioning mechanism available in Spark RAPIDS, allowing efficient data distribution across partitions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_57\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.partitioning.HashPartitioning\"></a>spark.rapids.sql.partitioning.HashPartitioning|Hash based partitioning|true|None|\n```\n\n----------------------------------------\n\nTITLE: Configuring and Creating GPU-Accelerated Spark Session\nDESCRIPTION: Sets up a SparkSession with NVIDIA RAPIDS GPU acceleration by configuring GPU resources, memory allocation, and performance parameters. The configuration optimizes for GPU-accelerated data processing with specific settings for batch sizes and memory management.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif \"sc\" in globals():\n    sc.stop()\n\n### Configure the parameters based on your dataproc cluster ###\nconf = SparkConf().setAppName(\"MortgageETL\")\nconf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\nconf.set(\"spark.executor.instances\", \"2\")\nconf.set(\"spark.executor.cores\", \"20\")\nconf.set(\"spark.task.resource.gpu.amount\", \"0.05\")\nconf.set(\"spark.task.cpus\", \"1\")\nconf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\nconf.set(\"spark.executor.memory\", \"20g\")\nconf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\nconf.set(\"spark.executor.resource.gpu.amount\", \"1\")\nconf.set('spark.rapids.sql.batchSizeBytes', '512M')\nconf.set('spark.rapids.sql.reader.batchSizeBytes', '768M')\nconf.set(\"spark.rapids.memory.pinnedPool.size\", \"2G\")\nconf.set(\"spark.executor.memoryOverhead\", \"2G\")\nconf.set(\"spark.sql.broadcastTimeout\", \"700\")\n\nspark = SparkSession.builder \\\n                    .config(conf=conf) \\\n                    .getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Using Division Operator in Spark\nDESCRIPTION: The division operator (/) performs division on numeric types. It supports Double and Decimal data types for both left and right hand sides, and returns the respective numeric type.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\n`/`\n```\n\n----------------------------------------\n\nTITLE: CSV to Parquet Transcoding with Spark in Python\nDESCRIPTION: This Python code snippet uses Apache Spark to read raw CSV data, transcode it to Parquet format, and write it to specified S3 locations. It configures `spark.sql.files.maxPartitionBytes` to control file sizes and calls the `read_raw_csv` function (not defined in this snippet). It extracts acquisition and performance columns using `extract_acq_columns` and `extract_perf_columns` (also not defined here), and writes the results to Parquet format.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Lets transcode the data first\nstart = time.time()\n# we want a few big files instead of lots of small files\nspark.conf.set('spark.sql.files.maxPartitionBytes', '2G')\nrawDf = read_raw_csv(spark, orig_raw_path)\n\nrawDf.write.parquet(output_csv2parquet, mode='overwrite')\nrawDf = spark.read.parquet(output_csv2parquet)\n\nacq = extract_acq_columns(rawDf)\nacq.repartition(20).write.parquet(tmp_acq_path, mode='overwrite')\nperf = extract_perf_columns(rawDf)\nperf.coalesce(80).write.parquet(tmp_perf_path, mode='overwrite')\nend = time.time()\nprint(end - start)\n```\n\n----------------------------------------\n\nTITLE: Software Requirements for RAPIDS Accelerator v24.06.0\nDESCRIPTION: Detailed software requirements for running RAPIDS Accelerator for Apache Spark, including supported operating systems, NVIDIA driver versions, runtime environments, and compatible Spark versions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_9\n\nLANGUAGE: plain\nCODE:\n```\nOS: Ubuntu 20.04, Ubuntu 22.04, CentOS 7, or Rocky Linux 8\n\nNVIDIA Driver*: R470+\n\nRuntime: \n        Scala 2.12, 2.13\n        Python, Java Virtual Machine (JVM) compatible with your spark-version. \n\n        * Check the Spark documentation for Python and Java version compatibility with your specific \n        Spark version. For instance, visit `https://spark.apache.org/docs/3.4.1` for Spark 3.4.1.\n\nSupported Spark versions:\n        Apache Spark 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4\n        Apache Spark 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4\n        Apache Spark 3.4.0, 3.4.1, 3.4.2, 3.4.3\n        Apache Spark 3.5.0, 3.5.1\n\nSupported Databricks runtime versions for Azure and AWS:\n        Databricks 11.3 ML LTS (GPU, Scala 2.12, Spark 3.3.0)\n        Databricks 12.2 ML LTS (GPU, Scala 2.12, Spark 3.3.2)\n        Databricks 13.3 ML LTS (GPU, Scala 2.12, Spark 3.4.1)\n\nSupported Dataproc versions (Debian/Ubuntu):\n        GCP Dataproc 2.0\n        GCP Dataproc 2.1\n\nSupported Dataproc Serverless versions:\n        Spark runtime 1.1 LTS\n        Spark runtime 2.0\n        Spark runtime 2.1\n        Spark runtime 2.2\n```\n\n----------------------------------------\n\nTITLE: Displaying Spark SQL Physical Query Plan\nDESCRIPTION: Example output of a Spark SQL EXPLAIN statement showing the physical query plan with CPU-based execution.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/README.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n== Physical Plan ==\n*(7) Sort [o_orderpriority#5 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(o_orderpriority#5 ASC NULLS FIRST, 200), true, [id=#446]\n   +- *(6) HashAggregate(keys=[o_orderpriority#5], functions=[count(1)])\n      +- Exchange hashpartitioning(o_orderpriority#5, 200), true, [id=#442]\n         +- *(5) HashAggregate(keys=[o_orderpriority#5], functions=[partial_count(1)])\n            +- *(5) Project [o_orderpriority#5]\n               +- SortMergeJoin [o_orderkey#0L], [l_orderkey#18L], LeftSemi\n                  :- *(2) Sort [o_orderkey#0L ASC NULLS FIRST], false, 0\n                  :  +- Exchange hashpartitioning(o_orderkey#0L, 200), true, [id=#424]\n                  :     +- *(1) Project [o_orderkey#0L, o_orderpriority#5]\n                  :        +- *(1) Filter ((isnotnull(o_orderdate#4) AND (o_orderdate#4 >= 8582)) AND (o_orderdate#4 < 8674))\n                  :           +- *(1) ColumnarToRow\n                  :              +- BatchScan[o_orderkey#0L, o_orderdate#4, o_orderpriority#5] ParquetScan Location: InMemoryFileIndex[file:/home/example/parquet/orders.tbl], ReadSchema: struct<o_orderkey:bigint,o_orderdate:date,o_orderpriority:string>\n                  +- *(4) Sort [l_orderkey#18L ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(l_orderkey#18L, 200), true, [id=#433]\n                        +- *(3) Project [l_orderkey#18L]\n                           +- *(3) Filter (((l_commitdate#29 < l_receiptdate#30) AND isnotnull(l_commitdate#29)) AND isnotnull(l_receiptdate#30))\n                              +- *(3) ColumnarToRow\n                                 +- BatchScan[l_orderkey#18L, l_commitdate#29, l_receiptdate#30] ParquetScan Location: InMemoryFileIndex[file:/home/example/parquet/lineitem.tbl], ReadSchema: struct<l_orderkey:bigint,l_commitdate:date,l_receiptdate:date>\n```\n\n----------------------------------------\n\nTITLE: Customizing Data Generation for Integration Tests in Spark-RAPIDS\nDESCRIPTION: Python code example demonstrating how to use the data_gen.py library to generate test data with custom special cases. This example creates an IntegerGen instance that produces a lot of 5s with higher weight.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nIntegerGen().with_special_case(5, weight=200)\n```\n\n----------------------------------------\n\nTITLE: Max Aggregation Function\nDESCRIPTION: Implementation of max aggregate function with support for primitive types, timestamps (UTC only) and some complex types. Includes window operation capabilities.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_75\n\nLANGUAGE: SQL\nCODE:\n```\nmax\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with Performance Measurement\nDESCRIPTION: Trains the XGBoost classifier on the training dataset while measuring the execution time using the Benchmark utility.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n// Start training\nprintln(\"\\n------ Training ------\")\nval (xgbClassificationModel, _) = Benchmark.time(\"train\") {\n  xgbClassifier.fit(trainSet)\n}\n```\n\n----------------------------------------\n\nTITLE: Using Dense_Rank Window Function in Spark\nDESCRIPTION: The dense_rank window function returns the dense rank value within the aggregation window. It supports various data types for ordering and returns an integer result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\n`dense_rank`\n```\n\n----------------------------------------\n\nTITLE: Implementing Case When Expression\nDESCRIPTION: This snippet implements a SQL CASE WHEN expression that evaluates a predicate and returns corresponding values based on its boolean result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nCASE WHEN expression\nNone\npredicate\nresult\n\n```\n\n----------------------------------------\n\nTITLE: Cast Operation Support Matrix in Markdown\nDESCRIPTION: A detailed matrix showing cast operation support between different data types. The matrix uses 'S' for supported operations, 'PS' for partially supported operations with specific conditions, and 'NS' for unsupported operations. Special attention is given to timestamp conversions where only UTC timezone is supported, and floating-point to string conversions which require specific configuration.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_82\n\nLANGUAGE: markdown\nCODE:\n```\n### `Cast`\n\n<table>\n<tr><th rowSpan=\"2\" colSpan=\"2\"></th><th colSpan=\"20\">TO</th></tr>\n<tr>\n<th>BOOLEAN</th>\n<th>BYTE</th>\n<th>SHORT</th>\n<th>INT</th>\n<th>LONG</th>\n<th>FLOAT</th>\n<th>DOUBLE</th>\n<th>DATE</th>\n<th>TIMESTAMP</th>\n<th>STRING</th>\n<th>DECIMAL</th>\n<th>NULL</th>\n<th>BINARY</th>\n<th>CALENDAR</th>\n<th>ARRAY</th>\n<th>MAP</th>\n<th>STRUCT</th>\n<th>UDT</th>\n<th>DAYTIME</th>\n<th>YEARMONTH</th>\n</tr>\n[... table content truncated for brevity ...]\n```\n\n----------------------------------------\n\nTITLE: Date Parsing for Mortgage Performance Data\nDESCRIPTION: Function to parse and transform various date fields in mortgage performance data using PySpark date functions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _parse_dates(perf):\n    return perf \\\n            .withColumn('monthly_reporting_period', to_date(col('monthly_reporting_period'), 'MM/dd/yyyy')) \\\n            .withColumn('monthly_reporting_period_month', month(col('monthly_reporting_period'))) \\\n            .withColumn('monthly_reporting_period_year', year(col('monthly_reporting_period'))) \\\n            .withColumn('monthly_reporting_period_day', dayofmonth(col('monthly_reporting_period'))) \\\n            .withColumn('last_paid_installment_date', to_date(col('last_paid_installment_date'), 'MM/dd/yyyy')) \\\n            .withColumn('foreclosed_after', to_date(col('foreclosed_after'), 'MM/dd/yyyy')) \\\n            .withColumn('disposition_date', to_date(col('disposition_date'), 'MM/dd/yyyy')) \\\n            .withColumn('maturity_date', to_date(col('maturity_date'), 'MM/yyyy')) \\\n            .withColumn('zero_balance_effective_date', to_date(col('zero_balance_effective_date'), 'MM/yyyy'))\n```\n\n----------------------------------------\n\nTITLE: Setting GPU-Specific XGBoost Parameters\nDESCRIPTION: Configures XGBoost parameters specifically for GPU-accelerated training, including setting the tree method to 'gpu_hist' and number of workers to match the GPU count.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nval xgbParamFinal = commParamMap ++ Map(\"tree_method\" -> \"gpu_hist\", \"num_workers\" -> 1)\n```\n\n----------------------------------------\n\nTITLE: Round Robin Partitioning in Spark RAPIDS\nDESCRIPTION: This snippet outlines the round-robin partitioning mechanism used within Spark RAPIDS, allowing balanced distribution of data across partitions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_59\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.partitioning.RoundRobinPartitioning\"></a>spark.rapids.sql.partitioning.RoundRobinPartitioning|Round robin partitioning|true|None|\n```\n\n----------------------------------------\n\nTITLE: Importing PySpark Libraries for ETL Processing\nDESCRIPTION: Imports necessary PySpark modules and functions for data processing, including SparkSession for managing Spark context, SQL functions for data transformations, and Window functions for analytical processing.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom pyspark import broadcast\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\n\n```\n\n----------------------------------------\n\nTITLE: Using DayOfYear SQL Function in Spark\nDESCRIPTION: The dayofyear function returns the day of the year from a date or timestamp input. This function supports Date type input and returns an integer result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\n`dayofyear`\n```\n\n----------------------------------------\n\nTITLE: Creating Benchmark Utility for Performance Measurement\nDESCRIPTION: Defines a utility object to measure and report the elapsed time for different operations like training and transformation, helping to compare performance.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nobject Benchmark {\n  def time[R](phase: String)(block: => R): (R, Float) = {\n    val t0 = System.currentTimeMillis\n    val result = block // call-by-name\n    val t1 = System.currentTimeMillis\n    println(\"Elapsed time [\" + phase + \"]: \" + ((t1 - t0).toFloat / 1000) + \"s\")\n    (result, (t1 - t0).toFloat / 1000)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Map in SQL\nDESCRIPTION: This function creates a map from specified key-value pairs. The keys and values are required to be of supported types. The output is a map structure that can be utilized within SQL statements.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\nCreateMap\n`map`\nCreate a map\nNone\nproject\nkey\nS\n...\nvalue\nS\n...\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PySpark Data Processing\nDESCRIPTION: Imports essential Python and PySpark libraries needed for data processing. This includes time and os modules from the standard library, and various components from PySpark for creating sessions, working with SQL functions, defining data types, and using window functions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport os\nfrom pyspark import broadcast\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for RAPIDS Plugin Configuration Using Scala REPL\nDESCRIPTION: This snippet demonstrates how to generate documentation for RAPIDS Plugin configuration options by using the Scala REPL. The code imports the RapidsConf class and calls the help method to display all available configuration options.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/README.md#2025-04-19_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nscala> import com.nvidia.spark.rapids.RapidsConf\nimport com.nvidia.spark.rapids.RapidsConf\n\nscala> RapidsConf.help(true)\n# Rapids Plugin 4 Spark Configuration\nThe following is the list of options that `rapids-plugin-4-spark` supports.\n\nOn startup use: `--conf [conf key]=[conf value]`. For example:\n\n[...]\n```\n\n----------------------------------------\n\nTITLE: Defining Shim Metadata with JSON Lines Comment in Scala\nDESCRIPTION: Example of the special JSON Lines comment format used to specify which Spark builds a shim file applies to. Each line inside the comment tags represents a Spark build version in JSON format, with the minimum version representing the 'owner shim'.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n/*** spark-rapids-shim-json-lines\n{\"spark\": \"320\"}\n{\"spark\": \"323\"}\nspark-rapids-shim-json-lines ***/\n```\n\n----------------------------------------\n\nTITLE: Setting Evaluation Set for XGBoost Model\nDESCRIPTION: Configures the evaluation dataset for monitoring model performance during training using the setEvalSets API method.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nxgbClassifier.setEvalSets(Map(\"eval\" -> evalSet))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries for PySpark\nDESCRIPTION: Imports essential Python and PySpark modules required for data processing, including time utilities, SparkConf for configuration, SparkSession for session management, and SQL functions for data manipulation.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport os\nfrom pyspark import broadcast, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\n```\n\n----------------------------------------\n\nTITLE: Using DayOfWeek SQL Function in Spark\nDESCRIPTION: The dayofweek function returns the day of the week (1 = Sunday to 7 = Saturday) for a given date input. This function supports Date type input and returns an integer result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\n`dayofweek`\n```\n\n----------------------------------------\n\nTITLE: Using MinBy Aggregate Function in SQL\nDESCRIPTION: The MinBy function is an aggregate operator that returns the value associated with the minimum value in the ordering column. Similar to MaxBy, it may produce different results than CPU in certain scenarios with multiple minimum values.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_78\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT min_by(value_column, ordering_column) FROM table_name\n```\n\n----------------------------------------\n\nTITLE: CSV Reading Function with PySpark\nDESCRIPTION: Function to read raw CSV mortgage data using PySpark, with specific options for null values, headers, and delimiters.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef read_raw_csv(spark, path):\n    return spark.read.format('csv') \\\n            .option('nullValue', '') \\\n            .option('header', False) \\\n            .option('delimiter', '|') \\\n            .schema(_csv_raw_schema) \\\n            .load(path) \\\n            .withColumn('quarter', _get_quarter_from_csv_file_name())\n```\n\n----------------------------------------\n\nTITLE: Starting Spark Shell with DataGen JAR\nDESCRIPTION: Launches a Spark shell with the datagen library in the classpath. This example uses version 25.06.0 built for Spark 3.3.0.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nspark-shell --jars target/datagen_2.12-25.06.0-spark330.jar\n```\n\n----------------------------------------\n\nTITLE: Running PySpark Integration Tests with Specific Test Selection\nDESCRIPTION: Examples of running integration tests using the provided script with different pytest options to target specific tests, modules, or keywords.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n## running all integration tests for Map\n./integration_tests/run_pyspark_from_build.sh -k map_test.py\n## Running a single integration test in map_test\n./integration_tests/run_pyspark_from_build.sh -k 'map_test.py and test_map_integration_1'\n## Running tests marching the keyword \"exist\" from any module\n./integration_tests/run_pyspark_from_build.sh -k exist\n## Running all parametrization of the method arithmetic_ops_test.py::test_addition\n## and a specific parametrization of array_test.py::test_array_exists\nTESTS=\"arithmetic_ops_test.py::test_addition array_test.py::test_array_exists[3VL:off-data_gen0]\" ./integration_tests/run_pyspark_from_build.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring CorrelatedKeyGroup for Related Column Values\nDESCRIPTION: Demonstrates how to use CorrelatedKeyGroup to ensure that columns have correlated values. This is useful for creating key relationships where multiple columns need to maintain consistent relationships.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a byte, b long\", 3)\ndataTable.toDF(spark).show()\ndataTable.configureKeyGroup(Seq(\"a\", \"b\"), CorrelatedKeyGroup(1, 0, 1), FlatDistribution())\ndataTable.toDF(spark).show()\ndataTable(\"b\").setValueRange(Byte.MinValue, Byte.MaxValue)\ndataTable.toDF(spark).show()\n```\n\n----------------------------------------\n\nTITLE: Loading Training, Evaluation, and Transformation Datasets\nDESCRIPTION: Reads the parquet files from the specified paths to create training, evaluation, and transformation datasets for the XGBoost model.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nval trainSet = reader.parquet(trainPath)\nval evalSet  = reader.parquet(evalPath)\nval transSet = reader.parquet(transPath)\n```\n\n----------------------------------------\n\nTITLE: Decimal Precision and Scale Rules in Spark Operations\nDESCRIPTION: Documents how decimal precision and scale are calculated for various operations in Apache Spark. Shows formulas for addition, subtraction, multiplication, division, modulo, and union operations between decimal numbers.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n * In particular, if we have expressions e1 and e2 with precision/scale p1/s1 and p2/s2\\n * respectively, then the following operations have the following precision / scale:\\n *\\n *   Operation    Result Precision                        Result Scale\\n *   ------------------------------------------------------------------------\\n *   e1 + e2      max(s1, s2) + max(p1-s1, p2-s2) + 1     max(s1, s2)\\n *   e1 - e2      max(s1, s2) + max(p1-s1, p2-s2) + 1     max(s1, s2)\\n *   e1 * e2      p1 + p2 + 1                             s1 + s2\\n *   e1 / e2      p1 - s1 + s2 + max(6, s1 + p2 + 1)      max(6, s1 + p2 + 1)\\n *   e1 % e2      min(p1-s1, p2-s2) + max(s1, s2)         max(s1, s2)\\n *   e1 union e2  max(s1, s2) + max(p1-s1, p2-s2)         max(s1, s2)\n```\n\n----------------------------------------\n\nTITLE: Creating Acquisition Data Frame with PySpark\nDESCRIPTION: This function creates an acquisition data frame by joining acquisition data with a name mapping DataFrame for seller name normalization. It also processes date columns to ensure consistent date formats. Dependencies include PySpark and configuration of name mappings in _name_mapping.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _create_acquisition(spark, acq):\n    nameMapping = spark.createDataFrame(_name_mapping, [\"from_seller_name\", \"to_seller_name\"])\n    return acq.join(nameMapping, col(\"seller_name\") == col(\"from_seller_name\"), \"left\") \\\n      .drop(\"from_seller_name\") \\\n      .withColumn(\"old_name\", col(\"seller_name\")) \\\n      .withColumn(\"seller_name\", coalesce(col(\"to_seller_name\"), col(\"seller_name\"))) \\\n      .drop(\"to_seller_name\") \\\n      .withColumn(\"orig_date\", to_date(col(\"orig_date\"), \"MM/yyyy\")) \\\n      .withColumn(\"first_pay_date\", to_date(col(\"first_pay_date\"), \"MM/yyyy\"))\n```\n\n----------------------------------------\n\nTITLE: PivotFirst Operation Support\nDESCRIPTION: Pivot operation support showing compatibility across data types. Includes special handling for timestamps and restrictions on complex types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_80\n\nLANGUAGE: SQL\nCODE:\n```\nPIVOT(value_column FOR pivot_column IN (pivot_values))\n```\n\n----------------------------------------\n\nTITLE: Date Subtraction in SQL\nDESCRIPTION: This function determines a date that is a specified number of days before a starting date. The input requires a start date and a number of days to subtract, producing a new date as output.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nDateSub\n`date_sub`\nReturns the date that is num_days before start_date\nNone\nproject\nstartDate\n...\ndays\n...\nresult\n...\n```\n\n----------------------------------------\n\nTITLE: Multiplication Operation Support\nDESCRIPTION: Defines support for multiplication operator (*) across different data types in both project and AST contexts. Supports numeric types with some limitations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_51\n\nLANGUAGE: SQL\nCODE:\n```\n*\n```\n\n----------------------------------------\n\nTITLE: Loading Common XGBoost Libraries for Spark\nDESCRIPTION: Imports essential libraries for using XGBoost with Spark, including the XGBoost classifier and model classes, SparkSession, evaluation metrics, and data structure types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport ml.dmlc.xgboost4j.scala.spark.{XGBoostClassifier, XGBoostClassificationModel}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}\n```\n\n----------------------------------------\n\nTITLE: Verifying Jar Signature with GPG\nDESCRIPTION: This snippet demonstrates how to verify the signature of a RAPIDS Accelerator for Apache Spark JAR file using GPG (GNU Privacy Guard). It involves using the `gpg --verify` command to check the signature against the downloaded JAR and ASC files, ensuring the integrity and authenticity of the downloaded package.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n\"gpg --verify rapids-4-spark_2.12-24.04.0.jar.asc rapids-4-spark_2.12-24.04.0.jar\"\n```\n\n----------------------------------------\n\nTITLE: Generating Large Datasets with Unique Values\nDESCRIPTION: Creates a dataset with 10 million rows and demonstrates how to check the unique count of values in columns.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string, b long\", 10000000)\ndataTable.toDF(spark).selectExpr(\"COUNT(DISTINCT a)\", \"COUNT(b)\").show\n```\n\n----------------------------------------\n\nTITLE: Workaround for Conditional Overflow Exception in ANSI Mode (Scala)\nDESCRIPTION: This Scala code snippet demonstrates a workaround for the overflow exception issue in ANSI mode. It reorders the operations to perform the conditional check before the addition, avoiding the potential overflow in the RAPIDS Accelerator implementation.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nSeq(0L, Long.MaxValue).toDF(\"val\")\n    .repartition(1) // The repartition makes Spark not optimize selectExpr away\n    .selectExpr(\"IF(val > 1000, null, val) + 1 as ret\")\n    .show()\n```\n\n----------------------------------------\n\nTITLE: Murmur3 Hash Function Support\nDESCRIPTION: Implementation of Murmur3 hash operator with support for most primitive types and some complex types. Has restrictions on timestamp timezone (UTC only) and certain complex data types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_52\n\nLANGUAGE: SQL\nCODE:\n```\nhash\n```\n\n----------------------------------------\n\nTITLE: SQL Map Operations Support Matrix\nDESCRIPTION: Support matrix for map operations (map_concat, map_entries, map_filter) showing compatibility with different data types and limitations around timestamp and other specialized types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_47\n\nLANGUAGE: SQL\nCODE:\n```\nmap_concat(MAP) -> MAP\nmap_entries(MAP) -> ARRAY\nmap_filter(MAP, function) -> MAP\n```\n\n----------------------------------------\n\nTITLE: Using array_min Function in SQL\nDESCRIPTION: Returns the minimum value in the array. Only supports arrays with non-binary, non-calendar, non-struct, and non-UDT types. For timestamp arrays, only UTC timezone is supported.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\narray_min(input)\n```\n\n----------------------------------------\n\nTITLE: Using reverse Function in Spark SQL\nDESCRIPTION: Returns a reversed string or an array with reverse order of elements. The function supports string input and arrays with various element types, with some limitations on timestamp timezone support.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_61\n\nLANGUAGE: SQL\nCODE:\n```\nreverse(input)\n```\n\n----------------------------------------\n\nTITLE: Date Addition in SQL\nDESCRIPTION: This function calculates a date that is a specified number of days after a given starting date. It requires a start date and a number of days to add as parameters, returning a new date.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\nDateAdd\n`date_add`\nReturns the date that is num_days after start_date\nNone\nproject\nstartDate\n...\ndays\n...\nresult\n...\n```\n\n----------------------------------------\n\nTITLE: Building for a Specific Spark Version\nDESCRIPTION: Maven command to build the RAPIDS Accelerator for a specific Spark version using the buildver parameter. This example shows building for Spark 3.2.0.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmvn -Dbuildver=320 verify\n```\n\n----------------------------------------\n\nTITLE: Setting Up ScalaTest Environment for RAPIDS Integration Tests\nDESCRIPTION: Scala code for importing ScalaTest and configuring the test resource finder to locate test files. This setup is required before running individual test suites.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\nimport org.scalatest._\ncom.nvidia.spark.rapids.TestResourceFinder.setPrefix(PATH_TO_TEST_FILES)\n```\n\n----------------------------------------\n\nTITLE: Creating a Named Struct in SQL\nDESCRIPTION: This function creates a structured data type with declared field names paired with their corresponding values. It outputs a struct, which can include nested data types as long as they are supported.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nCreateNamedStruct\n`named_struct`, `struct`\nCreates a struct with the given field names and values\nNone\nproject\nname\n...\nresult\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Numeric Columns for Mortgage Analysis in Python\nDESCRIPTION: Defines lists of numeric column names used for mortgage data analysis, including interest rates, loan terms, credit scores and other financial metrics.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlabel_col_name = \"delinquency_12\"\nnumeric_col_names = [\n        \"orig_interest_rate\",\n        \"orig_upb\",\n        \"orig_loan_term\",\n        \"orig_ltv\",\n        \"orig_cltv\",\n        \"num_borrowers\",\n        \"dti\",\n        \"borrower_credit_score\",\n        \"num_units\",\n        \"zip\",\n        \"mortgage_insurance_percent\",\n        \"current_loan_delinquency_status\",\n        \"current_actual_upb\",\n        \"interest_rate\",\n        \"loan_age\",\n        \"msa\",\n        \"non_interest_bearing_upb\",\n        label_col_name\n]\nall_col_names = cate_col_names + numeric_col_names\n```\n\n----------------------------------------\n\nTITLE: Creating XGBoost Classifier with Features and Label\nDESCRIPTION: Initializes an XGBoost classifier with the specified parameters, label column, and feature columns for training the mortgage delinquency prediction model.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nval xgbClassifier = new XGBoostClassifier(xgbParamFinal)\n      .setLabelCol(labelColName)\n      .setFeaturesCol(featureNames)\n```\n\n----------------------------------------\n\nTITLE: Using Equal Operators in Spark\nDESCRIPTION: The equal operators (== and =) check if two values are equal. They support most primitive data types and some complex types like structs, returning a Boolean result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\n`==`, `=`\n```\n\n----------------------------------------\n\nTITLE: Using row_number Function in Spark SQL\nDESCRIPTION: Window function that returns the index for the row within the aggregation window. The function supports various data types for ordering, with some limitations on complex types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_64\n\nLANGUAGE: SQL\nCODE:\n```\nrow_number() OVER (ORDER BY ordering)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAPIDS Accelerated UDF Interface in Java\nDESCRIPTION: The RapidsUDF interface requires implementing the evaluateColumnar method to process data in columnar format on the GPU. This method takes the number of rows and input ColumnVector arguments, and must return a ColumnVector result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nai.rapids.cudf.ColumnVector evaluateColumnar(int numRows, ai.rapids.cudf.ColumnVector... args);\n```\n\n----------------------------------------\n\nTITLE: Creating SparkSession and Adding Jar Dependencies\nDESCRIPTION: Demonstrates how to create a SparkSession and optionally add jar dependencies using Toree magic commands. This setup is required for executing Spark operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.SparkSession\nval spark = SparkSession.builder().appName(\"mortgage-GPU\").getOrCreate\n%AddJar file:/data/libs/rapids-4-spark-XXX.jar\n%AddJar file:/data/libs/xgboost4j-spark-gpu_2.12-XXX.jar\n%AddJar file:/data/libs/xgboost4j-gpu_2.12-XXX.jar\n// ...\n```\n\n----------------------------------------\n\nTITLE: Using like Operator in Spark SQL\nDESCRIPTION: The like operator performs pattern matching on string data. It supports string inputs and returns a boolean indicating if the pattern matches.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_44\n\nLANGUAGE: SQL\nCODE:\n```\nstring_column like pattern\n```\n\n----------------------------------------\n\nTITLE: Sample Spark Submit Command for Scale Test Data Generation\nDESCRIPTION: Example command for submitting the Scale Test Data Generation job to a Spark cluster. It demonstrates how to configure Spark properties, specify the main class, include required dependencies, and pass the necessary parameters.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/ScaleTest.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$SPARK_HOME/bin/spark-submit \\\n--master spark://<SPARK_MASTER>:7077 \\\n--conf spark.driver.memory=10G \\\n--conf spark.executor.memory=32G \\\n--conf spark.sql.parquet.int96RebaseModeInWrite=CORRECTED \\\n--conf spark.sql.parquet.datetimeRebaseModeInWrite=CORRECTED \\\n--class com.nvidia.rapids.tests.scaletest.ScaleTestDataGen \\ # the main class\n--jars $SPARK_HOME/examples/jars/scopt_2.12-3.7.1.jar \\ # one dependency jar just shipped with Spark under $SPARK_HOME\n./target/datagen_2.12-25.06.0-SNAPSHOT-spark332.jar \\\n1 \\\n10 \\\nparquet \\\n<PATH_TO_SAVE_DATA>\n```\n\n----------------------------------------\n\nTITLE: Launching Spark RAPIDS Scale Test (Bash)\nDESCRIPTION: Example command to launch the Scale Test using spark-submit, demonstrating how to set Spark configurations, specify the main class, and provide arguments for the test run.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/ScaleTest.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$SPARK_HOME/bin/spark-submit \\\n--master spark://<SPARK_MASTER>:7077 \\\n--conf spark.driver.memory=10G \\\n--conf spark.executor.memory=32G \\\n--conf spark.sql.parquet.int96RebaseModeInWrite=CORRECTED \\\n--conf spark.sql.parquet.datetimeRebaseModeInWrite=CORRECTED \\\n--jars $SPARK_HOME/examples/jars/scopt_2.12-3.7.1.jar \\\n--class com.nvidia.spark.rapids.tests.scaletest.ScaleTest \\\n./target/rapids-4-spark-integration-tests_2.12-25.06.0-SNAPSHOT-spark332.jar \\\n10 \\\n100 \\\nparquet \\\n<path-to-data-generated-by-data-gen-tool> \\\n./output \\\n./report.json \\\n--overwrite \\\n--queries q1,q2\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for RAPIDS Accelerator JAR Files\nDESCRIPTION: Commands for verifying the authenticity of downloaded RAPIDS Accelerator JAR files using GPG signatures. Includes steps for importing the public key and verifying both Scala 2.12 and 2.13 versions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/download.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-25.04.0.jar.asc rapids-4-spark_2.12-25.04.0.jar\ngpg --verify rapids-4-spark_2.13-25.04.0.jar.asc rapids-4-spark_2.13-25.04.0.jar\n```\n\n----------------------------------------\n\nTITLE: Configuration Setting for UDF Compiler\nDESCRIPTION: Configuration property to enable UDF to Catalyst expression compilation for GPU acceleration. Set this to true to enable the feature.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/udf-to-catalyst-expressions.md#2025-04-19_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nspark.rapids.sql.udfCompiler.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Conditional Overflow Exception in ANSI Mode (Scala)\nDESCRIPTION: This Scala code snippet demonstrates a scenario where the RAPIDS Accelerator may throw an overflow exception in ANSI mode, while Spark on CPU would not. It shows the difference in behavior between CPU and GPU implementations for conditional operations with potential side effects.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nspark.conf.set(\"spark.sql.ansi.enabled\", \"true\")\n\nSeq(0L, Long.MaxValue).toDF(\"val\")\n    .repartition(1) // The repartition makes Spark not optimize selectExpr away\n    .selectExpr(\"IF(val > 1000, null, val + 1) as ret\")\n    .show()\n```\n\n----------------------------------------\n\nTITLE: Configuring RAPIDS Acceleration for PySpark\nDESCRIPTION: Spark configuration settings for optimizing performance with NVIDIA RAPIDS acceleration, including memory batch sizes for SQL operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set('spark.rapids.sql.explain', 'ALL')\nspark.conf.set('spark.rapids.sql.batchSizeBytes', '512M')\nspark.conf.set('spark.rapids.sql.reader.batchSizeBytes', '768M')\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkSession and DataReader\nDESCRIPTION: Creates a SparkSession with a specific application name and initializes a data reader for loading parquet files.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n// Build the spark session and data reader as usual\nval sparkSession = SparkSession.builder.appName(\"mortgage-gpu\").getOrCreate\nval reader = sparkSession.read\n```\n\n----------------------------------------\n\nTITLE: Using EndsWith Function in Spark\nDESCRIPTION: The EndsWith function checks if a string ends with a specified suffix. It supports String type for the source and requires a literal string value for the search parameter, returning a Boolean result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Structured HTML Table of Spark-RAPIDS Aggregation Functions Support\nDESCRIPTION: An HTML table depicting support status for NVIDIA RAPIDS aggregation functions in Spark. The table shows compatibility for CollectSet, Count, and First functions across different data types, operation contexts (aggregation, reduction, window), with markers indicating full support (S), partial support with limitations (PS), and non-support (NS).\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_72\n\nLANGUAGE: HTML\nCODE:\n```\n<tr>\n<td rowSpan=\"6\">CollectSet</td>\n<td rowSpan=\"6\">`collect_set`</td>\n<td rowSpan=\"6\">Collect a set of unique elements, not supported in reduction</td>\n<td rowSpan=\"6\">None</td>\n<td rowSpan=\"2\">aggregation</td>\n<td>input</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td><em>PS<br/>UTC is only supported TZ for TIMESTAMP</em></td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td><b>NS</b></td>\n<td><b>NS</b></td>\n<td><em>PS<br/>UTC is only supported TZ for child TIMESTAMP;<br/>unsupported child types BINARY, CALENDAR, MAP, UDT, DAYTIME, YEARMONTH</em></td>\n<td><b>NS</b></td>\n<td><em>PS<br/>UTC is only supported TZ for child TIMESTAMP;<br/>unsupported child types BINARY, CALENDAR, MAP, UDT, DAYTIME, YEARMONTH</em></td>\n<td><b>NS</b></td>\n<td><b>NS</b></td>\n<td><b>NS</b></td>\n</tr>\n```\n\n----------------------------------------\n\nTITLE: Verifying RAPIDS Accelerator Package Signature using GPG\nDESCRIPTION: This code demonstrates the process for verifying the digital signature of the RAPIDS Accelerator for Apache Spark using GPG. It includes importing the public key and using the gpg command to verify the signature of the downloaded jar file.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-23.02.0.jar.asc rapids-4-spark_2.12-23.02.0.jar\n```\n\n----------------------------------------\n\nTITLE: Calculating Cube Root\nDESCRIPTION: This snippet provides a SQL function to calculate the cube root of a given input, returning the resulting value. It is applicable for numeric types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\nCube root\nNone\ninput\nresult\n\n```\n\n----------------------------------------\n\nTITLE: Using NullSafe Equal Operator in Spark\nDESCRIPTION: The null-safe equal operator (<=>) checks if two values are equal, including handling nulls. It supports most primitive data types and some complex types like structs, returning a Boolean result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\n`<=>`\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Assignment for Pandas UDF\nDESCRIPTION: Spark configuration to enable GPU assignment/scheduling for Pandas UDF, allowing the Python process to share the same GPU with the Spark executor JVM.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n--conf spark.rapids.sql.python.gpu.enabled=true \\\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Sort Merge Join in Spark RAPIDS\nDESCRIPTION: This snippet specifies the use of sort merge joins as a replacement for shuffled hash joins, enhancing join performance in Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_42\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.SortMergeJoinExec\"></a>spark.rapids.sql.exec.SortMergeJoinExec|Sort merge join, replacing with shuffled hash join|true|None|\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Settings for ETL Execution\nDESCRIPTION: Sets further Spark configurations specific to the ETL execution process, including GPU enablement for RAPIDS SQL and file partition settings, crucial for managing computational resources efficiently.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# GPU run, set to true\nspark.conf.set('spark.rapids.sql.enabled', 'true')\n# CPU run, set to false\n# spark.conf.set('spark.rapids.sql.enabled', 'false')\nspark.conf.set('spark.sql.files.maxPartitionBytes', '1G')\n```\n\n----------------------------------------\n\nTITLE: Processing Mortgage Data with PySpark\nDESCRIPTION: Complete script that processes mortgage data through a main function, saves the results as Parquet files, optionally splits the dataset for machine learning, and reports performance metrics. The script demonstrates data pipeline operations with PySpark including data processing, persistence, and dataset splitting.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# run main function to process data\nout = run_mortgage(spark, perf, acq)\n\n# save processed data\nout.write.parquet(output_path, mode='overwrite')\n\n# save processed data\nif save_train_eval_dataset:\n    etlDf = spark.read.parquet(output_path)\n\n    # split 80% for training, 20% for test\n    splits = etlDf.randomSplit([0.8, 0.2])\n\n    splits[0].write.format('parquet').save(output_path_train, mode=\"overwrite\")\n    splits[1].write.format('parquet').save(output_path_eval, mode=\"overwrite\")\n\n# print explain and time\nprint(out.explain())\nend = time.time()\nprint(end - start)\nspark.stop()\n```\n\n----------------------------------------\n\nTITLE: Using array_join Function in SQL\nDESCRIPTION: Concatenates the elements of the given array using a delimiter and an optional string to replace nulls. If no value is set for nullReplacement, any null value is filtered.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\narray_join(array, delimiter, nullReplacement)\n```\n\n----------------------------------------\n\nTITLE: Building DataGen JAR for Specific Spark Version\nDESCRIPTION: Compiles the datagen library for a specific Spark version. The buildver parameter specifies the compressed version number (e.g., 330 for Spark 3.3.0).\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd datagen\nmvn clean package -Dbuildver=$SPARK_VERSION\n```\n\n----------------------------------------\n\nTITLE: CSV Timestamp Format Examples in Spark Rapids\nDESCRIPTION: List of supported timestamp formats in CSV parsing. All formats must start with a supported date format followed by time portion starting with 'T'.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nHH:mm:ss.SSSXXX\nHH:mm:ss[.SSS][XXX]\nHH:mm:ss[.SSSXXX]\nHH:mm\nHH:mm:ss\nHH:mm[:ss]\nHH:mm:ss.SSS\nHH:mm:ss[.SSS]\n```\n\n----------------------------------------\n\nTITLE: Computing Inverse Hyperbolic Cosine in SQL\nDESCRIPTION: The 'Acosh' function calculates the inverse hyperbolic cosine of a numeric input. It works with double-precision floating-point numbers in both project and AST contexts.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nacosh(numeric_column)\n```\n\n----------------------------------------\n\nTITLE: Using regexp_extract_all Function in Spark SQL\nDESCRIPTION: Extract all strings matching a regular expression corresponding to the regex group index. The function takes a string, a regex pattern, and an index as inputs, and returns an array of matching strings.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_58\n\nLANGUAGE: SQL\nCODE:\n```\nregexp_extract_all(str, regexp, idx)\n```\n\n----------------------------------------\n\nTITLE: Inverse Hyperbolic Sine Operation\nDESCRIPTION: Implementation of asinh function that calculates the inverse hyperbolic sine of a numeric input.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nasinh(double)\n```\n\n----------------------------------------\n\nTITLE: Arrays Zip Operation\nDESCRIPTION: Implementation of arrays_zip function that combines multiple arrays into an array of structs where each struct contains corresponding elements from input arrays.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\narrays_zip(array1, array2, ...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Avro Reader Type for RAPIDS Accelerator\nDESCRIPTION: Sets the Avro reader type for optimized GPU-accelerated reading. Options include PERFILE, COALESCING, and MULTITHREADED, with AUTO as the default to select the best option based on the environment.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nspark.rapids.sql.format.avro.reader.type=AUTO\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark for GPU or CPU Execution\nDESCRIPTION: Spark configuration settings to enable GPU acceleration via RAPIDS or fall back to CPU processing, along with partition size optimization.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# GPU run, set to true\nspark.conf.set('spark.rapids.sql.enabled', 'true')\n# CPU run, set to false\n# spark.conf.set('spark.rapids.sql.enabled', 'false')\nspark.conf.set('spark.sql.files.maxPartitionBytes', '1G')\n```\n\n----------------------------------------\n\nTITLE: Building a Multi-version Distribution Package\nDESCRIPTION: Maven commands to build a distribution jar that supports multiple Spark versions. It involves installing each Spark version individually and then packaging them together with a specific profile.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmvn clean\nmvn -Dbuildver=320 install -Drat.skip=true -DskipTests\nmvn -Dbuildver=321 install -Drat.skip=true -DskipTests\nmvn -Dbuildver=321cdh install -Drat.skip=true -DskipTests\nmvn -pl dist -PnoSnapshots package -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Verifying Signature for RAPIDS Accelerator JAR Files using GPG\nDESCRIPTION: Command-line instructions for verifying the digital signature of RAPIDS Accelerator JAR files using GPG. This includes importing the public key and verifying the signature for both Scala 2.12 and 2.13 builds.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-24.12.1.jar.asc rapids-4-spark_2.12-24.12.1.jar\ngpg --verify rapids-4-spark_2.13-24.12.1.jar.asc rapids-4-spark_2.13-24.12.1.jar\n```\n\n----------------------------------------\n\nTITLE: Inverse Sine Operation\nDESCRIPTION: Implementation of asin function that calculates the inverse sine of a numeric input.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nasin(double)\n```\n\n----------------------------------------\n\nTITLE: Using transform Function in SQL\nDESCRIPTION: Transforms elements in an array using the transform function. This is similar to a `map` in functional programming. The full implementation details are not provided in the given context.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\ntransform(argument)\n```\n\n----------------------------------------\n\nTITLE: Map Operations in Spark-RAPIDS\nDESCRIPTION: SQL functions for map operations including map_from_arrays, map_keys, and map_values with timestamp and data type constraints. UTC timezone is required for timestamp operations, and several data types like BINARY, CALENDAR, UDT, DAYTIME, and YEARMONTH are unsupported.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_48\n\nLANGUAGE: SQL\nCODE:\n```\nmap_from_arrays(keys, values)\nmap_keys(input)\nmap_values(input)\n```\n\n----------------------------------------\n\nTITLE: Running All Unit Tests Against Specific Spark Version\nDESCRIPTION: Command to run all unit tests against Apache Spark 3.2.1 using Maven. This packages the tests module and its dependencies (-pl tests -am) with the specific build version parameter.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/tests/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn package -pl tests -am -Dbuildver=321\n```\n\n----------------------------------------\n\nTITLE: SQL Logarithmic Functions Support Matrix\nDESCRIPTION: Support matrix for logarithmic functions (ln, log10, log1p, log2, log) showing compatibility with different data types. These functions primarily support DOUBLE type inputs and outputs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_45\n\nLANGUAGE: SQL\nCODE:\n```\nln(DOUBLE) -> DOUBLE\nlog10(DOUBLE) -> DOUBLE\nlog1p(DOUBLE) -> DOUBLE\nlog2(DOUBLE) -> DOUBLE\nlog(DOUBLE, DOUBLE) -> DOUBLE\n```\n\n----------------------------------------\n\nTITLE: Setting Parquet Multithreaded Read Parameters for RAPIDS\nDESCRIPTION: Configures the maximum number of files processed in parallel and the number of threads used for reading small Parquet files. These settings affect memory usage and performance for the MULTITHREADED Parquet reader.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\nspark.rapids.sql.format.parquet.multiThreadedRead.maxNumFilesParallel=2147483647\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.rapids.sql.format.parquet.multiThreadedRead.numThreads=None\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Spark Settings in Python\nDESCRIPTION: Sets common configurations for Spark, such as enabling RAPIDS SQL explanations and setting batch size bytes, which influence the execution and performance of Spark jobs. Adjust these settings based on the environment and data size for optimal performance.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set('spark.rapids.sql.explain', 'ALL')\nspark.conf.set('spark.rapids.sql.batchSizeBytes', '512M')\nspark.conf.set('spark.rapids.sql.reader.batchSizeBytes', '768M')\n```\n\n----------------------------------------\n\nTITLE: Running the Scale Test Data Generation Tool with Spark Submit\nDESCRIPTION: Command-line usage pattern for the Scale Test Data Generation tool. The tool accepts parameters for scale factor, complexity, output format, output directory, and optional arguments for specific tables, random seed, and overwrite flag.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/ScaleTest.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nUsage: DataGenEntry [options] <scale factor> <complexity> <format> <output directory>\n\n  <scale factor>        scale factor for data size\n  <complexity>          complexity level for processing\n  <format>              output format for the data\n  <output directory>    output directory for data generated\n  -t, --tables <value>  tables to generate. If not specified, all tables will be generated\n  -d, --seed <value>    seed used to generate random data columns. default is 41 if not specified\n  --overwrite           Flag argument. Whether to overwrite the existing data in the path.\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Non-UTC Timezones in Spark-RAPIDS\nDESCRIPTION: Commands to run integration tests with non-UTC timezones to verify timezone handling. Tests with both non-DST (Asia/Shanghai) and DST (America/Los_Angeles) timezones are recommended.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n$ TZ=Asia/Shanghai ./integration_tests/run_pyspark_from_build.sh\n$ TZ=America/Los_Angeles ./integration_tests/run_pyspark_from_build.sh\n```\n\n----------------------------------------\n\nTITLE: Registering Optimizer Rules for RAPIDS Accelerator in Spark\nDESCRIPTION: This snippet shows how the SQLExecPlugin registers two sets of optimizer rules on startup: ColumnarOverrideRules for general use and GpuQueryStagePrepOverrides specifically for AQE.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/adaptive-query.md#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nextensions.injectColumnar(_ => ColumnarOverrideRules())\nextensions.injectQueryStagePrepRule(_ => GpuQueryStagePrepOverrides())\n```\n\n----------------------------------------\n\nTITLE: Implementing Absolute Value Function in SQL\nDESCRIPTION: The 'Abs' function calculates the absolute value of a numeric input. It supports various numeric data types in both project and AST contexts.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nabs(numeric_column)\n```\n\n----------------------------------------\n\nTITLE: Acquisition Data Processing with PySpark\nDESCRIPTION: Function to process and clean mortgage acquisition data, including name mapping and date formatting.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _create_acquisition(spark, acq):\n    nameMapping = spark.createDataFrame(_name_mapping, [\"from_seller_name\", \"to_seller_name\"])\n    return acq.join(nameMapping, col(\"seller_name\") == col(\"from_seller_name\"), \"left\") \\\n      .drop(\"from_seller_name\") \\\n      .withColumn(\"old_name\", col(\"seller_name\")) \\\n      .withColumn(\"seller_name\", coalesce(col(\"to_seller_name\"), col(\"seller_name\"))) \\\n      .drop(\"to_seller_name\") \\\n      .withColumn(\"orig_date\", to_date(col(\"orig_date\"), \"MM/yyyy\")) \\\n      .withColumn(\"first_pay_date\", to_date(col(\"first_pay_date\"), \"MM/yyyy\"))\n```\n\n----------------------------------------\n\nTITLE: Using MaxBy Aggregate Function in SQL\nDESCRIPTION: The MaxBy function is an aggregate operator that returns the value associated with the maximum value in the ordering column. It may produce different results than CPU when multiple rows in a group have the same minimum value in the ordering column and different associated values in the value column.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_76\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT max_by(value_column, ordering_column) FROM table_name\n```\n\n----------------------------------------\n\nTITLE: Enabling/Disabling CSV Input Acceleration for RAPIDS\nDESCRIPTION: Controls the acceleration of CSV input. When set to false, it disables CSV input acceleration for the RAPIDS Accelerator.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nspark.rapids.sql.format.csv.read.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Memory Efficiency for Python Processes in Spark\nDESCRIPTION: These configuration settings control GPU memory allocation for Python processes in Spark. They specify the GPU pool size, with a default of half the available GPU memory if not set.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n--conf spark.rapids.python.memory.gpu.pooling.enabled=false \\\n--conf spark.rapids.python.memory.gpu.allocFraction=0.1 \\\n--conf spark.rapids.python.memory.gpu.maxAllocFraction= 0.2 \\\n```\n\n----------------------------------------\n\nTITLE: Calling Shimmed Methods with Signature Discrepancies in Scala\nDESCRIPTION: Example of how to call a method through the shim layer to handle signature differences across Spark versions. The SparkShimImpl implementation handles the correct dispatch to version-specific method signatures.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nSparkShimImpl.methodWithDiscrepancies(p_1, ..., p_n)\n```\n\n----------------------------------------\n\nTITLE: Date Formatting in SQL\nDESCRIPTION: This function converts a timestamp into a string formatted according to a specified date format. It requires a timestamp and a format string, returning the formatted date as a string output.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nDateFormatClass\n`date_format`\nConverts timestamp to a value of string in the format specified by the date format\nNone\nproject\ntimestamp\n...\nstrfmt\n...\nresult\n...\n```\n\n----------------------------------------\n\nTITLE: Array Union Operation in Spark-RAPIDS\nDESCRIPTION: Implementation of array_union function that returns union of two arrays without duplicates. Note that GPU implementation treats -0.0 and 0.0 as equal, and handles NaNs differently from older Spark versions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\narray_union(array1, array2)\n```\n\n----------------------------------------\n\nTITLE: Using round Function in Spark SQL\nDESCRIPTION: Round an expression to d decimal places using HALF_UP rounding mode. The function supports various numeric input types and takes an optional scale parameter to specify the number of decimal places.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_63\n\nLANGUAGE: SQL\nCODE:\n```\nround(value, scale)\n```\n\n----------------------------------------\n\nTITLE: Defining CSV Schema Structure with PySpark for Mortgage Loan Data\nDESCRIPTION: Comprehensive definition of a DataFrame schema for mortgage loan data using PySpark's StructType and StructField. The schema includes over 100 fields covering all aspects of mortgage loans including loan details, borrower information, payment history, and foreclosure data.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# File schema\n_csv_raw_schema = StructType([\n      StructField(\"reference_pool_id\", StringType()),\n      StructField(\"loan_id\", LongType()),\n      StructField(\"monthly_reporting_period\", StringType()),\n      StructField(\"orig_channel\", StringType()),\n      StructField(\"seller_name\", StringType()),\n      StructField(\"servicer\", StringType()),\n      StructField(\"master_servicer\", StringType()),\n      StructField(\"orig_interest_rate\", DoubleType()),\n      StructField(\"interest_rate\", DoubleType()),\n      StructField(\"orig_upb\", DoubleType()),\n      StructField(\"upb_at_issuance\", StringType()),\n      StructField(\"current_actual_upb\", DoubleType()),\n      StructField(\"orig_loan_term\", IntegerType()),\n      StructField(\"orig_date\", StringType()),\n      StructField(\"first_pay_date\", StringType()),    \n      StructField(\"loan_age\", DoubleType()),\n      StructField(\"remaining_months_to_legal_maturity\", DoubleType()),\n      StructField(\"adj_remaining_months_to_maturity\", DoubleType()),\n      StructField(\"maturity_date\", StringType()),\n      StructField(\"orig_ltv\", DoubleType()),\n      StructField(\"orig_cltv\", DoubleType()),\n      StructField(\"num_borrowers\", DoubleType()),\n      StructField(\"dti\", DoubleType()),\n      StructField(\"borrower_credit_score\", DoubleType()),\n      StructField(\"coborrow_credit_score\", DoubleType()),\n      StructField(\"first_home_buyer\", StringType()),\n      StructField(\"loan_purpose\", StringType()),\n      StructField(\"property_type\", StringType()),\n      StructField(\"num_units\", IntegerType()),\n      StructField(\"occupancy_status\", StringType()),\n      StructField(\"property_state\", StringType()),\n      StructField(\"msa\", DoubleType()),\n      StructField(\"zip\", IntegerType()),\n      StructField(\"mortgage_insurance_percent\", DoubleType()),\n      StructField(\"product_type\", StringType()),\n      StructField(\"prepayment_penalty_indicator\", StringType()),\n      StructField(\"interest_only_loan_indicator\", StringType()),\n      StructField(\"interest_only_first_principal_and_interest_payment_date\", StringType()),\n      StructField(\"months_to_amortization\", StringType()),\n      StructField(\"current_loan_delinquency_status\", IntegerType()),\n      StructField(\"loan_payment_history\", StringType()),\n      StructField(\"mod_flag\", StringType()),\n      StructField(\"mortgage_insurance_cancellation_indicator\", StringType()),\n      StructField(\"zero_balance_code\", StringType()),\n      StructField(\"zero_balance_effective_date\", StringType()),\n      StructField(\"upb_at_the_time_of_removal\", StringType()),\n      StructField(\"repurchase_date\", StringType()),\n      StructField(\"scheduled_principal_current\", StringType()),\n      StructField(\"total_principal_current\", StringType()),\n      StructField(\"unscheduled_principal_current\", StringType()),\n      StructField(\"last_paid_installment_date\", StringType()),\n      StructField(\"foreclosed_after\", StringType()),\n      StructField(\"disposition_date\", StringType()),\n      StructField(\"foreclosure_costs\", DoubleType()),\n      StructField(\"prop_preservation_and_repair_costs\", DoubleType()),\n      StructField(\"asset_recovery_costs\", DoubleType()),\n      StructField(\"misc_holding_expenses\", DoubleType()),\n      StructField(\"holding_taxes\", DoubleType()),\n      StructField(\"net_sale_proceeds\", DoubleType()),\n      StructField(\"credit_enhancement_proceeds\", DoubleType()),\n      StructField(\"repurchase_make_whole_proceeds\", StringType()),\n      StructField(\"other_foreclosure_proceeds\", DoubleType()),\n      StructField(\"non_interest_bearing_upb\", DoubleType()),\n      StructField(\"principal_forgiveness_upb\", StringType()),\n      StructField(\"original_list_start_date\", StringType()),\n      StructField(\"original_list_price\", StringType()),\n      StructField(\"current_list_start_date\", StringType()),\n      StructField(\"current_list_price\", StringType()),\n      StructField(\"borrower_credit_score_at_issuance\", StringType()),\n      StructField(\"co-borrower_credit_score_at_issuance\", StringType()),\n      StructField(\"borrower_credit_score_current\", StringType()),\n      StructField(\"co-Borrower_credit_score_current\", StringType()),\n      StructField(\"mortgage_insurance_type\", DoubleType()),\n      StructField(\"servicing_activity_indicator\", StringType()),\n      StructField(\"current_period_modification_loss_amount\", StringType()),\n      StructField(\"cumulative_modification_loss_amount\", StringType()),\n      StructField(\"current_period_credit_event_net_gain_or_loss\", StringType()),\n      StructField(\"cumulative_credit_event_net_gain_or_loss\", StringType()),\n      StructField(\"homeready_program_indicator\", StringType()),\n      StructField(\"foreclosure_principal_write_off_amount\", StringType()),\n      StructField(\"relocation_mortgage_indicator\", StringType()),\n      StructField(\"zero_balance_code_change_date\", StringType()),\n      StructField(\"loan_holdback_indicator\", StringType()),\n      StructField(\"loan_holdback_effective_date\", StringType()),\n      StructField(\"delinquent_accrued_interest\", StringType()),\n      StructField(\"property_valuation_method\", StringType()),\n      StructField(\"high_balance_loan_indicator\", StringType()),\n      StructField(\"arm_initial_fixed-rate_period_lt_5_yr_indicator\", StringType()),\n      StructField(\"arm_product_type\", StringType()),\n      StructField(\"initial_fixed-rate_period\", StringType()),\n      StructField(\"interest_rate_adjustment_frequency\", StringType()),\n      StructField(\"next_interest_rate_adjustment_date\", StringType()),\n      StructField(\"next_payment_change_date\", StringType()),\n      StructField(\"index\", StringType()),\n      StructField(\"arm_cap_structure\", StringType()),\n      StructField(\"initial_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"periodic_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"lifetime_interest_rate_cap_up_percent\", StringType()),\n      StructField(\"mortgage_margin\", StringType()),\n      StructField(\"arm_balloon_indicator\", StringType()),\n      StructField(\"arm_plan_number\", StringType()),\n      StructField(\"borrower_assistance_plan\", StringType()),\n      StructField(\"hltv_refinance_option_indicator\", StringType()),\n      StructField(\"deal_name\", StringType()),\n      StructField(\"repurchase_make_whole_proceeds_flag\", StringType()),\n      StructField(\"alternative_delinquency_resolution\", StringType()),\n      StructField(\"alternative_delinquency_resolution_count\", StringType()),\n      StructField(\"total_deferral_amount\", StringType())\n      ])\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing Raw Data Files with PySpark\nDESCRIPTION: This snippet reads raw CSV data into a PySpark DataFrame, converts it to a Parquet format, and reads it back for processing. It assumes the existence of functions to extract columns needed for further processing.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nrawDf = read_raw_csv(spark, orig_raw_path)\nrawDf.write.parquet(output_csv2parquet, mode='overwrite')\nrawDf = spark.read.parquet(output_csv2parquet)\n\nacq = extract_acq_columns(rawDf)\nperf = extract_perf_columns(rawDf)\n```\n\n----------------------------------------\n\nTITLE: Using rint Function in Spark SQL\nDESCRIPTION: Rounds up a double value to the nearest double equal to an integer. The function takes a double input and returns a double result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_62\n\nLANGUAGE: SQL\nCODE:\n```\nrint(input)\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Hash Aggregate in Spark RAPIDS\nDESCRIPTION: This snippet details the backend implementation for hash-based aggregations within Spark RAPIDS, enabling efficient data aggregation techniques.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_25\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.HashAggregateExec\"></a>spark.rapids.sql.exec.HashAggregateExec|The backend for hash based aggregations|true|None|\n```\n\n----------------------------------------\n\nTITLE: Timestamp Conversion Functions\nDESCRIPTION: Functions for converting microseconds and milliseconds to timestamps, including timestamp_micros and timestamp_millis. UTC timezone is mandatory for all timestamp operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_49\n\nLANGUAGE: SQL\nCODE:\n```\ntimestamp_micros(input)\ntimestamp_millis(input)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Iterative Maven Build for Spark RAPIDS Development\nDESCRIPTION: Maven command for quickly repackaging the rapids-4-spark library during development by skipping compression and JNI unpacking, targeting only the distribution module.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmvn package -pl dist -PnoSnapshots -Ddist.jar.compress=false -Drapids.jni.unpack.skip\n```\n\n----------------------------------------\n\nTITLE: Bitwise Shift Operations in Spark-RAPIDS\nDESCRIPTION: Implementation of bitwise shift operations (left, right, unsigned right) supporting INTEGER and LONG data types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_66\n\nLANGUAGE: SQL\nCODE:\n```\nshiftleft(value, amount)\nshiftright(value, amount)\nshiftrightunsigned(value, amount)\n```\n\n----------------------------------------\n\nTITLE: Parquet Input Scan in Spark RAPIDS\nDESCRIPTION: This snippet explains the scanning mechanism used for parsing Parquet files within Spark RAPIDS, enhancing the ability to handle Parquet data inputs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_55\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.input.ParquetScan\"></a>spark.rapids.sql.input.ParquetScan|Parquet parsing|true|None|\n```\n\n----------------------------------------\n\nTITLE: Using Modulo Operation in Spark SQL\nDESCRIPTION: Calculate the remainder or modulo of two numeric values. The function is supported for various numeric data types including byte, short, int, long, float, double, and decimal.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_60\n\nLANGUAGE: SQL\nCODE:\n```\nlhs % rhs\n```\n\nLANGUAGE: SQL\nCODE:\n```\nmod(lhs, rhs)\n```\n\n----------------------------------------\n\nTITLE: Defining Output Paths for Data in Python\nDESCRIPTION: Sets up various output paths that determine where the processed data will be stored, including paths for training, evaluation, and intermediate file storage. These paths are critical for organizing output data and should be reviewed for correctness.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\noutput_path = dataRoot + '/output/data/'\noutput_path_train = dataRoot + '/output/train/'\noutput_path_eval = dataRoot + '/output/eval/'\noutput_csv2parquet = dataRoot + '/output/csv2parquet/'\n\nsave_train_eval_dataset = True\n```\n\n----------------------------------------\n\nTITLE: Using LessThan Operator in Spark SQL\nDESCRIPTION: The less than operator (<) compares two values across various data types. It returns a boolean indicating if the left side is less than the right side.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_42\n\nLANGUAGE: SQL\nCODE:\n```\ncolumn1 < column2\n```\n\n----------------------------------------\n\nTITLE: Execution Step for AQUE Shuffle Read in Spark RAPIDS\nDESCRIPTION: This snippet describes the execution wrapper for shuffle query stages in Spark RAPIDS, facilitating improved data processing under AQUE conditions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_24\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.AQEShuffleReadExec\"></a>spark.rapids.sql.exec.AQEShuffleReadExec|A wrapper of shuffle query stage|true|None|\n```\n\n----------------------------------------\n\nTITLE: Using array_repeat Function in SQL\nDESCRIPTION: Returns the array containing the given input value repeated a specified number of times. Supports various data types with limitations on timestamp (UTC only) and unsupported types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\narray_repeat(left, right)\n```\n\n----------------------------------------\n\nTITLE: Last Value Aggregation Functions\nDESCRIPTION: Implementation of last_value and last aggregate functions with support for most primitive types and timestamps (UTC only). Includes window operation support and handles NULL values.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_74\n\nLANGUAGE: SQL\nCODE:\n```\nlast_value, last\n```\n\n----------------------------------------\n\nTITLE: Size Function (cardinality, size)\nDESCRIPTION: Returns the size/length of arrays and maps. Only supported for array and map data types, with special handling required for timestamps in UTC timezone.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_68\n\nLANGUAGE: SQL\nCODE:\n```\ncardinality(array)\nsize(array)\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Batch Scan in Spark RAPIDS\nDESCRIPTION: This snippet defines the backend for performing batch scans on most file inputs within the Spark RAPIDS framework, enhancing input data processing.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_34\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.BatchScanExec\"></a>spark.rapids.sql.exec.BatchScanExec|The backend for most file input|true|None|\n```\n\n----------------------------------------\n\nTITLE: Trigonometric Functions in Spark-RAPIDS\nDESCRIPTION: Implementation of trigonometric functions (sin, sinh) operating on DOUBLE data type inputs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_67\n\nLANGUAGE: SQL\nCODE:\n```\nsin(double)\nsinh(double)\n```\n\n----------------------------------------\n\nTITLE: Running cudf_udf Tests with Spark-RAPIDS\nDESCRIPTION: Example spark-submit command for running cudf_udf tests. These tests validate Pandas UDF functionality with cuDF and require specific configuration parameters for Python processes and GPU memory allocation.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n$SPARK_HOME/bin/spark-submit --jars \"rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar,rapids-4-spark-tests_2.12-25.06.0-SNAPSHOT.jar\" --conf spark.rapids.memory.gpu.allocFraction=0.3 --conf spark.rapids.python.memory.gpu.allocFraction=0.3 --conf spark.rapids.python.concurrentPythonWorkers=2 --py-files \"rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar\" --conf spark.executorEnv.PYTHONPATH=\"rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar\" ./runtests.py --cudf_udf\n```\n\n----------------------------------------\n\nTITLE: Spark Physical Plan Explanation in Python\nDESCRIPTION: This Python code snippet prints the physical plan of a Spark DataFrame's query execution. It accesses the internal Java representation of the DataFrame and uses `PythonSQLUtils.explainString` to retrieve a simplified explanation of the plan.  The explained DataFrame is `train_out`.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#train_out.explain()\nprint(spark._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.explainString(train_out._jdf.queryExecution(), 'simple'))\n```\n\n----------------------------------------\n\nTITLE: Aliasing Columns in SQL\nDESCRIPTION: The 'Alias' operation gives a column a new name. It supports most data types in the project context, with some limitations for complex types and specific timestamp formats.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\ncolumn_name AS alias_name\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Filter in Spark RAPIDS\nDESCRIPTION: This snippet specifies the backend for executing most filter statements within the Spark RAPIDS framework, enabling optimized data filtering operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.FilterExec\"></a>spark.rapids.sql.exec.FilterExec|The backend for most filter statements|true|None|\n```\n\n----------------------------------------\n\nTITLE: Using array_remove Function in SQL\nDESCRIPTION: Returns the array after removing all elements that equal to the input element from the input array. Supports various data types with limitations on timestamp (UTC only) and unsupported types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\narray_remove(array, element)\n```\n\n----------------------------------------\n\nTITLE: Using Min Aggregate Function in SQL\nDESCRIPTION: The Min function is an aggregate operator that returns the minimum value from a set of values. It supports various data types including numeric, date, timestamp, and string types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_77\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT min(column_name) FROM table_name\n```\n\n----------------------------------------\n\nTITLE: Arrays Overlap Check Operation\nDESCRIPTION: Implementation of arrays_overlap function that checks if arrays share any non-null elements. Returns true if arrays have common elements, null if no common elements but either contains null, false otherwise.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\narrays_overlap(array1, array2)\n```\n\n----------------------------------------\n\nTITLE: Calculating Inverse Cosine in SQL\nDESCRIPTION: The 'Acos' function computes the inverse cosine of a numeric input. It operates on double-precision floating-point numbers in both project and AST contexts.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nacos(numeric_column)\n```\n\n----------------------------------------\n\nTITLE: Building Gluten for Hybrid Execution\nDESCRIPTION: These bash commands clone the Gluten repository, checkout the v1.2.0 tag, apply a necessary fix, and build the package. This process creates the required Gluten bundle and third-party jars for hybrid execution.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/hybrid-execution.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apache/incubator-gluten.git\ngit checkout v1.2.0\n# Cherry pick a fix from main branch: Fix ObjectStore::stores initialized twice issue\ngit cherry-pick 2a6a974d6fbaa38869eb9a0b91b2e796a578884c\n./dev/package.sh\n```\n\n----------------------------------------\n\nTITLE: Extracting Quarter Information from Input CSV Filenames in PySpark\nDESCRIPTION: Defines a function to extract quarter information from the input CSV filename. This information is used to tag data with its temporal source during the ETL process.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _get_quarter_from_csv_file_name():\n    return substring_index(substring_index(input_file_name(), '.', 1), '/', -1)\n```\n\n----------------------------------------\n\nTITLE: Single Partitioning in Spark RAPIDS\nDESCRIPTION: This snippet defines the single partitioning approach in Spark RAPIDS, which directs all data to a single partition, typically for specific data processing needs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_60\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.partitioning.SinglePartition$\"></a>spark.rapids.sql.partitioning.SinglePartition$|Single partitioning|true|None|\n```\n\n----------------------------------------\n\nTITLE: Using array_position Function in SQL\nDESCRIPTION: Returns the (1-based) index of the first matching element of the array as long, or 0 if no match is found. Supports various data types except calendar and UDT.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\narray_position(array, key)\n```\n\n----------------------------------------\n\nTITLE: Defining Bitwise OR Operation\nDESCRIPTION: This snippet defines a SQL function for executing a bitwise OR operation on two operands (lhs and rhs), returning the result according to the standard SQL definition.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nReturns the bitwise OR of the operands\nNone\nlhs\nresult\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark for NVTX Collection\nDESCRIPTION: Add configuration keys to enable NVTX collection for Spark shell executors and drivers.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/nvtx_profiling.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--conf spark.driver.extraJavaOptions=-Dai.rapids.cudf.nvtx.enabled=true\n--conf spark.executor.extraJavaOptions=-Dai.rapids.cudf.nvtx.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark in Local Mode with Remote Debugging\nDESCRIPTION: This command shows how to configure Spark to run in local mode with JDWP remote debugging enabled on port 5005. This setup is useful for developers to debug the RAPIDS plugin, as it runs both driver and executor in the same JVM process.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n--conf spark.driver.extraJavaOptions=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005\"\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests with Fixed Data Generation Seed\nDESCRIPTION: Shell command for running integration tests with a specific data generation seed to reproduce test behavior with consistent test data. Sets the DATAGEN_SEED environment variable before calling the test runner script.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n$ DATAGEN_SEED=1702166057 SPARK_HOME=~/spark-3.4.0-bin-hadoop3 integration_tests/run_pyspark_from_build.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Bitwise XOR Operation\nDESCRIPTION: This snippet defines a SQL function for performing a bitwise XOR operation on two operands (lhs and rhs), returning the resultant output.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nReturns the bitwise XOR of the operands\nNone\nlhs\nresult\n\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Data Writing Command in Spark RAPIDS\nDESCRIPTION: This snippet describes the command for writing data to a specified data source within the Spark RAPIDS execution framework.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_29\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.DataWritingCommandExec\"></a>spark.rapids.sql.exec.DataWritingCommandExec|Writing data|true|None|\n```\n\n----------------------------------------\n\nTITLE: Using String Length Functions in Spark SQL\nDESCRIPTION: The char_length, character_length, and length functions return the number of characters in a string or bytes in binary data.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_41\n\nLANGUAGE: SQL\nCODE:\n```\nchar_length(string_column)\ncharacter_length(string_column)\nlength(string_column)\n```\n\n----------------------------------------\n\nTITLE: Configuring User Permissions for Microk8s\nDESCRIPTION: Commands to add current user to microk8s group and set proper ownership of .kube directory to avoid using sudo.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo usermod -a -G microk8s $USER\nsudo chown -f -R $USER ~/.kube\n```\n\n----------------------------------------\n\nTITLE: ASCII Value Extraction\nDESCRIPTION: Implementation of ascii function that returns numeric value of first character. Only supports strings starting with ASCII or Latin-1 characters in newer Spark versions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nascii(string)\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Aggregate in Pandas in Spark RAPIDS\nDESCRIPTION: This snippet describes the backend functionality for an Aggregation Pandas UDF, enhancing data transfer between Java and Python processes while scheduling GPU resources when enabled.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_43\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.AggregateInPandasExec\"></a>spark.rapids.sql.exec.AggregateInPandasExec|The backend for an Aggregation Pandas UDF, this accelerates the data transfer between the Java process and the Python process. It also supports scheduling GPU resources for the Python process when enabled.|true|None|\n```\n\n----------------------------------------\n\nTITLE: Software Requirements for RAPIDS Accelerator v24.04.1\nDESCRIPTION: Software requirements for running RAPIDS Accelerator v24.04.1, showing supported operating systems, NVIDIA drivers, runtimes, and compatible versions of Spark and cloud platforms.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_10\n\nLANGUAGE: plain\nCODE:\n```\nOS: Ubuntu 20.04, Ubuntu 22.04, CentOS 7, or Rocky Linux 8\n\nNVIDIA Driver*: R470+\n\nRuntime: \n\tScala 2.12, 2.13\n\tPython, Java Virtual Machine (JVM) compatible with your spark-version. \n\n\t* Check the Spark documentation for Python and Java version compatibility with your specific \n\tSpark version. For instance, visit `https://spark.apache.org/docs/3.4.1` for Spark 3.4.1.\n\nSupported Spark versions:\n\tApache Spark 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4\n\tApache Spark 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4\n\tApache Spark 3.4.0, 3.4.1, 3.4.2\n\tApache Spark 3.5.0, 3.5.1\n\nSupported Databricks runtime versions for Azure and AWS:\n\tDatabricks 11.3 ML LTS (GPU, Scala 2.12, Spark 3.3.0)\n\tDatabricks 12.2 ML LTS (GPU, Scala 2.12, Spark 3.3.2)\n\tDatabricks 13.3 ML LTS (GPU, Scala 2.12, Spark 3.4.1)\n\nSupported Dataproc versions (Debian/Ubuntu):\n\tGCP Dataproc 2.0\n\tGCP Dataproc 2.1\n\nSupported Dataproc Serverless versions:\n\tSpark runtime 1.1 LTS\n\tSpark runtime 2.0\n\tSpark runtime 2.1\n```\n\n----------------------------------------\n\nTITLE: Adding NVTX Ranges in C++\nDESCRIPTION: Example of adding NVTX ranges to C++ code for profiling specific code blocks.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/nvtx_profiling.md#2025-04-19_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ngdf_nvtx_range_push_hex(\"write_orc_all\", 0xffff0000);\n// the code you want to profile\ngdf_nvtx_range_pop();\n```\n\n----------------------------------------\n\nTITLE: Define S3 Bucket Paths in Python\nDESCRIPTION: This Python code defines variables to store paths to data in an S3 bucket. It uses string formatting to create paths for raw data, training data, testing data, temporary data, and intermediate Parquet files.  The `TARGET_BUCKET` placeholder should be replaced with the actual name of the S3 bucket.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nbucket_name = 'TARGET_BUCKET'\n\norig_raw_path = 's3://{0}/mortgage-etl-demo/raw/*'.format(bucket_name)\n\ntrain_path = 's3://{0}/mortgage-xgboost-demo/train/'.format(bucket_name)\ntest_path = 's3://{0}/mortgage-xgboost-demo/test/'.format(bucket_name)\noutput_csv2parquet = 's3://{0}/mortgage-xgboost-demo/tmp/'.format(bucket_name)\n\n\ntmp_perf_path = 's3://{0}/mortgage_parquet_gpu/perf/'.format(bucket_name)\ntmp_acq_path = 's3://{0}/mortgage_parquet_gpu/acq/'.format(bucket_name)\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Window in Pandas in Spark RAPIDS\nDESCRIPTION: This snippet outlines the backend for Window Aggregation Pandas UDFs, with certain limitations currently focused only on row-based window frames.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_48\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.WindowInPandasExec\"></a>spark.rapids.sql.exec.WindowInPandasExec|The backend for Window Aggregation Pandas UDF, Accelerates the data transfer between the Java process and the Python process. It also supports scheduling GPU resources for the Python process when enabled. For now it only supports row based window frame.|false|This is disabled by default because it only supports row based frame for now|\n```\n\n----------------------------------------\n\nTITLE: Percentile Aggregation Support\nDESCRIPTION: Exact percentile computation support across numeric types. Limited to numeric input types with special handling for literal percentage values.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_79\n\nLANGUAGE: SQL\nCODE:\n```\npercentile(numeric_column, percentage_literal)\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiDistribution in Spark-RAPIDS DBGen\nDESCRIPTION: Demonstrates how to combine multiple distributions with different weights using MultiDistribution to create skewed data. This example combines a NormalDistribution with weight 10 and a FlatDistribution with weight 1.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string\", 100000)\ndataTable(\"a\").setSeedMapping(MultiDistribution(Seq(\n  (10.0, NormalDistribution(50, 1.0)),\n  (1.0, FlatDistribution())))).setSeedRange(0, 100)\ndataTable.toDF(spark).groupBy(\"a\").count().orderBy(desc(\"count\")).show()\n```\n\n----------------------------------------\n\nTITLE: Calculating Ceiling of a Number\nDESCRIPTION: This snippet describes how to compute the ceiling of a given number, returning the smallest integer greater than or equal to that number.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nCeiling of a number\nNone\ninput\nresult\n\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Flat Map CoGroups in Pandas in Spark RAPIDS\nDESCRIPTION: This snippet provides the backend for CoGrouped Aggregation Pandas UDFs, although disabled by default due to performance concerns with small groups.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_45\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.FlatMapCoGroupsInPandasExec\"></a>spark.rapids.sql.exec.FlatMapCoGroupsInPandasExec|The backend for CoGrouped Aggregation Pandas UDF. Accelerates the data transfer between the Java process and the Python process. It also supports scheduling GPU resources for the Python process when enabled.|false|This is disabled by default because Performance is not ideal with many small groups|\n```\n\n----------------------------------------\n\nTITLE: Testing GPU Models for RAPIDS Accelerator\nDESCRIPTION: List of NVIDIA GPU models on which the RAPIDS Accelerator plugin is tested, including V100, T4, A10, A100, L4, and H100 GPUs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_8\n\nLANGUAGE: plain\nCODE:\n```\nGPU Models: NVIDIA V100, T4, A10/A100, L4 and H100 GPUs\n```\n\n----------------------------------------\n\nTITLE: CSV Input Scan in Spark RAPIDS\nDESCRIPTION: This snippet defines the scanning mechanism for parsing CSV files within the Spark RAPIDS framework, enabling the processing of CSV data inputs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_52\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.input.CSVScan\"></a>spark.rapids.sql.input.CSVScan|CSV parsing|true|None|\n```\n\n----------------------------------------\n\nTITLE: Enabling CPU Profiling\nDESCRIPTION: Command to enable CPU profiling features before running nsys profile.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/nvtx_profiling.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo sh -c 'echo [level] >/proc/sys/kernel/perf_event_paranoid'\n```\n\n----------------------------------------\n\nTITLE: Setting File Path for Input Data in Python\nDESCRIPTION: Defines the file paths for input data, which need to be updated with actual paths to access data in a cloud storage bucket. These paths are critical for the script to locate necessary input files.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# You need to update these to your real paths!\ndataRoot = os.getenv(\"DATA_ROOT\", 'gs://your-bucket/your-paths')\norig_raw_path = dataRoot + '/'\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Executed Command in Spark RAPIDS\nDESCRIPTION: This snippet details the functionality for executing commands eagerly within the Spark RAPIDS execution context.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_30\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.ExecutedCommandExec\"></a>spark.rapids.sql.exec.ExecutedCommandExec|Eagerly executed commands|true|None|\n```\n\n----------------------------------------\n\nTITLE: Population Standard Deviation Support\nDESCRIPTION: Standard deviation computation support, primarily for double precision floating point values in aggregation contexts.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_81\n\nLANGUAGE: SQL\nCODE:\n```\nstddev_pop(numeric_column)\n```\n\n----------------------------------------\n\nTITLE: Running API Validation Script for Spark Rapids\nDESCRIPTION: Commands to run the API validation script for checking compatibility between Spark Execs and GPU Execs. Can be run either for all Spark versions or for a specific version using a Maven profile.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/api_validation/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd api_validation\n// To run validation script on all version of Spark\nsh auditAllVersions.sh\n\n// To run script on particular version we can use profile\nmvn scala:run -P spark320\n```\n\n----------------------------------------\n\nTITLE: Implementing Lazy Loading Pattern for Public Classes in Scala\nDESCRIPTION: Pattern for delaying class initialization using lazy values to prevent loading classes from Parallel Worlds before they're needed. This approach prevents early loading of incompatible dependencies.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_1\n\nLANGUAGE: Scala\nCODE:\n```\n  def method(x: SomeThing) = realImpl.method(x)\n```\n\n----------------------------------------\n\nTITLE: Running Tests and Generating Code Coverage Report for Spark-RAPIDS\nDESCRIPTION: This snippet shows how to run tests and generate a code coverage report for the Spark-RAPIDS plugin using Maven and a custom coverage-report script. The coverage report is generated using Jacoco and covers test execution with Spark 3.2.0 by default.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean verify\n./build/coverage-report\n```\n\n----------------------------------------\n\nTITLE: Setting JAVA_OPTS for NVTX in Java Profile Tests\nDESCRIPTION: Export JAVA_OPTS environment variable to enable NVTX for Java-based profile tests.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/nvtx_profiling.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport JAVA_OPTS=\"-Dai.rapids.cudf.nvtx.enabled=true\"\n```\n\n----------------------------------------\n\nTITLE: Mapping Financial Institution Names to Standardized Names in Python\nDESCRIPTION: A mapping list that standardizes financial institution names by converting long formal company names to shorter, more consistent versions. This mapping helps normalize seller and servicer names in mortgage data analysis.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n_name_mapping = [\n        (\"WITMER FUNDING, LLC\", \"Witmer\"),\n        (\"WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015\", \"Wells Fargo\"),\n        (\"WELLS FARGO BANK,  NA\" , \"Wells Fargo\"),\n        (\"WELLS FARGO BANK, N.A.\" , \"Wells Fargo\"),\n        (\"WELLS FARGO BANK, NA\" , \"Wells Fargo\"),\n        (\"USAA FEDERAL SAVINGS BANK\" , \"USAA\"),\n        (\"UNITED SHORE FINANCIAL SERVICES, LLC D\\\\/B\\\\/A UNITED WHOLESALE MORTGAGE\" , \"United Seq(e\"),\n        (\"U.S. BANK N.A.\" , \"US Bank\"),\n        (\"SUNTRUST MORTGAGE INC.\" , \"Suntrust\"),\n        (\"STONEGATE MORTGAGE CORPORATION\" , \"Stonegate Mortgage\"),\n        (\"STEARNS LENDING, LLC\" , \"Stearns Lending\"),\n        (\"STEARNS LENDING, INC.\" , \"Stearns Lending\"),\n        (\"SIERRA PACIFIC MORTGAGE COMPANY, INC.\" , \"Sierra Pacific Mortgage\"),\n        (\"REGIONS BANK\" , \"Regions\"),\n        (\"RBC MORTGAGE COMPANY\" , \"RBC\"),\n        (\"QUICKEN LOANS INC.\" , \"Quicken Loans\"),\n        (\"PULTE MORTGAGE, L.L.C.\" , \"Pulte Mortgage\"),\n        (\"PROVIDENT FUNDING ASSOCIATES, L.P.\" , \"Provident Funding\"),\n        (\"PROSPECT MORTGAGE, LLC\" , \"Prospect Mortgage\"),\n        (\"PRINCIPAL RESIDENTIAL MORTGAGE CAPITAL RESOURCES, LLC\" , \"Principal Residential\"),\n        (\"PNC BANK, N.A.\" , \"PNC\"),\n        (\"PMT CREDIT RISK TRANSFER TRUST 2015-2\" , \"PennyMac\"),\n        (\"PHH MORTGAGE CORPORATION\" , \"PHH Mortgage\"),\n        (\"PENNYMAC CORP.\" , \"PennyMac\"),\n        (\"PACIFIC UNION FINANCIAL, LLC\" , \"Other\"),\n        (\"OTHER\" , \"Other\"),\n        (\"NYCB MORTGAGE COMPANY, LLC\" , \"NYCB\"),\n        (\"NEW YORK COMMUNITY BANK\" , \"NYCB\"),\n        (\"NETBANK FUNDING SERVICES\" , \"Netbank\"),\n        (\"NATIONSTAR MORTGAGE, LLC\" , \"Nationstar Mortgage\"),\n        (\"METLIFE BANK, NA\" , \"Metlife\"),\n        (\"LOANDEPOT.COM, LLC\" , \"LoanDepot.com\"),\n        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2015-1\" , \"JP Morgan Chase\"),\n        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2014-1\" , \"JP Morgan Chase\"),\n        (\"JPMORGAN CHASE BANK, NATIONAL ASSOCIATION\" , \"JP Morgan Chase\"),\n        (\"JPMORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n        (\"JP MORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n        (\"IRWIN MORTGAGE, CORPORATION\" , \"Irwin Mortgage\"),\n        (\"IMPAC MORTGAGE CORP.\" , \"Impac Mortgage\"),\n        (\"HSBC BANK USA, NATIONAL ASSOCIATION\" , \"HSBC\"),\n        (\"HOMEWARD RESIDENTIAL, INC.\" , \"Homeward Mortgage\"),\n        (\"HOMESTREET BANK\" , \"Other\"),\n        (\"HOMEBRIDGE FINANCIAL SERVICES, INC.\" , \"HomeBridge\"),\n        (\"HARWOOD STREET FUNDING I, LLC\" , \"Harwood Mortgage\"),\n        (\"GUILD MORTGAGE COMPANY\" , \"Guild Mortgage\"),\n        (\"GMAC MORTGAGE, LLC (USAA FEDERAL SAVINGS BANK)\" , \"GMAC\"),\n        (\"GMAC MORTGAGE, LLC\" , \"GMAC\"),\n        (\"GMAC (USAA)\" , \"GMAC\"),\n        (\"FREMONT BANK\" , \"Fremont Bank\"),\n        (\"FREEDOM MORTGAGE CORP.\" , \"Freedom Mortgage\"),\n        (\"FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"Franklin America\"),\n        (\"FLEET NATIONAL BANK\" , \"Fleet National\"),\n        (\"FLAGSTAR CAPITAL MARKETS CORPORATION\" , \"Flagstar Bank\"),\n        (\"FLAGSTAR BANK, FSB\" , \"Flagstar Bank\"),\n        (\"FIRST TENNESSEE BANK NATIONAL ASSOCIATION\" , \"Other\"),\n        (\"FIFTH THIRD BANK\" , \"Fifth Third Bank\"),\n        (\"FEDERAL HOME LOAN BANK OF CHICAGO\" , \"Fedral Home of Chicago\"),\n        (\"FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB\" , \"FDIC\"),\n        (\"DOWNEY SAVINGS AND LOAN ASSOCIATION, F.A.\" , \"Downey Mortgage\"),\n        (\"DITECH FINANCIAL LLC\" , \"Ditech\"),\n        (\"CITIMORTGAGE, INC.\" , \"Citi\"),\n\n```\n\n----------------------------------------\n\nTITLE: Output Path Configuration for Mortgage Data Processing with PySpark\nDESCRIPTION: Configuration settings for output paths where processed data, training datasets, evaluation datasets, and intermediate parquet files will be stored.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\noutput_path = dataRoot + '/output/data/'\noutput_path_train = dataRoot + '/output/train/'\noutput_path_eval = dataRoot + '/output/eval/'\noutput_csv2parquet = dataRoot + '/output/csv2parquet/'\n\nsave_train_eval_dataset = True\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Subquery Broadcast in Spark RAPIDS\nDESCRIPTION: This snippet describes the execution plan for collecting and transforming broadcast key values in Spark RAPIDS, enhancing subquery performance.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_21\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.SubqueryBroadcastExec\"></a>spark.rapids.sql.exec.SubqueryBroadcastExec|Plan to collect and transform the broadcast key values|true|None|\n```\n\n----------------------------------------\n\nTITLE: Not Operator Implementation\nDESCRIPTION: Boolean NOT operator implementation supporting boolean input and output types only.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_54\n\nLANGUAGE: SQL\nCODE:\n```\n!, not\n```\n\n----------------------------------------\n\nTITLE: Using array_max Function in SQL\nDESCRIPTION: Returns the maximum value in the array. Only supports arrays with non-binary, non-calendar, non-struct, and non-UDT types. For timestamp arrays, only UTC timezone is supported.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\narray_max(input)\n```\n\n----------------------------------------\n\nTITLE: Displaying sample dataset for range window example\nDESCRIPTION: A console representation of a sample dataset with ID and dollar values used to demonstrate range window function behavior differences between CPU and GPU implementations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n+------+---------+\n| id   | dollars |\n+------+---------+\n|    1 |    NULL |\n|    1 |      13 |\n|    1 |      14 |\n|    1 |      15 |\n|    1 |      15 |\n|    1 |      17 |\n|    1 |      18 |\n|    1 |      52 |\n|    1 |      53 |\n|    1 |      61 |\n|    1 |      65 |\n|    1 |      72 |\n|    1 |      73 |\n|    1 |      75 |\n|    1 |      78 |\n|    1 |      84 |\n|    1 |      85 |\n|    1 |      86 |\n|    1 |      92 |\n|    1 |      98 |\n+------+---------+\n```\n\n----------------------------------------\n\nTITLE: Testing After Shimplify Conversion with RAPIDS Shuffle Manager\nDESCRIPTION: Complete test command sequence that builds a specific version (3.3.1) with the new structure and runs integration tests with RAPIDS Shuffle Manager enabled, providing comprehensive validation of the conversion.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean package -DskipTests -Dbuildver=331\nSPARK_HOME=~/dist/spark-3.3.1-bin-hadoop3 \\\n    NUM_LOCAL_EXECS=2 \\\n    PYSP_TEST_spark_rapids_shuffle_mode=MULTITHREADED \\\n    PYSP_TEST_spark_rapids_shuffle_multiThreaded_writer_threads=2 \\\n    PYSP_TEST_spark_rapids_shuffle_multiThreaded_reader_threads=2 \\\n    PYSP_TEST_spark_shuffle_manager=com.nvidia.spark.rapids.spark331.RapidsShuffleManager \\\n    PYSP_TEST_spark_rapids_memory_gpu_minAllocFraction=0 \\\n    PYSP_TEST_spark_rapids_memory_gpu_maxAllocFraction=0.1 \\\n    PYSP_TEST_spark_rapids_memory_gpu_allocFraction=0.1 \\\n    ./integration_tests/run_pyspark_from_build.sh -k test_hash_grpby_sum\n```\n\n----------------------------------------\n\nTITLE: Verifying Conventional Class Loading in Single-Version Jar\nDESCRIPTION: Command to verify that classes in a single-version distribution jar are conventionally loadable without requiring special prefixes, showing the successful class access.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ javap -cp dist/target/rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar com.nvidia.spark.rapids.shims.SparkShimImpl | head -2\nCompiled from \"SparkShims.scala\"\npublic final class com.nvidia.spark.rapids.shims.SparkShimImpl {\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Sort in Spark RAPIDS\nDESCRIPTION: This snippet details the backend for the sort operator in Spark RAPIDS, optimizing sorting operations on data.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.SortExec\"></a>spark.rapids.sql.exec.SortExec|The backend for the sort operator|true|None|\n```\n\n----------------------------------------\n\nTITLE: Defining Categorical and Numeric Columns for Mortgage Data Analysis in Python\nDESCRIPTION: This snippet defines lists of categorical and numeric column names for mortgage data analysis. It includes a label column for delinquency prediction and separates features into categorical and numeric types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# String columns\ncate_col_names = [\n        \"orig_channel\",\n        \"first_home_buyer\",\n        \"loan_purpose\",\n        \"property_type\",\n        \"occupancy_status\",\n        \"property_state\",\n        \"product_type\",\n        \"relocation_mortgage_indicator\",\n        \"seller_name\",\n        \"mod_flag\"\n]\n# Numberic columns\nlabel_col_name = \"delinquency_12\"\nnumeric_col_names = [\n        \"orig_interest_rate\",\n        \"orig_upb\",\n        \"orig_loan_term\",\n        \"orig_ltv\",\n        \"orig_cltv\",\n        \"num_borrowers\",\n        \"dti\",\n        \"borrower_credit_score\",\n        \"num_units\",\n        \"zip\",\n        \"mortgage_insurance_percent\",\n        \"current_loan_delinquency_status\",\n        \"current_actual_upb\",\n        \"interest_rate\",\n        \"loan_age\",\n        \"msa\",\n        \"non_interest_bearing_upb\",\n        label_col_name\n]\nall_col_names = cate_col_names + numeric_col_names\n```\n\n----------------------------------------\n\nTITLE: File Path Configuration for Mortgage Data ETL with PySpark\nDESCRIPTION: Configuration settings for input and output file paths used in the mortgage data ETL process. The paths are configurable through environment variables or use default locations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# You need to update them to your real paths!\ndataRoot = os.getenv(\"DATA_ROOT\", 'dbfs:///FileStore/tables/mortgage-fannieMae')\norig_raw_path = dataRoot + '/'\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark for Hybrid Execution\nDESCRIPTION: These Spark configurations enable hybrid execution and Parquet offloading to CPU. They set the necessary parameters for using the V1 Parquet source, enabling hybrid Parquet execution, and loading the backend.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/hybrid-execution.md#2025-04-19_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n\"spark.sql.sources.useV1SourceList\": \"parquet\"\n\"spark.rapids.sql.hybrid.parquet.enabled\": \"true\"\n\"spark.rapids.sql.hybrid.loadBackend\": \"true\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Environment Variables for GPU Core Dumps\nDESCRIPTION: Spark configuration settings to enable lightweight GPU core dumps and direct the dump files to the container log directory. These settings enable core dump generation on GPU exceptions and configure the output location using YARN's log directory variable expansion.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/gpu-core-dumps.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nspark.executorEnv.CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1\nspark.executorEnv.CUDA_ENABLE_LIGHTWEIGHT_COREDUMP=1\nspark.executorEnv.CUDA_COREDUMP_FILE=\"<LOG_DIR>/executor-%h-%p.nvcudmp\"\n```\n\n----------------------------------------\n\nTITLE: Timestamp Seconds Function in Spark-RAPIDS\nDESCRIPTION: Converts unix epoch seconds to timestamp with UTC timezone support. Only UTC timezone is supported for timestamp conversions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_65\n\nLANGUAGE: SQL\nCODE:\n```\ntimestamp_seconds\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Broadcast Hash Join in Spark RAPIDS\nDESCRIPTION: This snippet outlines the implementation for joining operations that utilize broadcast data, ensuring optimized performance in Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_38\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.BroadcastHashJoinExec\"></a>spark.rapids.sql.exec.BroadcastHashJoinExec|Implementation of join using broadcast data|true|None|\n```\n\n----------------------------------------\n\nTITLE: SQL String Operations Support Matrix\nDESCRIPTION: Support matrix for string operations like lower/lcase showing compatibility across data types. Note: Unicode handling may differ between cuDF and JVM implementations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_46\n\nLANGUAGE: SQL\nCODE:\n```\nlower(STRING) -> STRING\nlcase(STRING) -> STRING\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Shuffle Exchange in Spark RAPIDS\nDESCRIPTION: This snippet details the backend implementation for managing most data exchanges between processes in the Spark RAPIDS framework, ensuring efficient data handling.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_37\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.ShuffleExchangeExec\"></a>spark.rapids.sql.exec.ShuffleExchangeExec|The backend for most data being exchanged between processes|true|None|\n```\n\n----------------------------------------\n\nTITLE: Defining Categorical and Numeric Columns for Mortgage Data Analysis in Python\nDESCRIPTION: Defines lists of column names separated by data type (categorical vs. numeric), including a specific label column for the delinquency_12 target variable. These column definitions are used for data preprocessing and model training.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# String columns\ncate_col_names = [\n        \"orig_channel\",\n        \"first_home_buyer\",\n        \"loan_purpose\",\n        \"property_type\",\n        \"occupancy_status\",\n        \"property_state\",\n        \"product_type\",\n        \"relocation_mortgage_indicator\",\n        \"seller_name\",\n        \"mod_flag\"\n]\n# Numberic columns\nlabel_col_name = \"delinquency_12\"\nnumeric_col_names = [\n        \"orig_interest_rate\",\n        \"orig_upb\",\n        \"orig_loan_term\",\n        \"orig_ltv\",\n        \"orig_cltv\",\n        \"num_borrowers\",\n        \"dti\",\n        \"borrower_credit_score\",\n        \"num_units\",\n        \"zip\",\n        \"mortgage_insurance_percent\",\n        \"current_loan_delinquency_status\",\n        \"current_actual_upb\",\n        \"interest_rate\",\n        \"loan_age\",\n        \"msa\",\n        \"non_interest_bearing_upb\",\n        label_col_name\n]\nall_col_names = cate_col_names + numeric_col_names\n```\n\n----------------------------------------\n\nTITLE: Defining Categorical Column Names in Python\nDESCRIPTION: This snippet lists the categorical column names that are likely used in the mortgage data processing pipeline. These column names refer to various mortgage attributes and are presumably used for data analysis or machine learning tasks within the script.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n[\n        \"orig_channel\",\n        \"first_home_buyer\",\n        \"loan_purpose\",\n        \"property_type\",\n        \"occupancy_status\",\n        \"property_state\",\n        \"product_type\",\n        \"relocation_mortgage_indicator\",\n        \"seller_name\",\n        \"mod_flag\"\n]\n```\n\n----------------------------------------\n\nTITLE: Running Individual ScalaTest Test Suite\nDESCRIPTION: Command for running a specific test suite (JoinsSuite) using the ScalaTest framework within the Spark shell. The durations object tracks and reports test execution times.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\ndurations.run(new com.nvidia.spark.rapids.JoinsSuite)\n```\n\n----------------------------------------\n\nTITLE: Defining Name Mappings for Financial Institutions in Python\nDESCRIPTION: This snippet creates a list of tuples mapping original financial institution names to simplified or standardized names. It's used for data normalization in the mortgage analysis process.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n_name_mapping = [\n        (\"WITMER FUNDING, LLC\", \"Witmer\"),\n        (\"WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015\", \"Wells Fargo\"),\n        (\"WELLS FARGO BANK,  NA\" , \"Wells Fargo\"),\n        (\"WELLS FARGO BANK, N.A.\" , \"Wells Fargo\"),\n        (\"WELLS FARGO BANK, NA\" , \"Wells Fargo\"),\n        (\"USAA FEDERAL SAVINGS BANK\" , \"USAA\"),\n        (\"UNITED SHORE FINANCIAL SERVICES, LLC D\\/B\\/A UNITED WHOLESALE MORTGAGE\" , \"United Seq(e\"),\n        (\"U.S. BANK N.A.\" , \"US Bank\"),\n        (\"SUNTRUST MORTGAGE INC.\" , \"Suntrust\"),\n        (\"STONEGATE MORTGAGE CORPORATION\" , \"Stonegate Mortgage\"),\n        (\"STEARNS LENDING, LLC\" , \"Stearns Lending\"),\n        (\"STEARNS LENDING, INC.\" , \"Stearns Lending\"),\n        (\"SIERRA PACIFIC MORTGAGE COMPANY, INC.\" , \"Sierra Pacific Mortgage\"),\n        (\"REGIONS BANK\" , \"Regions\"),\n        (\"RBC MORTGAGE COMPANY\" , \"RBC\"),\n        (\"QUICKEN LOANS INC.\" , \"Quicken Loans\"),\n        (\"PULTE MORTGAGE, L.L.C.\" , \"Pulte Mortgage\"),\n        (\"PROVIDENT FUNDING ASSOCIATES, L.P.\" , \"Provident Funding\"),\n        (\"PROSPECT MORTGAGE, LLC\" , \"Prospect Mortgage\"),\n        (\"PRINCIPAL RESIDENTIAL MORTGAGE CAPITAL RESOURCES, LLC\" , \"Principal Residential\"),\n        (\"PNC BANK, N.A.\" , \"PNC\"),\n        (\"PMT CREDIT RISK TRANSFER TRUST 2015-2\" , \"PennyMac\"),\n        (\"PHH MORTGAGE CORPORATION\" , \"PHH Mortgage\"),\n        (\"PENNYMAC CORP.\" , \"PennyMac\"),\n        (\"PACIFIC UNION FINANCIAL, LLC\" , \"Other\"),\n        (\"OTHER\" , \"Other\"),\n        (\"NYCB MORTGAGE COMPANY, LLC\" , \"NYCB\"),\n        (\"NEW YORK COMMUNITY BANK\" , \"NYCB\"),\n        (\"NETBANK FUNDING SERVICES\" , \"Netbank\"),\n        (\"NATIONSTAR MORTGAGE, LLC\" , \"Nationstar Mortgage\"),\n        (\"METLIFE BANK, NA\" , \"Metlife\"),\n        (\"LOANDEPOT.COM, LLC\" , \"LoanDepot.com\"),\n        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2015-1\" , \"JP Morgan Chase\"),\n        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2014-1\" , \"JP Morgan Chase\"),\n        (\"JPMORGAN CHASE BANK, NATIONAL ASSOCIATION\" , \"JP Morgan Chase\"),\n        (\"JPMORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n        (\"JP MORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n        (\"IRWIN MORTGAGE, CORPORATION\" , \"Irwin Mortgage\"),\n        (\"IMPAC MORTGAGE CORP.\" , \"Impac Mortgage\"),\n        (\"HSBC BANK USA, NATIONAL ASSOCIATION\" , \"HSBC\"),\n        (\"HOMEWARD RESIDENTIAL, INC.\" , \"Homeward Mortgage\"),\n        (\"HOMESTREET BANK\" , \"Other\"),\n        (\"HOMEBRIDGE FINANCIAL SERVICES, INC.\" , \"HomeBridge\"),\n        (\"HARWOOD STREET FUNDING I, LLC\" , \"Harwood Mortgage\"),\n        (\"GUILD MORTGAGE COMPANY\" , \"Guild Mortgage\"),\n        (\"GMAC MORTGAGE, LLC (USAA FEDERAL SAVINGS BANK)\" , \"GMAC\"),\n        (\"GMAC MORTGAGE, LLC\" , \"GMAC\"),\n        (\"GMAC (USAA)\" , \"GMAC\"),\n        (\"FREMONT BANK\" , \"Fremont Bank\"),\n        (\"FREEDOM MORTGAGE CORP.\" , \"Freedom Mortgage\"),\n        (\"FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"Franklin America\"),\n        (\"FLEET NATIONAL BANK\" , \"Fleet National\"),\n        (\"FLAGSTAR CAPITAL MARKETS CORPORATION\" , \"Flagstar Bank\"),\n        (\"FLAGSTAR BANK, FSB\" , \"Flagstar Bank\"),\n        (\"FIRST TENNESSEE BANK NATIONAL ASSOCIATION\" , \"Other\"),\n        (\"FIFTH THIRD BANK\" , \"Fifth Third Bank\"),\n        (\"FEDERAL HOME LOAN BANK OF CHICAGO\" , \"Fedral Home of Chicago\"),\n        (\"FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB\" , \"FDIC\"),\n        (\"DOWNEY SAVINGS AND LOAN ASSOCIATION, F.A.\" , \"Downey Mortgage\"),\n        (\"DITECH FINANCIAL LLC\" , \"Ditech\"),\n        (\"CITIMORTGAGE, INC.\" , \"Citi\"),\n        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERFIRST MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERBANK MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n        (\"CHASE HOME FINANCE, LLC\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE (CIE 1)\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE\" , \"JP Morgan Chase\"),\n        (\"CASHCALL, INC.\" , \"CashCall\"),\n        (\"CAPITAL ONE, NATIONAL ASSOCIATION\" , \"Capital One\"),\n        (\"CALIBER HOME LOANS, INC.\" , \"Caliber Funding\"),\n        (\"BISHOPS GATE RESIDENTIAL MORTGAGE TRUST\" , \"Bishops Gate Mortgage\"),\n        (\"BANK OF AMERICA, N.A.\" , \"Bank of America\"),\n        (\"AMTRUST BANK\" , \"AmTrust\"),\n        (\"AMERISAVE MORTGAGE CORPORATION\" , \"Amerisave\"),\n        (\"AMERIHOME MORTGAGE COMPANY, LLC\" , \"AmeriHome Mortgage\"),\n        (\"ALLY BANK\" , \"Ally Bank\"),\n        (\"ACADEMY MORTGAGE CORPORATION\" , \"Academy Mortgage\"),\n        (\"NO CASH-OUT REFINANCE\" , \"OTHER REFINANCE\"),\n        (\"REFINANCE - NOT SPECIFIED\" , \"OTHER REFINANCE\"),\n        (\"Other REFINANCE\" , \"OTHER REFINANCE\")]\n```\n\n----------------------------------------\n\nTITLE: Displaying RAPIDS Plugin GPU-Accelerated Query Plan\nDESCRIPTION: Example output of a Spark SQL EXPLAIN statement showing the physical query plan after RAPIDS Plugin optimization, with GPU-based execution nodes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/README.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n*(5) Sort [o_orderpriority#5 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(o_orderpriority#5 ASC NULLS FIRST, 200), true, [id=#611]\n   +- *(4) HashAggregate(keys=[o_orderpriority#5], functions=[count(1)])\n      +- Exchange hashpartitioning(o_orderpriority#5, 200), true, [id=#607]\n         +- *(3) HashAggregate(keys=[o_orderpriority#5], functions=[partial_count(1)])\n            +- *(3) GpuColumnarToRow false\n               +- !GpuProject [o_orderpriority#5]\n                  +- GpuRowToColumnar TargetSize(1000000)\n                     +- SortMergeJoin [o_orderkey#0L], [l_orderkey#18L], LeftSemi\n                        :- *(1) GpuColumnarToRow false\n                        :  +- !GpuSort [o_orderkey#0L ASC NULLS FIRST], false, 0\n                        :     +- GpuCoalesceBatches com.nvidia.spark.rapids.PreferSingleBatch$@40dcd875\n                        :        +- !GpuColumnarExchange gpuhashpartitioning(o_orderkey#0L, 200), true, [id=#543]\n                        :           +- !GpuProject [o_orderkey#0L, o_orderpriority#5]\n                        :              +- GpuCoalesceBatches TargetSize(1000000)\n                        :                 +- !GpuFilter ((gpuisnotnull(o_orderdate#4) AND (o_orderdate#4 >= 8582)) AND (o_orderdate#4 < 8674))\n                        :                    +- GpuBatchScan[o_orderkey#0L, o_orderdate#4, o_orderpriority#5] GpuParquetScan Location: InMemoryFileIndex[file:/home/example/parquet/orders.tbl], ReadSchema: struct<o_orderkey:bigint,o_orderdate:date,o_orderpriority:string>\n                        +- *(2) GpuColumnarToRow false\n                           +- !GpuSort [l_orderkey#18L ASC NULLS FIRST], false, 0\n                              +- GpuCoalesceBatches com.nvidia.spark.rapids.PreferSingleBatch$@40dcd875\n                                 +- !GpuColumnarExchange gpuhashpartitioning(l_orderkey#18L, 200), true, [id=#551]\n                                    +- !GpuProject [l_orderkey#18L]\n                                       +- GpuCoalesceBatches TargetSize(1000000)\n                                          +- !GpuFilter (((l_commitdate#29 < l_receiptdate#30) AND gpuisnotnull(l_commitdate#29)) AND gpuisnotnull(l_receiptdate#30))\n                                             +- GpuBatchScan[l_orderkey#18L, l_commitdate#29, l_receiptdate#30] GpuParquetScan Location: InMemoryFileIndex[file:/home/example/parquet/lineitem.tbl], ReadSchema: struct<l_orderkey:bigint,l_commitdate:date,l_receiptdate:date>\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Shuffled Hash Join in Spark RAPIDS\nDESCRIPTION: This snippet provides the backend for hash joins utilizing shuffled data, optimizing join operations within the Spark RAPIDS framework.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_41\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.ShuffledHashJoinExec\"></a>spark.rapids.sql.exec.ShuffledHashJoinExec|Implementation of join using hashed shuffled data|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Atomic Replace Table as Select in Spark RAPIDS\nDESCRIPTION: This snippet describes the functionality for replacing a table based on a select statement for datasource V2 tables currently enabling staging table creation within Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_33\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.AtomicReplaceTableAsSelectExec\"></a>spark.rapids.sql.exec.AtomicReplaceTableAsSelectExec|Replace table as select for datasource V2 tables that support staging table creation|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Broadcast Exchange in Spark RAPIDS\nDESCRIPTION: This snippet specifies the backend for conducting broadcast exchanges of data within the Spark RAPIDS execution framework, optimizing data flows across partitions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_36\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.BroadcastExchangeExec\"></a>spark.rapids.sql.exec.BroadcastExchangeExec|The backend for broadcast exchange of data|true|None|\n```\n\n----------------------------------------\n\nTITLE: SQL Join and Aggregation Query Collection for RAPIDS Accelerator Testing\nDESCRIPTION: A collection of 36 SQL queries designed to test various aspects of the RAPIDS Accelerator for Apache Spark, including different join types, aggregation operations, and window functions with varying levels of complexity and data patterns.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/ScaleTest.md#2025-04-19_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_facts.*, b_data_{1-10} FROM b_data JOIN a_facts WHERE primary_a = b_foreign_a\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_facts.*, b_data_{1-10} FROM b_data FULL OUTER JOIN a_facts WHERE primary_a = b_foreign_a\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_facts.*, b_data_{1-10} FROM b_data LEFT OUTER JOIN a_facts WHERE primary_a = b_foreign_a\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT c_data.* FROM c_data LEFT ANTI JOIN a_facts WHERE primary_a = c_foreign_a\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT c_data.* FROM c_data LEFT SEMI JOIN a_facts WHERE primary_a = c_foreign_a\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT c_key2_*, COUNT(1), MIN(c_data_*), MAX(d_data_*) FROM c_data JOIN d_data WHERE c_key2_* = d_key2_* GROUP BY c_key2_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT c_key2_*, COUNT(1), MIN(c_data_*), MAX(d_data_*) FROM c_data FULL OUTER JOIN d_data WHERE c_key2_* = d_key2_* GROUP BY c_key2_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT c_key2_*, COUNT(1), MIN(c_data_*), MAX(d_data_*) FROM c_data LEFT OUTER JOIN d_data WHERE c_key2_* = d_key2_* GROUP BY c_key2_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT c_key2_*, COUNT(1), MIN(c_data_*) FROM c_data LEFT SEMI JOIN d_data WHERE c_key2_* = d_key2_* GROUP BY c_key2_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT c_key2_*, COUNT(1), MIN(c_data_*) FROM c_data LEFT ANTI JOIN d_data WHERE c_key2_* = d_key2_* GROUP BY c_key2_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT b_key3_*, e_data_*, b_data_* FROM b_data JOIN e_data WHERE b_key3_* = e_key3_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT b_key3_*, e_data_*, b_data_* FROM b_data FULL OUTER JOIN e_data WHERE b_key3_* = e_key3_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT b_key3_*, e_data_*, b_data_* FROM b_data LEFT OUTER JOIN e_data WHERE b_key3_* = e_key3_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT b_key3_*, b_data_* FROM b_data LEFT SEMI JOIN e_data WHERE b_key3_* = e_key3_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT b_key3_*, b_data_* FROM b_data LEFT ANTI JOIN e_data WHERE b_key3_* = e_key3_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_key4_1, a_data_(1-complexit/2), f_data_(1-complexity/2) FROM a_facts JOIN f_facts WHERE a_key4_1 = f_key4_1 && (a_data_low_unique_1 + f_data_low_unique_1) = 2\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_key4_1, a_data_(1-complexit/2), f_data_(1-complexity/2) FROM a_fact FULL OUTER JOIN f_fact WHERE a_key4_1 = f_key4_1 && (a_data_low_unique_1 + f_data_low_unique_1) = 2\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_key4_1, a_data_(1-complexit/2), f_data_(1-complexity/2) FROM a_fact LEFT OUTER JOIN f_fact WHERE a_key4_1 = f_key4_1 && (a_data_low_unique_1 + f_data_low_unique_1) = 2\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_key4_1, a_data_*, FROM a_fact LEFT ANTI JOIN f_fact WHERE a_key4_1 = f_key4_1 && (a_data_low_unique_1 + f_data_low_unique_1) != 2\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_key4_1, a_data_* FROM a_fact LEFT SEMI JOIN f_fact WHERE a_key4_1 = f_key4_1 && (a_data_low_unique_1 + f_data_low_unique_1) = 2\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a_key4_1, a_data_(1-complexity/2), f_data_(1-complexity/2) FROM a_fact JOIN f_fact WHERE a_key4_1 = f_key4_1 && (length(concat(a_data_low_unique_len_1, f_data_low_unique_len_1))) = 2\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT b_key3_*, complexity number of aggregations that are SUMs of 2 or more numeric data columns multiplied together or MIN/MAX of any data column FROM b_data GROUP BY b_key3_*.\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT complexity number of aggregations that are SUMs of 2 or more numeric data columns multiplied together or MIN/MAX of any data column FROM b_data.\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT g_key3_*, complexity number of aggregations that are SUM/MIN/MAX/AVERAGE/COUNT of 2 or more byte columns cast to int and added, subtracted, multiplied together. FROM g_data GROUP BY g_key3_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect g_key3_*, collect_set(g_data_enum_1) FROM g_data GROUP BY g_key3_*\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect b_foreign_a, collect_list(b_data_1) FROM b_data GROUP BY b_foreign_a\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {min(g_data_1), max(g_data_1), sum(g_data_2), count(g_data_3), average(g_data_4), row_number} over (UNBOUNDED PRECEDING TO CURRENT ROW PARTITION BY g_key3_* ORDER BY g_data_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {min(g_data_1), max(g_data_1), sum(g_data_2), count(g_data_3), average(g_data_4)} over (RANGE BETWEEN 1000 * scale_factor PRECEDING AND 5000 * scale_factor FOLLOWING PARTITION BY g_key3_* ORDER BY g_data_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {min(g_data_1), max(g_data_1), sum(g_data_2), count(g_data_3), average(g_data_4)} over (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING PARTITION BY g_key3_* ORDER BY g_data_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {min(g_data_1), max(g_data_1), sum(g_data_2), count(g_data_3), average(g_data_4)} over (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ORDER BY g_data_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {min(g_data_1), max(g_data_1), sum(g_data_2), count(g_data_3), average(g_data_4)} over (RANGE BETWEEN 1000 * scale_factor PRECEDING AND 5000 * scale_factor FOLLOWING ORDER BY g_data_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect  {min(g_data_1), max(g_data_1), sum(g_data_2), count(g_data_3), average(g_data_4)}  over (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ORDER BY g_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {lag(g_data_1, 10 * scale_factor, lead(g_data_2, 10 * scale_factor)} OVER (PARTITION BY g_key3_* ORDER BY g_data_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {lag(g_data_1, 10 * scale_factor, lead(g_data_2, 10 * scale_factor)} OVER (ORDER BY g_data_row_num_1)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {min(c_data_1), max(c_data_2)} over (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW PARTITION BY c_key2_(1 to complexity/2) ORDER BY c_key2_(complexity/2 to complexity)\n```\n\nLANGUAGE: sql\nCODE:\n```\nselect {min(c_data_1), max(c_data_2)} over (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING PARTITION BY c_key2_(1 to complexity/2) ORDER BY c_key2_(complexity/2 to complexity)\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Paths for XGBoost Training\nDESCRIPTION: Configures the file paths for training, evaluation, and transformation datasets. These paths point to the output of mortgage-ETL jobs stored in a cloud bucket.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// You need to update them to your real paths! The input data files is the output of mortgage-etl jobs\nval dataRoot = sys.env.getOrElse(\"DATA_ROOT\", \"gs://your-bucket/your-ETL-output-paths\")\nval trainPath = dataRoot + \"/train/\"\nval evalPath  = dataRoot + \"/eval/\"\nval transPath = dataRoot + \"/eval/\"\n```\n\n----------------------------------------\n\nTITLE: Defining Name Mappings for Financial Institutions in Python\nDESCRIPTION: Creates a list of tuples for mapping financial institution names to standardized shortened names. This helps normalize data where the same institution might appear under different variations of its legal name.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# name mappings\n_name_mapping = [\n        (\"WITMER FUNDING, LLC\", \"Witmer\"),\n        (\"WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015\", \"Wells Fargo\"),\n        (\"WELLS FARGO BANK,  NA\" , \"Wells Fargo\"),\n        (\"WELLS FARGO BANK, N.A.\" , \"Wells Fargo\"),\n        (\"WELLS FARGO BANK, NA\" , \"Wells Fargo\"),\n        (\"USAA FEDERAL SAVINGS BANK\" , \"USAA\"),\n        (\"UNITED SHORE FINANCIAL SERVICES, LLC D\\/B\\/A UNITED WHOLESALE MORTGAGE\" , \"United Seq(e\"),\n        (\"U.S. BANK N.A.\" , \"US Bank\"),\n        (\"SUNTRUST MORTGAGE INC.\" , \"Suntrust\"),\n        (\"STONEGATE MORTGAGE CORPORATION\" , \"Stonegate Mortgage\"),\n        (\"STEARNS LENDING, LLC\" , \"Stearns Lending\"),\n        (\"STEARNS LENDING, INC.\" , \"Stearns Lending\"),\n        (\"SIERRA PACIFIC MORTGAGE COMPANY, INC.\" , \"Sierra Pacific Mortgage\"),\n        (\"REGIONS BANK\" , \"Regions\"),\n        (\"RBC MORTGAGE COMPANY\" , \"RBC\"),\n        (\"QUICKEN LOANS INC.\" , \"Quicken Loans\"),\n        (\"PULTE MORTGAGE, L.L.C.\" , \"Pulte Mortgage\"),\n        (\"PROVIDENT FUNDING ASSOCIATES, L.P.\" , \"Provident Funding\"),\n        (\"PROSPECT MORTGAGE, LLC\" , \"Prospect Mortgage\"),\n        (\"PRINCIPAL RESIDENTIAL MORTGAGE CAPITAL RESOURCES, LLC\" , \"Principal Residential\"),\n        (\"PNC BANK, N.A.\" , \"PNC\"),\n        (\"PMT CREDIT RISK TRANSFER TRUST 2015-2\" , \"PennyMac\"),\n        (\"PHH MORTGAGE CORPORATION\" , \"PHH Mortgage\"),\n        (\"PENNYMAC CORP.\" , \"PennyMac\"),\n        (\"PACIFIC UNION FINANCIAL, LLC\" , \"Other\"),\n        (\"OTHER\" , \"Other\"),\n        (\"NYCB MORTGAGE COMPANY, LLC\" , \"NYCB\"),\n        (\"NEW YORK COMMUNITY BANK\" , \"NYCB\"),\n        (\"NETBANK FUNDING SERVICES\" , \"Netbank\"),\n        (\"NATIONSTAR MORTGAGE, LLC\" , \"Nationstar Mortgage\"),\n        (\"METLIFE BANK, NA\" , \"Metlife\"),\n        (\"LOANDEPOT.COM, LLC\" , \"LoanDepot.com\"),\n        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2015-1\" , \"JP Morgan Chase\"),\n        (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2014-1\" , \"JP Morgan Chase\"),\n        (\"JPMORGAN CHASE BANK, NATIONAL ASSOCIATION\" , \"JP Morgan Chase\"),\n        (\"JPMORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n        (\"JP MORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n        (\"IRWIN MORTGAGE, CORPORATION\" , \"Irwin Mortgage\"),\n        (\"IMPAC MORTGAGE CORP.\" , \"Impac Mortgage\"),\n        (\"HSBC BANK USA, NATIONAL ASSOCIATION\" , \"HSBC\"),\n        (\"HOMEWARD RESIDENTIAL, INC.\" , \"Homeward Mortgage\"),\n        (\"HOMESTREET BANK\" , \"Other\"),\n        (\"HOMEBRIDGE FINANCIAL SERVICES, INC.\" , \"HomeBridge\"),\n        (\"HARWOOD STREET FUNDING I, LLC\" , \"Harwood Mortgage\"),\n        (\"GUILD MORTGAGE COMPANY\" , \"Guild Mortgage\"),\n        (\"GMAC MORTGAGE, LLC (USAA FEDERAL SAVINGS BANK)\" , \"GMAC\"),\n        (\"GMAC MORTGAGE, LLC\" , \"GMAC\"),\n        (\"GMAC (USAA)\" , \"GMAC\"),\n        (\"FREMONT BANK\" , \"Fremont Bank\"),\n        (\"FREEDOM MORTGAGE CORP.\" , \"Freedom Mortgage\"),\n        (\"FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"Franklin America\"),\n        (\"FLEET NATIONAL BANK\" , \"Fleet National\"),\n        (\"FLAGSTAR CAPITAL MARKETS CORPORATION\" , \"Flagstar Bank\"),\n        (\"FLAGSTAR BANK, FSB\" , \"Flagstar Bank\"),\n        (\"FIRST TENNESSEE BANK NATIONAL ASSOCIATION\" , \"Other\"),\n        (\"FIFTH THIRD BANK\" , \"Fifth Third Bank\"),\n        (\"FEDERAL HOME LOAN BANK OF CHICAGO\" , \"Fedral Home of Chicago\"),\n        (\"FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB\" , \"FDIC\"),\n        (\"DOWNEY SAVINGS AND LOAN ASSOCIATION, F.A.\" , \"Downey Mortgage\"),\n        (\"DITECH FINANCIAL LLC\" , \"Ditech\"),\n        (\"CITIMORTGAGE, INC.\" , \"Citi\"),\n        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERFIRST MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERBANK MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n        (\"CHASE HOME FINANCE, LLC\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE (CIE 1)\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE\" , \"JP Morgan Chase\"),\n        (\"CASHCALL, INC.\" , \"CashCall\"),\n        (\"CAPITAL ONE, NATIONAL ASSOCIATION\" , \"Capital One\"),\n        (\"CALIBER HOME LOANS, INC.\" , \"Caliber Funding\"),\n        (\"BISHOPS GATE RESIDENTIAL MORTGAGE TRUST\" , \"Bishops Gate Mortgage\"),\n        (\"BANK OF AMERICA, N.A.\" , \"Bank of America\"),\n        (\"AMTRUST BANK\" , \"AmTrust\"),\n        (\"AMERISAVE MORTGAGE CORPORATION\" , \"Amerisave\"),\n        (\"AMERIHOME MORTGAGE COMPANY, LLC\" , \"AmeriHome Mortgage\"),\n        (\"ALLY BANK\" , \"Ally Bank\"),\n        (\"ACADEMY MORTGAGE CORPORATION\" , \"Academy Mortgage\"),\n        (\"NO CASH-OUT REFINANCE\" , \"OTHER REFINANCE\"),\n        (\"REFINANCE - NOT SPECIFIED\" , \"OTHER REFINANCE\"),\n        (\"Other REFINANCE\" , \"OTHER REFINANCE\")]\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Shell with ScalaTest for Integration Testing\nDESCRIPTION: Command for launching spark-shell with the required test JAR files to run integration tests using the ScalaTest framework. This includes the RAPIDS test JAR, integration test JAR, and ScalaTest dependencies.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nspark-shell --jars rapids-4-spark-tests_2.12-25.06.0-SNAPSHOT-tests.jar,rapids-4-spark-integration-tests_2.12-25.06.0-SNAPSHOT-tests.jar,scalatest_2.12-3.0.5.jar,scalactic_2.12-3.0.5.jar\n```\n\n----------------------------------------\n\nTITLE: Partitioning Support Matrix Table in Markdown\nDESCRIPTION: HTML table documenting support levels for different partitioning strategies across various data types. Includes support status indicators: S (Supported), NS (Not Supported), PS (Partially Supported with conditions), and special notes for timestamp handling and nested structure limitations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_83\n\nLANGUAGE: markdown\nCODE:\n```\n<table>\n<tr>\n<th>Partition</th>\n<th>Description</th>\n<th>Notes</th>\n<th>Param</th>\n<th>BOOLEAN</th>\n<th>BYTE</th>\n<th>SHORT</th>\n<th>INT</th>\n<th>LONG</th>\n<th>FLOAT</th>\n<th>DOUBLE</th>\n<th>DATE</th>\n<th>TIMESTAMP</th>\n<th>STRING</th>\n<th>DECIMAL</th>\n<th>NULL</th>\n<th>BINARY</th>\n<th>CALENDAR</th>\n<th>ARRAY</th>\n<th>MAP</th>\n<th>STRUCT</th>\n<th>UDT</th>\n<th>DAYTIME</th>\n<th>YEARMONTH</th>\n</tr>\n<tr>\n<td rowSpan=\"1\">HashPartitioning</td>\n<td rowSpan=\"1\">Hash based partitioning</td>\n<td rowSpan=\"1\">None</td>\n<td>hash_key</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td><em>PS<br/>UTC is only supported TZ for TIMESTAMP</em></td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td><b>NS</b></td>\n<td><b>NS</b></td>\n<td><em>PS<br/>Arrays of structs are not supported;<br/>UTC is only supported TZ for child TIMESTAMP;<br/>unsupported child types BINARY, CALENDAR, MAP, UDT, DAYTIME, YEARMONTH</em></td>\n<td><b>NS</b></td>\n<td><em>PS<br/>UTC is only supported TZ for child TIMESTAMP;<br/>unsupported child types BINARY, CALENDAR, MAP, UDT, DAYTIME, YEARMONTH</em></td>\n<td><b>NS</b></td>\n<td><b>NS</b></td>\n<td><b>NS</b></td>\n</tr>\n<tr>\n<td rowSpan=\"1\">RangePartitioning</td>\n<td rowSpan=\"1\">Range partitioning</td>\n<td rowSpan=\"1\">None</td>\n<td>order_key</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td><em>PS<br/>UTC is only supported TZ for TIMESTAMP</em></td>\n<td>S</td>\n<td>S</td>\n<td>S</td>\n<td><b>NS</b></td>\n<td><b>NS</b></td>\n<td><em>PS<br/>STRUCT is not supported as a child type for ARRAY;<br/>UTC is only supported TZ for child TIMESTAMP;<br/>unsupported child types BINARY, CALENDAR, ARRAY, UDT</em></td>\n<td> </td>\n<td><em>PS<br/>UTC is only supported TZ for child TIMESTAMP;<br/>unsupported child types BINARY, CALENDAR, ARRAY, UDT</em></td>\n<td><b>NS</b></td>\n<td> </td>\n<td> </td>\n</tr>\n<tr>\n<td>RoundRobinPartitioning</td>\n<td>Round robin partitioning</td>\n<td>None</td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n</tr>\n<tr>\n<td>SinglePartition$</td>\n<td>Single partitioning</td>\n<td>None</td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n<td> </td>\n</tr>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Verifying Shim Class Accessibility in Multi-version Jar\nDESCRIPTION: Command to verify that shim-specific classes are hidden from conventional classloaders in a multi-version distribution jar, showing the expected error when trying to access them directly.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ javap -cp dist/target/rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar com.nvidia.spark.rapids.shims.SparkShimImpl\nError: class not found: com.nvidia.spark.rapids.shims.SparkShimImpl\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Project in Spark RAPIDS\nDESCRIPTION: This snippet provides the backend functionality associated with most select, withColumn, and dropColumn statements in Spark RAPIDS, optimizing data projection.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.ProjectExec\"></a>spark.rapids.sql.exec.ProjectExec|The backend for most select, withColumn and dropColumn statements|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Sample in Spark RAPIDS\nDESCRIPTION: This snippet explains the backend implementation for the sample operator in Spark RAPIDS, allowing users to perform sampling of data subsets.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.SampleExec\"></a>spark.rapids.sql.exec.SampleExec|The backend for the sample operator|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Window in Spark RAPIDS\nDESCRIPTION: This snippet specifies the window-operator backend within the Spark RAPIDS framework, facilitating window-based operations on dataset analytics.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_49\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.WindowExec\"></a>spark.rapids.sql.exec.WindowExec|Window-operator backend|true|None|\n```\n\n----------------------------------------\n\nTITLE: Generating Comments and Moving Files with Shimplify\nDESCRIPTION: Maven command that both generates the shimplify comments and moves files to their new locations according to the simplified directory structure. This completes the conversion to the new structure.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmvn generate-sources -Dshimplify=true -Dshimplify.move=true\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Version Constraints\nDESCRIPTION: List of Python packages required for testing, including pytest for testing, pandas and pyarrow for data processing, and version-specific requirements for fastparquet based on Python version.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npytest\nsre_yield\npandas\npyarrow\npytest-xdist >= 2.0.0\nfindspark\nfsspec == 2025.3.0\nfastparquet == 0.8.3 ; python_version == '3.8'\nfastparquet == 2024.5.0 ; python_version >= '3.9'\n```\n\n----------------------------------------\n\nTITLE: Running Pytest with RAPIDS Plugin for Spark\nDESCRIPTION: Command for submitting pytest integration tests using spark-submit with the RAPIDS plugin JAR. This example assumes CUDA 11.0 and Spark built with Scala 2.12.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n$SPARK_HOME/bin/spark-submit --jars \"rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar\" ./runtests.py\n```\n\n----------------------------------------\n\nTITLE: Call Hierarchy for Tagging File Source Scans for GPU Execution in RAPIDS Accelerator\nDESCRIPTION: This snippet demonstrates the typical hierarchical flow for tagging a FileSourceScanExec for GPU execution in the RAPIDS Accelerator. The process starts in GpuOverrides, proceeds through FileSourceScanExecMeta and ScanExecShims, and eventually reaches format-specific tagging code.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/data-sources.md#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nFileSourceScanExecMeta.tagPlanForGpu\n  ScanExecShims.tagGpuFileSourceScanExecSupport\n    GpuFileSourceScanExec.tagSupport\n```\n\n----------------------------------------\n\nTITLE: Building Memory Leak Detection Library\nDESCRIPTION: Commands to compile and build the leak detection shared library from source code.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/dev/host_memory_leaks/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ng++ -fPIC -c leak_detect.cpp\ng++ -shared -Wl,-init,init_leak_detect -o libleak_detect.so leak_detect.o\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Generate in Spark RAPIDS\nDESCRIPTION: This snippet defines the backend for operations that generate more output rows than input rows, such as the explode operation, within the Spark RAPIDS framework.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.GenerateExec\"></a>spark.rapids.sql.exec.GenerateExec|The backend for operations that generate more output rows than input rows like explode|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Range in Spark RAPIDS\nDESCRIPTION: This snippet outlines the backend for managing range operator executions within the Spark RAPIDS framework, enabling range-based data operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.RangeExec\"></a>spark.rapids.sql.exec.RangeExec|The backend for range operator|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Broadcast Nested Loop Join in Spark RAPIDS\nDESCRIPTION: This snippet describes the implementation for joins using brute force methods, not supporting some join types, within the Spark RAPIDS framework.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_39\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.BroadcastNestedLoopJoinExec\"></a>spark.rapids.sql.exec.BroadcastNestedLoopJoinExec|Implementation of join using brute force. Full outer joins and joins where the broadcast side matches the join side (e.g.: LeftOuter with left broadcast) are not supported|true|None|\n```\n\n----------------------------------------\n\nTITLE: Multi-Column Join and Aggregation with Spark-RAPIDS DBGen\nDESCRIPTION: Shows how to perform a complex operation involving a join between a fact table and a data table followed by aggregation. This example demonstrates the performance difference when using the RAPIDS Accelerator.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\nval dbgen = DBGen()\nval factTable = dbgen.addTable(\"facts\", \"join_key long, value int\", 1000L)\nfactTable(\"join_key\").setSeedRange(0, 999).setSeedMapping(DistinctDistribution())\nval dataTable = dbgen.addTable(\"data\", \"join_key long, agg_key long\", 100000000L)\ndataTable(\"join_key\").setSeedRange(0, 999)\ndataTable(\"agg_key\").setSeedRange(0, 9)\nval fdf = factTable.toDF(spark)\nval ddf = dataTable.toDF(spark)\nspark.time(fdf.join(ddf).groupBy(\"agg_key\").agg(min(\"value\"),\n  max(\"value\"), sum(\"value\")).orderBy(\"agg_key\").show())\n```\n\n----------------------------------------\n\nTITLE: Verifying Jar Signature with GPG\nDESCRIPTION: This snippet demonstrates how to verify the signature of a RAPIDS Accelerator for Apache Spark JAR file using GPG (GNU Privacy Guard). It involves using the `gpg --verify` command to check the signature against the downloaded JAR and ASC files, ensuring the integrity and authenticity of the downloaded package.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n\"gpg --verify rapids-4-spark_2.12-24.02.0.jar.asc rapids-4-spark_2.12-24.02.0.jar\"\n```\n\n----------------------------------------\n\nTITLE: DateTime Parsing Operations\nDESCRIPTION: Examples of supported DateTime parsing operations using LocalDateTime with pattern formatting\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/udf-to-catalyst-expressions.md#2025-04-19_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nLocalDateTime.parse(x, DateTimeFormatter.ofPattern(pattern)).getYear\nLocalDateTime.parse(x, DateTimeFormatter.ofPattern(pattern)).getMonthValue\nLocalDateTime.parse(x, DateTimeFormatter.ofPattern(pattern)).getDayOfMonth\nLocalDateTime.parse(x, DateTimeFormatter.ofPattern(pattern)).getHour\nLocalDateTime.parse(x, DateTimeFormatter.ofPattern(pattern)).getMinute\nLocalDateTime.parse(x, DateTimeFormatter.ofPattern(pattern)).getSecond\n```\n\n----------------------------------------\n\nTITLE: Starting Spark History Server to Review Integration Test Results\nDESCRIPTION: Shell command to start the Spark History Server for reviewing test execution details. This command points the server to the event logs generated during test execution for a specific worker.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nSPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=integration_tests/target/run_dir/eventlog_gw0\" \\\n  ${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.history.HistoryServer\n```\n\n----------------------------------------\n\nTITLE: Calculating HyperLogLogPlusPlus Precision from RSD (Scala)\nDESCRIPTION: This Scala code snippet shows the formula used to calculate the precision for the HyperLogLogPlusPlus (approx_count_distinct) operation based on the relative standard deviation (RSD) parameter. The GPU supports a precision range of [5, 14].\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nMath.ceil(2.0d * Math.log(1.106d / rsd) / Math.log(2.0d)).toInt\n```\n\n----------------------------------------\n\nTITLE: Performing Addition in SQL\nDESCRIPTION: The 'Add' operation performs addition between two numeric values. It supports various numeric data types in both project and AST contexts.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\ncolumn1 + column2\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Object Hash Aggregate in Spark RAPIDS\nDESCRIPTION: This snippet specifies the backend for hash-based aggregations that support TypedImperativeAggregate functions in Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_26\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.ObjectHashAggregateExec\"></a>spark.rapids.sql.exec.ObjectHashAggregateExec|The backend for hash based aggregations supporting TypedImperativeAggregate functions|true|None|\n```\n\n----------------------------------------\n\nTITLE: Current Row Indicator in SQL\nDESCRIPTION: This special identifier serves as a boundary marker for a window frame, specifying that operations should stop at the current row. It does not require any parameters.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nCurrentRow$\n\nSpecial boundary for a window frame, indicating stopping at the current row\nNone\nproject\nresult\n...\n```\n\n----------------------------------------\n\nTITLE: ORC Input Scan in Spark RAPIDS\nDESCRIPTION: This snippet describes the scanning mechanism for parsing ORC files within the Spark RAPIDS framework, allowing users to manage ORC data inputs effectively.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_54\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.input.OrcScan\"></a>spark.rapids.sql.input.OrcScan|ORC parsing|true|None|\n```\n\n----------------------------------------\n\nTITLE: Verifying RAPIDS Accelerator for Apache Spark Jar Signature with GPG\nDESCRIPTION: Commands to import the NVIDIA public key and verify the signature of the RAPIDS Accelerator for Apache Spark jar file using GPG. This verification ensures the authenticity of the downloaded jar.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-22.02.0.jar.asc rapids-4-spark_2.12-22.02.0.jar\n```\n\n----------------------------------------\n\nTITLE: Array Empty Creation Example\nDESCRIPTION: Examples of creating empty arrays of different types using Array.empty[] syntax in Scala\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/udf-to-catalyst-expressions.md#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nArray.empty[Boolean]\nArray.empty[Byte]\nArray.empty[Short]\nArray.empty[Int]\nArray.empty[Long]\nArray.empty[Float]\nArray.empty[Double]\nArray.empty[String]\n```\n\n----------------------------------------\n\nTITLE: Generating Bloop Projects for Spark 3.2.0\nDESCRIPTION: Maven command to generate Bloop build server projects for Spark 3.2.0 dependency with source downloads enabled, facilitating IDE integration.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmvn -B clean install \\\n    -DbloopInstall \\\n    -DdownloadSources=true \\\n    -Dbuildver=320\n```\n\n----------------------------------------\n\nTITLE: ArrayBuffer Operations Example\nDESCRIPTION: Common ArrayBuffer operations that are supported by the UDF compiler including initialization, distinct, toArray, and append operations\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/udf-to-catalyst-expressions.md#2025-04-19_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nnew ArrayBuffer()\nx.distinct\nx.toArray\nlhs += rhs\nlhs :+ rhs\n```\n\n----------------------------------------\n\nTITLE: Using LessThanOrEqual Operator in Spark SQL\nDESCRIPTION: The less than or equal operator (<=) compares two values across various data types. It returns a boolean indicating if the left side is less than or equal to the right side.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_43\n\nLANGUAGE: SQL\nCODE:\n```\ncolumn1 <= column2\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Sort Aggregate in Spark RAPIDS\nDESCRIPTION: This snippet outlines the backend functionality for sort-based aggregations within the Spark RAPIDS framework, facilitating optimized sorting and aggregating of data.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_27\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.SortAggregateExec\"></a>spark.rapids.sql.exec.SortAggregateExec|The backend for sort based aggregations|true|None|\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Configuration for Compute Sanitizer Java Home\nDESCRIPTION: Spark configuration parameter to set the executor environment to use the 'fake' Java home directory, enabling Compute Sanitizer for all executor processes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/compute_sanitizer.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n--conf spark.executorEnv.JAVA_HOME=\"/opt/compute-sanitizer-java\"\n```\n\n----------------------------------------\n\nTITLE: Finding All Files for a Specific Shim with Git\nDESCRIPTION: Command to search for all files that include a specific Spark version (323) in their shim metadata using git grep. This allows developers to easily locate all files associated with a particular shim after converting to the simplified structure.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit grep '{\"spark\": \"323\"}' '*.scala' '*.java'\n```\n\n----------------------------------------\n\nTITLE: Using ElementAt Function in Spark\nDESCRIPTION: The element_at function returns the element at a given index (1-based) if the column is an array, or the value for a given key if the column is a map. It supports various primitive types as results.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\n`element_at`\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for RAPIDS Accelerator JAR\nDESCRIPTION: Command-line instructions for downloading and verifying the digital signature of RAPIDS Accelerator JAR files using GPG for Scala 2.12 and 2.13\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ngpg --import PUB_KEY\n```\n\nLANGUAGE: shell\nCODE:\n```\ngpg --verify rapids-4-spark_2.12-23.12.2.jar.asc rapids-4-spark_2.12-23.12.2.jar\n```\n\nLANGUAGE: shell\nCODE:\n```\ngpg --verify rapids-4-spark_2.13-23.12.2.jar.asc rapids-4-spark_2.13-23.12.2.jar\n```\n\n----------------------------------------\n\nTITLE: Replaying Dumped Data using Scala in Spark-RAPIDS\nDESCRIPTION: This Scala code snippet demonstrates how to replay dumped data using the LORE framework in Spark-RAPIDS. It restores a GPU executor based on a given LORE ID, executes it, and prints the results to the console.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/lore.md#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nGpuColumnarToRowExec(\n   GpuLore.restoreGpuExec(\n      new Path(s\"${TEST_FILES_ROOT.getAbsolutePath}/loreId-$loreId\"), \n      spark))\n        .executeCollect()\n        .foreach(println)\n```\n\n----------------------------------------\n\nTITLE: Building Spark RAPIDS on ARM Platform with Maven\nDESCRIPTION: Command for building Spark RAPIDS on ARM64 platform using Maven with a specific build version. The arm64 profile is enabled to target ARM architecture.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean verify -Dbuildver=320 -Parm64\n```\n\n----------------------------------------\n\nTITLE: Starting ETL Process Timer with Python\nDESCRIPTION: Code to begin timing the ETL process execution for performance measurement.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/Databricks/Mortgage-ETL-db.ipynb#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nstart = time.time()\n```\n\n----------------------------------------\n\nTITLE: Running Java Application with Leak Detection\nDESCRIPTION: Command to run a Java application with the leak detection library preloaded and stderr capture.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/dev/host_memory_leaks/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nLD_PRELOAD=$PWD/libleak_detect.so java -cp ... com.nvidia.MyTest 2>alloc_log.txt\n```\n\n----------------------------------------\n\nTITLE: Execution Step for In-Memory Table Scan in Spark RAPIDS\nDESCRIPTION: This snippet details the implementation of the InMemoryTableScanExec to utilize GPU-accelerated caching capabilities within Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_28\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.InMemoryTableScanExec\"></a>spark.rapids.sql.exec.InMemoryTableScanExec|Implementation of InMemoryTableScanExec to use GPU accelerated caching|true|None|\n```\n\n----------------------------------------\n\nTITLE: Adding NVTX Ranges in Java/Scala\nDESCRIPTION: Example of adding NVTX ranges to Java or Scala code for profiling specific code blocks.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/nvtx_profiling.md#2025-04-19_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval nvtxRange = new NvtxRangeWithDoc(<NvtxId>, NvtxColor.YELLOW)\ntry {\n  // the code you want to profile\n} finally {\n  nvtxRange.close()\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Tests\nDESCRIPTION: Command to install all required Python dependencies for running the integration tests, using pip and the requirements.txt file provided with the project.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for Scala 2.13 Jar\nDESCRIPTION: This shell command imports a public key and verifies the signature of the RAPIDS Accelerator for Apache Spark 23.12.0 jar built for Scala 2.13, ensuring the file's authenticity and integrity. GPG must be installed to execute these commands.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_17\n\nLANGUAGE: Shell\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.13-23.12.0.jar.asc rapids-4-spark_2.13-23.12.0.jar\n```\n\n----------------------------------------\n\nTITLE: Examining jdeps Output for Class Dependencies in Spark-RAPIDS\nDESCRIPTION: Command to inspect the generated DOT file for a specific class dependency, showing how jdeps labels source and target nodes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ grep 'com.nvidia.spark.rapids.GpuFilterExec\\$' spark-shared.dot\n   \"com.nvidia.spark.rapids.GpuFilterExec$\"           -> \"com.nvidia.spark.rapids.GpuFilterExec (spark330)\";\n   \"com.nvidia.spark.rapids.GpuOverrides$$anon$204\"   -> \"com.nvidia.spark.rapids.GpuFilterExec$ (spark-shared)\";\n```\n\n----------------------------------------\n\nTITLE: Spark Submit Configuration for Kubernetes\nDESCRIPTION: Example spark-submit command with necessary configurations for running Spark jobs on Microk8s cluster.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$SPARK_HOME/bin/spark-submit \\\n    --master k8s://https://kubernetes.default.svc.cluster.local:16443 \\\n    --deploy-mode cluster \\\n    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n    --conf spark.kubernetes.authenticate.caCertFile=/var/snap/microk8s/current/certs/ca.crt \\\n    --conf spark.kubernetes.authenticate.submission.oauthToken=$K8S_TOKEN \\\n    --conf spark.kubernetes.container.image=spark-rapids\n```\n\n----------------------------------------\n\nTITLE: Updating Build Files for Scala 2.13\nDESCRIPTION: Command to sync changes between Scala 2.12 and 2.13 build files, ensuring any new dependencies or modifications are properly propagated to Scala 2.13 builds.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scala2.13/README.md#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./build/make-scala-version-build-files.sh 2.13\n```\n\n----------------------------------------\n\nTITLE: Configuring Length for String and Array Columns\nDESCRIPTION: Shows how to configure fixed and variable lengths for string and array columns. This example sets different length constraints for each column and demonstrates how to configure nested array elements.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string, b array<string>, c string\", 3)\ndataTable(\"a\").setLength(1)\ndataTable(\"b\").setLength(2)\ndataTable(\"b\")(\"data\").setLength(3)\ndataTable(\"c\").setLength(1,5)\ndataTable.toDF(spark).show(false)\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Expand in Spark RAPIDS\nDESCRIPTION: This snippet describes the backend implementation for the expand operator in Spark RAPIDS, enabling the expansion of data across partitions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.ExpandExec\"></a>spark.rapids.sql.exec.ExpandExec|The backend for the expand operator|true|None|\n```\n\n----------------------------------------\n\nTITLE: Installing Maven Project for Specific Spark Version\nDESCRIPTION: Maven command to prepare the project for IDE import by installing artifacts for a specific Spark version (3.4.0) while skipping tests and documentation generation.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -Dbuildver=340 -Dmaven.scaladoc.skip -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Environment Variables\nDESCRIPTION: Shell commands to configure Maven environment variables for manual installation, including setting the Maven home directory and adding its bin directory to the PATH.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport M2_HOME=PATH_TO_MAVEN_ROOT_DIRECTOTY\nexport M2=${M2_HOME}/bin\nexport PATH=$M2:$PATH\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Coalesce in Spark RAPIDS\nDESCRIPTION: This snippet defines the backend for the DataFrame coalesce method in Spark RAPIDS, optimizing how data is managed across partitions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.CoalesceExec\"></a>spark.rapids.sql.exec.CoalesceExec|The backend for the dataframe coalesce method|true|None|\n```\n\n----------------------------------------\n\nTITLE: Building RAPIDS Accelerator for Specific Spark Version with Scala 2.13\nDESCRIPTION: Maven command to build the RAPIDS Accelerator with Scala 2.13 for a specific version of Apache Spark (3.3.0) using the buildver parameter.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scala2.13/README.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmvn -Dbuildver=330 verify\n```\n\n----------------------------------------\n\nTITLE: Running jdeps for Class Dependency Analysis in Spark-RAPIDS\nDESCRIPTION: Command that uses jdeps to analyze dependencies between public, spark-shared, and specific Spark version classes, generating DOT files for visualization.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n${JAVA_HOME}/bin/jdeps -v \\\n  -dotoutput /tmp/jdeps330 \\\n  -regex '(com|org)\\..*\\.rapids\\..*' \\\n  public spark-shared spark330\n```\n\n----------------------------------------\n\nTITLE: Configuring RAPIDS ParquetCachedBatchSerializer in Spark\nDESCRIPTION: Command to start Spark shell with the RAPIDS ParquetCachedBatchSerializer configuration. This enables Parquet-based compression for cached data with potential GPU acceleration.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/cache-serializer.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nspark-shell --conf spark.sql.cache.serializer=com.nvidia.spark.ParquetCachedBatchSerializer\n```\n\n----------------------------------------\n\nTITLE: Building Scale Test Suite with Specific Spark and JDK Versions\nDESCRIPTION: Maven command demonstrating how to build the Scale Test suite with specific build version (332) and JDK profile (JDK 17)\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/ScaleTest.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn package -Dbuildver=332 -Pjdk17\n```\n\n----------------------------------------\n\nTITLE: Execution Step for File Source Scan in Spark RAPIDS\nDESCRIPTION: This snippet details the implementation of reading data from files, commonly from Hive tables, in the Spark RAPIDS execution framework.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.FileSourceScanExec\"></a>spark.rapids.sql.exec.FileSourceScanExec|Reading data from files, often from Hive tables|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Flat Map Groups in Pandas in Spark RAPIDS\nDESCRIPTION: This snippet describes the backend for Flat Map Groups Pandas UDFs, optimizing data transfers and GPU scheduling, enabled by default.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_46\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.FlatMapGroupsInPandasExec\"></a>spark.rapids.sql.exec.FlatMapGroupsInPandasExec|The backend for Flat Map Groups Pandas UDF, Accelerates the data transfer between the Java process and the Python process. It also supports scheduling GPU resources for the Python process when enabled.|true|None|\n```\n\n----------------------------------------\n\nTITLE: Setting CPU-Specific XGBoost Parameters\nDESCRIPTION: Specifies parameters for CPU version of XGBoost, including the number of workers (recommended to be equal to CPU cores) and tree method ('hist' for CPU).\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n// difference in parameters\n  \"num_workers\" -> 12,\n  \"tree_method\" -> \"hist\",\n```\n\n----------------------------------------\n\nTITLE: Checking Pyenv Installation\nDESCRIPTION: Command to verify that pyenv is correctly installed and accessible in the current environment by checking its location in the PATH.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwhich pyenv\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Pattern for Scala Version-Specific Code\nDESCRIPTION: Pattern showing how source files are organized to support both Scala 2.12 and 2.13 versions, allowing for version-specific implementations when necessary.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scala2.13/README.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nspark-rapids/<module>/src/main/scala-<scala_version>/<path to package>/Source.scala\n```\n\n----------------------------------------\n\nTITLE: Setting Java Home Path in Shell Environment\nDESCRIPTION: Command to set the JAVA_HOME environment variable by determining the path from the java executable. This approach automatically finds the correct Java installation directory.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nJAVA_HOME=$(readlink -nf $(which java) | xargs dirname | xargs dirname | xargs dirname)\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Support for Pandas UDF on Standalone Spark\nDESCRIPTION: Shell commands to configure Spark job for enabling GPU support with Pandas UDF on standalone Spark. This includes setting the PYTHONPATH and adding the RAPIDS Accelerator jar as a Python file.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n--conf spark.executorEnv.PYTHONPATH=${SPARK_RAPIDS_PLUGIN_JAR} \\\n--py-files ${SPARK_RAPIDS_PLUGIN_JAR} \\\n```\n\n----------------------------------------\n\nTITLE: Listing Available Python Versions in Pyenv\nDESCRIPTION: Command to list all Python versions available for installation through pyenv. This helps users identify which Python versions they can install.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nls ~/.pyenv/versions/\n```\n\n----------------------------------------\n\nTITLE: Date and Time Component Extraction\nDESCRIPTION: Functions for extracting time components like minute and month from timestamps and dates. UTC timezone is required for timestamp operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_50\n\nLANGUAGE: SQL\nCODE:\n```\nminute(input)\nmonth(input)\nmonths_between(timestamp1, timestamp2, round)\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Map in Pandas in Spark RAPIDS\nDESCRIPTION: This snippet details the backend for Map Pandas Iterator UDFs, optimizing the data transfer and resource management between processes in Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_47\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.MapInPandasExec\"></a>spark.rapids.sql.exec.MapInPandasExec|The backend for Map Pandas Iterator UDF. Accelerates the data transfer between the Java process and the Python process. It also supports scheduling GPU resources for the Python process when enabled.|true|None|\n```\n\n----------------------------------------\n\nTITLE: Enabling Memory Bookkeeping in Spark-RAPIDS\nDESCRIPTION: System properties to enable memory allocation bookkeeping at the thread level. The basic mode logs allocation sizes while the callstack option adds stack traces for each allocation, helpful for diagnosing OOM errors.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/mem_debug.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n-Dai.rapids.memory.bookkeep=true\n```\n\nLANGUAGE: bash\nCODE:\n```\n-Dai.rapids.memory.bookkeep=true\n-Dai.rapids.memory.bookkeep.callstack=true\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Overwrite by Expression in Spark RAPIDS\nDESCRIPTION: This snippet describes the process of overwriting data into a datasource V2 table using the version 1 write interface within Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_35\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.OverwriteByExpressionExecV1\"></a>spark.rapids.sql.exec.OverwriteByExpressionExecV1|Overwrite into a datasource V2 table using the V1 write interface|true|None|\n```\n\n----------------------------------------\n\nTITLE: Nth Value Window Function\nDESCRIPTION: Window function implementation for nth_value operation, supporting most data types with restrictions on complex types and timestamp timezones.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_55\n\nLANGUAGE: SQL\nCODE:\n```\nnth_value\n```\n\n----------------------------------------\n\nTITLE: Restoring Files and Staged Changes After Failed Conversion\nDESCRIPTION: Git commands to completely undo shimplify changes if issues are discovered after conversion. These commands restore both staged and unstaged changes to revert to the previous state.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit restore --staged sql-plugin tests\ngit restore sql-plugin tests\n```\n\n----------------------------------------\n\nTITLE: Building Spark Distribution from Source\nDESCRIPTION: Shell commands to clone and build Apache Spark from source code with Kubernetes and Hive support. This is useful when needing to test against specific or snapshot versions of Spark.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n## clone locally\ngit clone https://github.com/apache/spark.git spark-src-latest\ncd spark-src-latest\n## build a distribution with hive support\n## generate a single tgz file $MY_SPARK_BUILD.tgz\n./dev/make-distribution.sh --name $MY_SPARK_BUILD --tgz -Pkubernetes -Phive\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Support for Pandas UDF on YARN\nDESCRIPTION: Shell commands to configure Spark job for enabling GPU support with Pandas UDF on YARN. This includes adding the RAPIDS Accelerator jar as a Python file.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n--py-files ${SPARK_RAPIDS_PLUGIN_JAR} \\\n```\n\n----------------------------------------\n\nTITLE: Checking Which New Directories Would Be Removed\nDESCRIPTION: Git command to preview which new directories would be removed when reverting shimplify changes. This allows safely reviewing what will be deleted before actually removing anything.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clean -f -d --dry-run\n```\n\n----------------------------------------\n\nTITLE: Creating Normal Distribution for Data Skew\nDESCRIPTION: Creates a dataset with values following a normal distribution centered around a mean of 50 with a standard deviation of 1.0.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string\", 100000)\ndataTable(\"a\").setSeedMapping(NormalDistribution(50, 1.0)).setSeedRange(0, 100)\ndataTable.toDF(spark).groupBy(\"a\").count().orderBy(desc(\"count\")).show()\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Hive Table Scan in Spark RAPIDS\nDESCRIPTION: This snippet describes the scan executor for reading Hive delimited text tables in Spark RAPIDS, facilitating the access and processing of Hive stored data.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_50\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.HiveTableScanExec\"></a>spark.rapids.sql.exec.HiveTableScanExec|Scan Exec to read Hive delimited text tables|true|None|\n```\n\n----------------------------------------\n\nTITLE: Generating Shim Comments with Shimplify\nDESCRIPTION: Maven command that triggers shimplify to generate and inject spark-rapids-shim-json-lines comments to all shim source files without moving them yet. This allows for verification before committing to the new structure.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmvn generate-sources -Dshimplify=true [-D...]\n```\n\n----------------------------------------\n\nTITLE: Displaying Cluster Information with Jupyter Magic Command\nDESCRIPTION: Uses the Jupyter notebook magic command to display information about the current cluster configuration and status.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%info\n\n```\n\n----------------------------------------\n\nTITLE: Generating Java Classes from FlatBuffers Schema\nDESCRIPTION: Command to generate mutable Java classes from FlatBuffers schema files (*.fbs) in the current directory.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/sql-plugin/src/main/format/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nflatc --java -o ../java/ --gen-mutable *.fbs\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Environment Variables\nDESCRIPTION: Shell commands to set up the Spark environment variables after installation, including SPARK_HOME and updating the PATH to include Spark's bin directory.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport SPARK_HOME=$SPARK_INSTALLS_DIR/$MY_SPARK_BUILD\nexport PATH=${SPARK_HOME}/bin:$PATH\n```\n\n----------------------------------------\n\nTITLE: Example of ArrowEvalPythonExec Usage in PySpark\nDESCRIPTION: This code snippet shows the use of UDFs with ArrowEvalPythonExec in PySpark. It demonstrates how multiple Python processes can be triggered for each Spark task, potentially leading to deadlocks if concurrent workers are limited.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark.sql.functions as F\n...\ndf = df.withColumn(\"c_1\",udf_1(F.col(\"a\"), F.col(\"b\")))\ndf = df.withColumn('c_2', F.hash(F.col('c_1')))\ndf = df.withColumn(\"c_3\",udf_2(F.col(\"c_2\")))\n...\n```\n\n----------------------------------------\n\nTITLE: Spark Python Runner Patch\nDESCRIPTION: Patch to modify Spark's PythonRunner to remove LD_PRELOAD when launching Python processes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/dev/host_memory_leaks/README.md#2025-04-19_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\ndiff --git a/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala b/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\nindex c3f73ed745..82203659c3 100644\n--- a/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\n+++ b/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\n@@ -74,6 +74,7 @@ object PythonRunner {\n     // Launch Python process\n     val builder = new ProcessBuilder((Seq(pythonExec, formattedPythonFile) ++ otherArgs).asJava)\n     val env = builder.environment()\n+    env.remove(\"LD_PRELOAD\")\n     env.put(\"PYTHONPATH\", pythonPath)\n     // This is equivalent to setting the -u flag; we use it because ipython doesn't support -u:\n     env.put(\"PYTHONUNBUFFERED\", \"YES\") // value is needed to be set to a non-empty string\n```\n\n----------------------------------------\n\nTITLE: Creating Exponential Distribution for Data Skew\nDESCRIPTION: Creates a dataset with values following an exponential distribution with a target seed of 50 and a standard deviation of 1.0.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string\", 100000)\ndataTable(\"a\").setSeedMapping(ExponentialDistribution(50, 1.0)).setSeedRange(0, 100)\ndataTable.toDF(spark).groupBy(\"a\").count().orderBy(desc(\"count\")).show()\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Appending Data in Spark RAPIDS\nDESCRIPTION: This snippet describes the backend for appending data into a datasource V2 table, utilizing the version 1 write interface within Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_31\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.AppendDataExecV1\"></a>spark.rapids.sql.exec.AppendDataExecV1|Append data into a datasource V2 table using the V1 write interface|true|None|\n```\n\n----------------------------------------\n\nTITLE: Type Error Example with Array Sum UDF\nDESCRIPTION: Demonstrates the error message when attempting to use the sum_array UDF with incorrect input types (double array instead of long array).\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/DF_UDF_README.md#2025-04-19_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nSeq(Array(1.0, 2.0, 3.0)).toDF(\"data\").selectExpr(\"sum_array(data) as result\").show()\norg.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"aggregate(data, 0, lambdafunction((coalesce(namedlambdavariable(), 0) + coalesce(namedlambdavariable(), 0)), namedlambdavariable(), namedlambdavariable()), lambdafunction(namedlambdavariable(), namedlambdavariable()))\" due to data type mismatch: Parameter 3 requires the \"BIGINT\" type, however \"lambdafunction((coalesce(namedlambdavariable(), 0) + coalesce(namedlambdavariable(), 0)), namedlambdavariable(), namedlambdavariable()))\" has the type \"DOUBLE\".; line 1 pos 0;\n```\n\n----------------------------------------\n\nTITLE: Syncing POM Changes Between Scala Versions\nDESCRIPTION: Script to synchronize changes made to pom.xml files between Scala 2.12 and 2.13 builds, ensuring dependencies and other modifications are propagated correctly.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./build/make-scala-version-build-files.sh 2.13\n```\n\n----------------------------------------\n\nTITLE: Implementing Array Sum UDF in Scala using Dataframe Operations\nDESCRIPTION: Demonstrates how to create a UDF that sums array elements using dataframe operations. The function aggregates a long array by adding its elements with null handling using coalesce.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/DF_UDF_README.md#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport com.nvidia.spark.functions._\n\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions._\n\nval sum_array = df_udf((longArray: Column) => \n  aggregate(longArray,\n    lit(0L),\n    (a, b) => coalesce(a, lit(0L)) + coalesce(b, lit(0L)),\n    a => a))\nspark.udf.register(\"sum_array\", sum_array)\n```\n\n----------------------------------------\n\nTITLE: Running the Commit Audit Script for RAPIDS Plugin\nDESCRIPTION: Command to execute the prioritize-commits.sh script which analyzes Spark commits against the RAPIDS Plugin codebase. The script requires environment variables for workspace location, Spark source tree path, and the commit list file path.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scripts/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nWORKSPACE=~/workspace SPARK_TREE=~/workspace/spark COMMIT_DIFF_LOG=~/workspace/commits.log \n./scripts/prioritize-commits.sh\n```\n\n----------------------------------------\n\nTITLE: Creating Spark Service Account\nDESCRIPTION: Command to create a Kubernetes service account for Spark applications.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmicrok8s.kubectl create serviceaccount spark\n```\n\n----------------------------------------\n\nTITLE: Building RAPIDS Accelerator with Maven for Scala 2.13\nDESCRIPTION: Command for building the RAPIDS Accelerator with Scala 2.13 using Maven to the verify phase. This builds the plugin for the default Apache Spark version (3.3.0).\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scala2.13/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmvn verify -f scala2.13/\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for RAPIDS Accelerator Jar\nDESCRIPTION: The shell commands import a public key and verify the signature of the RAPIDS Accelerator for Apache Spark 23.10.0 jar for Scala 2.12. This verification process ensures the file's integrity and authenticity. These commands require GPG installed on your system.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-23.10.0.jar.asc rapids-4-spark_2.12-23.10.0.jar\n```\n\n----------------------------------------\n\nTITLE: Range Partitioning in Spark RAPIDS\nDESCRIPTION: This snippet specifies the range partitioning mechanism available in Spark RAPIDS to distribute data based on value ranges across partitions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_58\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.partitioning.RangePartitioning\"></a>spark.rapids.sql.partitioning.RangePartitioning|Range partitioning|true|None|\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Atomic Create Table as Select in Spark RAPIDS\nDESCRIPTION: This snippet outlines the execution logic for creating a table as a select statement for datasource V2 tables, supporting staging table creation in Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_32\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.AtomicCreateTableAsSelectExec\"></a>spark.rapids.sql.exec.AtomicCreateTableAsSelectExec|Create table as select for datasource V2 tables that support staging table creation|true|None|\n```\n\n----------------------------------------\n\nTITLE: Running Compute Sanitizer with a Single Spark Worker\nDESCRIPTION: Shell command to start a Spark worker in standalone mode with Compute Sanitizer. It sanitizes both the worker and shell until exit, then stops the worker process.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/compute_sanitizer.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncompute-sanitizer --target-processes all bash -c \" \\\n${SPARK_HOME}/sbin/start-worker.sh $master_url & \\\n$SPARK_HOME/bin/spark-shell; \\\n${SPARK_HOME}/sbin/stop-worker.sh\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Shim Classes with Spark Version Prefix\nDESCRIPTION: Command demonstrating how to access shim classes in a multi-version jar by using the Spark version prefix, showing that the bytecode can be loaded when properly prefixed.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ javap -cp dist/target/rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar spark320.com.nvidia.spark.rapids.shims.SparkShimImpl | head -2\nWarning: File dist/target/rapids-4-spark_2.12-25.06.0-SNAPSHOT-cuda11.jar(/spark320/com/nvidia/spark/rapids/shims/SparkShimImpl.class) does not contain class spark320.com.nvidia.spark.rapids.shims.SparkShimImpl\nCompiled from \"SparkShims.scala\"\npublic final class com.nvidia.spark.rapids.shims.SparkShimImpl {\n```\n\n----------------------------------------\n\nTITLE: Reference Count Debug Output for Memory Leaks in Spark-RAPIDS\nDESCRIPTION: Sample output when reference count debugging is enabled, showing the stack trace when a ColumnVector's reference count was incremented but not properly decremented, resulting in a memory leak.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/mem_debug.md#2025-04-19_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nERROR ColumnVector: A DEVICE COLUMN VECTOR WAS LEAKED (ID: 7 7f756c1337e0)\n23/12/27 20:21:32 ERROR MemoryCleaner: Leaked vector (ID: 7): 2023-12-27 20:21:20.0002 GMT: INC\njava.lang.Thread.getStackTrace(Thread.java:1564)\nai.rapids.cudf.MemoryCleaner$RefCountDebugItem.<init>(MemoryCleaner.java:336)\nai.rapids.cudf.MemoryCleaner$Cleaner.addRef(MemoryCleaner.java:90)\nai.rapids.cudf.ColumnVector.incRefCountInternal(ColumnVector.java:298)\nai.rapids.cudf.ColumnVector.<init>(ColumnVector.java:120)\n```\n\n----------------------------------------\n\nTITLE: Creating a Fake Java Binary for Compute Sanitizer\nDESCRIPTION: Shell commands to create a wrapper script that intercepts calls to Java and runs them through Compute Sanitizer instead, then makes the script executable.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/compute_sanitizer.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\necho 'compute-sanitizer java \"$@\"' > /opt/compute-sanitizer-java/bin/java\nchmod +x /opt/compute-sanitizer-java/bin/java\n```\n\n----------------------------------------\n\nTITLE: Double Free Debug Output in Spark-RAPIDS\nDESCRIPTION: Debug output showing a double free issue where a ColumnVector's reference count was decremented twice after a single increment, providing stack traces for all operations to help identify the root cause.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/mem_debug.md#2025-04-19_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nERROR MemoryCleaner: double free ColumnVector{rows=6, type=INT32, nullCount=Optional[0], offHeap=(ID: 15 0)} (ID: 15): 2023-12-27 20:12:34.0607 GMT: INC\njava.lang.Thread.getStackTrace(Thread.java:1564)\nai.rapids.cudf.MemoryCleaner$RefCountDebugItem.<init>(MemoryCleaner.java:336)\nai.rapids.cudf.MemoryCleaner$Cleaner.addRef(MemoryCleaner.java:90)\n...\n2023-12-27 20:12:34.0607 GMT: DEC\njava.lang.Thread.getStackTrace(Thread.java:1564)\nai.rapids.cudf.MemoryCleaner$RefCountDebugItem.<init>(MemoryCleaner.java:336)\nai.rapids.cudf.MemoryCleaner$Cleaner.delRef(MemoryCleaner.java:98)\nai.rapids.cudf.ColumnVector.close(ColumnVector.java:262)\n...\n2023-12-27 20:12:34.0607 GMT: DEC\njava.lang.Thread.getStackTrace(Thread.java:1564)\nai.rapids.cudf.MemoryCleaner$RefCountDebugItem.<init>(MemoryCleaner.java:336)\nai.rapids.cudf.MemoryCleaner$Cleaner.delRef(MemoryCleaner.java:98)\nai.rapids.cudf.ColumnVector.close(ColumnVector.java:262)\n...\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for RAPIDS Accelerator Jar v23.08.2\nDESCRIPTION: These shell commands verify the signature of the RAPIDS Accelerator for Apache Spark 23.08.2 jar for Scala 2.12, by importing a public key and performing the verification. This ensures the integrity and authenticity of the jar file. GPG must be installed to run these commands.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_19\n\nLANGUAGE: Shell\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-23.08.2.jar.asc rapids-4-spark_2.12-23.08.2.jar\n```\n\n----------------------------------------\n\nTITLE: Command for Saving into Data Source in Spark RAPIDS\nDESCRIPTION: This snippet provides the command functionality for writing data to a specified data source within Spark RAPIDS, facilitating data output processes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_51\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.command.SaveIntoDataSourceCommand\"></a>spark.rapids.sql.command.SaveIntoDataSourceCommand|Write to a data source|true|None|\n```\n\n----------------------------------------\n\nTITLE: Sample Compute Sanitizer Error Output\nDESCRIPTION: Example of error messages produced by Compute Sanitizer when it detects an invalid device context in a CUDA API call, showing the full stack trace to help with debugging.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/compute_sanitizer.md#2025-04-19_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n========= COMPUTE-SANITIZER\n========= Program hit invalid device context (error 201) on CUDA API call to cuCtxGetDevice.\n=========     Saved host backtrace up to driver entry point at error\n=========     Host Frame: [0x24331b]\n=========                in /lib/x86_64-linux-gnu/libcuda.so.1\n=========     Host Frame:uct_cuda_base_query_devices_common [0x6209]\n=========                in /usr/lib/ucx/libuct_cuda.so.0.0.0\n=========     Host Frame:uct_md_query_tl_resources [0x12976]\n=========                in /usr/lib/libuct.so.0.0.0\n=========     Host Frame: [0x17b17]\n=========                in /usr/lib/libucp.so.0.0.0\n=========     Host Frame: [0x191c0]\n=========                in /usr/lib/libucp.so.0.0.0\n=========     Host Frame: [0x19674]\n=========                in /usr/lib/libucp.so.0.0.0\n=========     Host Frame:ucp_init_version [0x1a4e3]\n=========                in /usr/lib/libucp.so.0.0.0\n=========     Host Frame:Java_org_openucx_jucx_ucp_UcpContext_createContextNative [0x38ff]\n=========                in /tmp/jucx4889414997471006264/libjucx.so\n=========     Host Frame: [0x7fdbc9017da7]\n=========                in\n```\n\n----------------------------------------\n\nTITLE: Decorating Source Nodes in DOT Files for Spark-RAPIDS Dependency Analysis\nDESCRIPTION: Bash commands using sed to add archive labels to source nodes in the DOT files for clearer dependency path analysis.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsed 's/\"\\([^(]*\\)\"\\(\\s*->.*;\\)/\"\\1 (public)\"\\2/' \\\n  /tmp/jdeps330/public.dot > public.dot\nsed 's/\"\\([^(]*\\)\"\\(\\s*->.*;\\)/\"\\1 (spark-shared)\"\\2/' \\\n  /tmp/jdeps330/spark-shared.dot > spark-shared.dot\nsed 's/\"\\([^(]*\\)\"\\(\\s*->.*;\\)/\"\\1 (spark330)\"\\2/' \\\n  /tmp/jdeps330/spark330.dot > spark330.dot\n```\n\n----------------------------------------\n\nTITLE: Defining Hive Simple UDF in Spark RAPIDS\nDESCRIPTION: This snippet defines a Hive Simple UDF within the Spark RAPIDS framework. Similar to the Generic UDF, this UDF also has the option to implement a RAPIDS accelerated interface for improved performance.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.expression.HiveSimpleUDF\"></a>spark.rapids.sql.expression.HiveSimpleUDF| |Hive UDF, the UDF can choose to implement a RAPIDS accelerated interface to get better performance|true|None|\n```\n\n----------------------------------------\n\nTITLE: Displaying LORE Directory Structure in Console\nDESCRIPTION: This snippet shows the typical directory hierarchy created by LORE when dumping data. It illustrates the structure for multiple LORE IDs, including plan metadata, input directories, and partition data.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/lore.md#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n+ loreId-10/\n  - plan.meta\n  + input-0/\n    - rdd.meta\n    + partition-0/\n      - partition.meta\n      - batch-0.parquet\n      - batch-1.parquet\n    + partition-1/\n      - partition.meta\n      - batch-0.parquet\n  + input-1/\n    - rdd.meta\n    + partition-0/\n      - partition.meta\n      - batch-0.parquet\n      - batch-1.parquet\n \n+ loreId-15/\n  - plan.meta\n  + input-0/\n    - rdd.meta\n    + partition-0/\n      - partition.meta\n      - batch-0.parquet\n```\n\n----------------------------------------\n\nTITLE: Cloning an Existing Shim for a New Spark Version\nDESCRIPTION: Maven command to create a new shim (324) based on an existing one (323). This adds the new version to all shared files, copies dedicated files with appropriate package substitutions, and modifies metadata comments for the new version.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmvn generate-sources -Dshimplify=true \\\n    -Dshimplify.move=true -Dshimplify.overwrite=true \\\n    -Dshimplify.add.shim=324 -Dshimplify.add.base=323\n```\n\n----------------------------------------\n\nTITLE: Building Dist Module for Spark Version Analysis in Spark-RAPIDS\nDESCRIPTION: Command to build the dist module for the lowest and highest supported Spark versions for dependency analysis.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./build/buildall --parallel=4  --profile=320,351 --module=dist\n```\n\n----------------------------------------\n\nTITLE: GPG Signature Verification Output\nDESCRIPTION: This shows the expected output of a successful GPG signature verification for the RAPIDS Accelerator for Apache Spark package. It confirms that the signature is valid and was created by the NVIDIA Spark team.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_22\n\nLANGUAGE: text\nCODE:\n```\ngpg: Good signature from \"NVIDIA Spark (For the signature of spark-rapids release jars) <sw-spark@nvidia.com>\"\n```\n\n----------------------------------------\n\nTITLE: Slice Function\nDESCRIPTION: Subsets an array starting from a specified index (1-based) with given length. Supports negative indices to start from end. Special handling required for timestamps in UTC timezone.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_69\n\nLANGUAGE: SQL\nCODE:\n```\nslice(array, start, length)\n```\n\n----------------------------------------\n\nTITLE: JSON Input Scan in Spark RAPIDS\nDESCRIPTION: This snippet specifies the scanning mechanism for parsing JSON files in Spark RAPIDS, facilitating the handling of JSON data inputs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_53\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.input.JsonScan\"></a>spark.rapids.sql.input.JsonScan|Json parsing|true|None|\n```\n\n----------------------------------------\n\nTITLE: Profiling Spark Worker in Standalone Mode\nDESCRIPTION: Start a Spark worker in standalone mode, profile it and the shell, and stop the worker process when finished.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/nvtx_profiling.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnsys profile bash -c \" \\\nCUDA_VISIBLE_DEVICES=0 ${SPARK_HOME}/sbin/start-worker.sh $master_url & \\\n$SPARK_HOME/bin/spark-shell; \\\n${SPARK_HOME}/sbin/stop-worker.sh\"\n```\n\n----------------------------------------\n\nTITLE: Building Against Multiple Specific Spark Versions with Scala 2.13\nDESCRIPTION: Command using the buildall script to build the RAPIDS Accelerator against specific versions of Apache Spark (3.3.0 and 3.4.0) with Scala 2.13.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scala2.13/README.md#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./build/buildall --profile=330,340 --scala213\n```\n\n----------------------------------------\n\nTITLE: Loading Additional Libraries for CPU Version\nDESCRIPTION: Imports additional libraries specifically required for CPU version of XGBoost, including vector assembler, DataFrame, SQL functions, and type conversions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types.FloatType\n```\n\n----------------------------------------\n\nTITLE: Running Filtered Tests with Multiple Parameters\nDESCRIPTION: Complex command showing how to run specific test cases across multiple suites. This example runs decimal-related tests in AnsiCastOpSuite and CastOpSuite using Apache Spark 3.3.0 on Scala 2.13.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/tests/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmvn package -f scala2.13 -pl tests -am -Dbuildver=330 -Dsuffixes='.*CastOpSuite' -Dtests=decimal\n```\n\n----------------------------------------\n\nTITLE: Verifying RAPIDS Accelerator Jar Signature Using GPG\nDESCRIPTION: Commands to verify the signature of RAPIDS Accelerator JAR files. The process involves importing the NVIDIA Spark public key and verifying the signature for both Scala 2.12 and 2.13 JAR files.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-25.02.1.jar.asc rapids-4-spark_2.12-25.02.1.jar\ngpg --verify rapids-4-spark_2.13-25.02.1.jar.asc rapids-4-spark_2.13-25.02.1.jar\n```\n\n----------------------------------------\n\nTITLE: Verifying Spark-RAPIDS Jar Signature with GPG\nDESCRIPTION: Commands to verify the signature of the RAPIDS Accelerator for Apache Spark jar file using GPG. This involves importing the public key and verifying the signature against the downloaded jar file.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-22.08.0.jar.asc rapids-4-spark_2.12-22.08.0.jar\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Copyright Year Updates\nDESCRIPTION: Shell command to set an environment variable that enables automatic updating of copyright year headers in source files when committing changes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nexport SPARK_RAPIDS_AUTO_COPYRIGHTER=ON\n```\n\n----------------------------------------\n\nTITLE: Confirming RegexReplace Can Be Safely Externalized in Spark-RAPIDS\nDESCRIPTION: Command to verify that RegexReplace has no dependencies on spark330-specific code, showing it's self-contained and safe to externalize.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ dijkstra -d -p \"org.apache.spark.sql.rapids.RegexReplace (spark-shared)\"  merged.dot | grep '\\[dist='\n        \"org.apache.spark.sql.rapids.RegexReplace (spark-shared)\"    [dist=0.000];\n        \"org.apache.spark.sql.rapids.RegexReplace$ (spark-shared)\"   [dist=1.000,\n```\n\n----------------------------------------\n\nTITLE: Defining NVTX Ranges in Markdown Table\nDESCRIPTION: A markdown table listing NVTX ranges used in the RAPIDS Accelerator for Apache Spark plugin. It includes the range name and a brief description of its purpose.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/nvtx_ranges.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nName | Description\n-----|-------------\nRelease GPU|Releasing the GPU semaphore\nAcquire GPU|Time waiting for GPU semaphore to be acquired\n```\n\n----------------------------------------\n\nTITLE: Using buildall Script for Optimized Distribution Rebuilding\nDESCRIPTION: Alternative command using the buildall script to quickly rebuild only the distribution module with optimizations for development cycles.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n./build/buildall --rebuild-dist-only --option=\"-Ddist.jar.compress=false -Drapids.jni.unpack.skip\"\n```\n\n----------------------------------------\n\nTITLE: Limiting Concurrent Python Workers in Spark Executor\nDESCRIPTION: This configuration limits the total concurrent running Python processes for a Spark executor. The default is 0 (no limit). Setting this too low may cause job hangs in certain scenarios.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/rapids-udfs.md#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n--conf spark.rapids.python.concurrentPythonWorkers=2 \\\n```\n\n----------------------------------------\n\nTITLE: Creating a Commit List File for Spark Commit Auditing\nDESCRIPTION: Example format for creating a file that lists the Apache Spark commits to be audited. Each line contains a commit SHA-1 followed by a description of the merged PR.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scripts/README.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nccbd9a7b98  [SPARK-41778][SQL] Add an alias \"reduce\" to ArrayAggregate\n 838954e508  [SPARK-41554] fix changing of Decimal scale when scale decreased by m…\n a77ae27f15  [SPARK-41442][SQL][FOLLOWUP] SQLMetric should not expose -1 value as it's invalid\n```\n\n----------------------------------------\n\nTITLE: Passing Custom Spark Configurations to Tests\nDESCRIPTION: Example showing how to pass custom Apache Spark configurations to the tests using the SPARK_CONF environment variable. This sets Spark dynamic allocation to false and configures task CPUs.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/tests/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nSPARK_CONF=\"spark.dynamicAllocation.enabled=false,spark.task.cpus=1\" mvn ...\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for RAPIDS Accelerator JARs\nDESCRIPTION: Commands to verify the signature of the RAPIDS Accelerator JAR files using GPG. The process involves importing the public key and verifying the signature for both Scala 2.12 and 2.13 JAR files.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-24.06.0.jar.asc rapids-4-spark_2.12-24.06.0.jar\ngpg --verify rapids-4-spark_2.13-24.06.0.jar.asc rapids-4-spark_2.13-24.06.0.jar\n```\n\n----------------------------------------\n\nTITLE: Expected GPG Verification Output for RAPIDS Accelerator\nDESCRIPTION: The expected successful output message when verifying the signature of RAPIDS Accelerator JAR files, showing a valid signature from NVIDIA Spark.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ngpg: Good signature from \"NVIDIA Spark (For the signature of spark-rapids release jars) <sw-spark@nvidia.com>\"\n```\n\n----------------------------------------\n\nTITLE: Avro Input Scan in Spark RAPIDS\nDESCRIPTION: This snippet outlines the scanning mechanism for Avro files within the Spark RAPIDS framework, enabling effective handling of Avro data formats.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_56\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.input.AvroScan\"></a>spark.rapids.sql.input.AvroScan|Avro parsing|true|None|\n```\n\n----------------------------------------\n\nTITLE: Analyzing Strong Component Clusters in Spark-RAPIDS Dependencies\nDESCRIPTION: Command using sccmap to identify strongly connected components in the dependency graph, helping to estimate the scope of refactoring needed.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ sccmap -d -s merged.dot\n2440 nodes, 11897 edges, 637 strong components\n```\n\n----------------------------------------\n\nTITLE: Enabling DNS and GPU Support\nDESCRIPTION: Commands to enable DNS and GPU support in Microk8s cluster.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmicrok8s.enable dns\nmicrok8s.enable gpu\n```\n\n----------------------------------------\n\nTITLE: Configuring IntelliJ IDEA Code Style Settings for Java and Scala\nDESCRIPTION: XML configuration that defines code style settings for both Java and Scala in IntelliJ IDEA. Includes settings for import organization, class count thresholds, indentation rules, and specific formatting preferences for both languages. Sets up import ordering with specific rules for Spark-related imports.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/idea-code-style-settings.md#2025-04-19_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<code_scheme name=\"Default\" version=\"173\">\n  <option name=\"SOFT_MARGINS\" value=\"100\" />\n  <JavaCodeStyleSettings>\n    <option name=\"CLASS_COUNT_TO_USE_IMPORT_ON_DEMAND\" value=\"10\" />\n    <option name=\"NAMES_COUNT_TO_USE_IMPORT_ON_DEMAND\" value=\"10\" />\n  </JavaCodeStyleSettings>\n  <ScalaCodeStyleSettings>\n    <option name=\"classCountToUseImportOnDemand\" value=\"15\" />\n    <option name=\"importLayout\">\n      <array>\n        <option value=\"java\" />\n        <option value=\"_______ blank line _______\" />\n        <option value=\"scala\" />\n        <option value=\"_______ blank line _______\" />\n        <option value=\"all other imports\" />\n        <option value=\"_______ blank line _______\" />\n        <option value=\"org.apache.spark\" />\n      </array>\n    </option>\n    <option name=\"sortAsScalastyle\" value=\"true\" />\n    <option name=\"USE_ALTERNATE_CONTINUATION_INDENT_FOR_PARAMS\" value=\"true\" />\n  </ScalaCodeStyleSettings>\n  <codeStyleSettings language=\"JAVA\">\n    <option name=\"INDENT_CASE_FROM_SWITCH\" value=\"false\" />\n    <option name=\"ALIGN_MULTILINE_PARAMETERS\" value=\"false\" />\n    <indentOptions>\n      <option name=\"INDENT_SIZE\" value=\"2\" />\n      <option name=\"CONTINUATION_INDENT_SIZE\" value=\"4\" />\n      <option name=\"TAB_SIZE\" value=\"2\" />\n    </indentOptions>\n  </codeStyleSettings>\n  <codeStyleSettings language=\"Scala\">\n    <option name=\"ALIGN_MULTILINE_PARAMETERS\" value=\"false\" />\n    <indentOptions>\n      <option name=\"CONTINUATION_INDENT_SIZE\" value=\"4\" />\n    </indentOptions>\n  </codeStyleSettings>\n</code_scheme>\n```\n\n----------------------------------------\n\nTITLE: Moving Externalized Classes to Public Directory in Spark-RAPIDS\nDESCRIPTION: Command to move current externalized classes to a dedicated 'public' directory for dependency analysis.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmv org com ai public/\n```\n\n----------------------------------------\n\nTITLE: Adding Non-UTC Fallback for Unsupported Operators in Spark-RAPIDS Tests\nDESCRIPTION: Python code showing how to annotate test cases to allow operator fallback when non-UTC timezones are not supported. This ensures tests pass in non-UTC configurations by allowing specific operations to run on CPU.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nnon_utc_allow_for_sequence = ['ProjectExec'] # Update after non-utc time zone is supported for sequence\n@allow_non_gpu(*non_utc_allow_for_sequence)\ntest_my_new_added_case_for_sequence_operator()\n```\n\n----------------------------------------\n\nTITLE: Setting Global Python Version with Pyenv\nDESCRIPTION: Command to set a specific Python version as the globally active version using pyenv. This affects all shells unless overridden locally.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npyenv global 3.X.Y\n```\n\n----------------------------------------\n\nTITLE: Using Array Sum UDF in Spark SQL\nDESCRIPTION: Shows how to use the registered sum_array UDF in a Spark SQL query, demonstrating the execution with a sample array input.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/DF_UDF_README.md#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nSeq(Array(1L, 2L, 3L)).toDF(\"data\").selectExpr(\"sum_array(data) as result\").show()\n\n+------+\n|result|\n+------+\n|     6|\n+------+\n```\n\n----------------------------------------\n\nTITLE: Deleting a Spark Shim using Maven in Spark-Rapids\nDESCRIPTION: This Maven command removes a specific Spark build shim (e.g., 320) from the project. It removes the shim identifier from source files and deletes files exclusive to that shim.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmvn generate-sources -Dshimplify=true -Dshimplify.move=true \\\n    -Dshimplify.remove.shim=320\n```\n\n----------------------------------------\n\nTITLE: Generating Kubernetes Configuration\nDESCRIPTION: Commands to create .kube directory and generate new kubernetes configuration file.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ~/.kube\nmicrok8s config > ~/.kube/config\n```\n\n----------------------------------------\n\nTITLE: Running CPD Tool for Duplicate Code Detection in Spark-Rapids\nDESCRIPTION: This Maven command runs the CPD tool to detect duplicate code in Scala files. It sets minimum token count, specifies the language, and defines a pattern to skip certain blocks. The output is redirected to a file.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmvn antrun:run@duplicate-code-detector \\\n    -Dcpd.argLine='--minimum-tokens 50 --language scala --skip-blocks-pattern /*|*/' \\\n    -Dcpd.sourceType='main' \\\n    > target/cpd.scala.txt\n```\n\n----------------------------------------\n\nTITLE: Using last_day Function in Spark SQL\nDESCRIPTION: The last_day function returns the last day of the month to which a date belongs. It operates in a project context and supports date input types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_38\n\nLANGUAGE: SQL\nCODE:\n```\nlast_day(date_column)\n```\n\n----------------------------------------\n\nTITLE: Fixed-width Mortgage Data Records\nDESCRIPTION: Fixed-width formatted mortgage loan data records containing loan identifiers, lender names, interest rates, loan amounts, origination dates, and other loan parameters. Each line represents a single mortgage record with specific field positions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/Acquisition_2007Q3/Acquisition_2007Q3.hive.txt#2025-04-19_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n100257795048RFIRST TENNESSEE BANK NATIONAL ASSOCIATION6.87515200036008/200710/20077575220808NPPU1IAZ852FRM808N\n100260220653RBANK OF AMERICA, N.A.6.8753900036006/200708/20079090243772NPSF1IOH45225FRM7301N\n100263906177BBISHOPS GATE RESIDENTIAL MORTGAGE TRUST633600036006/200708/20078080232687NPSF1PCA923FRM661N\n```\n\n----------------------------------------\n\nTITLE: Array Data Structure Example\nDESCRIPTION: Example showing nested array data structure with null values and empty rows for testing array type operations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n[\n    [0,1,2,3],\n    [], \n    [4,5,6,7],\n    ...\n]\n```\n\n----------------------------------------\n\nTITLE: Importing DataGen Classes in Spark\nDESCRIPTION: Imports all classes from the datagen package to make them available in the Spark session.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.tests.datagen._\n```\n\n----------------------------------------\n\nTITLE: Logical OR Operation\nDESCRIPTION: Implementation of logical OR operator, supporting boolean input and output types only.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_57\n\nLANGUAGE: SQL\nCODE:\n```\nor\n```\n\n----------------------------------------\n\nTITLE: Restoring Files After Failed Shimplify Comment Generation\nDESCRIPTION: Git command to restore files if shimplify comment generation results in undesired changes. This provides an easy way to revert changes during the conversion process.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit restore sql-plugin tests\n```\n\n----------------------------------------\n\nTITLE: Memory Leak Error Messages in Spark-RAPIDS\nDESCRIPTION: Example error messages that indicate memory leaks in DeviceMemoryBuffer and HostMemoryBuffer. These messages include buffer ID and memory address information to help identify leaked resources.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/mem_debug.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nERROR DeviceMemoryBuffer: A DEVICE BUFFER WAS LEAKED (ID: 5 30a000200)\nERROR HostMemoryBuffer: A HOST BUFFER WAS LEAKED (ID: 6 7fb62b07f570)\n```\n\n----------------------------------------\n\nTITLE: Maven Dependency for Rapids Hybrid Jar\nDESCRIPTION: This XML snippet specifies the Maven dependency for the Rapids Hybrid jar. It includes the groupId, artifactId, and version placeholders for the specific Scala and project versions required.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/hybrid-execution.md#2025-04-19_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n   <groupId>com.nvidia</groupId>\n   <artifactId>rapids-4-spark-hybrid_${scala.binary.version}</artifactId>\n   <version>${project.version}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Initializing Jupyter Notebook for GPU-Accelerated Spark\nDESCRIPTION: Command to start a Jupyter notebook server that is accessible from remote machines, running on port 8124 without automatically opening a browser window.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/Mortgage-ETL.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ jupyter notebook --ip=0.0.0.0 --port=8124 --no-browser\n```\n\n----------------------------------------\n\nTITLE: Expected GPG Verification Output for RAPIDS Jar\nDESCRIPTION: The expected output when successfully verifying the signature of a RAPIDS Accelerator for Apache Spark jar file, confirming it was signed by NVIDIA Spark.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ngpg: Good signature from \"NVIDIA Spark (For the signature of spark-rapids release jars) <sw-spark@nvidia.com>\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Table with DataGen\nDESCRIPTION: Creates a simple table with string and byte columns, generating 5 rows of data with random values.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string, b byte\", 5)\ndataTable.toDF(spark).show()\n```\n\n----------------------------------------\n\nTITLE: Octet Length Function\nDESCRIPTION: Function to calculate byte length of string data, supporting only string input type and returning integer result.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_56\n\nLANGUAGE: SQL\nCODE:\n```\noctet_length\n```\n\n----------------------------------------\n\nTITLE: Installing Microk8s 1.20 Stable Version\nDESCRIPTION: Command to install Microk8s version 1.20 stable using snap package manager. This specific version is recommended due to GPU support compatibility with RAPIDS Accelerator.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo snap install microk8s --classic --channel=1.20/stable\n```\n\n----------------------------------------\n\nTITLE: ColumnVector Memory Leak Error Messages in Spark-RAPIDS\nDESCRIPTION: Error messages showing ColumnVector and HostColumnVector memory leaks. These messages differ slightly from buffer leaks but still provide ID information for cross-referencing leaked resources.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/mem_debug.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nERROR ColumnVector: A DEVICE COLUMN VECTOR WAS LEAKED (ID: 15 7fb5f94d8fa0)\nERROR HostColumnVector: A HOST COLUMN VECTOR WAS LEAKED (ID: 19)\n```\n\n----------------------------------------\n\nTITLE: Managing Docker Images for Microk8s\nDESCRIPTION: Commands to export Docker images and import them into Microk8s cluster.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker save spark-rapids > /tmp/spark-rapids.tar\nmicrok8s ctr image import /tmp/spark-rapids.tar\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Local Limit in Spark RAPIDS\nDESCRIPTION: This snippet describes the implementation for per-partition limiting of results in Spark RAPIDS, allowing controlled data output from each partition.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.LocalLimitExec\"></a>spark.rapids.sql.exec.LocalLimitExec|Per-partition limiting of results|true|None|\n```\n\n----------------------------------------\n\nTITLE: Expected GPG Verification Output for v24.10.1\nDESCRIPTION: The expected success message when verifying the signature of RAPIDS Accelerator v24.10.1 JAR files, confirming a good signature from NVIDIA Spark.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ngpg: Good signature from \"NVIDIA Spark (For the signature of spark-rapids release jars) <sw-spark@nvidia.com>\"\n```\n\n----------------------------------------\n\nTITLE: Using Distinct Distribution for Unique Keys\nDESCRIPTION: Shows how to use DistinctDistribution to generate non-sequential unique values, useful for primary keys in fact tables.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a byte\", 10)\ndataTable(\"a\").setSeedMapping(DistinctDistribution())\ndataTable.toDF(spark).show\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Global Limit in Spark RAPIDS\nDESCRIPTION: This snippet defines the functionality for limiting results across partitions in Spark RAPIDS, facilitating data management.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.GlobalLimitExec\"></a>spark.rapids.sql.exec.GlobalLimitExec|Limiting of results across partitions|true|None|\n```\n\n----------------------------------------\n\nTITLE: SQL Query Example with Constant Folding\nDESCRIPTION: Example SQL query demonstrating constant folding behavior in Spark, where literal expressions are evaluated during logical planning phase before Rapids Accelerator is invoked.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nSELECT SUM(1+2+3) FROM ...\n```\n\n----------------------------------------\n\nTITLE: Merging DOT Files for Comprehensive Dependency Analysis in Spark-RAPIDS\nDESCRIPTION: Command to combine multiple DOT files into a single graph for cross-archive path analysis, enabling identification of dependencies between different components.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncat public.dot spark-shared.dot spark330.dot | \\\n  tr '\\n' '\\r' | \\\n  sed 's/}\\rdigraph \"[^\"]*\" {\\r//g' | \\\n  tr '\\r' '\\n' > merged.dot\n```\n\n----------------------------------------\n\nTITLE: Jekyll Front Matter Configuration for Additional Functionality Documentation Page\nDESCRIPTION: This Jekyll front matter block configures a documentation page titled 'Additional Functionality' with navigation order 10. It specifies that the page has child pages and sets the permalink to '/additional-functionality/'.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: page\ntitle: Additional Functionality\nnav_order: 10\nhas_children: true\npermalink: /additional-functionality/\n---\n```\n\n----------------------------------------\n\nTITLE: Double Precision Scientific Notation Constants\nDESCRIPTION: A set of floating point constants in scientific notation format representing very small numbers. These demonstrate different ways of expressing scientific notation including both uppercase 'E' and lowercase 'e' notation.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/extended-float-values/ext_float_values.hive.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n1.7976931348623157E-308\n1.7976931348623157e-100\n1.2e-234\n1111111111111111111111111111111111111111E-39\n```\n\n----------------------------------------\n\nTITLE: Verifying Jar Signature with GPG\nDESCRIPTION: This snippet demonstrates how to verify the signature of a RAPIDS Accelerator for Apache Spark JAR file using GPG (GNU Privacy Guard). It involves using the `gpg --verify` command to check the signature against the downloaded JAR and ASC files, ensuring the integrity and authenticity of the downloaded package.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n\"gpg --verify rapids-4-spark_2.13-24.02.0.jar.asc rapids-4-spark_2.13-24.02.0.jar\"\n```\n\n----------------------------------------\n\nTITLE: Running OpcodeSuite Tests for RAPIDS UDF Compiler\nDESCRIPTION: Maven command to execute the OpcodeSuite test suite from the rapids-plugin-4-spark root directory. This command specifically targets the com.nvidia.spark.OpcodeSuite test class.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/udf-compiler/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmvn test -DwildcardSuites=com.nvidia.spark.OpcodeSuite\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Take Ordered and Project in Spark RAPIDS\nDESCRIPTION: This snippet outlines the steps taken to return the first limit elements as defined by sorting criteria, along with projection if necessary, within Spark RAPIDS.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_22\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.TakeOrderedAndProjectExec\"></a>spark.rapids.sql.exec.TakeOrderedAndProjectExec|Take the first limit elements as defined by the sortOrder, and do projection if needed|true|None|\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Python Version with Pyenv\nDESCRIPTION: Command to install a specific Python version using pyenv, where X.Y represents the desired version numbers (e.g., 3.8.0).\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npyenv install 3.X.Y\n```\n\n----------------------------------------\n\nTITLE: Analyzing GpuTypeColumnVector Dependencies in Spark-RAPIDS\nDESCRIPTION: Command using dijkstra to find paths from GpuColumnVector to spark330-specific nodes, showing that refactoring is needed before externalization.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shims.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ dijkstra -d -p \"com.nvidia.spark.rapids.GpuColumnVector (spark-shared)\" merged.dot | \\\n  grep '\\[dist=' | grep '(spark330)'\n        \"org.apache.spark.sql.rapids.GpuFileSourceScanExec (spark330)\"  [dist=5.000,\n        \"com.nvidia.spark.rapids.GpuExec (spark330)\"    [dist=3.000,\n...\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Commit Hook with Missing Config\nDESCRIPTION: Command to install pre-commit hooks with the --allow-missing-config option, which makes it easier to work with older branches that do not have a pre-commit configuration file.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npre-commit install --allow-missing-config\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Collect Limit in Spark RAPIDS\nDESCRIPTION: This snippet outlines the Collect Limit execution step for Spark RAPIDS, which reduces to a single partition and applies a limit, though it can be slower in some scenarios when handling a large number of rows.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.CollectLimitExec\"></a>spark.rapids.sql.exec.CollectLimitExec|Reduce to single partition and apply limit|false|This is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU|\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for RAPIDS Accelerator for Apache Spark\nDESCRIPTION: Commands to verify the authenticity of the downloaded RAPIDS Accelerator for Apache Spark jar file using GPG signature verification. This process includes importing the public key and verifying the jar file against its signature.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-23.08.1.jar.asc rapids-4-spark_2.12-23.08.1.jar\n```\n\n----------------------------------------\n\nTITLE: Submitting Spark RAPIDS Scale Test Application (Bash)\nDESCRIPTION: Command-line usage instructions for the ScaleTest application, detailing the required and optional arguments for configuring the test run.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/ScaleTest.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nUsage: ScaleTest [options] <scale factor> <complexity> <format> <input directory> <output directory> <path to save report file>\n\n  <scale factor>           scale factor for data size\n  <complexity>             complexity level for processing\n  <format>                 output format for the data\n  <input directory>        input directory for table data\n  <output directory>       directory for query output\n  <path to save report file>\n                           path to save the report file that contains test results\n  -d, --seed <value>       seed used to generate random data columns. default is 41 if not specified\n  --overwrite              Flag argument. Whether to overwrite the existing data in the path.\n  --iterations <value>     iterations to run for each query. default: 1\n  --queries <value>        Specify queries to run specifically. the format must be query names with comma separated. e.g. --tables q1,q2,q3. If not specified, all queries will be run for `--iterations` rounds\n  --timeout <value>        timeout for each query in milliseconds, default is 10 minutes(600000)\n  --dry                    Flag argument. Only print the queries but not execute them\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Union in Spark RAPIDS\nDESCRIPTION: This snippet specifies the backend functionality for the union operator in Spark RAPIDS, allowing for the combination of multiple datasets.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_23\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.UnionExec\"></a>spark.rapids.sql.exec.UnionExec|The backend for the union operator|true|None|\n```\n\n----------------------------------------\n\nTITLE: Building Scale Test Suite with Maven\nDESCRIPTION: Basic Maven command to build the Scale Test suite within the integration_tests module\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/ScaleTest.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn package\n```\n\n----------------------------------------\n\nTITLE: Verifying Bloop and Metals Processes with JPS Command\nDESCRIPTION: A shell command to verify that Bloop build server and Metals language server are running by listing Java processes that include their main classes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\njps -l\n72960 sun.tools.jps.Jps\n72356 bloop.Server\n72349 scala.meta.metals.Main\n```\n\n----------------------------------------\n\nTITLE: HTML Comment for Generated Content Warning\nDESCRIPTION: HTML comment indicating the content is automatically generated by RapidsConf.help and should not be manually edited.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!-- Generated by RapidsConf.help. DO NOT EDIT! -->\n```\n\n----------------------------------------\n\nTITLE: Managing Kubernetes Secrets\nDESCRIPTION: Commands to view and manage Kubernetes secrets for authentication.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/microk8s.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmicrok8s.kubectl -n kube-system get secrets\nmicrok8s.kubectl -n kube-system describe secret default-token-5bmbh\nexport K8S_TOKEN=<value from above>\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Cartesian Product Join in Spark RAPIDS\nDESCRIPTION: This snippet defines the implementation for performing joins using brute force methods within the Spark RAPIDS framework, enabling Cartesian product functionalities.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_40\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.CartesianProductExec\"></a>spark.rapids.sql.exec.CartesianProductExec|Implementation of join using brute force|true|None|\n```\n\n----------------------------------------\n\nTITLE: Installing GNU sed on macOS for Pre-commit Hooks\nDESCRIPTION: Commands to install GNU sed on macOS and update the PATH environment variable to use it as the default sed, which is needed to avoid issues with the copyright update script.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nbrew install gnu-sed\n# and add to PATH to make it as default sed for your shell\nexport PATH=\"/usr/local/opt/gnu-sed/libexec/gnubin:$PATH\"\n```\n\n----------------------------------------\n\nTITLE: Fixed-width Financial Transaction Records in Plaintext Format\nDESCRIPTION: A dataset of fixed-width formatted financial records containing loan or mortgage transaction data. Each record includes a unique identifier, transaction date, interest rate, principal amount, and various other financial indicators. The data spans multiple years and appears to track loan amortization schedules.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/Performance_2007Q3/Performance_2007Q3.hive.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n10004072919104/01/20106.875194517.1830.0330.0330.010/203746060.00NN\n10004072919105/01/20106.875194317.7431.0329.0329.010/203746060.00NN\n10004072919106/01/20106.875194117.1632.0328.0328.010/203746060.00NN\n10004072919107/01/20106.875193915.4333.0327.0327.010/203746060.00NN\n10004072919108/01/20106.875193712.5434.0326.0326.010/203746060.00NN\n10004072919109/01/20106.875193508.4935.0325.0325.010/203746060.00NN\n10004072919110/01/20106.875193303.2736.0324.0324.010/203746060.00NN\n10004072919111/01/20106.875193096.8837.0323.0323.010/203746060.00NN\n10004072919112/01/20106.875192889.338.0322.0322.010/203746060.00NN\n10004072919101/01/20116.875192680.5339.0321.0321.010/203746060.00NN\n10004072919102/01/20116.875192470.5740.0320.0320.010/203746060.00NN\n10004072919103/01/20116.875192259.4141.0319.0319.010/203746060.00NN\n10004072919104/01/20116.875192047.0442.0318.0318.010/203746060.00NN\n10004072919105/01/20116.875191833.4543.0317.0317.010/203746060.00NN\n10004072919106/01/20116.875191618.6444.0316.0316.010/203746060.00NN\n10004072919107/01/20116.875191402.645.0315.0315.010/203746060.00NN\n10004072919108/01/20116.875191185.3246.0314.0314.010/203746060.00NN\n10004072919109/01/20116.875190966.7947.0313.0313.010/203746060.00NN\n10004072919110/01/20116.875190747.0148.0312.0312.010/203746060.00NN\n10004072919111/01/20116.875190525.9749.0311.0311.010/203746060.00NN\n10004072919112/01/20116.875190303.6750.0310.0310.010/203746060.00NN\n10004072919101/01/20126.875190080.0951.0309.0309.010/203746060.00NN\n10004072919102/01/20126.875190080.0952.0308.00.010/203746060.0-2N0102/2012N\n10004500302109/01/2007BANK OF AMERICA, N.A.6.52.0358.0357.007/203726420.00N\n10004500302110/01/20076.53.0357.0356.007/203726420.00N\n10004500302111/01/20076.54.0356.0355.007/203726420.00N\n10004500302112/01/20076.55.0355.0355.007/203726420.00N\n10004500302101/01/20086.56.0354.0354.007/203726420.00N\n10004500302102/01/20086.57.0353.0353.007/203726420.00NN\n10004500302103/01/20086.5206125.48.0352.0352.007/203726420.00NN\n10004500302104/01/20086.5205929.389.0351.0351.007/203726420.00NN\n10004500302105/01/20086.5205732.310.0350.0350.007/203726420.00NN\n10004500302106/01/20086.5205534.1511.0349.0349.007/203726420.00NN\n10004500302107/01/20086.5205334.9312.0348.0348.007/203726420.00NN\n10004500302108/01/20086.5205134.6313.0347.0347.007/203726420.00NN\n10004500302109/01/20086.5204933.2514.0346.0346.007/203726420.00NN\n10004500302110/01/20086.5204730.7815.0345.0345.007/203726420.00NN\n10004500302111/01/20086.5204527.2116.0344.0344.007/203726420.00NN\n10004500302112/01/20086.5204322.5417.0343.0343.007/203726420.00NN\n10004500302101/01/20096.5204116.7618.0342.0342.007/203726420.00NN\n10004500302102/01/20096.5203909.8619.0341.0341.007/203726420.00NN\n10004500302103/01/20096.5203701.8420.0340.0340.007/203726420.00NN\n10004500302104/01/20096.5203492.6921.0339.0339.007/203726420.00NN\n10004500302105/01/20096.5203282.4122.0338.0338.007/203726420.00NN\n10004500302106/01/20096.5203070.9923.0337.0337.007/203726420.00NN\n10004500302107/01/20096.5202858.4324.0336.0336.007/203726420.00NN\n10004500302108/01/20096.5202644.7225.0335.0335.007/203726420.00NN\n10004500302109/01/20096.5202429.8526.0334.0334.007/203726420.00NN\n10004500302110/01/20096.5202213.8227.0333.0333.007/203726420.00NN\n10004500302111/01/20096.5201996.6128.0332.0332.007/203726420.00NN\n10004500302112/01/20096.5201778.2329.0331.0331.007/203726420.00NN\n10004500302101/01/20106.5201558.6730.0330.0330.007/203726420.00NN\n10004500302102/01/20106.5201337.9231.0329.0329.007/203726420.00NN\n10004500302103/01/20106.5201115.9732.0328.0328.007/203726420.00NN\n10004500302104/01/20106.5200892.8233.0327.0327.007/203726420.00NN\n10004500302105/01/20106.5200668.4634.0326.0326.007/203726420.00NN\n10004500302106/01/20106.5200442.8835.0325.0325.007/203726420.00NN\n10004500302107/01/20106.5200216.0836.0324.0324.007/203726420.00NN\n10004500302108/01/20106.5199988.0537.0323.0323.007/203726420.00NN\n10004500302109/01/20106.5199758.7938.0322.0322.007/203726420.00NN\n10004500302110/01/20106.5199528.2939.0321.0321.007/203726420.00NN\n10004500302111/01/20106.5199296.5440.0320.0320.007/203726420.00NN\n10004500302112/01/20106.5199063.5341.0319.0319.007/203726420.00NN\n10004500302101/01/20116.5199063.5342.0318.00.007/203726420.0-2N0101/2011N\n10004519801207/01/2007BANK OF AMERICA, N.A.6.51.0359.0359.006/203741700.00N\n10004519801208/01/20076.52.0358.0358.006/203741700.00N\n10004519801209/01/20076.53.0357.0355.006/203741700.00N\n10004519801210/01/20076.54.0356.0352.006/203741700.00N\n10004519801211/01/20076.55.0355.0349.006/203741700.00N\n10004519801212/01/20076.56.0354.0346.006/203741700.00N\n10004519801201/01/20086.552153.637.0353.0343.006/203741700.00N\n10004519801202/01/20086.552001.138.0352.0340.006/203741700.00NN\n10004519801203/01/20086.551847.89.0351.0338.006/203741700.00NN\n10004519801204/01/20086.551693.6410.0350.0335.006/203741700.00NN\n10004519801205/01/20086.551538.6511.0349.0332.006/203741700.00NN\n10004519801206/01/20086.551382.8212.0348.0329.006/203741700.00NN\n10004519801207/01/20086.551226.1413.0347.0326.006/203741700.00NN\n10004519801208/01/20086.551068.6114.0346.0324.006/203741700.00NN\n10004519801209/01/20086.550910.2315.0345.0321.006/203741700.00NN\n10004519801210/01/20086.550750.9916.0344.0318.006/203741700.00NN\n10004519801211/01/20086.550590.8917.0343.0316.006/203741700.00NN\n10004519801212/01/20086.550429.9218.0342.0313.006/203741700.00NN\n10004519801201/01/20096.550268.0819.0341.0311.006/203741700.00NN\n10004519801202/01/20096.550105.3720.0340.0308.006/203741700.00NN\n10004519801203/01/20096.549941.7721.0339.0306.006/203741700.00NN\n10004519801204/01/20096.549777.2922.0338.0303.006/203741700.00NN\n10004519801205/01/20096.549611.9223.0337.0300.006/203741700.00NN\n10004519801206/01/20096.549445.6524.0336.0298.006/203741700.00NN\n10004519801207/01/20096.549278.4825.0335.0295.006/203741700.00NN\n10004519801208/01/20096.549110.4126.0334.0293.006/203741700.00NN\n10004519801209/01/20096.548941.4227.0333.0291.006/203741700.00NN\n10004519801210/01/20096.548771.5228.0332.0288.006/203741700.00NN\n10004519801211/01/20096.548600.729.0331.0286.006/203741700.00NN\n10004519801212/01/20096.548428.9530.0330.0283.006/203741700.00NN\n10004519801201/01/20106.548256.2731.0329.0281.006/203741700.00NN\n10004519801202/01/20106.548082.6632.0328.0279.006/203741700.00NN\n10004519801203/01/20106.547908.1133.0327.0276.006/203741700.00NN\n10004519801204/01/20106.547732.6134.0326.0274.006/203741700.00NN\n10004519801205/01/20106.547556.1635.0325.0272.006/203741700.00NN\n10004519801206/01/20106.547378.7636.0324.0269.006/203741700.00NN\n10004519801207/01/20106.547200.3937.0323.0267.006/203741700.00NN\n10004519801208/01/20106.547021.0638.0322.0265.006/203741700.00NN\n10004519801209/01/20106.546840.7639.0321.0263.006/203741700.00NN\n10004519801210/01/20106.546659.4840.0320.0260.006/203741700.00NN\n```\n\n----------------------------------------\n\nTITLE: Defining Entity Name Mappings in Python\nDESCRIPTION: This snippet provides mappings from original mortgage company names to standardized identifiers. It is used for cleaning and standardizing data in the mortgage processing workflow. The tuples represent key-value pairs where the first element is the original entity name and the second is its corresponding standardized name.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/AWS-EMR/Mortgage-ETL-GPU-EMR.ipynb#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n[\n        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERFIRST MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n        (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERBANK MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n        (\"CHASE HOME FINANCE, LLC\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE (CIE 1)\" , \"JP Morgan Chase\"),\n        (\"CHASE HOME FINANCE\" , \"JP Morgan Chase\"),\n        (\"CASHCALL, INC.\" , \"CashCall\"),\n        (\"CAPITAL ONE, NATIONAL ASSOCIATION\" , \"Capital One\"),\n        (\"CALIBER HOME LOANS, INC.\" , \"Caliber Funding\"),\n        (\"BISHOPS GATE RESIDENTIAL MORTGAGE TRUST\" , \"Bishops Gate Mortgage\"),\n        (\"BANK OF AMERICA, N.A.\" , \"Bank of America\"),\n        (\"AMTRUST BANK\" , \"AmTrust\"),\n        (\"AMERISAVE MORTGAGE CORPORATION\" , \"Amerisave\"),\n        (\"AMERIHOME MORTGAGE COMPANY, LLC\" , \"AmeriHome Mortgage\"),\n        (\"ALLY BANK\" , \"Ally Bank\"),\n        (\"ACADEMY MORTGAGE CORPORATION\" , \"Academy Mortgage\"),\n        (\"NO CASH-OUT REFINANCE\" , \"OTHER REFINANCE\"),\n        (\"REFINANCE - NOT SPECIFIED\" , \"OTHER REFINANCE\"),\n        (\"Other REFINANCE\" , \"OTHER REFINANCE\")\n]\n```\n\n----------------------------------------\n\nTITLE: Execution Step for Arrow Eval in Pandas in Spark RAPIDS\nDESCRIPTION: This snippet details the backend implementation for Scalar Pandas UDFs, optimizing data transfer between processes and GPU resource scheduling.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_44\n\nLANGUAGE: scala\nCODE:\n```\n<a name=\"sql.exec.ArrowEvalPythonExec\"></a>spark.rapids.sql.exec.ArrowEvalPythonExec|The backend of the Scalar Pandas UDFs. Accelerates the data transfer between the Java process and the Python process. It also supports scheduling GPU resources for the Python process when enabled|true|None|\n```\n\n----------------------------------------\n\nTITLE: Markdown Page Front Matter Configuration\nDESCRIPTION: Jekyll front matter configuration block that defines the page metadata including title, navigation order, and parent page relationship.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/testing.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: page\ntitle: Testing\nnav_order: 2\nparent: Developer Overview\n---\n```\n\n----------------------------------------\n\nTITLE: Setting Null Probability for Columns\nDESCRIPTION: Demonstrates how to control the percentage of null values in a specific column by setting the null probability to 0.5 (50%).\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string, b byte\", 5)\ndataTable(\"a\").setNullProbability(0.5)\ndataTable.toDF(spark).show()\n```\n\n----------------------------------------\n\nTITLE: File Path Configuration for Service Providers\nDESCRIPTION: Configuration file paths used for different Spark version service providers in the distribution jar. These files contain version-specific implementations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/dist/README.md#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncom.nvidia.spark.rapids.SparkShimServiceProvider.sparkNonSnapshot\ncom.nvidia.spark.rapids.SparkShimServiceProvider.sparkSnapshot\ncom.nvidia.spark.rapids.SparkShimServiceProvider.sparkNonSnapshotDB\ncom.nvidia.spark.rapids.SparkShimServiceProvider.sparkSnapshotDB\n```\n\n----------------------------------------\n\nTITLE: SQL query with range window function on byte-cast column\nDESCRIPTION: SQL statement that demonstrates a range window function with PRECEDING and FOLLOWING clauses that can cause different results between CPU and GPU due to byte range overflow handling.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n COUNT(dollars) over\n    (PARTITION BY id\n    ORDER BY CAST (dollars AS Byte) ASC\n    RANGE BETWEEN 127 PRECEDING AND 127 FOLLOWING)\nFROM table\n```\n\n----------------------------------------\n\nTITLE: Controlling Unique Value Count with Seed Range\nDESCRIPTION: Shows how to limit the number of unique values in a column by setting an inclusive seed range from 0 to 100.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\ndataTable(\"a\").setSeedRange(0, 100)\ndataTable.toDF(spark).selectExpr(\"COUNT(DISTINCT a)\", \"COUNT(b)\").show\n```\n\n----------------------------------------\n\nTITLE: Distribution Configuration Files\nDESCRIPTION: Text files controlling which classes are included in the base jar without shading. Defines common and version-specific file inclusions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/dist/README.md#2025-04-19_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nunshimmed-common-from-spark320.txt\nunshimmed-from-each-spark3xx.txt\n```\n\n----------------------------------------\n\nTITLE: Controlling Value Distribution with Flat Distribution\nDESCRIPTION: Demonstrates the default flat distribution where every seed has approximately the same probability of being generated.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nval dataTable = DBGen().addTable(\"data\", \"a string\", 10000000)\ndataTable(\"a\").setSeedRange(0, 5)\ndataTable.toDF(spark).groupBy(\"a\").count.show\n```\n\n----------------------------------------\n\nTITLE: Spark Python Worker Factory Patch\nDESCRIPTION: Patch to modify Spark's PythonWorkerFactory to remove LD_PRELOAD when launching Python processes, preventing interference with Python memory management.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/dev/host_memory_leaks/README.md#2025-04-19_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\ndiff --git a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala\nindex 7b2c36bb10..cef095d716 100644\n--- a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala\n+++ b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala\n@@ -158,6 +158,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String\n       val pb = new ProcessBuilder(Arrays.asList(pythonExec, \"-m\", workerModule))\n       val workerEnv = pb.environment()\n       workerEnv.putAll(envVars.asJava)\n+      workerEnv.remove(\"LD_PRELOAD\")\n       workerEnv.put(\"PYTHONPATH\", pythonPath)\n       // This is equivalent to setting the -u flag; we use it because ipython doesn't support -u:\n       workerEnv.put(\"PYTHONUNBUFFERED\", \"YES\")\n@@ -208,6 +209,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String\n         val pb = new ProcessBuilder(command)\n         val workerEnv = pb.environment()\n         workerEnv.putAll(envVars.asJava)\n+        workerEnv.remove(\"LD_PRELOAD\")\n         workerEnv.put(\"PYTHONPATH\", pythonPath)\n         workerEnv.put(\"PYTHON_WORKER_FACTORY_SECRET\", authHelper.secret)\n         // This is equivalent to setting the -u flag; we use it because ipython doesn't support -u:\n```\n\n----------------------------------------\n\nTITLE: Comparing CPU vs GPU results for byte range window function\nDESCRIPTION: Console output showing the different results between CPU and GPU implementations of the range window function with byte casting, demonstrating the overflow handling differences.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nCPU: WrappedArray([0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0])\nGPU: WrappedArray([0], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19], [19])\n```\n\n----------------------------------------\n\nTITLE: Boolean Test Data Values in Plain Text\nDESCRIPTION: A comprehensive list of boolean value representations that might be encountered when parsing data. Includes standard boolean literals in various cases, numeric values, text abbreviations, and whitespace variations.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/simple-boolean-values/bools.hive.txt#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"booleans\"\ntrue\nfalse\n\\N\nTrue\nFalse\nTRUE\nFALSE\nBAD\ny\nn\nyes\nno\n1\n0\nt\nf\n true\ntrue \n false\nfalse \n 1\n1 \n 0\n0 \n```\n\n----------------------------------------\n\nTITLE: Defining Page Layout and Navigation in Markdown\nDESCRIPTION: This snippet sets up the page layout, title, and navigation order using YAML front matter in Markdown. It's used to configure the page within a static site generator or documentation system.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/security.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: page\ntitle: Security\nnav_order: 13\n---\n```\n\n----------------------------------------\n\nTITLE: Running Hybrid Execution Tests in Spark-RAPIDS\nDESCRIPTION: Shell command for running hybrid execution tests which require Gluten bundle jar, Gluten thirdparty jar, and a Hybrid jar. These tests validate the hybrid CPU/GPU execution modes.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/README.md#2025-04-19_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n$ LOAD_HYBRID_BACKEND=1 \\\n  HYBRID_BACKEND_JARS=/path/to/${GLUTEN_BUNDLE_JAR},/path/to/${GLUTEN_THIRD_PARTY_JAR},/path/to/HYBRID_JAR \\\n  ./integration_tests/run_pyspark_from_build.sh -m hybrid_test\n```\n\n----------------------------------------\n\nTITLE: Sample Output of API Validation Script\nDESCRIPTION: Example output from the API validation script showing parameter type mismatches and length differences between standard Spark Execs and their GPU counterparts. The output is stored in api_validation/target/api-validation-result.log.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/api_validation/README.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n************************************************************************************************\nTypes differ for below parameters in this Exec\nSparkExec  - [org.apache.spark.sql.execution.aggregate.HashAggregateExec]\nGpuExec - [com.nvidia.spark.rapids.GpuHashAggregateExec]\nSpark parameters                                                    Plugin parameters\n------------------------------------------------------------------------------------------------\nSeq[org.apache.spark.sql.catalyst.expressions.Attribute]          | Seq[com.nvidia.spark.rapids.GpuAttributeReference]\n\n************************************************************************************************\nTypes differ for below parameters in this Exec\nSparkExec  - [org.apache.spark.sql.execution.GenerateExec]\nGpuExec - [com.nvidia.spark.rapids.GpuGenerateExec]\nSpark parameters                                                    Plugin parameters\n------------------------------------------------------------------------------------------------\norg.apache.spark.sql.catalyst.expressions.Generator               | Boolean\nSeq[org.apache.spark.sql.catalyst.expressions.Attribute]          | Seq[com.nvidia.spark.rapids.GpuExpression]\nBoolean                                                           | Seq[org.apache.spark.sql.catalyst.expressions.Attribute]\n\n************************************************************************************************\nTypes differ for below parameters in this Exec\nSparkExec  - [org.apache.spark.sql.execution.ExpandExec]\nGpuExec - [com.nvidia.spark.rapids.GpuExpandExec]\nSpark parameters                                                    Plugin parameters\n------------------------------------------------------------------------------------------------\nSeq[org.apache.spark.sql.catalyst.expressions.Attribute]          | Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression]\n\n************************************************************************************************\nTypes differ for below parameters in this Exec\nSparkExec  - [org.apache.spark.sql.execution.joins.SortMergeJoinExec]\nGpuExec - [com.nvidia.spark.rapids.GpuShuffledHashJoinExec]\nSpark parameters                                                    Plugin parameters\n------------------------------------------------------------------------------------------------\nOption[org.apache.spark.sql.catalyst.expressions.Expression]      | org.apache.spark.sql.execution.joins.BuildSide\norg.apache.spark.sql.execution.SparkPlan                          | Option[com.nvidia.spark.rapids.GpuExpression]\nBoolean                                                           | org.apache.spark.sql.execution.SparkPlan\n\n\n\n************************************************************************************************\nParameter lengths don't match between Execs\nSparkExec - [org.apache.spark.sql.execution.CollectLimitExec]\nGpuExec - [com.nvidia.spark.rapids.GpuCollectLimitExec]\nSpark code has 2 parameters where as plugin code has 3 parameters\nSpark parameters                                                    Plugin parameters\n------------------------------------------------------------------------------------------------\nInt                                                               | Int\norg.apache.spark.sql.execution.SparkPlan                          | com.nvidia.spark.rapids.GpuPartitioning\n                                                                  | org.apache.spark.sql.execution.SparkPlan\n\n************************************************************************************************\nParameter lengths don't match between Execs\nSparkExec - [org.apache.spark.sql.execution.SortExec]\nGpuExec - [com.nvidia.spark.rapids.GpuSortExec]\nSpark code has 4 parameters where as plugin code has 5 parameters\nSpark parameters                                                    Plugin parameters\n------------------------------------------------------------------------------------------------\nSeq[org.apache.spark.sql.catalyst.expressions.SortOrder]          | Seq[com.nvidia.spark.rapids.GpuSortOrder]\nBoolean                                                           | Boolean\norg.apache.spark.sql.execution.SparkPlan                          | org.apache.spark.sql.execution.SparkPlan\nInt                                                               | com.nvidia.spark.rapids.CoalesceGoal\n                                                                  | Int\n\n************************************************************************************************\nParameter lengths don't match between Execs\nSparkExec - [org.apache.spark.sql.execution.window.WindowExec]\nGpuExec - [com.nvidia.spark.rapids.GpuWindowExec]\nSpark code has 4 parameters where as plugin code has 2 parameters\nSpark parameters                                                    Plugin parameters\n------------------------------------------------------------------------------------------------\nSeq[org.apache.spark.sql.catalyst.expressions.NamedExpression]    | Seq[com.nvidia.spark.rapids.GpuExpression]\nSeq[org.apache.spark.sql.catalyst.expressions.Expression]         | org.apache.spark.sql.execution.SparkPlan\nSeq[org.apache.spark.sql.catalyst.expressions.SortOrder]          | \norg.apache.spark.sql.execution.SparkPlan \n```\n\n----------------------------------------\n\nTITLE: Running Maven Build Before Shimplify Conversion\nDESCRIPTION: Command to build the project before running shimplify conversion, ensuring that all dependencies are available. This step is recommended before attempting to convert the shim structure.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/shimplify.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean install -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Null Generator Function in Scala for Spark RAPIDS\nDESCRIPTION: This code snippet demonstrates how to create a custom NullGeneratorFunction in Scala for Spark RAPIDS. It generates null values for rows within a specified range and uses a provided generator function for other rows. The function can be applied to specific columns in a data table.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/datagen/README.md#2025-04-19_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\ncase class MyNullGen(minRow: Long, maxRow: Long,\n    gen: GeneratorFunction = null) extends NullGeneratorFunction {\n\n  override def withWrapped(gen: GeneratorFunction): MyNullGen =\n    MyNullGen(minRow, maxRow, gen)\n\n  override def withLocationToSeedMapping(\n      mapping: LocationToSeedMapping): MyNullGen =\n    this\n\n  override def apply(rowLoc: RowLocation): Any = {\n    val rowNum = rowLoc.rowNum\n    if (rowNum <= maxRow && rowNum >= minRow) {\n      null\n    } else {\n      gen(rowLoc)\n    }\n  }\n}\n\n...\n\ndataTable(\"a\").setNullGen(MyNullGen(1024, 9999999L))\n```\n\n----------------------------------------\n\nTITLE: NaNvl Function Implementation\nDESCRIPTION: Implements nanvl function that returns the first argument if it is not NaN, otherwise returns the second argument. Limited to float and double data types.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/supported_ops.md#2025-04-19_snippet_53\n\nLANGUAGE: SQL\nCODE:\n```\nnanvl\n```\n\n----------------------------------------\n\nTITLE: Closing Spark Session\nDESCRIPTION: Properly terminates the SparkSession to release resources after all operations are completed.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/demo/GCP/mortgage-xgboost4j-gpu-scala.ipynb#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsparkSession.close()\n```\n\n----------------------------------------\n\nTITLE: Building Against All Supported Spark Versions with Scala 2.13\nDESCRIPTION: Command using the buildall script to build the RAPIDS Accelerator against all supported versions of Apache Spark with Scala 2.13.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/scala2.13/README.md#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./build/buildall --profile=noSnapshotsScala213\n```\n\n----------------------------------------\n\nTITLE: Reading Debug Data with Spark Scala\nDESCRIPTION: Code for reading the dumped debug data using Spark with Scala. This snippet handles the special character substitutions that were applied during data storage and reconstructs the original format.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/get-json-object-dump-tool.md#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n// Replace this with the actual path to read from \nval readPath = \"/data/tmp/DEBUG_JSON_DUMP\"\n\nval df = spark.read.\n  schema(\"isLegacy boolean, path string, originalInput string, cpuOutput string, gpuOutput string\").\n  csv(readPath)\n\nval strUnescape = Seq(\"isLegacy\") ++ Seq(\"path\", \"originalInput\", \"cpuOutput\", \"gpuOutput\").\n  map(c => s\"\"\"replace(replace(replace(replace($c, '**CR**', '\\r'), '**LF**', '\\n'), '**QT**', '\"'), '**COMMA**', ',') as $c\"\"\")\n\nval data = df.selectExpr(strUnescape : _*)\n```\n\n----------------------------------------\n\nTITLE: Generating and Activating Bloop Projects for Multiple Spark Versions\nDESCRIPTION: Command sequence using buildall script to generate Bloop projects for multiple Spark versions and activate a specific version via symlink.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n./build/buildall --generate-bloop --profile=320,330\nrm -vf .bloop\nln -s .bloop-spark330 .bloop\n```\n\n----------------------------------------\n\nTITLE: Building a Single-Spark-Version Distribution Jar\nDESCRIPTION: Maven command to build a distribution jar for a single specific Spark release with conventional class structure, using the allowConventionalDistJar parameter to avoid complications in class loading.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmvn package -pl dist -am -Dbuildver=340 -DallowConventionalDistJar=true\n```\n\n----------------------------------------\n\nTITLE: Building the RAPIDS Accelerator with Maven\nDESCRIPTION: Basic Maven command to build the RAPIDS Accelerator for Apache Spark through the 'verify' phase, which executes important build tasks and produces the plugin jar in the dist/target/ directory.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmvn verify\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter for Advanced Configuration Page\nDESCRIPTION: Jekyll front matter metadata defining the page layout, title, parent section, and navigation order for the advanced configuration documentation page.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/additional-functionality/advanced_configs.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: page\ntitle: Advanced Configuration\nparent: Additional Functionality\nnav_order: 10\n---\n```\n\n----------------------------------------\n\nTITLE: Enabling get_json_object Debugging in RAPIDS Accelerator\nDESCRIPTION: Configuration steps for enabling the get_json_object debugging tool. First enable the expression, then set the debug path where data will be stored, and optionally limit the number of rows per batch.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/get-json-object-dump-tool.md#2025-04-19_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n'spark.rapids.sql.expression.GetJsonObject': 'true'\n'spark.rapids.sql.expression.GetJsonObject.debugPath': '/tmp/DEBUG_JSON_DUMP/'\n'spark.rapids.sql.test.get_json_object.saveRows': '1024'\n```\n\n----------------------------------------\n\nTITLE: Spark Error Stack Trace for ORC Statistics\nDESCRIPTION: Example error stack trace when Spark 3.3.0+ encounters an ORC file with missing column statistics\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/compatibility.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\norg.apache.spark.SparkException: Cannot read columns statistics in file: /PATH_TO_ORC_FILE\nE    Caused by: java.util.NoSuchElementException\nE        at java.util.LinkedList.removeFirst(LinkedList.java:270)\nE        at java.util.LinkedList.remove(LinkedList.java:685)\nE        at org.apache.spark.sql.execution.datasources.orc.OrcFooterReader.convertStatistics(OrcFooterReader.java:54)\nE        at org.apache.spark.sql.execution.datasources.orc.OrcFooterReader.readStatistics(OrcFooterReader.java:45)\nE        at org.apache.spark.sql.execution.datasources.orc.OrcUtils$.createAggInternalRowFromFooter(OrcUtils.scala:428)\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test Suite\nDESCRIPTION: Command to run all tests in the ParquetWriterSuite class within the com.nvidia.spark.rapids package. This demonstrates how to target specific test suites using the wildcard suites parameter.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/tests/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn package -pl tests -am -DwildcardSuites=\"com.nvidia.spark.rapids.ParquetWriterSuite\"\n```\n\n----------------------------------------\n\nTITLE: GPU Allocation Logging Output in Spark-RAPIDS\nDESCRIPTION: Sample output from low-level GPU memory allocation logging, showing allocate and free operations with timestamps, memory addresses, allocation sizes, and CUDA stream information for debugging memory usage patterns.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/mem_debug.md#2025-04-19_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n1979932,14:21:19.998533,allocate,0x30a000000,256,0x2\n1980043,14:21:32.661875,free,0x30a000000,256,0x2\n```\n\n----------------------------------------\n\nTITLE: Implementing Addition UDF in Java using Dataframe Operations\nDESCRIPTION: Shows how to create and register a UDF in Java that adds two numbers using dataframe operations, demonstrating Java API compatibility.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/DF_UDF_README.md#2025-04-19_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nimport com.nvidia.spark.functions.df_udf\n\nimport org.apache.spark.sql.*;\nimport org.apache.spark.sql.api.java.UDF2;\nimport org.apache.spark.sql.expressions.UserDefinedFunction;\n\n\nUserDefinedFunction myAdd = df_udf((Column lhs, Column rhs) -> lhs + rhs)\nspark.udf().register(\"myadd\", myAdd)\n\nspark.sql(\"SELECT myadd(1, 1) as r\").show();\n// +--+\n// | r|\n// +--+\n// | 2|\n// +--+\n```\n\n----------------------------------------\n\nTITLE: Building FlatBuffers Compiler 1.11.0\nDESCRIPTION: Commands to clone, checkout and build version 1.11.0 of the FlatBuffers compiler from source using CMake.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/sql-plugin/src/main/format/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/google/flatbuffers.git\ncd flatbuffers\ngit checkout 1.11.0\nmkdir build\ncd build\ncmake ..\nmake\n```\n\n----------------------------------------\n\nTITLE: Enabling Reference Count Debugging in Spark-RAPIDS\nDESCRIPTION: Java system property configuration to enable detailed reference counting debug information. This setting captures stack traces for each increment and decrement operation on reference counts.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/mem_debug.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n-Dai.rapids.refcount.debug=true\n```\n\n----------------------------------------\n\nTITLE: Spark RAPIDS Scale Test Report Format (JSON)\nDESCRIPTION: Example of the JSON format used for the test report file, showing the structure of recorded execution times for each query run.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/ScaleTest.md#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[ {\n  \"name\" : \"q1\",\n  \"executionTime\" : [ 5175 ]\n}, {\n  \"name\" : \"q2\",\n  \"executionTime\" : [ 2830 ]\n} ]\n```\n\n----------------------------------------\n\nTITLE: Building libcudf with Line Numbers for Debugging\nDESCRIPTION: CMake command to enable line number information when building libcudf, which helps with debugging by providing more precise error location information in Compute Sanitizer reports.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/compute_sanitizer.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncmake .. <other options> -DCUDA_ENABLE_LINEINFO=ON\n```\n\n----------------------------------------\n\nTITLE: Creating a Directory for Compute Sanitizer Java Home\nDESCRIPTION: Command to create a 'fake' Java home directory that will be used to intercept Java execution with Compute Sanitizer in a distributed environment.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/dev/compute_sanitizer.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p /opt/compute-sanitizer-java/bin\n```\n\n----------------------------------------\n\nTITLE: Git Commit with Sign-Off Flag\nDESCRIPTION: Shows how to use the --signoff (or -s) flag with git commit to add a Signed-off-by line to the commit message, which certifies that the contribution is original work or you have rights to submit it.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/CONTRIBUTING.md#2025-04-19_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ngit commit -s -m \"Add cool feature.\"\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for Scala 2.12 Jar\nDESCRIPTION: This shell command imports a public key and verifies the signature of the RAPIDS Accelerator for Apache Spark 23.12.0 jar built for Scala 2.12. This is crucial for ensuring the integrity and authenticity of the downloaded files. Dependencies include GPG installed on your system.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-23.12.0.jar.asc rapids-4-spark_2.12-23.12.0.jar\n```\n\n----------------------------------------\n\nTITLE: Verifying Jar Signature with GPG\nDESCRIPTION: This snippet demonstrates how to verify the signature of a RAPIDS Accelerator for Apache Spark JAR file using GPG (GNU Privacy Guard). It involves using the `gpg --verify` command to check the signature against the downloaded JAR and ASC files, ensuring the integrity and authenticity of the downloaded package.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n\"gpg --verify rapids-4-spark_2.13-24.04.0.jar.asc rapids-4-spark_2.13-24.04.0.jar\"\n```\n\n----------------------------------------\n\nTITLE: Verifying RAPIDS Accelerator JAR Signature using GPG\nDESCRIPTION: Commands to import the public key and verify the signature of RAPIDS Accelerator JAR files using GPG. This process ensures the authenticity and integrity of the downloaded JAR files.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\ngpg --import PUB_KEY\ngpg --verify rapids-4-spark_2.12-24.10.0.jar.asc rapids-4-spark_2.12-24.10.0.jar\ngpg --verify rapids-4-spark_2.13-24.10.0.jar.asc rapids-4-spark_2.13-24.10.0.jar\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for v24.10.1 JAR Files\nDESCRIPTION: Commands for verifying the GPG signature of the RAPIDS Accelerator v24.10.1 JAR files for different Scala versions, ensuring file authenticity.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\n```\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify rapids-4-spark_2.12-24.10.1.jar.asc rapids-4-spark_2.12-24.10.1.jar\n```\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify rapids-4-spark_2.13-24.10.1.jar.asc rapids-4-spark_2.13-24.10.1.jar\n```\n\n----------------------------------------\n\nTITLE: Verifying GPG Signature for RAPIDS Accelerator JAR Files\nDESCRIPTION: Commands for importing the NVIDIA public key and verifying the GPG signature of the RAPIDS Accelerator JAR files for different Scala versions. This verification ensures the integrity and authenticity of the downloaded files.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/docs/archive.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngpg --import PUB_KEY\n```\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify rapids-4-spark_2.12-24.12.0.jar.asc rapids-4-spark_2.12-24.12.0.jar\n```\n\nLANGUAGE: bash\nCODE:\n```\ngpg --verify rapids-4-spark_2.13-24.12.0.jar.asc rapids-4-spark_2.13-24.12.0.jar\n```\n\n----------------------------------------\n\nTITLE: Listing Floating-Point Numbers in Plaintext\nDESCRIPTION: This snippet provides a comprehensive list of floating-point numbers in various formats, including normal decimal notation, scientific notation, and edge cases such as very large or small numbers. It includes both positive and negative numbers, as well as examples of different precisions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/simple-float-values/simple_float_values.hive.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumber\n1.042\n--\nbad\n\"\"\n,\n98.343\n223823.9484\n23848545.0374\n184721.23987223\n3.4028235e+38\n-0.0\n3.4028235E38\n3.4028236E38\n3.4028236e+38\n3.4028236e-38\n1.7976931348623157E99\n1.7976931348623157e+99\n1.7976931348623157e-99\n1.2    \n   1.2\n 1 \n1\n1.7976931348623157E308\n1.7976931348623157e+308\n1.111111111111111111111111111111111111111E-39\n1.11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n```\n\n----------------------------------------\n\nTITLE: Defining Numerical and Character Constants in Plaintext\nDESCRIPTION: This snippet lists various numerical constants, including integers of different sizes (8-bit, 16-bit, 32-bit, 64-bit, and beyond), both positive and negative. It also includes character constants and special symbols.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/simple-int-values/simple_int_values.hive.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumber\n-1\n0\n1\n127\n128\n-128\n-129\n32767\n-32768\n32768\n-32769\n2147483647\n-2147483648\n2147483648\n-2147483649\n9223372036854775807\n-9223372036854775808\n9223372036854775808\n-9223372036854775809\n18446744073709551615\nA\nB\nC\n+50\n\\n\n+\n-\n 100\n100\nBAD\n```\n\n----------------------------------------\n\nTITLE: Timestamp Format Examples in Plain Text\nDESCRIPTION: Comprehensive set of timestamp format examples showing different variations including date-only format, date-time with T separator, space-separated date-time, various decimal precision levels for seconds, and timezone indicators. Includes an invalid date example and non-timestamp text for contrast.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/timestamp/ts.hive.txt#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2020-09-16\n2020-09-16T22:32\n2020-09-16 22:32\n2020-09-16T22:32:01\n 2020-09-16T22:32:01\n2020-09-16T22:32:01\n2020-09-16 22:32:01\n2020-09-16 22:32:01.1\n2020-09-16 22:32:01.12\n2020-09-16 22:32:01.123\n2020-09-16 22:32:01.1234\n2020-09-16 22:32:01.12345\n2020-09-16 22:32:01.123456\n2020-09-16 22:32:01.1234567\n2020-09-16T22:32:01Z\n2020-09-16 22:32:01Z\n2020-90-16 22:32:01Z\nPatently not a timestamp row\n```\n\n----------------------------------------\n\nTITLE: Parsing Fixed Width Financial Records\nDESCRIPTION: Fixed width data records containing financial or transaction information with consistent formatting. Each record includes account numbers, dates, decimal values, and status indicators in specific positions. Records follow a standard format with fields aligned at fixed character positions.\nSOURCE: https://github.com/nvidia/spark-rapids/blob/branch-25.06/integration_tests/src/test/resources/hive-delim-text/Performance_2007Q3/Performance_2007Q3.hive.txt#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n10004519801211/01/20106.546477.2241.0319.0258.006/203741700.00NN\n10004519801212/01/20106.546293.9742.0318.0256.006/203741700.00NN\n10004519801201/01/20116.546109.7343.0317.0254.006/203741700.00NN\n```"
  }
]
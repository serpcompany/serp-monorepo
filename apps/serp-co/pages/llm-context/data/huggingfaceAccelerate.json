[
  {
    "owner": "huggingface",
    "repo": "accelerate",
    "content": "TITLE: Loading and Dispatching Checkpoint Accelerate Python\nDESCRIPTION: Illustrates loading weights into an empty model and distributing them across available devices (GPU, CPU, disk) using Accelerate's `load_checkpoint_and_dispatch`. Setting `device_map=\"auto\"` automatically determines the distribution strategy based on device speed and memory availability.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import load_checkpoint_and_dispatch\n\nmodel = load_checkpoint_and_dispatch(\n    model, checkpoint=checkpoint_file, device_map=\"auto\"\n)\n```\n\n----------------------------------------\n\nTITLE: Example Accelerate Config for DeepSpeed Stage 3 Offload - YAML\nDESCRIPTION: Provides a sample Accelerate configuration file content in YAML format, configured to use a DeepSpeed configuration file for ZeRO Stage 3 with CPU offloading. It specifies `distributed_type: DEEPSPEED` and points to the relevant `deepspeed_config_file` for Stage 3 offload.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n deepspeed_config_file: /home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage3_offload_config.json\n zero3_init_flag: true\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Inferring Automatic Device Map with Accelerate in Python\nDESCRIPTION: This Python snippet demonstrates how to use the Accelerate library's `infer_auto_device_map` function to automatically compute a device map for a model based on specified maximum memory constraints per device and CPU RAM. It requires a model instance (`my_model`) and a dictionary defining memory limits per GPU and CPU. The output is a dictionary that places model layers across devices to optimize memory usage, enabling efficient use of GPUs and CPU offload when physical GPU memory is insufficient.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import infer_auto_device_map\n\ndevice_map = infer_auto_device_map(my_model, max_memory={0: \"10GiB\", 1: \"10GiB\", \"cpu\": \"30GiB\"})\n```\n\n----------------------------------------\n\nTITLE: Initializing Gradient Accumulation with Huggingface Accelerate in Python\nDESCRIPTION: This snippet demonstrates the setup of gradient accumulation using the Huggingface Accelerate library to prepare model, optimizer, dataloader, and scheduler for multi-device distributed training in Python. It shows how to iterate over batches while aggregating gradients across multiple steps and devices, user-defined loss scaling based on token counts, and controlled synchronization to reduce unnecessary communications in distributed data parallel (DDP) setups. Key parameters include 'gradient_accumulation_steps' to specify how many steps gradients are accumulated before an optimizer update, and 'num_items_in_batch' to correctly normalize loss during training. Dependencies include the 'accelerate' library and 'math' and 'contextlib' modules. Inputs are prepared model and data batches, outputs are updated model parameters after each gradient accumulation cycle.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nimport math\nimport contextlib\n\ngradient_accumulation_steps = 2\naccelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\n\ntraining_iterator = iter(training_dataloader)\nnum_samples_in_epoch = len(training_dataloader)\nremainder = num_samples_in_epoch % gradient_accumulation_steps\nremainder = remainder if remainder != 0 else gradient_accumulation_steps\ntotal_updates = math.ceil(num_samples_in_epoch / gradient_accumulation_steps)\n        \ntotal_batched_samples = 0\nfor update_step in range(total_updates):\n        # In order to correctly the total number of non-padded tokens on which we'll compute the cross-entropy loss\n        # we need to pre-load the full local batch - i.e the next per_device_batch_size * accumulation_steps samples\n        batch_samples = []\n        num_batches_in_step = gradient_accumulation_steps if update_step != (total_updates - 1) else remainder\n        for _ in range(num_batches_in_step):\n            batch_samples += [next(training_iterator)]\n            \n        # get local num items in batch \n        num_items_in_batch = sum([(batch[\"labels\"].ne(-100)).sum() for batch in batch_samples])\n        # to compute it correctly in a multi-device DDP training, we need to gather the total number of items in the full batch.\n        num_items_in_batch = accelerator.gather(num_items_in_batch).sum().item()\n            \n        for i, batch in enumerate(batch_samples):\n            # if we perform gradient accumulation in a multi-devices set-up, we want to avoid unecessary communications when accumulating\n            # cf: https://muellerzr.github.io/blog/gradient_accumulation.html\n            if (i < len(batch_samples) - 1 and accelerator.num_processes > 1):\n                ctx = model.no_sync\n            else:\n                ctx = contextlib.nullcontext\n            \n            total_batched_samples += 1\n\n            with ctx():\n                inputs, targets = batch\n                outputs = model(inputs)\n                loss = loss_function(outputs, targets) # the loss function shoud sum over samples rather than averaging\n                \n                # We multiply by num_processes because the DDP calculates the average gradient across all devices whereas dividing by num_items_in_batch already takes into account all devices\n                # Same reason for gradient_accumulation_steps, but this times it's Accelerate that calculate the average gradient across the accumulated steps\n                loss = (loss * gradient_accumulation_steps * accelerator.num_processes) / num_items_in_batch\n                \n                accelerator.backward(loss)\n\n        # Sync gradients and perform optimization steps once every gradient_accumulation_steps\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Saving and Restoring Training State with Accelerate in Python\nDESCRIPTION: This snippet demonstrates how to use the HuggingFace Accelerate library to checkpoint and restore the complete state of a PyTorch training session. It covers initialization of Accelerator, registering a learning rate scheduler for checkpointing, saving the state at any training step, and restoring the saved state later. Prerequisites include the accelerate and torch packages, along with an instantiated model, optimizer, DataLoader, and loss function. Inputs include model objects and training batches; outputs are checkpointed files and restored in-memory objects. This example assumes use in the same training script, and requires registered objects to implement state_dict and load_state_dict methods.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/checkpoint.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nimport torch\n\naccelerator = Accelerator(project_dir=\"my/save/path\")\n\nmy_scheduler = torch.optim.lr_scheduler.StepLR(my_optimizer, step_size=1, gamma=0.99)\nmy_model, my_optimizer, my_training_dataloader = accelerator.prepare(my_model, my_optimizer, my_training_dataloader)\n\n# Register the LR scheduler\naccelerator.register_for_checkpointing(my_scheduler)\n\n# Save the starting state\naccelerator.save_state()\n\ndevice = accelerator.device\nmy_model.to(device)\n\n# Perform training\nfor epoch in range(num_epochs):\n    for batch in my_training_dataloader:\n        my_optimizer.zero_grad()\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = my_model(inputs)\n        loss = my_loss_function(outputs, targets)\n        accelerator.backward(loss)\n        my_optimizer.step()\n    my_scheduler.step()\n\n# Restore the previous state\naccelerator.load_state(\"my/save/path/checkpointing/checkpoint_0\")\n```\n\n----------------------------------------\n\nTITLE: Loading Sharded Checkpoint with Accelerate\nDESCRIPTION: This code demonstrates how to load a sharded checkpoint into a model using Accelerate.  It utilizes the `load_checkpoint_in_model` function, allowing you to specify the device to load the checkpoint onto.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nload_checkpoint_in_model(unwrapped_model, save_directory, device_map={\"\":device})\n```\n\n----------------------------------------\n\nTITLE: Integrating Accelerate into a PyTorch Training Loop\nDESCRIPTION: Example showing how to modify a PyTorch training script to use Accelerate. The snippet demonstrates adding just a few lines of code to enable distributed training and mixed precision.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\ndevice = accelerator.device\n\nmodel = torch.nn.Transformer().to(device)\noptimizer = torch.optim.Adam(model.parameters())\n\ndataset = load_dataset('my_dataset')\ndata = torch.utils.data.DataLoader(dataset, shuffle=True)\n\nmodel, optimizer, data = accelerator.prepare(model, optimizer, data)\n\nmodel.train()\nfor epoch in range(10):\n    for source, targets in data:\n        source = source.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(source)\n        loss = F.cross_entropy(output, targets)\n\n        accelerator.backward(loss)\n\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple DeepSpeed Plugins in Python\nDESCRIPTION: Demonstrates how to create multiple `DeepSpeedPlugin` instances with different configurations (e.g., ZeRO-2 for a student model, ZeRO-3 for a teacher model) and store them in a dictionary for use with Accelerate. This setup allows switching between configurations for different models.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import DeepSpeedPlugin\n\nzero2_plugin = DeepSpeedPlugin(hf_ds_config=\"zero2_config.json\")\nzero3_plugin = DeepSpeedPlugin(hf_ds_config=\"zero3_config.json\")\n\ndeepspeed_plugins = {\"student\": zero2_plugin, \"teacher\": zero3_plugin}\n```\n\n----------------------------------------\n\nTITLE: Resuming DataLoader State Using skip_first_batches with Accelerate in Python\nDESCRIPTION: This snippet illustrates how to resume training from a specific point in a DataLoader after loading a saved checkpoint using Accelerate. It shows how to use skip_first_batches to continue from a particular iteration, such as resuming after 100 steps into an epoch. Dependencies include accelerate for checkpoint handling, while expected inputs are DataLoader objects and a checkpoint state directory. Outputs are a DataLoader advanced by a set number of batches, allowing seamless continuation of training. The method is useful where checkpointing occurs mid-epoch, and should be used immediately after loading state for consistent batch order.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/checkpoint.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(project_dir=\"my/save/path\")\n\ntrain_dataloader = accelerator.prepare(train_dataloader)\naccelerator.load_state(\"my_state\")\n\n# Assume the checkpoint was saved 100 steps into the epoch\nskipped_dataloader = accelerator.skip_first_batches(train_dataloader, 100)\n\n# After the first iteration, go back to `train_dataloader`\n\n# First epoch\nfor batch in skipped_dataloader:\n    # Do something\n    pass\n\n# Second epoch\nfor batch in train_dataloader:\n    # Do something\n    pass\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading 8-bit Quantized Model\nDESCRIPTION: This code demonstrates how to save and load an 8-bit quantized model using Accelerate. `accelerate.save_model` saves the quantized model to a specified directory. `load_and_quantize_model` is used to load the saved model back, with an optional `device_map` for placing the model on the appropriate device. Note: Saving/loading of 4-bit models is currently unsupported.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\naccelerate = Accelerator()\nnew_weights_location = \"path/to/save_directory\"\naccelerate.save_model(quantized_model, new_weights_location)\n\nquantized_model_from_saved = load_and_quantize_model(empty_model, weights_location=new_weights_location, bnb_quantization_config=bnb_quantization_config, device_map = \"auto\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Dummy Optimizer/Scheduler Usage - Python\nDESCRIPTION: Illustrates how to conditionally instantiate `accelerate.utils.DummyOptim` and `accelerate.utils.DummyScheduler` within the user's training script. This is necessary when the DeepSpeed configuration file specifies its own optimizer and/or scheduler, allowing Accelerate to handle their initialization internally via DeepSpeed.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Creates Dummy Optimizer if `optimizer` was specified in the config file else creates Adam Optimizer\noptimizer_cls = (\n    torch.optim.AdamW\n    if accelerator.state.deepspeed_plugin is None\n    or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n    else DummyOptim\n)\noptimizer = optimizer_cls(optimizer_grouped_parameters, lr=args.learning_rate)\n\n# Creates Dummy Scheduler if `scheduler` was specified in the config file else creates `args.lr_scheduler_type` Scheduler\nif (\n    accelerator.state.deepspeed_plugin is None\n    or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n):\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps,\n        num_training_steps=args.max_train_steps,\n    )\nelse:\n    lr_scheduler = DummyScheduler(\n        optimizer, total_num_steps=args.max_train_steps, warmup_num_steps=args.num_warmup_steps\n    )\n```\n\n----------------------------------------\n\nTITLE: Preparing Student Model with Default DeepSpeed Plugin in Python\nDESCRIPTION: Shows how to prepare the student model, optimizer, scheduler, and dataloader using `accelerator.prepare()`. Since the 'student' plugin is active by default, its configuration (ZeRO-2) will be used.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstudent_model, optimizer, scheduler = ...\nstudent_model, optimizer, scheduler, train_dataloader = accelerator.prepare(student_model, optimizer, scheduler, train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard Training Function with Accelerate Python\nDESCRIPTION: This snippet defines a basic training function suitable for CPU/GPU notebook environments. It initializes the Accelerate `Accelerator` object, loads a Hugging Face model, prepares objects using `accelerator.prepare()`, and runs a standard training loop including forward pass, loss calculation, `accelerator.backward()`, optimizer step, and gradient zeroing. It requires the model and dataloaders to be passed indirectly or created inside.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/training_tpu.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef training_function():\n    # Initialize accelerator\n    accelerator = Accelerator()\n    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n    train_dataloader, eval_dataloader = create_dataloaders(\n        train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n    )\n\n    # Instantiate optimizer\n    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[\"learning_rate\"])\n\n    # Prepare everything\n    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n    # prepare method.\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    num_epochs = hyperparameters[\"num_epochs\"]\n    # Now we train the model\n    for epoch in range(num_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n```\n\n----------------------------------------\n\nTITLE: Example DeepSpeed Config Snippet for ZeRO++ - JSON\nDESCRIPTION: Shows a partial DeepSpeed configuration file snippet in JSON format focusing on parameters specific to ZeRO++, which is an extension of ZeRO Stage 3. It illustrates enabling quantized weights and gradients (`zero_quantized_weights`, `zero_quantized_gradients`) and setting the hierarchical partitioning size (`zero_hpz_partition_size`).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"reduce_bucket_size\": \"auto\",\n\n        \"zero_quantized_weights\": true,\n        \"zero_hpz_partition_size\": 8,\n        \"zero_quantized_gradients\": true,\n\n        \"contiguous_gradients\": true,\n        \"overlap_comm\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Accelerate's accumulate Context Manager\nDESCRIPTION: Replacing manual gradient accumulation logic with Accelerate's context manager which automatically handles accumulation, loss scaling, and optimizer stepping.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- for index, batch in enumerate(training_dataloader):\n+ for batch in training_dataloader:\n+     with accelerator.accumulate(model):\n          inputs, targets = batch\n          outputs = model(inputs)\n\n```\n\n----------------------------------------\n\nTITLE: Configure TransformersEngine Backend for FP8 Accelerator in Python\nDESCRIPTION: Demonstrates configuring the TransformersEngine backend in Python. It uses `FP8RecipeKwargs` to set `backend=\"te\"` and allows passing additional, specific arguments for TransformersEngine (represented by `...`) to the `Accelerator` via `kwarg_handlers`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/low_precision_training.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom accelerate.utils import FP8RecipeKwargs\nkwargs = [FP8RecipeKwargs(backend=\"te\", ...)]\naccelerator = Accelerator(mixed_precision=\"fp8\", kwarg_handlers=kwargs)\n```\n\n----------------------------------------\n\nTITLE: Preparing PyTorch Objects for Distributed Training\nDESCRIPTION: Shows how to prepare the model, optimizer, dataloader, and scheduler for distributed training using Accelerator's prepare method. This step is crucial for optimizing these objects for the specific distributed setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_dataloader, lr_scheduler\n)\n```\n\n----------------------------------------\n\nTITLE: Out-of-Memory with Accelerate\nDESCRIPTION: This code demonstrates the use of the `find_executable_batch_size` utility from the Accelerate library to handle out-of-memory errors. It restructures the training loop using an inner function that automatically decreases the batch size upon OOM errors. The model, optimizer, and dataloaders are all declared within the inner function.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/troubleshooting.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef training_function(args):\n    accelerator = Accelerator()\n\n    @find_executable_batch_size(starting_batch_size=args.batch_size)\n    def inner_training_loop(batch_size):\n        nonlocal accelerator # Ensure they can be used in our context\n        accelerator.free_memory() # Free all lingering references\n        model = get_model()\n        model.to(accelerator.device)\n        optimizer = get_optimizer()\n        train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n        lr_scheduler = get_scheduler(\n            optimizer, \n            num_training_steps=len(train_dataloader)*num_epochs\n        )\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n            model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n        )\n        train(model, optimizer, train_dataloader, lr_scheduler)\n        validate(model, eval_dataloader)\n    inner_training_loop()\n```\n\n----------------------------------------\n\nTITLE: Configuring MegatronLMPlugin for Multi-Query Attention (Python)\nDESCRIPTION: This snippet demonstrates how to configure the `MegatronLMPlugin` to enable Multi-Query Attention for supported Megatron-LM models. It involves setting the `attention_head_type` key to \"multiquery\" within the `other_megatron_args` dictionary, which is subsequently passed to the plugin constructor. This setup step is necessary to utilize Multi-Query Attention when integrating Megatron-LM with Accelerate.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nother_megatron_args = {\"attention_head_type\": \"multiquery\"}\nmegatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)\n```\n\n----------------------------------------\n\nTITLE: Loading and Quantizing the Model\nDESCRIPTION: This code loads and quantizes the empty model using `load_and_quantize_model` from Accelerate. It takes the empty model, the location of the weights, and the quantization configuration as input. The result is a quantized model.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import load_and_quantize_model\nquantized_model = load_and_quantize_model(empty_model, weights_location=weights_location, bnb_quantization_config=bnb_quantization_config)\n```\n\n----------------------------------------\n\nTITLE: Using Accelerator with Built-in Trackers in Python\nDESCRIPTION: Demonstrates initializing the Accelerator with various predefined trackers such as 'wandb' or all available, configuring experiment tracking, and logging training metrics during a training loop. It also shows how to finalize tracking and handle directory setups for trackers requiring file storage.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/tracking.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom accelerate.utils import LoggerType\n\naccelerator = Accelerator(log_with=\"all\")  # Initialize with all available trackers\naccelerator = Accelerator(log_with=\"wandb\")  # Initialize with Weights & Biases\naccelerator = Accelerator(log_with=[\"wandb\", LoggerType.TENSORBOARD])  # Multiple trackers\n\n# Setup project with trackers\nhps = {\"num_iterations\": 5, \"learning_rate\": 1e-2}\naccelerator.init_trackers(\"my_project\", config=hps)\n\n# Prepare model, optimizer, data loader\nmy_model, my_optimizer, my_training_dataloader = accelerator.prepare(my_model, my_optimizer, my_training_dataloader)\n\ndevice = accelerator.device\nmy_model.to(device)\n\n# Training loop\nfor iteration in range(hps[\"num_iterations\"]):\n    for step, batch in enumerate(my_training_dataloader):\n        my_optimizer.zero_grad()\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = my_model(inputs)\n        loss = my_loss_function(outputs, targets)\n        accelerator.backward(loss)\n        my_optimizer.step()\n        accelerator.log({\"training_loss\": loss}, step=step)\n\n# Finalize experiments\naccelerator.end_training()\n\n```\n\n----------------------------------------\n\nTITLE: Accelerate Training Loop\nDESCRIPTION: This code demonstrates how to integrate Accelerate into the training loop. It removes the manual device placement of inputs/targets and replaces the standard `loss.backward()` call with `accelerator.backward(loss)` to handle gradient scaling appropriately.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\naccelerator = Accelerator()\n\ndevice = accelerator.device\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\n\nfor batch in training_dataloader:\n    optimizer.zero_grad()\n    inputs, targets = batch\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Running Inference on a big model\nDESCRIPTION: This code shows how to run inference on a large model after loading it with `load_checkpoint_and_dispatch` and using the Accelerate library.  The tokenizer is used to convert the input text to tensors, and then the model's `generate` method is called to generate output sequences.  The output is then decoded back into text.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_10\n\nLANGUAGE: py\nCODE:\n```\nfrom mingpt.bpe import BPETokenizer\ntokenizer = BPETokenizer()\ninputs = tokenizer(\"Hello, my name is\").to(0)\n\noutputs = model.generate(x1, max_new_tokens=10, do_sample=False)[0]\ntokenizer.decode(outputs.cpu().squeeze())\n```\n\n----------------------------------------\n\nTITLE: Initializing a Diffusion Pipeline with Diffusers in Python\nDESCRIPTION: Imports necessary libraries (`torch`, `torch.distributed`, `diffusers`) and initializes a pre-trained Stable Diffusion pipeline from `diffusers`, specifying the data type (`torch.float16`) for potential memory and speed optimization. This setup is foundational for running inference.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Prepare Model for ZeRO Inference\nDESCRIPTION: This code illustrates the correct usage of the `accelerator.prepare()` function with a model and a dataloader for running ZeRO inference.  It prepares the model for efficient ZeRO stage 3 inference without optimizer/lr scheduler.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nmodel, eval_dataloader = accelerator.prepare(model, eval_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Complete Gradient Accumulation Implementation with Accelerate\nDESCRIPTION: The final implementation using Accelerate for gradient accumulation. This code efficiently handles accumulation in distributed environments with minimal changes to the original code.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\naccelerator = Accelerator(gradient_accumulation_steps=2)\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\nfor batch in training_dataloader:\n    with accelerator.accumulate(model):\n        inputs, targets = batch\n        outputs = model(inputs)\n        loss = loss_function(outputs, targets)\n        accelerator.backward(loss)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Configuring Scheduled Profiling with PyTorch - Python\nDESCRIPTION: Illustrates how to set up a scheduled profiler in PyTorch to efficiently analyze long-running jobs like training loops. Uses the torch.profiler.schedule to configure custom profiling cycles including skipping initial steps, warmup, active profiling, and repeated cycles. Relies on PyTorch Profiler and requires the ProfilerActivity module, a model, and inputs. The trace_handler function is specified to export trace files and print top operations by CUDA time. Expected input: a model and inputs; output: printed table and exported trace files per cycle.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.profiler import schedule\n\nmy_schedule = schedule(\n    skip_first=1,\n    wait=5,\n    warmup=1,\n    active=3,\n    repeat=2\n)\n\ndef trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    schedule=my_schedule,\n    on_trace_ready=trace_handler\n) as p:\n    for idx in range(8):\n        model(inputs)\n        p.step()\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate and Preparing Model with Accelerator in Python\nDESCRIPTION: This code snippet demonstrates how to instantiate the Accelerate library and prepare model components for distributed training. It involves creating an Accelerator object and using its 'prepare' method to move the model, optimizer, dataloader, and scheduler into the appropriate distributed environment. The main training loop then utilizes 'accelerator.backward' for loss backpropagation, simplifying distributed training setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/index.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom accelerate import Accelerator\naccelerator = Accelerator()\n\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\n\nfor batch in training_dataloader:\n    optimizer.zero_grad()\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Using accelerator.backward() for Loss Backpropagation\nDESCRIPTION: Shows how to replace the standard loss.backward() call with accelerator.backward() which handles gradient computation correctly in distributed environments.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naccelerator.backward(loss)\n```\n\n----------------------------------------\n\nTITLE: Launching ZeRO Stage 2 DeepSpeed Example - Bash\nDESCRIPTION: Shows a specific command-line example using `accelerate launch` to run the sample script `examples/by_feature/deepspeed_with_config_support.py`. It assumes Accelerate is configured to use the ZeRO Stage 2 DeepSpeed config file and passes various script-specific arguments.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/by_feature/deepspeed_with_config_support.py \\\n--config_name \"gpt2-large\" \\\n--tokenizer_name \"gpt2-large\" \\\n--dataset_name \"wikitext\" \\\n--dataset_config_name \"wikitext-2-raw-v1\" \\\n--block_size 128 \\\n--output_dir \"./clm/clm_deepspeed_stage2_accelerate\" \\\n--learning_rate 5e-4 \\\n--per_device_train_batch_size 24 \\\n--per_device_eval_batch_size 24 \\\n--num_train_epochs 3 \\\n--with_tracking \\\n--report_to \"wandb\"\n```\n\n----------------------------------------\n\nTITLE: Saving and loading training state with Accelerate\nDESCRIPTION: Demonstrates how to save and restore the current training state, including models, optimizers, and other components, using the Accelerator class. Highlights the use of save_state and load_state methods, customization of checkpoint locations through ProjectConfiguration, and registration of additional stateful objects that implement load_state_dict and state_dict functions.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n### State\n\nDuring training, you may want to save the current state of the model, optimizer, random generators, and potentially learning rate schedulers so they can be restored in the *same script*. You should add the [`~Accelerator.save_state`] and [`~Accelerator.load_state`] methods to your script to save and load states.\n\nTo further customize where and how states are saved through [`~utils.ProjectConfiguration`], use the [`~utils.ProjectConfiguration`] class. For example, if `automatic_checkpoint_naming` is enabled, each saved checkpoint is stored at `Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`.\n\nAny other stateful items to be stored should be registered with the [`~Accelerator.register_for_checkpointing`] method so they can be saved and loaded. Every object passed to this method to be stored must have a `load_state_dict` and `state_dict` function.\n\n> [!TIP]\n> If you have [`torchdata>=0.8.0`](https://github.com/pytorch/data/tree/main) installed, you can additionally pass `use_stateful_dataloader=True` into your [`~utils.DataLoaderConfiguration`]. This extends Accelerate's DataLoader classes with a `load_state_dict` and `state_dict` function, and makes it so `Accelerator.save_state` and `Accelerator.load_state` also track how far into the training dataset it has read when persisting the model.\n```\n\n----------------------------------------\n\nTITLE: Launching TorchRun Script (Bash)\nDESCRIPTION: Alternative command using the `torchrun` utility (part of PyTorch Distributed) to run a Python script (`phi2.py`) for distributed execution. The `--nproc-per-node` flag specifies the number of processes per node, typically corresponding to the number of GPUs. Requires `torch` installed.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/inference/distributed/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc-per-node {NUM_GPUS} phi2.py\n```\n\n----------------------------------------\n\nTITLE: Defining Training Loop - PyTorch\nDESCRIPTION: This code defines a basic PyTorch training loop with gradient accumulation steps. It sets the device (CUDA), iterates through the training data, calculates the loss, performs backpropagation, and updates the optimizer and scheduler based on the accumulation steps.  It is a foundational example that will be converted to use Accelerate.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/local_sgd.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cuda\"\nmodel.to(device)\n\ngradient_accumulation_steps = 2\n\nfor index, batch in enumerate(training_dataloader):\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss = loss / gradient_accumulation_steps\n    loss.backward()\n    if (index + 1) % gradient_accumulation_steps == 0:\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Saving Model with Accelerate (Single Checkpoint)\nDESCRIPTION: This snippet shows how to save a model after training using Accelerate. The `accelerator.wait_for_everyone()` ensures all processes are complete before saving. The `accelerator.save_model()` method unwraps the model and saves the state dictionary.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\naccelerator.wait_for_everyone()\naccelerator.save_model(model, save_directory)\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Evaluation with Accelerate\nDESCRIPTION: Demonstrates how to perform distributed evaluation by preparing the validation dataloader and using gather_for_metrics to collect predictions from all processes for accurate metric calculation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvalidation_dataloader = accelerator.prepare(validation_dataloader)\n\nfor inputs, targets in validation_dataloader:\n    predictions = model(inputs)\n    # Gather all predictions and targets\n    all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n    # Example of use with a *Datasets.Metric*\n    metric.add_batch(all_predictions, all_targets)\n```\n\n----------------------------------------\n\nTITLE: Writing Basic GPU Configuration and Restarting Jupyter Notebook in Python\nDESCRIPTION: Uses the `write_basic_config` utility from the Accelerate library to generate a GPU configuration file with default settings without interactive prompts. Following this, the notebook process is programmatically terminated with `os._exit(00)` to reset CUDA, which cannot be re-initialized multiple times in one session on multi-GPU systems. This snippet should be run before training to ensure correct environment setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()  # Write a config file\nos._exit(00)  # Restart the notebook\n```\n\n----------------------------------------\n\nTITLE: Launching Accelerate Script with DeepSpeed (Bash)\nDESCRIPTION: Executes a Python script (e.g., `my_script.py`, `examples/nlp_example.py`) using the HuggingFace Accelerate launcher. This command applies the configuration settings previously defined (e.g., via `accelerate config`), enabling distributed training with DeepSpeed. Additional arguments can be passed to the script, such as `--mixed_precision fp16`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\naccelerate launch my_script.py --args_to_my_script\n```\n\nLANGUAGE: Bash\nCODE:\n```\naccelerate launch examples/nlp_example.py --mixed_precision fp16\n```\n\n----------------------------------------\n\nTITLE: Configuring 8-bit Quantization\nDESCRIPTION: This code configures 8-bit quantization using `BnbQuantizationConfig`. The `load_in_8bit` parameter is set to `True` to enable 8-bit quantization. `llm_int8_threshold` specifies the threshold for outlier detection.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import BnbQuantizationConfig\nbnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold = 6)\n```\n\n----------------------------------------\n\nTITLE: Loading Checkpoint and Dispatching with accelerate\nDESCRIPTION: This snippet loads a sharded checkpoint into the model, and dispatches those weights across the available devices, using `load_checkpoint_and_dispatch`. The parameters `model`, `checkpoint`, `device_map`, and `no_split_module_classes` are used. The `device_map=\"auto\"` option allows Accelerate to automatically place layers across GPUs, CPU RAM, and the hard drive. The code loads the model weights from a specified location into the model.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_7\n\nLANGUAGE: py\nCODE:\n```\nfrom accelerate import load_checkpoint_and_dispatch\n\nmodel = load_checkpoint_and_dispatch(\n    model, checkpoint=weights_location, device_map=\"auto\", no_split_module_classes=['Block']\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Example Inputs for Model Tracing\nDESCRIPTION: This snippet creates a batch of random input tensors on the CPU to serve as example inputs for tracing the model with torch.distributed.pipelining. It defines batch size and sequence length, and uses GPT2's vocabulary size for input value ranges. Dependencies include torch and the model config.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ninput = torch.randint(\n    low=0,\n    high=config.vocab_size,\n    size=(2, 1024),  # bs x seq_len\n    device=\"cpu\",\n    dtype=torch.int64,\n    requires_grad=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerator for Distributed Training\nDESCRIPTION: Shows how to import and instantiate the Accelerator class which automatically detects the training environment and configures the necessary components for distributed training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n```\n\n----------------------------------------\n\nTITLE: Offloading Model to Disk Using Accelerate in Python\nDESCRIPTION: This snippet uses the Accelerate `disk_offload` function to store all model parameters as memory-mapped files on disk in a specified directory, reducing GPU and CPU RAM usage. During inference, parameters are loaded from disk and moved to the execution device as needed, then offloaded back. The function requires the model, the path to an offload directory, and an execution device. This method trades off memory use for I/O latency and depends on having fast disk-to-CPU transfer speeds for acceptable performance.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndisk_offload(model, offload_dir, execution_device)\n```\n\n----------------------------------------\n\nTITLE: Wrapping the Model for Pipeline Parallelism with prepare_pippy\nDESCRIPTION: This code demonstrates how to wrap the model for pipeline parallelism using accelerate's `prepare_pippy`, which traces and prepares the model for distributed pipeline execution. It also passes example inputs for tracing. Dependencies include accelerate.inference and torch.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom accelerate.inference import prepare_pippy\nexample_inputs = {\"input_ids\": input}\nmodel = prepare_pippy(model, example_args=(input,))\n```\n\n----------------------------------------\n\nTITLE: Simplifying the Training Loop with Accelerate\nDESCRIPTION: Removing manual loss scaling and step counting logic since Accelerate's accumulate context manager handles these automatically.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n- loss = loss / gradient_accumulation_steps\n  accelerator.backward(loss)\n- if (index+1) % gradient_accumulation_steps == 0:\n  optimizer.step()\n  scheduler.step()\n  optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Custom PyTorch Dataset Class for Pet Images in Python\nDESCRIPTION: Defines a PyTorch `Dataset` subclass named `PetsDataset` for loading images and their associated labels. The class initializes with a list of image file paths, an optional image transformation pipeline, and an optional label-to-ID mapping dictionary. The `__getitem__` method loads images using PIL, applies specified transforms, extracts labels via the previously defined function, converts labels to IDs if mapping is given, and returns a dictionary containing the transformed image tensor and label.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass PetsDataset(Dataset):\n    def __init__(self, file_names, image_transform=None, label_to_id=None):\n        self.file_names = file_names\n        self.image_transform = image_transform\n        self.label_to_id = label_to_id\n\n    def __len__(self):\n        return len(self.file_names)\n\n    def __getitem__(self, idx):\n        fname = self.file_names[idx]\n        raw_image = PIL.Image.open(fname)\n        image = raw_image.convert(\"RGB\")\n        if self.image_transform is not None:\n            image = self.image_transform(image)\n        label = extract_label(fname)\n        if self.label_to_id is not None:\n            label = self.label_to_id[label]\n        return {\"image\": image, \"label\": label}\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Device Map\nDESCRIPTION: This code defines a custom device map for offloading modules to CPU or disk. This allows you to reduce GPU memory usage by placing less frequently used modules on slower memory.  It's compatible with Big Model Inference. The keys of the dictionary are the module names, and the values are the devices to which they should be offloaded (e.g., 'cpu', 'disk', or a GPU index).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndevice_map = {\n    \"transformer.wte\": 0,\n    \"transformer.wpe\": 0,\n    \"transformer.drop\": 0,\n    \"transformer.h\": \"cpu\",\n    \"transformer.ln_f\": \"disk\",\n    \"lm_head\": \"disk\",\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Distributed Inference with torch.no_grad\nDESCRIPTION: This snippet runs inference on the parallelized model without tracking gradients, passing input arguments, and handling multiple arguments with unpacking. Dependencies include torch. It illustrates how to execute forward pass in a distributed environment.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nargs = some_more_arguments\nwith torch.no_grad():\n    output = model(*args)\n```\n\n----------------------------------------\n\nTITLE: Loading Model Weights with Accelerate\nDESCRIPTION: This demonstrates how to load model weights after training using Accelerate.  It unwraps the model first, then loads the state dictionary from the saved checkpoint.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nunwrapped_model = accelerator.unwrap_model(model)\npath_to_checkpoint = os.path.join(save_directory,\"pytorch_model.bin\")\nunwrapped_model.load_state_dict(torch.load(path_to_checkpoint))\n```\n\n----------------------------------------\n\nTITLE: Training Toy Causal Language Model with Huggingface Accelerate in Python\nDESCRIPTION: This self-contained Python example shows how to implement a toy causal language model training loop using the Huggingface Accelerate library with gradient accumulation. It includes dataset and dataloader definition using PyTorch Dataset and DataLoader, device placement management, logging, and a linear model optimized via SGD. The example manages multi-device synchronization and loss normalization, explicitly applying gradient accumulation steps and normalizing cross-entropy loss by the total number of non-padded tokens across devices. Dependencies include 'torch', 'accelerate', and standard Python modules. Inputs are custom generated sequences padded for batching; outputs are updated model parameters with tracked weights logged per device.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport copy\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom accelerate.logging import  get_logger\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport contexlib\n\n# seed\nset_seed(0)\nlogger = get_logger(__name__)\n\nclass MyDataset(Dataset):\n    def __init__(self, num_samples):\n        super().__init__()\n        self.len = num_samples\n\n    def __getitem__(self, index):\n        input_ids = torch.arange(1, index+2, dtype=torch.float32)\n        labels = torch.remainder(input_ids, 2)\n        return {\"input_ids\": input_ids, \"labels\": labels}\n\n    def __len__(self):\n        return self.len\n    \ndef collate_fn(features):\n    input_ids = torch.nn.utils.rnn.pad_sequence([f[\"input_ids\"] for f in features], batch_first=True, padding_value=-100)\n    labels = torch.nn.utils.rnn.pad_sequence([f[\"labels\"] for f in features], batch_first=True, padding_value=-100)\n    return {\"input_ids\": input_ids[..., None], \"labels\": labels[..., None]}\n\n# define toy inputs and labels\ngradient_accumulation_steps = 2\nper_device_batch_size = 4\n\n# define accelerator\naccelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n\n# define dataset and dataloader\n# for this toy example, we'll compute gradient descent over one single global batch\ndataset = MyDataset(per_device_batch_size*gradient_accumulation_steps*accelerator.num_processes)\ndataloader = DataLoader(dataset, batch_size=per_device_batch_size, collate_fn=collate_fn)\n\n# define model, model_optimizer and loss function\nmodel = torch.nn.Linear(1, 2, bias=False)\nmodel_clone = copy.deepcopy(model)\ncriterion = torch.nn.CrossEntropyLoss(reduction=\"sum\") # must sum over samples rather than averaging\nmodel_optimizer = torch.optim.SGD(model.parameters(), lr=0.08)\n\n\nlogger.warning(f\"initial model weight is {model.weight.detach().cpu().squeeze()}\")\nlogger.warning(f\"initial model clone weight is {model_clone.weight.detach().cpu().squeeze()}\")\n\n# prepare artifacts - accelerator handles device placement and dataloader splitting\nmodel, model_optimizer = accelerator.prepare(model, model_optimizer)\ndataloader = accelerator.prepare_data_loader(dataloader, device_placement=True)\ntraining_iterator = iter(dataloader)\n\nnum_samples_in_epoch = len(dataloader)\nremainder = num_samples_in_epoch % gradient_accumulation_steps\nremainder = remainder if remainder != 0 else gradient_accumulation_steps\ntotal_gradient_updates = math.ceil(num_samples_in_epoch / gradient_accumulation_steps)\n\ntotal_batched_samples = 0\nfor update_step in range(total_gradient_updates):\n        # In order to correctly the total number of non-padded tokens on which we'll compute the cross-entropy loss\n        # we need to pre-load the full local batch - i.e the next per_device_batch_size * accumulation_steps samples\n        batch_samples = []\n        num_batches_in_step = gradient_accumulation_steps if update_step != (total_gradient_updates - 1) else remainder\n        for _ in range(num_batches_in_step):\n            batch_samples += [next(training_iterator)]\n            \n        # get local num items in batch \n        local_num_items_in_batch = sum([(batch[\"labels\"].ne(-100)).sum() for batch in batch_samples])\n        logger.warning(f\"Step {update_step} - Device {accelerator.process_index} - num items in the local batch {local_num_items_in_batch}\", main_process_only=False)\n\n        # to compute it correctly in a multi-device DDP training, we need to gather the total number of items in the full batch.\n        num_items_in_batch = accelerator.gather(local_num_items_in_batch).sum().item()\n        logger.warning(f\"Total num items {num_items_in_batch}\")\n\n        for i, batch in enumerate(batch_samples):\n            inputs, labels = batch[\"input_ids\"], batch[\"labels\"]\n            total_batched_samples += 1\n            # if we perform gradient accumulation in a multi-devices set-up, we want to avoid unecessary communications when accumulating\n            # cf: https://muellerzr.github.io/blog/gradient_accumulation.html\n            if (i < len(batch_samples) - 1 and accelerator.num_processes > 1):\n                ctx = model.no_sync\n            else:\n                ctx = contextlib.nullcontext\n            with ctx():\n\n                outputs = model(inputs)\n                loss = criterion(outputs.view(-1, 2), labels.view(-1).to(torch.int64))\n                \n                # We multiply by num_processes because the DDP calculates the average gradient across all devices whereas dividing by num_items_in_batch already takes into account all devices\n                # Same reason for gradient_accumulation_steps, but this times it's Accelerate that calculate the average gradient across the accumulated steps \n                loss = (loss * gradient_accumulation_steps * accelerator.num_processes) / num_items_in_batch\n                accelerator.backward(loss)\n        model_optimizer.step()\n        model_optimizer.zero_grad()\n                \n\nlogger.warning(f\"Device {accelerator.process_index} - w/ accumulation, the final model weight is {accelerator.unwrap_model(model).weight.detach().cpu().squeeze()}\", main_process_only=False)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Launching Distributed Training with Accelerate CLI\nDESCRIPTION: Shows how to use the Accelerate CLI commands to configure, test, and launch distributed training scripts. These commands help set up the environment and run scripts across distributed hardware.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate test\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch path_to_script.py --args_for_the_script\n```\n\n----------------------------------------\n\nTITLE: Initializing FullyShardedDataParallelPlugin for Advanced FSDP Configuration in Python\nDESCRIPTION: This Python snippet shows how to instantiate `FullyShardedDataParallelPlugin` from the accelerate library to override or specify extra FSDP options beyond the CLI config. It imports required FSDP config classes from PyTorch, sets up `state_dict_config` and `optim_state_dict_config` with options to disable CPU offload and include all ranks, and then passes this plugin into the `Accelerator` constructor. This approach allows fine-grained control over FSDP behavior when launching accelerated training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/fsdp.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import FullyShardedDataParallelPlugin\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n\nfsdp_plugin = FullyShardedDataParallelPlugin(\n    state_dict_config=FullStateDictConfig(offload_to_cpu=False, rank0_only=False),\n    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=False, rank0_only=False),\n)\n\naccelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n```\n\n----------------------------------------\n\nTITLE: CPU Offloading with Hook Support for Iterative Processing in Python\nDESCRIPTION: This code demonstrates `cpu_offload_with_hook`, which offloads a model to CPU but allows the model to remain on the execution device after forward passes until an explicit offload call via a returned hook. It supports chaining hooks across multiple models for efficient pipeline loops. This method requires a model, an execution device, and optionally, a hook from a previous module to synchronize offloading. It is more performant but consumes more memory compared to standard CPU offloading.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel_1, hook_1 = cpu_offload_with_hook(model_1, execution_device)\nmodel_2, hook_2 = cpu_offload_with_hook(model_2, execution_device, prev_module_hook=hook_1)\nmodel_3, hook_3 = cpu_offload_with_hook(model_3, execution_device, prev_module_hook=hook_2)\n\nhid_1 = model_1(input)\nfor i in range(50):\n    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.\n    hid_2 = model_2(hid_1)\n# model2 is offloaded to the CPU just before this forward.\nhid_3 = model_3(hid_3)\n\n# For model3, you need to manually call the hook offload method.\nhook_3.offload()\n```\n\n----------------------------------------\n\nTITLE: Mixed Precision Training with Accelerate\nDESCRIPTION: This code shows how to enable mixed precision training with Accelerate. It requires setting the `mixed_precision` parameter during Accelerator initialization and using the `accelerator.autocast` context manager to automatically cast values to the specified data type.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\naccelerator = Accelerator(mixed_precision=\"fp16\")\nwith accelerator.autocast():\n    loss = complex_loss_function(outputs, target)\n```\n\n----------------------------------------\n\nTITLE: Preparing PyTorch Objects with Accelerate\nDESCRIPTION: This snippet demonstrates how to use the `accelerator.prepare` method to prepare PyTorch objects for distributed training. This involves adapting the model, optimizer, scheduler, and dataloader for the distributed environment.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed ZeRO-3 Configuration for Inference (JSON)\nDESCRIPTION: Example JSON configuration for DeepSpeed using ZeRO stage 3 optimized for inference. It enables bf16 automatically and sets parameters for ZeRO optimization, omitting optimizer and scheduler sections as the model is not being trained. The `train_micro_batch_size_per_gpu` is required even for inference.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"overlap_comm\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": \"auto\",\n        \"stage3_max_reuse_distance\": \"auto\",\n    },\n    \"train_micro_batch_size_per_gpu\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating Memory for Transformers Model\nDESCRIPTION: This command estimates the memory footprint of a model from the `transformers` library, explicitly specifying the library name. This is useful when the library cannot be determined automatically.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/model_size_estimator.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate estimate-memory HuggingFaceM4/idefics-80b-instruct --library_name transformers\n```\n\n----------------------------------------\n\nTITLE: Config Generated After accelerate config\nDESCRIPTION: This YAML code snippet shows an example of an Accelerate configuration file. This configuration is created automatically after using `accelerate config`. It references a DeepSpeed configuration file, sets the distributed type to DEEPSPEED, and specifies the number of processes.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  deepspeed_config_file: ds_config.json\n  zero3_init_flag: true\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config: {}\nmachine_rank: 0\nmain_training_function: main\nmegatron_lm_config: {}\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Custom Device Map Example\nDESCRIPTION: This code provides an example of how to create a custom `device_map`. The code creates a dictionary to manually specify which layers of the model should be placed on the CPU, GPU, or hard drive. The dictionary is then passed to the `load_checkpoint_and_dispatch` function.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_9\n\nLANGUAGE: py\nCODE:\n```\ndevice_map = {\n    \"transformer.wte\": \"cpu\",\n    \"transformer.wpe\": 0,\n    \"transformer.drop\": \"cpu\",\n    \"transformer.h.0\": \"disk\"\n}\n\nmodel = load_checkpoint_and_dispatch(\n    model, checkpoint=weights_location, device_map=device_map\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Megatron-LM GPT Model using Beam Search (Python)\nDESCRIPTION: This snippet demonstrates text generation using a Megatron-LM GPT model integrated with Accelerate via the `megatron_generate` method. It prepares batched input, tokenizes it, calls the generation function with beam search parameters (`num_beams`, `length_penalty`), and decodes the output tokens for printing. It requires access to a configured tokenizer, model, and Accelerate `accelerator` object.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nbatch_texts = [\"The purpose of life is\"]\nbatch_encodings = tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"],\n    batch_encodings[\"attention_mask\"],\n    max_new_tokens=max_new_tokens,\n    num_beams=20,\n    length_penalty=1.5,\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n```\n\n----------------------------------------\n\nTITLE: Loading and Dispatching Weights with Accelerate (Python)\nDESCRIPTION: This snippet illustrates how to load a checkpoint into a model initialized with empty weights and dispatch the weights across available devices using the `load_checkpoint_and_dispatch` function from the `accelerate` library. It uses the `device_map` parameter to automatically place model layers on different devices (GPU, CPU, or hard drive) based on available memory. The `no_split_module_classes` parameter prevents specified modules (e.g., those with residual connections) from being split across devices.  `model_checkpoint` needs to be replaced with the actual path.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import load_checkpoint_and_dispatch\n\nmodel_checkpoint = \"your-local-model-folder\"\nmodel = load_checkpoint_and_dispatch(\n    model, checkpoint=model_checkpoint, device_map=\"auto\", no_split_module_classes=['Block']\n)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading FSDP Model Checkpoints using Accelerate Utilities in Python\nDESCRIPTION: Demonstrates how to save and load model and optimizer states when training with FSDP using the Accelerate library's `save_state` and `load_state` methods. Saving is performed by calling `accelerator.save_state` with a checkpoint directory, which stores optimizer shards and model shards separately per process. Resuming training uses the corresponding `load_state`. Additionally, the snippet illustrates how to save a transformers model via `save_pretrained` with `state_dict` obtained from `accelerator.get_state_dict`. This workflow ensures correct handling of sharded state dicts in distributed environments.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/fsdp.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naccelerator.save_state(\"ckpt\")\n```\n\nLANGUAGE: python\nCODE:\n```\naccelerator.load_state(\"ckpt\")\n```\n\nLANGUAGE: python\nCODE:\n```\nunwrapped_model.save_pretrained(\n    args.output_dir,\n    is_main_process=accelerator.is_main_process,\n    save_function=accelerator.save,\n    state_dict=accelerator.get_state_dict(model),\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Code Execution on Main Process - Python\nDESCRIPTION: This code shows how to use `accelerator.is_main_process` to conditionally execute a block of code only on the main process across all machines. In this case, it's used to push a model to the Hub.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif accelerator.is_main_process:\n    repo.push_to_hub()\n```\n\n----------------------------------------\n\nTITLE: Initialize Accelerator for Basic FP8 Training in Python\nDESCRIPTION: Demonstrates the simplest way to enable FP8 mixed precision training by passing `mixed_precision=\"fp8\"` during `Accelerator` initialization. Accelerate will use the default backend (MS-AMP if available).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/low_precision_training.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\naccelerator = Accelerator(mixed_precision=\"fp8\")\n```\n\n----------------------------------------\n\nTITLE: Converting to Accelerate - PyTorch\nDESCRIPTION: This code snippet modifies the basic PyTorch training loop to use Accelerate.  It initializes the `Accelerator`, prepares the model, optimizer, dataloader, and scheduler using the `prepare` method, and uses `accelerator.backward()` for backpropagation. The inputs and targets are no longer sent to the device explicitly. This setup is a prerequisite for integrating Local SGD.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/local_sgd.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n\n+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n+     model, optimizer, training_dataloader, scheduler\n+ )\n\n  for index, batch in enumerate(training_dataloader):\n      inputs, targets = batch\n-     inputs = inputs.to(device)\n-     targets = targets.to(device)\n      outputs = model(inputs)\n      loss = loss_function(outputs, targets)\n      loss = loss / gradient_accumulation_steps\n+     accelerator.backward(loss)\n      if (index+1) % gradient_accumulation_steps == 0:\n          optimizer.step()\n          scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: DDP Hook with comm_wrapper in Accelerate\nDESCRIPTION: This snippet shows how to combine DDP communication hooks using the `comm_wrapper`. It uses the `PowerSGD` hook combined with `FP16` compression.  It configures `DistributedDataParallelKwargs` with `comm_hook` and `comm_wrapper` settings, and then trains using Accelerate. This requires Accelerate and PyTorch, and depends on `DDPCommunicationHookType`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\n# DDP Communication Hook setup\nddp_kwargs = DistributedDataParallelKwargs(\n    comm_hook=DDPCommunicationHookType.POWER_SGD,\n    comm_wrapper=DDPCommunicationHookType.FP16\n)\naccelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n\nmodel = MyModel()\noptimizer = torch.optim.Adam(model.parameters())\ndata_loader = DataLoader(dataset, batch_size=16)\n\nmodel, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Scaling Learning Rate According to Number of Processes in Accelerate (Python)\nDESCRIPTION: This code sample illustrates dynamically scaling the learning rate based on the number of distributed processes when using HuggingFace Accelerate. The snippet instantiates an Accelerator object, multiplies the initial learning rate by accelerator.num_processes to scale according to hardware, and demonstrates creating an AdamW optimizer from the PyTorch model parameters and adjusted learning rate. Dependencies include accelerate, torch, and a defined model. Use this pattern to preserve training dynamics when increasing batch size via more devices during distributed training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/performance.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlearning_rate = 1e-3\naccelerator = Accelerator()\nlearning_rate *= accelerator.num_processes\n\noptimizer = AdamW(params=model.parameters(), lr=learning_rate)\n\n```\n\n----------------------------------------\n\nTITLE: Modifying Training Script to Save Model at the Correct Directory\nDESCRIPTION: This Python code snippet demonstrates how to modify a training script to save the trained model to '/opt/ml/model' using the 'accelerator.save' method, which is necessary for SageMaker to capture the output artifacts.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/sagemaker.md#_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- torch.save('/opt/ml/model')\n+ accelerator.save('/opt/ml/model')\n```\n\n----------------------------------------\n\nTITLE: Complete Big Model Inference Example Accelerate Python\nDESCRIPTION: Provides a full example combining the steps for Big Model Inference using Accelerate: initializing with empty weights, loading and dispatching the checkpoint, and performing a forward pass. This demonstrates the core workflow for handling models larger than available GPU memory.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom accelerate import init_empty_weights, load_checkpoint_and_dispatch\n\nwith init_empty_weights():\n    model = MyModel(...)\n\nmodel = load_checkpoint_and_dispatch(\n    model, checkpoint=checkpoint_file, device_map=\"auto\"\n)\n\ninput = torch.randn(2,3)\ndevice_type = next(iter(model.parameters())).device.type\ninput = input.to(device_type)\noutput = model(input)\n```\n\n----------------------------------------\n\nTITLE: Defining a Distributed Training Loop with Huggingface Accelerate in Python\nDESCRIPTION: Defines a complete training loop function for an animal classification task using Huggingface Accelerate. The function sets seeds for reproducibility, initializes the Accelerator for mixed precision training, prepares dataloaders, and creates a resnet50d model with frozen backbone parameters except the classifier head. It normalizes input batches before forwarding through the model, sets up the Adam optimizer and OneCycleLR scheduler, and prepares all components with accelerator.prepare for distributed training. The training loop iterates over epochs and batches, computes loss, backpropagates with accelerator.backward, updates weights and scheduler, then evaluates accuracy with distributed gathering for metric calculation. Accelerator.print ensures output is shown only by the main process. Dependencies include PyTorch, torchvision, and Huggingface Accelerate. Inputs are batch dicts with images and labels; output is printed epoch accuracy.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef training_loop(mixed_precision=\"fp16\", seed: int = 42, batch_size: int = 64):\n    set_seed(seed)\n    # Initialize accelerator\n    accelerator = Accelerator(mixed_precision=mixed_precision)\n    # Build dataloaders\n    train_dataloader, eval_dataloader = get_dataloaders(batch_size)\n\n    # Instantiate the model (you build the model here so that the seed also controls new weight initializations)\n    model = create_model(\"resnet50d\", pretrained=True, num_classes=len(label_to_id))\n\n    # Freeze the base model\n    for param in model.parameters():\n        param.requires_grad = False\n    for param in model.get_classifier().parameters():\n        param.requires_grad = True\n\n    # You can normalize the batches of images to be a bit faster\n    mean = torch.tensor(model.default_cfg[\"mean\"])[None, :, None, None]\n    std = torch.tensor(model.default_cfg[\"std\"])[None, :, None, None]\n\n    # To make these constants available on the active device, set it to the accelerator device\n    mean = mean.to(accelerator.device)\n    std = std.to(accelerator.device)\n\n    # Instantiate the optimizer\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2 / 25)\n\n    # Instantiate the learning rate scheduler\n    lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-2, epochs=5, steps_per_epoch=len(train_dataloader))\n\n    # Prepare everything\n    # There is no specific order to remember, you just need to unpack the objects in the same order you gave them to the\n    # prepare method.\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # Now you train the model\n    for epoch in range(5):\n        model.train()\n        for batch in train_dataloader:\n            inputs = (batch[\"image\"] - mean) / std\n            outputs = model(inputs)\n            loss = torch.nn.functional.cross_entropy(outputs, batch[\"label\"])\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        model.eval()\n        accurate = 0\n        num_elems = 0\n        for batch in eval_dataloader:\n            inputs = (batch[\"image\"] - mean) / std\n            with torch.no_grad():\n                outputs = model(inputs)\n            predictions = outputs.argmax(dim=-1)\n            accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch[\"label\"])\n            num_elems += accurate_preds.shape[0]\n            accurate += accurate_preds.long().sum()\n\n        eval_metric = accurate.item() / num_elems\n        # Use accelerator.print to print only on the main process.\n        accelerator.print(f\"epoch {epoch}: {100 * eval_metric:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate with Conda - Bash\nDESCRIPTION: This snippet installs the Accelerate library using the conda package manager.  This method is useful when managing multiple Python environments and their dependencies.  Requires conda to be installed and configured, and the command specifies the conda-forge channel to find the package.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge accelerate\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed JSON Config Example\nDESCRIPTION: This JSON file provides a detailed DeepSpeed configuration, including settings for bf16, ZeRO optimization (stage, offload configuration), gradient clipping, batch size, and gradient accumulation steps. It can be passed in as the `deepspeed_config_file` in the accelerate config.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bf16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"stage3_gather_16bit_weights_on_model_save\": false,\n        \"offload_optimizer\": {\n            \"device\": \"none\"\n        },\n        \"offload_param\": {\n            \"device\": \"none\"\n        }\n    },\n    \"gradient_clipping\": 1.0,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"gradient_accumulation_steps\": 10,\n    \"steps_per_print\": 2000000\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Megatron-LM GPT Pre-training with Accelerate CLI in Bash\nDESCRIPTION: This bash command demonstrates how to launch a distributed GPT pre-training job using the `accelerate launch` CLI tool with a custom Megatron-LM config file and training script. It specifies model and tokenizer names, dataset details, batch sizes, learning rate, number of epochs, tracking options with Weights & Biases (`wandb`), and output directory. This command supports multi-GPU training using the parameters defined in the configuration YAML.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file megatron_gpt_config.yaml \\\nexamples/by_feature/megatron_lm_gpt_pretraining.py \\\n--config_name \"gpt2-large\" \\\n--tokenizer_name \"gpt2-large\" \\\n--dataset_name wikitext \\\n--dataset_config_name wikitext-2-raw-v1 \\\n--block_size 1024 \\\n--learning_rate 5e-5 \\\n--per_device_train_batch_size 24 \\\n--per_device_eval_batch_size 24 \\\n--num_train_epochs 5 \\\n--with_tracking \\\n--report_to \"wandb\" \\\n--output_dir \"awesome_model\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MegatronLMPlugin for ALiBi Positional Embedding (Python)\nDESCRIPTION: This snippet shows how to configure the `MegatronLMPlugin` to specify the ALiBi positional embedding type for Megatron-LM models. It involves setting the `position_embedding_type` key to \"alibi\" within the `other_megatron_args` dictionary, which is then passed to the plugin constructor. This configuration is part of the setup process before preparing the model with Accelerate.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nother_megatron_args = {\"position_embedding_type\": \"alibi\"}\nmegatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)\n```\n\n----------------------------------------\n\nTITLE: Simplifying Gradient Accumulation with Accelerator.accumulate (Python)\nDESCRIPTION: This snippet showcases Accelerate's high-level `accelerator.accumulate(model)` context manager. It automatically handles the conditional application of `no_sync` based on the current step and the configured gradient accumulation steps, greatly simplifying the training loop logic for gradient accumulation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/gradient_synchronization.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)\n\nfor batch in dataloader:\n    with accelerator.accumulate(model):\n        optimizer.zero_grad()\n        inputs, targets = batch\n        outputs = model(inputs)\n        loss = loss_function(outputs, targets)\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Load Model from Checkpoint\nDESCRIPTION: This Python code shows how to load a model state dictionary from a ZeRO checkpoint saved using `model.save_checkpoint()`. This function uses `load_state_dict_from_zero_checkpoint` from `deepspeed.utils.zero_to_fp32` and unwraps it to get the full 32-bit model. The process requires ~2x memory (general RAM) of the size of the final checkpoint.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\nunwrapped_model = accelerator.unwrap_model(model)\nfp32_model = load_state_dict_from_zero_checkpoint(unwrapped_model, checkpoint_dir)\n```\n\n----------------------------------------\n\nTITLE: Measuring Model FLOPS with HuggingFace Accelerate - Python\nDESCRIPTION: Profiles a model with HuggingFace Accelerate to calculate floating point operations (FLOPS) per layer or operator. Sets up profiling by passing ProfileKwargs with with_flops=True to an Accelerator instance, which wraps the model and enables FLOPS estimation with minimal code changes. Depends on Accelerate, PyTorch Profiler, a model, and its inputs. Input: model and profile settings; output: summary of operator-wise FLOPS printed as a table.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nprofile_kwargs = ProfileKwargs(\n    with_flops=True\n)\naccelerator = Accelerator(kwargs_handlers=[profile_kwargs])\n\nwith accelerator.profile() as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Initializing Empty Weights with Accelerate (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a model with empty weights using the `init_empty_weights` context manager from the `accelerate` library. It leverages `transformers` to load the configuration of a pre-trained model (e.g., Mixtral-8x7B) and creates a model skeleton on the meta device, reducing memory usage. The `AutoConfig` and `AutoModelForCausalLM` classes from `transformers` are used to load the model configuration and model, respectively.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import init_empty_weights\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nconfig = AutoConfig.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_config(config)\n```\n\n----------------------------------------\n\nTITLE: Specify FP8 Backend and Kwargs for Accelerator in Python\nDESCRIPTION: Shows how to explicitly select an FP8 backend (`MS-AMP`, `TransformersEngine`, or `torchao`) and provide backend-specific configurations using `kwarg_handlers` and the corresponding `RecipeKwargs` dataclass (`MSAMPRecipeKwargs`, `TERecipeKwargs`, `AORecipeKwargs`) when initializing the `Accelerator`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/low_precision_training.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom accelerate.utils import MSAMPRecipeKwargs, TERecipeKwargs, AORecipeKwargs\nkwargs = [MSAMPRecipeKwargs()]\n# Or to specify the backend as `TransformersEngine` even if MS-AMP is installed\n# kwargs = [TERecipeKwargs()]\n# Or to use torchao\n# kwargs = [AORecipeKwargs()]\naccelerator = Accelerator(mixed_precision=\"fp8\", kwarg_handlers=kwargs)\n```\n\n----------------------------------------\n\nTITLE: Configure MS-AMP Backend for FP8 Accelerator in Python\nDESCRIPTION: Illustrates configuring the MS-AMP backend for FP8 training in Python. It uses `FP8RecipeKwargs` to specify `backend=\"msamp\"` and sets the desired `optimization_level` (e.g., \"O2\"), passing these configurations to the `Accelerator` via `kwarg_handlers`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/low_precision_training.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom accelerate.utils import FP8RecipeKwargs\nkwargs = [FP8RecipeKwargs(backend=\"msamp\", optimization_level=\"O2\")]\naccelerator = Accelerator(mixed_precision=\"fp8\", kwarg_handlers=kwargs)\n```\n\n----------------------------------------\n\nTITLE: Configuring 4-bit Quantization\nDESCRIPTION: This code configures 4-bit quantization using `BnbQuantizationConfig`. The `load_in_4bit` parameter is set to `True` to enable 4-bit quantization. `bnb_4bit_compute_dtype` specifies the compute data type (e.g., bfloat16), `bnb_4bit_use_double_quant` enables double quantization, and `bnb_4bit_quant_type` specifies the quantization type (e.g., nf4). Requires the `torch` library to be installed.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import BnbQuantizationConfig\nbnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n```\n\n----------------------------------------\n\nTITLE: Command to Launch Training with accelerate CLI\nDESCRIPTION: Shows the shell command to launch a training script using the accelerate CLI tool, which respects the configuration file instead of ignoring it like notebook_launcher does.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch\n```\n\n----------------------------------------\n\nTITLE: Launching Training Function with notebook_launcher Python\nDESCRIPTION: This snippet demonstrates how to launch the `training_function` defined previously from a Jupyter notebook cell using `accelerate.notebook_launcher`. It takes the training function as an argument and initiates the multi-process execution, handling the setup for distributed training in a notebook environment. This is the standard way to launch Accelerate training from notebooks.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/training_tpu.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import notebook_launcher\n\nnotebook_launcher(training_function)\n\n```\n\n----------------------------------------\n\nTITLE: PowerSGD Hook (Accelerate)\nDESCRIPTION: This code snippet uses the PowerSGD hook within the Accelerate framework. It defines a model, sets up `DistributedDataParallelKwargs` with `DDPCommunicationHookType.POWER_SGD`, initializes an `Accelerator`, and prepares the training components (model, optimizer, and data loader). Then, it runs a typical training loop. Requires Accelerate and PyTorch.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\n# DDP Communication Hook setup\nddp_kwargs = DistributedDataParallelKwargs(comm_hook=DDPCommunicationHookType.POWER_SGD)\naccelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n\nmodel = MyModel()\noptimizer = torch.optim.Adam(model.parameters())\ndata_loader = DataLoader(dataset, batch_size=16)\n\nmodel, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Initiating Accelerate Configuration (Bash)\nDESCRIPTION: Runs the interactive configuration setup for HuggingFace Accelerate. This command prompts the user for details about their compute environment and desired distributed training setup, including options for DeepSpeed integration via the plugin method (answering 'no' to using a DeepSpeed config file).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Moving Model to Device with Accelerate\nDESCRIPTION: This code shows how to use the `accelerator.device` to move the model to the correct device.  The `accelerator.device` automatically determines the appropriate device (CPU, GPU, etc.) based on the distributed setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndevice = accelerator.device\nmodel.to(device)\n```\n\n----------------------------------------\n\nTITLE: Running the Python Script with GPU Count using accelerate launch\nDESCRIPTION: This command launches a Python script using accelerate launch, specifying the number of processes to be used, and thus GPUs. This command is a variation of the previous command that allows the user to explicitly determine the number of GPUs to be used with {NUM_GPUS} being the number of GPUs to use. It is still using the accelerate framework for parallel processing. The script takes no parameters but requires the correct path to the python file.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/inference/pippy/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --num_processes {NUM_GPUS} bert.py\n```\n\n----------------------------------------\n\nTITLE: Save Model with ZeRO Stage-3\nDESCRIPTION: This code snippet illustrates how to save a model in ZeRO stage 3. It utilizes `accelerator.unwrap_model` to access the underlying model. If the DeepSpeed config or plugin is configured to save the full model weights, it saves the model using `save_pretrained`. Otherwise, saves checkpoints by calling `model.save_checkpoint()`, which will allow for future offline consolidation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nunwrapped_model = accelerator.unwrap_model(model)\n\n# New Code #\n# Saves the whole/unpartitioned fp16 model when in ZeRO Stage-3 to the output directory if\n# `stage3_gather_16bit_weights_on_model_save` is True in DeepSpeed Config file or\n# `zero3_save_16bit_model` is True in DeepSpeed Plugin.\n# For Zero Stages 1 and 2, models are saved as usual in the output directory.\n# The model name saved is `pytorch_model.bin`\nunwrapped_model.save_pretrained(\n    args.output_dir,\n    is_main_process=accelerator.is_main_process,\n    save_function=accelerator.save,\n    state_dict=accelerator.get_state_dict(model),\n)\n```\n\n----------------------------------------\n\nTITLE: Launching FSDP2 Benchmark with Torchrun (Multi-GPU) - Bash\nDESCRIPTION: Runs the benchmark script, main.py, using torchrun for distributed training across two GPUs. Requires PyTorch (with FSDP2 support) and at least two GPUs available. This launches main.py in a distributed manner suitable for scaling benchmarks and observing memory/convergence effects under multiple processes.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fsdp2/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node 2 main.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Scheduled Profiling with HuggingFace Accelerate - Python\nDESCRIPTION: Shows profiling of long-running jobs using Accelerate's profiling API. Demonstrates specifying schedule options and a trace handler, using ProfileKwargs to integrate profiler settings with an Accelerator instance. Assumes an instantiated model and inputs, as well as required Accelerate and PyTorch dependencies. Inputs include a model and schedule configuration; outputs are profile step data and exported traces.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nprofile_kwargs = ProfileKwargs(\n    activities=[\"cpu\", \"cuda\"],\n    schedule_option={\"wait\": 5, \"warmup\": 1, \"active\": 3, \"repeat\": 2, \"skip_first\": 1},\n    on_trace_ready=trace_handler\n)\n\naccelerator = Accelerator(kwargs_handlers=[profile_kwargs])\nmodel = accelerator.prepare(model)\n\nwith accelerator.profile() as prof:\n    for idx in range(8):\n        model(inputs)\n        prof.step()\n```\n\n----------------------------------------\n\nTITLE: Example Accelerate Config for DeepSpeed ZeRO Stage-2 (YAML)\nDESCRIPTION: Sample Accelerate configuration file content (typically `default_config.yaml`) generated via `accelerate config` for using the DeepSpeed plugin with ZeRO Stage-2. It specifies settings like local machine environment, DeepSpeed distributed type, gradient accumulation/clipping, FP16 mixed precision, and disables optimizer/parameter offloading.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n gradient_accumulation_steps: 1\n gradient_clipping: 1.0\n offload_optimizer_device: none\n offload_param_device: none\n zero3_init_flag: true\n zero_stage: 2\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Analyzing Memory Consumption with Accelerate's Profiler\nDESCRIPTION: Example showing how to profile memory usage of a ResNet-18 model using Accelerate's profiler wrapper, analyzing the memory allocation patterns of model operators.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nprofile_kwargs = ProfileKwargs(\n    activities=[\"cpu\"],\n    profile_memory=True,\n    record_shapes=True\n)\n\naccelerator = Accelerator(cpu=True, kwargs_handlers=[profile_kwargs])\nmodel = accelerator.prepare(model)\n\nwith accelerator.profile() as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch DistributedDataParallel (Python)\nDESCRIPTION: This snippet demonstrates the standard method for wrapping a PyTorch model using `torch.nn.parallel.DistributedDataParallel` to enable distributed training. It requires importing `torch.nn` and `DistributedDataParallel`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/gradient_synchronization.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\n\nmodel = nn.Linear(10, 10)\nddp_model = DistributedDataParallel(model)\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Distributed Inference with Manual Rank Checking in Python\nDESCRIPTION: Defines a function `run_inference` that demonstrates basic distributed inference. It initializes a process group using `torch.distributed`, assigns the model pipeline to the specific GPU rank, and manually checks the process rank (`torch.distributed.get_rank()`) to assign different prompts to different processes. The results are saved to rank-specific files.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef run_inference(rank, world_size):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    pipe.to(rank)\n\n    if torch.distributed.get_rank() == 0:\n        prompt = \"a dog\"\n    elif torch.distributed.get_rank() == 1:\n        prompt = \"a cat\"\n\n    result = pipe(prompt).images[0]\n    result.save(f\"result_{rank}.png\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Empty PyTorch Model Accelerate Python\nDESCRIPTION: Demonstrates how to initialize a PyTorch model skeleton without loading any weights into memory using Accelerate's `init_empty_weights` context manager. This is the first step in the Big Model Inference workflow when the model is too large for standard loading.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import init_empty_weights\nwith init_empty_weights():\n    my_model = ModelClass(...)\n```\n\n----------------------------------------\n\nTITLE: Using set_trigger and check_trigger for early stopping in training\nDESCRIPTION: This code snippet shows how to implement early stopping conditions that work across distributed processes. The set_trigger method is called when a stopping criterion (like loss threshold) is met, and check_trigger is used to determine if any process has triggered the condition, allowing synchronized breakpoints.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/deferring_execution.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfor (x,y) in data_loader:\n    logits = model(x)\n    loss = loss_func(logits, y)\n    # Custom early stopping condition\n    if should_do_early_stopping(loss):\n        accelerator.set_trigger()\n\n    # Check if any process has triggered early stopping\n    if accelerator.check_trigger():\n        break\n```\n\n----------------------------------------\n\nTITLE: Extracting Label from Filename Using Regex in Python\nDESCRIPTION: Defines a helper function `extract_label` which takes a filename string, isolates the base filename, and uses a regular expression to extract the label portion preceding an underscore and trailing numeric characters. The function returns the label string, enabling consistent label extraction from filenames that follow the pattern `<label>_\\d+.jpg`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\n\ndef extract_label(fname):\n    stem = fname.split(os.path.sep)[-1]\n    return re.search(r\"^(.*)_\\d+\\.jpg$\", stem).groups()[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Big Model with Transformers device_map auto Python\nDESCRIPTION: Demonstrates enabling Big Model Inference directly through the Hugging Face Transformers library's `from_pretrained` method by setting `device_map=\"auto\"`. This simplifies the process, handling empty initialization and dispatching internally for compatible models.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Saving Model State with Megatron-LM using Huggingface Accelerate in Python\nDESCRIPTION: This snippet details saving the trained model state conditionally based on whether Megatron-LM is used. If using Megatron-LM, the centralized `accelerator.save_state` method stores the training state at the specified output directory. Otherwise, the unwrapped model's `save_pretrained` function is called with proper arguments for main process and save function. This differentiation aligns with Megatron-LM's distributed training architecture.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    accelerator.save_state(args.output_dir)\nelse:\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\n        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n    )\n```\n\n----------------------------------------\n\nTITLE: Building Full Paths List of Image Filenames in Python\nDESCRIPTION: Creates an updated list of complete file paths for image files filtered by the '.jpg' extension. It combines the relative path `../../images` with each filename from the initial listing, ensuring compatibility with loading routines that require absolute or relative full paths rather than just filenames.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfnames = [os.path.join(\"../../images\", fname) for fname in fnames if fname.endswith(\".jpg\")]\n```\n\n----------------------------------------\n\nTITLE: Performing Inference after Dispatch Accelerate Python\nDESCRIPTION: Shows how to perform inference with a model that has been loaded and dispatched across multiple devices. The input tensor is moved to the device type of the model's parameters (indicating where the currently active layer resides) before passing it through the model.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninput = torch.randn(2,3)\ndevice_type = next(iter(model.parameters())).device.type\ninput = input.to(device_type)\noutput = model(input)\n```\n\n----------------------------------------\n\nTITLE: Simplifying Distributed Inference with Accelerate `split_between_processes` in Python\nDESCRIPTION: Illustrates using Accelerate's `PartialState` (or `Accelerator`/`AcceleratorState`) and its `split_between_processes` context manager to simplify distributed inference. It automatically distributes the list of prompts (`[\"a dog\", \"a cat\"]`) across the available processes, assigns the pipeline to the device managed by `distributed_state`, and runs inference on the assigned prompt(s). This eliminates the need for manual rank checking for prompt distribution.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom accelerate import PartialState  # Can also be Accelerator or AcceleratorState\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\ndistributed_state = PartialState()\npipe.to(distributed_state.device)\n\n# Assume two processes\nwith distributed_state.split_between_processes([\"a dog\", \"a cat\"]) as prompt:\n    result = pipe(prompt).images[0]\n    result.save(f\"result_{distributed_state.process_index}.png\")\n```\n\n----------------------------------------\n\nTITLE: accelerate launch Output Example\nDESCRIPTION: This Bash output demonstrates the execution of `accelerate launch` using DeepSpeed. The output shows the distributed environment, backend, number of processes, device, mixed precision type, and the DeepSpeed configuration loaded. It's used to verify that the launch command with accelerate, and ds_config is correctly configured.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nDistributed environment: DEEPSPEED  Backend: nccl\nNum processes: 4\nProcess index: 0\nLocal process index: 0\nDevice: cuda:0\nMixed precision type: bf16\nds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'stage3_gather_16bit_weights_on_model_save': False, 'offload_optimizer': {'device': 'none'}, 'offload_param': {'device': 'none'}}, 'gradient_clipping': 1.0, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 10, 'steps_per_print': inf, 'fp16': {'enabled': False}}\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerator with Multiple DeepSpeed Plugins in Python\nDESCRIPTION: Shows how to initialize the Accelerate `Accelerator` by passing the dictionary of `DeepSpeedPlugin` instances created previously. This enables Accelerate to manage multiple DeepSpeed configurations.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(deepspeed_plugins=deepspeed_plugins)\n```\n\n----------------------------------------\n\nTITLE: Adapting PyTorch Training Code for Distributed Environments\nDESCRIPTION: Demonstrates the minimal changes needed to adapt standard PyTorch training code to work with Accelerate for distributed training. The key modifications include initializing the Accelerator, preparing PyTorch objects, and using accelerator.backward() for loss backpropagation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\naccelerator = Accelerator()\n\ndevice = accelerator.device\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\n\nfor batch in training_dataloader:\n    optimizer.zero_grad()\n    inputs, targets = batch\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Custom Manual Device Map Creation in Python\nDESCRIPTION: These Python snippets illustrate how to manually create device maps for a model by mapping module names to specific devices (GPUs or CPU). The device map dictionary keys correspond to module components (e.g., model blocks or layers) and values indicate the device identifier. Users must ensure the mapping covers the entire model hierarchy to avoid incomplete device assignments. The examples show a valid coarse-grained map assigning entire blocks to devices and a finer-grained device map distributing internal submodules across GPUs.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndevice_map = {\"block1\": 0, \"block2\": 1}\n```\n\nLANGUAGE: python\nCODE:\n```\ndevice_map = {\"block1\": 0, \"block2.linear1\": 0, \"block2.linear2\": 1, \"block2.linear3\": 1}\n```\n\n----------------------------------------\n\nTITLE: Basic Gradient Accumulation in PyTorch\nDESCRIPTION: A standard PyTorch implementation of gradient accumulation that updates the model parameters every 2 batches. This approach works but isn't optimized for distributed training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cuda\"\nmodel.to(device)\n\ngradient_accumulation_steps = 2\n\nfor index, batch in enumerate(training_dataloader):\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss = loss / gradient_accumulation_steps\n    loss.backward()\n    if (index + 1) % gradient_accumulation_steps == 0:\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Saving Sharded Checkpoint with Accelerate\nDESCRIPTION: This snippet illustrates how to save a model as a sharded checkpoint or in the safetensors format using Accelerate. It sets `safe_serialization=True` to save the model in safetensor format.  The `max_shard_size` argument specifies the maximum size of each shard.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\naccelerator.wait_for_everyone()\naccelerator.save_model(model, save_directory, max_shard_size=\"1GB\", safe_serialization=True)\n```\n\n----------------------------------------\n\nTITLE: Training Loop Using Huggingface Accelerate in Python\nDESCRIPTION: This snippet implements the core training loop for a PyTorch model, leveraging Huggingface Accelerate to manage backward propagation and support distributed training with optimized efficiency. It requires initialization of a data loader, model, loss criterion, optimizer, and an accelerator instance. Each batch from the data loader is fed into the model to compute outputs, which are compared against targets to calculate loss. The accelerator handles gradient calculation during backpropagation with the accelerator.backward() method. The optimizer updates model parameters through optimizer.step(), and gradients are reset with optimizer.zero_grad() to prepare for the next iteration. The snippet highlights best practices for training in a distributed or accelerated environment but does not include explicit model or accelerator setup, which should be handled externally.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Preparing a Model with Accelerate (Python Diff)\nDESCRIPTION: This diff illustrates how to adapt code from using PyTorch's `DistributedDataParallel` directly to using Hugging Face Accelerate. It shows importing `Accelerator`, instantiating it, and replacing the `DistributedDataParallel` wrapper with a call to `accelerator.prepare(model)`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/gradient_synchronization.md#_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n  import torch.nn as nn\n- from torch.nn.parallel import DistributedDataParallel\n\n  model = nn.Linear(10,10)\n+ model = accelerator.prepare(model)\n```\n\n----------------------------------------\n\nTITLE: Launching Accelerate with PyTorch MPS Backend on Mac (Bash)\nDESCRIPTION: This Bash snippet demonstrates how to launch a computer vision example script using Hugging Face Accelerate on an MPS-enabled Apple Silicon Mac. It assumes that you have installed torch with MPS support and have the 'accelerate' CLI available. The '--data_dir' argument specifies the directory containing your images for training or inference. Replace '/examples/cv_example.py' with the path to your actual script as needed.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/mps.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch /examples/cv_example.py --data_dir images\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Megatron-LM GPT Text Generation with megatron_generate - Python\nDESCRIPTION: This snippet details configuring the Megatron-LM GPT plugin with tokenizer vocab and merges files, then demonstrates batch text generation using the megatron_generate function supporting top-p, top-k, and add_BOS options. It covers input encoding, sampling configuration, and decoding outputs. Requires availability of MegatronLMPugin, model, and tokenizer as well as checkpoint resume paths. Inputs are encoded prompt tensors and sampling parameters; outputs are generated token IDs and decoded text. Limitation is that only a subset of features from transformers.generate is supported and certain settings for parallelism are required.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# specifying tokenizer's vocab and merges file\nvocab_file = os.path.join(args.resume_from_checkpoint, \"vocab.json\")\nmerge_file = os.path.join(args.resume_from_checkpoint, \"merges.txt\")\nother_megatron_args = {\"vocab_file\": vocab_file, \"merge_file\": merge_file}\nmegatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)\n\n# inference using `megatron_generate` functionality\ntokenizer.pad_token = tokenizer.eos_token\nmax_new_tokens = 64\nbatch_texts = [\n    \"Are you human?\",\n    \"The purpose of life is\",\n    \"The arsenal was constructed at the request of\",\n    \"How are you doing these days?\",\n]\nbatch_encodings = tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\n\n# top-p sampling\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"],\n    batch_encodings[\"attention_mask\"],\n    max_new_tokens=max_new_tokens,\n    top_p=0.8,\n    top_p_decay=0.5,\n    temperature=0.9,\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n\n# top-k sampling\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"],\n    batch_encodings[\"attention_mask\"],\n    max_new_tokens=max_new_tokens,\n    top_k=50,\n    temperature=0.9,\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n\n# adding `bos` token at the start\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"], batch_encodings[\"attention_mask\"], max_new_tokens=max_new_tokens, add_BOS=True\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n```\n\n----------------------------------------\n\nTITLE: Converting Standard PyTorch to Accelerate Without Gradient Accumulation Helper\nDESCRIPTION: Basic conversion of PyTorch code to use Accelerate without utilizing the special gradient accumulation utilities. This version still doesn't perform gradient accumulation efficiently in distributed settings.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n\n+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n+     model, optimizer, training_dataloader, scheduler\n+ )\n\n  for index, batch in enumerate(training_dataloader):\n      inputs, targets = batch\n-     inputs = inputs.to(device)\n-     targets = targets.to(device)\n      outputs = model(inputs)\n      loss = loss_function(outputs, targets)\n      loss = loss / gradient_accumulation_steps\n+     accelerator.backward(loss)\n      if (index+1) % gradient_accumulation_steps == 0:\n          optimizer.step()\n          scheduler.step()\n          optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Running CV example on single CPU\nDESCRIPTION: These commands demonstrate how to run the `cv_example.py` script on a single CPU. The script requires a `--data_dir` argument. The first option is used from a server without a GPU, the second passes `cpu=True` to the `Accelerator`, and the third uses the `accelerate launch` command with the `--cpu` flag.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython ./cv_example.py --data_dir path_to_data\n```\n\nLANGUAGE: bash\nCODE:\n```\npython ./cv_example.py --data_dir path_to_data --cpu\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --cpu ./cv_example.py --data_dir path_to_data\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate from PyPI - Bash\nDESCRIPTION: This snippet installs the Accelerate library from PyPI using the pip package manager. It's a standard way to get the stable release of the library.  Requires Python and pip to be installed and configured on the system.  The command downloads and installs the latest released version from the Python Package Index (PyPI).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install accelerate\n```\n\n----------------------------------------\n\nTITLE: Example Knowledge Distillation Training Loop in Python\nDESCRIPTION: Provides a basic training loop structure for knowledge distillation using Accelerate and multiple DeepSpeed plugins. It shows getting outputs from both teacher (in `eval` mode with `no_grad`) and student models, combining losses, performing backward pass via `accelerator.backward()`, and updating optimizer/scheduler for the student model.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nteacher_model.eval()\nstudent_model.train()\nfor batch in train_dataloader:\n    with torch.no_grad():\n        output_teacher = teacher_model(**batch)\n    output_student = student_model(**batch)\n    # Combine the losses or modify it in some way\n    loss = output_teacher.loss + output_student.loss\n    accelerator.backward(loss)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Using Accelerate Config Default Command (Bash)\nDESCRIPTION: Demonstrates the basic usage of the `accelerate config default` command with potential arguments. This command creates a default Accelerate configuration file with basic settings without interactive prompts. It supports specifying a custom config file path and setting mixed precision options (no, fp16, bf16).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config default [arguments]\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Config with Auto Values\nDESCRIPTION: This JSON file represents a DeepSpeed configuration with values set to 'auto'. This configuration is used with the `accelerate launch` command, and allows values to be dynamically determined based on the environment and command-line arguments, when applicable.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"zero_optimization\": {\n        \"stage\": \"auto\",\n        \"stage3_gather_16bit_weights_on_model_save\": \"auto\",\n        \"offload_optimizer\": {\n            \"device\": \"auto\"\n        },\n        \"offload_param\": {\n            \"device\": \"auto\"\n        }\n    },\n    \"gradient_clipping\": \"auto\",\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"gradient_accumulation_steps\": \"auto\",\n    \"steps_per_print\": 2000000\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Distributed Training Launch Using notebook_launcher\nDESCRIPTION: Example output illustrating the progress printed by notebook_launcher during distributed training on 2 GPUs. Reflects epoch-wise accuracy percentages printed only by the main process. Demonstrates typical formatted logs during training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nLaunching training on 2 GPUs.\nepoch 0: 88.12\nepoch 1: 91.73\nepoch 2: 92.58\nepoch 3: 93.90\nepoch 4: 94.71\n```\n\n----------------------------------------\n\nTITLE: Launching Script with Accelerate Configuration - Bash\nDESCRIPTION: Shows the general command structure for launching a training script using `accelerate launch`. When an Accelerate configuration (potentially linking to a DeepSpeed config) exists, `accelerate launch` automatically applies these settings to the script execution.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch my_script.py --args_to_my_script\n```\n\n----------------------------------------\n\nTITLE: Example DeepSpeed Config for ZeRO Stage 2 - JSON\nDESCRIPTION: Presents a sample DeepSpeed configuration file content in JSON format, specifically tailored for ZeRO Stage 2 optimization. It includes settings for FP16, optimizer, scheduler, and ZeRO Stage 2 specific parameters, illustrating the use of `auto` values for certain fields handled by Accelerate's `prepare` method.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\",\n            \"torch_adam\": true,\n            \"adam_w_mode\": true\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"contiguous_gradients\": true\n    },\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT2 Model for Sequence Classification with Transformers\nDESCRIPTION: This code snippet shows how to create and prepare a GPT2 model for sequence classification tasks using the Transformers library. It initializes the model configuration, creates the model instance, and sets it to evaluation mode. Dependencies include the transformers library.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom transformers import GPT2ForSequenceClassification, GPT2Config\n\nconfig = GPT2Config()\nmodel = GPT2ForSequenceClassification(config)\nmodel.eval()\n```\n\n----------------------------------------\n\nTITLE: Using `accelerate test` Command (Bash)\nDESCRIPTION: Illustrates how to execute the `accelerate test` command. This command runs a test script (`accelerate/test_utils/test_script.py`) to verify that the Accelerate library is correctly installed and configured on the system. It accepts optional arguments, such as specifying a custom configuration file path.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate test [arguments]\n```\n\n----------------------------------------\n\nTITLE: Logging with Accelerate\nDESCRIPTION: This snippet demonstrates how to use Accelerate's logging utility for synchronized logging in a distributed setup. It replaces the standard Python `logging` module and allows setting log levels. It also shows how to control which processes the logs are written on, including all processes and ensuring that they are in order.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/troubleshooting.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.logging import get_logger\n\nlogger = get_logger(__name__, log_level=\"DEBUG\")\n# log all processes\nlogger.debug(\"thing_to_log\", main_process_only=False)\n# log all processes in order\nlogger.debug(\"thing_to_log\", main_process_only=False, in_order=True)\n```\n\n----------------------------------------\n\nTITLE: Running Checkpointing Example with Accelerate\nDESCRIPTION: Command for launching the checkpointing example script with epoch-based checkpointing and resuming from a previous checkpoint.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./checkpointing.py --checkpointing_steps epoch output_dir \"checkpointing_tutorial\" --resume_from_checkpoint \"checkpointing_tutorial/epoch_0\"\n```\n\n----------------------------------------\n\nTITLE: Estimating Memory for TIMM Model\nDESCRIPTION: This command estimates the memory footprint of a model from the `timm` library, explicitly specifying the library name using the `--library_name` argument.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/model_size_estimator.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate estimate-memory timm/resnet50.a1_in1k --library_name timm\n```\n\n----------------------------------------\n\nTITLE: Gradient Accumulation with Accelerate\nDESCRIPTION: This snippet demonstrates how to use gradient accumulation with Accelerate.  It requires setting the `gradient_accumulation_steps` parameter during Accelerator initialization and using the `accelerator.accumulate` context manager within the training loop.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naccelerator = Accelerator(gradient_accumulation_steps=2)\nmodel, optimizer, training_dataloader = accelerator.prepare(model, optimizer, training_dataloader)\n\nfor input, label in training_dataloader:\n    with accelerator.accumulate(model):\n        predictions = model(input)\n        loss = loss_function(predictions, label)\n        accelerator.backward(loss)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Function Execution on Local Main Process - Python\nDESCRIPTION: This code defines a function `do_my_thing` and decorates it with `@accelerator.on_local_main_process`. This ensures the function is only executed once per server, on the local main process.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@accelerator.on_local_main_process\ndef do_my_thing():\n    \"Something done once per server\"\n    do_thing_once_per_server()\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerator with BF16 Mixed Precision Python\nDESCRIPTION: This snippet demonstrates how to initialize the `accelerate.Accelerator` object to enable `bf16` mixed precision when training on TPUs. Passing the `mixed_precision=\"bf16\"` argument configures the environment, typically by setting `XLA_USE_BF16` to `1`, which casts `torch.float` and `torch.double` tensors to `bfloat16` on the TPU device by default.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/training_tpu.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naccelerator = Accelerator(mixed_precision=\"bf16\")\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Accelerate's Gradient Accumulation Support\nDESCRIPTION: Configuring the Accelerator to handle gradient accumulation by specifying the number of steps to accumulate gradients over before updating model parameters.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n  from accelerate import Accelerator\n- accelerator = Accelerator()\n+ accelerator = Accelerator(gradient_accumulation_steps=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate for SageMaker using CLI\nDESCRIPTION: This bash code snippet runs 'accelerate config' to generate a configuration file for SageMaker, prompting users to specify their compute environment, which is stored in a YAML file for subsequent training runs.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/sagemaker.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n# In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 1\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerator with BF16 Mixed Precision and Downcasting Python\nDESCRIPTION: This snippet shows how to initialize the `accelerate.Accelerator` object with `bf16` mixed precision and downcasting enabled for TPUs. Setting `mixed_precision=\"bf16\"` and `downcast_bf16=True` configures the environment to set `XLA_DOWNCAST_BF16` to `1`, resulting in `torch.float` tensors being `bfloat16` and `torch.double` tensors remaining `float32` on the TPU device. This is useful for operations like metrics calculation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/training_tpu.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\naccelerator = Accelerator(mixed_precision=\"bf16\", downcast_bf16=True)\n\n```\n\n----------------------------------------\n\nTITLE: Running Code Quality Checks using Make\nDESCRIPTION: Command using `make` to execute various custom scripts that check for common coding mistakes and quality issues. Running this helps catch potential problems before submitting a PR.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\n$ make quality\n```\n\n----------------------------------------\n\nTITLE: Using Accelerate `split_between_processes` with Padding in Python\nDESCRIPTION: Shows how to use the `split_between_processes` context manager with the `apply_padding=True` argument. This is useful when the number of data items is not evenly divisible by the number of processes. Padding ensures each process receives a list of the same length by duplicating the last item on processes with fewer original items, facilitating operations like gathering results.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom accelerate import PartialState  # Can also be Accelerator or AcceleratorState\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\ndistributed_state = PartialState()\npipe.to(distributed_state.device)\n\n# Assume two processes\nwith distributed_state.split_between_processes([\"a dog\", \"a cat\", \"a chicken\"], apply_padding=True) as prompt:\n    result = pipe(prompt).images\n```\n\n----------------------------------------\n\nTITLE: Checking if the Current Process is the Last in the Pipeline\nDESCRIPTION: This code checks whether the current process is the last in the pipeline, and if so, outputs the inference results. It uses accelerate's PartialState for process management. Dependencies include accelerate.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom accelerate import PartialState\nif PartialState().is_last_process:\n    print(output)\n```\n\n----------------------------------------\n\nTITLE: Initializing Megatron-LM Dummy DataLoader for Indexed Datasets - Python\nDESCRIPTION: This snippet illustrates the initialization and configuration of the MegatronLMDummyDataLoader for loading Megatron-LM indexed datasets within Accelerate, supporting tensor parallelism scenarios. The dataloader is instantiated with parameters such as data path, split string, sequence length, and batch size, with integration into the MegatronLM plugin state. Prerequisites include Megatron-LM compatible dataset files and correct argument structures. Inputs are dataset configuration dictionaries; output is a dataloader instance integrated into the plugin state. It is limited to compatible distributed setups where only specific tensor parallel ranks have access to the dataloader.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import MegatronLMDummyDataLoader\n\nmegatron_dataloader_config = {\n    \"data_path\": args.data_path,\n    \"splits_string\": args.splits_string,\n    \"seq_length\": args.block_size,\n    \"micro_batch_size\": args.per_device_train_batch_size\n}\nmegatron_dataloader = MegatronLMDummyDataLoader(**megatron_dataloader_config)\naccelerator.state.megatron_lm_plugin.megatron_dataset_flag = True\n```\n\n----------------------------------------\n\nTITLE: Function Execution on a Local Process - Python\nDESCRIPTION: This code shows how to use the `@accelerator.on_local_process` decorator to execute a function only on a specific local process, identified by its `local_process_idx`. Here, `do_my_thing` is executed on process index 0 on each server.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@accelerator.on_local_process(local_process_idx=0)\ndef do_my_thing():\n    \"Something done on process index 0 on each server\"\n    do_thing_on_index_zero_on_each_server()\n```\n\n----------------------------------------\n\nTITLE: Formatting Function/Method Arguments in Docstrings (Google Style)\nDESCRIPTION: Illustrates the Google documentation style format for defining arguments within docstrings in Markdown files, intended for processing by `doc-builder`. Examples show the `Args:` section, indentation, type hints (e.g., `int`, `str`, `bool`), argument names, descriptions, handling long descriptions, marking optional arguments (`*optional*`), and specifying default values (`defaults to X`), omitting the default when it's `None`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n    Args:\n        n_layers (`int`): The number of layers of the model.\n```\n\nLANGUAGE: markdown\nCODE:\n```\n    Args:\n        gradient_accumulation_steps (`int`, *optional*, default to 1):\n            The number of steps that should pass before gradients are accumulated. A number > 1 should be combined with `Accelerator.accumulate`.\n        cpu (`bool`, *optional*):\n            Whether or not to force the script to execute on CPU. Will ignore GPU available if set to `True` and force the execution on one process only.\n```\n\nLANGUAGE: markdown\nCODE:\n```\n    Args:\n        x (`str`, *optional*):\n            This argument controls ... and has a description longer than 119 chars.\n        a (`float`, *optional*, defaults to 1):\n            This argument is used to ... and has a description longer than 119 chars.\n```\n\n----------------------------------------\n\nTITLE: Using Accelerate's GradientAccumulationPlugin with Custom Configuration\nDESCRIPTION: Configuring gradient accumulation behavior by using the GradientAccumulationPlugin with the sync_with_dataloader parameter set to False to prevent automatic syncing at the end of the dataloader iteration.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom accelerate.utils import GradientAccumulationPlugin\n\nplugin = GradientAccumulationPlugin(sync_with_dataloader=False)\naccelerator = Accelerator(..., gradient_accumulation_plugin=plugin)\n```\n\n----------------------------------------\n\nTITLE: Measuring Model FLOPS with PyTorch Profiler - Python\nDESCRIPTION: Demonstrates profiling a model in PyTorch to estimate floating point operations (FLOPS) for key operators such as matrix multiplication and convolution. The profile function is used with with_flops=True, and after running the model, performance statistics sorted by FLOPS are printed. Requires PyTorch Profiler and the target model and inputs. Input is the model and data; output is a printed summary table with FLOPS per operation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    with_flops=True\n) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Previewing Documentation Locally using doc-builder\nDESCRIPTION: Shows the command syntax for previewing documentation locally using `doc-builder`. Requires the `watchdog` library. The first command shows the generic syntax, while the second provides a specific example for the `accelerate` package. This starts a local web server (usually at http://localhost:3000) to view the docs.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndoc-builder preview {package_name} {path_to_docs}\n```\n\nLANGUAGE: bash\nCODE:\n```\ndoc-builder preview accelerate docs/source/\n```\n\n----------------------------------------\n\nTITLE: Example Accelerate Config for DeepSpeed Stage 2 - YAML\nDESCRIPTION: Provides a sample Accelerate configuration file content in YAML format. This configuration demonstrates how to specify `distributed_type: DEEPSPEED`, reference a `deepspeed_config_file`, and include other DeepSpeed-related settings like `zero3_init_flag` for ZeRO Stage 2.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n deepspeed_config_file: /home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage2_config.json\n zero3_init_flag: true\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Debugging with Accelerate (Config File)\nDESCRIPTION: This snippet shows how to enable Accelerate's debug mode via the `config.yaml` file. The `debug: true` setting is added under the `compute_environment` section of the config file to activate debugging.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/troubleshooting.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndebug: true\n```\n\n----------------------------------------\n\nTITLE: Generating Unique Label Mappings from Dataset Filenames in Python\nDESCRIPTION: Extracts label strings from every image filename, converts the collection into a sorted list of unique labels (`id_to_label`), and constructs a dictionary mapping label strings to integer IDs (`label_to_id`). These mappings are essential for converting categorical labels into numerical form, a common requirement for machine learning models.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nall_labels = [extract_label(fname) for fname in fnames]\nid_to_label = list(set(all_labels))\nid_to_label.sort()\nlabel_to_id = {lbl: i for i, lbl in enumerate(id_to_label)}\n```\n\n----------------------------------------\n\nTITLE: Initializing Empty Model with init_empty_weights\nDESCRIPTION: This snippet shows how to use the `init_empty_weights` context manager from the `accelerate` library. This allows you to initialize a model without loading the weights directly into RAM, which is beneficial for large models. The context manager handles parameter creation by moving them to the 'meta' device during initialization.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nfrom accelerate import init_empty_weights\n\nwith init_empty_weights():\n    my_model = ModelClass(...)\n```\n\n----------------------------------------\n\nTITLE: Initializing FSDP2 Plugin in Python\nDESCRIPTION: This snippet demonstrates how to initialize the `FullyShardedDataParallelPlugin` in Python, specifically configuring it to use `FSDP2`. It takes the form of a small code block showcasing the key part of setting up the fsdp plugin. This enables the user to make use of `FSDP2` features.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/fsdp1_vs_fsdp2.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom accelerate import FullyShardedDataParallelPlugin, Accelerator\n\nfsdp_plugin = FullyShardedDataParallelPlugin(\n    fsdp_version=2\n    # other options...\n)\naccelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n```\n```\n\n----------------------------------------\n\nTITLE: Enabling Logit Return in MegatronLMPlugin - Python\nDESCRIPTION: This snippet sets the MegatronLMPlugin to return logits when running the model, which is necessary for advanced inference scenarios such as custom sampling or evaluation. It demonstrates initialization with the return_logits parameter. Prerequisites are a properly initialized plugin and distributed environment. Input is return_logits boolean; output is plugin instance configured to expose logits. Limitation is that the logits are only available at the last pipeline stage.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmegatron_lm_plugin = MegatronLMPlugin(return_logits=True)\n```\n\n----------------------------------------\n\nTITLE: Manually Initializing Model with deepspeed.zero.Init in Python\nDESCRIPTION: Shows the alternative method for preparing a model with the selected DeepSpeed plugin by manually using the `deepspeed.zero.Init` context manager, passing the active plugin's configuration. This is useful for custom model classes not using Transformers' `AutoModel`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith deepspeed.zero.Init(accelerator.deepspeed_plugin.config):\n    model = MyModel(...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Local SGD - PyTorch\nDESCRIPTION: This snippet integrates Local SGD into the training loop with Accelerate. It introduces a `LocalSGD` context manager, specifying the synchronization frequency via `local_sgd_steps`. Inside the context, the training loop is placed and a `local_sgd.step()` is called after each optimizer step to handle the parameter averaging, and accumulation using `accelerator.accumulate()` context manager.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/local_sgd.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n+local_sgd_steps=8\n\n+with LocalSGD(accelerator=accelerator, model=model, local_sgd_steps=8, enabled=True) as local_sgd:\n    for batch in training_dataloader:\n        with accelerator.accumulate(model):\n            inputs, targets = batch\n            outputs = model(inputs)\n            loss = loss_function(outputs, targets)\n            accelerator.backward(loss)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n+           local_sgd.step()\n```\n\n----------------------------------------\n\nTITLE: Example Accelerate Config for DeepSpeed ZeRO Stage-3 CPU Offload (YAML)\nDESCRIPTION: Sample Accelerate configuration file content (typically `default_config.yaml`) generated via `accelerate config` for using the DeepSpeed plugin with ZeRO Stage-3 and CPU offloading. It enables optimizer and parameter offloading to the CPU, sets `zero_stage` to 3, enables `zero3_init_flag` for large model instantiation, and configures saving the model in 16-bit format.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Tqdm Progress Bar on Local Main Process - Python\nDESCRIPTION: This code initializes a tqdm progress bar that is only enabled on the local main process. This prevents multiple progress bars from being displayed when running on multiple GPUs.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n```\n\n----------------------------------------\n\nTITLE: Example Megatron-LM Configuration YAML File\nDESCRIPTION: Shows the resulting YAML configuration file generated by the accelerate config command above. It contains fields specifying the compute environment as local machine, the distributed type set to MEGATRON_LM, mixed precision set to BF16, parallelism degrees for tensor and pipeline parallelism, activation recomputation, distributed optimizer usage, micro-batch count, and other training parameters. This file is essential for defining the distributed training setup compatible with Megatron-LM.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: MEGATRON_LM\ndowncast_bf16: 'no'\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmegatron_lm_config:\n  megatron_lm_gradient_clipping: 1.0\n  megatron_lm_num_micro_batches: 2\n  megatron_lm_pp_degree: 2\n  megatron_lm_recompute_activations: true\n  megatron_lm_sequence_parallelism: true\n  megatron_lm_tp_degree: 2\n  megatron_lm_use_distributed_optimizer: true\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Fully Sharded Data Parallel (FSDP) via HuggingFace Accelerate CLI in Bash\nDESCRIPTION: This snippet demonstrates how to generate the accelerate configuration interactively by running `accelerate config` on a local machine. The generated YAML-style config enables FSDP by setting `distributed_type` to FSDP and specifies numerous FSDP parameters such as auto wrap policy, sharding strategy, and precision. It also shows the command to launch a Python script (`examples/nlp_example.py`) with this configuration. The input expects a local environment and multiple processes. This CLI-driven setup eliminates manual config file creation, streamlining FSDP usage.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/fsdp.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_environment: LOCAL_MACHINE\n# Other config settings below\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_forward_prefetch: false\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: FULL_SHARD\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: BertLayer\n  fsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: MPI Running Check\nDESCRIPTION: This snippet provides a command for running an MPI sanity check to verify the setup for distributed CPU training using MPI. It utilizes the `mpirun` command with a hostfile to print hostnames, ensuring passwordless SSH is configured between nodes.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/troubleshooting.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -f hostfile -n {number of nodes} -ppn 1 hostname\n```\n\n----------------------------------------\n\nTITLE: Downloading Model Weights from Hugging Face Hub\nDESCRIPTION: This code downloads the model weights from the Hugging Face Hub using the `snapshot_download` function. The `repo_id` specifies the repository containing the weights. The downloaded weights will be used to populate the empty model.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import snapshot_download\nweights_location = snapshot_download(repo_id=\"marcsun13/gpt2-xl-linear-sharded\")\n```\n\n----------------------------------------\n\nTITLE: Self-contained Gradient Accumulation Example with Accelerate\nDESCRIPTION: A complete, runnable example demonstrating gradient accumulation with Accelerate using a simple linear model. It compares the results with and without gradient accumulation to show they produce equivalent results.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport copy\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# seed\nset_seed(0)\n\n# define toy inputs and labels\nx = torch.tensor([1., 2., 3., 4., 5., 6., 7., 8.])\ny = torch.tensor([2., 4., 6., 8., 10., 12., 14., 16.])\ngradient_accumulation_steps = 4\nper_device_batch_size = len(x) // gradient_accumulation_steps\n\n# define dataset and dataloader\ndataset = TensorDataset(x, y)\ndataloader = DataLoader(dataset, batch_size=per_device_batch_size)\n\n# define model, optimizer and loss function\nclass SimpleLinearModel(torch.nn.Module):\n    def __init__(self):\n        super(SimpleLinearModel, self).__init__()\n        self.weight = torch.nn.Parameter(torch.zeros((1, 1)))\n\n    def forward(self, inputs):\n        return inputs @ self.weight\n\nmodel = SimpleLinearModel()\nmodel_clone = copy.deepcopy(model)\ncriterion = torch.nn.MSELoss()\nmodel_optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\naccelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\nmodel, model_optimizer, dataloader = accelerator.prepare(model, model_optimizer, dataloader)\nmodel_clone_optimizer = torch.optim.SGD(model_clone.parameters(), lr=0.02)\nprint(f\"initial model weight is {model.weight.mean().item():.5f}\")\nprint(f\"initial model weight is {model_clone.weight.mean().item():.5f}\")\nfor i, (inputs, labels) in enumerate(dataloader):\n    with accelerator.accumulate(model):\n        inputs = inputs.view(-1, 1)\n        print(i, inputs.flatten())\n        labels = labels.view(-1, 1)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        accelerator.backward(loss)\n        model_optimizer.step()\n        model_optimizer.zero_grad()\nloss = criterion(x.view(-1, 1) @ model_clone.weight, y.view(-1, 1))\nmodel_clone_optimizer.zero_grad()\nloss.backward()\nmodel_clone_optimizer.step()\nprint(f\"w/ accumulation, the final model weight is {model.weight.mean().item():.5f}\")\nprint(f\"w/o accumulation, the final model weight is {model_clone.weight.mean().item():.5f}\")\n```\n\n----------------------------------------\n\nTITLE: Get State Dict from Checkpoint\nDESCRIPTION: This snippet uses the function `get_fp32_state_dict_from_zero_checkpoint` which allows the user to get the model `state_dict` directly from a ZeRO checkpoint without creating a full model copy.  It is used in cases where only the model weights are needed for saving or inspection.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\nstate_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)\n```\n\n----------------------------------------\n\nTITLE: Exporting Chrome Trace with Accelerate's Profiler\nDESCRIPTION: Example showing how to export profiling data to Chrome trace format using Accelerate's profiler, which automatically handles the export process to a specified directory.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224).cuda()\nprofile_kwargs = ProfileKwargs(\n    activities=[\"cpu\", \"cuda\"],\n    output_trace_dir=\"trace\"\n)\n\naccelerator = Accelerator(kwargs_handlers=[profile_kwargs])\nmodel = accelerator.prepare(model)\n\nwith accelerator.profile() as prof:\n    model(inputs)\n\n# The trace will be saved to the specified directory\n```\n\n----------------------------------------\n\nTITLE: Passing External Model to notebook_launcher Python\nDESCRIPTION: This snippet shows a diff demonstrating how to pass the externally declared model instance to the modified `training_function` using `notebook_launcher`. The `+` indicates adding a second argument to `notebook_launcher`, which is a tuple containing the object(s) (in this case, `(model,)`) to be passed to the training function.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/training_tpu.md#_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n  from accelerate import notebook_launcher\n- notebook_launcher(training_function)\n+ notebook_launcher(training_function, (model,))\n\n```\n\n----------------------------------------\n\nTITLE: Merging Sharded FSDP Weights into a Single State Dict using Accelerate in Python\nDESCRIPTION: Provides a snippet to merge multiple sharded weight files produced by FSDP training into a single model weight file, facilitating checkpoint loading or conversion to formats like safetensors. It imports the `merge_fsdp_weights` utility from accelerate and performs the merge from an FSDP shard directory (`pytorch_model_fsdp_0`) to an output path, with optional safe serialization. This utility complements the sharded checkpointing strategy by generating a consolidated checkpoint for inference or further training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/fsdp.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import merge_fsdp_weights\n\n# Our weights are saved usually in a `pytorch_model_fsdp_{model_number}` folder\nmerge_fsdp_weights(\"pytorch_model_fsdp_0\", \"output_path\", safe_serialization=True)\n```\n\n----------------------------------------\n\nTITLE: Running NLP example on multi GPUs (PyTorch distributed)\nDESCRIPTION: These commands demonstrate how to run the `nlp_example.py` script on multiple GPUs using Accelerate config and launcher and with traditional PyTorch launcher.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config  # This will create a config file on your server\naccelerate launch ./nlp_example.py  # This will run the script on your server\n```\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node 2 ./nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate - Bash\nDESCRIPTION: This snippet runs the configuration tool for Accelerate. This tool guides the user through setting up the training environment. Requires Accelerate to be installed. The command launches the configuration utility which guides the user through a series of prompts to configure the environment.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: Loading state_dict with synchronization after distributed loading\nDESCRIPTION: This snippet demonstrates how to load a saved state_dict into a model in a distributed setting, ensuring all processes wait until loading is complete before proceeding. It uses main_process_first context to perform loading and then synchronizes all processes.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/deferring_execution.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nwith accelerator.main_process_first():\n    state = torch.load(\"weights.pth\")\n    model.load_state_dict(state)\n```\n\n----------------------------------------\n\nTITLE: Displaying Benchmark Script Options - Bash\nDESCRIPTION: Displays all command-line arguments and configurable options accepted by main.py. This command helps users understand and customize the benchmark behavior. Requires the script and Python 3 to be installed; outputs help information to the terminal.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fsdp2/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 main.py --help\n```\n\n----------------------------------------\n\nTITLE: Debugging with Accelerate\nDESCRIPTION: This section explains how to troubleshoot hanging code and timeout errors in a distributed training setup, particularly focusing on mismatched tensor shapes, early stopping, and low kernel versions. It illustrates how to enable the debug mode of accelerate for catching errors during operations like gather, and reduce. Debugging flags can be passed from CLI, environment variables, or config files.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/troubleshooting.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --debug {my_script.py} --arg1 --arg2\n```\n\n----------------------------------------\n\nTITLE: Downloading Data for CV Example\nDESCRIPTION: These commands download and extract the data for the computer vision example, specifically the Oxford-IIT Pet Dataset.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nwget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\ntar -xzf images.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Applying tokenization across multiple workers with main_process_first()\nDESCRIPTION: This code applies a tokenization operation on a dataset in a multi-worker environment. Tokenization is performed on the main process to prevent duplicate work, then the tokenized dataset is propagated to other workers, facilitating efficient parallel processing.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/deferring_execution.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndatasets = load_dataset(\"glue\", \"mrpc\")\n\nwith accelerator.main_process_first():\n    tokenized_datasets = datasets.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Example DeepSpeed Config for ZeRO Stage 3 Offload - JSON\nDESCRIPTION: Presents a sample DeepSpeed configuration file content in JSON format for ZeRO Stage 3 optimization with CPU offloading. It details `zero_optimization` settings including `stage: 3` and the `offload_optimizer` and `offload_param` configurations to move states to CPU.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"sub_group_size\": 1e9,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": \"auto\"\n    },\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Libraries for Dataset and Training Setup in Python\nDESCRIPTION: Imports required Python modules and functions for building the dataset, transformations, model creation, and training utilities. Key dependencies include PyTorch components (data utilities and schedulers), PIL for image processing, HuggingFace Accelerate for distributed training utilities, and TIMM for efficient model creation. This prepares the environment for subsequent dataset handling and training setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os, re, torch, PIL\nimport numpy as np\n\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import Compose, RandomResizedCrop, Resize, ToTensor\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom timm import create_model\n```\n\n----------------------------------------\n\nTITLE: Converting Transformers Checkpoint to Megatron-LM Checkpoint with Parallelism - Bash\nDESCRIPTION: This command-line example demonstrates transforming a Transformers checkpoint into a Megatron-LM checkpoint via the checkpoint_reshaping_and_interoperability.py utility, specifying tensor, pipeline, and data parallel sizes, as well as config options for parameter dtype, vocab alignment, and optimizer usage. Required dependencies are the requisite conversion utility script and compatible checkpoint formats. Inputs are paths, size configurations, and feature flags; outputs are restructured checkpoints for distributed Megatron-LM training. Limitations: Only tested scripts/models are officially supported.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython checkpoint_utils/megatgron_gpt2/checkpoint_reshaping_and_interoperability.py \\\n--load_path \"gpt/trfs_checkpoint\" \\\n--save_path \"gpt/megatron_lm_checkpoint\" \\\n--target_tensor_model_parallel_size 2 \\\n--target_pipeline_model_parallel_size 2 \\\n--target_data_parallel_size 2 \\\n--target_params_dtype \"bf16\" \\\n--make_vocab_size_divisible_by 128 \\\n--use_distributed_optimizer \\\n--print-checkpoint-structure\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Tracker by Extending GeneralTracker in Python\nDESCRIPTION: Shows how to create a new custom experiment tracker by subclassing the GeneralTracker class, implementing required methods and properties to integrate a third-party tracking library. Includes usage example with a hypothetical tracker, ensuring operations execute on the main process where needed.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/tracking.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom accelerate.tracking import GeneralTracker, on_main_process\nfrom typing import Optional\n\nimport wandb\n\nclass MyCustomTracker(GeneralTracker):\n    name = \"wandb\"\n    requires_logging_directory = False\n\n    @on_main_process\n    def __init__(self, run_name: str):\n        self.run_name = run_name\n        self.run = wandb.init(project=self.run_name)\n\n    @property\n    def tracker(self):\n        return self.run.run\n\n    @on_main_process\n    def store_init_configuration(self, values: dict):\n        wandb.config.update(values)\n\n    @on_main_process\n    def log(self, values: dict, step: Optional[int] = None):\n        wandb.log(values, step=step)\n\n# Usage example\ntracker = MyCustomTracker(\"some_run_name\")\naccelerator = Accelerator(log_with=tracker)\n\n```\n\n----------------------------------------\n\nTITLE: Configure torchao Backend for FP8 Accelerator in Python\nDESCRIPTION: Shows how to initialize the Accelerator to use the experimental `torchao` backend for FP8 training. This is done by instantiating `AORecipeKwargs` and passing it to the `Accelerator`'s `kwarg_handlers` parameter.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/low_precision_training.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\nfrom accelerate.utils import AORecipeKwargs\nkwargs = [AORecipeKwargs()]\naccelerator = Accelerator(mixed_precision=\"fp8\", kwarg_handlers=kwargs)\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate from GitHub - Bash\nDESCRIPTION: This snippet installs Accelerate directly from the GitHub repository. It allows users to access the latest changes, including new features that haven't been officially released on PyPI. It depends on git and pip.  The command installs the package directly from the specified GitHub repository URL.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/accelerate\n```\n\n----------------------------------------\n\nTITLE: Conditional Print Statement on Local Main Process - Python\nDESCRIPTION: This code demonstrates how to use `accelerator.is_local_main_process` to conditionally execute a print statement only on the local main process. This prevents duplicate print statements when using multiple GPUs.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif accelerator.is_local_main_process:\n    print(\"Accelerate is the best\")\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Training Using notebook_launcher in Python\nDESCRIPTION: Shows basic usage examples of notebook_launcher to launch the distributed training function across multiple processes or nodes. It accepts the training function, parameters as a tuple, and configuration such as number of processes, master address, node rank, and number of nodes. An elastic launch example includes max_restarts for fault tolerance. The launcher manages process spawning and distributed runtime environment initialization.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nargs = (\"fp16\", 42, 64)\nnotebook_launcher(training_loop, args, num_processes=2)\n```\n\nLANGUAGE: python\nCODE:\n```\nnotebook_launcher(training_loop, args, master_addr=\"172.31.43.8\", node_rank=0, num_nodes=2, num_processes=8)\n```\n\nLANGUAGE: python\nCODE:\n```\nnotebook_launcher(training_loop, args, master_addr=\"172.31.43.8\", node_rank=1, num_nodes=2, num_processes=8)\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel = create_model(\"resnet50d\", pretrained=True, num_classes=len(label_to_id))\n\nargs = (model, \"fp16\", 42, 64)\nnotebook_launcher(training_loop, args, num_processes=8)\n```\n\nLANGUAGE: python\nCODE:\n```\nnotebook_launcher(\n    training_loop,\n    args,\n    num_processes=2,\n    max_restarts=3\n)\n```\n\n----------------------------------------\n\nTITLE: Checking the Active DeepSpeed Plugin in Python\nDESCRIPTION: Illustrates how to verify the currently active DeepSpeed plugin using `get_active_deepspeed_plugin` from `accelerate.utils.deepspeed`. By default, Accelerate uses the first plugin in the provided dictionary.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nactive_plugin = get_active_deepspeed_plugin(accelerator.state)\nassert active_plugin is deepspeed_plugins[\"student\"]\n```\n\n----------------------------------------\n\nTITLE: Running Accelerate GPU Image with Container Access\nDESCRIPTION: Docker command to launch an interactive container with the Accelerate GPU image, enabling GPU access for acceleration tasks.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docker/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker container run --gpus all -it huggingface/accelerate:gpu-nightly\n```\n\n----------------------------------------\n\nTITLE: Creating DataLoaders for Training and Evaluation in Python\nDESCRIPTION: Defines the `get_dataloaders` function to build PyTorch DataLoader instances for training and evaluation using a specified batch size. The dataset is split randomly into training (80%) and evaluation (20%) subsets using permutation. Different image transformations are applied for training (random resized crop) and evaluation (resize). Both DataLoaders use multiprocessing with `num_workers=4` to increase throughput. This function ensures data is loaded on CPU and transferred correctly during training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_dataloaders(batch_size: int = 64):\n    \"Builds a set of dataloaders with a batch_size\"\n    random_perm = np.random.permutation(len(fnames))\n    cut = int(0.8 * len(fnames))\n    train_split = random_perm[:cut]\n    eval_split = random_perm[cut:]\n\n    # For training a simple RandomResizedCrop will be used\n    train_tfm = Compose([RandomResizedCrop((224, 224), scale=(0.5, 1.0)), ToTensor()])\n    train_dataset = PetsDataset([fnames[i] for i in train_split], image_transform=train_tfm, label_to_id=label_to_id)\n\n    # For evaluation a deterministic Resize will be used\n    eval_tfm = Compose([Resize((224, 224)), ToTensor()])\n    eval_dataset = PetsDataset([fnames[i] for i in eval_split], image_transform=eval_tfm, label_to_id=label_to_id)\n\n    # Instantiate dataloaders\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=4)\n    eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=batch_size * 2, num_workers=4)\n    return train_dataloader, eval_dataloader\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Script with Accelerate Specific Config using Bash\nDESCRIPTION: Demonstrates launching a distributed Python script (`distributed_inference.py`) with Accelerate, explicitly specifying a configuration file (`my_config.json`) using the `--config_file` argument.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --config_file my_config.json distributed_inference.py\n```\n\n----------------------------------------\n\nTITLE: Running FP8 Benchmarks on Single GPU with Python\nDESCRIPTION: This bash snippet demonstrates how to execute the `non_distributed.py` script for single GPU training. It directly uses the python command to launch the training process. There are no external dependencies beyond the script itself and required libraries like torch and accelerate.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fp8/torchao/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython non_distributed.py\n```\n\n----------------------------------------\n\nTITLE: Running Experiment Tracking Example with Accelerate\nDESCRIPTION: Command for launching the experiment tracking example script with tracking enabled for integrations like W&B, TensorBoard, or CometML.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./tracking.py --with_tracking\n```\n\n----------------------------------------\n\nTITLE: Writing Basic Accelerate Configuration - Python\nDESCRIPTION: This snippet writes a minimal Accelerate configuration file using Python. It sets the mixed_precision option. It requires the `accelerate` library to be installed. The command directly calls a function from `accelerate.utils` to write a basic configuration file with specified settings.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython -c \"from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='fp16')\"\n```\n\n----------------------------------------\n\nTITLE: Docker Pull Command for Accelerate Release Version\nDESCRIPTION: Example of a Docker image tag for a specific Accelerate release version (0.28.0) with GPU support.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docker/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface/accelerate:gpu-release-0.28.0\n```\n\n----------------------------------------\n\nTITLE: accelerate launch Output with Auto and CLI args\nDESCRIPTION: This output of `accelerate launch` when combined with the auto DeepSpeed parameters. This demonstrates how command-line arguments (e.g., `--mixed_precision`, `--gradient_accumulation_steps`) take precedence when 'auto' values are used in the DeepSpeed configuration, setting the device, precision, and ZeRO Stage.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nDistributed environment: DEEPSPEED  Backend: nccl\nNum processes: 4\nProcess index: 0\nLocal process index: 0\nDevice: cuda:0\nMixed precision type: fp16\nds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'stage3_gather_16bit_weights_on_model_save': True, 'offload_optimizer': {'device': 'nvme'}, 'offload_param': {'device': 'cpu'}}, 'gradient_clipping': 1.0, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 5, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}}\n```\n\n----------------------------------------\n\nTITLE: Example Training Loop for Multiple Disjoint Models in Python\nDESCRIPTION: Shows a sample training loop for two disjoint models prepared with separate Accelerators or using plugin selection. Each model performs its forward pass, loss calculation, backward pass (using its specific `accelerator.backward` call or the appropriate accelerator instance), and optimizer/scheduler steps independently within the loop.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfor batch in dl:\n    outputs1 = first_model(**batch)\n    first_accelerator.backward(outputs1.loss)\n    first_optimizer.step()\n    first_scheduler.step()\n    first_optimizer.zero_grad()\n    \n    outputs2 = model2(**batch)\n    second_accelerator.backward(outputs2.loss)\n    second_optimizer.step()\n    second_scheduler.step()\n    second_optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Plugin Configuration Options via Accelerate CLI (Description)\nDESCRIPTION: Lists the configurable parameters available through the `accelerate config` interactive CLI for the DeepSpeed plugin. It covers settings for ZeRO stages, gradient handling, optimizer/parameter offloading (CPU/NVMe), ZeRO Stage-3 specifics, mixed precision, MoE layer specification, multi-node setup (hostfile, filters, launcher), and the option to use a full DeepSpeed JSON config file instead of the plugin.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\n`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning\n`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.\n`gradient_clipping`: Enable gradient clipping with value.\n`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.\n`offload_optimizer_nvme_path`: Decides Nvme Path to offload optimizer states. If unspecified, will default to 'none'.\n`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.\n`offload_param_nvme_path`: Decides Nvme Path to offload parameters. If unspecified, will default to 'none'.\n`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.\n`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.\n`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training.\n`deepspeed_moe_layer_cls_names`: Comma-separated list of transformer Mixture-of-Experts (MoE) layer class names (case-sensitive) to wrap ,e.g, `MixtralSparseMoeBlock`, `Qwen2MoeSparseMoeBlock`, `JetMoEAttention,JetMoEBlock` ...\n`deepspeed_hostfile`: DeepSpeed hostfile for configuring multi-node compute resources.\n`deepspeed_exclusion_filter`: DeepSpeed exclusion filter string when using mutli-node setup.\n`deepspeed_inclusion_filter`: DeepSpeed inclusion filter string when using mutli-node setup.\n`deepspeed_multinode_launcher`: DeepSpeed multi-node launcher to use, e.g. `pdsh`, `standard`, `openmpi`, `mvapich`, `mpich`, `slurm`, `nossh` (requires DeepSpeed >= 0.14.5). If unspecified, will default to `pdsh`.\n`deepspeed_config_file`: path to the DeepSpeed config file in `json` format. See the next section for more details on this.\n```\n\n----------------------------------------\n\nTITLE: Running CV example with fp16 (mixed-precision)\nDESCRIPTION: These commands demonstrate how to run the `cv_example.py` script with fp16 mixed-precision. The first passes `mixed_precision=fp16` to the `Accelerator`, and the second uses the `accelerate launch` command with the `--mixed_precision` flag.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython ./cv_example.py --data_dir path_to_data --mixed_precison fp16\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precison fp16 ./cv_example.py --data_dir path_to_data\n```\n\n----------------------------------------\n\nTITLE: Checking Accelerate Configuration - Bash\nDESCRIPTION: This snippet runs the Accelerate environment check. It provides information about the current configuration and system environment. It requires Accelerate to be installed and configured.  The command checks the current configuration and prints the relevant information about the setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate env\n```\n\n----------------------------------------\n\nTITLE: Importing notebook_launcher from Huggingface Accelerate in Python\nDESCRIPTION: Imports the notebook_launcher utility from the accelerate module for launching distributed training sessions in Jupyter notebooks. This launcher wraps a training function, arguments, and configuration to run the training loop over several processes and nodes. It is a prerequisite step before calling notebook_launcher with the training function and parameters.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import notebook_launcher\n```\n\n----------------------------------------\n\nTITLE: Running CV example on single GPU\nDESCRIPTION: This command demonstrates how to run the `cv_example.py` script on a single GPU. It assumes that the script is executed from a server with a GPU.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython ./cv_example.py  # from a server with a GPU\n```\n\n----------------------------------------\n\nTITLE: Running CV example on multi GPUs, multi node (PyTorch distributed)\nDESCRIPTION: These commands demonstrate how to run the `cv_example.py` script on multiple GPUs across multiple nodes using Accelerate config and launcher and with traditional PyTorch launcher. The commands are run on each node.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config --config_file config.yaml  # This will create a config file on your server to `config.yaml`\naccelerate launch --config_file config.yaml ./cv_example.py --data_dir path_to_data  # This will run the script on each server\n```\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun \\\n    --nproc_per_node 2 \\\n    --nnodes 2 \\\n    --rdzv_id 2299 \\\n    --rdzv_backend c10d \\\n    --rdzv_endpoint master_node_ip_address:29500 \\\n    ./cv_example.py --data_dir path_to_data\n```\n\n----------------------------------------\n\nTITLE: Installing Editable Accelerate from Source - Bash\nDESCRIPTION: This snippet installs an editable version of the Accelerate library from a locally cloned repository. This is useful for contributing to the library or testing changes. It requires git for cloning, and pip.  The command clones the repository, navigates into its directory, and then installs the library in 'editable' mode.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/accelerate\ncd accelerate\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Development Environment Dependencies\nDESCRIPTION: Command to install the Accelerate library in editable mode along with all necessary dependencies for development, testing, and code quality checks. This should be run within a dedicated virtual or conda environment.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n$ pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Running the Python Script using torchrun\nDESCRIPTION: This command executes a Python script using torchrun, specifying the number of processes per node and thus the number of GPUs used. This approach provides another way to execute the provided Python scripts in a distributed environment.  It offers an alternative to `accelerate launch`, which might be preferable in specific configurations.  The script takes no parameters but requires the correct path to the python file, where {NUM_GPUS} represents the number of GPUs to utilize.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/inference/pippy/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc-per-node {NUM_GPUS} bert.py\n```\n\n----------------------------------------\n\nTITLE: Estimating Model Memory with Accelerate\nDESCRIPTION: This command estimates the memory footprint of the `bert-base-cased` model using the `accelerate estimate-memory` tool. It downloads the model's `config.json`, loads the model on the `meta` device, and reports the memory usage for different data types.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/model_size_estimator.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate estimate-memory bert-base-cased\n```\n\n----------------------------------------\n\nTITLE: Using Accelerate Config Command (Bash)\nDESCRIPTION: Demonstrates the basic usage of the `accelerate config` command with potential arguments. This command is used to interactively create or modify the default Accelerate configuration file by launching a series of prompts. It supports specifying a custom config file path.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config [arguments]\n```\n\n----------------------------------------\n\nTITLE: Running Gradient Accumulation Example with Accelerate\nDESCRIPTION: Command for launching the gradient accumulation example script with a specified number of accumulation steps.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./gradient_accumulation.py --gradient_accumulation_steps 5\n```\n\n----------------------------------------\n\nTITLE: Converting Megatron-LM Checkpoint to Transformers Checkpoint - Bash\nDESCRIPTION: This command-line example invokes the checkpoint_reshaping_and_interoperability.py script from the Transformers repository to convert a checkpoint from Megatron-LM to a Transformers sharded checkpoint. It requires paths for load/save directories, maximum shard size, target tokenizer, and optional checkpoint structure printing. Prerequisites include Transformers and Megatron-LM checkpoint files. Inputs are file paths and configuration flags; output is a new sharded checkpoint. Limitation is the need for script availability and proper environment setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython checkpoint_reshaping_and_interoperability.py \\\n--convert_checkpoint_from_megatron_to_transformers \\\n--load_path \"gpt/iter_0005000\" \\\n--save_path \"gpt/trfs_checkpoint\" \\\n--max_shard_size \"200MB\" \\\n--tokenizer_name \"gpt2\" \\\n--print-checkpoint-structure\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs the required dependencies for the NLP example, which includes `datasets`, `evaluate` and `transformers` libraries.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets evaluate transformers\n```\n\n----------------------------------------\n\nTITLE: Analyzing CPU Execution Time with PyTorch Profiler\nDESCRIPTION: Example of using PyTorch's built-in profiler to analyze CPU execution time of a ResNet-18 model, showing operator-level performance metrics and timing information.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\nmodel = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers Dependency (Bash)\nDESCRIPTION: Installs the `transformers` library using pip. This library is a required dependency for running the big model inference benchmark scripts provided in the Accelerate project.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/big_model_inference/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers\n```\n\n----------------------------------------\n\nTITLE: Import AcceleratorState\nDESCRIPTION: This snippet imports the `AcceleratorState` class from the `accelerate.state` module. This class provides information about the current state of the accelerator, such as whether it's the main process.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.state import AcceleratorState\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Script with Accelerate Default Config using Bash\nDESCRIPTION: Shows the command to launch a distributed Python script (`distributed_inference.py`) using the `accelerate launch` command. This assumes a default Accelerate configuration file has been created using `accelerate config`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch distributed_inference.py\n```\n\n----------------------------------------\n\nTITLE: Saving model state_dict on the main process\nDESCRIPTION: This code illustrates saving a model's state dictionary only on the main process to avoid conflicts during distributed training. It involves unwrapping the model from accelerator and saving weights to a file using torch.save.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/deferring_execution.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nif accelerator.is_main_process:\n    model = accelerator.unwrap_model(model)\n    torch.save(model.state_dict(), \"weights.pth\")\n```\n\n----------------------------------------\n\nTITLE: Cloning Fork and Adding Upstream Remote\nDESCRIPTION: Instructions for cloning your forked repository to your local machine and adding the original Hugging Face repository as an 'upstream' remote. This setup is necessary for synchronizing your local copy with the main project.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n$ git clone git@github.com:<your Github handle>/accelerate.git\n$ cd accelerate\n$ git remote add upstream https://github.com/huggingface/accelerate.git\n```\n\n----------------------------------------\n\nTITLE: Running NLP example on single CPU\nDESCRIPTION: These commands demonstrate how to run the `nlp_example.py` script on a single CPU. The first option is used from a server without a GPU, the second passes `cpu=True` to the `Accelerator`, and the third uses the `accelerate launch` command with the `--cpu` flag.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ./nlp_example.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npython ./nlp_example.py --cpu\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --cpu ./nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: Analyzing CPU Execution Time with Accelerate's Profiler\nDESCRIPTION: Example showing how to use Accelerate's profiler wrapper to analyze CPU execution time of a ResNet-18 model with the same functionality as PyTorch's profiler, but with simplified integration.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, ProfileKwargs\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nprofile_kwargs = ProfileKwargs(\n    activities=[\"cpu\"],\n    record_shapes=True\n)\n\naccelerator = Accelerator(cpu=True, kwargs_handlers=[profile_kwargs])\nmodel = accelerator.prepare(model)\n\nwith accelerator.profile() as prof:\n    with torch.no_grad():\n        model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Declaring Model Before Training Function Python\nDESCRIPTION: This snippet shows the required modification for low-resource TPU notebook training: declaring the model instance outside the `training_function`. By instantiating the model in a separate cell or scope before launching, the single model instance can be shared across forked TPU processes, conserving memory compared to instantiating it inside each process.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/training_tpu.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# In another Jupyter cell\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n\n```\n\n----------------------------------------\n\nTITLE: Replacing PyTorch no_sync with Accelerate's no_sync (Python Diff)\nDESCRIPTION: This diff demonstrates replacing the model-specific `ddp_model.no_sync()` context manager with Accelerate's universal `accelerator.no_sync(model)`. This provides a consistent API for controlling gradient synchronization, regardless of whether DDP is actually being used, simplifying code across different training setups.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/gradient_synchronization.md#_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n  ddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)\n\n  for index, batch in enumerate(dataloader):\n      inputs, targets = batch\n      # Trigger gradient synchronization on the last batch\n      if index != (len(dataloader)-1):\n-         with ddp_model.no_sync():\n+         with accelerator.no_sync(model):\n              # Gradients only accumulate\n              outputs = ddp_model(inputs)\n              loss = loss_func(outputs, targets)\n              accelerator.backward(loss)\n      else:\n          # Gradients finally sync\n          outputs = ddp_model(inputs)\n          loss = loss_func(outputs)\n          accelerator.backward(loss)\n          optimizer.step()\n          optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Running Inference Benchmark Script (Python)\nDESCRIPTION: Executes the Python script `inference_acc.py` to benchmark inference performance for a specified large language model (`model_name`). The script supports several pre-configured models (e.g., 'gpt-j-6b', 'opt') or any valid checkpoint name. Optional arguments like `--torch_dtype` can force a specific data type, and `--disk-offload` enables disk offloading.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/big_model_inference/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython inference_acc.py model_name\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate and PyTorch (Bash)\nDESCRIPTION: Command to install the necessary Python libraries, `accelerate` and `torch`, using the pip package manager. These libraries are required to run the distributed inference examples.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/inference/distributed/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install accelerate torch\n```\n\n----------------------------------------\n\nTITLE: Consolidate Zero Checkpoint\nDESCRIPTION: This snippet shows the usage of the `zero_to_fp32.py` script, which is used to convert a ZeRO stage 3 checkpoint to a full 32-bit model. It is used for converting the ZeRO checkpoint to fp32, which can be saved and used.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n$ cd /path/to/checkpoint_dir\n$ ./zero_to_fp32.py . pytorch_model.bin\nProcessing zero checkpoint at global_step1\nDetected checkpoint of type zero stage 3, world_size: 2\nSaving fp32 state dict to pytorch_model.bin (total_numel=60506624)\n```\n\n----------------------------------------\n\nTITLE: Importing CosineAnnealingLR Scheduler from PyTorch in Python\nDESCRIPTION: Imports the `CosineAnnealingLR` learning rate scheduler from PyTorch's optimization package which will be used later to adjust the learning rate during training. This scheduler modifies the learning rate using a cosine annealing schedule, helping to improve training convergence.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n```\n\n----------------------------------------\n\nTITLE: Downloading Sharded Model\nDESCRIPTION: This snippet downloads a sharded version of a pre-trained GPT-2 model from the Hugging Face Hub using the `snapshot_download` function. The `repo_id` parameter specifies the model repository, and the function downloads the model files to a local directory specified by `weights_location`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_4\n\nLANGUAGE: py\nCODE:\n```\nfrom huggingface_hub import snapshot_download\ncheckpoint = \"marcsun13/gpt2-xl-linear-sharded\"\nweights_location = snapshot_download(repo_id=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Running CV example on TPUs\nDESCRIPTION: These commands demonstrate how to run the `cv_example.py` script on TPUs using Accelerate config and launcher.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config --config_file config.yaml  # This will create a config file on your server to `config.yaml`\naccelerate launch --config_file config.yaml ./cv_example.py --data_dir path_to_data  # This will run the script on each server\n```\n\n----------------------------------------\n\nTITLE: Analyzing Memory Consumption with PyTorch Profiler\nDESCRIPTION: Example of profiling memory usage of a ResNet-18 model using PyTorch's profiler, showing which operators allocate and release memory during execution.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nwith profile(activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n```\n\n----------------------------------------\n\nTITLE: Configuring Megatron-LM Plugin Using Accelerate CLI in Bash\nDESCRIPTION: This snippet demonstrates using the `accelerate config` command in a bash environment to interactively generate a configuration file (`megatron_gpt_config.yaml`) for enabling Megatron-LM features. It presents example user inputs corresponding to machine environment, distributed setup, tensor and pipeline parallelism degrees, mixed precision settings, and more. This configuration enables distributed training with the Megatron-LM distributed type and specifies parameters such as gradient clipping, micro-batches, and optimizer sharding.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n:~$ accelerate config --config_file \"megatron_gpt_config.yaml\"\nIn which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\nWhich type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 2\nHow many different machines will you use (use more than 1 for multi-node training)? [1]: \nDo you want to use DeepSpeed? [yes/NO]: \nDo you want to use FullyShardedDataParallel? [yes/NO]: \nDo you want to use Megatron-LM ? [yes/NO]: yes\nWhat is the Tensor Parallelism degree/size? [1]:2\nDo you want to enable Sequence Parallelism? [YES/no]: \nWhat is the Pipeline Parallelism degree/size? [1]:2\nWhat is the number of micro-batches? [1]:2\nDo you want to enable selective activation recomputation? [YES/no]: \nDo you want to use distributed optimizer which shards optimizer state and gradients across data parallel ranks? [YES/no]: \nWhat is the gradient clipping value based on global L2 Norm (0 to disable)? [1.0]: \nHow many GPU(s) should be used for distributed training? [1]:4\nDo you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: bf16\n```\n\n----------------------------------------\n\nTITLE: Running NLP example on multi GPUs, multi node (PyTorch distributed)\nDESCRIPTION: These commands demonstrate how to run the `nlp_example.py` script on multiple GPUs across multiple nodes using Accelerate config and launcher and with traditional PyTorch launcher. The commands are run on each node.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config  # This will create a config file on each server\naccelerate launch ./nlp_example.py  # This will run the script on each server\n```\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun \\\n    --nproc_per_node 2 \\\n    --nnodes 2 \\\n    --rdzv_id 2299 \\\n    --rdzv_backend c10d \\\n    --rdzv_endpoint master_node_ip_address:29500 \\\n    ./nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: Listing Dataset Filenames from Directory in Python\nDESCRIPTION: Lists all filenames within the specified `data_dir` directory, intended to represent the dataset images. This snippet helps in inspecting the dataset content and verifying that files are correctly accessible. The example prints the first filename to demonstrate the naming convention.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ndata_dir = \"../../images\"\nfnames = os.listdir(data_dir)\nfname = fnames[0]\nprint(fname)\n```\n\n----------------------------------------\n\nTITLE: Launching FP8 Benchmarks using Accelerate Launch\nDESCRIPTION: This bash snippet describes how to run the distributed training scripts (ddp.py, distrib_deepspeed.py, or fsdp.py) using `accelerate launch`. This command initializes the necessary distributed training configuration based on the script's contents. The `accelerate launch` command handles the multi-GPU setup and management.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fp8/torchao/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ddp.py # or distrib_deepspeed.py, ddp.py\n```\n\n----------------------------------------\n\nTITLE: Exporting Chrome Trace with PyTorch Profiler\nDESCRIPTION: Example of exporting profiling data to a Chrome trace file for visualization in the Chrome trace viewer, capturing both CPU and CUDA activities of a GPU-accelerated ResNet-18 model.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/profiler.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = models.resnet18().cuda()\ninputs = torch.randn(5, 3, 224, 224).cuda()\n\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    model(inputs)\n\nprof.export_chrome_trace(\"trace.json\")\n```\n\n----------------------------------------\n\nTITLE: Running CV example on multi GPUs (PyTorch distributed)\nDESCRIPTION: These commands demonstrate how to run the `cv_example.py` script on multiple GPUs using Accelerate config and launcher and with traditional PyTorch launcher.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config --config_file config.yaml  # This will create a config file on your server to `config.yaml`\naccelerate launch --config_file config.yaml ./cv_example.py --data_dir path_to_data  # This will run the script on your server\n```\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node 2 ./cv_example.py --data_dir path_to_data\n```\n\n----------------------------------------\n\nTITLE: Launching Accelerate with a Configuration File (Shell)\nDESCRIPTION: Demonstrates how to launch a Python script (`run_me.py`) using Hugging Face Accelerate, specifying a custom configuration via a YAML file (`{file}`). This command utilizes the `accelerate launch` utility to set up the distributed or mixed-precision environment according to the provided config. The `run_me.py` script is expected to print the `AcceleratorState`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/config_yaml_templates/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\naccelerate launch --config_file {file} run_me.py\n```\n\n----------------------------------------\n\nTITLE: Running Full Test Suite using Make\nDESCRIPTION: Convenience command using `make` to run the entire test suite for the Accelerate library. This ensures all functionalities are working as expected before submitting a pull request.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\n$ make test\n```\n\n----------------------------------------\n\nTITLE: Accelerate Config CLI\nDESCRIPTION: This bash script demonstrates the interactive use of `accelerate config` to generate an Accelerate configuration. The user answers a series of questions about the compute environment, hardware, DeepSpeed usage, and other settings.  The generated configuration is then saved to a YAML file.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ accelerate config\n-------------------------------------------------------------------------------------------------------------------------------\nIn which compute environment are you running?\nThis machine\n-------------------------------------------------------------------------------------------------------------------------------\nWhich type of machine are you using?\nmulti-GPU\nHow many different machines will you use (use more than 1 for multi-node training)? [1]:\nDo you wish to optimize your script with torch dynamo?[yes/NO]:\nDo you want to use DeepSpeed? [yes/NO]: yes\nDo you want to specify a json file to a DeepSpeed config? [yes/NO]: yes\nPlease enter the path to the json DeepSpeed config file: ds_config.json\nDo you want to enable `deepspeed.zero.Init` when using ZeRO Stage-3 for constructing massive models? [yes/NO]: yes\nHow many GPU(s) should be used for distributed training? [1]:4\naccelerate configuration saved at ds_config_sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Standard PyTorch Model Loading Python\nDESCRIPTION: Shows the typical method for loading a PyTorch model from a checkpoint. This approach requires the entire model state dictionary to fit into memory, which is problematic for large models that exceed device capacity.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nmy_model = ModelClass(...)\nstate_dict = torch.load(checkpoint_file)\nmy_model.load_state_dict(state_dict)\n```\n\n----------------------------------------\n\nTITLE: Running LocalSGD Example with Accelerate\nDESCRIPTION: Command for launching the LocalSGD example script with a specified number of local SGD steps, demonstrating gradient synchronization control.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./local_sgd.py --local_sgd_steps 4\n```\n\n----------------------------------------\n\nTITLE: Installing bitsandbytes Library\nDESCRIPTION: This command installs the bitsandbytes library, which is required for quantization. It is necessary for utilizing 8-bit and 4-bit quantization techniques within Accelerate.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install bitsandbytes\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate from Source\nDESCRIPTION: This command installs the latest version of the Accelerate library directly from the GitHub repository. This ensures that you have the most up-to-date features and bug fixes related to bitsandbytes integration.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/accelerate.git\n```\n\n----------------------------------------\n\nTITLE: Training Without Gradient Accumulation Using PyTorch and Accelerate in Python\nDESCRIPTION: This snippet demonstrates single-device training of a PyTorch model using Hugging Face Accelerate without gradient accumulation. It prepares a full batch with a DataLoader, computes cross-entropy loss, and performs one optimization step, logging the final model weights. Dependencies: torch, accelerate, and appropriate dataset/model/optimizer setup. Inputs: dataset, collate_fn, model_clone. Outputs: updated model_clone weights. The code assumes main process execution, disables gradient accumulation, and requires that the model and optimizer are properly initialized before use.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/gradient_accumulation.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nif accelerator.is_main_process:\n    # prepare one single entire batch\n    dataloader = DataLoader(dataset, batch_size=len(dataset), collate_fn=collate_fn)\n    full_batch_without_accum = next(iter(dataloader))\n    total_inputs, total_labels = full_batch_without_accum[\"input_ids\"], full_batch_without_accum[\"labels\"]\n    model_clone_optimizer = torch.optim.SGD(model_clone.parameters(), lr=0.08)\n    \n    # train the cloned model\n    loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")(model_clone(total_inputs).view(-1, 2), total_labels.view(-1).to(torch.int64))\n    model_clone_optimizer.zero_grad()\n    loss.backward()\n    model_clone_optimizer.step()\n    \n    # We should have the same final weights.\n    logger.warning(f\"w/o accumulation, the final model weight is {model_clone.weight.detach().cpu().squeeze()}\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Model and Dataloaders for Megatron-LM Training in Accelerate - Python\nDESCRIPTION: This snippet demonstrates how to prepare a model, optimizer, scheduler, and replicate the same dataloader instance for training, validation, and testing using the accelerator.prepare method. It assumes that the splits are managed internally and that three identical dataloader instances are required as per Megatron-LM requirements. Dependencies include correctly constructed model and dataloader objects. Inputs are model components and dataloaders; outputs are prepared objects for distributed training. Limitation is the requirement that dataloader splitting matches expected split string proportions.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer, lr_scheduler, train_dataloader, eval_dataloader, _ = accelerator.prepare(\n    model, optimizer, lr_scheduler, megatron_dataloader, megatron_dataloader, megatron_dataloader\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Loss Function and Training Step for Megatron Model - Python\nDESCRIPTION: This snippet extends the existing GPTTrainStep by implementing a custom loss calculation that applies weighted loss per sample based on key token IDs and a scaling factor alpha. It includes methods for integrating the custom loss into the training loop, with proper loss reduction across data parallel groups. It assumes access to Megatron arguments, a compatible model, and the torch library. Key inputs are input IDs, loss mask, and model outputs; outputs are the weighted loss and auxiliary metrics. Limitations include dependence on the specific distribution and arguments of Megatron-LM and Accelerate.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass GPTTrainStepWithCustomLoss(GPTTrainStep):\n    def __init__(self, megatron_args, **kwargs):\n        super().__init__(megatron_args)\n        self.kwargs = kwargs\n\n    def get_loss_func(self):\n        def loss_func(inputs, loss_mask, output_tensor):\n            batch_size, seq_length = output_tensor.shape\n            losses = output_tensor.float()\n            loss_mask = loss_mask.view(-1).float()\n            loss = losses.view(-1) * loss_mask\n\n            # Resize and average loss per sample\n            loss_per_sample = loss.view(batch_size, seq_length).sum(axis=1)\n            loss_mask_per_sample = loss_mask.view(batch_size, seq_length).sum(axis=1)\n            loss_per_sample = loss_per_sample / loss_mask_per_sample\n\n            # Calculate and scale weighting\n            weights = torch.stack([(inputs == kt).float() for kt in self.kwargs[\"keytoken_ids\"]]).sum(axis=[0, 2])\n            weights = 1.0 + self.kwargs[\"alpha\"] * weights\n            # Calculate weighted average\n            weighted_loss = (loss_per_sample * weights).mean()\n\n            # Reduce loss across data parallel groups\n            averaged_loss = avg_losses_across_data_parallel_group([weighted_loss])\n\n            return weighted_loss, {\"lm loss\": averaged_loss[0]}\n\n        return loss_func\n\n    def get_forward_step_func(self):\n        def forward_step(data_iterator, model):\n            \"\"\"Forward step.\"\"\"\n            # Get the batch.\n            tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)\n            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)\n\n            return output_tensor, partial(self.loss_func, tokens, loss_mask)\n\n        return forward_step\n```\n\n----------------------------------------\n\nTITLE: Syncing and Rebasing with Upstream\nDESCRIPTION: Commands to fetch updates from the upstream repository and reapply your local commits on top of the latest upstream changes using `git rebase`. This keeps your feature branch updated with the main project.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\n$ git fetch upstream\n$ git rebase upstream/main\n```\n\n----------------------------------------\n\nTITLE: Installing minGPT and huggingface_hub\nDESCRIPTION: These commands install the minGPT library and huggingface_hub, which are used for running the examples provided in the documentation. minGPT provides a simple GPT implementation, and huggingface_hub allows for downloading model weights.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/karpathy/minGPT.git\npip install minGPT/\npip install huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Using Accelerate Env Command (Bash)\nDESCRIPTION: Demonstrates the basic usage of the `accelerate env` command with potential arguments. This command displays the contents of the Accelerate configuration file and relevant environment details, useful for debugging and reporting issues on the GitHub repository. It supports specifying the config file path.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate env [arguments]\n```\n\n----------------------------------------\n\nTITLE: Calculating Total Batch Size with Megatron-LM Parallelism in Python\nDESCRIPTION: This Python snippet determines the effective total batch size used for training based on whether the Megatron-LM distributed backend is used. When using Megatron-LM, it fetches the global batch size from `accelerator.state.megatron_lm_plugin.global_batch_size`, accounting for tensor and pipeline parallelism. Otherwise, it computes the total batch size as the product of per-device batch size, number of processes, and gradient accumulation steps.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size\nelse:\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n```\n\n----------------------------------------\n\nTITLE: Early Stopping with Accelerate\nDESCRIPTION: This snippet showcases implementing early stopping conditionals in a distributed setup using `set_trigger` and `check_trigger` from the Accelerate library to ensure correct termination across processes.  This is particularly useful when a break or stopping condition depends on individual process results, such as validation loss.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/troubleshooting.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Assume `should_do_breakpoint` is a custom defined function that returns a conditional, \n# and that conditional might be true only on process 1\nif should_do_breakpoint(loss):\n    accelerator.set_trigger()\n\n# Later in the training script when we need to check for the breakpoint\nif accelerator.check_trigger():\n    break\n```\n\n----------------------------------------\n\nTITLE: Pushing Local Branch Changes to Remote\nDESCRIPTION: Command to push your local commits on the current branch to your remote fork on GitHub. This makes your changes available online and is necessary for creating or updating a pull request.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\n$ git push -u origin a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Estimating Memory with Specific Data Types\nDESCRIPTION: This command estimates the memory footprint of the `bert-base-cased` model for specific data types (`float32` and `float16`) using the `--dtypes` argument.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/model_size_estimator.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate estimate-memory bert-base-cased --dtypes float32 float16\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace Accelerate Environment Using Bash\nDESCRIPTION: Runs the Accelerate configuration CLI to create and customize the distributed training setup file. This bash snippet is used outside the notebook to generate a config file by answering interactive prompts. The config file guides subsequent training runs for device management and resource allocation. No parameters are passed; it runs as a terminal command.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/notebook.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: FP16 Compression Hook (Accelerate)\nDESCRIPTION: This code snippet shows how to use the FP16 compression hook with the Accelerate library. It sets up an `Accelerator` object, specifies the FP16 communication hook using `DistributedDataParallelKwargs`, and integrates the hook into the training loop. This requires Accelerate and PyTorch, and utilizes `DDPCommunicationHookType.FP16`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\n# DDP Communication Hook setup\nddp_kwargs = DistributedDataParallelKwargs(comm_hook=DDPCommunicationHookType.FP16)\naccelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n\nmodel = MyModel()\noptimizer = torch.optim.Adam(model.parameters())\ndata_loader = DataLoader(dataset, batch_size=16)\n\nmodel, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Launching FSDP2 Benchmark with Accelerate - Bash\nDESCRIPTION: Runs the benchmark script, main.py, using the Accelerate launch utility. Requires huggingface-accelerate to be installed and main.py present in the working directory. This command executes the default benchmark with configuration specified in the script or by additional command-line arguments. Standard outputs and results will be stored according to script settings.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fsdp2/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch main.py\n```\n\n----------------------------------------\n\nTITLE: Using PyTorch's no_sync for Gradient Accumulation (Python)\nDESCRIPTION: This snippet shows how to manually implement gradient accumulation optimization in PyTorch DDP. It uses the `model.no_sync()` context manager to prevent gradient synchronization on all batches except the last one within an accumulation cycle, reducing communication overhead. The gradients are only synchronized when `accelerator.backward(loss)` is called outside the `no_sync` context, typically before `optimizer.step()`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/gradient_synchronization.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)\n\nfor index, batch in enumerate(dataloader):\n    inputs, targets = batch\n    # Trigger gradient synchronization on the last batch\n    if index != (len(dataloader) - 1):\n        with ddp_model.no_sync():\n            # Gradients only accumulate\n            outputs = ddp_model(inputs)\n            loss = loss_func(outputs)\n            accelerator.backward(loss)\n    else:\n        # Gradients finally sync\n        outputs = ddp_model(inputs)\n        loss = loss_func(outputs)\n        accelerator.backward(loss)\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Using `accelerate estimate-memory` Command (Bash)\nDESCRIPTION: Demonstrates how to use the `accelerate estimate-memory` command to estimate the VRAM required for a specific model from the Hugging Face Hub. It requires the model name and optionally accepts the library name (like 'transformers' or 'timm') and desired data types (`float32`, `float16`, `int8`, `int4`).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate estimate-memory {MODEL_NAME} --library_name {LIBRARY_NAME} --dtypes {dtype_1} {dtype_2} ...\n```\n\n----------------------------------------\n\nTITLE: Accessing Custom Trackers with Accelerator.get_tracker in Python\nDESCRIPTION: Describes how to retrieve a specific custom tracker instance from the Accelerator object, enabling direct interaction with the underlying tracking library's API. Demonstrates unwrapping the tracker for manual operations on the main process.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/tracking.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nwandb_tracker = accelerator.get_tracker(\"wandb\")\n# Now you can call wandb-specific methods\nwandb_tracker.log_artifact(some_artifact_to_log)\n\n# To forcibly unwrap and ensure execution on main process\nif accelerator.is_main_process:\n    wandb_tracker.log_artifact(some_artifact_to_log)\n\n```\n\n----------------------------------------\n\nTITLE: Handling Training and Evaluation Loops with Rank-Dependent DataLoader Availability - Python\nDESCRIPTION: This snippet details a while loop structure used for Megatron-LM indexed datasets, where only certain ranks have access to the dataloader. It demonstrates conditional iteration, fallback to empty batches, and orchestrates both training and periodic evaluation based on completed steps and evaluation interval. Key parameters include max_train_steps, eval_interval, and eval_iters. Inputs are model, dataloaders, and step-tracking variables; outputs are periodic training and evaluation steps. Limitation is that dataloader must be checked for None for each rank each iteration.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwhile completed_steps < args.max_train_steps:\n    model.train()\n    batch = next(train_dataloader) if train_dataloader is not None else {}\n    outputs = model(**batch)\n    loss = outputs.loss\n    ...\n\n    if completed_steps % eval_interval == 0:\n        eval_completed_steps = 0\n        losses = []\n        while eval_completed_steps < eval_iters:\n            model.eval()\n            with torch.no_grad():\n                batch = next(eval_dataloader) if eval_dataloader is not None else {}\n                outputs = model(**batch)\n```\n\n----------------------------------------\n\nTITLE: Handling Loss Averaging Across Data Parallel Group with Megatron-LM in Python\nDESCRIPTION: This snippet shows how to append and aggregate losses differently based on the distributed backend. For Megatron-LM, the losses are already averaged across data parallel ranks and can be appended directly as tensors. For other setups, losses are gathered and repeated appropriately before concatenation. This distinction ensures correct loss computation for evaluation metrics in multi-GPU or parallel training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    losses.append(loss)\nelse:\n    losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    losses = torch.tensor(losses)\nelse:\n    losses = torch.cat(losses)\n```\n\n----------------------------------------\n\nTITLE: Installing and Setting Up Pre-commit Hooks\nDESCRIPTION: Commands to install the `pre-commit` tool and set up its hooks for the repository. This automates the execution of style and quality checks before each commit, ensuring code standards are met locally.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\n$ pip install pre-commit\n$ pre-commit install\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Accelerate Nightly GPU Image\nDESCRIPTION: Docker command to download the most recent nightly build of the Accelerate GPU image from Docker Hub.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docker/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull huggingface/accelerate:gpu-nightly\n```\n\n----------------------------------------\n\nTITLE: BF16 Compression Hook (Accelerate)\nDESCRIPTION: This code sets up BF16 compression hook with Accelerate.  It defines a model, creates an `Accelerator` with `DistributedDataParallelKwargs` set to `DDPCommunicationHookType.BF16`, prepares the model, optimizer, and data loader, and then executes a training loop. Requires Accelerate and PyTorch; BF16 compression requires an NCCL version later than 2.9.6.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\n# DDP Communication Hook setup\nddp_kwargs = DistributedDataParallelKwargs(comm_hook=DDPCommunicationHookType.BF16)\naccelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n\nmodel = MyModel()\noptimizer = torch.optim.Adam(model.parameters())\ndata_loader = DataLoader(dataset, batch_size=16)\n\nmodel, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Initializing Megatron-LM Compatible Scheduler in Python\nDESCRIPTION: This Python snippet shows how to initialize an optimizer scheduler compatible with Megatron-LM when using the Huggingface Accelerate framework. It imports `MegatronLMDummyScheduler` from `accelerate.utils` and conditionally chooses it if the distributed type is MEGATRON_LM. Otherwise, it falls back to a generic scheduler from `get_scheduler`. The scheduler requires the optimizer instance, total training steps, and number of warmup steps to manage learning rate adjustments during training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import MegatronLMDummyScheduler\n\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    lr_scheduler = MegatronLMDummyScheduler(\n        optimizer=optimizer,\n        total_num_steps=args.max_train_steps,\n        warmup_num_steps=args.num_warmup_steps,\n    )\nelse:\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n    )\n```\n\n----------------------------------------\n\nTITLE: Running CV example on multi CPUs\nDESCRIPTION: These commands demonstrate how to run the `cv_example.py` script on multiple CPUs using Accelerate config and launcher and with Intel MPI.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config --config_file config.yaml  # Select to have accelerate launch mpirun\naccelerate launch ./cv_example.py --data_dir path_to_data # This will run the script on each server\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport CCL_WORKER_COUNT=1\nexport MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip\nmpirun -f hostfile -n 16 -ppn 4 python ./cv_example.py --data_dir path_to_data\n```\n\n----------------------------------------\n\nTITLE: Docker Image Naming Convention Pattern for Accelerate\nDESCRIPTION: The standard naming pattern for Accelerate Docker images, showing the format that includes the accelerator type and build type (nightly or release).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docker/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface/accelerate:{accelerator}-{nightly,release}\n```\n\n----------------------------------------\n\nTITLE: Declaring Python Project Dependencies using requirements.txt\nDESCRIPTION: This code snippet represents the contents of a requirements.txt file, specifying the Python packages and their version constraints necessary for the project. Each line indicates a package name, optionally followed by a version specifier (e.g., `==` for exact version, `>=` for minimum version). Comments starting with `#` provide additional context.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naccelerate # used to be installed in Amazon SageMaker environment\nevaluate\ndatasets==2.3.2\nschedulefree\nhuggingface_hub>=0.20.0\n```\n\n----------------------------------------\n\nTITLE: BF16 Compression Hook (PyTorch)\nDESCRIPTION: This snippet applies the BF16 compression hook using PyTorch's `DistributedDataParallel`. It sets up a model and registers the `bf16_compress_hook`. The hook converts gradients to `torch.bfloat16` for reduced communication. It also includes a training loop. The BF16 Compression Hook API is experimental, and it requires NCCL version later than 2.9.6.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed.algorithms.ddp_comm_hooks import default_hooks\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\nmodel = MyModel()\nmodel = DDP(model, device_ids=[torch.cuda.current_device()])\nmodel.register_comm_hook(state=None, hook=default_hooks.bf16_compress_hook)\n\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Configure TransformersEngine FP8 Backend via config.yaml\nDESCRIPTION: Provides an example `config.yaml` snippet for configuring FP8 mixed precision training using the `TransformersEngine` backend. It sets `mixed_precision` to `fp8`, specifies `backend: TE`, and lists various `TransformersEngine`-specific parameters under the `fp8_config` key like `amax_compute_algo`, `fp8_format`, and `interval`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/low_precision_training.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmixed_precision: fp8\nfp8_config:\n  amax_compute_algo: max\n  amax_history_len: 1024\n  backend: TE\n  fp8_format: HYBRID\n  interval: 1\n  margin: 0\n  override_linear_precision: (false, false, false)\n  use_autocast_during_eval: false\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test File with Pytest\nDESCRIPTION: Command to execute a specific test file using pytest. This allows contributors to run only the tests relevant to their changes, speeding up the testing process during development.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n$ pytest tests/<TEST_TO_RUN>.py\n```\n\n----------------------------------------\n\nTITLE: Applying Code Style Corrections using Make\nDESCRIPTION: Command using `make` to automatically apply code style corrections and run automated code verifications using tools like `ruff`. This helps maintain code consistency across the project.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\n$ make style\n```\n\n----------------------------------------\n\nTITLE: DDP Hook with comm_state_option in Accelerate\nDESCRIPTION: This code uses the `comm_state_option` to pass additional state to a communication hook in Accelerate. Specifically, it sets the `matrix_approximation_rank` for PowerSGD.  It configures `DistributedDataParallelKwargs` with `comm_state_option` and then trains using Accelerate.  Requires Accelerate, PyTorch, and understanding the requirements of the specific hook, PowerSGD in this case.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, DDPCommunicationHookType, DistributedDataParallelKwargs\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\n# DDP Communication Hook setup\nddp_kwargs = DistributedDataParallelKwargs(\n    comm_hook=DDPCommunicationHookType.POWER_SGD,\n    comm_state_option={\"matrix_approximation_rank\": 2}\n)\naccelerator = Accelerator(kwargs_handlers=[ddp_kwargs])\n\nmodel = MyModel()\noptimizer = torch.optim.Adam(model.parameters())\ndata_loader = DataLoader(dataset, batch_size=16)\n\nmodel, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n\n# Training loop\n```\n\n----------------------------------------\n\nTITLE: Using `accelerate tpu-config` Command (Bash)\nDESCRIPTION: Shows the basic usage structure for the `accelerate tpu-config` command. This command is used to configure TPU settings, potentially using arguments specified directly or read from the Accelerate configuration file.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate tpu-config [arguments]\n```\n\n----------------------------------------\n\nTITLE: Preparing Multiple Disjoint Models in Python\nDESCRIPTION: Illustrates how to prepare two separate, disjoint models using their respective `Accelerator` instances or the shared state. It involves selecting the correct DeepSpeed plugin via the shared state (`accelerator.state.select_deepspeed_plugin`) before calling `prepare` for each model and its associated training components (optimizer, scheduler, dataloaders). Assumes a helper function `get_training_items` exists.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# can be `accelerator_0`, `accelerator_1`, or by calling `AcceleratorState().select_deepspeed_plugin(...)`\nfirst_accelerator.state.select_deepspeed_plugin(\"first_model\")\nfirst_model = AutoModel.from_pretrained(...)\n# For this example, `get_training_items` is a nonexistent function that gets the setup we need for training\nfirst_optimizer, first_scheduler, train_dl, eval_dl = get_training_items(model1)\nfirst_model, first_optimizer, first_scheduler, train_dl, eval_dl = accelerator.prepare(\n    first_model, first_optimizer, first_scheduler, train_dl, eval_dl\n)\n\nsecond_accelerator.state.select_deepspeed_plugin(\"second_model\")\nsecond_model = AutoModel.from_pretrained(...)\n# For this example, `get_training_items` is a nonexistent function that gets the setup we need for training\nsecond_optimizer, second_scheduler, _, _ = get_training_items(model2)\nsecond_model, second_optimizer, second_scheduler = accelerator.prepare(\n    second_model, second_optimizer, second_scheduler\n)\n```\n\n----------------------------------------\n\nTITLE: Docker Image Tag for Nightly CPU Build with Date\nDESCRIPTION: Example of a Docker image tag for a nightly CPU build of Accelerate from a specific date (March 14, 2024).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docker/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface/accelerate:cpu-nightly-2024-03-14\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Line Code Blocks in Markdown for Examples\nDESCRIPTION: Shows the standard Markdown syntax using triple backticks (```) followed by an optional language identifier (like `python`) to create multi-line code blocks. This format is used for displaying code examples within the documentation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n```python\n# first line of code\n# second line\n# etc\n```\n```\n\n----------------------------------------\n\nTITLE: FP16 Compression Hook (PyTorch)\nDESCRIPTION: This code snippet demonstrates how to apply the FP16 compression hook using PyTorch's `DistributedDataParallel` module.  It sets up a model, registers the `fp16_compress_hook`, and then runs a training loop.  The main dependency is PyTorch and the `torch.distributed.algorithms.ddp_comm_hooks` module.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed.algorithms.ddp_comm_hooks import default_hooks\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\nmodel = MyModel()\nmodel = DDP(model, device_ids=[torch.cuda.current_device()])\nmodel.register_comm_hook(state=None, hook=default_hooks.fp16_compress_hook)\n\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Using Accelerate Config Update Command (Bash)\nDESCRIPTION: Demonstrates the basic usage of the `accelerate config update` command with potential arguments. This command updates an existing Accelerate configuration file, incorporating new default settings while preserving existing ones. It supports specifying the path to the config file to update.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config update [arguments]\n```\n\n----------------------------------------\n\nTITLE: Initializing an Empty GPT Model\nDESCRIPTION: This code initializes an empty GPT model using the `init_empty_weights` context manager from Accelerate. This allows for creating a model without allocating memory for the weights, saving memory during the quantization process. The `GPT` class is from the `minGPT` library, and `model_config` defines the model's architecture.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/quantization.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import init_empty_weights\nfrom mingpt.model import GPT\n\nmodel_config = GPT.get_default_config()\nmodel_config.model_type = 'gpt2-xl'\nmodel_config.vocab_size = 50257\nmodel_config.block_size = 1024\n\nwith init_empty_weights():\n    empty_model = GPT(model_config)\n```\n\n----------------------------------------\n\nTITLE: Running Full Test Suite with Python -m pytest\nDESCRIPTION: Alternative command to run the entire test suite using the `python -m pytest` entry point. This is the underlying command executed by `make test` and can be used directly.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\n$ python -m pytest -sv ./tests\n```\n\n----------------------------------------\n\nTITLE: Launching Training Script with Accelerate CLI\nDESCRIPTION: This bash code snippet launches a training script with 'accelerate launch', passing any script-specific arguments. It enables running the training within the configured SageMaker environment with distributed or local setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/sagemaker.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./examples/sagemaker_example.py\n```\n\n----------------------------------------\n\nTITLE: Installing Watchdog for Live Preview\nDESCRIPTION: Installs the `watchdog` Python library using pip. This library is required by `doc-builder` to enable the live preview feature, which automatically rebuilds the documentation when source files are modified.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install watchdog\n```\n\n----------------------------------------\n\nTITLE: Running NLP example on TPUs\nDESCRIPTION: These commands demonstrate how to run the `nlp_example.py` script on TPUs using Accelerate config and launcher.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config  # This will create a config file on your TPU server\naccelerate launch ./nlp_example.py  # This will run the script on each server\n```\n\n----------------------------------------\n\nTITLE: Initializing Multiple Accelerators for Disjoint Models in Python\nDESCRIPTION: Demonstrates initializing two separate `Accelerator` instances when training multiple disjoint models with different DeepSpeed configurations. The first accelerator is initialized with the plugin dictionary, while the second uses the shared state and requires no arguments.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfirst_accelerator = Accelerator(deepspeed_plugins=deepspeed_plugins)\nsecond_accelerator = Accelerator()\n```\n\n----------------------------------------\n\nTITLE: Installing the SageMaker SDK for Accelerate via pip\nDESCRIPTION: This bash code snippet installs the 'accelerate' package with SageMaker support using pip, ensuring the necessary SDK is available for executing Accelerate scripts on SageMaker.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/sagemaker.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"accelerate[sagemaker]\" --upgrade\n```\n\n----------------------------------------\n\nTITLE: Saving Transformers Model with Accelerate\nDESCRIPTION: This shows how to save a Transformer model after training when using the Accelerate library with the Hugging Face Transformers library.  It uses the `save_pretrained` method to save the model.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModel\n\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(\n    \"path/to/my_model_directory\",\n    is_main_process=accelerator.is_main_process,\n    save_function=accelerator.save,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT model configuration with minGPT\nDESCRIPTION: This code initializes the configuration for a GPT model.  It uses the `GPT.get_default_config()` method to load the default configuration and then updates it to specify a model type, vocabulary size, and block size. These parameters determine the model's architecture and the size of the input sequences.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_6\n\nLANGUAGE: py\nCODE:\n```\nfrom accelerate import init_empty_weights\nfrom mingpt.model import GPT\n\nmodel_config = GPT.get_default_config()\nmodel_config.model_type = 'gpt2-xl'\nmodel_config.vocab_size = 50257\nmodel_config.block_size = 1024\n\nwith init_empty_weights():\n    model = GPT(model_config)\n```\n\n----------------------------------------\n\nTITLE: Preparing Teacher Model with Transformers Integration in Python\nDESCRIPTION: Demonstrates preparing the teacher model using `accelerator.prepare()` after selecting the appropriate DeepSpeed plugin ('teacher'). This leverages the automatic `deepspeed.zero.Init` context manager integration provided by Transformers if the model is loaded via `AutoModel`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nteacher_model = AutoModel.from_pretrained(...)\nteacher_model = accelerator.prepare(teacher_model)\n```\n\n----------------------------------------\n\nTITLE: Running DDP Communication Hook Example with Accelerate\nDESCRIPTION: Command for launching the DDP communication hook example script with mixed precision and PowerSGD hook for optimized gradient communication.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./ddp_comm_hook.py --mixed_precision fp16 --ddp_comm_hook power_sgd\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed FP8 Benchmark Scripts with Accelerate (Bash)\nDESCRIPTION: Uses the `accelerate launch` command to run distributed FP8 benchmark scripts like `ddp.py`, `distrib_deepspeed.py`, or `fsdp.py`. This command manages the execution environment for multi-GPU (DDP), DeepSpeed, or FSDP setups without requiring manual configuration via `accelerate config`. Requires the Accelerate library and the specified script.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fp8/transformer_engine/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ddp.py # or distrib_deepspeed.py, ddp.py\n```\n\n----------------------------------------\n\nTITLE: Selecting a Specific DeepSpeed Plugin in Python\nDESCRIPTION: Explains how to switch the active DeepSpeed plugin using `accelerator.state.select_deepspeed_plugin()`. This is necessary before preparing a model that requires a different configuration (e.g., the teacher model with ZeRO-3).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\naccelerator.state.select_deepspeed_plugin(\"teacher\")\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Training with Accelerate - Shell Example\nDESCRIPTION: This Bash snippet demonstrates the typical usage of the 'accelerate launch' command to execute a training script in a distributed fashion. It shows the command-line structure, where additional arguments can be appended for script and configuration parameters. Dependencies include the Accelerate Python package, valid and pre-configured environment (e.g., GPUs, TPUs), and access to the target training script. The main input is the training script path, followed by any script-specific arguments; outputs are the standard logs and side effects of the launched training job. Supported training paradigms and hardware types are determined by supplementary flags and configuration files.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/package_reference/cli.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Accelerate with DeepSpeed File - Bash\nDESCRIPTION: Guides the user to run the `accelerate config` command interactively to set up Accelerate. This process includes prompts to specify whether to use a DeepSpeed config file and its path, generating a configuration file that Accelerate will use for subsequent launches.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Config File Example\nDESCRIPTION: This is an example of a DeepSpeed configuration file (in YAML format) used with Accelerate. It specifies settings such as gradient accumulation steps, gradient clipping, ZeRO stage, and offload device configuration. It uses local machine as compute environment and DeepSpeed as distributed type.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\ncommand_file: null\ncommands: null\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: 'cpu'\n  offload_param_device: 'cpu'\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\n  deepspeed_config_file: 'ds_config.json'\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config: {}\ngpu_ids: null\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmegatron_lm_config: {}\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_name: null\ntpu_zone: null\nuse_cpu: false\n```\n\n----------------------------------------\n\nTITLE: Accessing Active DeepSpeed Plugin via Accelerator State in Python\nDESCRIPTION: Demonstrates an alternative way to access the currently active DeepSpeed plugin directly through the `Accelerator`'s state object (`accelerator.deepspeed_plugin`). This confirms the active plugin.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nassert active_plugin is accelerator.deepspeed_plugin\n```\n\n----------------------------------------\n\nTITLE: Building Accelerate Documentation using doc-builder\nDESCRIPTION: Executes the `doc-builder` command to build the documentation for the `accelerate` package. It takes the package name (`accelerate`) and the path to the source documentation files (`docs/source/`) as input, and outputs the built MDX files to the specified temporary directory (`--build_dir ~/tmp/test-build`).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndoc-builder build accelerate docs/source/ --build_dir ~/tmp/test-build\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate using pip (Bash)\nDESCRIPTION: Installs the Hugging Face Accelerate library using the pip package manager. This command should be run in a virtual environment after installing PyTorch (version 1.10.0+ is required), as Accelerate depends on it. Python 3.8+ is also a prerequisite.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install accelerate\n```\n\n----------------------------------------\n\nTITLE: Launching ZeRO Stage 3 Offload Example - Bash\nDESCRIPTION: Shows a specific command-line example using `accelerate launch` to run the sample script `examples/by_feature/deepspeed_with_config_support.py`. It assumes Accelerate is configured to use the ZeRO Stage 3 offload DeepSpeed config file and includes various script-specific arguments.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch examples/by_feature/deepspeed_with_config_support.py \\\n--config_name \"gpt2-large\" \\\n--tokenizer_name \"gpt2-large\" \\\n--dataset_name \"wikitext\" \\\n--dataset_config_name \"wikitext-2-raw-v1\" \\\n--block_size 128 \\\n--output_dir \"./clm/clm_deepspeed_stage3_offload_accelerate\" \\\n--learning_rate 5e-4 \\\n--per_device_train_batch_size 32 \\\n--per_device_eval_batch_size 32 \\\n--num_train_epochs 3 \\\n--with_tracking \\\n--report_to \"wandb\"\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies for Accelerate using Pip\nDESCRIPTION: Installs the necessary Python packages required to build the Accelerate documentation. It uses pip to install the extra dependencies defined under the `[docs]` section in the project's setup file in editable mode. Run this command from the root of the `accelerate` repository.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[docs]\"\n```\n\n----------------------------------------\n\nTITLE: Running Cross Validation Example with Accelerate\nDESCRIPTION: Command for launching the cross validation example script with a specified number of folds.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./cross_validation.py --num_folds 2\n```\n\n----------------------------------------\n\nTITLE: Save Checkpoint with ZeRO Stage-3\nDESCRIPTION: This code snippet is used when saving the DeepSpeed checkpoint and its state. It uses `model.save_checkpoint()`, which will produce the ZeRO model and optimizer partitions, along with `zero_to_fp32.py` script in checkpoint directory for consolidation.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nsuccess = model.save_checkpoint(PATH, ckpt_id, checkpoint_state_dict)\nstatus_msg = f\"checkpointing: PATH={PATH}, ckpt_id={ckpt_id}\"\nif success:\n    logging.info(f\"Success {status_msg}\")\nelse:\n    logging.warning(f\"Failure {status_msg}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Seeds for Distributed Reproducibility with Accelerate (Python)\nDESCRIPTION: This snippet demonstrates setting reproducible random seeds across different random number generators in distributed training using HuggingFace Accelerate's set_seed utility. It requires the accelerate library installed, and takes an integer as a seed parameter. The function ensures reproducibility by setting seed values for Python’s random module, NumPy, PyTorch, CUDA devices, and, if available, torch_xla for TPUs. No inputs aside from the seed integer are required; no outputs are produced. Use this before model initialization for fully deterministic behavior.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/performance.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate.utils import set_seed\n\nset_seed(42)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # or torch.xpu.manual_seed_all, etc\n    # ^^ safe to call this function even if cuda is not available\n    if is_torch_xla_available():\n        xm.set_rng_state(seed)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Redirects for Moved Documentation Sections\nDESCRIPTION: Demonstrates how to add HTML anchor tags within Markdown at the original location of a moved documentation section to preserve old links. This ensures users following old links are redirected to the new location. The examples show linking within the same file and linking to a different file using relative paths.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n[ <a href=\"#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\n\nLANGUAGE: markdown\nCODE:\n```\n[ <a href=\"../new-file#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\n\n----------------------------------------\n\nTITLE: Installing the doc-builder Tool from GitHub using Pip\nDESCRIPTION: Installs the `doc-builder` tool directly from its GitHub repository using pip. This tool is specifically used by Hugging Face projects to build documentation from source files.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/doc-builder\n```\n\n----------------------------------------\n\nTITLE: Example of init_empty_weights usage\nDESCRIPTION: This code provides a concrete example of how to use the `init_empty_weights` context manager to initialize a large, empty model. It demonstrates creating a sequential model with many linear layers, all initialized without consuming significant RAM, relying on the meta device.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nfrom accelerate import init_empty_weights\n\nwith init_empty_weights():\n    model = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])\n```\n\n----------------------------------------\n\nTITLE: Launching Accelerate Script (Bash)\nDESCRIPTION: Command using the `accelerate launch` utility to run a Python script (`phi2.py`) designed for distributed execution. The `--num_processes` flag specifies the number of GPU processes to use. This is the recommended way to run the examples.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/inference/distributed/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --num_processes {NUM_GPUS} phi2.py\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Accelerator Usage\nDESCRIPTION: This example demonstrates how to use the `Accelerator` class in the `accelerate` library. It shows how to implement conditional execution based on the main process, simulate a delay, and synchronize all processes.  Dependencies include the `accelerate` library and the `time` module. The expected output includes a print statement from non-main processes while the main process sleeps, followed by a print statement from all processes signaling that everyone has arrived.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> import time\n>>> from accelerate import Accelerator\n>>> accelerator = Accelerator()\n>>> if accelerator.is_main_process:\n...     time.sleep(2)\n>>> else:\n...     print(\"I'm waiting for the main process to finish its sleep...\")\n>>> accelerator.wait_for_everyone()\n>>> # Should print on every process at the same time\n>>> print(\"Everyone is here\")\n```\n\n----------------------------------------\n\nTITLE: Example Python Function Signature for Docstring Context\nDESCRIPTION: Provides an example Python function signature to illustrate the context for the accompanying docstring argument formatting example. It shows type hinting (`str`, `float`) and default argument values (`None`, `1`).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef my_function(x: str = None, a: float = 1):\n```\n\n----------------------------------------\n\nTITLE: Adding and Committing File Changes\nDESCRIPTION: Basic Git commands to stage modified files for inclusion in the next commit and then create a local commit with a descriptive message. This records your changes in your local branch history.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\n$ git add modified_file.py\n$ git commit\n```\n\n----------------------------------------\n\nTITLE: Conditionally Tying Weights for TPU in Python\nDESCRIPTION: This snippet checks if the current distributed type is TPU and, if so, applies weight tying to the model by calling its tie_weights() method. It is used to maintain the weight tying consistency when models are moved to TPUs, addressing issues where weight tying is broken.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/tpu.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nif accelerator.distributed_type == DistributedType.TPU:\n    model.tie_weights()\n```\n\n----------------------------------------\n\nTITLE: Configuring Megatron-LM Plugin with Custom Loss and Keytoken IDs - Python\nDESCRIPTION: This snippet sets up the Megatron-LM plugin in Accelerate with a custom training step class that utilizes a user-specified set of key token IDs and an alpha scaling parameter for custom loss weighting. It demonstrates initialization of keytoken_ids via tokenization, dynamic configuration, and integration with the plugin state. Requires a tokenizer and an initialized accelerator state. Inputs are keywords for token IDs and configuration parameters; outputs update the plugin state. Limitations are reliant on proper tokenizer availability and plugin state.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/megatron_lm.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    # Custom loss function for the Megatron model\n    keytoken_ids = []\n    keywords = [\"plt\", \"pd\", \"sk\", \"fit\", \"predict\", \" plt\", \" pd\", \" sk\", \" fit\", \" predict\"]\n    for keyword in keywords:\n        ids = tokenizer([keyword]).input_ids[0]\n        if len(ids) == 1:\n            keytoken_ids.append(ids[0])\n    accelerator.print(f\"Keytoken ids: {keytoken_ids}\")\n    accelerator.state.megatron_lm_plugin.custom_train_step_class = GPTTrainStepWithCustomLoss\n    accelerator.state.megatron_lm_plugin.custom_train_step_kwargs = {\n        \"keytoken_ids\": keytoken_ids,\n        \"alpha\": 0.25\n    }\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed ZeRO-2 Configuration for Training (JSON)\nDESCRIPTION: Example JSON configuration for DeepSpeed using ZeRO stage 2 optimized for training. It includes sections for bf16, AdamW optimizer, WarmupLR scheduler, and ZeRO-2 settings with optimizer offloading to CPU. Essential training parameters like batch sizes and gradient clipping are set to 'auto'.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed_multiple_model.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\",\n            \"torch_adam\": true,\n            \"adam_w_mode\": true\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n    },\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_clipping\": \"auto\",\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading dataset on main process with accelerate.main_process_first()\nDESCRIPTION: This snippet shows how to download datasets on the main process and load cached data across multiple processes. The context manager ensures that dataset download and initialization happen only once on the main process, then propagated to other processes, preventing redundant downloads.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/deferring_execution.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nwith accelerator.main_process_first():\n    datasets = load_dataset(\"glue\", \"mrpc\")\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Main Branch with Upstream\nDESCRIPTION: Commands to synchronize your local `main` branch with the latest changes from the official upstream repository. It's crucial to keep your local `main` branch updated before creating new feature branches.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n$ git checkout main\n$ git fetch upstream\n$ git merge upstream/main\n```\n\n----------------------------------------\n\nTITLE: Using accelerate.wait_for_everyone() for process synchronization in distributed training\nDESCRIPTION: This code demonstrates how to synchronize processes during distributed training to ensure that all processes reach a certain point before continuing. It is useful when performing operations like saving models or loading datasets ensuring consistency across processes.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/deferring_execution.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naccelerator.wait_for_everyone()\n```\n\n----------------------------------------\n\nTITLE: Creating a New Development Branch\nDESCRIPTION: Command to create and switch to a new Git branch for your specific contribution. It is recommended to create a new branch for every pull request to isolate changes.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n$ git checkout -b a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Processes with wait_for_everyone - Python\nDESCRIPTION: This code uses `accelerator.wait_for_everyone()` to synchronize all processes. It ensures that all processes reach this point before any continue, preventing issues like saving a model before all processes have finished training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\naccelerator.wait_for_everyone()\n```\n\n----------------------------------------\n\nTITLE: Function Execution on a Specific Process - Python\nDESCRIPTION: This code demonstrates how to use the `@accelerator.on_process` decorator to execute a function only on a specific process, identified by its `process_index`. Here, it executes `do_my_thing` only on process index 0.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@accelerator.on_process(process_index=0)\ndef do_my_thing():\n    \"Something done on process index 0\"\n    do_thing_on_index_zero()\n```\n\n----------------------------------------\n\nTITLE: Function Execution on Main Process - Python\nDESCRIPTION: This code defines a function `do_my_thing` and decorates it with `@accelerator.on_main_process`. This decorator ensures the function is executed only once across all processes, regardless of the number of machines.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/execution.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@accelerator.on_main_process\ndef do_my_thing():\n    \"Something done once per server\"\n    do_thing_once()\n```\n\n----------------------------------------\n\nTITLE: Debugging with Accelerate (Environment Variable)\nDESCRIPTION: This snippet illustrates how to enable Accelerate's debug mode using an environment variable, which is an alternative method to using the CLI.  It demonstrates using `torchrun` to run the script, and the use of `--arg1` and `--arg2` as command-line arguments.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/troubleshooting.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nACCELERATE_DEBUG_MODE=\"1\" torchrun {my_script.py} --arg1 --arg2\n```\n\n----------------------------------------\n\nTITLE: Offloading Model to CPU Using Accelerate in Python\nDESCRIPTION: This Python snippet demonstrates the use of the `cpu_offload` function from Accelerate to offload all model parameters to the CPU. During forward passes, the parameters are dynamically moved to the specified execution device and offloaded afterward again. This approach minimizes GPU memory usage at the cost of performance overhead and maintains a single state dictionary copy. It requires a model to offload and an execution device where the model will run forward computations.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncpu_offload(model, execution_device)\n```\n\n----------------------------------------\n\nTITLE: Running NLP example on multi CPUs\nDESCRIPTION: These commands demonstrate how to run the `nlp_example.py` script on multiple CPUs using Accelerate config and launcher and with Intel MPI.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config  # Select to have accelerate launch mpirun\naccelerate launch ./nlp_example.py  # This will run the script on each server\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport CCL_WORKER_COUNT=1\nexport MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip\nmpirun -f hostfile -n 16 -ppn 4 python ./nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: Modifying Training Function Signature for Model Input Python\nDESCRIPTION: This snippet shows a diff illustrating how to modify the `training_function` to accept the pre-declared model instance as an argument. The `+` indicates adding the `model` parameter to the function signature, and the `-` indicates removing the internal model instantiation line. This allows the external model instance to be passed into the training logic.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/training_tpu.md#_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n+ def training_function(model):\n      # Initialize accelerator\n      accelerator = Accelerator()\n-     model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n      train_dataloader, eval_dataloader = create_dataloaders(\n          train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n      )\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Script with Accelerate Specifying Process Count using Bash\nDESCRIPTION: Illustrates how to launch a distributed Python script (`distributed_inference.py`) on a specific number of processes (e.g., 2 GPUs) using the `--num_processes` flag, without requiring a pre-existing configuration file.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/distributed_inference.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --num_processes 2 distributed_inference.py\n```\n\n----------------------------------------\n\nTITLE: PowerSGD Hook (PyTorch)\nDESCRIPTION: This snippet demonstrates the use of PowerSGD hook in PyTorch for gradient compression. It sets up the model, initializes the `PowerSGDState`, registers the `powerSGD_hook`, and includes a training loop.  It has dependencies on PyTorch and `torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/ddp_comm_hook.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 10)\n\n    def forward(self, x):\n        return self.layer(x)\n\nmodel = MyModel()\nmodel = DDP(model, device_ids=[torch.cuda.current_device()])\nstate = powerSGD_hook.PowerSGDState(process_group=None)\nmodel.register_comm_hook(state=state, hook=powerSGD_hook.powerSGD_hook)\n\n# Training loop\nfor data, targets in data_loader:\n    outputs = model(data)\n    loss = criterion(outputs, targets)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Loading Big Model with Transformers dtype and device_map Python\nDESCRIPTION: Shows how to combine Big Model Inference via `device_map=\"auto\"` with memory saving techniques by specifying a lower precision data type (`torch_dtype=torch.float16`) during model loading using Hugging Face Transformers `from_pretrained`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\", torch_dtype=torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Benchmark Results - Bash\nDESCRIPTION: Executes visualize.py to generate plots showing memory usage from prior benchmark runs. Accepts a --dir argument specifying the path to the directory containing output results generated by running main.py with memory logging enabled. Requires Python 3 and all dependencies for visualize.py (such as matplotlib). Outputs visualization files to the same or specified directory.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fsdp2/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 visualize.py --dir <path_to_output_dir>\n```\n\n----------------------------------------\n\nTITLE: Installing accelerate and torchpippy via pip\nDESCRIPTION: This snippet demonstrates how to install the necessary dependencies for using PiPPy with accelerate.  It specifies the minimum required versions for accelerate (0.27.0 or greater) and torchpippy (0.2.0 or greater). Dependencies are installed using pip, either directly from the repository's setup.py file or manually.  The primary purpose is to set up the environment for distributed inference.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/inference/pippy/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'accelerate>=0.27.0' 'torchpippy>=0.2.0'\n```\n\n----------------------------------------\n\nTITLE: Defining Maximum Memory Dictionary for Efficient GPU Usage in Python\nDESCRIPTION: This snippet defines a Python dictionary representing memory limits for multiple GPUs to optimize batch size and memory allocation when running a very large model such as BLOOM-176B on an 8-GPU setup. GPU 0 is allocated less memory to reserve capacity for output processing, while the other seven GPUs are assigned approximately 50% more memory to balance workload and avoid out-of-memory errors. This dictionary can be used as the `max_memory` parameter in device map inference or manual device assignment.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmax_memory = {0: \"30GIB\", 1: \"46GIB\", 2: \"46GIB\", 3: \"46GIB\", 4: \"46GIB\", 5: \"46GIB\", 6: \"46GIB\", 7: \"46GIB\"}\n```\n\n----------------------------------------\n\nTITLE: Running the Python Script using accelerate launch\nDESCRIPTION: This command executes a Python script (bert.py in this example) using the accelerate launch command for distributed training.  It's the recommended way to run the scripts provided in the documentation.  The command utilizes the accelerate framework for parallel processing across multiple GPUs without requiring additional configuration in most cases.  The script takes no parameters but requires the correct path to the python file.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/inference/pippy/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch bert.py\n```\n\n----------------------------------------\n\nTITLE: Error Message for Config Conflict\nDESCRIPTION: This bash snippet shows the error message displayed when the Accelerate config file and the DeepSpeed config file are inconsistent, specifically when settings are specified in both places which causes them to conflict. The error suggests the user to reconfigure using `accelerate config`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nValueError: When using `deepspeed_config_file`, the following accelerate config variables will be ignored:\n['gradient_accumulation_steps', 'gradient_clipping', 'zero_stage', 'offload_optimizer_device', 'offload_param_device',\n'zero3_save_16bit_model', 'mixed_precision'].\nPlease specify them appropriately in the DeepSpeed config file.\nIf you are using an accelerate config file, remove other config variables mentioned in the above specified list.\nThe easiest method is to create a new config following the questionnaire via `accelerate config`.\n```\n\n----------------------------------------\n\nTITLE: Formatting Return Values in Docstrings (Google Style)\nDESCRIPTION: Demonstrates the format for documenting return values in docstrings using the `Returns:` keyword, following Google style guidelines for `doc-builder`. It shows how to specify the return type and provide a description for both single return values (e.g., `List[int]`) and tuples containing multiple, potentially complex, elements (e.g., `torch.FloatTensor`).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/README.md#_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n    Returns:\n        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.\n```\n\nLANGUAGE: markdown\nCODE:\n```\n    Returns:\n        `tuple(torch.FloatTensor)` comprising various elements depending on the configuration ([`BertConfig`]) and inputs:\n        - ** loss** (*optional*, returned when `masked_lm_labels` is provided) `torch.FloatTensor` of shape `(1,)` --\n          Total loss is the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        - **prediction_scores** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --\n          Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n```\n\n----------------------------------------\n\nTITLE: Accessing hf_device_map\nDESCRIPTION: This code shows how to access the `hf_device_map` attribute of the model after loading the checkpoint and dispatching the weights.  This attribute is a dictionary that specifies the device each layer of the model is placed on (e.g., GPU 0, GPU 1, CPU, or disk).\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_8\n\nLANGUAGE: py\nCODE:\n```\nmodel.hf_device_map\n```\n\n----------------------------------------\n\nTITLE: Basic PyTorch Training Loop\nDESCRIPTION: This is a standard PyTorch training loop that serves as the starting point for integrating Accelerate. It assumes that the `model`, `optimizer`, `training_dataloader`, `loss_function`, and `scheduler` are already defined and initialized. The loop iterates through batches, performs forward and backward passes, and updates the model parameters.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cuda\"\nmodel.to(device)\n\nfor batch in training_dataloader:\n    optimizer.zero_grad()\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Loading Model Weights\nDESCRIPTION: This code snippet demonstrates the typical PyTorch workflow for loading a model's weights from a checkpoint file. It involves creating a model instance, loading the state dictionary from the checkpoint file using `torch.load`, and then applying these weights to the model with `load_state_dict`. This standard approach can have memory limitations for very large models.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport torch\n\nmy_model = ModelClass(...)\nstate_dict = torch.load(checkpoint_file)\nmy_model.load_state_dict(state_dict)\n```\n\n----------------------------------------\n\nTITLE: Configure MS-AMP FP8 Backend via config.yaml\nDESCRIPTION: Shows the `config.yaml` equivalent for configuring the MS-AMP backend for FP8 training. It sets `mixed_precision: fp8` and specifies `backend: MSAMP` along with the desired `opt_level` (e.g., `O2`) under the `fp8_config` section.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/low_precision_training.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmixed_precision: fp8\nfp8_config:\n    backend: MSAMP\n    opt_level: O2\n```\n\n----------------------------------------\n\nTITLE: Using DeepSpeed with Accelerate for Distributed Training\nDESCRIPTION: This snippet shows how to integrate DeepSpeed with Accelerate for more efficient distributed training, including how to save Transformer models properly.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator, DeepSpeedPlugin\n\n# deepspeed needs to know your gradient accumulation steps beforehand, so don't forget to pass it\n# Remember you still need to do gradient accumulation by yourself, just like you would have done without deepspeed\ndeepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=2)\naccelerator = Accelerator(mixed_precision='fp16', deepspeed_plugin=deepspeed_plugin)\n\n# How to save your 🤗 Transformer?\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(save_dir, save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n```\n\n----------------------------------------\n\nTITLE: Simplified PyTorch Training with Device Placement Handled by Accelerate\nDESCRIPTION: This example shows how Accelerate can manage device placement automatically, further simplifying the PyTorch training loop by removing manual device placement code.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n\nmodel = torch.nn.Transformer()\noptimizer = torch.optim.Adam(model.parameters())\n\ndataset = load_dataset('my_dataset')\ndata = torch.utils.data.DataLoader(dataset, shuffle=True)\n\nmodel, optimizer, data = accelerator.prepare(model, optimizer, data)\n\nmodel.train()\nfor epoch in range(10):\n    for source, targets in data:\n        optimizer.zero_grad()\n\n        output = model(source)\n        loss = F.cross_entropy(output, targets)\n\n        accelerator.backward(loss)\n\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Print Accelerator State\nDESCRIPTION: This code initializes an `Accelerator` object and then prints the current `AcceleratorState`. This is useful for debugging and understanding the distributed training setup. It provides information about the environment configuration.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/deepspeed.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    accelerator = Accelerator()\n    accelerator.print(f\"{AcceleratorState()}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Basic PyTorch Training Loop\nDESCRIPTION: A standard PyTorch training loop that includes forward pass, loss calculation, backpropagation, and optimization steps. This serves as the starting point for Accelerate integration examples.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/explore.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor batch in dataloader:\n    optimizer.zero_grad()\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Citing Accelerate using BibTeX\nDESCRIPTION: Provides the BibTeX entry for citing the Hugging Face Accelerate library in academic publications or research papers. Includes authors, title, publication URL, and year.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_7\n\nLANGUAGE: bibtex\nCODE:\n```\n@Misc{accelerate,\n  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},\n  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},\n  howpublished = {\\url{https://github.com/huggingface/accelerate}},\n  year =         {2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Running NLP example with fp16 (mixed-precision)\nDESCRIPTION: These commands demonstrate how to run the `nlp_example.py` script with fp16 mixed-precision. The first passes `mixed_precision=fp16` to the `Accelerator`, and the second uses the `accelerate launch` command with the `--mixed_precision` flag.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ./nlp_example.py --mixed_precision fp16\n```\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --mixed_precision fp16 ./nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: Installing huggingface_hub\nDESCRIPTION: This snippet shows the command to install the `huggingface_hub` Python package, which is used to download models from the Hugging Face Hub. This is a prerequisite for downloading the sharded GPT-2 model in the subsequent example.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Running Single-GPU FP8 Benchmark Script (Bash)\nDESCRIPTION: Executes the Python script `non_distributed.py` to perform FP8 benchmark testing on a single GPU. This script compares native TransformerEngine performance with Accelerate's integration in a non-distributed setting. Requires Python and the script file to be available.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/benchmarks/fp8/transformer_engine/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython non_distributed.py\n```\n\n----------------------------------------\n\nTITLE: Setting Device with Accelerator\nDESCRIPTION: Demonstrates how to get the appropriate device for training from the Accelerator. This replaces manual device selection with .cuda() calls and works across different hardware configurations.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/quicktour.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndevice = accelerator.device\n```\n\n----------------------------------------\n\nTITLE: Initializing Accelerate\nDESCRIPTION: This snippet demonstrates how to import and initialize the `Accelerator` class.  The `Accelerator` instance manages the distributed training setup and provides access to methods for enabling PyTorch code to work in distributed environments.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/migration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n```\n\n----------------------------------------\n\nTITLE: Cloning minGPT\nDESCRIPTION: This is a set of bash commands to install the required minGPT library. First, the repository is cloned using git and then the necessary dependencies are installed using pip.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/docs/source/concept_guides/big_model_inference.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/karpathy/minGPT.git\npip install minGPT/\n```\n\n----------------------------------------\n\nTITLE: Launching Training from a Jupyter Notebook with Accelerate\nDESCRIPTION: Shows how to use the notebook_launcher function to run distributed training directly from a Jupyter notebook, which is useful for environments like Colab or Kaggle with TPU backends.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom accelerate import notebook_launcher\n\nnotebook_launcher(training_function)\n```\n\n----------------------------------------\n\nTITLE: Running NLP example on single GPU\nDESCRIPTION: This command demonstrates how to run the `nlp_example.py` script on a single GPU. It assumes that the script is executed from a server with a GPU.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ./nlp_example.py  # from a server with a GPU\n```\n\n----------------------------------------\n\nTITLE: Running Profiler Example with Accelerate\nDESCRIPTION: Command for launching the profiler example script with various profiling options enabled to analyze performance and resource usage during training.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ./profiler.py --record_shapes --profile_memory --with_flops --output_trace_dir \"profiler\"\n```\n\n----------------------------------------\n\nTITLE: Launching Accelerate Training with CLI Commands\nDESCRIPTION: These bash commands demonstrate how to configure and launch training scripts using Accelerate's CLI tool, which simplifies distributed training setup.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\naccelerator launch my_script.py --args_to_my_script\naccelerator launch examples/nlp_example.py\naccelerator launch --multi_gpu --num_processes 2 examples/nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: Launching Multi-CPU Training Using MPI with Accelerate\nDESCRIPTION: Example showing how to launch multi-CPU training using MPI with Accelerate, either through the CLI configuration or directly using mpirun.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naccelerate config\naccelerator launch examples/nlp_example.py\nmpirun -np 2 python examples/nlp_example.py\n```\n\n----------------------------------------\n\nTITLE: Running the Base Example with Accelerate\nDESCRIPTION: Command for launching the base NLP example script with mixed precision and CPU options.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/by_feature/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch ../nlp_example.py --mixed_precision fp16 --cpu 0\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for CV Example\nDESCRIPTION: This command installs the required dependencies for the computer vision example, including `timm` and `torchvision`.\nSOURCE: https://github.com/huggingface/accelerate/blob/main/examples/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install timm torchvision\n```"
  }
]
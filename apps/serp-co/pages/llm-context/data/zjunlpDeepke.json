[
  {
    "owner": "zjunlp",
    "repo": "deepke",
    "content": "TITLE: Installing DeepKE from Source\nDESCRIPTION: Clone the DeepKE repository and set up a virtual environment using Anaconda. The setup process includes installing dependencies and the package itself.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 https://github.com/zjunlp/DeepKE.git\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n deepke python=3.8\n\nconda activate deepke\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n\npython setup.py install\n\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Docker Setup Commands\nDESCRIPTION: Commands for pulling and running the DeepKE Docker image for containerized deployment.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull zjunlp/deepke:latest\ndocker run -it zjunlp/deepke:latest /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Defining Instruction Mappings for OneKE Tasks in Python\nDESCRIPTION: This code snippet defines a dictionary mapping task types to instruction templates for various information extraction tasks in Chinese and English. It covers tasks like named entity recognition, relation extraction, event extraction, and knowledge graph construction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE_old.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninstruction_mapper = {\n    'NERzh': \"你是专门进行实体抽取的专家。请从input中抽取出符合schema定义的实体，不存在的实体类型返回空列表。请按照JSON字符串的格式回答。\",\n    'REzh': \"你是专门进行关系抽取的专家。请从input中抽取出符合schema定义的关系三元组，不存在的关系返回空列表。请按照JSON字符串的格式回答。\",\n    'EEzh': \"你是专门进行事件提取的专家。请从input中抽取出符合schema定义的事件，不存在的事件返回空列表，不存在的论元返回NAN，如果论元存在多值请返回列表。请按照JSON字符串的格式回答。\",\n    'EETzh': \"你是专门进行事件提取的专家。请从input中抽取出符合schema定义的事件类型及事件触发词，不存在的事件返回空列表。请按照JSON字符串的格式回答。\",\n    'EEAzh': \"你是专门进行事件论元提取的专家。请从input中抽取出符合schema定义的事件论元及论元角色，不存在的论元返回NAN或空字典，如果论元存在多值请返回列表。请按照JSON字符串的格式回答。\",\n    'KGzh': '你是一个图谱实体知识结构化专家。根据输入实体类型(entity type)的schema描述，从文本中抽取出相应的实体实例和其属性信息，不存在的属性不输出, 属性存在多值就返回列表，并输出为可解析的json格式。',\n    'NERen': \"You are an expert in named entity recognition. Please extract entities that match the schema definition from the input. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.\",\n    'REen': \"You are an expert in relationship extraction. Please extract relationship triples that match the schema definition from the input. Return an empty list for relationships that do not exist. Please respond in the format of a JSON string.\",\n    'EEen': \"You are an expert in event extraction. Please extract events from the input that conform to the schema definition. Return an empty list for events that do not exist, and return NAN for arguments that do not exist. If an argument has multiple values, please return a list. Respond in the format of a JSON string.\",\n    'EETen': \"You are an expert in event extraction. Please extract event types and event trigger words from the input that conform to the schema definition. Return an empty list for non-existent events. Please respond in the format of a JSON string.\",\n    'EEAen': \"You are an expert in event argument extraction. Please extract event arguments and their roles from the input that conform to the schema definition, which already includes event trigger words. If an argument does not exist, return NAN or an empty dictionary. Please respond in the format of a JSON string.\", \n    'KGen': 'You are an expert in structured knowledge systems for graph entities. Based on the schema description of the input entity type, you extract the corresponding entity instances and their attribute information from the text. Attributes that do not exist should not be output. If an attribute has multiple values, a list should be returned. The results should be output in a parsable JSON format.',\n}\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE from source in Python\nDESCRIPTION: Step-by-step guide for installing DeepKE from source code using Git and Anaconda. This includes creating a virtual environment and installing dependencies.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 https://github.com/zjunlp/DeepKE.git\n\nconda create -n deepke python=3.8\nconda activate deepke\n\npip install -r requirements.txt\npython setup.py install\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE-LLM Environment Using Conda\nDESCRIPTION: Commands to create and set up a dedicated conda environment for DeepKE-LLM, which is the large language model version of DeepKE. This snippet shows how to create a Python 3.9 environment and install the required dependencies.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE-LLM Environment Using Conda\nDESCRIPTION: Commands to create and set up a dedicated conda environment for DeepKE-LLM, which is the large language model version of DeepKE. This snippet shows how to create a Python 3.9 environment and install the required dependencies.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for CPM-Bee Fine-tuning\nDESCRIPTION: Installs required packages for fine-tuning CPM-Bee. Note that specific versions of torch (≥1.10, <2.0.0) and Python (≥3.7) are required.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: LoRA Fine-tuning for LLaMA/Alpaca Models in Bash\nDESCRIPTION: This script demonstrates how to perform LoRA fine-tuning on LLaMA or Alpaca models. It includes various parameters for training configuration, such as batch size, learning rate, and LoRA-specific settings.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/oneke-continue'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/OneKE' \\\n    --stage 'sft' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_r 64 \\\n    --lora_alpha 64 \\\n    --lora_dropout 0.05 \\\n    --bf16 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Converting Training Data for Knowledge Graph Tasks in Python\nDESCRIPTION: This script converts raw knowledge graph data into instruction-formatted training data, supporting tasks like NER, RE, and EE. It allows customization of language, negative sampling ratio, schema embedding, and instruction formats.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/kg2instruction/README_ZH.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython kg2instruction/convert.py \\\n  --src_path data/NER/sample.json \\\n  --tgt_path data/NER/processed.json \\\n  --schema_path data/NER/schema.json \\\n  --language zh \\      # 指定转换脚本和模板使用的语言, ['zh', 'en']\n  --task NER \\         # 指定任务类型：['RE', 'NER', 'EE', 'EET', 'EEA'] 中的一种\n  --sample -1 \\        # 如果为-1，则随机采样20种指令和4种输出格式中的一种；如果为指定数值，则使用对应的指令格式，取值范围为 -1<=sample<20\n  --neg_ratio 1 \\      # 设置所有样本的负采样比例, 1表示所有样本都负采样\n  --neg_schema 1 \\     # 设置从schema中负采样的比例, 1表示整个schema都要嵌入到指令中\n  --random_sort        # 是否对指令中的schema列表进行随机排序\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE Dependencies with pip\nDESCRIPTION: Commands to clone the DeepKE repository, navigate to the multimodal relation extraction example directory, and install required dependencies using pip.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/multimodal/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/re/multimodal\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies Declaration\nDESCRIPTION: Specifies required Python packages and their version constraints for the DeepKE project. Includes major dependencies like PyTorch, BMTrain, and various NLP/ML related packages.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch>=1.10,<2.0.0\nbmtrain>=0.2.1\njieba\ntqdm\ntensorboard\nnumpy>=1.21.0\nspacy\nopendelta\n```\n\n----------------------------------------\n\nTITLE: Running In-context Learning for Relation Extraction with GPT-3\nDESCRIPTION: Command-line usage for the gpt3ICL.py script, which performs in-context learning for relation extraction using GPT-3. It allows specifying API key, data paths, output directories, prompt types, and the number of demonstration shots.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/UnleashLLMRE/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n>> python gpt3ICL.py -h\n    usage: gpt3ICL.py [-h] --api_key API_KEY --train_path TRAIN_PATH --test_path TEST_PATH --output_success OUTPUT_SUCCESS --output_nores OUTPUT_NORES --prompt {text,text_schema,instruct,instruct_schema} [--k K]\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --api_key API_KEY, -ak API_KEY\n      --train_path TRAIN_PATH, -tp TRAIN_PATH\n                            The path of training / demonstration data.\n      --test_path TEST_PATH, -ttp TEST_PATH\n                            The path of test data.\n      --output_success OUTPUT_SUCCESS, -os OUTPUT_SUCCESS\n                            The output directory of successful ICL samples.\n      --output_nores OUTPUT_NORES, -on OUTPUT_NORES\n                            The output directory of failed ICL samples.\n      --prompt {text,text_schema,instruct,instruct_schema}\n      --k K                 k-shot demonstrations\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for DeepKE\nDESCRIPTION: This snippet lists the required Python packages and their specific versions needed to run the DeepKE project. It includes PyTorch, transformers libraries, Hydra framework, and PyLD for JSON-LD processing.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/cnschema/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\ntorch==1.10\npytorch_transformers==1.2.0\ntransformers==3.4.0\nhydra-core==1.0.6\npyld==2.0.3\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for DeepKE Project\nDESCRIPTION: This snippet lists the required Python packages and their versions for the DeepKE project. It includes popular machine learning libraries like PyTorch and Transformers, as well as utilities for data processing and Chinese language support.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\ntorch==2.0.0\naccelerate==0.21.0\ntransformers==4.33.0\nbitsandbytes==0.39.1\npeft==0.4.0\ndatasets==2.16.1\nsentencepiece==0.1.98\nscipy==1.9.1\nprotobuf==3.20.1\npydantic==1.10.7\ntiktoken==0.6.0\njieba\nrouge_chinese\nhuggingface-hub==0.20.3\n```\n\n----------------------------------------\n\nTITLE: Custom Relation Prediction Function\nDESCRIPTION: Python function for handling relation prediction input either using example text or custom user input. Processes entity pairs and their types for relation extraction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _get_predict_instance(cfg):\n    flag = input('是否使用范例[y/n]，退出请输入: exit .... ')\n    flag = flag.strip().lower()\n    if flag == 'y' or flag == 'yes':\n        sentence = '歌曲《人生长路》出自刘德华国语专辑《男人的爱》，由李泉作词作曲，2001年出行发版'\n        head = '男人的爱'\n        tail = '人生长路'\n        head_type = '所属专辑'\n        tail_type = '歌曲'\n    elif flag == 'n' or flag == 'no':\n        sentence = input('请输入句子：')\n        head = input('请输入句中需要预测关系的头实体：')\n        head_type = input('请输入头实体类型（可以为空，按enter跳过）：')\n        tail = input('请输入句中需要预测关系的尾实体：')\n        tail_type = input('请输入尾实体类型（可以为空，按enter跳过）：')\n    elif flag == 'exit':\n        sys.exit(0)\n    else:\n        print('please input yes or no, or exit!')\n        _get_predict_instance()\n\n    instance = dict()\n    instance['sentence'] = sentence.strip()\n    instance['head'] = head.strip()\n    instance['tail'] = tail.strip()\n    if head_type.strip() == '' or tail_type.strip() == '':\n        cfg.replace_entity_with_type = False\n        instance['head_type'] = 'None'\n        instance['tail_type'] = 'None'\n    else:\n        instance['head_type'] = head_type.strip()\n        instance['tail_type'] = tail_type.strip()\n\n    return instance\n```\n\n----------------------------------------\n\nTITLE: Prompt Example for Data Augmentation in Relation Extraction\nDESCRIPTION: An example prompt used for data augmentation in relation extraction tasks. This prompt guides the language model to generate additional labeled data based on existing few-shot examples for a specific relation type.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/LLMICL/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n'''\nOne sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context and their entity types. \n\nThe head entity has the relation with the tail entity and entities are pre-categorized as the following types: URL, LOCATION, IDEOLOGY, CRIMINAL CHARGE, TITLE, STATE OR PROVINCE, DATE, PERSON, NUMBER, CITY, DURATION, CAUSE OF DEATH, COUNTRY, NATIONALITY, RELIGION, ORGANIZATION, MISCELLANEOUS. \n\nHere are some samples for relation 'org:founded_by':\n\nRelation: org:founded_by. Context: Talansky is also the US contact for the New Jerusalem Foundation , an organization founded by Olmert while he was Jerusalem 's mayor . Head Entity: New Jerusalem Foundation. Head Type: ORGANIZATION. Tail Entity: Olmert. Tail Type: PERSON.\n\nRelation: org:founded_by. Context: Sharpton has said he will not endorse any candidate until hearing more about their views on civil rights and other issues at his National Action Network convention next week in New York City . Head Entity: National Action Network. Head Type: ORGANIZATION. Tail Entity: his. Tail Type: PERSON.\n\nRelation: org:founded_by. Context: `` We believe that we can best serve our clients by offering a single multistrategy hedge fund platform , '' wrote John Havens , who was a founder of Old Lane with Pandit and is president of the alternative investment group . Head Entity: Old Lane. Head Type: ORGANIZATION. Tail Entity: John Havens. Tail Type: PERSON.\n\nGenerate more samples for the relation 'org:founded_by'.\n'''\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for DeepKE Project\nDESCRIPTION: This requirements file lists all the necessary Python packages and their specific versions required to run the DeepKE project. It includes machine learning frameworks like PyTorch, utilities like Hydra, visualization tools like TensorBoard and Matplotlib, and NLP-specific libraries like Transformers and Jieba.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PRGC/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch == 1.10\nhydra-core == 1.0.6\ntensorboard == 2.4.1\nmatplotlib == 3.4.1\nscikit-learn == 0.24.1\ntransformers == 3.4.0\njieba == 0.42.1\nwandb == 0.13.9\ndeepke\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for DeepKE Project\nDESCRIPTION: This snippet lists the required Python packages and their versions for the DeepKE project. It includes core machine learning libraries, NLP tools, and utility packages. The specific versions are crucial for ensuring compatibility and reproducibility of the project environment.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PURE/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch<1.13.0\ntorchvision<0.14.0\ntorchaudio<0.13.0\ntqdm==4.62.0\nallennlp==2.10.1\ntransformers==4.20.0\nwandb==0.12.7\nhydra-core==1.3.1\noverrides\nrequests\n```\n\n----------------------------------------\n\nTITLE: Example Data Structure for InstructIE Dataset in JSON\nDESCRIPTION: Shows the structure of a single data entry in the InstructIE dataset, including fields for id, text, and relation triples.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"841ef2af4cfe766dd9295fb7daf321c299df0fd0cef14820dfcb421161eed4a1\", \n  \"text\": \"NGC1313 is a galaxy in the constellation of Reticulum. It was discovered by the Australian astronomer James Dunlop on September 27, 1826. It has a prominent uneven shape, and its axis does not completely revolve around its center. Near NGC1313, there is another galaxy, NGC1309.\", \n  \"relation\": [\n    {\"head\": \"NGC1313\", \"head_type\": \"astronomical object type\", \"relation\": \"time of discovery\", \"tail\": \"September 27, 1826\", \"tail_type\": \"time\"}, \n    {\"head\": \"NGC1313\", \"head_type\": \"astronomical object type\", \"relation\": \"discoverer or inventor\", \"tail\": \"James Dunlop\", \"tail_type\": \"organization/human\"}, \n    {\"head\": \"NGC1313\", \"head_type\": \"astronomical object type\", \"relation\": \"of\", \"tail\": \"Reticulum\", \"tail_type\": \"astronomical object type\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies List for DeepKE\nDESCRIPTION: Specifies the exact versions of Python packages required to run the DeepKE project. Includes essential machine learning libraries like PyTorch and Transformers, along with utility packages for progress tracking, configuration management, and text processing.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntqdm==4.64.1\nnumpy==1.24.1\nscipy==1.10.1\ntorch==1.13.1\nhuggingface_hub==0.12.1\ntruecase==0.0.14\npyhocon==0.3.60\nsentencepiece==0.1.97\nwandb==0.13.9\nhydra-core==1.3.1\ntransformers==4.26.0\n```\n\n----------------------------------------\n\nTITLE: Citing DeepKE in BibTeX Format\nDESCRIPTION: BibTeX citation for the DeepKE paper published in EMNLP 2022. This citation should be used when referencing DeepKE in academic work.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{EMNLP2022_Demo_DeepKE,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population},\n  booktitle = {{EMNLP} (Demos)},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10}\n}\n```\n\n----------------------------------------\n\nTITLE: CPM-Bee Fine-tuning Script\nDESCRIPTION: Bash script for fine-tuning the CPM-Bee model with detailed configuration options including learning rate, batch size, and model paths.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n#! /bin/bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nGPUS_PER_NODE=4\n\nNNODES=1\nMASTER_ADDR=\"localhost\"\nMASTER_PORT=12345\n\nOPTS=\"\"\nOPTS+=\" --use-delta\"\nOPTS+=\" --model-config config/cpm-bee-10b.json\"\nOPTS+=\" --dataset path/to/dataset\"\nOPTS+=\" --eval_dataset path/to/eval/dataset\"\nOPTS+=\" --epoch 100\"\nOPTS+=\" --batch-size 5\"\nOPTS+=\" --train-iters 100\"\nOPTS+=\" --save-name cpm_bee_finetune\"\nOPTS+=\" --max-length 2048\"\nOPTS+=\" --save results/\"\nOPTS+=\" --lr 0.0001\"\nOPTS+=\" --inspect-iters 100\"\nOPTS+=\" --warmup-iters 1\"\nOPTS+=\" --eval-interval 1000\"\nOPTS+=\" --early-stop-patience 5\"\nOPTS+=\" --lr-decay-style noam\"\nOPTS+=\" --weight-decay 0.01\"\nOPTS+=\" --clip-grad 1.0\"\nOPTS+=\" --loss-scale 32768\"\nOPTS+=\" --start-step 0\"\nOPTS+=\" --load path/to/your/model.pt\"\n\nCMD=\"torchrun --nnodes=${NNODES} --nproc_per_node=${GPUS_PER_NODE} --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} finetune_cpm_bee.py ${OPTS}\"\n\necho ${CMD}\n$CMD\n```\n\n----------------------------------------\n\nTITLE: F1 Score Calculation for Information Extraction Tasks in Bash\nDESCRIPTION: This script demonstrates how to calculate F1 scores for various Information Extraction tasks using the provided evaluation script. It supports tasks like NER, RE, EE, EET, and EEA.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython ie2instruction/eval_func.py \\\n  --path1 results/llm_output.json \\\n  --task NER \n```\n\n----------------------------------------\n\nTITLE: Citing DeepKE in BibTeX Format\nDESCRIPTION: This BibTeX entry provides the citation information for the DeepKE project paper published in the Proceedings of EMNLP 2022. It includes details such as authors, title, publication venue, and URL.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md#2025-04-07_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{DBLP:conf/emnlp/ZhangXTYYQXCLL22,\n  author    = {Ningyu Zhang and\n               Xin Xu and\n               Liankuan Tao and\n               Haiyang Yu and\n               Hongbin Ye and\n               Shuofei Qiao and\n               Xin Xie and\n               Xiang Chen and\n               Zhoubo Li and\n               Lei Li},\n  editor    = {Wanxiang Che and\n               Ekaterina Shutova},\n  title     = {DeepKE: {A} Deep Learning Based Knowledge Extraction Toolkit for Knowledge\n               Base Population},\n  booktitle = {Proceedings of the The 2022 Conference on Empirical Methods in Natural\n               Language Processing, {EMNLP} 2022 - System Demonstrations, Abu Dhabi,\n               UAE, December 7-11, 2022},\n  pages     = {98--108},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.emnlp-demos.10},\n  timestamp = {Thu, 23 Mar 2023 16:56:00 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/ZhangXTYYQXCLL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n----------------------------------------\n\nTITLE: Source File Format Example in JSON\nDESCRIPTION: Demonstrates the required JSON structure for source files containing entity pairs. Each entry must include sentence, head entity, tail entity, and their offsets.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Example JSON Format for Relation Extraction Task\nDESCRIPTION: Example of a JSON-formatted data instance for Relation Extraction (RE) task from the NYT11 dataset. It demonstrates the format for extracting relationship triples between entities based on defined schema.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"task\": \"RE\", \n  \"source\": \"NYT11\", \n  \"instruction\": \"{\\\"instruction\\\": \\\"You are an expert in relationship extraction. Please extract relationship triples that match the schema definition from the input. Return an empty list for relationships that do not exist. Please respond in the format of a JSON string.\\\", \\\"schema\\\": [\\\"neighborhood of\\\", \\\"nationality\\\", \\\"children\\\", \\\"place of death\\\"], \\\"input\\\": \\\" In the way New Jersey students know that Thomas Edison 's laboratory is in West Orange , the people of Colma know that Wyatt Earp 's ashes are buried at Hills of Eternity , a Jewish cemetery he was n't ; his wife was , and that Joe DiMaggio is at Holy Cross Cemetery , where visitors often lean bats against his gravestone . \\\"}\", \n  \"output\": \"{\\\"neighborhood of\\\": [], \\\"nationality\\\": [], \\\"children\\\": [], \\\"place of death\\\": [{\\\"subject\\\": \\\"Thomas Edison\\\", \\\"object\\\": \\\"West Orange\\\"}]}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Input for Knowledge Graph Construction Task\nDESCRIPTION: An example of input data for a knowledge graph construction task, including the instruction, input text, and expected output format. This demonstrates how to structure inputs for entity and relation extraction tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/LLMICL/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninstruction=\"使用自然语言抽取三元组,已知下列句子,请从句子中抽取出可能的实体、关系,抽取实体类型为{'专业','时间','人类','组织','地理地区','事件'},关系类型为{'体育运动','包含行政领土','参加','国家','邦交国','夺得','举办地点','属于','获奖'},你可以先识别出实体再判断实体之间的关系,以(头实体,关系,尾实体)的形式回答\"\ninput=\"2006年，弗雷泽出战中国天津举行的女子水球世界杯，协助国家队夺得冠军。2008年，弗雷泽代表澳大利亚参加北京奥运会女子水球比赛，赢得铜牌。\"\noutput=\"(弗雷泽,获奖,铜牌)(女子水球世界杯,举办地点,天津)(弗雷泽,属于,国家队)(弗雷泽,国家,澳大利亚)(弗雷泽,参加,北京奥运会女子水球比赛)(中国,包含行政领土,天津)(中国,邦交国,澳大利亚)(北京奥运会女子水球比赛,举办地点,北京)(女子水球世界杯,体育运动,水球)(国家队,夺得,冠军)\"\n```\n\n----------------------------------------\n\nTITLE: Data Format Conversion Example for CPM-Bee\nDESCRIPTION: Shows the data format before and after conversion for CPM-Bee fine-tuning. The original knowledge graph data is transformed into a text generation format with input, prompt, and answer fields.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"文本生成\": {\"input\": \"今天天气很好，我和妈妈一起去公园，\", \"prompt\": \"往后写约100字\", \"<ans>\": \"\"}\n#数据转化前后\n初始数据：{\"id\": 10000, \"cate\": \"建筑\", \"instruction\": \"已知候选的关系列表：['事件', '位于', '名称由来']，请你根据关系列表，从以下输入中抽取出可能存在的头实体(Subject)与尾实体(Object)，并给出对应的关系三元组。请按照 (Subject,Relation,Object) 的格式回答。\", \"input\": \"浅草神社位于日本东京都台东区浅草的浅草寺本堂东侧，供奉的是土师真中知、桧田浜成、桧前武成，三位对于浅草寺创立有密切关联的人，每年5月17日都会举行三社祭。现在被指定为重要文化财产。\", \"output\": \"(浅草神社,事件,三社祭),(浅草神社,位于,浅草),(台东区,位于,东京都),(浅草寺,位于,浅草),(浅草寺,名称由来,浅草)\", \"kg\": [[\"浅草神社\", \"事件\", \"三社祭\"], [\"浅草神社\", \"位于\", \"浅草\"], [\"台东区\", \"位于\", \"东京都\"], [\"浅草寺\", \"位于\", \"浅草\"], [\"浅草寺\", \"名称由来\", \"浅草\"]]}\n转化后：{ \"input\": \"浅草神社位于日本东京都台东区浅草的浅草寺本堂东侧，供奉的是土师真中知、桧田浜成、桧前武成，三位对于浅草寺创立有密切关联的人，每年5月17日都会举行三社祭。现在被指定为重要文化财产。\", \"prompt\": \"已知候选的关系列表：['事件', '位于', '名称由来']，请你根据关系列表，从以下输入中抽取出可能存在的头实体(Subject)与尾实体(Object)，并给出对应的关系三元组。请按照 (Subject,Relation,Object) 的格式回答。\", \"<ans>\": \"(浅草神社,事件,三社祭),(浅草神社,位于,浅草),(台东区,位于,东京都),(浅草寺,位于,浅草),(浅草寺,名称由来,浅草)\"}\n```\n\n----------------------------------------\n\nTITLE: Running Model Finetuning with DeepSpeed\nDESCRIPTION: DeepSpeed command for finetuning the MT5 model with detailed training parameters including batch sizes, learning rate, and model configurations\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/mt5/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed  --include localhost:0,1 run_finetune.py \\\n    --do_train --do_eval --do_predict \\\n    --num_train_epochs 10 \\\n    --per_device_train_batch_size 16 \\\n    --per_device_eval_batch_size 48 \\\n    --gradient_accumulation_steps 2 \\\n    --predict_with_generate \\\n    --from_checkpoint=True \\\n    --overwrite_output_dir=False \\\n    --model_name_or_path google/mt5-base   \\\n    --output_dir output/ccks_mt5-base_f1_1e-4  \\\n    --logging_dir output/ccks_mt5-base_f1_1e-4_log \\\n    --train_file data/train.json \\\n    --test_file data/valid.json \\\n    --save_total_limit 1 \\\n    --load_best_model_at_end \\\n    --save_strategy \"epoch\" \\\n    --evaluation_strategy \"epoch\" \\\n    --metric_for_best_model \"overall-score\" \\\n    --learning_rate 1e-4 \\\n    --use_fast_tokenizer=True \\\n    --preprocessing_num_workers 4 \\\n    --generation_max_length 256 \\\n    --generation_num_beams 1 \\\n    --gradient_checkpointing=True \\\n    --deepspeed \"configs/ds_mt5_z3_config_bf16.json\" \\\n    --seed 42 \\\n    --bf16=True \\\n```\n\n----------------------------------------\n\nTITLE: Inferencing with LoRA-Tuned LLM for Information Extraction in Python\nDESCRIPTION: Command to run inference using a LoRA-tuned large language model for information extraction tasks. It specifies the model, LoRA weights, input/output files, and various inference parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python src/inference.py \\\n    --stage sft \\\n    --model_name_or_path 'models/llama2-13B-Chat' \\\n    --checkpoint_dir 'lora/llama2-13b-IEPile-lora' \\\n    --model_name 'llama' \\\n    --template 'llama2' \\\n    --do_predict \\\n    --input_file 'data/input.json' \\\n    --output_file 'results/llama2-13b-IEPile-lora_output.json' \\\n    --finetuning_type lora \\\n    --output_dir 'lora/test' \\\n    --predict_with_generate \\\n    --cutoff_len 512 \\\n    --bf16 \\\n    --max_new_tokens 300 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Example JSON Format for Event Extraction Task\nDESCRIPTION: Example of a JSON-formatted data instance for Event Extraction (EE) task from the PHEE dataset. It includes complex schema definitions with event types, triggers, and multiple argument types.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"task\": \"EE\", \n  \"source\": \"PHEE\", \n  \"instruction\": \"{\\\"instruction\\\": \\\"You are an expert in event extraction. Please extract events from the input that conform to the schema definition. Return an empty list for events that do not exist, and return NAN for arguments that do not exist. If an argument has multiple values, please return a list. Respond in the format of a JSON string.\\\", \\\"schema\\\": [{\\\"event_type\\\": \\\"potential therapeutic event\\\", \\\"trigger\\\": true, \\\"arguments\\\": [\\\"Treatment.Time_elapsed\\\", \\\"Treatment.Route\\\", \\\"Treatment.Freq\\\", \\\"Treatment\\\", \\\"Subject.Race\\\", \\\"Treatment.Disorder\\\", \\\"Effect\\\", \\\"Subject.Age\\\", \\\"Combination.Drug\\\", \\\"Treatment.Duration\\\", \\\"Subject.Population\\\", \\\"Subject.Disorder\\\", \\\"Treatment.Dosage\\\", \\\"Treatment.Drug\\\"]}, {\\\"event_type\\\": \\\"adverse event\\\", \\\"trigger\\\": true, \\\"arguments\\\": [\\\"Subject.Population\\\", \\\"Subject.Age\\\", \\\"Effect\\\", \\\"Treatment.Drug\\\", \\\"Treatment.Dosage\\\", \\\"Treatment.Freq\\\", \\\"Subject.Gender\\\", \\\"Treatment.Disorder\\\", \\\"Subject\\\", \\\"Treatment\\\", \\\"Treatment.Time_elapsed\\\", \\\"Treatment.Duration\\\", \\\"Subject.Disorder\\\", \\\"Subject.Race\\\", \\\"Combination.Drug\\\"]}], \\\"input\\\": \\\"Our findings reveal that even in patients without a history of seizures, pregabalin can cause a cortical negative myoclonus.\\\"}\", \n  \"output\": \"{\\\"potential therapeutic event\\\": [], \\\"adverse event\\\": [{\\\"trigger\\\": \\\"cause \\\", \\\"arguments\\\": {\\\"Subject.Population\\\": \\\"NAN\\\", \\\"Subject.Age\\\": \\\"NAN\\\", \\\"Effect\\\": \\\"cortical negative myoclonus\\\", \\\"Treatment.Drug\\\": \\\"pregabalin\\\", \\\"Treatment.Dosage\\\": \\\"NAN\\\", \\\"Treatment.Freq\\\": \\\"NAN\\\", \\\"Subject.Gender\\\": \\\"NAN\\\", \\\"Treatment.Disorder\\\": \\\"NAN\\\", \\\"Subject\\\": \\\"patients without a history of seizures\\\", \\\"Treatment\\\": \\\"pregabalin\\\", \\\"Treatment.Time_elapsed\\\": \\\"NAN\\\", \\\"Treatment.Duration\\\": \\\"NAN\\\", \\\"Subject.Disorder\\\": \\\"NAN\\\", \\\"Subject.Race\\\": \\\"NAN\\\", \\\"Combination.Drug\\\": \\\"NAN\\\"}}]}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Output File Format Example in JSON\nDESCRIPTION: Shows the structure of labeled output files including the automatically assigned relation type. The data is split into train, dev, and test sets.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n\t{\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\",\n    \"relation\": \"/location/location/contains\"\n\t}\n]\n```\n\n----------------------------------------\n\nTITLE: Example JSON Format for Named Entity Recognition Task\nDESCRIPTION: Example of a JSON-formatted data instance for the Named Entity Recognition (NER) task. The instance includes task type, source dataset, instruction in JSON format, and expected output in JSON format.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"task\": \"NER\", \n    \"source\": \"CoNLL2003\", \n    \"instruction\": \"{\\\"instruction\\\": \\\"You are an expert in named entity recognition. Please extract entities that match the schema definition from the input. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.\\\", \\\"schema\\\": [\\\"person\\\", \\\"organization\\\", \\\"else\\\", \\\"location\\\"], \\\"input\\\": \\\"284 Robert Allenby ( Australia ) 69 71 71 73 , Miguel Angel Martin ( Spain ) 75 70 71 68 ( Allenby won at first play-off hole )\\\"}\", \n    \"output\": \"{\\\"person\\\": [\\\"Robert Allenby\\\", \\\"Allenby\\\", \\\"Miguel Angel Martin\\\"], \\\"organization\\\": [], \\\"else\\\": [], \\\"location\\\": [\\\"Australia\\\", \\\"Spain\\\"]}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Prediction with IE-Specific Model in Bash\nDESCRIPTION: This script demonstrates how to use a trained Information Extraction (IE) specific model for prediction. It includes parameters for specifying the model path and prediction settings.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python src/inference.py \\\n    --stage sft \\\n    --model_name_or_path 'models/OneKE' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --do_predict \\\n    --input_file 'data/input.json' \\\n    --output_file 'results/OneKE_output.json' \\\n    --output_dir 'lora/test' \\\n    --predict_with_generate \\\n    --cutoff_len 512 \\\n    --bf16 \\\n    --max_new_tokens 300 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Converting Training Data for IE Tasks using Bash Script\nDESCRIPTION: Bash command to convert training data for IE tasks using the provided convert_func.py script. It specifies input and output paths, schema, language, task type, and other parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ie2instruction/convert_func.py \\\n    --src_path data/NER/sample.json \\\n    --tgt_path data/NER/train.json \\\n    --schema_path data/NER/schema.json \\\n    --language zh \\\n    --task NER \\\n    --split_num 6 \\\n    --random_sort \\\n    --split train\n```\n\n----------------------------------------\n\nTITLE: Example JSON Format for Named Entity Recognition Task\nDESCRIPTION: Example of a JSON-formatted data instance for the Named Entity Recognition (NER) task. The instance includes task type, source dataset, instruction in JSON format, and expected output in JSON format.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"task\": \"NER\", \n    \"source\": \"CoNLL2003\", \n    \"instruction\": \"{\\\"instruction\\\": \\\"You are an expert in named entity recognition. Please extract entities that match the schema definition from the input. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.\\\", \\\"schema\\\": [\\\"person\\\", \\\"organization\\\", \\\"else\\\", \\\"location\\\"], \\\"input\\\": \\\"284 Robert Allenby ( Australia ) 69 71 71 73 , Miguel Angel Martin ( Spain ) 75 70 71 68 ( Allenby won at first play-off hole )\\\"}\", \n    \"output\": \"{\\\"person\\\": [\\\"Robert Allenby\\\", \\\"Allenby\\\", \\\"Miguel Angel Martin\\\"], \\\"organization\\\": [], \\\"else\\\": [], \\\"location\\\": [\\\"Australia\\\", \\\"Spain\\\"]}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Listing English Relation Types in Python\nDESCRIPTION: A list of relation types used in the English triple file, derived from the NYT dataset.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"/business/company/place_founded\",\n\"/people/person/place_lived\",\n\"/location/country/administrative_divisions\",\n\"/business/company/major_shareholders\",\n\"/sports/sports_team_location/teams\",\n\"/people/person/religion\",\n\"/people/person/place_of_birth\",\n\"/people/person/nationality\",\n\"/location/country/capital\",\n\"/business/company/advisors\",\n\"/people/deceased_person/place_of_death\",\n\"/business/company/founders\",\n\"/location/location/contains\",\n\"/people/person/ethnicity\",\n\"/business/company_shareholder/major_shareholder_of\",\n\"/people/ethnicity/geographic_distribution\",\n\"/people/person/profession\",\n\"/business/person/company\",\n\"/people/person/children\",\n\"/location/administrative_division/country\",\n\"/people/ethnicity/people\",\n\"/sports/sports_team/location\",\n\"/location/neighborhood/neighborhood_of\",\n\"/business/company/industry\"\n```\n\n----------------------------------------\n\nTITLE: Training W2NER Model for NER\nDESCRIPTION: Commands to navigate to the W2NER directory and train the W2NER model, which shows the highest F1 score among the provided models. Configuration parameters are specified in model.yaml.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd w2ner\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Generating Cyclic Instructions for OneKE Tasks in Python\nDESCRIPTION: This function generates cyclic instructions for OneKE tasks by splitting the schema into smaller chunks and creating JSON-formatted instructions for each chunk. It handles different schema types and uses the instruction mapper to get the appropriate instruction template.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE_old.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_instruction(language, task, schema, input):\n    sintructs = []\n    split_num = split_num_mapper[task]\n    if type(schema) == dict:\n        sintruct = json.dumps({'instruction':instruction_mapper[task+language], 'schema':schema, 'input':input}, ensure_ascii=False)\n        sintructs.append(sintruct)\n    else:\n        split_schemas = [schema[i:i+split_num] for i in range(0, len(schema), split_num)]\n        for split_schema in split_schemas:\n            sintruct = json.dumps({'instruction':instruction_mapper[task+language], 'schema':split_schema, 'input':input}, ensure_ascii=False)\n            sintructs.append(sintruct)\n    return sintructs\n```\n\n----------------------------------------\n\nTITLE: Defining Chinese Relation Types in JSON\nDESCRIPTION: A list of relation types used in the Chinese triple file, including subject type, predicate, and object type.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"object_type\": \"地点\", \"predicate\": \"祖籍\", \"subject_type\": \"人物\"}\n{\"object_type\": \"人物\", \"predicate\": \"父亲\", \"subject_type\": \"人物\"}\n{\"object_type\": \"地点\", \"predicate\": \"总部地点\", \"subject_type\": \"企业\"}\n{\"object_type\": \"地点\", \"predicate\": \"出生地\", \"subject_type\": \"人物\"}\n{\"object_type\": \"目\", \"predicate\": \"目\", \"subject_type\": \"生物\"}\n{\"object_type\": \"Number\", \"predicate\": \"面积\", \"subject_type\": \"行政区\"}\n{\"object_type\": \"Text\", \"predicate\": \"简称\", \"subject_type\": \"机构\"}\n{\"object_type\": \"Date\", \"predicate\": \"上映时间\", \"subject_type\": \"影视作品\"}\n{\"object_type\": \"人物\", \"predicate\": \"妻子\", \"subject_type\": \"人物\"}\n{\"object_type\": \"音乐专辑\", \"predicate\": \"所属专辑\", \"subject_type\": \"歌曲\"}\n{\"object_type\": \"Number\", \"predicate\": \"注册资本\", \"subject_type\": \"企业\"}\n{\"object_type\": \"城市\", \"predicate\": \"首都\", \"subject_type\": \"国家\"}\n{\"object_type\": \"人物\", \"predicate\": \"导演\", \"subject_type\": \"影视作品\"}\n{\"object_type\": \"Text\", \"predicate\": \"字\", \"subject_type\": \"历史人物\"}\n{\"object_type\": \"Number\", \"predicate\": \"身高\", \"subject_type\": \"人物\"}\n{\"object_type\": \"企业\", \"predicate\": \"出品公司\", \"subject_type\": \"影视作品\"}\n{\"object_type\": \"Number\", \"predicate\": \"修业年限\", \"subject_type\": \"学科专业\"}\n{\"object_type\": \"Date\", \"predicate\": \"出生日期\", \"subject_type\": \"人物\"}\n{\"object_type\": \"人物\", \"predicate\": \"制片人\", \"subject_type\": \"影视作品\"}\n{\"object_type\": \"人物\", \"predicate\": \"母亲\", \"subject_type\": \"人物\"}\n{\"object_type\": \"人物\", \"predicate\": \"编剧\", \"subject_type\": \"影视作品\"}\n{\"object_type\": \"国家\", \"predicate\": \"国籍\", \"subject_type\": \"人物\"}\n{\"object_type\": \"Number\", \"predicate\": \"海拔\", \"subject_type\": \"地点\"}\n{\"object_type\": \"网站\", \"predicate\": \"连载网站\", \"subject_type\": \"网络小说\"}\n{\"object_type\": \"人物\", \"predicate\": \"丈夫\", \"subject_type\": \"人物\"}\n{\"object_type\": \"Text\", \"predicate\": \"朝代\", \"subject_type\": \"历史人物\"}\n{\"object_type\": \"Text\", \"predicate\": \"民族\", \"subject_type\": \"人物\"}\n{\"object_type\": \"Text\", \"predicate\": \"号\", \"subject_type\": \"历史人物\"}\n{\"object_type\": \"出版社\", \"predicate\": \"出版社\", \"subject_type\": \"书籍\"}\n{\"object_type\": \"人物\", \"predicate\": \"主持人\", \"subject_type\": \"电视综艺\"}\n{\"object_type\": \"Text\", \"predicate\": \"专业代码\", \"subject_type\": \"学科专业\"}\n{\"object_type\": \"人物\", \"predicate\": \"歌手\", \"subject_type\": \"歌曲\"}\n{\"object_type\": \"人物\", \"predicate\": \"作词\", \"subject_type\": \"歌曲\"}\n{\"object_type\": \"人物\", \"predicate\": \"主角\", \"subject_type\": \"网络小说\"}\n{\"object_type\": \"人物\", \"predicate\": \"董事长\", \"subject_type\": \"企业\"}\n{\"object_type\": \"Date\", \"predicate\": \"成立日期\", \"subject_type\": \"机构\"}\n{\"object_type\": \"学校\", \"predicate\": \"毕业院校\", \"subject_type\": \"人物\"}\n{\"object_type\": \"Number\", \"predicate\": \"占地面积\", \"subject_type\": \"机构\"}\n{\"object_type\": \"语言\", \"predicate\": \"官方语言\", \"subject_type\": \"国家\"}\n{\"object_type\": \"Text\", \"predicate\": \"邮政编码\", \"subject_type\": \"行政区\"}\n{\"object_type\": \"Number\", \"predicate\": \"人口数量\", \"subject_type\": \"行政区\"}\n{\"object_type\": \"城市\", \"predicate\": \"所在城市\", \"subject_type\": \"景点\"}\n{\"object_type\": \"人物\", \"predicate\": \"作者\", \"subject_type\": \"图书作品\"}\n{\"object_type\": \"Date\", \"predicate\": \"成立日期\", \"subject_type\": \"企业\"}\n{\"object_type\": \"人物\", \"predicate\": \"作曲\", \"subject_type\": \"歌曲\"}\n{\"object_type\": \"气候\", \"predicate\": \"气候\", \"subject_type\": \"行政区\"}\n{\"object_type\": \"人物\", \"predicate\": \"嘉宾\", \"subject_type\": \"电视综艺\"}\n{\"object_type\": \"人物\", \"predicate\": \"主演\", \"subject_type\": \"影视作品\"}\n{\"object_type\": \"作品\", \"predicate\": \"改编自\", \"subject_type\": \"影视作品\"}\n{\"object_type\": \"人物\", \"predicate\": \"创始人\", \"subject_type\": \"企业\"}\n```\n\n----------------------------------------\n\nTITLE: Output File Format - JSON\nDESCRIPTION: Example of the labeled output file structure showing sentence, entity pairs, offsets and relation information\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n[\n\t{\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\",\n    \"relation\": \"/location/location/contains\"\n\t}\n]\n```\n\n----------------------------------------\n\nTITLE: Complete CodeKGC Input with Schema and Examples\nDESCRIPTION: The full input for CodeKGC, including the schema definitions, in-context examples, and the test case. This format helps code language models understand how to extract structured knowledge from text.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CodeKGC/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nclass Rel:\n...(schema prompt)\n\n\"\"\" In 1856 , the 28th President...\"\"\"\nextract = Extract([Triple(person('Thomas Woodrow Wilson'), Rel('Live in'), location('Staunton , Va')),])\n...(in-context examples)\n\n\"\"\" Boston University 's Michael D. Papagiannis said he believes the crater was created 100 million years ago when a 50-mile-wide meteorite slammed into the Earth . \"\"\"\n```\n\n----------------------------------------\n\nTITLE: P-Tuning Configuration for ChatGLM\nDESCRIPTION: Configuration for P-Tuning fine-tuning method specifically for ChatGLM model using DeepSpeed optimization.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --include localhost:0 src/finetuning_pt.py \\\n  --train_path data/train.json \\\n  --model_dir /model \\\n  --num_train_epochs 20 \\\n  --train_batch_size 2 \\\n  --gradient_accumulation_steps 1 \\\n  --output_dir output_dir_pt \\\n  --log_steps 10 \\\n  --max_len 768 \\\n  --max_src_len 450 \\\n  --pre_seq_len 16 \\\n  --prefix_projection true\n```\n\n----------------------------------------\n\nTITLE: In-Context Learning Example for CodeKGC\nDESCRIPTION: An example of an in-context learning prompt that shows how to extract a triple from natural language text. This demonstrates creating entities and relations from text about Thomas Woodrow Wilson.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CodeKGC/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\" In 1856 , the 28th President of the United States , Thomas Woodrow Wilson , was born in Staunton , Va . \"\"\"\nextract = Extract([Triple(person('Thomas Woodrow Wilson'), Rel('Live in'), location('Staunton , Va')),])\n```\n\n----------------------------------------\n\nTITLE: Converting Data Formats for DeepKE NER Module in Python\nDESCRIPTION: The code references two transformation functions (json2txt and doc2txt) available in the transform_data.py file of the DeepKE project. These functions help convert JSON and DOCX files to TXT format for use with the NER module.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/data/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njson2txt\n```\n\nLANGUAGE: python\nCODE:\n```\ndoc2txt\n```\n\n----------------------------------------\n\nTITLE: Example Data Structure for IEPile Dataset in JSON\nDESCRIPTION: Demonstrates the structure of a single data entry in the IEPile dataset, including fields for task, source, instruction, and output.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"task\": \"NER\", \n    \"source\": \"CoNLL2003\", \n    \"instruction\": \"{\\\"instruction\\\": \\\"You are an expert in named entity recognition. Please extract entities that match the schema definition from the input. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.\\\", \\\"schema\\\": [\\\"person\\\", \\\"organization\\\", \\\"else\\\", \\\"location\\\"], \\\"input\\\": \\\"284 Robert Allenby ( Australia ) 69 71 71 73 , Miguel Angel Martin ( Spain ) 75 70 71 68 ( Allenby won at first play-off hole )\\\"}\", \n    \"output\": \"{\\\"person\\\": [\\\"Robert Allenby\\\", \\\"Allenby\\\", \\\"Miguel Angel Martin\\\"], \\\"organization\\\": [], \\\"else\\\": [], \\\"location\\\": [\\\"Australia\\\", \\\"Spain\\\"]}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Information Extraction Model Output in Python\nDESCRIPTION: Command to evaluate the F1 score of model outputs for various information extraction tasks. It specifies the output file path and the task type.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython ie2instruction/eval_func.py \\\n  --path1 results/llm_output.json \\\n  --task NER\n```\n\n----------------------------------------\n\nTITLE: Referencing Transform Functions in DeepKE\nDESCRIPTION: References to utility functions that can convert JSON and XLSX files to CSV format in the DeepKE transform_data.py module. These functions (json2csv and xlsx2csv) can be used when input data is not already in CSV format.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/data/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nIf it is not csv file,you can use function `json2csv` and function `xlsx2csv` to transfer in [transform_data.py](https://github.com/zjunlp/DeepKE/blob/main/src/deepke/transform_data.py).\n```\n\n----------------------------------------\n\nTITLE: CPM-Bee Fine-tuning Configuration Script\nDESCRIPTION: A bash script for fine-tuning CPM-Bee with various hyperparameters. The script configures GPU devices, distributed training settings, model parameters, dataset paths, and training hyperparameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README_CN.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n#! /bin/bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nGPUS_PER_NODE=4\n\nNNODES=1\nMASTER_ADDR=\"localhost\"\nMASTER_PORT=12345\n\nOPTS=\"\"\nOPTS+=\" --use-delta\"\nOPTS+=\" --model-config config/cpm-bee-10b.json\"\nOPTS+=\" --dataset path/to/dataset\"\nOPTS+=\" --eval_dataset path/to/eval/dataset\"\nOPTS+=\" --epoch 100\"\nOPTS+=\" --batch-size 5\"\nOPTS+=\" --train-iters 100\"\nOPTS+=\" --save-name cpm_bee_finetune\"\nOPTS+=\" --max-length 2048\"\nOPTS+=\" --save results/\"\nOPTS+=\" --lr 0.0001\"\nOPTS+=\" --inspect-iters 100\"\nOPTS+=\" --warmup-iters 1\"\nOPTS+=\" --eval-interval 1000\"\nOPTS+=\" --early-stop-patience 5\"\nOPTS+=\" --lr-decay-style noam\"\nOPTS+=\" --weight-decay 0.01\"\nOPTS+=\" --clip-grad 1.0\"\nOPTS+=\" --loss-scale 32768\"\nOPTS+=\" --start-step 0\"\nOPTS+=\" --load path/to/your/model.pt\"\n\nCMD=\"torchrun --nnodes=${NNODES} --nproc_per_node=${GPUS_PER_NODE} --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} finetune_cpm_bee.py ${OPTS}\"\n\necho ${CMD}\n```\n\n----------------------------------------\n\nTITLE: Few-shot NER Module Setup and Execution\nDESCRIPTION: Commands for implementing few-shot NER in low-resource scenarios. Includes data preparation, model training with CoNLL-2003 dataset, and prediction steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd example/ner/few-shot\nwget 120.27.214.45/Data/ner/few_shot/data.tar.gz\ntar -xzvf data.tar.gz\npython run.py\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output for Information Extraction Evaluation\nDESCRIPTION: Example of a JSON object representing a single data sample in the model output file used for evaluation. It includes the input instruction, true label, and model prediction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"e88d2b42f8ca14af1b77474fcb18671ed3cacc0c75cf91f63375e966574bd187\", \n  \"instruction\": \"{\\\"instruction\\\": \\\"你是专门进行实体抽取的专家。请从input中抽取出符合schema定义的实体，不存在的实体类型返回空列表。请按照JSON字符串的格式回答。\\\", \\\"schema\\\": [\\\"组织机构\\\", \\\"地理位置\\\", \\\"人物\\\"], \\\"input\\\": \\\"相比之下，青岛海牛队和广州松日队的雨中之战虽然也是0∶0，但乏善可陈。\\\"}\", \n  \"label\": \"[{\\\"entity\\\": \\\"广州松日队\\\", \\\"entity_type\\\": \\\"组织机构\\\"}, {\\\"entity\\\": \\\"青岛海牛队\\\", \\\"entity_type\\\": \\\"组织机构\\\"}]\",\n  \"output\": \"{\\\"组织机构\\\": [\\\"广州松日队\\\", \\\"青岛海牛队\\\"], \\\"人物\\\": [], \\\"地理位置\\\": []}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Single GPU LoRA Fine-tuning for LLaMA2\nDESCRIPTION: Script for fine-tuning a LLaMA2-13B-chat model with LoRA on a single GPU. The script includes various parameters for controlling the training process, data preprocessing, model configuration, and output settings. It uses 4-bit quantization for memory efficiency.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/llama2-13b-chat-v1'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" python3 src/test_finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/llama2-13b-chat' \\\n    --stage 'sft' \\\n    --model_name 'llama' \\\n    --template 'llama2' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_r 16 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --bf16 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: P-Tuning Fine-tuning with ChatGLM in Bash\nDESCRIPTION: This script demonstrates how to perform P-Tuning fine-tuning using ChatGLM. It includes parameters specific to P-Tuning, such as pre_seq_len and prefix_projection.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndeepspeed --include localhost:0 src/finetuning_pt.py \\\n  --train_path data/train.json \\\n  --model_dir /model \\\n  --num_train_epochs 20 \\\n  --train_batch_size 2 \\\n  --gradient_accumulation_steps 1 \\\n  --output_dir output_dir_pt \\\n  --log_steps 10 \\\n  --max_len 768 \\\n  --max_src_len 450 \\\n  --pre_seq_len 16 \\\n  --prefix_projection true\n```\n\n----------------------------------------\n\nTITLE: Downloading NER Dataset\nDESCRIPTION: Commands to download and extract the default NER dataset (People's Daily Chinese NER corpus). This dataset contains training, validation, and test files in the required format.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ner/standard/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Full Fine-tuning for LLaMA/Alpaca Models in Bash\nDESCRIPTION: This script shows how to perform full fine-tuning on LLaMA or Alpaca models. It includes similar parameters to the LoRA fine-tuning script but with the finetuning_type set to 'full'.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/oneke-continue'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/OneKE' \\\n    --stage 'sft' \\\n    --finetuning_type 'full' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_dropout 0.05 \\\n    --bf16 \n```\n\n----------------------------------------\n\nTITLE: Model Parallel Training with LoRA for LLaMA2\nDESCRIPTION: Script for distributed training of a LLaMA2 model using torchrun with 4 GPUs. This enables model parallelism to handle larger models that don't fit on a single GPU, while maintaining the same training parameters as the single-GPU variant.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/llama2-13b-chat-v1'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/test_finetune.py \\\n    ...Other as above\n```\n\n----------------------------------------\n\nTITLE: Converting Generated Results to CCKS2023 Format\nDESCRIPTION: Converts the prediction results to the format required for CCKS2023 submission by extracting knowledge graph triples from the output field and formatting them properly.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README_CN.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ../InstructKGC/utils/convert.py\n-pred_path \"cpm_bee_TG.json\"\n-tgt_path \"cpm_bee_TG_kg.json'\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting RE Dataset\nDESCRIPTION: Commands to download the relation extraction dataset and extract it in the current directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/standard/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Running Trigger Classification Training\nDESCRIPTION: Command to train the trigger classification model, which predicts the trigger of each instance. The task_name should be set to 'trigger' in the configuration file before running this command.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Single GPU LoRA Training Configuration\nDESCRIPTION: Basic LoRA fine-tuning configuration for single GPU training setup with key hyperparameters for model adaptation.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/llama2-13b-chat-v1'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0\" python3 src/finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/llama2-13b-chat' \\\n    --stage 'sft' \\\n    --model_name 'llama' \\\n    --template 'llama2' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_r 16 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --bf16 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Citing CodeKGC and KnowLM in Research Papers\nDESCRIPTION: BibTeX citation format for referencing the KnowLM project in academic papers. This citation should be used when implementing or building upon the CodeKGC methodology for Knowledge Graph Construction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{knowlm,\n  author = {Ningyu Zhang and Jintian Zhang and Xiaohan Wang and Honghao Gui and Kangwei Liu and Yinuo Jiang and Xiang Chen and Shengyu Mao and Shuofei Qiao and Yuqi Zhu and Zhen Bi and Jing Chen and Xiaozhuan Liang and Yixin Ou and Runnan Fang and Zekun Xi and Xin Xu and Lei Li and Peng Wang and Mengru Wang and Yunzhi Yao and Bozhong Tian and Yin Fang and Guozhou Zheng and Huajun Chen},\n  title = {KnowLM Technical Report},\n  year = {2023},\n url = {http://knowlm.zjukg.cn/},\n}\n```\n\n----------------------------------------\n\nTITLE: Training Relation Extraction Model in Python\nDESCRIPTION: Python command to run the training script for relation extraction. Dataset and parameters can be customized in the data and conf folders.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: LoRA Prediction with LLaMA2 in Bash\nDESCRIPTION: This script shows how to use a LoRA-fine-tuned LLaMA2 model for prediction. It includes parameters for specifying the base model, LoRA weights, and prediction settings.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python src/inference.py \\\n    --stage sft \\\n    --model_name_or_path 'models/llama2-13B-Chat' \\\n    --checkpoint_dir 'lora/llama2-13b-IEPile-lora' \\\n    --model_name 'llama' \\\n    --template 'llama2' \\\n    --do_predict \\\n    --input_file 'data/input.json' \\\n    --output_file 'results/llama2-13b-IEPile-lora_output.json' \\\n    --finetuning_type lora \\\n    --output_dir 'lora/test' \\\n    --predict_with_generate \\\n    --cutoff_len 512 \\\n    --bf16 \\\n    --max_new_tokens 300 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Downloading Few-Shot Relation Extraction Dataset in Bash\nDESCRIPTION: Commands to download and extract the relation extraction dataset for the few-shot learning approach.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/few_shot/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Downloading Multimodal Relation Extraction Dataset in Bash\nDESCRIPTION: Commands to download and extract the relation extraction dataset for the multimodal approach.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/multimodal/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Training DeepKE Event Extraction Model\nDESCRIPTION: This command starts the training process for the event extraction model using the parameters specified in the config.yaml file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE-LLM Dependencies\nDESCRIPTION: Commands for creating a Conda environment and installing the required dependencies for DeepKE-LLM. It sets up a Python 3.9 environment and installs packages from a requirements.txt file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Downloading Twitter Dataset with Visual Objects\nDESCRIPTION: Commands to download and extract the Twitter2015 and Twitter2017 datasets with detected visual objects for multimodal NER training and evaluation.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/multimodal/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ner/multimodal/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository\nDESCRIPTION: Commands to clone the DeepKE repository and navigate to the relation extraction example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/re/standard\n```\n\n----------------------------------------\n\nTITLE: Downloading Conll Dataset - Shell Command\nDESCRIPTION: Command to download the Conll dataset for English NER that includes entity types for persons (PER), locations (LOC), organizations (ORG) and miscellaneous (MISC).\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Full Parameter Fine-tuning Configuration\nDESCRIPTION: Configuration for full parameter fine-tuning without LoRA adaptation for domain-specific training.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/oneke-continue'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/OneKE' \\\n    --stage 'sft' \\\n    --finetuning_type 'full' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_dropout 0.05 \\\n    --bf16\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install the required dependencies using pip and the requirements.txt file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running RE Model Prediction\nDESCRIPTION: Command to start the prediction process using a trained relation extraction model with the predict.py script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Training and Prediction Commands\nDESCRIPTION: Commands for training the model and running predictions using the trained model. Parameters can be configured in the conf folder.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Installing PRGC Repository\nDESCRIPTION: Commands to clone the DeepKE repository and navigate to the PRGC example directory\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PRGC/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/triple/PRGC\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DeepKE PURE Model\nDESCRIPTION: Command to install the required Python packages for the PURE model. It uses pip to install dependencies listed in the requirements.txt file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PURE/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE via PyPI\nDESCRIPTION: Command to install DeepKE directly using pip from the Python Package Index (PyPI).\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/install.rst#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install deepke\n```\n\n----------------------------------------\n\nTITLE: LoRA Fine-tuning Script\nDESCRIPTION: Bash script for fine-tuning the model using LoRA approach with detailed training parameters\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/oneke-continue'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/OneKE' \\\n    --stage 'sft' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_r 64 \\\n    --lora_alpha 64 \\\n    --lora_dropout 0.05 \\\n    --bf16 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for DeepKE\nDESCRIPTION: Commands to install the necessary Python packages for running DeepKE. Requires Python 3.8 and includes a note about handling conflicts with the hydra-core package.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython==3.8\npip install -r requirements.txt\npip install hydra-core==1.3.1 # ignore the conlict with deepke\n```\n\n----------------------------------------\n\nTITLE: Cloning and Navigating DeepKE Repository\nDESCRIPTION: Commands to clone the DeepKE repository from GitHub and navigate to the event extraction example directory\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ee/standard/degree\n```\n\n----------------------------------------\n\nTITLE: Setting Up Task Environment\nDESCRIPTION: Navigate to task directory and download required dataset files for training and evaluation.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd DeepKE/example/re/standard\n```\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Domain-Specific LoRA Training Configuration\nDESCRIPTION: Extended LoRA fine-tuning configuration for domain-specific training with increased LoRA rank parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/oneke-continue'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/OneKE' \\\n    --stage 'sft' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_r 64 \\\n    --lora_alpha 64 \\\n    --lora_dropout 0.05 \\\n    --bf16 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Cloning the DeepKE Repository for NER\nDESCRIPTION: Commands to clone the DeepKE repository and navigate to the standard NER example directory. This is the initial step to access the NER models and example code.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ner/standard\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository and Navigating to Cross-Domain NER Example\nDESCRIPTION: Commands to clone the DeepKE repository from GitHub and navigate to the cross-domain NER example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ner/cross\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository and Navigating to Cross-Domain NER Example\nDESCRIPTION: Commands to clone the DeepKE repository from GitHub and navigate to the cross-domain NER example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ner/cross\n```\n\n----------------------------------------\n\nTITLE: Training LightNER Model on CoNLL-2003 Dataset\nDESCRIPTION: Python command to run the training script for the LightNER model using the CoNLL-2003 dataset.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/few-shot/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Full Parameter Fine-tuning Script\nDESCRIPTION: Bash script for full parameter fine-tuning with training configuration\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/oneke-continue'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/finetune.py \\\n    --do_train --do_eval \\\n    --overwrite_output_dir \\\n    --model_name_or_path 'models/OneKE' \\\n    --stage 'sft' \\\n    --finetuning_type 'full' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --train_file 'data/train.json' \\\n    --valid_file 'data/dev.json' \\\n    --output_dir=${output_dir} \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 4 \\\n    --preprocessing_num_workers 16 \\\n    --num_train_epochs 10 \\\n    --learning_rate 5e-5 \\\n    --max_grad_norm 0.5 \\\n    --optim \"adamw_torch\" \\\n    --max_source_length 400 \\\n    --cutoff_len 700 \\\n    --max_target_length 300 \\\n    --evaluation_strategy \"epoch\" \\\n    --save_strategy \"epoch\" \\\n    --save_total_limit 10 \\\n    --lora_dropout 0.05 \\\n    --bf16\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository and Navigating to Few-Shot RE Directory\nDESCRIPTION: Commands to clone the DeepKE GitHub repository and navigate to the few-shot relation extraction example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/few-shot/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/re/few-shot\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI API for Python\nDESCRIPTION: Command to install the OpenAI Python library, which is required to use OpenAI's language models like GPT-3 and GPT-3.5 for relation extraction tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/UnleashLLMRE/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n>> pip install openai\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset for DeepKE PURE Model\nDESCRIPTION: Bash commands to download and extract the CMeIE dataset for training and testing the PURE model. It uses wget to download the zip file and unzip to extract it.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PURE/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd data/\nwget 120.27.214.45/Data/triple/PURE/CMeIE.zip\nunzip CMeIE.zip\n```\n\n----------------------------------------\n\nTITLE: Raw NER Data Format Example\nDESCRIPTION: Initial data format required before conversion to training format\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"text\": \"相比之下，青岛海牛队和广州松日队的雨中之战虽然也是0∶0，但乏善可陈。\", \"entity\": [{\"entity\": \"广州松日队\", \"entity_type\": \"组织机构\"}, {\"entity\": \"青岛海牛队\", \"entity_type\": \"组织机构\"}]}\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Package for LLM-based Relation Extraction\nDESCRIPTION: Command to install the OpenAI Python package, which is required to use the OpenAI API for relation extraction tasks with large language models.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/UnleashLLMRE/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n>> pip install openai\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository\nDESCRIPTION: Commands to clone the DeepKE repository and navigate to the document relation extraction example directory\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/document/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/re/document\n```\n\n----------------------------------------\n\nTITLE: Pip Virtual Environment Setup\nDESCRIPTION: Commands to create and activate a Python virtual environment using pip and install requirements\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv <path_to_venv>/asp  \nsource <path_to_venv>/asp/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Triple Extraction Configuration Example\nDESCRIPTION: Example configuration settings for running triple extraction task with ChatGPT, showing input parameters including task type, language, engine, and text input with corresponding instruction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/LLMICL/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntask=\"rte\"\nlanguage=\"ch\"\nengine=\"gpt-3.5-turbo\"\ntext_input=\"2006年，弗雷泽出战中国天津举行的女子水球世界杯，协助国家队夺得冠军。2008年，弗雷泽代表澳大利亚参加北京奥运会女子水球比赛，赢得铜牌。\"\ninstruction=\"使用自然语言抽取三元组,已知下列句子,请从句子中抽取出可能的实体、关系,抽取实体类型为{'专业','时间','人类','组织','地理地区','事件'},关系类型为{'体育运动','包含行政领土','参加','国家','邦交国','夺得','举办地点','属于','获奖'},你可以先识别出实体再判断实体之间的关系,以(头实体,关系,尾实体)的形式回答\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Conll Dataset - Shell Command\nDESCRIPTION: Command to download the Conll dataset used for English NER dictionary, containing PER (Person), LOC (Location), and MISC (Miscellaneous) entity types.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE-LLM from source in Python\nDESCRIPTION: Commands for setting up a virtual environment and installing the DeepKE-LLM version which supports large language models for knowledge extraction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n deepke-llm python=3.9\nconda activate deepke-llm\n\ncd example/llm\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment\nDESCRIPTION: Commands to create and activate a conda environment and install required dependencies\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/mt5/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n ccks-mt5 python=3.9   \nconda activate ccks-mt5\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: NER Training Data Example in JSON\nDESCRIPTION: Example of a single NER training data entry showing the required task, source, instruction and output fields\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"task\": \"NER\", \"source\": \"NER\", \"instruction\": \"{\\\"instruction\\\": \\\"你是专门进行实体抽取的专家。请从input中抽取出符合schema定义的实体，不存在的实体类型返回空列表。请按照JSON字符串的格式回答。\\\", \\\"schema\\\": [\\\"组织机构\\\", \\\"人物\\\", \\\"地理位置\\\"], \\\"input\\\": \\\"相比之下，青岛海牛队和广州松日队的雨中之战虽然也是0∶0，但乏善可陈。\\\"}\", \"output\": \"{\\\"组织机构\\\": [\\\"广州松日队\\\", \\\"青岛海牛队\\\"], \\\"人物\\\": [], \\\"地理位置\\\": []}\"}\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE with pip in Python\nDESCRIPTION: Commands for installing the DeepKE package using pip. This method is not recommended due to potential package compatibility issues.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepke\n```\n\n----------------------------------------\n\nTITLE: Installing Missing Python Module\nDESCRIPTION: Command to install the 'future' package when encountering ModuleNotFoundError for 'past' module\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/faq.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install future\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Few-Shot NER Dataset\nDESCRIPTION: Bash commands to download the few-shot named entity recognition dataset and extract it to the current directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/few-shot/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ner/few_shot/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Inferencing with IE-Specific LLM Model in Python\nDESCRIPTION: Command to run inference using an IE-specific large language model without LoRA. It specifies the model path, input/output files, and various inference parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python src/inference.py \\\n    --stage sft \\\n    --model_name_or_path 'models/OneKE' \\\n    --model_name 'llama' \\\n    --template 'llama2_zh' \\\n    --do_predict \\\n    --input_file 'data/input.json' \\\n    --output_file 'results/OneKE_output.json' \\\n    --output_dir 'lora/test' \\\n    --predict_with_generate \\\n    --cutoff_len 512 \\\n    --bf16 \\\n    --max_new_tokens 300 \\\n    --bits 4\n```\n\n----------------------------------------\n\nTITLE: Predicting Relations in Python\nDESCRIPTION: Python command to run the prediction script for relation extraction after training the model.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Making predictions with a trained DeepKE model\nDESCRIPTION: Command for using a trained model to make predictions. The prediction parameters can be modified in the conf/predict.yaml file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Downloading DocRED Dataset\nDESCRIPTION: Commands to download and extract the DocRED dataset for document-level relation extraction\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/document/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/document/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Converting Test Data for Knowledge Graph Tasks in Python\nDESCRIPTION: Script for converting test data into instruction format for knowledge graph tasks. Unlike the training conversion, this script does not require label fields and allows schema partitioning for generating multiple test instances from a single input.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/kg2instruction/README_ZH.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython kg2instruction/convert_test.py \\\n  --src_path data/NER/sample.json \\\n  --tgt_path data/NER/processed.json \\\n  --schema_path data/NER/schema.json \\\n  --language zh \\\n  --task NER \\\n  --sample 0 \\\n  --schema_num 4     # 对于单条数据是否对schema进行切分, 若有16个schema label, 则切分后每条数据对应4条测试数据, 以`split`字段区分\n```\n\n----------------------------------------\n\nTITLE: Training a relation extraction model in DeepKE\nDESCRIPTION: Command for training a relation extraction model with DeepKE. The training parameters can be modified in the configuration files in the conf folder.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Running RE Model Training\nDESCRIPTION: Command to start the training process for the relation extraction model using the run.py script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Accelerated Inference using VLLM in Python\nDESCRIPTION: Command to run accelerated inference using VLLM with a merged model. It specifies the model path, input/output files, and various inference parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython src/inference_vllm.py \\\n    --stage sft \\\n    --model_name_or_path 'lora_results/baichuan2-13b-v1/baichuan2-13b-v1' \\\n    --model_name 'baichuan' \\\n    --template 'baichuan2' \\\n    --do_predict \\\n    --input_file 'data/input.json' \\\n    --output_file 'results/baichuan2-13b-IEPile-lora_output.json' \\\n    --output_dir 'lora_results/test' \\\n    --batch_size 4 \\\n    --predict_with_generate \\\n    --max_source_length 1024 \\\n    --bf16 \\\n    --max_new_tokens 512\n```\n\n----------------------------------------\n\nTITLE: Running Model Prediction\nDESCRIPTION: Command to run model prediction using the predict.py script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README_CN.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Training PRGC Model\nDESCRIPTION: Command to start the training process using the run.py script\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PRGC/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository and Navigating to RE Standard Directory\nDESCRIPTION: Commands to clone the DeepKE repository from GitHub and navigate to the standard relation extraction example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/re/standard\n```\n\n----------------------------------------\n\nTITLE: Merging Base Model with LoRA Weights in Python\nDESCRIPTION: Command to merge a base model with trained LoRA weights and export the resulting model. It specifies the base model, LoRA checkpoint, and export directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython src/export_model.py \\\n    --model_name_or_path 'models/Baichuan2-13B-Chat' \\\n    --checkpoint_dir 'lora_results/baichuan2-13b-v1/checkpoint-xxx' \\\n    --export_dir 'lora_results/baichuan2-13b-v1/baichuan2-13b-v1' \\\n    --stage 'sft' \\\n    --model_name 'baichuan' \\\n    --template 'baichuan2' \\\n    --output_dir 'lora_results/test'\n```\n\n----------------------------------------\n\nTITLE: Training DeepKE PURE Model\nDESCRIPTION: Python command to start the training process for the PURE model. It runs the run.py script which handles the training configuration and execution.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PURE/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Predicting with DeepKE PURE Model\nDESCRIPTION: Python command to run predictions using the trained PURE model. It executes the predict.py script which applies the model to generate predictions on new data.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PURE/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Downloading Event Extraction Dataset in Bash\nDESCRIPTION: Commands to download and extract the event extraction dataset.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ee/DuEE.zip\nunzip DuEE.zip\n```\n\n----------------------------------------\n\nTITLE: P-Tuning Prediction for Information Extraction in Python\nDESCRIPTION: Command to run prediction using a P-Tuning trained model on a test dataset. It specifies the model paths, input file, and various prediction parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python src/inference_pt.py \\\n  --test_path data/valid.json \\\n  --device 0 \\\n  --ori_model_dir /model \\\n  --model_dir /output_dir_lora/global_step- \\\n  --max_len 768 \\\n  --max_src_len 450\n```\n\n----------------------------------------\n\nTITLE: Training BERT Model for NER\nDESCRIPTION: Command to train the BERT-based NER model with configuration specified in yaml files. For optimal results with the default dataset, it's recommended to set learning_rate to 2e-5 and num_train_epochs to 10.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython run_bert.py\n```\n\n----------------------------------------\n\nTITLE: Training W2NER Model\nDESCRIPTION: Command to start training the W2NER model using the run.py script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/w2ner/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository and Navigating to Few-Shot NER Directory\nDESCRIPTION: Commands to clone the DeepKE repository from GitHub and navigate to the few-shot named entity recognition example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/few-shot/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ner/few-shot\n```\n\n----------------------------------------\n\nTITLE: Running DeepKE LLM Module for Information Extraction\nDESCRIPTION: Command to execute the main script for information extraction tasks using the DeepKE LLM module. This script processes the configured task and input data using the specified large language model.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/LLMICL/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n>> python run.py\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Training of LightNER Model\nDESCRIPTION: Python command to run few-shot training of the LightNER model using a custom configuration file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/few-shot/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py +train=few_shot\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Training of LightNER Model for Chinese\nDESCRIPTION: Python command to run few-shot training of the LightNER model for Chinese language using a specific configuration file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/few-shot/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython run.py +train=few_shot_cn\n```\n\n----------------------------------------\n\nTITLE: Downloading MNRE Dataset with Visual Objects\nDESCRIPTION: Bash commands to download and extract the MNRE dataset with detected visual objects for use in multimodal relation extraction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/multimodal/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/multimodal/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Data for InstructIE Dataset\nDESCRIPTION: Example of a single data entry in the InstructIE dataset, showing the structure with id, text, and relation fields. The relation field contains triples of head, relation, and tail entities.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/README.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"841ef2af4cfe766dd9295fb7daf321c299df0fd0cef14820dfcb421161eed4a1\", \n  \"text\": \"NGC1313 is a galaxy in the constellation of Reticulum. It was discovered by the Australian astronomer James Dunlop on September 27, 1826. It has a prominent uneven shape, and its axis does not completely revolve around its center. Near NGC1313, there is another galaxy, NGC1309.\", \n  \"relation\": [\n    {\"head\": \"NGC1313\", \"head_type\": \"astronomical object type\", \"relation\": \"time of discovery\", \"tail\": \"September 27, 1826\", \"tail_type\": \"time\"}, \n    {\"head\": \"NGC1313\", \"head_type\": \"astronomical object type\", \"relation\": \"discoverer or inventor\", \"tail\": \"James Dunlop\", \"tail_type\": \"organization/human\"}, \n    {\"head\": \"NGC1313\", \"head_type\": \"astronomical object type\", \"relation\": \"of\", \"tail\": \"Reticulum\", \"tail_type\": \"astronomical object type\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Predicting with Trained LightNER Model\nDESCRIPTION: Python command to run prediction using a trained LightNER model. Requires modifying configuration files to specify model and output paths.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/few-shot/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Running Few-Shot Relation Extraction Training\nDESCRIPTION: Command to start the training process for few-shot relation extraction using the KnowPrompt model on the SEMEVAL dataset.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/few-shot/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Downloading CMeIE Dataset\nDESCRIPTION: Commands to download and extract the CMeIE dataset for training the model\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PRGC/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/triple/PRGC/CMeIE.zip\nunzip ./CMeIE.zip\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Data for IEPile Dataset\nDESCRIPTION: Example of a single data entry in the IEPile dataset, demonstrating the structure with task, source, instruction, and output fields. The instruction field contains a JSON string with instruction, schema, and input information.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/README.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"task\": \"NER\", \n    \"source\": \"CoNLL2003\", \n    \"instruction\": \"{\\\"instruction\\\": \\\"You are an expert in named entity recognition. Please extract entities that match the schema definition from the input. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.\\\", \\\"schema\\\": [\\\"person\\\", \\\"organization\\\", \\\"else\\\", \\\"location\\\"], \\\"input\\\": \\\"284 Robert Allenby ( Australia ) 69 71 71 73 , Miguel Angel Martin ( Spain ) 75 70 71 68 ( Allenby won at first play-off hole )\\\"}\", \n    \"output\": \"{\\\"person\\\": [\\\"Robert Allenby\\\", \\\"Allenby\\\", \\\"Miguel Angel Martin\\\"], \\\"organization\\\": [], \\\"else\\\": [], \\\"location\\\": [\\\"Australia\\\", \\\"Spain\\\"]}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Training IFAformer Model on MNRE Dataset\nDESCRIPTION: Python command to start training the IFAformer model on the MNRE dataset using the provided configuration files.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/multimodal/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Running IFAformer Training for Multimodal NER\nDESCRIPTION: Command to start the training process for the IFAformer model on multimodal NER data using configurations specified in the conf folder.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/multimodal/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Data Reformatting Script Execution\nDESCRIPTION: Command to run the data reformatting script that converts event data to CPM-Bee format.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython data_reformate.py\n```\n\n----------------------------------------\n\nTITLE: NER Schema Format Example in JSON\nDESCRIPTION: Example of the schema format for Named Entity Recognition (NER) tasks, showing the three required lines: entity type list, empty list, and empty dictionary.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/kg2instruction/README_ZH.md#2025-04-07_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\"书名\", \"地址\", \"电影\", ...]    # 实体类型列表\n[]    # 空列表\n{}    # 空字典\n```\n\n----------------------------------------\n\nTITLE: Running IFAformer Prediction for Multimodal NER\nDESCRIPTION: Command to run prediction using a trained IFAformer model. Users can use their own trained model or the pre-trained model provided for Twitter2017 dataset.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/multimodal/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Executing Multi-Domain NER Transfer\nDESCRIPTION: Command to run the transfer learning script for multi-domain NER using the configured YAML file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython run.py hydra/run=multi_transfer.yaml\n```\n\n----------------------------------------\n\nTITLE: Binary Data Conversion\nDESCRIPTION: Command to convert jsonl files to binary format required by CPM-Bee.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython preprocess_dataset.py --input bee_data --output_path bin_data --output_name ccpm_data\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Cyclic Instruction Generation in Python\nDESCRIPTION: This code snippet demonstrates how to use the cyclic instruction generation function for a named entity recognition task in English. It splits the schema into chunks and generates JSON-formatted instructions for each chunk.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE_old.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntask = 'NER'\nlanguage = 'en'\nschema = ['person', 'organization', 'else', 'location']\nsplit_num = split_num_mapper[task]\nsplit_schemas = [schema[i:i+split_num] for i in range(0, len(schema), split_num)]\ninput = '284 Robert Allenby ( Australia ) 69 71 71 73 , Miguel Angel Martin ( Spain ) 75 70 71 68 ( Allenby won at first play-off hole )'\nsintructs = []\nfor split_schema in split_schemas:\n    sintruct = json.dumps({'instruction':instruction_mapper[task+language], 'schema':split_schema, 'input':input}, ensure_ascii=False)\n    sintructs.append(sintruct)\n```\n\n----------------------------------------\n\nTITLE: Predicting with DeepKE AE Model\nDESCRIPTION: Python command to run predictions using the trained attribute extraction model with the predict.py script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README_CN.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Running Event Prediction Pipeline\nDESCRIPTION: Command for executing the prediction pipeline using trained models for event extraction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository for Multimodal NER\nDESCRIPTION: Commands to clone the DeepKE repository from GitHub and navigate to the multimodal NER example directory to access the IFAformer implementation.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/multimodal/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ner/multimodal\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Output for Knowledge Graph Tasks in Python\nDESCRIPTION: Script for converting model string outputs into structured format and calculating F1 scores for knowledge graph tasks. The evaluation compares the model predictions against standard answers for various tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/kg2instruction/README_ZH.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython kg2instruction/evaluate.py \\\n  --standard_path data/NER/processed.json \\\n  --submit_path data/NER/processed.json \\\n  --task NER \\\n  --language zh\n```\n\n----------------------------------------\n\nTITLE: Running Model Prediction\nDESCRIPTION: Command to run prediction using the trained model, which generates result.json in the current directory\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/document/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Running Model Prediction\nDESCRIPTION: Command to run model inference using predict.py script after configuring the trained model path\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Conda Environment Setup\nDESCRIPTION: Command to create a new Python environment using Conda\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n asp python=3.8.16\n```\n\n----------------------------------------\n\nTITLE: CPM-Bee Text Generation Format Example\nDESCRIPTION: Example showing the data format conversion from initial event data to CPM-Bee compatible format for text generation tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"文本生成\": {\"input\": \"今天天气很好，我和妈妈一起去公园，\", \"prompt\": \"往后写约100字\", \"<ans>\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: Evaluation Execution\nDESCRIPTION: Command to run model evaluation with specific GPU configuration\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python evaluate_ere.py CMeIE Mar05_19-39-56_2000 0\n```\n\n----------------------------------------\n\nTITLE: Running Fine-tuning Script\nDESCRIPTION: Command to execute the CPM-Bee fine-tuning script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/finetune_cpm_bee.sh\n```\n\n----------------------------------------\n\nTITLE: Dataset Download Commands\nDESCRIPTION: Commands to download and extract the CMeIE dataset\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/DeepKE/example/triple/ASP/data\nwget 120.27.214.45/Data/triple/ASP/CMeIE.zip\nunzip ./CMeIE.zip\nrm ./CMeIE.zip\ncd ..\n```\n\n----------------------------------------\n\nTITLE: Text Generation Execution\nDESCRIPTION: Command to run the text generation script using the fine-tuned model.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython text_generation.py\n```\n\n----------------------------------------\n\nTITLE: Text Generation Execution\nDESCRIPTION: Command to run the text generation script using the fine-tuned model.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython text_generation.py\n```\n\n----------------------------------------\n\nTITLE: Running CPM-Bee Fine-tuning Script\nDESCRIPTION: Executes the fine-tuning script for CPM-Bee. The fine-tuned model will be saved in the path specified in the configuration.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README_CN.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/finetune_cpm_bee.sh\n```\n\n----------------------------------------\n\nTITLE: Running Triple Extraction Task with EasyInstruct\nDESCRIPTION: Shell command to execute the run.py script for triple extraction tasks using the EasyInstruct framework.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/LLMICL/README.md#2025-04-07_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n>> python run.py\n```\n\n----------------------------------------\n\nTITLE: Training Execution\nDESCRIPTION: Commands to set environment variable and run training\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport ASP=$PWD\npython run_ere.py CMeIE\n```\n\n----------------------------------------\n\nTITLE: Competition Format Conversion\nDESCRIPTION: Command to convert the generated output to CCKS2023 submission format by extracting knowledge graph fields.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ../InstructKGC/convert.py\n-pred_path \"cpm_bee_TG.json\"\n-tgt_path \"cpm_bee_TG_kg.json\"\n```\n\n----------------------------------------\n\nTITLE: Running In-Context Learning for Relation Extraction with GPT-3\nDESCRIPTION: Command-line usage for the gpt3ICL.py script that implements in-context learning for relation extraction. The script supports different prompt types and configurable k-shot demonstrations.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/UnleashLLMRE/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n>> python gpt3ICL.py -h\n    usage: gpt3ICL.py [-h] --api_key API_KEY --train_path TRAIN_PATH --test_path TEST_PATH --output_success OUTPUT_SUCCESS --output_nores OUTPUT_NORES --prompt {text,text_schema,instruct,instruct_schema} [--k K]\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --api_key API_KEY, -ak API_KEY\n      --train_path TRAIN_PATH, -tp TRAIN_PATH\n                            The path of training / demonstration data.\n      --test_path TEST_PATH, -ttp TEST_PATH\n                            The path of test data.\n      --output_success OUTPUT_SUCCESS, -os OUTPUT_SUCCESS\n                            The output directory of successful ICL samples.\n      --output_nores OUTPUT_NORES, -on OUTPUT_NORES\n                            The output directory of failed ICL samples.\n      --prompt {text,text_schema,instruct,instruct_schema}\n      --k K                 k-shot demonstrations\n```\n\n----------------------------------------\n\nTITLE: Generating Augmented Data for Relation Extraction using GPT-3\nDESCRIPTION: Command-line usage for the gpt3DA.py script, which generates augmented data for relation extraction using GPT-3. It allows specifying API key, demonstration data path, output directory, dataset type, and the number of demonstration shots.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/UnleashLLMRE/README.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n>> python gpt3DA.py -h\n  usage: gpt3DA.py [-h] --api_key API_KEY --demo_path DEMO_PATH --output_dir OUTPUT_DIR --dataset {tacred,tacrev,retacred} [--k K]\n\n  optional arguments:\n    -h, --help            show this help message and exit\n    --api_key API_KEY, -ak API_KEY\n    --demo_path DEMO_PATH, -dp DEMO_PATH\n                          The directory of demonstration data.\n    --output_dir OUTPUT_DIR\n                          The output directory of generated data.\n    --dataset {tacred,tacrev,retacred}\n    --k K                 k-shot demonstrations\n```\n\n----------------------------------------\n\nTITLE: Running Model Inference\nDESCRIPTION: Python script for running inference using the trained model with DeepSpeed configuration\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/mt5/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel_name=\"output/ccks_mt5-base_f1_1e-4\"\noutput_dir=\"output/ccks_mt5-base_f1_1e-4_test_result\"\ndata_dir=\"data\"\n\ndeepspeed  --include localhost:0 run_finetune.py \\\n    --do_predict \\\n    --predict_with_generate \\\n    --use_fast_tokenizer=True \\\n    --per_device_eval_batch_size 16 \\\n    --test_file=${data_dir}/valid.json \\\n    --model_name_or_path=${model_name}   \\\n    --output_dir=${output_dir}  \\\n    --overwrite_output_dir=False \\\n    --logging_dir=${output_dir}_log \\\n    --preprocessing_num_workers 4 \\\n    --generation_max_length 256 \\\n    --generation_num_beams 1 \\\n    --gradient_checkpointing=True \\\n    --bf16=True \\\n    --deepspeed \"configs/ds_mt5_z3_config_bf16.json\" \\\n    --seed 42\n```\n\n----------------------------------------\n\nTITLE: Running Data Format Conversion Script\nDESCRIPTION: Executes a Python script to convert the original knowledge graph data into the format required by CPM-Bee for fine-tuning.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython data_reformate.py\n```\n\n----------------------------------------\n\nTITLE: Running Data Augmentation Script with Command-line Arguments\nDESCRIPTION: This snippet demonstrates how to use the DA.py script to perform data augmentation. It shows an example of context-level augmentation using BERT embeddings on the SemEval dataset.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/few-shot/DA/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython DA.py \\\n    -i train.txt \\\n    -o aug \\\n    -d word_embedding_bert \\\n    -mn bert-base-uncased \\\n    -l sent1 sent2 sent3\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Directories for IE Tasks\nDESCRIPTION: Bash commands to activate the Python virtual environment, create necessary directories, and place data in the appropriate location for IE tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nconda activate deepke-llm\n\nmkdir results\nmkdir lora\nmkdir data\n```\n\n----------------------------------------\n\nTITLE: Converting Output Format\nDESCRIPTION: Command to run the format conversion script for CCKS2023 competition submission\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/mt5/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython convert.py \\\n    --src_path=\"data/valid.json\" \\\n    --pred_path=\"output/ccks_mt5-base_f1_1e-4/test_preds.json\" \\\n    --tgt_path=\"output/valid_result.json\" \\\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Dataset for CPM-Bee\nDESCRIPTION: Converts the formatted data into binary files required by CPM-Bee for training. This uses the preprocess_dataset.py script from CPM-Bee.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython preprocess_dataset.py --input bee_data --output_path bin_data --output_name ccpm_data\n```\n\n----------------------------------------\n\nTITLE: Running English NER Data Preparation - Bash Command\nDESCRIPTION: Command to run the data preparation script for English NER using a specified dictionary file. Processes input text files and generates train/dev/test splits.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython prepare_weaksupervised_data.py --language en --dict_dir vocab_dict_en.csv\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Dataset\nDESCRIPTION: Commands to download and extract the training dataset for DeepKE\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ae/standard/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Running PRGC Prediction\nDESCRIPTION: Command to execute prediction using the trained model\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PRGC/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Data Conversion Command\nDESCRIPTION: Command for converting raw data to training format with parameters for language, task type, and schema splitting\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ie2instruction/convert_func.py \\\n    --src_path data/NER/sample.json \\\n    --tgt_path data/NER/train.json \\\n    --schema_path data/NER/schema.json \\\n    --language zh \\\n    --task NER \\\n    --split_num 6 \\\n    --random_sort \\\n    --split train\n```\n\n----------------------------------------\n\nTITLE: Running Preprocessing Script for ACE05 Event Data\nDESCRIPTION: Commands to navigate to the preprocess directory and execute the process.sh script to prepare the ACE05 Event data for use in DeepKE.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/data/ACE/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ./preprocess\nbash process.sh\n```\n\n----------------------------------------\n\nTITLE: Running Data Labeling Script - Bash\nDESCRIPTION: Command to execute the data labeling script with language, source file and triple file parameters\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython ds_label_data.py --language en --source_file source_data.json --triple_file triple_file.csv\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation Command\nDESCRIPTION: Command for evaluating model performance on NER task\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/OneKE.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ie2instruction/eval_func.py \\\n  --path1 data/NER/processed.json \\\n  --task NER\n```\n\n----------------------------------------\n\nTITLE: Training Attribute Extraction Model with Python\nDESCRIPTION: This snippet shows how to train the attribute extraction model using a Python script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Standard RE Module Setup and Execution\nDESCRIPTION: Commands for setting up and running standard relation extraction using various deep learning models. Includes data preparation and model execution steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd example/re/standard\nwget 120.27.214.45/Data/re/standard/data.tar.gz\ntar -xzvf data.tar.gz\npython run.py\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU LoRA Training Configuration\nDESCRIPTION: Distributed training setup for LoRA fine-tuning using multiple GPUs with torchrun launcher.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir='lora/llama2-13b-chat-v1'\nmkdir -p ${output_dir}\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node=4 --master_port=1287 src/finetune.py \\\n    ...其余同上\n```\n\n----------------------------------------\n\nTITLE: Installing EasyInstruct and Hydra for DeepKE LLM Module\nDESCRIPTION: Commands to install the required EasyInstruct toolkit and Hydra for the DeepKE LLM module. EasyInstruct is used to interact with large language models, while Hydra is used for configuration management.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/LLMICL/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n>> pip install git+https://github.com/zjunlp/EasyInstruct\n>> pip install hydra-core\n```\n\n----------------------------------------\n\nTITLE: Multimodal NER Module Setup and Execution\nDESCRIPTION: Commands for implementing multimodal NER using both text and image data. Includes data download, setup, training and prediction steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd example/ner/multimodal\nwget 120.27.214.45/Data/ner/multimodal/data.tar.gz\ntar -xzvf data.tar.gz\npython run.py\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML for Cross-Domain NER Transfer (CoNLL03 to AI)\nDESCRIPTION: YAML configuration for transferring the CP-NER model from CoNLL03 (source) to AI (target) domain, specifying file paths and model locations.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntrain_file: 'data/ai/train.json'  # train file of AI\nvalidation_file: 'data/ai/val.json'\ntest_file: 'data/ai/test.json'\nrecord_schema: '../../data/ai/record.schema'\noutput_dir: 'output/conll_to_ai-t5-base'\nlogging_dir: 'output/conll_to_ai-t5-base_log'\nmodel_name_or_path: '../xxx/output/ai-t5-base'\nsource_prefix_path: '../xxx/output//conll-t5-base'  # the tuned model of source doamin CoNLL03\ntarget_prefix_path: '../xxx/output/ai-t5-base'      # the tuned model of target doamin AI\n```\n\n----------------------------------------\n\nTITLE: Converting Test Data for IE Tasks using Bash Script\nDESCRIPTION: Bash command to convert test data for IE tasks using the provided convert_func.py script. It specifies input and output paths, schema, language, task type, and other parameters for test data conversion.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ie2instruction/convert_func.py \\\n    --src_path data/NER/sample.json \\\n    --tgt_path data/NER/test.json \\\n    --schema_path data/NER/schema.json \\\n    --language zh \\\n    --task NER \\\n    --split_num 6 \\\n    --split test\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output for Model Prediction\nDESCRIPTION: This JSON snippet shows the structure of the output file generated by the model prediction step. It includes fields for id, instruction, label, and the model's output.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"e88d2b42f8ca14af1b77474fcb18671ed3cacc0c75cf91f63375e966574bd187\", \n  \"instruction\": \"{\\\"instruction\\\": \\\"You are an expert specializing in entity extraction. Please extract the entities that conform to the schema definition from the input. Return an empty list for non-existent entity types. Please answer in the format of a JSON string.\\\", \\\"schema\\\": [\\\"Organization\\\", \\\"Geographical Location\\\", \\\"Person\\\"], \\\"input\\\": \\\"In contrast, although the rain battle between Qingdao Hainiu Team and Guangzhou Songri Team was also 0∶0, it was lackluster.\\\"}\", \n  \"label\": \"[{\\\"entity\\\": \\\"Guangzhou Songri Team\\\", \\\"entity_type\\\": \\\"Organization\\\"}, {\\\"entity\\\": \\\"Qingdao Hainiu Team\\\", \\\"entity_type\\\": \\\"Organization\\\"}]\",\n  \"output\": \"{\\\"Organization\\\": [\\\"Guangzhou Songri Team\\\", \\\"Qingdao Hainiu Team\\\"], \\\"Person\\\": [], \\\"Geographical Location\\\": []}\"\n}\n```\n\n----------------------------------------\n\nTITLE: DeepKE Installation\nDESCRIPTION: Commands to build and install the DeepKE package\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/DeepKE\npython setup.py build\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: English Relation Types List - Python\nDESCRIPTION: List of relation types supported in the English NYT dataset triple file format\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"/business/company/place_founded\",\n\"/people/person/place_lived\",\n\"/location/country/administrative_divisions\",\n\"/business/company/major_shareholders\",\n\"/sports/sports_team_location/teams\",\n\"/people/person/religion\",\n\"/people/person/place_of_birth\",\n\"/people/person/nationality\",\n\"/location/country/capital\",\n\"/business/company/advisors\",\n\"/people/deceased_person/place_of_death\",\n\"/business/company/founders\",\n\"/location/location/contains\",\n\"/people/person/ethnicity\",\n\"/business/company_shareholder/major_shareholder_of\",\n\"/people/ethnicity/geographic_distribution\",\n\"/people/person/profession\",\n\"/business/person/company\",\n\"/people/person/children\",\n\"/location/administrative_division/country\",\n\"/people/ethnicity/people\",\n\"/sports/sports_team/location\",\n\"/location/neighborhood/neighborhood_of\",\n\"/business/company/industry\"\n```\n\n----------------------------------------\n\nTITLE: P-Tuning Prediction in Bash\nDESCRIPTION: This script shows how to use a P-Tuning fine-tuned model for prediction. It includes parameters for specifying the model paths and prediction settings.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python src/inference_pt.py \\\n  --test_path data/valid.json \\\n  --device 0 \\\n  --ori_model_dir /model \\\n  --model_dir /output_dir_lora/global_step- \\\n  --max_len 768 \\\n  --max_src_len 450\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citations for DeepKE Project Papers\nDESCRIPTION: BibTeX formatted citations for two papers - 'InstructIE: A Bilingual Instruction-based Information Extraction Dataset' from 2023 and 'IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus' from 2024.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md#2025-04-07_snippet_18\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{DBLP:journals/corr/abs-2305-11527,\n  author       = {Honghao Gui and Shuofei Qiao and Jintian Zhang and Hongbin Ye and Mengshu Sun and Lei Liang and Huajun Chen and Ningyu Zhang},\n  title        = {InstructIE: A Bilingual Instruction-based Information Extraction Dataset},\n  journal      = {CoRR},\n  volume       = {abs/2305.11527},\n  year         = {2023},\n  url          = {https://doi.org/10.48550/arXiv.2305.11527},\n  doi          = {10.48550/arXiv.2305.11527},\n  eprinttype    = {arXiv},\n  eprint       = {2305.11527},\n  timestamp    = {Thu, 25 May 2023 15:41:47 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-11527.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{DBLP:journals/corr/abs-2402-14710,\n  author       = {Honghao Gui and\n                  Lin Yuan and\n                  Hongbin Ye and\n                  Ningyu Zhang and\n                  Mengshu Sun and\n                  Lei Liang and\n                  Huajun Chen},\n  title        = {IEPile: Unearthing Large-Scale Schema-Based Information Extraction\n                  Corpus},\n  journal      = {CoRR},\n  volume       = {abs/2402.14710},\n  year         = {2024},\n  url          = {https://doi.org/10.48550/arXiv.2402.14710},\n  doi          = {10.48550/ARXIV.2402.14710},\n  eprinttype    = {arXiv},\n  eprint       = {2402.14710},\n  timestamp    = {Tue, 09 Apr 2024 07:32:43 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-14710.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Few Shot Documentation in reStructuredText\nDESCRIPTION: This snippet defines a table of contents for the Few Shot module documentation using reStructuredText syntax. It specifies a maximum depth of 4 and includes links to documentation for models, module, and utilities.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.name_entity_re.few_shot.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.name_entity_re.few_shot.models\n   deepke.name_entity_re.few_shot.module\n   deepke.name_entity_re.few_shot.utils\n```\n\n----------------------------------------\n\nTITLE: Defining Name Entity Recognition Documentation Structure in reStructuredText\nDESCRIPTION: This snippet defines the table of contents for Name Entity Recognition documentation using reStructuredText. It includes references to standard, few-shot, and multimodal approaches within the DeepKE project.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.name_entity_re.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.name_entity_re.standard\n   deepke.name_entity_re.few_shot\n   deepke.name_entity_re.multimodal\n```\n\n----------------------------------------\n\nTITLE: Data Generation for Relation Extraction using GPT-3\nDESCRIPTION: Command-line usage for the gpt3DA.py script that generates additional labeled data for relation extraction tasks using large language models based on few-shot examples.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/UnleashLLMRE/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n>> python gpt3DA.py -h\n  usage: gpt3DA.py [-h] --api_key API_KEY --demo_path DEMO_PATH --output_dir OUTPUT_DIR --dataset {tacred,tacrev,retacred} [--k K]\n\n  optional arguments:\n    -h, --help            show this help message and exit\n    --api_key API_KEY, -ak API_KEY\n    --demo_path DEMO_PATH, -dp DEMO_PATH\n                          The directory of demonstration data.\n    --output_dir OUTPUT_DIR\n                          The output directory of generated data.\n    --dataset {tacred,tacrev,retacred}\n    --k K                 k-shot demonstrations\n```\n\n----------------------------------------\n\nTITLE: NER Training and Prediction Commands\nDESCRIPTION: Commands for training and running predictions specifically for the Named Entity Recognition (NER) task.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ner/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Setting up Sphinx toctree for Multimodal Documentation\nDESCRIPTION: A Sphinx toctree directive that organizes documentation for the multimodal components of DeepKE. It includes references to model documentation and module documentation with a maximum depth of 4 levels.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.name_entity_re.multimodal.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.name_entity_re.multimodal.models\n   deepke.name_entity_re.multimodal.modules\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Structure for DeepKE Models\nDESCRIPTION: RST-formatted documentation structure defining the module hierarchy and documentation options for DeepKE's multimodal models.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.name_entity_re.multimodal.models.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.name_entity_re.multimodal.models.clip\n\ndeepke.name\\_entity\\_re.multimodal.models.IFA\\_model module\n-----------------------------------------------------------\n\n.. automodule:: deepke.name_entity_re.multimodal.models.IFA_model\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\ndeepke.name\\_entity\\_re.multimodal.models.modeling\\_IFA module\n--------------------------------------------------------------\n\n.. automodule:: deepke.name_entity_re.multimodal.models.modeling_IFA\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Downloading Relation Extraction Dataset in Bash\nDESCRIPTION: Commands to download and extract the relation extraction dataset for the standard supervised approach.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Cross-Domain NER Dataset\nDESCRIPTION: Bash commands to download the cross-domain NER dataset archive and extract it to the current directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ner/cross/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Downloading Relation Extraction Dataset in Bash\nDESCRIPTION: Commands to download and extract the relation extraction dataset for the standard supervised approach.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Academic Citation\nDESCRIPTION: BibTeX citation for the ASP paper\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{DBLP:conf/emnlp/LiuJMCS22,\n  author    = {Tianyu Liu and\n               Yuchen Eleanor Jiang and\n               Nicholas Monath and\n               Ryan Cotterell and\n               Mrinmaya Sachan},\n  editor    = {Yoav Goldberg and\n               Zornitsa Kozareva and\n               Yue Zhang},\n  title     = {Autoregressive Structured Prediction with Language Models},\n  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}\n               2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},\n  pages     = {993--1005},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.findings-emnlp.70},\n  timestamp = {Tue, 07 Feb 2023 17:10:51 +0100},\n  biburl    = {https://dblp.org/rec/conf/emnlp/LiuJMCS22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n----------------------------------------\n\nTITLE: Academic Citation Format\nDESCRIPTION: BibTeX citation for the PRGC paper published in ACL/IJCNLP 2021\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/PRGC/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{DBLP:conf/acl/ZhengWCYZZZQMZ20,\n  author    = {Hengyi Zheng and\n               Rui Wen and\n               Xi Chen and\n               Yifan Yang and\n               Yunyan Zhang and\n               Ziheng Zhang and\n               Ningyu Zhang and\n               Bin Qin and\n               Xu Ming and\n               Yefeng Zheng},\n  editor    = {Chengqing Zong and\n               Fei Xia and\n               Wenjie Li and\n               Roberto Navigli},\n  title     = {{PRGC:} Potential Relation and Global Correspondence Based Joint Relational\n               Triple Extraction},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational\n               Linguistics and the 11th International Joint Conference on Natural\n               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual\n               Event, August 1-6, 2021},\n  pages     = {6225--6235},\n  publisher = {Association for Computational Linguistics},\n  year      = {2021},\n  url       = {https://doi.org/10.18653/v1/2021.acl-long.486},\n  doi       = {10.18653/v1/2021.acl-long.486},\n  timestamp = {Tue, 24 Jan 2023 18:41:07 +0100},\n  biburl    = {https://dblp.org/rec/conf/acl/ZhengWCYZZZQMZ20.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML for Single-Domain NER Training (CoNLL03 Example)\nDESCRIPTION: YAML configuration for training the CP-NER model on the CoNLL03 dataset, specifying file paths, output directories, and model parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntrain_file: 'data/conll03/train.json'\nvalidation_file: 'data/conll03/val.json'\ntest_file: 'data/conll03/test.json'\nrecord_schema: '../../data/conll03/record.schema'\noutput_dir: 'output/conll03-t5-base'        # output path\nlogging_dir: 'output/conll03-t5-base_log'   # log path\nmodel_name_or_path: '../../hf_models/t5-base' # model path\n```\n\n----------------------------------------\n\nTITLE: Predicting NER Results with DeepKE\nDESCRIPTION: Python code for predicting named entities using a pre-trained BERT model. Takes text input and outputs entity labels with their positions.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: JavaScript Object Property Closing\nDESCRIPTION: A code fragment showing a closing curly brace followed by a comma, typically used in JavaScript object notation to separate properties within an object.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n},\n```\n\n----------------------------------------\n\nTITLE: JavaScript Empty String Declaration\nDESCRIPTION: A JavaScript code fragment showing the declaration of an empty string using double quotes. This is commonly used to initialize a string variable.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n\"\"\n```\n\n----------------------------------------\n\nTITLE: Executing Cross-Domain NER Transfer\nDESCRIPTION: Command to run the transfer learning script for cross-domain NER using the configured YAML file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython run.py hydra/run=single_transfer.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Custom Training Pipeline\nDESCRIPTION: Commands for executing custom training pipelines for both NER and RE tasks using specified configuration files.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CNSCHEMA.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML for Saving Domain Prefixes and Label Words\nDESCRIPTION: YAML configuration for saving domain-specific prefixes and label words, used in multi-domain transfer setup.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\noutput_dir: '../xxx/output/conll-t5-base'\nmodel_name_or_path: '../xxx/output/conll-t5-base' # the tuned model of CoNLL03\nmodel_ckpt_path: '../xxx/output/conll-t5-base'    # the tuned model of CoNLL03\nsave_prefix: true\nsave_label_word: true\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository\nDESCRIPTION: Commands to clone the DeepKE repository and navigate to the standard AE example directory\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ae/standard\n```\n\n----------------------------------------\n\nTITLE: Executing Prefix and Label Word Saving\nDESCRIPTION: Command to run the script for saving domain-specific prefixes and label words using the configured YAML file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython run.py hydra/run=save_prefix_label.yaml\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository and Navigating to AE Module\nDESCRIPTION: Git commands to clone the DeepKE repository and navigate to the attribute extraction standard example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ae/standard\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML for Multi-Domain NER Transfer\nDESCRIPTION: YAML configuration for transferring the CP-NER model from multiple source domains to a target domain, specifying model paths and source domains.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_name_or_path: '../xxx/output/ai-t5-base' # the tuned model of target domain AI\nmodel_ckpt_path: '../xxx/output/ai-t5-bases'  # the tuned model of target domain AI\nmulti_source_path: '../xxx/output/conll-t5-base,../xxx/output/politics-t5-base,../xxx/output/music-t5-base,../xxx/output/literature-t5-base' # the tuned model of source domains (separated by commas)\n```\n\n----------------------------------------\n\nTITLE: Downloading Training Data for DeepKE AE Module\nDESCRIPTION: Wget command to download the attribute extraction training data archive from a specified URL.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ae/standard/data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Running Chinese NER Data Preparation - Bash Command\nDESCRIPTION: Command to run the weak supervision data preparation script for Chinese NER using a specified dictionary file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython prepare_weaksupervised_data.py --language cn --dict_dir vocab_dict_cn.csv\n```\n\n----------------------------------------\n\nTITLE: Downloading and Navigating to the DeepKE Code Repository\nDESCRIPTION: Commands to clone the DeepKE repository from GitHub and navigate to the event extraction standard example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ee/standard\n```\n\n----------------------------------------\n\nTITLE: Running English NER Data Preparation - Bash Command\nDESCRIPTION: Command to run the weak supervision data preparation script for English NER using a specified dictionary file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/prepare-data/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython prepare_weaksupervised_data.py --language en --dict_dir vocab_dict_en.csv\n```\n\n----------------------------------------\n\nTITLE: Running Event Arguments Extraction Training\nDESCRIPTION: Command to train the event arguments extraction model using gold triggers. The task_name should be set to 'role' in the configuration file before running this command.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment for DeepKE\nDESCRIPTION: Commands to create and activate a dedicated Python virtual environment for DeepKE, ensuring proper isolation of dependencies and consistent execution environment.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n deepke python=3.8\nconda activate deepke\n```\n\n----------------------------------------\n\nTITLE: Running Pipeline Prediction for Event Arguments\nDESCRIPTION: Command to predict event arguments using previously predicted triggers. Configuration parameters should be set in predict.yaml, including the trained role model path and enabling pipeline prediction.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE Dependencies\nDESCRIPTION: Command to install all required Python packages for DeepKE from the requirements file, ensuring all necessary libraries are available for the NER models.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository\nDESCRIPTION: Git commands to clone the DeepKE repository and navigate to the event extraction example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ee/standard\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Dataset for Few-Shot Relation Extraction\nDESCRIPTION: Commands to download and extract the dataset required for few-shot relation extraction training and evaluation.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/few-shot/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/few_shot/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Training Event Extraction Model\nDESCRIPTION: Command for running the training script for both trigger word classification and event role extraction models.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Downloading Training Data\nDESCRIPTION: Command to download the training data for relation extraction tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/standard/data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install required project dependencies from requirements file\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\npip install -r requirements\n```\n\n----------------------------------------\n\nTITLE: Starting Model Training\nDESCRIPTION: Command to start the model training process using the run.py script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/standard/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Running Model Training\nDESCRIPTION: Command to start the model training process using run.py script\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for DeepKE\nDESCRIPTION: Commands to create and activate a virtual conda environment for DeepKE with Python 3.8.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/install.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n deepke python=3.8\nconda activate deepke\n```\n\n----------------------------------------\n\nTITLE: Cloning DeepKE Repository\nDESCRIPTION: This code clones the DeepKE repository from GitHub and navigates to the event extraction example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ee/standard/degree\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting up DeepKE Repository\nDESCRIPTION: Instructions for getting started with DeepKE by cloning the GitHub repository and changing to the project directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/start.rst#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting DuEE Dataset using Bash\nDESCRIPTION: A bash script that downloads the DuEE dataset from a specified IP address and extracts its contents. The dataset is likely used for event extraction tasks within the DeepKE framework.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/data/DuEE/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ee/DuEE.zip\nunzip DuEE.zip\n```\n\n----------------------------------------\n\nTITLE: Downloading Document-Level Relation Extraction Dataset in Bash\nDESCRIPTION: Commands to download and extract the relation extraction dataset for the document-level approach.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/re/document/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Executing Single-Domain NER Training\nDESCRIPTION: Command to run the training script for single-domain NER using the configured YAML file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/cross/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Downloading Attribute Extraction Dataset in Bash\nDESCRIPTION: Commands to download and extract the attribute extraction dataset for the standard supervised approach.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ae/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Training BiLSTM-CRF Model for NER\nDESCRIPTION: Command to train the BiLSTM-CRF model for NER. This model requires building a vocabulary from the training dataset and typically runs faster than BERT during inference.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython run_lstmcrf.py\n```\n\n----------------------------------------\n\nTITLE: Preparing data for relation extraction in DeepKE\nDESCRIPTION: Commands for downloading and extracting a dataset for training a relation extraction model in DeepKE's standard scenario.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd DeepKE/example/re/standard\n\nwget 120.27.214.45/Data/re/standard/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Running NER Prediction\nDESCRIPTION: Command to execute prediction using the trained NER model. This will apply the model to new text data based on the configuration specified in the config files.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Exporting Annotated Entity Data in JSON Format for DeepKE\nDESCRIPTION: Example of exported entity recognition data from doccano in JSON format. Contains entity annotations with spans and labels that can be processed for use with DeepKE.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\":10,\n    \"text\":\"University of California is located in California, United States.\",\n    \"entities\":[\n        {\n            \"id\":15,\n            \"label\":\"ORG\",\n            \"start_offset\":0,\n            \"end_offset\":24\n        },\n        {\n            \"id\":16,\n            \"label\":\"LOC\",\n            \"start_offset\":39,\n            \"end_offset\":49\n        },\n        {\n            \"id\":17,\n            \"label\":\"LOC\",\n            \"start_offset\":51,\n            \"end_offset\":64\n        }\n    ],\n    \"relations\":[\n        \n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading DeepKE W2NER Code\nDESCRIPTION: Commands to clone the DeepKE repository and navigate to the W2NER example directory.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/w2ner/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/ner/standard/w2ner\n```\n\n----------------------------------------\n\nTITLE: Standard AE Module Setup and Execution\nDESCRIPTION: Commands for setting up and running standard attribute extraction using various deep learning models. Includes data preparation steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd example/ae/standard\nwget 120.27.214.45/Data/ae/standard/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting NER Dataset\nDESCRIPTION: Commands to download and extract the NER dataset for training and evaluation.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/w2ner/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget 120.27.214.45/Data/ner/standard/data.tar.gz\ntar -xzvf data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Running Training Process\nDESCRIPTION: Command to start the training process using the run.py script\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: W2NER Model Citation\nDESCRIPTION: BibTeX citation for the W2NER model paper.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/w2ner/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{DBLP:conf/aaai/Li00WZTJL22,\n  author    = {Jingye Li and\n               Hao Fei and\n               Jiang Liu and\n               Shengqiong Wu and\n               Meishan Zhang and\n               Chong Teng and\n               Donghong Ji and\n               Fei Li},\n  title     = {Unified Named Entity Recognition as Word-Word Relation Classification},\n  booktitle = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence, {AAAI}\n               2022, Thirty-Fourth Conference on Innovative Applications of Artificial\n               Intelligence, {IAAI} 2022, The Twelveth Symposium on Educational Advances\n               in Artificial Intelligence, {EAAI} 2022 Virtual Event, February 22\n               - March 1, 2022},\n  pages     = {10965--10973},\n  publisher = {{AAAI} Press},\n  year      = {2022},\n  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/21344},\n  timestamp = {Tue, 12 Jul 2022 14:14:21 +0200},\n  biburl    = {https://dblp.org/rec/conf/aaai/Li00WZTJL22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n----------------------------------------\n\nTITLE: Running Prediction Process\nDESCRIPTION: Command to execute prediction using the predict.py script\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Training DocuNet Model\nDESCRIPTION: Command to start the training process using the DocuNet model on DocRED dataset\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/document/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for DeepKE AE Module\nDESCRIPTION: Pip command to install the required Python packages listed in the requirements.txt file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Merging Augmented Datasets\nDESCRIPTION: This snippet shows how to use the merge_dataset.py script to combine the original dataset with the augmented data, removing duplicates in the process.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/few-shot/DA/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython merge_dataset.py \\\n    -i train.txt aug/aug.txt \\\n    -o aug/merge.txt\n```\n\n----------------------------------------\n\nTITLE: Training DeepKE AE Model\nDESCRIPTION: Python command to start the training process for the attribute extraction model using the run.py script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ae/standard/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run.py\n```\n\n----------------------------------------\n\nTITLE: Running Prediction with Trained Few-Shot Relation Extraction Model\nDESCRIPTION: Command to run prediction using the trained few-shot relation extraction model.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/few-shot/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for DeepKE\nDESCRIPTION: Commands for installing required Python packages and dependencies. Specifies Python version 3.8 and includes hydra-core installation with version conflict handling.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython==3.8\npip install -r requirements.txt\npip install hydra-core==1.3.1 # ignore the conlict with deepke\n```\n\n----------------------------------------\n\nTITLE: Predicting with Trained IFAformer Model\nDESCRIPTION: Python command to run predictions using a trained IFAformer model, with instructions to modify the configuration file to specify the model path.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/multimodal/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Displaying Initial Data Directory Structure in DeepKE Project\nDESCRIPTION: This shows the expected directory structure for the raw data before processing. The raw_data folder should contain three JSON files: dev.json, test.json, and train.json.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/data/ACE/README.md#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nraw_data\n├── dev.json\n├── test.json\n└── train.json\n```\n\n----------------------------------------\n\nTITLE: Running Data Preparation Script in Bash\nDESCRIPTION: Command to run the ds_label_data.py script for preparing relation extraction data, specifying language, source file, and triple file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ds_label_data.py --language en --source_file source_data.json --triple_file triple_file.csv\n```\n\n----------------------------------------\n\nTITLE: Displaying Final ACE Dataset Directory Structure\nDESCRIPTION: The expected directory structure after processing the ACE05 Event data. Shows the organization of files for different components including degree data, preprocessing scripts, raw data, role data for event argument extraction, schema for BertCRF, and trigger data for event detection.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/data/ACE/README.md#2025-04-07_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nACE\n├── degree # data for degree\n├── preprocess # preprocess scripts\n├── raw_data \n├── role # data for eae in bertcrf\n├── schema # schema for bertcrf\n└── trigger # data for ed in bertcrf\n```\n\n----------------------------------------\n\nTITLE: Downloading DeepKE Repository\nDESCRIPTION: Git commands to clone the DeepKE repository and navigate to the ASP example directory\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/triple/ASP/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/zjunlp/DeepKE.git\ncd DeepKE/example/triple/ASP\n```\n\n----------------------------------------\n\nTITLE: Installing DeepKE Dependencies\nDESCRIPTION: This snippet shows how to install the required dependencies for the DeepKE project using pip.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\npip install -r requirements\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Attribution Extraction in reStructuredText\nDESCRIPTION: This snippet creates a table of contents (toctree) for the Attribution Extraction documentation. It sets the maximum depth to 4 and includes a reference to the standard attribution extraction module.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.attribution_extraction.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.attribution_extraction.standard\n```\n\n----------------------------------------\n\nTITLE: Generating Processed Data for Event Extraction\nDESCRIPTION: This command processes the ACE dataset for event extraction tasks. It uses the settings specified in the config.yaml file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README_CN.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython generate_data.py\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Standard Attribution Extraction Module in reStructuredText\nDESCRIPTION: This code defines a table of contents (toctree) with maxdepth of 4 that includes documentation for different components of the standard attribution extraction module in DeepKE. It references models, module, tools, and utilities.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.attribution_extraction.standard.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.attribution_extraction.standard.models\n   deepke.attribution_extraction.standard.module\n   deepke.attribution_extraction.standard.tools\n   deepke.attribution_extraction.standard.utils\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Trained Event Extraction Model\nDESCRIPTION: This command runs the prediction script using the trained model. The model path should be specified in the config.yaml file before running this command.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ee/standard/degree/README_CN.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Standard Module in reStructuredText\nDESCRIPTION: This snippet defines a table of contents (toctree) for the Standard module documentation with a maximum depth of 4, including references to name entity recognition models and tools.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.name_entity_re.standard.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.name_entity_re.standard.models\n   deepke.name_entity_re.standard.tools\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install required Python packages for the project. Requires torch>=1.10,<2.0.0 and python>=3.7.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing Few-Shot Relation Extraction Modules in Python\nDESCRIPTION: This snippet shows the structure of the few-shot relation extraction package in DeepKE. It includes references to dataset and model modules, as well as specific modules for k-shot generation and label word retrieval.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.few_shot.rst#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom deepke.relation_extraction.few_shot import dataset\nfrom deepke.relation_extraction.few_shot import lit_models\nfrom deepke.relation_extraction.few_shot import generate_k_shot\nfrom deepke.relation_extraction.few_shot import get_label_word\n```\n\n----------------------------------------\n\nTITLE: CodeKGC Output Example for Relation Extraction\nDESCRIPTION: The output from the code language model that identifies a work relationship between a person and an organization. This demonstrates how CodeKGC extracts structured triples from unstructured text.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CodeKGC/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nextract = Extract([Triple(person('Michael D. Papagiannis'), Rel('Work for'), organization('Boston University')),])\n```\n\n----------------------------------------\n\nTITLE: Documenting Dataset Module for Multimodal Relation Extraction in Python\nDESCRIPTION: This snippet uses Sphinx autodoc to generate documentation for the dataset module in the multimodal relation extraction component of DeepKE. It includes all members, undocumented members, and shows inheritance.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.multimodal.modules.rst#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: deepke.relation_extraction.multimodal.modules.dataset\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Documenting Train Module for Multimodal Relation Extraction in Python\nDESCRIPTION: This snippet uses Sphinx autodoc to generate documentation for the train module in the multimodal relation extraction component of DeepKE. It includes all members, undocumented members, and shows inheritance.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.multimodal.modules.rst#2025-04-07_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: deepke.relation_extraction.multimodal.modules.train\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Execution Command in Bash\nDESCRIPTION: Command line instruction for running the distant supervision labeling tool with specified language, source file, and triple file parameters.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ds_label_data.py --language en --source_file source_data.json --triple_file triple_file.csv\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure for Multimodal Relation Extraction in reStructuredText\nDESCRIPTION: This code snippet sets up the documentation structure for the multimodal relation extraction component of DeepKE. It uses a toctree directive to include documentation for models and modules.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.multimodal.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nMultimodal\n==========\n\n\n.. toctree::\n   :maxdepth: 4\n\n   deepke.relation_extraction.multimodal.models\n   deepke.relation_extraction.multimodal.modules\n```\n\n----------------------------------------\n\nTITLE: Defining Source File Structure in JSON\nDESCRIPTION: Example of the required JSON structure for the source file, including sentence, head entity, tail entity, and their offsets.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"sentence\": \"如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈\",\n    \"head\": \"周星驰\",\n    \"tail\": \"喜剧之王\",\n    \"head_offset\": \"26\",\n    \"tail_offset\": \"21\",\n    //...\n\t},\n  //...\n]\n```\n\n----------------------------------------\n\nTITLE: Defining DeepKE Documentation Structure with Sphinx toctree\nDESCRIPTION: This snippet uses Sphinx's toctree directive to define the structure of the DeepKE project documentation. It specifies a maximum depth of 5 and includes three main modules of the project.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 5\n\n   deepke.attribution_extraction\n   deepke.name_entity_re\n   deepke.relation_extraction\n```\n\n----------------------------------------\n\nTITLE: Structuring Output Files in JSON\nDESCRIPTION: Example of the JSON structure for output files (labeled_train.json, labeled_dev.json, labeled_test.json) including sentence, entities, offsets, and relation.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/re/prepare-data/README_CN.md#2025-04-07_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n\t{\n    \"sentence\": \"如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈\",\n    \"head\": \"周星驰\",\n    \"tail\": \"喜剧之王\",\n    \"head_offset\": \"26\",\n    \"tail_offset\": \"21\",\n    \"relation\": \"主演\",\n    //...\n\t},\n  //...\n]\n```\n\n----------------------------------------\n\nTITLE: Standard NER Module Setup and Execution\nDESCRIPTION: Commands for setting up and running the standard NER module which uses BERT pretrained model. Includes data download, extraction, training and prediction steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd example/ner/standard\nwget 120.27.214.45/Data/ner/standard/data.tar.gz\ntar -xzvf data.tar.gz\npython run.py\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Importing DeepKE Named Entity Recognition Dataset Module\nDESCRIPTION: This snippet shows how to import the dataset module from the DeepKE named entity recognition package. The module likely contains functions and classes for handling datasets in named entity recognition tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.name_entity_re.standard.tools.rst#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom deepke.name_entity_re.standard.tools import dataset\n```\n\n----------------------------------------\n\nTITLE: Few-shot RE Module Setup and Execution\nDESCRIPTION: Commands for implementing few-shot relation extraction in low-resource scenarios. Includes data setup and model training steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd example/re/few-shot\nwget 120.27.214.45/Data/re/few_shot/data.tar.gz\ntar -xzvf data.tar.gz\npython run.py\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Importing DeepKE Named Entity Recognition Preprocess Module\nDESCRIPTION: This snippet demonstrates the import of the preprocess module from the DeepKE named entity recognition package. The module probably includes functions for preprocessing data before named entity recognition tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.name_entity_re.standard.tools.rst#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom deepke.name_entity_re.standard.tools import preprocess\n```\n\n----------------------------------------\n\nTITLE: Document RE Module Setup and Execution\nDESCRIPTION: Commands for implementing document-level relation extraction. Includes data preparation and model execution steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd example/re/document\nwget 120.27.214.45/Data/re/document/data.tar.gz\ntar -xzvf data.tar.gz\npython run.py\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: Sphinx documentation structure defining the organization of multimodal model documentation in DeepKE project\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.multimodal.models.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.relation_extraction.multimodal.models.clip\n\n\ndeepke.relation\\_extraction.multimodal.models.IFA\\_model module\n---------------------------------------------------------------\n\n.. automodule:: deepke.relation_extraction.multimodal.models.IFA_model\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\ndeepke.relation\\_extraction.multimodal.models.modeling\\_IFA module\n------------------------------------------------------------------\n\n.. automodule:: deepke.relation_extraction.multimodal.models.modeling_IFA\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Multimodal RE Module Setup and Execution\nDESCRIPTION: Commands for implementing multimodal relation extraction using both textual and visual data. Includes setup and execution steps.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd example/re/multimodal\nwget 120.27.214.45/Data/re/multimodal/data.tar.gz\ntar -xzvf data.tar.gz\npython run.py\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Documenting Metrics Module for Multimodal Relation Extraction in Python\nDESCRIPTION: This snippet uses Sphinx autodoc to generate documentation for the metrics module in the multimodal relation extraction component of DeepKE. It includes all members, undocumented members, and shows inheritance.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.multimodal.modules.rst#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: deepke.relation_extraction.multimodal.modules.metrics\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Predicting with Trained Attribute Extraction Model in Python\nDESCRIPTION: This code demonstrates how to use the trained model for prediction using a Python script.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Defining Relation Extraction Documentation Structure in reStructuredText\nDESCRIPTION: This snippet uses reStructuredText to define the structure of the Relation Extraction documentation. It creates a table of contents that links to different submodules of relation extraction within the DeepKE project.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nRelation Extraction\n===================\n\n\n.. toctree::\n   :maxdepth: 4\n\n   deepke.relation_extraction.document\n   deepke.relation_extraction.few_shot\n   deepke.relation_extraction.standard\n   deepke.relation_extraction.multimodal\n```\n\n----------------------------------------\n\nTITLE: Downloading Data and Running Attribute Extraction Model with Bash\nDESCRIPTION: This bash script changes to the example directory, downloads the required data, extracts it, and runs the training and prediction scripts.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/example.rst#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncd example/ae/regular\n\nwget 120.27.214.45/Data/ae/standard/data.tar.gz\n\ntar -xzvf data.tar.gz\n\npython run.py\n\npython predict.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Structure with toctree in reStructuredText\nDESCRIPTION: This snippet defines a table of contents tree (toctree) directive that organizes documentation for the standard relation extraction module in DeepKE. It sets the maximum depth to 4 and lists the key documentation files for models, modules, tools, and utilities.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/deepke.relation_extraction.standard.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke.relation_extraction.standard.models\n   deepke.relation_extraction.standard.module\n   deepke.relation_extraction.standard.tools\n   deepke.relation_extraction.standard.utils\n```\n\n----------------------------------------\n\nTITLE: Building DeepKE Docker Image\nDESCRIPTION: Commands to navigate to the docker directory and build a Docker image for DeepKE.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/install.rst#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ndocker build -t deepke .\nconda activate deepke\n```\n\n----------------------------------------\n\nTITLE: Citation for DeepKE Paper in BibTeX Format\nDESCRIPTION: BibTeX entry for citing the DeepKE paper in academic publications. The citation includes author information, title, journal, and publication year.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/index.rst#2025-04-07_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{Zhang_DeepKE_A_Deep_2022,\n   author = {Zhang, Ningyu and Xu, Xin and Tao, Liankuan and Yu, Haiyang and Ye, Hongbin and Xie, Xin and Chen, Xiang and Li, Zhoubo and Li, Lei and Liang, Xiaozhuan and Yao, Yunzhi and Deng, Shumin and Zhang, Zhenru and Tan, Chuanqi and Huang, Fei and Zheng, Guozhou and Chen, Huajun},\n   journal = {http://arxiv.org/abs/2201.03335},\n   title = {{DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population}},\n   year = {2022}\n   }\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Structure for DeepKE\nDESCRIPTION: A reStructuredText directive that creates a table of contents for the DeepKE project, specifically for the src directory. It sets the maximum depth to 4 levels and includes the deepke module in the documentation tree.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/docs/source/modules.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n\n   deepke\n```\n\n----------------------------------------\n\nTITLE: Special Tokens and Markup Elements List\nDESCRIPTION: A comprehensive list of special tokens and markup elements used for text processing, including padding tokens (<pad>), unknown tokens (<unk>), mask tokens (<mask>), separator tokens (<sep>), line breaks (</n>), underscores (</_>), sentence markers (<s>, </s>), document structure tags (<root>, <ans>, <option>), and basic punctuation (!, \")\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n<pad>\n<unk>\n<mask>\n<sep>\n</n>\n</_>\n<s>\n</s>\n<root>\n<ans>\n<option>\n!\n\"\n```\n\n----------------------------------------\n\nTITLE: Exported JSON Data Structure for Named Entity Recognition and Relation Extraction\nDESCRIPTION: Example of the JSON format used for exporting annotated data with entity and relation information. The structure contains fields for sample ID, raw text, entity spans (with offsets and labels), and relations between entities.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\":13,\n    \"text\":\"The collision resulted in two more crashes in the intersection, including a central concrete truck that was about to turn left onto college ave. *collision*crashes**\",\n    \"entities\":[\n        {\n            \"id\":20,\n            \"label\":\"MISC\",\n            \"start_offset\":4,\n            \"end_offset\":13\n        },\n        {\n            \"id\":21,\n            \"label\":\"MISC\",\n            \"start_offset\":35,\n            \"end_offset\":42\n        }\n    ],\n    \"relations\":[\n        {\n            \"id\":2,\n            \"from_id\":20,\n            \"to_id\":21,\n            \"type\":\"Cause-Effect\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Sudo Command in Shell\nDESCRIPTION: A command used in Unix/Linux systems to execute a command with administrative privileges. This allows users to perform operations that require elevated permissions.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo\n```\n\n----------------------------------------\n\nTITLE: Downloading Conll Dataset for Entity Recognition\nDESCRIPTION: Shell command for downloading the Conll dataset which contains named entities related to persons, locations, and other categories for English entity recognition tasks.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget 120.27.214.45/Data/ner/few_shot/data.tar.gz\n```\n\n----------------------------------------\n\nTITLE: JavaScript Object Opening with Double Braces\nDESCRIPTION: A code fragment showing double opening braces which could be used in a JavaScript template literal or in some front-end templating systems like Handlebars or Mustache.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{{\n```\n\n----------------------------------------\n\nTITLE: Running Chinese Automatic Entity Recognition Annotation\nDESCRIPTION: Command to run the weak-supervised data preparation script for Chinese language entity recognition using a predefined Chinese vocabulary dictionary.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython prepare_weaksupervised_data.py --language cn --dict_dir vocab_dict_cn.csv\n```\n\n----------------------------------------\n\nTITLE: File Path Reference with Tilde\nDESCRIPTION: A shorthand notation for referring to the home directory in Unix-based systems. The tilde (~) followed by a slash represents the current user's home directory path.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n~/\n```\n\n----------------------------------------\n\nTITLE: Running English Automatic Entity Recognition Annotation\nDESCRIPTION: Command to run the weak-supervised data preparation script for English language entity recognition using a predefined English vocabulary dictionary.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython prepare_weaksupervised_data.py --language en --dict_dir vocab_dict_en.csv\n```\n\n----------------------------------------\n\nTITLE: JavaScript Less Than Operator with HTML Tag Context\nDESCRIPTION: A code fragment showing a less than operator followed by a quote, which might be part of an HTML tag parsing operation or string manipulation in JavaScript.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n\"<\n```\n\n----------------------------------------\n\nTITLE: Relation Extraction Source File Format for Distant Supervision\nDESCRIPTION: Example JSON format for the source file used in distant supervised relation extraction. Each entry contains a sentence, head and tail entities, and their offsets for automatic relation labeling.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"sentence\": \"This summer, the United States Embassy in Beirut, Lebanon, once again made its presence felt on the cultural scene by sponsoring a photo exhibition, an experimental jazz performance, a classical music concert and a visit from the Whiffenpoofs, Yale University's a cappella singers.\",\n    \"head\": \"Lebanon\",\n    \"tail\": \"Beirut\",\n    \"head_offset\": \"50\",\n    \"tail_offset\": \"42\",\n    //...\n  },\n  //... \n]\n```\n\n----------------------------------------\n\nTITLE: Data Format Conversion Functions Reference - Python\nDESCRIPTION: Reference to transformation functions (json2txt and doc2txt) located in transform_data.py for converting JSON and DOCX files to TXT format for processing.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/ner/standard/data/README_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom deepke.transform_data import json2txt, doc2txt\n```\n\n----------------------------------------\n\nTITLE: Chinese Relation Types - JSON Format\nDESCRIPTION: JSON formatted relation types for Chinese dataset showing object type, predicate, and subject type mappings\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG.md#2025-04-07_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"object_type\": \"地点\", \"predicate\": \"祖籍\", \"subject_type\": \"人物\"}\n{\"object_type\": \"人物\", \"predicate\": \"父亲\", \"subject_type\": \"人物\"}\n{\"object_type\": \"地点\", \"predicate\": \"总部地点\", \"subject_type\": \"企业\"}\n```\n\n----------------------------------------\n\nTITLE: Required BERT Model Files Structure\nDESCRIPTION: Lists the three essential files required for BERT model implementation: configuration file, model parameters, and vocabulary file.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/pretrained/readme.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- config.json `Configuration file of BERT model structure`\n- pytorch_model.bin `Pretraining model parameters`\n- vocab.txt `BERT vocabulary`\n```\n\n----------------------------------------\n\nTITLE: Creating Entity Recognition Data in JSON Lines Format\nDESCRIPTION: Example JSON structure showing manually annotated entity data export from doccano, including entity spans with locations and labels.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\":8,\n    \"text\":\"曾经去过深圳、珠海的俭汤青年农民回乡后意味深长地说，从南往北走，越走越保守。\",\n    \"entities\":[\n        {\n            \"id\":9,\n            \"label\":\"LOC\",\n            \"start_offset\":4,\n            \"end_offset\":6\n        },\n        {\n            \"id\":10,\n            \"label\":\"LOC\",\n            \"start_offset\":7,\n            \"end_offset\":9\n        },\n        {\n            \"id\":11,\n            \"label\":\"PER\",\n            \"start_offset\":12,\n            \"end_offset\":16\n        }\n    ],\n    \"relations\":[\n\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Relation Extraction Data in JSON Lines Format\nDESCRIPTION: Example JSON structure showing manually annotated relation data export from doccano, including entities and their relationships.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/README_TAG_CN.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\":12,\n    \"text\":\"张廷智， 毕业于大连军医学院，从事中医治疗中晚期肿瘤临床工作30余年*张延智*大连军医学院*人物*学校\",\n    \"entities\":[\n        {\n            \"id\":18,\n            \"label\":\"PER\",\n            \"start_offset\":0,\n            \"end_offset\":3\n        },\n        {\n            \"id\":19,\n            \"label\":\"ORG\",\n            \"start_offset\":8,\n            \"end_offset\":14\n        }\n    ],\n    \"relations\":[\n        {\n            \"id\":1,\n            \"from_id\":18,\n            \"to_id\":19,\n            \"type\":\"毕业\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a JavaScript Console Statement\nDESCRIPTION: A simple console logging operation in JavaScript. This is likely part of a debugging or logging functionality.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconsole\n```\n\n----------------------------------------\n\nTITLE: Using JavaScript Object Closing Syntax\nDESCRIPTION: A code fragment showing the closing syntax for a JavaScript object. It includes a closing curly brace and semicolon, which indicates the end of an object definition.\nSOURCE: https://github.com/zjunlp/DeepKE/blob/main/example/llm/CPM-Bee/src/cpm_live/vocabs/bee.txt#2025-04-07_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n});\n```"
  }
]
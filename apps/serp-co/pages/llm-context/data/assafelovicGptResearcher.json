[
  {
    "owner": "assafelovic",
    "repo": "gpt-researcher",
    "content": "TITLE: Constructing the Multi-Agent Research Workflow Graph\nDESCRIPTION: This function initializes all specialized agent skills, creates a LangGraph StateGraph, adds nodes for each agent, establishes the connections between them through edges, and sets the entry point for the workflow. It creates a complete research pipeline from initial browsing to final publication.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-05-19-gptr-langgraph/index.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef init_research_team(self):\n    # Initialize skills\n    editor_agent = EditorAgent(self.task)\n    research_agent = ResearchAgent()\n    writer_agent = WriterAgent()\n    publisher_agent = PublisherAgent(self.output_dir)\n    \n    # Define a Langchain StateGraph with the ResearchState\n    workflow = StateGraph(ResearchState)\n    \n    # Add nodes for each agent\n    workflow.add_node(\"browser\", research_agent.run_initial_research)\n    workflow.add_node(\"planner\", editor_agent.plan_research)\n    workflow.add_node(\"researcher\", editor_agent.run_parallel_research)\n    workflow.add_node(\"writer\", writer_agent.run)\n    workflow.add_node(\"publisher\", publisher_agent.run)\n    \n    workflow.add_edge('browser', 'planner')\n    workflow.add_edge('planner', 'researcher')\n    workflow.add_edge('researcher', 'writer')\n    workflow.add_edge('writer', 'publisher')\n    \n    # set up start and end nodes\n    workflow.set_entry_point(\"browser\")\n    workflow.add_edge('publisher', END)\n    \n    return workflow\n```\n\n----------------------------------------\n\nTITLE: Implementing GPT Researcher Agent in Python for Automated Research Reports\nDESCRIPTION: This code demonstrates how to import and use the GPT Researcher agent to generate research reports asynchronously. It includes two functions: one to fetch the report using the researcher agent and another to execute the research process with a specific query. The example is set up to research information about the Burning Man floods.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/example.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def fetch_report(query):\n    \"\"\"\n    Fetch a research report based on the provided query and report type.\n    \"\"\"\n    researcher = GPTResearcher(query=query)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n\nasync def generate_research_report(query):\n    \"\"\"\n    This is a sample script that executes an async main function to run a research report.\n    \"\"\"\n    report = await fetch_report(query)\n    print(report)\n\nif __name__ == \"__main__\":\n    QUERY = \"What happened in the latest burning man floods?\"\n    asyncio.run(generate_research_report(query=QUERY))\n```\n\n----------------------------------------\n\nTITLE: Audience Targeting with Custom Prompts in GPT Researcher\nDESCRIPTION: Example of using a custom prompt to tailor the research report for a specific audience, focusing on technical stakeholders with emphasis on methodologies and implementation details.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nreport = await researcher.write_report(\n    custom_prompt=\"Create a report for technical stakeholders, focusing on methodologies and implementation details.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Basic GPT Researcher Usage Example in Python\nDESCRIPTION: Complete example showing how to use the GPT Researcher to conduct research on a query and generate a report. Includes retrieving research context, costs, images, and sources.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(query: str, report_type: str):\n    researcher = GPTResearcher(query, report_type)\n    research_result = await researcher.conduct_research()\n    report = await researcher.write_report()\n    \n    # Get additional information\n    research_context = researcher.get_research_context()\n    research_costs = researcher.get_costs()\n    research_images = researcher.get_research_images()\n    research_sources = researcher.get_research_sources()\n    \n    return report, research_context, research_costs, research_images, research_sources\n\nif __name__ == \"__main__\":\n    query = \"what team may win the NBA finals?\"\n    report_type = \"research_report\"\n\n    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))\n    \n    print(\"Report:\")\n    print(report)\n    print(\"\\nResearch Costs:\")\n    print(costs)\n    print(\"\\nNumber of Research Images:\")\n    print(len(images))\n    print(\"\\nNumber of Research Sources:\")\n    print(len(sources))\n```\n\n----------------------------------------\n\nTITLE: Advanced Report Generation with GPT Researcher\nDESCRIPTION: Comprehensive example showing various ways to handle research results and generate different types of reports with GPT Researcher, including standard reports, custom reports, and specialized outputs.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Conduct research\nresearch_result = await researcher.conduct_research()\n\n# Generate a standard report\nreport = await researcher.write_report()\n\n# Generate a customized report with specific formatting requirements\ncustom_report = await researcher.write_report(custom_prompt=\"Answer in short, 2 paragraphs max without citations.\")\n\n# Generate a focused report for a specific audience\nexecutive_summary = await researcher.write_report(custom_prompt=\"Create an executive summary focused on business impact and ROI. Keep it under 500 words.\")\n\n# Generate a report with specific structure requirements\ntechnical_report = await researcher.write_report(custom_prompt=\"Create a technical report with problem statement, methodology, findings, and recommendations sections.\")\n\n# Generate a conclusion\nconclusion = await researcher.write_report_conclusion(report)\n\n# Get subtopics\nsubtopics = await researcher.get_subtopics()\n\n# Get draft section titles for a subtopic\ndraft_titles = await researcher.get_draft_section_titles(\"Subtopic name\")\n```\n\n----------------------------------------\n\nTITLE: Working with Research Context in GPT Researcher\nDESCRIPTION: Example showing how to retrieve and use the research context for further processing or analysis, including getting similar written contents based on draft section titles.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Get the full research context\ncontext = researcher.get_research_context()\n\n# Get similar written contents based on draft section titles\nsimilar_contents = await researcher.get_similar_written_contents_by_draft_section_titles(\n    current_subtopic=\"Subtopic name\",\n    draft_section_titles=[\"Title 1\", \"Title 2\"],\n    written_contents=some_written_contents,\n    max_results=10\n)\n```\n\n----------------------------------------\n\nTITLE: Specialized Output Generation with Custom Prompts in GPT Researcher\nDESCRIPTION: Example of using a custom prompt to generate a specialized type of content, specifically a FAQ section based on the research with at least 5 questions and detailed answers.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nreport = await researcher.write_report(\n    custom_prompt=\"Create a FAQ section based on the research with at least 5 questions and detailed answers.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Implementation of an OpenAI Assistant with Internet Access\nDESCRIPTION: The full implementation of a finance assistant using OpenAI's Assistants API with Tavily Search integration. This code provides a complete solution that initializes the assistant, creates threads, processes user inputs, handles function calls, and displays responses.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport time\nfrom openai import OpenAI\nfrom tavily import TavilyClient\n\n# Initialize clients with API keys\nclient = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\nassistant_prompt_instruction = \"\"\"You are a finance expert. \nYour goal is to provide answers based on information from the internet. \nYou must use the provided Tavily search API function to find relevant online information. \nYou should never use your own knowledge to answer questions.\nPlease include relevant url sources in the end of your answers.\n\"\"\"\n\n# Function to perform a Tavily search\ndef tavily_search(query):\n    search_result = tavily_client.get_search_context(query, search_depth=\"advanced\", max_tokens=8000)\n    return search_result\n\n# Function to wait for a run to complete\ndef wait_for_run_completion(thread_id, run_id):\n    while True:\n        time.sleep(1)\n        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n        print(f\"Current run status: {run.status}\")\n        if run.status in ['completed', 'failed', 'requires_action']:\n            return run\n\n# Function to handle tool output submission\ndef submit_tool_outputs(thread_id, run_id, tools_to_call):\n    tool_output_array = []\n    for tool in tools_to_call:\n        output = None\n        tool_call_id = tool.id\n        function_name = tool.function.name\n        function_args = tool.function.arguments\n\n        if function_name == \"tavily_search\":\n            output = tavily_search(query=json.loads(function_args)[\"query\"])\n\n        if output:\n            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n\n    return client.beta.threads.runs.submit_tool_outputs(\n        thread_id=thread_id,\n        run_id=run_id,\n        tool_outputs=tool_output_array\n    )\n\n# Function to print messages from a thread\ndef print_messages_from_thread(thread_id):\n    messages = client.beta.threads.messages.list(thread_id=thread_id)\n    for msg in messages:\n        print(f\"{msg.role}: {msg.content[0].text.value}\")\n\n# Create an assistant\nassistant = client.beta.assistants.create(\n    instructions=assistant_prompt_instruction,\n    model=\"gpt-4-1106-preview\",\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"tavily_search\",\n            \"description\": \"Get information on recent events from the web.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"The search query to use. For example: 'Latest news on Nvidia stock performance'\"},\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }]\n)\nassistant_id = assistant.id\nprint(f\"Assistant ID: {assistant_id}\")\n\n# Create a thread\nthread = client.beta.threads.create()\nprint(f\"Thread: {thread}\")\n```\n\n----------------------------------------\n\nTITLE: Researcher Configuration with Config File - Python\nDESCRIPTION: Shows how to initialize the researcher with a configuration file path for deep research parameters.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/deep_research.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresearcher = GPTResearcher(\n    query=\"your query\",\n    report_type=\"deep\",\n    config_path=\"path/to/config.yaml\"  # Configure deep research parameters here\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Search Queries for Deep Research in Python\nDESCRIPTION: This function generates a set of diverse search queries based on an initial question. It uses GPT to create targeted queries, each with a specific research goal.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nasync def generate_search_queries(self, query: str, num_queries: int = 3) -> List[Dict[str, str]]:\n    \"\"\"Generate SERP queries for research\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert researcher generating search queries.\"},\n        {\"role\": \"user\",\n         \"content\": f\"Given the following prompt, generate {num_queries} unique search queries to research the topic thoroughly. For each query, provide a research goal. Format as 'Query: <query>' followed by 'Goal: <goal>' for each pair: {query}\"}\n    ]\n```\n\n----------------------------------------\n\nTITLE: Conducting Research with GPT Researcher\nDESCRIPTION: Demonstrates how to use the GPT Researcher library to conduct research on a given query. The example creates an async function that initializes the researcher, conducts research, generates a report, and extracts additional information like costs and sources.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio # required for notebooks\nnest_asyncio.apply()\n\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(query: str, report_type: str) -> str:\n    researcher = GPTResearcher(query, report_type)\n    research_result = await researcher.conduct_research()\n    report = await researcher.write_report()\n    \n    # Get additional information\n    research_context = researcher.get_research_context()\n    research_costs = researcher.get_costs()\n    research_images = researcher.get_research_images()\n    research_sources = researcher.get_research_sources()\n    \n    return report, research_context, research_costs, research_images, research_sources\n\nif __name__ == \"__main__\":\n    query = \"Should I invest in Nvidia?\"\n    report_type = \"research_report\"\n\n    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))\n    \n    print(\"Report:\")\n    print(report)\n    print(\"\\nResearch Costs:\")\n    print(costs)\n    print(\"\\nResearch Images:\")\n    print(images)\n    print(\"\\nResearch Sources:\")\n    print(sources)\n```\n\n----------------------------------------\n\nTITLE: Integrating Faiss Vector Store with GPT Researcher in Python\nDESCRIPTION: This snippet demonstrates how to create a Faiss vector store from a text document, split it into chunks, and use it with GPT Researcher for conducting research and generating a report. It uses OpenAI embeddings and the CharacterTextSplitter for text processing.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/vector-stores.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\n\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\n# exerpt taken from - https://paulgraham.com/wealth.html\nessay = \"\"\"\nMay 2004\n\n(This essay was originally published in Hackers & Painters.)\n\nIf you wanted to get rich, how would you do it? I think your best bet would be to start or join a startup.\nThat's been a reliable way to get rich for hundreds of years. The word \"startup\" dates from the 1960s,\nbut what happens in one is very similar to the venture-backed trading voyages of the Middle Ages.\n\nStartups usually involve technology, so much so that the phrase \"high-tech startup\" is almost redundant.\nA startup is a small company that takes on a hard technical problem.\n\nLots of people get rich knowing nothing more than that. You don't have to know physics to be a good pitcher.\nBut I think it could give you an edge to understand the underlying principles. Why do startups have to be small?\nWill a startup inevitably stop being a startup as it grows larger?\nAnd why do they so often work on developing new technology? Why are there so many startups selling new drugs or computer software,\nand none selling corn oil or laundry detergent?\n\n\nThe Proposition\n\nEconomically, you can think of a startup as a way to compress your whole working life into a few years.\nInstead of working at a low intensity for forty years, you work as hard as you possibly can for four.\nThis pays especially well in technology, where you earn a premium for working fast.\n\nHere is a brief sketch of the economic proposition. If you're a good hacker in your mid twenties,\nyou can get a job paying about $80,000 per year. So on average such a hacker must be able to do at\nleast $80,000 worth of work per year for the company just to break even. You could probably work twice\nas many hours as a corporate employee, and if you focus you can probably get three times as much done in an hour.[1]\nYou should get another multiple of two, at least, by eliminating the drag of the pointy-haired middle manager who\nwould be your boss in a big company. Then there is one more multiple: how much smarter are you than your job\ndescription expects you to be? Suppose another multiple of three. Combine all these multipliers,\nand I'm claiming you could be 36 times more productive than you're expected to be in a random corporate job.[2]\nIf a fairly good hacker is worth $80,000 a year at a big company, then a smart hacker working very hard without \nany corporate bullshit to slow him down should be able to do work worth about $3 million a year.\n...\n...\n...\n\"\"\"\n\ndocument = [Document(page_content=essay)]\ntext_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=30, separator=\"\\n\")\ndocs = text_splitter.split_documents(documents=document)\n\nvector_store = FAISS.from_documents(documents, OpenAIEmbeddings())\n\nquery = \"\"\"\n    Summarize the essay into 3 or 4 succinct sections.\n    Make sure to include key points regarding wealth creation.\n\n    Include some recommendations for entrepreneurs in the conclusion.\n\"\"\"\n\n\n# Create an instance of GPTResearcher\nresearcher = GPTResearcher(\n    query=query,\n    report_type=\"research_report\",\n    report_source=\"langchain_vectorstore\",\n    vector_store=vector_store,\n)\n\n# Conduct research and write the report\nawait researcher.conduct_research()\nreport = await researcher.write_report()\n```\n\n----------------------------------------\n\nTITLE: Summarizing Text with OpenAI API in Python\nDESCRIPTION: Summarizes text from a URL using the OpenAI API. Takes a URL, text content, a question to answer, and optionally a WebDriver for scrolling the page. Returns a summary of the text relevant to the question.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/processing/text.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef summarize_text(url: str,\n                   text: str,\n                   question: str,\n                   driver: Optional[WebDriver] = None) -> str\n```\n\n----------------------------------------\n\nTITLE: Generating Research Report with Tavily and GPT-4\nDESCRIPTION: Combines Tavily search results with GPT-4 using LangChain to generate a comprehensive research report. The code performs a search, formats the results into a prompt, and generates a structured report in MLA format using markdown syntax.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/examples.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# libraries\nfrom langchain.adapters.openai import convert_openai_messages\nfrom langchain_community.chat_models import ChatOpenAI\n\n# setup query\nquery = \"What happend in the latest burning man floods?\"\n\n# run tavily search\ncontent = client.search(query, search_depth=\"advanced\")[\"results\"]\n\n# setup prompt\nprompt = [{\n    \"role\": \"system\",\n    \"content\":  f'You are an AI critical thinker research assistant. '\\\n                f'Your sole purpose is to write well written, critically acclaimed,'\\\n                f'objective and structured reports on given text.'\n}, {\n    \"role\": \"user\",\n    \"content\": f'Information: \"\"\"{content}\"\"\"\\\n\\\n' \\\n               f'Using the above information, answer the following'\\\n               f'query: \"{query}\" in a detailed report --'\\\n               f'Please use MLA format and markdown syntax.'\n}]\n\n# run gpt-4\nlc_messages = convert_openai_messages(prompt)\nreport = ChatOpenAI(model='gpt-4',openai_api_key=openai_api_key).invoke(lc_messages).content\n\n# print report\nprint(report)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Research Agent with GPT Researcher\nDESCRIPTION: This code defines a Research Agent class that utilizes the GPT Researcher package to conduct research on a given query. The agent initializes the researcher, conducts the research, and returns the generated report.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-05-19-gptr-langgraph/index.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\n\nclass ResearchAgent:\n    def __init__(self):\n        pass\n  \n    async def research(self, query: str):\n        # Initialize the researcher\n        researcher = GPTResearcher(parent_query=parent_query, query=query, report_type=research_report, config_path=None)\n        # Conduct research on the given query\n        await researcher.conduct_research()\n        # Write the report\n        report = await researcher.write_report()\n  \n        return report\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running GPT Researcher for Deep Research in Python\nDESCRIPTION: This code snippet demonstrates how to initialize the GPTResearcher class with a deep research query, conduct research, and generate a report. It uses asyncio for asynchronous execution.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def main():\n    # Initialize researcher with deep research type\n    researcher = GPTResearcher(\n        query=\"What are the latest developments in quantum computing?\",\n        report_type=\"deep\",  # This triggers deep research mode\n    )\n    \n    # Run research\n    research_data = await researcher.conduct_research()\n    \n    # Generate report\n    report = await researcher.write_report()\n    print(report)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using Vector Store with GPT-Researcher in Python\nDESCRIPTION: Code example showing how to initialize the GPT-Researcher with a PostgreSQL vector store for research report generation. This demonstrates creating an asynchronous connection to the database and configuring the researcher with the vector store.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/data-ingestion.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync_connection_string = pgvector_connection_string.replace(\"postgresql://\", \"postgresql+psycopg://\")\n\n# Initialize the async engine with the psycopg3 driver\nasync_engine = create_async_engine(\n    async_connection_string,\n    echo=True\n)\n\nasync_vector_store = PGVector(\n    embeddings=embeddings,\n    collection_name=collection_name,\n    connection=async_engine,\n    use_jsonb=True\n)\n\n\nresearcher = GPTResearcher(\n    query=query,\n    report_type=\"research_report\",\n    report_source=\"langchain_vectorstore\",\n    vector_store=async_vector_store,\n)\nawait researcher.conduct_research()\nreport = await researcher.write_report()\n```\n\n----------------------------------------\n\nTITLE: Managing Context for Deep Research in Python\nDESCRIPTION: This code snippet demonstrates how the system manages and trims the gathered information to stay within word limits while retaining the most relevant content.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Trim context to stay within word limits\ntrimmed_context = trim_context_to_word_limit(all_context)\nlogger.info(f\"Trimmed context from {len(all_context)} items to {len(trimmed_context)} items to stay within word limit\")\n```\n\n----------------------------------------\n\nTITLE: Researching Specific URLs with GPT Researcher in Python\nDESCRIPTION: Example of using GPT Researcher to conduct research on specific provided URLs. Allows controlling whether to complement with additional sources using the complement_source_urls parameter.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/tailored-research.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(query: str, report_type: str, sources: list) -> str:\n    researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources, complement_source_urls=False)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n\nif __name__ == \"__main__\":\n    query = \"What are the biggest trends in AI lately?\"\n    report_source = \"static\"\n    sources = [\n        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n        \"https://www.ibm.com/think/insights/artificial-intelligence-trends\",\n        \"https://www.forbes.com/advisor/business/ai-statistics\"\n    ]\n    report = asyncio.run(get_report(query=query, report_source=report_source, sources=sources))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Using GPT Researcher Python Package\nDESCRIPTION: Example Python code showing how to import and use the GPT Researcher package for conducting research and generating reports.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\n\nquery = \"why is Nvidia stock going up?\"\nresearcher = GPTResearcher(query=query, report_type=\"research_report\")\n# Conduct research on the given query\nresearch_result = await researcher.conduct_research()\n# Write the report\nreport = await researcher.write_report()\n```\n\n----------------------------------------\n\nTITLE: Adding Scraped Data to InMemoryVectorStore with GPT Researcher in Python\nDESCRIPTION: This snippet demonstrates how to use GPT Researcher to scrape web data and store it in an InMemoryVectorStore. It shows the process of conducting research, storing the context in the vector store, and then querying the stored data for relevant information.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/vector-stores.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\n\nfrom langchain_community.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n\nquery = \"The best LLM\"\n\n# Create an instance of GPTResearcher\nresearcher = GPTResearcher(\n    query=query,\n    report_type=\"research_report\",\n    report_source=\"web\",\n    vector_store=vector_store, \n)\n\n# Conduct research, the context will be chunked and stored in the vector_store\nawait researcher.conduct_research()\n\n# Query the 5 most relevant context in our vector store\nrelated_contexts = await vector_store.asimilarity_search(\"GPT-4\", k = 5) \nprint(related_contexts)\nprint(len(related_contexts)) #Should be 5 \n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Research Workflow with LangGraph StateGraph\nDESCRIPTION: Creates a LangGraph StateGraph with conditional edges to implement a circular review process for research drafts. The workflow connects researcher, reviewer, and reviser nodes, with conditional branching based on review status.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-05-19-gptr-langgraph/index.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def run_parallel_research(self, research_state: dict):\n    workflow = StateGraph(DraftState)\n    \n    workflow.add_node(\"researcher\", research_agent.run_depth_research)\n    workflow.add_node(\"reviewer\", reviewer_agent.run)\n    workflow.add_node(\"reviser\", reviser_agent.run)\n    \n    # set up edges researcher->reviewer->reviser->reviewer...\n    workflow.set_entry_point(\"researcher\")\n    workflow.add_edge('researcher', 'reviewer')\n    workflow.add_edge('reviser', 'reviewer')\n    workflow.add_conditional_edges('reviewer',\n                                   (lambda draft: \"accept\" if draft['review'] is None else \"revise\"),\n                                   {\"accept\": END, \"revise\": \"reviser\"})\n```\n\n----------------------------------------\n\nTITLE: Customizing GPTResearcher Behavior in Python\nDESCRIPTION: This code snippet shows how to customize the behavior of the GPTResearcher instance used in evaluations. It demonstrates setting various parameters like report type, format, source, and tone.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/evals/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresearcher = GPTResearcher(\n    query=query,\n    report_type=ReportType.ResearchReport.value,  # Type of report to generate\n    report_format=\"markdown\",                      # Output format\n    report_source=ReportSource.Web.value,         # Source of research\n    tone=Tone.Objective,                          # Writing tone\n    verbose=True                                  # Enable detailed logging\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating GPT Researcher with Flask\nDESCRIPTION: Example showing how to integrate GPT Researcher into a Flask application with async support. Creates a route that accepts a query parameter and report type to generate and return research results.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom flask import Flask, request, jsonify\nfrom gpt_researcher import GPTResearcher\n\napp = Flask(__name__)\n\n@app.route('/report/<report_type>', methods=['GET'])\nasync def get_report(report_type):\n    query = request.args.get('query')\n    researcher = GPTResearcher(query, report_type)\n    research_result = await researcher.conduct_research()\n    report = await researcher.write_report()\n    \n    source_urls = researcher.get_source_urls()\n    research_costs = researcher.get_costs()\n    research_images = researcher.get_research_images()\n    research_sources = researcher.get_research_sources()\n    \n    return jsonify({\n        \"report\": report,\n        \"source_urls\": source_urls,\n        \"research_costs\": research_costs,\n        \"num_images\": len(research_images),\n        \"num_sources\": len(research_sources)\n    })\n\n# Run the server\n# flask run\n```\n\n----------------------------------------\n\nTITLE: Using GPT Researcher in Python Code\nDESCRIPTION: Example Python script demonstrating how to use GPT Researcher to generate a research report. It includes importing the necessary module, defining an async function to get the report, and running the research query.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\n\nasync def get_report(query: str, report_type: str) -> str:\n    researcher = GPTResearcher(query, report_type)\n    report = await researcher.run()\n    return report\n\nif __name__ == \"__main__\":\n    query = \"what team may win the NBA finals?\"\n    report_type = \"research_report\"\n\n    report = asyncio.run(get_report(query, report_type))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys as Environment Variables\nDESCRIPTION: Bash commands to export OpenAI and Tavily API keys as environment variables.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/hybrid_research.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_openai_api_key_here\nexport TAVILY_API_KEY=your_tavily_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Defining Research State Schema for LangGraph\nDESCRIPTION: This code defines a TypedDict class that represents the state structure for the research process in the LangGraph workflow. It includes fields for storing the research task, initial research data, sections, and various components of the final report structure.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-05-19-gptr-langgraph/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ResearchState(TypedDict):\n    task: dict\n    initial_research: str\n    sections: List[str]\n    research_data: List[dict]\n    # Report layout\n    title: str\n    headers: dict\n    date: str\n    table_of_contents: str\n    introduction: str\n    conclusion: str\n    sources: List[str]\n    report: str\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running GPT Researcher with Hybrid Configuration in Python\nDESCRIPTION: This code snippet demonstrates how to initialize GPT Researcher with hybrid research configuration and conduct research. It sets up an asynchronous function to get a research report based on a given query, report type, and report source.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-09-7-hybrid-research/index.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_research_report(query: str, report_type: str, report_source: str) -> str:\n    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source)\n    research = await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n    \nif __name__ == \"__main__\":\n    query = \"How does our product roadmap compare to emerging market trends in our industry?\"\n    report_source = \"hybrid\"\n\n    report = asyncio.run(get_research_report(query=query, report_type=\"research_report\", report_source=report_source))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Configuring Deep Research Parameters in GPT Researcher\nDESCRIPTION: This code snippet shows how to configure deep research parameters when initializing the GPT Researcher. It allows customization of research breadth, depth, and concurrency.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/backend/report_type/deep_research/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresearcher = GPTResearcher(\n    query=\"your query\",\n    report_type=\"deep\",\n    config_path=\"path/to/config.yaml\"  # Configure deep research parameters here\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT Researcher with Deep Research Mode - Python\nDESCRIPTION: Demonstrates how to initialize and run GPT Researcher in deep research mode using async/await pattern. Shows basic setup for conducting research and generating reports.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/deep_research.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nfrom gpt_researcher.utils.enum import ReportType, Tone\nimport asyncio\n\nasync def main():\n    # Initialize researcher with deep research type\n    researcher = GPTResearcher(\n        query=\"What are the latest developments in quantum computing?\",\n        report_type=\"deep\",  # This triggers deep research modd\n    )\n    \n    # Run research\n    research_data = await researcher.conduct_research()\n    \n    # Generate report\n    report = await researcher.write_report()\n    print(report)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Processing Local Documents with LangChain in Python\nDESCRIPTION: This snippet demonstrates how to load and process various document types using LangChain's document loaders, split text into chunks, create embeddings with OpenAI, and store them in a Chroma vector database for similarity search.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-09-7-hybrid-research/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.document_loaders import (\n    PyMuPDFLoader, \n    TextLoader, \n    UnstructuredCSVLoader, \n    UnstructuredExcelLoader,\n    UnstructuredMarkdownLoader, \n    UnstructuredPowerPointLoader,\n    UnstructuredWordDocumentLoader\n)\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\ndef load_local_documents(file_paths):\n    documents = []\n    for file_path in file_paths:\n        if file_path.endswith('.pdf'):\n            loader = PyMuPDFLoader(file_path)\n        elif file_path.endswith('.txt'):\n            loader = TextLoader(file_path)\n        elif file_path.endswith('.csv'):\n            loader = UnstructuredCSVLoader(file_path)\n        elif file_path.endswith('.xlsx'):\n            loader = UnstructuredExcelLoader(file_path)\n        elif file_path.endswith('.md'):\n            loader = UnstructuredMarkdownLoader(file_path)\n        elif file_path.endswith('.pptx'):\n            loader = UnstructuredPowerPointLoader(file_path)\n        elif file_path.endswith('.docx'):\n            loader = UnstructuredWordDocumentLoader(file_path)\n        else:\n            raise ValueError(f\"Unsupported file type: {file_path}\")\n        \n        documents.extend(loader.load())\n    \n    return documents\n\n# Use the function to load your local documents\nlocal_docs = load_local_documents(['company_report.pdf', 'meeting_notes.docx', 'data.csv'])\n\n# Split the documents into smaller chunks for more efficient processing\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(local_docs)\n\n# Create embeddings and store them in a vector database for quick retrieval\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n\n# Example of how to perform a similarity search\nquery = \"What were the key points from our last strategy meeting?\"\nrelevant_docs = vectorstore.similarity_search(query, k=3)\n\nfor doc in relevant_docs:\n    print(doc.page_content)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running DetailedReport Class in Python\nDESCRIPTION: This snippet demonstrates how to create an instance of the DetailedReport class and use it to generate a comprehensive report on the impact of AI on healthcare. It includes setting up a FastAPI websocket endpoint to handle the report generation process.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/detailed_report.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom fastapi import WebSocket\nfrom gpt_researcher.utils.enum import Tone\nfrom backend.report_type import DetailedReport\n\nasync def generate_report(websocket: WebSocket):\n    detailed_report = DetailedReport(\n        query=\"The impact of artificial intelligence on modern healthcare\",\n        report_type=\"research_report\",\n        report_source=\"web_search\",\n        source_urls=[],  # You can provide initial source URLs if available\n        config_path=\"path/to/config.yaml\",\n        tone=Tone.FORMAL,\n        websocket=websocket,\n        subtopics=[],  # You can provide predefined subtopics if desired\n        headers={}  # Add any necessary HTTP headers\n    )\n\n    final_report = await detailed_report.run()\n    return final_report\n\n# In your FastAPI app\n@app.websocket(\"/generate_report\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    report = await generate_report(websocket)\n    await websocket.send_text(report)\n```\n\n----------------------------------------\n\nTITLE: Research Progress Tracking Class - Python\nDESCRIPTION: Defines the ResearchProgress class structure for tracking research progress, including depth, breadth, and query status information.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/deep_research.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ResearchProgress:\n    current_depth: int       # Current depth level\n    total_depth: int         # Maximum depth to explore\n    current_breadth: int     # Current number of parallel paths\n    total_breadth: int       # Maximum breadth at each level\n    current_query: str       # Currently processing query\n    completed_queries: int   # Number of completed queries\n    total_queries: int       # Total queries to process\n```\n\n----------------------------------------\n\nTITLE: Researching Local Documents with GPT Researcher\nDESCRIPTION: Example of using GPT Researcher to analyze local documents from a specified directory. Supports PDF, text, CSV, Excel, Markdown, PowerPoint, and Word formats.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/tailored-research.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(query: str, report_source: str) -> str:\n    researcher = GPTResearcher(query=query, report_source=report_source)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n    \nif __name__ == \"__main__\":\n    query = \"What can you tell me about myself based on my documents?\"\n    report_source = \"local\" # \"local\" or \"web\"\n\n    report = asyncio.run(get_report(query=query, report_source=report_source))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Bedrock Models\nDESCRIPTION: Configuration for AWS Bedrock service using Claude-3 and Titan embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nFAST_LLM=\"bedrock:anthropic.claude-3-sonnet-20240229-v1:0\"\nSMART_LLM=\"bedrock:anthropic.claude-3-sonnet-20240229-v1:0\"\nSTRATEGIC_LLM=\"bedrock:anthropic.claude-3-sonnet-20240229-v1:0\"\n\nEMBEDDING=\"bedrock:amazon.titan-embed-text-v2:0\"\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Deep Research with GPT Researcher in Python\nDESCRIPTION: This snippet demonstrates how to initialize the GPT Researcher with deep research type, conduct research, and generate a report. It uses asyncio for asynchronous execution.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/backend/report_type/deep_research/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nfrom gpt_researcher.utils.enum import ReportType, Tone\nimport asyncio\n\nasync def main():\n    # Initialize researcher with deep research type\n    researcher = GPTResearcher(\n        query=\"What are the latest developments in quantum computing?\",\n        report_type=\"deep\",  # This triggers deep research modd\n    )\n    \n    # Run research\n    research_data = await researcher.conduct_research()\n    \n    # Generate report\n    report = await researcher.write_report()\n    print(report)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing GPT Researcher in JavaScript\nDESCRIPTION: This snippet demonstrates how to use the GPT Researcher package in a JavaScript application. It shows how to create an instance of the GPTResearcher class, configure it with a host and log listener, and send a query message.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/npm-package.md#2025-04-21_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst GPTResearcher = require('gpt-researcher');\n\nconst researcher = new GPTResearcher({\n  host: 'localhost:8000',\n  logListener: (data) => console.log('logListener logging data: ',data)\n});\n\nresearcher.sendMessage({\n  query: 'Does providing better context reduce LLM hallucinations?',\n  moreContext: 'Provide a detailed answer'\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing WebSocket Client for GPTR Backend Communication in JavaScript\nDESCRIPTION: This code establishes a WebSocket connection to the GPTR backend server, handles connection events, and provides a function to send messages to the server. It includes event handlers for received data processing, connection state changes, and error handling.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/querying-the-backend.md#2025-04-21_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst WebSocket = require('ws');\n\nlet socket = null;\nlet responseCallback = null;\n\nasync function initializeWebSocket() {\n  if (!socket) {\n    const host = 'localhost:8000';\n    const ws_uri = `ws://${host}/ws`;\n\n    socket = new WebSocket(ws_uri);\n\n    socket.onopen = () => {\n      console.log('WebSocket connection established');\n    };\n\n    socket.onmessage = (event) => {\n      const data = JSON.parse(event.data);\n      console.log('WebSocket data received:', data);\n\n      if (data.content === 'dev_team_result' \n          && data.output.rubber_ducker_thoughts != undefined\n          && data.output.tech_lead_review != undefined) {\n        if (responseCallback) {\n          responseCallback(data.output);\n          responseCallback = null; // Clear callback after use\n        }\n      } else {\n        console.log('Received data:', data);\n      }\n    };\n\n    socket.onclose = () => {\n      console.log('WebSocket connection closed');\n      socket = null;\n    };\n\n    socket.onerror = (error) => {\n      console.error('WebSocket error:', error);\n    };\n  }\n}\n\nasync function sendWebhookMessage(message) {\n  return new Promise((resolve, reject) => {\n    if (!socket || socket.readyState !== WebSocket.OPEN) {\n      initializeWebSocket();\n    }\n\n    const data = {\n      task: message,\n      report_type: 'dev_team',\n      report_source: 'web',\n      tone: 'Objective',\n      headers: {},\n      repo_name: 'elishakay/gpt-researcher'\n    };\n\n    const payload = \"start \" + JSON.stringify(data);\n\n    responseCallback = (response) => {\n      resolve(response); // Resolve the promise with the WebSocket response\n    };\n\n    if (socket.readyState === WebSocket.OPEN) {\n      socket.send(payload);\n      console.log('Message sent:', payload);\n    } else {\n      socket.onopen = () => {\n        socket.send(payload);\n        console.log('Message sent after connection:', payload);\n      };\n    }\n  });\n}\n\nmodule.exports = {\n  sendWebhookMessage\n};\n```\n\n----------------------------------------\n\nTITLE: Transforming Content into Langchain Documents in Python\nDESCRIPTION: Function to transform GitHub repository content into Langchain Document objects. Each file is split into smaller chunks using RecursiveCharacterTextSplitter, with metadata including source and title for seamless GPTR integration.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/data-ingestion.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.documents import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nasync def transform_to_langchain_docs(self, directory_structure):\n    documents = []\n    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n    run_timestamp = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n\n    for file_name in directory_structure:\n        if not file_name.endswith('/'):\n            try:\n                content = self.repo.get_contents(file_name, ref=self.branch_name)\n                try:\n                    decoded_content = base64.b64decode(content.content).decode()\n                except Exception as e:\n                    print(f\"Error decoding content: {e}\")\n                    print(\"the problematic file_name is\", file_name)\n                    continue\n                print(\"file_name\", file_name)\n                print(\"content\", decoded_content)\n\n                # Split each document into smaller chunks\n                chunks = splitter.split_text(decoded_content)\n\n                # Extract metadata for each chunk\n                for index, chunk in enumerate(chunks):\n                    metadata = {\n                        \"id\": f\"{run_timestamp}_{uuid4()}\",  # Generate a unique UUID for each document\n                        \"source\": file_name,\n                        \"title\": file_name,\n                        \"extension\": os.path.splitext(file_name)[1],\n                        \"file_path\": file_name\n                    }\n                    document = Document(\n                        page_content=chunk,\n                        metadata=metadata\n                    )\n                    documents.append(document)\n\n            except Exception as e:\n                print(f\"Error saving to vector store: {e}\")\n                return None\n\n    await save_to_vector_store(documents)\n```\n\n----------------------------------------\n\nTITLE: Customizing GPT Researcher Initialization\nDESCRIPTION: Example showing advanced initialization options for GPT Researcher, including report format, tone, and maximum subtopics. These parameters control how the research is conducted and formatted.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nresearcher = GPTResearcher(\n    query=\"Your research query\",\n    report_type=\"research_report\",\n    report_format=\"APA\",\n    tone=\"formal and objective\",\n    max_subtopics=5,\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Google VertexAI LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This snippet shows how to set up Google VertexAI LLMs and embeddings for use with GPT Researcher. It includes specifying the LLM and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nFAST_LLM=\"google_vertexai:gemini-1.5-flash-001\"\nSMART_LLM=\"google_vertexai:gemini-1.5-pro-001\"\nSTRATEGIC_LLM=\"google_vertexai:gemini-1.5-pro-001\"\n\nEMBEDDING=\"google_vertexai:text-embedding-004\"\n```\n\n----------------------------------------\n\nTITLE: Inserting Documents into a PostgreSQL Vector Store in Python\nDESCRIPTION: Function to save Langchain Document objects to a PostgreSQL vector store using PGVector. The documents are processed in chunks of 100 to avoid rate limiting issues, with OpenAI embeddings used for vectorization.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/data-ingestion.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_postgres import PGVector\nfrom langchain_postgres.vectorstores import PGVector\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\nasync def save_to_vector_store(self, documents):\n    # The documents are already Document objects, so we don't need to convert them\n    embeddings = OpenAIEmbeddings()\n    # self.vector_store = FAISS.from_documents(documents, embeddings)\n    pgvector_connection_string = os.environ[\"PGVECTOR_CONNECTION_STRING\"]\n\n    collection_name = \"my_docs\"\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=collection_name,\n        connection=pgvector_connection_string,\n        use_jsonb=True\n    )\n\n    # for faiss\n    # self.vector_store = vector_store.add_documents(documents, ids=[doc.metadata[\"id\"] for doc in documents])\n\n    # Split the documents list into chunks of 100\n    for i in range(0, len(documents), 100):\n        chunk = documents[i:i+100]\n        # Insert the chunk into the vector store\n        vector_store.add_documents(chunk, ids=[doc.metadata[\"id\"] for doc in chunk])\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Logs Handler with GPT-Researcher\nDESCRIPTION: This code demonstrates how to create a custom logs handler for GPT-Researcher, initialize the researcher with custom parameters, and run an asynchronous research task. The CustomLogsHandler class captures and stores JSON log data from the research process.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/handling-logs/simple-logs-example.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, Any\nimport asyncio\nfrom gpt_researcher import GPTResearcher\n\nclass CustomLogsHandler:\n    \"\"\"A custom Logs handler class to handle JSON data.\"\"\"\n    def __init__(self):\n        self.logs = []  # Initialize logs to store data\n\n    async def send_json(self, data: Dict[str, Any]) -> None:\n        \"\"\"Send JSON data and log it.\"\"\"\n        self.logs.append(data)  # Append data to logs\n        print(f\"My custom Log: {data}\")  # For demonstration, print the log\n\nasync def run():\n    # Define the necessary parameters with sample values\n    \n    query = \"What happened in the latest burning man floods?\"\n    report_type = \"research_report\"  # Type of report to generate\n    report_source = \"online\"  # Could specify source like 'online', 'books', etc.\n    tone = \"informative\"  # Tone of the report ('informative', 'casual', etc.)\n    config_path = None  # Path to a config file, if needed\n    \n    # Initialize researcher with a custom WebSocket\n    custom_logs_handler = CustomLogsHandler()\n\n    researcher = GPTResearcher(\n        query=query,\n        report_type=report_type,\n        report_source=report_source,\n        tone=tone,\n        config_path=config_path,\n        websocket=custom_logs_handler\n    )\n\n    await researcher.conduct_research()  # Conduct the research\n    report = await researcher.write_report()  # Write the research report\n\n    return report\n\n# Run the asynchronous function using asyncio\nif __name__ == \"__main__\":\n    asyncio.run(run())\n```\n\n----------------------------------------\n\nTITLE: Implementing Singleton Configuration Class in Python\nDESCRIPTION: Defines a Config class using the Singleton metaclass to store and manage various configuration settings for the GPT Researcher project.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/config/config.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Config(metaclass=Singleton):\n\n    def __init__() -> None:\n        \"\"\"Initialize the Config class\"\"\"\n\n    def set_fast_llm_model(value: str) -> None:\n        \"\"\"Set the fast LLM model value.\"\"\"\n\n    def set_smart_llm_model(value: str) -> None:\n        \"\"\"Set the smart LLM model value.\"\"\"\n\n    def set_fast_token_limit(value: int) -> None:\n        \"\"\"Set the fast token limit value.\"\"\"\n\n    def set_smart_token_limit(value: int) -> None:\n        \"\"\"Set the smart token limit value.\"\"\"\n\n    def set_browse_chunk_max_length(value: int) -> None:\n        \"\"\"Set the browse_website command chunk max length value.\"\"\"\n\n    def set_openai_api_key(value: str) -> None:\n        \"\"\"Set the OpenAI API key value.\"\"\"\n\n    def set_debug_mode(value: bool) -> None:\n        \"\"\"Set the debug mode value.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom Prompts with GPT Researcher\nDESCRIPTION: Example showing how to use custom prompts when generating reports with GPT Researcher. The custom prompt gives control over the format, structure, and content of the generated report.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# After conducting research\nresearch_result = await researcher.conduct_research()\n\n# Generate a report with a custom prompt\nreport = await researcher.write_report(\n    custom_prompt=\"Based on the research, provide a bullet-point summary of the key findings.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Usage of GPT Researcher with State Management\nDESCRIPTION: An advanced example showing how to use the GPTResearcher component with React state management. It demonstrates handling research results, using multiple props, and integrating the component into a more complex application structure.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/nextjs/README.md#2025-04-21_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport React, { useState } from 'react';\nimport { GPTResearcher } from 'gpt-researcher-ui';\n\nfunction App() {\n  const [results, setResults] = useState([]);\n\n  const handleResultsChange = (newResults) => {\n    setResults(newResults);\n    console.log('Research progress:', newResults);\n  };\n\n  return (\n    <div className=\"App\">\n      <h1>My Research Application</h1>\n      \n      <GPTResearcher \n        apiUrl=\"http://localhost:8000\"\n        apiKey=\"your-api-key-if-needed\"\n        defaultPrompt=\"Explain the impact of quantum computing on cryptography\"\n        onResultsChange={handleResultsChange}\n      />\n      \n      {/* You can use the results state elsewhere in your app */}\n      <div className=\"results-summary\">\n        {results.length > 0 && (\n          <p>Research in progress: {results.length} items processed</p>\n        )}\n      </div>\n    </div>\n  );\n}\n\nexport default App;\n```\n\n----------------------------------------\n\nTITLE: Custom Retriever Response Format in JSON\nDESCRIPTION: Illustrates the expected JSON response format for custom retrievers, including URLs and raw content for each retrieved page.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/search-engines/retrievers.md#2025-04-21_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"url\": \"http://example.com/page1\",\n    \"raw_content\": \"Content of page 1\"\n  },\n  {\n    \"url\": \"http://example.com/page2\",\n    \"raw_content\": \"Content of page 2\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Research Report Using GPT-4 Prompt\nDESCRIPTION: This snippet shows the prompt used to generate a detailed research report using GPT-4. It provides context with aggregated information and specifies requirements for the report, including structure, depth, and formatting.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-09-22-gpt-researcher/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"{research_summary}\" Using the above information, answer the following question or topic: \"{question}\" in a detailed report — The report should focus on the answer to the question, should be well structured, informative, in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format. Write all source urls at the end of the report in apa format. You should write your report only based on the given information and nothing else.\n```\n\n----------------------------------------\n\nTITLE: Using GPT Researcher as a PIP Package\nDESCRIPTION: Shows how to use GPT Researcher as a Python package after installing it via pip.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README-ko_KR.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\n\nquery = \"왜 Nvidia 주식이 오르고 있나요?\"\nresearcher = GPTResearcher(query=query, report_type=\"research_report\")\n# 주어진 질문에 대한 연구 수행\nresearch_result = await researcher.conduct_research()\n# 보고서 작성\nreport = await researcher.write_report()\n```\n\n----------------------------------------\n\nTITLE: Testing LLM Configuration and Connection in GPT Researcher\nDESCRIPTION: This Python script verifies that the LLM-related environment variables are set up correctly. It loads environment variables, initializes the configuration, and attempts to create a chat completion with a simple prompt to test the connection to the LLM provider.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/testing-your-llm.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher.config.config import Config\nfrom gpt_researcher.utils.llm import create_chat_completion\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\n\nasync def main():\n    cfg = Config()\n\n    try:\n        report = await create_chat_completion(\n            model=cfg.smart_llm_model,\n            messages = [{\"role\": \"user\", \"content\": \"sup?\"}],\n            temperature=0.35,\n            llm_provider=cfg.smart_llm_provider,\n            stream=True,\n            max_tokens=cfg.smart_token_limit,\n            llm_kwargs=cfg.llm_kwargs\n        )\n    except Exception as e:\n        print(f\"Error in calling LLM: {e}\")\n\n# Run the async function\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Format Control with Custom Prompts in GPT Researcher\nDESCRIPTION: Example of using a custom prompt to control the format and style of the research report, specifying a conversational blog post format with headings and a conclusion.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nreport = await researcher.write_report(\n    custom_prompt=\"Write a blog post in a conversational tone using the research. Include headings and a conclusion.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Research Sources from GPT Researcher\nDESCRIPTION: Example of how to retrieve the source URLs used during the research process. These are the websites and references that the researcher used to gather information.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsource_urls = researcher.get_source_urls()\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Retriever in Bash\nDESCRIPTION: Shows how to set up a custom retriever by specifying the retriever type, endpoint URL, and additional arguments using environment variables in Bash.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/search-engines/retrievers.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nRETRIEVER=custom\nRETRIEVER_ENDPOINT=https://api.myretriever.com\nRETRIEVER_ARG_API_KEY=YOUR_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion Message in Python\nDESCRIPTION: Creates a formatted message dictionary for chat completion API. Takes a chunk of text to summarize and a question to answer, returning a dictionary with the properly formatted message for API consumption.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/processing/text.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_message(chunk: str, question: str) -> Dict[str, str]\n```\n\n----------------------------------------\n\nTITLE: Managing Tool Output Submission for OpenAI Assistant Function Calls\nDESCRIPTION: Handles the submission of tool outputs when the assistant calls a function. This function processes tool calls, extracts parameters, executes the appropriate function (in this case, Tavily search), and returns the results to the assistant.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Function to handle tool output submission\ndef submit_tool_outputs(thread_id, run_id, tools_to_call):\n    tool_output_array = []\n    for tool in tools_to_call:\n        output = None\n        tool_call_id = tool.id\n        function_name = tool.function.name\n        function_args = tool.function.arguments\n\n        if function_name == \"tavily_search\":\n            output = tavily_search(query=json.loads(function_args)[\"query\"])\n\n        if output:\n            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n\n    return client.beta.threads.runs.submit_tool_outputs(\n        thread_id=thread_id,\n        run_id=run_id,\n        tool_outputs=tool_output_array\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment Variables for GPT Researcher\nDESCRIPTION: Configures the necessary API keys for OpenAI and Tavily in environment variables. These keys are required for the GPT Researcher library to access external services for AI and search capabilities.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['OPENAI_API_KEY'] = 'your_openai_api_key'\nos.environ['TAVILY_API_KEY'] = 'your_tavily_api_key' # Get a free key here: https://app.tavily.com\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Exploration for Deep Research in Python\nDESCRIPTION: This function implements the recursive exploration pattern, allowing the system to dive deeper into promising research paths while maintaining breadth of coverage.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Continue deeper if needed\nif depth > 1:\n    new_breadth = max(2, breadth // 2)\n    new_depth = depth - 1\n    progress.current_depth += 1\n\n    # Create next query from research goal and follow-up questions\n    next_query = f\"\"\"\n    Previous research goal: {result['researchGoal']}\n    Follow-up questions: {' '.join(result['followUpQuestions'])}\n    \"\"\"\n\n    # Recursive research\n    deeper_results = await self.deep_research(\n        query=next_query,\n        breadth=new_breadth,\n        depth=new_depth,\n        # Additional parameters\n    )\n```\n\n----------------------------------------\n\nTITLE: Integrating PGVector with GPT Researcher for PostgreSQL-based Vector Store in Python\nDESCRIPTION: This snippet shows how to use PGVector, a PostgreSQL-based vector store, with GPT Researcher. It demonstrates connecting to an existing index, setting up OpenAI embeddings, and using the vector store for research and report generation.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/vector-stores.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain_openai import OpenAIEmbeddings\n\nCONNECTION_STRING = 'postgresql://someuser:somepass@localhost:5432/somedatabase'\n\n\n# assuming the vector store exists and contains the relevent documents\n# also assuming embeddings have been or will be generated\nvector_store = PGVector.from_existing_index(\n    use_jsonb=True,\n    embedding=OpenAIEmbeddings(),\n    collection_name='some collection name',\n    connection=CONNECTION_STRING,\n    async_mode=True,\n)\n\nquery = \"\"\"\n    Create a short report about apples.\n    Include a section about which apples are considered best\n    during each season.\n\"\"\"\n\n# Create an instance of GPTResearcher\nresearcher = GPTResearcher(\n    query=query,\n    report_type=\"research_report\",\n    report_source=\"langchain_vectorstore\",\n    vector_store=vector_store, \n)\n\n# Conduct research and write the report\nawait researcher.conduct_research()\nreport = await researcher.write_report()\n```\n\n----------------------------------------\n\nTITLE: Performing Advanced Search with Tavily\nDESCRIPTION: Executes a search query using Tavily's advanced search depth option to get more comprehensive results about a specific topic.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/examples.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# simple query using tavily's advanced search\nclient.search(\"What happend in the latest burning man floods?\", search_depth=\"advanced\")\n```\n\n----------------------------------------\n\nTITLE: Processing Function Call Requirements in OpenAI Assistant\nDESCRIPTION: Handles the 'requires_action' status when the assistant needs to call a function. This code checks if the run requires action, submits tool outputs, and then waits for the run to complete after providing the function results.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif run.status == 'requires_action':\n    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n    run = wait_for_run_completion(thread.id, run.id)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This snippet shows how to set up environment variables for using OpenAI's LLMs and embedding models with GPT Researcher. It includes API key configuration and model specifications for fast, smart, and strategic LLMs, as well as the embedding model.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# set the custom OpenAI API key\nOPENAI_API_KEY=[Your Key]\n\n# specify llms\nFAST_LLM=\"openai:gpt-4o-mini\"\nSMART_LLM=\"openai:gpt-4o\"\nSTRATEGIC_LLM=\"openai:o1-preview\"\n\n# specify embedding\nEMBEDDING=\"openai:text-embedding-3-small\"\n```\n\n----------------------------------------\n\nTITLE: Using LangChain Documents with GPT Researcher\nDESCRIPTION: Demonstrates integration with LangChain documents and PostgreSQL vector store for document retrieval and research.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/tailored-research.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.documents import Document\nfrom typing import List, Dict\nfrom gpt_researcher import GPTResearcher\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain_openai import OpenAIEmbeddings\nfrom sqlalchemy import create_engine\nimport asyncio\n\n\n\nCONNECTION_STRING = 'postgresql://someuser:somepass@localhost:5432/somedatabase'\n\ndef get_retriever(collection_name: str, search_kwargs: Dict[str, str]):\n    engine = create_engine(CONNECTION_STRING)\n    embeddings =  OpenAIEmbeddings()\n\n    index = PGVector.from_existing_index(\n        use_jsonb=True,\n        embedding=embeddings,\n        collection_name=collection_name,\n        connection=engine,\n    )\n\n    return index.as_retriever(search_kwargs=search_kwargs)\n\n\nasync def get_report(query: str, report_type: str, report_source: str, documents: List[Document]) -> str:\n    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source, documents=documents)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n\nif __name__ == \"__main__\":\n    query = \"What can you tell me about blue cheese based on my documents?\"\n    report_type = \"research_report\"\n    report_source = \"langchain_documents\"\n\n    langchain_retriever = get_retriever(\"cheese_collection\", { \"k\": 3 })\n    documents = langchain_retriever.invoke(\"All the documents about cheese\")\n    report = asyncio.run(get_report(query=query, report_type=report_type, report_source=report_source, documents=documents))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Domain Filtering in Python using GPTResearcher\nDESCRIPTION: Example of using domain filtering with the GPTResearcher pip package. Shows how to initialize the researcher with specific domains to filter search results.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/filtering-by-domain.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nreport = GPTResearcher(\n    query=\"Latest AI Startups\",\n    report_type=\"research_report\",\n    report_source=\"web\",\n    domains=[\"forbes.com\", \"techcrunch.com\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Usage of GPT Researcher with Custom Settings\nDESCRIPTION: This snippet demonstrates advanced usage of GPT Researcher, including setting custom headers, enabling verbose logging, and accessing raw research data and sources.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/backend/report_type/deep_research/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresearcher = GPTResearcher(\n    query=\"your query\",\n    report_type=\"deep\",\n    tone=Tone.Objective,\n    headers={\"User-Agent\": \"your-agent\"},  # Custom headers for web requests\n    verbose=True  # Enable detailed logging\n)\n\n# Get raw research context\ncontext = await researcher.conduct_research()\n\n# Access research sources\nsources = researcher.get_research_sources()\n\n# Get visited URLs\nurls = researcher.get_source_urls()\n\n# Generate formatted report\nreport = await researcher.write_report()\n```\n\n----------------------------------------\n\nTITLE: Installing FireCrawl Python SDK for Production Scraping\nDESCRIPTION: Installs the FireCrawl Python SDK required for using the FireCrawl scraping method, which provides production-level scraping with markdown formatting.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install firecrawl-py\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT Researcher with Custom Configuration\nDESCRIPTION: Python code example showing how to initialize the GPT Researcher with a custom query, report type, and configuration file path.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresearcher = GPTResearcher(\n    query=\"your query\",\n    report_type=\"deep\",\n    config_path=\"path/to/config.yaml\"\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Research Context from GPT Researcher\nDESCRIPTION: Example of how to retrieve the full context of the research, which includes all the information gathered during the research process, including sources and their content.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresearch_context = researcher.get_research_context()\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT Researcher with Environment Variables\nDESCRIPTION: Example of environment variable configuration for GPT Researcher. This method provides an alternative to JSON configuration, useful for deployment in various environments.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nRETRIEVERS=google\nFAST_LLM=cohere:command\nSMART_LLM=cohere:command-nightly\nMAX_ITERATIONS=3\nMAX_SUBTOPICS=1\n```\n\n----------------------------------------\n\nTITLE: Retrieving Research Sources Details from GPT Researcher\nDESCRIPTION: Example of how to retrieve detailed information about the research sources, including title, content, and images. This provides more comprehensive data than just the URLs.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nresearch_sources = researcher.get_research_sources()\n```\n\n----------------------------------------\n\nTITLE: Using GPT Researcher React Component in a React App\nDESCRIPTION: Example of importing and using the GPTResearcher component in a React application. It demonstrates how to set the API URL, default prompt, and handle results changes.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/react-package.md#2025-04-21_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport React from 'react';\nimport { GPTResearcher } from 'gpt-researcher-ui';\n\nfunction App() {\n  return (\n    <div className=\"App\">\n      <GPTResearcher \n        apiUrl=\"http://localhost:8000\"\n        defaultPrompt=\"What is quantum computing?\"\n        onResultsChange={(results) => console.log('Research results:', results)}\n      />\n    </div>\n  );\n}\n\nexport default App;\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily API Key for Production Scraping\nDESCRIPTION: Sets the environment variable for the Tavily API key, which is required when using the Tavily Extract scraping method.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport TAVILY_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Concurrent Query Processing for Deep Research in Python\nDESCRIPTION: This code snippet shows how the system executes multiple queries concurrently, using a semaphore to manage resource allocation and ensure system stability.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Process queries with concurrency limit\nsemaphore = asyncio.Semaphore(self.concurrency_limit)\n\nasync def process_query(serp_query: Dict[str, str]) -> Optional[Dict[str, Any]]:\n    async with semaphore:\n        # Research execution logic\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Local Ollama Integration with GPT-Researcher\nDESCRIPTION: This snippet shows the required environment variables in .env file to connect GPT-Researcher with a locally deployed Ollama instance. It specifies the API endpoints, model names for different components, and embedding configuration.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/running-with-ollama.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=\"123\"\nOPENAI_API_BASE=\"http://127.0.0.1:11434/v1\"\nOLLAMA_BASE_URL=\"http://127.0.0.1:11434/\"\nFAST_LLM=\"ollama:qwen2:1.5b\"\nSMART_LLM=\"ollama:qwen2:1.5b\"\nSTRATEGIC_LLM=\"ollama:qwen2:1.5b\"\nEMBEDDING_PROVIDER=\"ollama\"\nOLLAMA_EMBEDDING_MODEL=\"nomic-embed-text\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude Desktop with Command Setup Using JSON\nDESCRIPTION: JSON configuration for Claude desktop that specifies the path to the GPT Researcher MCP server with command execution. This configuration allows Claude to start the server automatically.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/claude-integration.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"gpt-researcher\": {\n      \"command\": \"/path/to/python\",\n      \"args\": [\"/path/to/gpt-researcher/mcp-server/server.py\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Research Images from GPT Researcher\nDESCRIPTION: Example of how to retrieve the images found during the research process. This returns a list of image data that can be used in reports or presentations.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresearch_images = researcher.get_research_images()\n```\n\n----------------------------------------\n\nTITLE: Adding New Research Tool to GPT Researcher MCP Server\nDESCRIPTION: This Python snippet demonstrates how to extend the MCP server functionality by adding a new research tool for sentiment analysis.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@app.tool(\"analyze_sentiment\")\nasync def analyze_sentiment(query: str):\n    \"\"\"Analyze the sentiment of research results.\"\"\"\n    # Implementation\n    return {\"sentiment\": \"positive\", \"confidence\": 0.87}\n```\n\n----------------------------------------\n\nTITLE: Installing ZenDriver for NoDriver Scraping in GPT Researcher\nDESCRIPTION: Installs the ZenDriver package required for using the NoDriver scraping method, which is an alternative to Selenium.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install zendriver\n```\n\n----------------------------------------\n\nTITLE: Configuring Research Task Parameters in JSON\nDESCRIPTION: Sample JSON configuration for the research task, including query topic, model selection, formatting options, and research guidelines. This example configures a research task exploring whether AI is in a hype cycle.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/multi_agents/langgraph.md#2025-04-21_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": \"Is AI in a hype cycle?\",\n  \"model\": \"gpt-4o\",\n  \"max_sections\": 3, \n  \"publish_formats\": { \n    \"markdown\": true,\n    \"pdf\": true,\n    \"docx\": true\n  },\n  \"include_human_feedback\": false,\n  \"source\": \"web\",\n  \"follow_guidelines\": true,\n  \"guidelines\": [\n    \"The report MUST fully answer the original question\",\n    \"The report MUST be written in apa format\",\n    \"The report MUST be written in english\"\n  ],\n  \"verbose\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Azure OpenAI with GPT-Researcher\nDESCRIPTION: This snippet contains the environment variables needed to configure GPT-Researcher to use Azure OpenAI models instead of standard OpenAI models. It includes API endpoints, keys, model deployments for embeddings and language models, and Bing search integration which is commonly used with Azure.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/running-with-azure.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_VERSION=\"2024-05-01-preview\" # or whatever you are using\nAZURE_OPENAI_ENDPOINT=\"https://CHANGEMEN.openai.azure.com/\" # change to the name of your deployment\nAZURE_OPENAI_API_KEY=\"[Your Key]\" # change to your API key\n\nEMBEDDING=\"azure_openai:text-embedding-ada-002\" # change to the deployment of your embedding model\n\nFAST_LLM=\"azure_openai:gpt-4o-mini\" # change to the name of your deployment (not model-name)\nFAST_TOKEN_LIMIT=4000\n\nSMART_LLM=\"azure_openai:gpt-4o\" # change to the name of your deployment (not model-name)\nSMART_TOKEN_LIMIT=4000\n\nRETRIEVER=\"bing\" # if you are using Bing as your search engine (which is likely if you use Azure)\nBING_API_KEY=\"[Your Key]\"\n```\n\n----------------------------------------\n\nTITLE: Using the GPTR WebSocket Client in JavaScript Applications\nDESCRIPTION: A usage example demonstrating how to leverage the WebSocket client helper function to send a query to the GPTR backend and handle the response. This example imports the sendWebhookMessage function and uses it within an async function to send a query and log the response.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/querying-the-backend.md#2025-04-21_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { sendWebhookMessage } = require('./gptr-webhook');\n\nasync function main() {\n  const message = 'How do I get started with GPT-Researcher Websockets?';\n  const response = await sendWebhookMessage(message);\n  console.log('Response:', response);\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced GPT Researcher Usage\nDESCRIPTION: Advanced example demonstrating usage of all available parameters including reportType, reportSource and queryDomains\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/npm/Readme.md#2025-04-21_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst researcher = new GPTResearcher({\n  host: 'http://localhost:8000',\n  logListener: (data) => console.log('Log:', data)\n});\n\n// Advanced usage with all parameters\nresearcher.sendMessage({\n  task: \"What are the latest developments in AI?\",\n  reportType: \"research_report\",\n  reportSource: \"web\",\n  queryDomains: [\"techcrunch.com\", \"wired.com\"]\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Research Draft State TypedDict in Python with LangGraph\nDESCRIPTION: Creates a TypedDict class that defines the state structure for the research draft processing sub-graph. It includes fields for the research task, topic, draft content, review feedback, and revision notes.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-05-19-gptr-langgraph/index.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DraftState(TypedDict):\n    task: dict\n    topic: str\n    draft: dict\n    review: str\n    revision_notes: str\n```\n\n----------------------------------------\n\nTITLE: Creating Detailed Analytical Report (Bash)\nDESCRIPTION: Example command to create a detailed report on artificial intelligence with an analytical tone using the GPT Researcher CLI tool.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/cli.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython cli.py \"The impact of artificial intelligence on job markets\" --report_type detailed_report --tone analytical\n```\n\n----------------------------------------\n\nTITLE: Configuring Research Task JSON for the Assistant\nDESCRIPTION: JSON configuration file that customizes the research assistant's behavior. It specifies the research query, maximum number of sections, output formats, language model to use, and optional research guidelines.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-05-19-gptr-langgraph/index.md#2025-04-21_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": \"Is AI in a hype cycle?\",\n  \"max_sections\": 3,\n  \"publish_formats\": {\n    \"markdown\": true,\n    \"pdf\": true,\n    \"docx\": true\n  },\n  \"follow_guidelines\": false,\n  \"model\": \"gpt-4-turbo\",\n  \"guidelines\": [\n    \"The report MUST be written in APA format\",\n    \"Each sub section MUST include supporting sources using hyperlinks. If none exist, erase the sub section or rewrite it to be a part of the previous section\",\n    \"The report MUST be written in spanish\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure OpenAI for GPT Researcher\nDESCRIPTION: This snippet shows the environment variable setup for using Azure OpenAI with GPT Researcher. It includes API key, endpoint, and version configuration, as well as specifying the LLM deployments and embedding model.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# set the azure api key and deployment as you have configured it in Azure Portal. There is no default access point unless you configure it yourself!\nAZURE_OPENAI_API_KEY=\"[Your Key]\"\nAZURE_OPENAI_ENDPOINT=\"https://{your-endpoint}.openai.azure.com/\"\nOPENAI_API_VERSION=\"2024-05-01-preview\"\n\n# each string is \"azure_openai:deployment_name\". ensure that your deployment have the same name as the model you use!\nFAST_LLM=\"azure_openai:gpt-4o-mini\" \nSMART_LLM=\"azure_openai:gpt-4o\"\nSTRATEGIC_LLM=\"azure_openai:o1-preview\"\n\n# specify embedding\nEMBEDDING=\"azure_openai:text-embedding-3-large\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Elestio-Deployed Ollama with GPT-Researcher\nDESCRIPTION: This snippet demonstrates the environment variables needed to connect GPT-Researcher with an Ollama instance deployed on Elestio. It uses custom domain URLs for API endpoints and specifies the models to use for different components.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/running-with-ollama.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=\"123\"\nOPENAI_API_BASE=\"https://ollama-2d52b-u21899.vm.elestio.app:57987/v1\"\nOLLAMA_BASE_URL=\"https://ollama-2d52b-u21899.vm.elestio.app:57987/\"\nFAST_LLM=\"openai:qwen2.5\"\nSMART_LLM=\"openai:qwen2.5\"\nSTRATEGIC_LLM=\"openai:qwen2.5\"\nEMBEDDING_PROVIDER=\"ollama\"\nOLLAMA_EMBEDDING_MODEL=\"nomic-embed-text\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ollama LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This code snippet demonstrates how to configure Ollama LLMs and embeddings for use with GPT Researcher. It includes setting the Ollama base URL and specifying the LLM and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nOLLAMA_BASE_URL=http://localhost:11434\nFAST_LLM=\"ollama:llama3\"\nSMART_LLM=\"ollama:llama3\"\nSTRATEGIC_LLM=\"ollama:llama3\"\n\nEMBEDDING=\"ollama:nomic-embed-text\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This snippet shows how to set up HuggingFace LLMs and embeddings for use with GPT Researcher. It includes setting the HuggingFace API key and specifying the LLM and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nHUGGINGFACE_API_KEY=[Your key]\nFAST_LLM=\"huggingface:HuggingFaceH4/zephyr-7b-beta\"\nSMART_LLM=\"huggingface:HuggingFaceH4/zephyr-7b-beta\"\nSTRATEGIC_LLM=\"huggingface:HuggingFaceH4/zephyr-7b-beta\"\n\nEMBEDDING=\"huggingface:sentence-transformers/all-MiniLM-L6-v2\"\n```\n\n----------------------------------------\n\nTITLE: Generating Persuasive Outline Report (Bash)\nDESCRIPTION: Example command to generate an outline report on renewable energy with a persuasive tone using the GPT Researcher CLI tool.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/cli.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython cli.py \"Renewable energy sources and their potential\" --report_type outline_report --tone persuasive\n```\n\n----------------------------------------\n\nTITLE: Running Hybrid Research with Online Documents\nDESCRIPTION: Python script to run hybrid research using GPT Researcher with online documents. It includes additional parameter for document URLs.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/hybrid_research.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_research_report(query: str, report_type: str, report_source: str) -> str:\n    researcher = GPTResearcher(query=query, report_type=report_type, document_urls=document_urls, report_source=report_source)\n    research = await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n\nif __name__ == \"__main__\":\n    query = \"How does our product roadmap compare to emerging market trends in our industry?\"\n    report_source = \"hybrid\"\n    document_urls = [\"https://xxxx.xxx.pdf\", \"https://xxxx.xxx.doc\"]\n\n    report = asyncio.run(get_research_report(query=query, report_type=\"research_report\", document_urls=document_urls, report_source=report_source))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher Package\nDESCRIPTION: Command to install the gpt-researcher package from PyPI using pip. This package allows you to integrate advanced AI research capabilities into your Python applications.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Retrieving Research Costs from GPT Researcher\nDESCRIPTION: Example of how to retrieve the token costs incurred during the research process. This provides information about the computational resources used.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresearch_costs = researcher.get_costs()\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral AI LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This snippet shows how to set up Mistral AI LLMs and embeddings for use with GPT Researcher. It includes setting the Mistral API key and specifying the LLM and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nMISTRAL_API_KEY=[Your key]\nFAST_LLM=\"mistralai:open-mistral-7b\"\nSMART_LLM=\"mistralai:mistral-large-latest\"\nSTRATEGIC_LLM=\"mistralai:mistral-large-latest\"\n\nEMBEDDING=\"mistralai:mistral-embed\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Together AI LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This code snippet demonstrates how to configure Together AI LLMs and embeddings for use with GPT Researcher. It includes setting the Together AI API key and specifying the LLM and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nTOGETHER_API_KEY=[Your key]\nFAST_LLM=\"together:meta-llama/Llama-3-8b-chat-hf\"\nSMART_LLM=\"together:meta-llama/Llama-3-70b-chat-hf\"\nSTRATEGIC_LLM=\"together:meta-llama/Llama-3-70b-chat-hf\"\n\nEMBEDDING=\"mistralai:nomic-ai/nomic-embed-text-v1.5\"\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration - Bash\nDESCRIPTION: Shows how to configure deep research parameters using environment variables, including breadth, depth, concurrency, and word count settings.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/deep_research.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport DEEP_RESEARCH_BREADTH=4\nexport DEEP_RESEARCH_DEPTH=2\nexport DEEP_RESEARCH_CONCURRENCY=4\nexport TOTAL_WORDS=2500\n```\n\n----------------------------------------\n\nTITLE: Running Hybrid Research with Local Documents\nDESCRIPTION: Python script to run hybrid research using GPT Researcher with local documents. It sets up the researcher, conducts research, and generates a report.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/hybrid_research.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_research_report(query: str, report_type: str, report_source: str) -> str:\n    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source)\n    research = await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n\nif __name__ == \"__main__\":\n    query = \"How does our product roadmap compare to emerging market trends in our industry?\"\n    report_source = \"hybrid\"\n\n    report = asyncio.run(get_research_report(query=query, report_type=\"research_report\", report_source=report_source))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Running GPT Researcher with Docker Compose\nDESCRIPTION: Command to build and run the GPT Researcher application using Docker Compose. This will start both the Python server and the React application as defined in the docker-compose file.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started-with-docker.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Setting Verbose Mode in GPT Researcher\nDESCRIPTION: Example of how to enable verbose mode in GPT Researcher, which provides more detailed logs about the research process for debugging or monitoring purposes.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nresearcher.set_verbose(True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq for GPT Researcher\nDESCRIPTION: This snippet shows how to set up Groq for use with GPT Researcher. It includes setting the Groq API key and specifying the LLM models supported by Groq.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nGROQ_API_KEY=[Your Key]\n\n# Set one of the LLM models supported by Groq\nFAST_LLM=\"groq:Mixtral-8x7b-32768\"\nSMART_LLM=\"groq:Mixtral-8x7b-32768\"\nSTRATEGIC_LLM=\"groq:Mixtral-8x7b-32768\"\n```\n\n----------------------------------------\n\nTITLE: Implementing API Key Authentication for GPT Researcher MCP Server\nDESCRIPTION: This Python snippet demonstrates how to add API key authentication to the MCP server using FastAPI middleware to verify the API key in the request headers.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Add to server.py\n@app.middleware(\"http\")\nasync def verify_api_key(request, call_next):\n    api_key = request.headers.get(\"X-API-Key\")\n    if api_key != os.getenv(\"MCP_API_KEY\"):\n        return JSONResponse(status_code=401, content={\"error\": \"Invalid API key\"})\n    return await call_next(request)\n```\n\n----------------------------------------\n\nTITLE: Configuring Langsmith Environment Variables in Bash\nDESCRIPTION: Sets the required environment variables to enable Langsmith tracing for the GPT Researcher project. The LANGCHAIN_TRACING_V2 variable enables the tracing functionality, while the LANGCHAIN_API_KEY variable should be set to your personal API key from the Langsmith dashboard.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/handling-logs/langsmith-logs.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nLANGCHAIN_TRACING_V2=true\nLANGCHAIN_API_KEY=Set this to your API key\n```\n\n----------------------------------------\n\nTITLE: Implementing Rate Limiting for GPT Researcher MCP Server\nDESCRIPTION: This Python snippet demonstrates how to set up rate limiting for the MCP server using FastAPI and slowapi, limiting requests to 10 per minute.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Add rate limiting\nfrom fastapi import Depends, HTTPException\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\n\nlimiter = Limiter(key_func=get_remote_address)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n@app.post(\"/mcp\")\n@limiter.limit(\"10/minute\")\nasync def mcp_endpoint(request: Request, payload: dict):\n    # Endpoint code\n```\n\n----------------------------------------\n\nTITLE: Creating Abstract Singleton Base Class in Python\nDESCRIPTION: Defines an AbstractSingleton class that uses the Singleton metaclass. This abstract base class can be inherited to create concrete singleton classes without having to specify the metaclass each time.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/config/singleton.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractSingleton(abc.ABC, metaclass=Singleton)\n```\n\n----------------------------------------\n\nTITLE: Configuring Openrouter.ai with Google Models\nDESCRIPTION: Configuration for Openrouter.ai using Google's Gemini models with rate limiting and embedding settings.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nOPENROUTER_API_KEY=[Your openrouter.ai key]\nOPENAI_BASE_URL=https://openrouter.ai/api/v1\nFAST_LLM=\"openrouter:google/gemini-2.0-flash-lite-001\"\nSMART_LLM=\"openrouter:google/gemini-2.0-flash-001\"\nSTRATEGIC_LLM=\"openrouter:google/gemini-2.5-pro-exp-03-25\"\nOPENROUTER_LIMIT_RPS=1\nEMBEDDING=google_genai:models/text-embedding-004\nGOOGLE_API_KEY=[Your *google gemini* key]\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude for Integration with GPT Researcher MCP\nDESCRIPTION: This JSON snippet shows how to configure Claude to integrate with the MCP server by specifying the tool name and endpoint.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tools\": [\n    {\n      \"name\": \"gptr-researcher\",\n      \"endpoint\": \"http://localhost:8000/mcp\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example GPT-Researcher Log Output Format\nDESCRIPTION: This snippet shows the structure of log data emitted by the GPT-Researcher library. The JSON format includes type, content, output, and metadata fields that provide detailed information about each step of the research process.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/handling-logs/simple-logs-example.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"logs\",\n    \"content\": \"added_source_url\",\n    \"output\": \"✅ Added source url to research: https://www.npr.org/2023/09/28/1202110410/how-rumors-and-conspiracy-theories-got-in-the-way-of-mauis-fire-recovery\\n\",\n    \"metadata\": \"https://www.npr.org/2023/09/28/1202110410/how-rumors-and-conspiracy-theories-got-in-the-way-of-mauis-fire-recovery\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Assistant Instructions for a Finance Expert\nDESCRIPTION: Defines the assistant prompt instructions for a finance expert that uses online information to answer questions. The prompt specifies that the assistant should use the Tavily search API and include URL sources in answers.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nassistant_prompt_instruction = \"\"\"You are a finance expert. \nYour goal is to provide answers based on information from the internet. \nYou must use the provided Tavily search API function to find relevant online information. \nYou should never use your own knowledge to answer questions.\nPlease include relevant url sources in the end of your answers.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Custom LLM with llama.cpp Server for GPT Researcher\nDESCRIPTION: This code snippet demonstrates how to configure GPT Researcher to use a custom LLM with a local OpenAI API using llama.cpp Server. It includes setting the custom API URL, API key, and specifying custom LLMs and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# set the custom OpenAI API url\nOPENAI_BASE_URL=\"http://localhost:1234/v1\"\n# set the custom OpenAI API key\nOPENAI_API_KEY=\"dummy_key\"\n\n# specify custom llms  \nFAST_LLM=\"openai:your_fast_llm\"\nSMART_LLM=\"openai:your_smart_llm\"\nSTRATEGIC_LLM=\"openai:your_strategic_llm\"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL=\"http://localhost:1234/v1\"\n# set the custom OpenAI API key\nOPENAI_API_KEY=\"dummy_key\"\n\n# specify the custom embedding model   \nEMBEDDING=\"custom:your_embedding\"\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Container for GPT Researcher MCP\nDESCRIPTION: These bash commands demonstrate how to build and run the Docker container for the GPT Researcher MCP, including setting environment variables for API keys.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t gpt-researcher-mcp .\ndocker run -p 8000:8000 -e OPENAI_API_KEY=your_key -e TAVILY_API_KEY=your_key gpt-researcher-mcp\n```\n\n----------------------------------------\n\nTITLE: Customizing Agent Prompt in GPT Researcher\nDESCRIPTION: Demonstrates how to specify a custom agent prompt for research direction and report layout using the custom_report report type.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/tailored-research.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(prompt: str, report_type: str) -> str:\n    researcher = GPTResearcher(query=prompt, report_type=report_type)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n    \nif __name__ == \"__main__\":\n    report_type = \"custom_report\"\n    prompt = \"Research the latest advancements in AI and provide a detailed report in APA format including sources.\"\n\n    report = asyncio.run(get_report(prompt=prompt, report_type=report_type))\n    print(report)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for GPT Researcher\nDESCRIPTION: Bash commands to set up the required environment variables for GPT Researcher, including API keys for OpenAI and Tavily search engine.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-09-7-hybrid-research/index.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY={your-openai-key}\nexport TAVILY_API_KEY={your-tavily-key}\n```\n\n----------------------------------------\n\nTITLE: Implementing Conversation Loop with OpenAI Assistant API in Python\nDESCRIPTION: This code creates an ongoing conversation loop between a user and an OpenAI Assistant. It handles user input, sends messages to the thread, creates and manages runs, and waits for responses. The implementation includes handling for tool outputs and various run statuses.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Ongoing conversation loop\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'exit':\n        break\n\n    # Create a message\n    message = client.beta.threads.messages.create(\n        thread_id=thread.id,\n        role=\"user\",\n        content=user_input,\n    )\n\n    # Create a run\n    run = client.beta.threads.runs.create(\n        thread_id=thread.id,\n        assistant_id=assistant_id,\n    )\n    print(f\"Run ID: {run.id}\")\n\n    # Wait for run to complete\n    run = wait_for_run_completion(thread.id, run.id)\n\n    if run.status == 'failed':\n        print(run.error)\n        continue\n    elif run.status == 'requires_action':\n        run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n        run = wait_for_run_completion(thread.id, run.id)\n\n    # Print messages from the thread\n    print_messages_from_thread(thread.id)\n```\n\n----------------------------------------\n\nTITLE: Extracting Hyperlinks from BeautifulSoup Object in Python\nDESCRIPTION: This function extracts hyperlinks from a BeautifulSoup object. It takes a BeautifulSoup object and a base URL as input, and returns a list of tuples containing the extracted hyperlinks.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/processing/html.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef extract_hyperlinks(soup: BeautifulSoup,\n                       base_url: str) -> list[tuple[str, str]]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Anthropic LLMs for GPT Researcher\nDESCRIPTION: This code snippet demonstrates how to configure Anthropic LLMs for use with GPT Researcher. It includes setting the Anthropic API key and specifying the LLM models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nANTHROPIC_API_KEY=[Your key]\nFAST_LLM=\"anthropic:claude-2.1\"\nSMART_LLM=\"anthropic:claude-3-opus-20240229\"\nSTRATEGIC_LLM=\"anthropic:claude-3-opus-20240229\"\n```\n\n----------------------------------------\n\nTITLE: Setting up local document path environment variable for GPT Researcher\nDESCRIPTION: This bash command sets the DOC_PATH environment variable to point to the folder containing your local documents for use with GPT Researcher. This is a required step for enabling local document research functionality.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/local-docs.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport DOC_PATH=\"./my-docs\"\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT Researcher with Custom JSON\nDESCRIPTION: Example JSON configuration file for customizing GPT Researcher settings. This allows users to override default settings with their preferred options.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/README.md#2025-04-21_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"retrievers\": [\"google\"],\n  \"fast_llm\": \"cohere:command\",\n  \"smart_llm\": \"cohere:command-nightly\",\n  \"max_iterations\": 3,\n  \"max_subtopics\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Setting FireCrawl for Production-Scale Scraping in GPT Researcher\nDESCRIPTION: Sets the environment variable to use FireCrawl for production-level web scraping with markdown formatting. Can use either official service or self-hosted option.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport SCRAPER=\"firecrawl\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Thread and Adding User Input in OpenAI Assistant\nDESCRIPTION: Initializes a new thread for conversation and adds a user message to the thread. This represents steps 2 and 3 in the Assistant API workflow where a conversation thread is created and user input is captured.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create()\nuser_input = input(\"You: \")\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=user_input,\n)\n```\n\n----------------------------------------\n\nTITLE: ResearchProgress Class Structure for Progress Tracking\nDESCRIPTION: Python class definition showing the structure of the ResearchProgress class used for tracking and displaying the real-time status of the research process.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass ResearchProgress:\n    current_depth: int       # Current depth level\n    total_depth: int         # Maximum depth to explore\n    current_breadth: int     # Current number of parallel paths\n    total_breadth: int       # Maximum breadth at each level\n    current_query: str       # Currently processing query\n    completed_queries: int   # Number of completed queries\n    total_queries: int       # Total queries to process\n```\n\n----------------------------------------\n\nTITLE: Setting Up Google Gemini LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This code snippet demonstrates how to configure Google Gemini LLMs and embeddings for use with GPT Researcher. It includes setting the Google API key and specifying the LLM and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_API_KEY=[Your key]\nFAST_LLM=\"google_genai:gemini-1.5-flash\"\nSMART_LLM=\"google_genai:gemini-1.5-pro\"\nSTRATEGIC_LLM=\"google_genai:gemini-1.5-pro\"\n\nEMBEDDING=\"google_genai:models/text-embedding-004\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Tavily Retriever Environment Variables\nDESCRIPTION: Environment variable configuration for setting up Tavily as the search retriever in GPT-Researcher. Requires setting RETRIEVER to 'tavily' and providing a Tavily API key.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/filtering-by-domain.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nRETRIEVER=tavily\nTAVILY_API_KEY=your_tavily_api_key\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for GPT Researcher MCP Server\nDESCRIPTION: This snippet shows how to set up environment variables in a .env file to customize the MCP server behavior, including API keys and optional configurations.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Required API keys\nOPENAI_API_KEY=your_openai_api_key\nTAVILY_API_KEY=your_tavily_api_key\n\n# Optional configurations assuming using OpenAI\nSTRATEGIC_LLM=openai:gpt-4o-mini # Change default to faster reasoning model\nMAX_ITERATIONS=2 # Make the research faster by reducing iterations\nSCRAPER=tavily_extract # For production use, using hosted scraping methods (assuming you use tavily)\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily Extract for Production-Scale Scraping in GPT Researcher\nDESCRIPTION: Sets the environment variable to use Tavily Extract for robust production-level web scraping. Requires a Tavily account and API key.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport SCRAPER=\"tavily_extract\"\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Assistant with Function Calling Capability\nDESCRIPTION: Creates an OpenAI Assistant using the GPT-4 Turbo model with 128K context and configures it to use the Tavily web search API as a function tool. The assistant is initialized with the previously defined instructions.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create an assistant\nassistant = client.beta.assistants.create(\n    instructions=assistant_prompt_instruction,\n    model=\"gpt-4-1106-preview\",\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"tavily_search\",\n            \"description\": \"Get information on recent events from the web.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"The search query to use. For example: 'Latest news on Nvidia stock performance'\"},\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }]\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT-4 Grader Model in Python\nDESCRIPTION: This snippet shows how to configure the GPT-4 model used for grading research responses. It demonstrates setting up the ChatOpenAI instance with specific parameters like temperature and model name.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/evals/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngrader_model = ChatOpenAI(\n    temperature=0,                           # Lower temperature for more consistent grading\n    model_name=\"gpt-4-turbo\",               # Can be changed to other OpenAI models\n    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n```\n\n----------------------------------------\n\nTITLE: Adding External Costs to GPT Researcher\nDESCRIPTION: Example of how to manually add costs to the GPT Researcher tracking, which is useful when integrating with external services that also incur token costs.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nresearcher.add_costs(0.22)\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily API Key Environment Variable\nDESCRIPTION: Command to set the Tavily API key as an environment variable, which is required for GPT Researcher's search functionality.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport TAVILY_API_KEY={Your Tavily API Key here}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for GPT Researcher Configuration\nDESCRIPTION: Example of how to set environment variables to override default configurations for GPT Researcher, specifically changing the web retriever to Bing and the report format to IEEE style.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/config.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport RETRIEVER=bing\nexport REPORT_FORMAT=IEEE\n```\n\n----------------------------------------\n\nTITLE: Setting FireCrawl API Key for Production Scraping\nDESCRIPTION: Sets the environment variable for the FireCrawl API key, which is required when using the FireCrawl scraping method with either their cloud service or self-hosted server.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport FIRECRAWL_API_KEY=<your-firecrawl-api>\n```\n\n----------------------------------------\n\nTITLE: Implementing API Key and URL Validation Functions in Python\nDESCRIPTION: Defines several utility functions to check if various API keys and URLs are properly set in the configuration or environment variables.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/config/config.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef check_openai_api_key(cfg) -> None:\n    \"\"\"Check if the OpenAI API key is set in config.py or as an environment variable.\"\"\"\n\ndef check_tavily_api_key(cfg) -> None:\n    \"\"\"Check if the Tavily Search API key is set in config.py or as an environment variable.\"\"\"\n\ndef check_google_api_key(cfg) -> None:\n    \"\"\"Check if the Google API key is set in config.py or as an environment variable.\"\"\"\n\ndef check_serp_api_key(cfg) -> None:\n    \"\"\"Check if the SERP API key is set in config.py or as an environment variable.\"\"\"\n\ndef check_searx_url(cfg) -> None:\n    \"\"\"Check if the Searx URL is set in config.py or as an environment variable.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for GPT Researcher Project\nDESCRIPTION: This requirements file lists all the necessary Python packages for the GPT Researcher project, organized by purpose. It includes the core GPT Researcher package, MCP dependencies like FastAPI and Uvicorn, and utility libraries like Loguru for logging.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/mcp-server/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# GPT Researcher dependencies\ngpt-researcher>=0.12.16\npython-dotenv\n\n# MCP dependencies\nmcp>=1.6.0\nfastapi>=0.103.1\nuvicorn>=0.23.2\npydantic>=2.3.0\n\n# Utility dependencies\nloguru>=0.7.0\n```\n\n----------------------------------------\n\nTITLE: Running GPT Researcher CLI (Bash)\nDESCRIPTION: Basic command-line syntax for using the GPT Researcher CLI tool. This command structure allows users to specify the research query, report type, and optional tone.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/cli.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython cli.py \"<query>\" --report_type <report_type> [--tone <tone>]\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Environment Variables\nDESCRIPTION: Environment variables required for Azure Blob Storage connection setup in a .env file\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/azure-storage.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAZURE_CONNECTION_STRING=\nAZURE_CONTAINER_NAME=\n```\n\n----------------------------------------\n\nTITLE: Resource Report Type Example\nDESCRIPTION: Example of how to specify a resource report type query. This report type provides a list of resources related to the topic.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquery = \"List of top AI conferences in 2023\"\nreport_type = \"resource_report\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Fireworks API and Models\nDESCRIPTION: Configuration for Fireworks.ai API including API key, base URL, LLM models, and embedding model settings.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nFIREWORKS_API_KEY=[Your key]\nbase_url=\"https://api.fireworks.ai/inference/v1/completions\"\nFAST_LLM=\"fireworks:accounts/fireworks/models/mixtral-8x7b-instruct\"\nSMART_LLM=\"fireworks:accounts/fireworks/models/mixtral-8x7b-instruct\"\nSTRATEGIC_LLM=\"fireworks:accounts/fireworks/models/mixtral-8x7b-instruct\"\n\nEMBEDDING=\"fireworks:nomic-ai/nomic-embed-text-v1.5\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Singleton Metaclass in Python\nDESCRIPTION: Defines a Singleton metaclass that inherits from both abc.ABCMeta and type. This metaclass ensures that only one instance of any class using it can be created, implementing the Singleton design pattern.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/config/singleton.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Singleton(abc.ABCMeta, type)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Cohere LLMs and Embeddings for GPT Researcher\nDESCRIPTION: This code snippet demonstrates how to configure Cohere LLMs and embeddings for use with GPT Researcher. It includes setting the Cohere API key and specifying the LLM and embedding models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nCOHERE_API_KEY=[Your key]\nFAST_LLM=\"cohere:command\"\nSMART_LLM=\"cohere:command-nightly\"\nSTRATEGIC_LLM=\"cohere:command-nightly\"\n\nEMBEDDING=\"cohere:embed-english-v3.0\"\n```\n\n----------------------------------------\n\nTITLE: Workaround for Mac M Chip Users\nDESCRIPTION: This snippet provides a series of commands to install and set up the necessary environment for running GPT Researcher on Mac M chip systems. It includes installing Python 3.11, required libraries, and running the application.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/troubleshooting.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install python@3.11\nbrew install pango glib gobject-introspection\npip3.11 install -r requirements.txt\npython3.11 -m uvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Setting BeautifulSoup for Static Scraping in GPT Researcher\nDESCRIPTION: Sets the environment variable to use BeautifulSoup for static web scraping. This is the default method and is faster but cannot handle dynamic JavaScript content.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport SCRAPER=\"bs\"\n```\n\n----------------------------------------\n\nTITLE: Initializing LangGraph StateGraph\nDESCRIPTION: This snippet shows how to initialize a StateGraph in LangGraph using the previously defined ResearchState. This creates the foundation for the multi-agent workflow graph.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-05-19-gptr-langgraph/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import StateGraph\nworkflow = StateGraph(ResearchState)\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Agent Tasks with Python's asyncio\nDESCRIPTION: This snippet demonstrates how to use Python's asyncio library to parallelize agent tasks, significantly reducing research time. It creates a list of coroutine tasks for asynchronous browsing and gathers the results as they become available.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-09-22-gpt-researcher/index.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Create a list to hold the coroutine agent tasks\ntasks = [async_browse(url, query, self.websocket) for url in await new_search_urls]\n\n# Gather the results as they become available\nresponses = await asyncio.gather(*tasks, return_exceptions=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Single Web Search Engine in Bash\nDESCRIPTION: Shows how to set a single web search engine (Bing) as the retriever using an environment variable in Bash.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/search-engines/retrievers.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nRETRIEVER=bing\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Logic for API Rate Limits in GPT Researcher\nDESCRIPTION: This Python snippet shows how to handle rate limits with external APIs by implementing retry logic using the tenacity library.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n\n@retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(5))\ndef search_with_retry(query):\n    try:\n        return search_engine.search(query)\n    except RateLimitError:\n        time.sleep(5)\n        raise\n```\n\n----------------------------------------\n\nTITLE: Setting Document Path for Local Research\nDESCRIPTION: Shows how to set the DOC_PATH environment variable for conducting research on local documents.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README-ko_KR.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport DOC_PATH=\"./my-docs\"\n```\n\n----------------------------------------\n\nTITLE: Setting Selenium for Dynamic Browser Scraping in GPT Researcher\nDESCRIPTION: Sets the environment variable to use Selenium for dynamic web scraping. This method can handle JavaScript-rendered content but requires additional setup and resources.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport SCRAPER=\"browser\"\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration for GPT-Researcher\nDESCRIPTION: Example of the necessary environment variables needed for the custom data ingestion process, including API keys for OpenAI and Tavily, as well as the PostgreSQL connection string.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/data-ingestion.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY={Your OpenAI API Key here}\nTAVILY_API_KEY={Your Tavily API Key here}\nPGVECTOR_CONNECTION_STRING=postgresql://username:password...\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT Researcher with Custom Config\nDESCRIPTION: Python code snippet showing how to initialize GPT Researcher with a custom configuration file. This allows for flexible configuration management in different environments.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresearcher = GPTResearcher(query, report_type, config_path=\"your_config.json\")\n```\n\n----------------------------------------\n\nTITLE: Formatting Extracted Hyperlinks for Display in Python\nDESCRIPTION: This function formats a list of hyperlinks for user display. It takes a list of tuples containing hyperlinks and returns a list of formatted strings.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/processing/html.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration File Setup - YAML\nDESCRIPTION: Demonstrates configuration of deep research parameters using a YAML config file format.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/deep_research.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndeep_research_breadth: 4\ndeep_research_depth: 2\ndeep_research_concurrency: 4\ntotal_words: 2500\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation Script in Python\nDESCRIPTION: This command demonstrates how to run the evaluation script. It uses Python to execute run_eval.py with an optional parameter to specify the number of examples to evaluate.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/evals/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython run_eval.py --num_examples <number>\n```\n\n----------------------------------------\n\nTITLE: Installing Selenium for Dynamic Browser Scraping\nDESCRIPTION: Installs the Selenium package required for dynamic browser-based scraping, which can handle JavaScript-rendered content and simulate user interactions.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install selenium\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Chat Models\nDESCRIPTION: Configuration for DeepSeek chat models including API key setup.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nDEEPSEEK_API_KEY=[Your key]\nFAST_LLM=\"deepseek:deepseek-chat\"\nSMART_LLM=\"deepseek:deepseek-chat\"\nSTRATEGIC_LLM=\"deepseek:deepseek-chat\"\n```\n\n----------------------------------------\n\nTITLE: Outline Report Type Example\nDESCRIPTION: Example of how to specify an outline report type query. This report type provides a structured outline for an article or document.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Outline for an article on the impact of AI in education\"\nreport_type = \"outline_report\"\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of GPT Researcher Component in React\nDESCRIPTION: A basic example of how to use the GPTResearcher component in a React application. It demonstrates importing the component, setting up props, and rendering it within a functional component.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/nextjs/README.md#2025-04-21_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport React from 'react';\nimport { GPTResearcher } from 'gpt-researcher-ui';\n\nfunction App() {\n  return (\n    <div className=\"App\">\n      <GPTResearcher \n        apiUrl=\"http://localhost:8000\"\n        defaultPrompt=\"What is quantum computing?\"\n        onResultsChange={(results) => console.log('Research results:', results)}\n      />\n    </div>\n  );\n}\n\nexport default App;\n```\n\n----------------------------------------\n\nTITLE: Customizing Server Configuration for GPT Researcher MCP\nDESCRIPTION: This JSON snippet demonstrates how to create a config.json file to customize server behavior, including host, port, debug mode, timeout, and concurrent request limits.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"host\": \"0.0.0.0\",\n  \"port\": 8000,\n  \"debug\": false,\n  \"timeout\": 300,\n  \"max_concurrent_requests\": 10\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nomic Embedding Model\nDESCRIPTION: Configuration for Nomic's text embedding model.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nEMBEDDING=\"nomic:nomic-embed-text-v1.5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Search Retriever Environment Variables\nDESCRIPTION: Environment variable configuration for setting up Google Search as the retriever in GPT-Researcher. Requires setting RETRIEVER to 'google' and providing Google API key and Custom Search Engine ID.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/filtering-by-domain.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRETRIEVER=google\nGOOGLE_API_KEY=your_google_api_key\nGOOGLE_CX_KEY=your_google_custom_search_engine_id\n```\n\n----------------------------------------\n\nTITLE: Configuring VoyageAI Embedding Model\nDESCRIPTION: Configuration for VoyageAI's law-specific embedding model including API key setup.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nVOYAGE_API_KEY=[Your Key]\nEMBEDDING=\"voyageai:voyage-law-2\"\n```\n\n----------------------------------------\n\nTITLE: Installing and Running GPT Researcher MCP Server with Bash\nDESCRIPTION: Commands for cloning the repository, installing dependencies, setting up environment variables, and running the GPT Researcher MCP server locally.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/claude-integration.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository (if you haven't already)\ngit clone https://github.com/assafelovic/gpt-researcher.git\ncd gpt-researcher/mcp-server\n\n# Install dependencies\npip install -r requirements.txt\n\n# Set up your environment variables\ncp .env.example .env\n# Edit the .env file with your API keys\n\n# Run the server\npython server.py\n```\n\n----------------------------------------\n\nTITLE: Minimal HTML Example for Embedding GPT Researcher\nDESCRIPTION: A complete HTML example demonstrating how to embed GPT Researcher into a web page. This includes setting the API URL and loading the embed script within the body of the HTML document.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/embed-script.md#2025-04-21_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>GPT Researcher Embed Demo</title>\n</head>\n<body style=\"margin: 0; padding: 0;\">\n    <!-- GPT Researcher Embed -->\n    <script>localStorage.setItem(\"GPTR_API_URL\", \"http://localhost:8000\");</script>\n    <script src=\"https://gptr.app/embed.js\"></script>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Running Docker Compose for NextJS Frontend\nDESCRIPTION: This snippet shows how to run the Docker Compose command to build and start the NextJS frontend along with the backend services.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/nextjs-frontend.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily API Key as Environment Variable\nDESCRIPTION: Command to export the Tavily API key as an environment variable. This is required for GPT Researcher to use Tavily's services.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ export TAVILY_API_KEY={Your Tavily API Key here}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for GPT Researcher MCP Server\nDESCRIPTION: This Python snippet sets up detailed logging for the MCP server, including file and stream handlers for comprehensive monitoring.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"mcp_server.log\"),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger(\"mcp_server\")\n```\n\n----------------------------------------\n\nTITLE: Basic GPT Researcher Client Implementation\nDESCRIPTION: Basic example showing how to initialize and use the GPT Researcher client with a simple query and log listener\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/npm/Readme.md#2025-04-21_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst GPTResearcher = require('gpt-researcher');\n\nconst researcher = new GPTResearcher({\n  host: 'http://localhost:8000',\n  logListener: (data) => console.log('logListener logging data: ',data)\n});\n\nresearcher.sendMessage({\n  query: 'Does providing better context reduce LLM hallucinations?'\n});\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys in Python Script\nDESCRIPTION: Python code to set OpenAI and Tavily API keys as environment variables within the script.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/hybrid_research.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'\nos.environ['TAVILY_API_KEY'] = 'your_tavily_api_key_here'\n```\n\n----------------------------------------\n\nTITLE: Research Report Type Example\nDESCRIPTION: Example of how to specify a research report type query. This report type provides a comprehensive analysis of the topic.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Latest developments in renewable energy technologies\"\nreport_type = \"research_report\"\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher Package with PIP\nDESCRIPTION: Simple bash command to install the GPT Researcher package using pip, which provides hybrid research capabilities out of the box.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2024-09-7-hybrid-research/index.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Running GPT Researcher with Docker CLI\nDESCRIPTION: Command for running the GPT Researcher using the Docker CLI directly instead of docker-compose. This includes mounting a local directory for document analysis and setting environment variables.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started-with-docker.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --name gpt-researcher -p 8000:8000 --env-file .env  -v /absolute/path/to/gptr_docs:/my-docs  gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Running the LLM Test Script for GPT-Researcher\nDESCRIPTION: This snippet shows the commands to execute the test-your-llm script, which verifies that GPT-Researcher can successfully connect to and receive responses from the Ollama LLM.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/running-with-ollama.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd tests\npython -m test-your-llm\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher NPM Package\nDESCRIPTION: This command installs the GPT Researcher package from npm. It should be run in the project directory where you want to use the package.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/npm-package.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Scrolling to Page Percentage in Python using WebDriver\nDESCRIPTION: Scrolls a web page to a specified percentage using a WebDriver. Takes a WebDriver instance and a ratio between 0 and 1 representing the percentage. Raises ValueError if the ratio is outside valid range.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/processing/text.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef scroll_to_percentage(driver: WebDriver, ratio: float) -> None\n```\n\n----------------------------------------\n\nTITLE: Installing Tavily Python SDK for Production Scraping\nDESCRIPTION: Installs the Tavily Python SDK required for using the Tavily Extract scraping method, which is recommended for production environments.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install tavily-python\n```\n\n----------------------------------------\n\nTITLE: Running the NextJS Frontend with Docker\nDESCRIPTION: Command to build and start Docker containers for both the Python backend server and NextJS frontend application as configured in the docker-compose file.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/multi_agents/langgraph.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ docker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Generating Quick Research Report (Bash)\nDESCRIPTION: Example command to generate a quick research report on climate change using the GPT Researcher CLI tool.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/cli.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython cli.py \"What are the main causes of climate change?\" --report_type research_report\n```\n\n----------------------------------------\n\nTITLE: Splitting Text into Chunks in Python\nDESCRIPTION: A generator function that splits input text into chunks of a specified maximum length. It yields each chunk one at a time and raises a ValueError if the text exceeds the maximum length.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/processing/text.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef split_text(text: str,\n               max_length: int = 8192) -> Generator[str, None, None]\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher React Package\nDESCRIPTION: Command to install the GPT Researcher UI package from npm using the package manager.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/react-package.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install gpt-researcher-ui\n```\n\n----------------------------------------\n\nTITLE: Defining APIKeyError Exception Class in Python\nDESCRIPTION: Creates a custom exception class APIKeyError for handling cases where API keys are not set in the configuration or environment variables.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/config/config.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass APIKeyError(Exception):\n    \"\"\"Exception raised when an API key is not set in config.py or as an environment variable.\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher as PIP Package\nDESCRIPTION: Command for installing the GPT Researcher package from PyPI using pip.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Running the LangGraph Research Application\nDESCRIPTION: Command to start the LangGraph multi-agent research system after installing dependencies and setting environment variables.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/multi_agents/langgraph.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Enabling HTTPS for GPT Researcher MCP Server\nDESCRIPTION: This bash command shows how to run the MCP server with HTTPS enabled using SSL certificate and key files.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Run with HTTPS\nuvicorn server:app --host 0.0.0.0 --port 8000 --ssl-keyfile=./key.pem --ssl-certfile=./cert.pem\n```\n\n----------------------------------------\n\nTITLE: Configuring LiteLLM with Perplexity Models\nDESCRIPTION: Configuration for LiteLLM using Perplexity's 7B and 70B chat models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nFAST_LLM=\"litellm:perplexity/pplx-7b-chat\"\nSMART_LLM=\"litellm:perplexity/pplx-70b-chat\"\nSTRATEGIC_LLM=\"litellm:perplexity/pplx-70b-chat\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for NextJS Frontend\nDESCRIPTION: Command to install the required dependencies for the NextJS frontend using npm.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/nextjs-frontend.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --legacy-peer-deps\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT Researcher with Azure Storage\nDESCRIPTION: Python code example showing how to initialize GPTResearcher class with Azure as the report source\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/azure-storage.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nreport = GPTResearcher(\n    query=\"What happened in the latest burning man floods?\",\n    report_type=\"research_report\",\n    report_source=\"azure\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher and Required Dependencies\nDESCRIPTION: Installs the latest version of the GPT Researcher library and nest_asyncio, which is required for running async code in Jupyter notebooks.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install -U gpt-researcher nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Running the MCP Server\nDESCRIPTION: Command to start the MCP server. This launches the server which listens for requests from AI assistants to perform research tasks.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/mcp-server/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython server.py\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for GPT Researcher MCP Server Deployment\nDESCRIPTION: This Dockerfile sets up a Python environment, installs dependencies, and configures the server for deployment of the GPT Researcher MCP.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_6\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"server.py\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install all required project dependencies using pip package manager\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Starting NextJS Development Server\nDESCRIPTION: Command to start the NextJS development server for the frontend.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/nextjs-frontend.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Deploying LangGraph with LangGraph CLI\nDESCRIPTION: Commands to install the LangGraph CLI and deploy the research system, enabling access to streaming and async endpoints as well as the playground interface.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/multi_agents/langgraph.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install langgraph-cli\nlanggraph up\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Command to set the OpenAI API key as an environment variable, which is required for GPT Researcher to function properly with OpenAI's language models.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY={Your OpenAI API Key here}\n```\n\n----------------------------------------\n\nTITLE: Starting the FastAPI Server\nDESCRIPTION: Command to start the FastAPI server that serves the static frontend files using uvicorn.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m uvicorn main:app\n```\n\n----------------------------------------\n\nTITLE: Running the NextJS Frontend with NPM\nDESCRIPTION: Series of commands to set up and run the NextJS frontend application using npm, including changing to the frontend directory, setting the Node.js version, installing dependencies, and starting the development server.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/multi_agents/langgraph.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd frontend/nextjs\nnvm install 18.17.0\nnvm use v18.17.0\nnpm install --legacy-peer-deps\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing GPT Researcher (Bash)\nDESCRIPTION: Commands to clone the GPT Researcher repository and install its dependencies. This is the first step in setting up the tool for use.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/cli.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/yourusername/gpt-researcher.git\ncd gpt-researcher\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Starting the FastAPI server for VanillaJS frontend\nDESCRIPTION: Command to start the FastAPI server using uvicorn, which will serve the VanillaJS frontend.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/vanilla-js-frontend.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m uvicorn main:app\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys via Export\nDESCRIPTION: Commands to set up OpenAI and Tavily API keys using environment variables in Linux or temporary Windows setup\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY={Your OpenAI API Key here}\nexport TAVILY_API_KEY={Your Tavily API Key here}\n```\n\n----------------------------------------\n\nTITLE: Installing Flask with Async Support\nDESCRIPTION: Command to install Flask with async support, which is required to use GPT Researcher asynchronously in a Flask application.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install 'flask[async]'\n```\n\n----------------------------------------\n\nTITLE: Cloning the GPT Researcher Repository\nDESCRIPTION: Git command to clone the GPT Researcher repository for the end-to-end application setup. This provides access to both frontend and backend components of the complete research solution.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/how-to-choose.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/assafelovic/gpt-researcher.git\n```\n\n----------------------------------------\n\nTITLE: Alternative Docker Compose Command\nDESCRIPTION: Alternative command to build and run the GPT Researcher application if the hyphenated version doesn't work. Some Docker installations use this syntax instead.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started-with-docker.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Discord Bot\nDESCRIPTION: Environment variables required for Discord bot authentication and configuration.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/discord-bot.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nDISCORD_BOT_TOKEN=\nDISCORD_CLIENT_ID=\n```\n\n----------------------------------------\n\nTITLE: Setting API URL and Loading Embed Script for GPT Researcher\nDESCRIPTION: These script tags set the API URL for GPT Researcher in local storage and load the embed script. The API URL can be customized to point to a different server.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/embed-script.md#2025-04-21_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n<script>localStorage.setItem(\"GPTR_API_URL\", \"http://localhost:8000\");</script>\n<script src=\"https://gptr.app/embed.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher Package\nDESCRIPTION: Command to install the gpt-researcher npm package using npm package manager\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/npm/Readme.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain for GPT Integration\nDESCRIPTION: Installs the LangChain library using pip, which will be used to interface with GPT-4 and process Tavily search results.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/examples.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# install lanchain\n!pip install langchain\n```\n\n----------------------------------------\n\nTITLE: Sample Claude Prompt for Using GPT Researcher Tools\nDESCRIPTION: Example prompt that instructs Claude to use the conduct_research tool provided by the GPT Researcher MCP server integration.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/claude-integration.md#2025-04-21_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nI need to research the latest advancements in quantum computing. Please use the conduct_research tool to gather information, then create a comprehensive report.\n```\n\n----------------------------------------\n\nTITLE: Installing NodeJS and NPM on Ubuntu\nDESCRIPTION: Script to install Node Version Manager (nvm), NodeJS version 18.17.0, and NPM on Ubuntu.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/discord-bot.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n#install nvm\nwget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.4/install.sh | bash\n\nexport NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] && printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n\n# install nodejs\nnvm install 18.17.0\n\n# install npm\nsudo apt-get install npm\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Starting the Server\nDESCRIPTION: Commands for installing Python dependencies and starting the Uvicorn server with hot reloading enabled.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython -m uvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Running Discord Bot with Docker\nDESCRIPTION: Docker command to run the Discord bot using docker-compose with the discord profile.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/discord-bot.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose --profile discord run --rm discord-bot\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher via pip\nDESCRIPTION: Command to install the GPT Researcher package using pip.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/hybrid_research.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GPT Researcher\nDESCRIPTION: Shows the command to install the required Python dependencies for the GPT Researcher project using pip.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README-ko_KR.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Cloning GPT Researcher Repository\nDESCRIPTION: Commands to clone the GPT Researcher project from GitHub and navigate to its directory\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/assafelovic/gpt-researcher.git\n$ cd gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Testing Retrievers with a Mock Researcher in Python\nDESCRIPTION: This script demonstrates how to test the retriever functionality in the GPT-Researcher project. It creates a mock researcher, initializes retrievers based on the current configuration, and tests them with a sample query. The script outputs search results that include title, body, and href for each result.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/search-engines/test-your-retriever.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dotenv import load_dotenv\nfrom gpt_researcher.config.config import Config\nfrom gpt_researcher.actions.retriever import get_retrievers\nfrom gpt_researcher.skills.researcher import ResearchConductor\nimport pprint\n# Load environment variables from .env file\nload_dotenv()\n\nasync def test_scrape_data_by_query():\n    # Initialize the Config object\n    config = Config()\n\n    # Retrieve the retrievers based on the current configuration\n    retrievers = get_retrievers({}, config)\n    print(\"Retrievers:\", retrievers)\n\n    # Create a mock researcher object with necessary attributes\n    class MockResearcher:\n        def init(self):\n            self.retrievers = retrievers\n            self.cfg = config\n            self.verbose = True\n            self.websocket = None\n            self.scraper_manager = None  # Mock or implement scraper manager\n            self.vector_store = None  # Mock or implement vector store\n\n    researcher = MockResearcher()\n    research_conductor = ResearchConductor(researcher)\n    # print('research_conductor',dir(research_conductor))\n    # print('MockResearcher',dir(researcher))\n    # Define a sub-query to test\n    sub_query = \"design patterns for autonomous ai agents\"\n\n    # Iterate through all retrievers\n    for retriever_class in retrievers:\n        # Instantiate the retriever with the sub-query\n        retriever = retriever_class(sub_query)\n\n        # Perform the search using the current retriever\n        search_results = await asyncio.to_thread(\n            retriever.search, max_results=10\n        )\n\n        print(\"\\033[35mSearch results:\\033[0m\")\n        pprint.pprint(search_results, indent=4, width=80)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_scrape_data_by_query())\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT Researcher Parameters in YAML\nDESCRIPTION: YAML configuration options for customizing the GPT Researcher's behavior, including research breadth, depth, concurrency, and word count for the final report.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2025-02-26-deep-research/index.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndeep_research_breadth: 4    # Number of parallel research paths\ndeep_research_depth: 2      # How many levels deep to explore\ndeep_research_concurrency: 4  # Maximum concurrent operations\ntotal_words: 2500           # Word count for final report\n```\n\n----------------------------------------\n\nTITLE: Running GPT Researcher with FastAPI\nDESCRIPTION: Demonstrates how to start the GPT Researcher FastAPI server using uvicorn.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README-ko_KR.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m uvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Storage Dependency\nDESCRIPTION: Required Python package dependency for Azure Blob Storage integration\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/azure-storage.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nazure-storage-blob\n```\n\n----------------------------------------\n\nTITLE: Starting MCP Server with Python\nDESCRIPTION: Command to start the MCP server directly using Python\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/getting-started.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython server.py\n```\n\n----------------------------------------\n\nTITLE: Setting Document Path Environment Variable\nDESCRIPTION: Shows how to set the DOC_PATH environment variable for local document research.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/tailored-research.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport DOC_PATH=\"./my-docs\"\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up GPT Researcher Repository\nDESCRIPTION: Commands for cloning the GPT Researcher repository from GitHub and navigating to the project directory.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/assafelovic/gpt-researcher.git\ncd gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Keys\nDESCRIPTION: This snippet shows the structure of the .env file used to set up necessary API keys for the evaluation process. It includes keys for OpenAI, Tavily, and LangChain.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/evals/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Use the root .env file\nOPENAI_API_KEY=your_openai_key_here\nTAVILY_API_KEY=your_tavily_key_here\nLANGCHAIN_API_KEY=your_langchain_key_here\n```\n\n----------------------------------------\n\nTITLE: Running FastAPI Server\nDESCRIPTION: Command to start the FastAPI application server with auto-reload enabled\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ uvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Deploying Discord Bot Commands\nDESCRIPTION: Command to deploy bot commands including 'ask' and 'ping' to make them available to users.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/discord-bot.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnode deploy-commands.js\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Configuration File\nDESCRIPTION: Command to create a .env file from the example template\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/getting-started.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Setting Document Path Environment Variable\nDESCRIPTION: Command for setting the DOC_PATH environment variable to specify where local documents are stored for research.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport DOC_PATH=\"./my-docs\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Keys\nDESCRIPTION: Commands for exporting OpenAI and Tavily API keys as environment variables for GPT Researcher to use.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY={Your OpenAI API Key here}\nexport TAVILY_API_KEY={Your Tavily API Key here}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies in Virtual Environment\nDESCRIPTION: Command to install project dependencies within the activated virtual environment\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for LangGraph Research System\nDESCRIPTION: Command to install all necessary dependencies for the LangGraph multi-agent research system from the requirements.txt file.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/multi_agents/langgraph.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Poetry Installation and Environment Setup\nDESCRIPTION: Command to install project dependencies and create a Poetry virtual environment\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Example cURL Request to Flask GPT Researcher Endpoint\nDESCRIPTION: Example cURL command to query the Flask application endpoint for GPT Researcher, requesting information about NBA finals predictions.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \"http://localhost:5000/report/research_report?query=what team may win the nba finals?\"\n```\n\n----------------------------------------\n\nTITLE: Running GPT Researcher with Docker Compose\nDESCRIPTION: Commands for building and running the Docker containers defined in docker-compose.yml file.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher via pip\nDESCRIPTION: Command to install the GPT Researcher package using pip. This is the first step in setting up the tool for use in Python projects.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Finalizing Nginx Configuration and Deploying GPT Researcher\nDESCRIPTION: Commands for editing the Nginx configuration file, verifying the configuration, restarting Nginx, and deploying the GPT Researcher application using Docker Compose.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/linux-deployment.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nvim /etc/nginx/nginx.conf\nsudo nginx -t\nsudo systemctl restart nginx\n\ndocker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher UI via npm\nDESCRIPTION: Command to install the GPT Researcher UI package using npm. This is the first step to integrate the component into a React project.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/nextjs/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install gpt-researcher-ui\n```\n\n----------------------------------------\n\nTITLE: Activating Poetry Shell\nDESCRIPTION: Command to activate the Poetry shell and enter the isolated environment\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npoetry shell\n```\n\n----------------------------------------\n\nTITLE: Implementing __call__ Method for Singleton Metaclass\nDESCRIPTION: Overrides the __call__ method in the Singleton metaclass to control object instantiation. This method ensures only one instance of a class is created by returning an existing instance if available.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/config/singleton.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(cls, *args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for GPT Researcher\nDESCRIPTION: Demonstrates how to set the required API keys as environment variables for the GPT Researcher project.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README-ko_KR.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY={OpenAI API 키 입력}\nexport TAVILY_API_KEY={Tavily API 키 입력}\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for GPT-4 Access\nDESCRIPTION: Defines a variable to store the OpenAI API key which will be used for authenticating requests to the GPT-4 model.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/examples.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# set up openai api key\nopenai_api_key = \"\"\n```\n\n----------------------------------------\n\nTITLE: Running Docker Tests for GPT-Researcher\nDESCRIPTION: Command to run the GPT-Researcher tests using Docker Compose. This executes the tests in a dedicated container using the test profile.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/automated-tests.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose --profile test run --rm gpt-researcher-tests\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment\nDESCRIPTION: Command to create a new Python virtual environment using venv module\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv env\n```\n\n----------------------------------------\n\nTITLE: Running App in Virtual Environment\nDESCRIPTION: Command to run the FastAPI application within a virtual environment or Poetry setup\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m uvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Installing GPT Researcher PIP Package\nDESCRIPTION: Command to install the GPT Researcher Python package via pip. This installation method enables easy integration of GPT Researcher into existing Python projects and workflows.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/how-to-choose.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Running GPT Researcher with Docker Compose\nDESCRIPTION: Demonstrates how to start GPT Researcher using Docker Compose.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README-ko_KR.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ docker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for GPT-Researcher Evaluation Results\nDESCRIPTION: Structured markdown document detailing the evaluation results, metrics, and historical context for GPT-Researcher performance testing. Includes performance metrics, cost analysis, and testing methodology information.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/evals/simple_evals/logs/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Evaluation Results\n\nThis directory contains historical evaluation results for GPT-Researcher using the SimpleQA methodology.\n\n## Latest Results\n\n### [SimpleQA Eval 100 Problems 2-22-25](./SimpleQA%20Eval%20100%20Problems%202-22-25.txt)\n\nEvaluation run by [Kelly Abbott (kga245)](https://github.com/kga245)\n\n**Summary:**\n- Date: February 22, 2025\n- Sample Size: 100 queries\n- Success Rate: 100% (100/100 queries completed)\n\n**Performance Metrics:**\n- Accuracy: 92.9%\n- F1 Score: 92.5%\n- Answer Rate: 99%\n\n**Response Distribution:**\n- Correct: 92%\n- Incorrect: 7%\n- Not Attempted: 1%\n\n**Cost Efficiency:**\n- Total Cost: $9.60\n- Average Cost per Query: $0.096\n\nThis evaluation demonstrates strong performance in factual accuracy while maintaining reasonable cost efficiency. The high answer rate (99%) and accuracy (92.9%) suggest that GPT-Researcher is effective at finding and reporting accurate information.\n\n## Historical Context\n\nThese logs are maintained in version control to:\n1. Track performance improvements over time\n2. Provide benchmarks for future enhancements\n3. Enable analysis of different configurations\n4. Ensure transparency in our evaluation process\n\nEach log file contains detailed information about:\n- Individual query results\n- Source citations\n- Cost breakdowns\n- Error analysis\n- Aggregate metrics\n\n## Running New Evaluations\n\nTo generate new evaluation logs, see the [main evaluation documentation](../README.md) for instructions on running evaluations with different configurations or sample sizes.\n```\n\n----------------------------------------\n\nTITLE: Running Flask Server\nDESCRIPTION: Command to start the Flask development server after creating a Flask application with GPT Researcher integration.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nflask run\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GPT Researcher MCP Server\nDESCRIPTION: Command to install the required Python dependencies from the requirements.txt file. This must be completed before running the server.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/mcp-server/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries on Linux\nDESCRIPTION: This snippet demonstrates how to install the required libraries glib and pango on Linux systems using apt-get.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/troubleshooting.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install libglib2.0-dev\nsudo apt install libpango-1.0-0\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Evaluation in Bash\nDESCRIPTION: This bash command installs the required dependencies for running the evaluation framework. It uses pip to install packages listed in the requirements.txt file.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/evals/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd evals/simple_evals\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variables File for API Keys\nDESCRIPTION: Example of creating a .env file with necessary API keys for OpenAI and Tavily services. These credentials are required for the research functionality.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/mcp-server/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=your_openai_api_key\nTAVILY_API_KEY=your_tavily_api_key\n```\n\n----------------------------------------\n\nTITLE: Markdown FAQ Section\nDESCRIPTION: FAQ section written in markdown format covering GPT Researcher's key features, costs, accuracy measures, and future development plans.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/faq.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# FAQ\n\n### How do I get started?\nIt really depends on what you're aiming for. \n\nIf you're looking to connect your AI application to the internet with Tavily tailored API, check out the [Tavily API](https://docs.tavily.com/docs/tavily-api/introductionn) documentation. \nIf you're looking to build and deploy our open source autonomous research agent GPT Researcher, please see [GPT Researcher](/docs/gpt-researcher/getting-started/introduction) documentation.\nYou can also check out demos and examples for inspiration [here](/docs/examples/examples).\n\n### What is GPT Researcher?\n\nGPT Researcher is a popular open source autonomous research agent that takes care of the tedious task of research for you, by scraping, filtering and aggregating over 20+ web sources per a single research task.\n\nGPT Researcher is built with best practices for leveraging LLMs (prompt engineering, RAG, chains, embeddings, etc), and is optimized for quick and efficient research. It is also fully customizable and can be tailored to your specific needs.\n\nTo learn more about GPT Researcher, check out the [documentation page](/docs/gpt-researcher/getting-started/introduction).\n```\n\n----------------------------------------\n\nTITLE: Virtual Environment Activation and Deactivation\nDESCRIPTION: Commands to activate and deactivate the Python virtual environment in Windows\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/getting-started.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n.\\env\\Scripts\\activate\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Adding Google Analytics Measurement ID\nDESCRIPTION: Example of adding the Google Analytics Measurement ID to the .env file for integration with the NextJS frontend.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/nextjs-frontend.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nNEXT_PUBLIC_GA_MEASUREMENT_ID=\"G-G2YVXKHJNZ\"\n```\n\n----------------------------------------\n\nTITLE: Writing Text to File in Python\nDESCRIPTION: A utility function that writes text content to a specified file. Takes a filename and text content as parameters with no return value.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/reference/processing/text.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef write_to_file(filename: str, text: str) -> None\n```\n\n----------------------------------------\n\nTITLE: Initializing Tavily Client with API Key\nDESCRIPTION: Imports the TavilyClient class and initializes a client instance with an API key for authentication with the Tavily API.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/examples.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# import and connect\nfrom tavily import TavilyClient\nclient = TavilyClient(api_key=\"\")\n```\n\n----------------------------------------\n\nTITLE: Starting NextJS Development Server\nDESCRIPTION: Command to start the NextJS development server for local frontend development.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key as Environment Variable\nDESCRIPTION: Command to export the OpenAI API key as an environment variable. This is required for GPT Researcher to authenticate with OpenAI's services.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ export OPENAI_API_KEY={Your OpenAI API Key here}\n```\n\n----------------------------------------\n\nTITLE: Setting up Node.js for NextJS Frontend\nDESCRIPTION: Commands to set up the recommended Node.js version using nvm for the NextJS frontend.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/nextjs-frontend.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnvm install 18.17.0\nnvm use v18.17.0\n```\n\n----------------------------------------\n\nTITLE: Publishing GPT Researcher Package to Private npm Registry\nDESCRIPTION: Commands to build and publish the GPT Researcher package to a private npm registry. This includes steps for building the library, generating type definitions, and publishing.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/react-package.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd frontend/nextjs/\nnpm run build:lib\nnpm run build:types\nnpm publish\n```\n\n----------------------------------------\n\nTITLE: Installing Tavily Python Client\nDESCRIPTION: Installs the Tavily Python client package using pip for integration with the Tavily search API.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/examples/examples.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install tavily\n!pip install tavily-python\n```\n\n----------------------------------------\n\nTITLE: Installing Project Documentation Dependencies\nDESCRIPTION: Commands for installing Python documentation generator (pydoc-markdown) and website dependencies using Yarn.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/README.md#2025-04-21_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install pydoc-markdown\ncd website\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Setting GPTR API URL Environment Variable\nDESCRIPTION: Examples of setting the NEXT_PUBLIC_GPTR_API_URL environment variable in the .env file to specify a custom GPTR API Server URL.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/nextjs-frontend.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nNEXT_PUBLIC_GPTR_API_URL=https://gptr.app\n```\n\nLANGUAGE: bash\nCODE:\n```\nNEXT_PUBLIC_GPTR_API_URL=http://localhost:7000\n```\n\n----------------------------------------\n\nTITLE: Running Local Documentation Development Server\nDESCRIPTION: Commands to generate Python documentation and start the local development server, which opens a browser window to preview the documentation with live updates.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/README.md#2025-04-21_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npydoc-markdown\nyarn start\n```\n\n----------------------------------------\n\nTITLE: Log Data Structure Example\nDESCRIPTION: Example showing the structure of log data received by the logListener callback function\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/npm/Readme.md#2025-04-21_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  type: 'logs',\n  content: string,    // e.g., 'added_source_url', 'researching', 'scraping_content'\n  output: string,     // Human-readable output message\n  metadata: any       // Additional data (URLs, counts, etc.)\n}\n```\n\n----------------------------------------\n\nTITLE: Starting MCP Server with MCP CLI\nDESCRIPTION: Command to start the MCP server using the MCP CLI if installed\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/getting-started.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmcp run server.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx for GPT Researcher\nDESCRIPTION: Nginx configuration file for setting up a reverse proxy to the GPT Researcher application. It includes settings for handling WebSocket connections and file uploads.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/linux-deployment.md#2025-04-21_snippet_3\n\nLANGUAGE: nginx\nCODE:\n```\nevents {}\n\nhttp {\n   server {\n       listen 80;\n       server_name name.example;\n       \n       client_max_body_size 64M;\n\n       location / {\n           proxy_pass http://localhost:3000;\n           proxy_http_version 1.1;\n           proxy_set_header Upgrade $http_upgrade;\n           proxy_set_header Connection 'upgrade';\n           proxy_set_header Host $host;\n           proxy_cache_bypass $http_upgrade;\n       }\n\n       location ~ ^/(ws|upload|files|outputs|getConfig|setConfig) {\n           proxy_pass http://localhost:8000;\n           proxy_http_version 1.1;\n           proxy_set_header Upgrade $http_upgrade;\n           proxy_set_header Connection \"Upgrade\";\n           proxy_set_header Host $host;\n       }\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for FastAPI Static Frontend\nDESCRIPTION: Command to install the necessary Python packages for the FastAPI static frontend option using pip.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Upgrading GPT Researcher PIP Package\nDESCRIPTION: Command to upgrade an existing GPT Researcher PIP installation to the latest version. This ensures access to the most recent features, improvements, and bug fixes.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/how-to-choose.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Cloning the GPT Researcher MCP Server Repository\nDESCRIPTION: Commands to clone the repository and navigate to the project directory. This is the first step in setting up the MCP server locally.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/mcp-server/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/yourusername/researcher-mcp-server.git\ncd researcher-mcp-server\n```\n\n----------------------------------------\n\nTITLE: Navigating to NextJS Directory\nDESCRIPTION: Command to change to the NextJS directory for the second frontend option.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd nextjs\n```\n\n----------------------------------------\n\nTITLE: Running Discord Bot via CLI\nDESCRIPTION: Commands to install dependencies and run the Discord bot locally using npm.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/discord-bot.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# install dependencies\nnpm install\n\n# run the bot\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx with SSL for GPT Researcher\nDESCRIPTION: Nginx configuration file with SSL support for secure deployment of GPT Researcher. It includes settings for SSL certificates managed by Certbot and redirects HTTP to HTTPS.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/linux-deployment.md#2025-04-21_snippet_4\n\nLANGUAGE: nginx\nCODE:\n```\nserver {\n    server_name name.example;\n    \n    client_max_body_size 64M;\n    \n    location / {\n        proxy_pass http://localhost:3000;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n    }\n    \n    location ~ ^/(ws|upload|files|outputs|getConfig|setConfig) {\n        proxy_pass http://localhost:8000;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"Upgrade\";\n        proxy_set_header Host $host;\n    }\n    \n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/name.example/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/name.example/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n}\n\nserver {\n    if ($host = name.example) {\n        return 301 https://$host$request_uri;\n    } # managed by Certbot\n    \n    listen 80;\n    server_name name.example;\n    return 404; # managed by Certbot\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for gpt-researcher\nDESCRIPTION: Specifies the minimum required versions of pandas and tqdm packages. The project requires pandas version 1.5.0 or higher and tqdm version 4.65.0 or higher for proper functionality.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/evals/simple_evals/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npandas>=1.5.0\ntqdm>=4.65.0 \n```\n\n----------------------------------------\n\nTITLE: Installing required packages for VanillaJS frontend with FastAPI\nDESCRIPTION: Command to install the necessary Python packages for the project using pip and the requirements.txt file.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/vanilla-js-frontend.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Alternative Docker Compose Command\nDESCRIPTION: An alternative Docker Compose command without the dash, in case the first command doesn't work.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/frontend/nextjs-frontend.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Setting Up Node.js Environment for NextJS Frontend\nDESCRIPTION: Commands to install and use the recommended Node.js version (18.17.0) using nvm for the NextJS frontend.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnvm install 18.17.0\nnvm use v18.17.0\n```\n\n----------------------------------------\n\nTITLE: Managing Memory for Large Research Tasks in GPT Researcher\nDESCRIPTION: This Python function demonstrates how to force garbage collection to free memory when handling large research tasks in the GPT Researcher.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport gc\n\ndef clean_memory():\n    \"\"\"Force garbage collection to free memory\"\"\"\n    gc.collect()\n```\n\n----------------------------------------\n\nTITLE: Disabling Elestio Authentication in Nginx Configuration\nDESCRIPTION: This snippet shows the Nginx configuration lines that need to be commented out to disable basic authentication when using Ollama deployed on Elestio.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/running-with-ollama.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nauth_basic           \"Authentication\"; \nauth_basic_user_file /etc/nginx/conf.d/.htpasswd;\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries on MacOS\nDESCRIPTION: This snippet shows how to install the necessary libraries glib and pango on MacOS using Homebrew. It also includes a command to link glib if issues persist.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/troubleshooting.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install glib pango\nbrew link glib\n```\n\n----------------------------------------\n\nTITLE: Overview of Research Flow Steps in GPT-Researcher\nDESCRIPTION: The three sequential steps for the research flow in GPT-Researcher, whether using web results or local documents.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/context/data-ingestion.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nStep 1: transform your content (web results or local documents) into Langchain Documents\n```\n\nLANGUAGE: bash\nCODE:\n```\nStep 2: Insert your Langchain Documents into a Langchain VectorStore\n```\n\nLANGUAGE: bash\nCODE:\n```\nStep 3: Pass your Langchain Vectorstore into your GPTR report ([more on that here](https://docs.gptr.dev/docs/gpt-researcher/context/vector-stores) and below)\n```\n\n----------------------------------------\n\nTITLE: Installing NextJS Frontend Dependencies\nDESCRIPTION: Command to install the necessary Node.js dependencies for the NextJS frontend with legacy peer dependencies support.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/frontend/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --legacy-peer-deps\n```\n\n----------------------------------------\n\nTITLE: Markdown Welcome Header and Project Description\nDESCRIPTION: Markdown content for the welcome page that introduces the GPT Researcher project and its mission.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/welcome.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Welcome\n\nHey there! 👋\n\nWe're a team of AI researchers and developers who are passionate about building the next generation of AI assistants. \nOur mission is to empower individuals and organizations with accurate, unbiased, and factual information.\n\n### GPT Researcher\nQuickly accessing relevant and trustworthy information is more crucial than ever. However, we've learned that none of today's search engines provide a suitable tool that provides factual, explicit and objective answers without the need to continuously click and explore multiple sites for a given research task. \n\nThis is why we've built the trending open source **[GPT Researcher](https://github.com/assafelovic/gpt-researcher)**. GPT Researcher is an autonomous agent that takes care of the tedious task of research for you, by scraping, filtering and aggregating over 20+ web sources per a single research task. \n\nTo learn more about GPT Researcher, check out the [documentation page](/docs/gpt-researcher/getting-started/introduction).\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Lists required Python packages for the GPT-Researcher project including web processing (beautifulsoup4), API framework (FastAPI), language models (langchain), document processing (PyMuPDF), and utility libraries. Includes commented testing dependencies (pytest).\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbeautifulsoup4\ncolorama\nmd2pdf\npython-dotenv\npyyaml\nuvicorn\npydantic\nfastapi\npython-multipart\nmarkdown\nlangchain\nlangchain_community\nlangchain-openai\ntiktoken\ngpt-researcher\narxiv\nPyMuPDF\nrequests\njinja2\naiofiles\nmistune\npython-docx\nhtmldocx\nlxml_html_clean\nwebsockets\nunstructured\njson_repair\njson5\nloguru\nlanggraph\n\n# uncomment for testing\n# pytest\n# pytest-asyncio\n```\n\n----------------------------------------\n\nTITLE: Star History Image Loading in Markdown\nDESCRIPTION: HTML/Markdown code for displaying a responsive star history chart with dark/light mode support using picture element and multiple sources.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n<a href=\"https://star-history.com/#assafelovic/gpt-researcher\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date\" />\n  </picture>\n</a>\n</p>\n```\n\n----------------------------------------\n\nTITLE: Verifying Ollama API URL with nslookup\nDESCRIPTION: This snippet shows how to verify that you're pointing to the correct Ollama API URL by performing a DNS lookup using the nslookup command.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/running-with-ollama.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnslookup ollama-2d52b-u21899.vm.elestio.app\n```\n\n----------------------------------------\n\nTITLE: Back to Top Navigation Link\nDESCRIPTION: Simple HTML/Markdown code for adding a 'Back to Top' navigation link aligned to the right.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"right\">\n  <a href=\"#top\">⬆️ Back to Top</a>\n</p>\n```\n\n----------------------------------------\n\nTITLE: Updating System and Installing Git on Ubuntu\nDESCRIPTION: Commands for updating the system package index and installing Git version control system on Ubuntu.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/linux-deployment.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install git -y\n\ngit --version\n```\n\n----------------------------------------\n\nTITLE: Handling Asynchronous Run Status in OpenAI Assistant\nDESCRIPTION: Implements a function to wait for a run to complete by continuously checking its status. This handles the asynchronous nature of the Assistants API by monitoring the run until it reaches a terminal state like 'completed', 'failed', or 'requires_action'.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Function to wait for a run to complete\ndef wait_for_run_completion(thread_id, run_id):\n    while True:\n        time.sleep(1)\n        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n        print(f\"Current run status: {run.status}\")\n        if run.status in ['completed', 'failed', 'requires_action']:\n            return run\n```\n\n----------------------------------------\n\nTITLE: Code of Conduct Full Document in Markdown\nDESCRIPTION: A complete markdown document containing the Contributor Covenant Code of Conduct version 2.0. Includes sections on community pledge, standards, enforcement guidelines and attribution.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/CODE_OF_CONDUCT.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe, as members, contributors, and leaders, pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, sexual identity, or\norientation.\n\nWe commit to acting and interacting in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n[...rest of markdown content omitted for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Alternative Docker Compose Command\nDESCRIPTION: Alternative syntax for running docker compose if the hyphenated version doesn't work.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on Ubuntu\nDESCRIPTION: Series of commands to install Docker on Ubuntu, including adding the official Docker repository, installing prerequisites, and verifying the installation.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/linux-deployment.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install apt-transport-https ca-certificates curl software-properties-common -y\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\nsudo apt update\nsudo apt install docker-ce -y\nsudo systemctl status docker\nsudo usermod -aG docker ${USER}\n```\n\n----------------------------------------\n\nTITLE: Running an OpenAI Assistant on a Thread\nDESCRIPTION: Executes the assistant on a thread to process the user message and potentially trigger function calls. This initiates the assistant's response generation process.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/blog/2023-11-12-openai-assistant/index.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrun = client.beta.threads.runs.create(\n    thread_id=thread.id,\n    assistant_id=assistant_id,\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating GPT Researcher with FastAPI\nDESCRIPTION: Example showing how to integrate GPT Researcher into a FastAPI application. Creates an endpoint that accepts a query and report type to generate and return research results.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/pip-package.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\nfrom gpt_researcher import GPTResearcher\nimport asyncio\n\napp = FastAPI()\n\n@app.get(\"/report/{report_type}\")\nasync def get_report(query: str, report_type: str) -> dict:\n    researcher = GPTResearcher(query, report_type)\n    research_result = await researcher.conduct_research()\n    report = await researcher.write_report()\n    \n    source_urls = researcher.get_source_urls()\n    research_costs = researcher.get_costs()\n    research_images = researcher.get_research_images()\n    research_sources = researcher.get_research_sources()\n    \n    return {\n        \"report\": report,\n        \"source_urls\": source_urls,\n        \"research_costs\": research_costs,\n        \"num_images\": len(research_images),\n        \"num_sources\": len(research_sources)\n    }\n\n# Run the server\n# uvicorn main:app --reload\n```\n\n----------------------------------------\n\nTITLE: Sample Search Results Output in JSON Format\nDESCRIPTION: This JSON snippet demonstrates the expected format of search results from the retriever testing script. Each result object contains a title, body snippet, and href URL of the search result.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/search-engines/test-your-retriever.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{   \n    \"body\": \"Jun 5, 2024 ... Three AI Design Patterns of Autonomous \"\"Agents. Overview of the Three Patterns. Three notable AI \"\"design patterns for autonomous agents include:.\",\n    \"href\": \"https://accredianpublication.medium.com/building-smarter-systems-the-role-of-agentic-design-patterns-in-genai-13617492f5df\",\n    \"title\": \"Building Smarter Systems: The Role of Agentic Design \"\"Patterns in ...\"\n    },\n    ...]\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Access\nDESCRIPTION: Commands to set the required API keys for OpenAI and Tavily as environment variables, which are necessary for the research agents to function.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/multi_agents/langgraph.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY={Your OpenAI API Key here}\nexport TAVILY_API_KEY={Your Tavily API Key here}\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude Desktop with External MCP Server Using JSON\nDESCRIPTION: Alternative JSON configuration for Claude desktop that connects to an already running GPT Researcher MCP server without starting it automatically.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/claude-integration.md#2025-04-21_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {},\n  \"externalMCPServers\": {\n    \"gpt-researcher\": \"http://localhost:8000/mcp\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Yarn Package Manager via NPM on Windows\nDESCRIPTION: Command to install Yarn package manager globally on a Windows system using NPM, which comes bundled with Node.js.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/README.md#2025-04-21_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nnpm install --global yarn\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Web Search Engines in Bash\nDESCRIPTION: Demonstrates how to set multiple web search engines (Tavily and Arxiv) as retrievers using environment variables in Bash.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/search-engines/retrievers.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nRETRIEVER=tavily, arxiv\n```\n\n----------------------------------------\n\nTITLE: Setting NoDriver (ZenDriver) for Dynamic Scraping in GPT Researcher\nDESCRIPTION: Sets the environment variable to use NoDriver (ZenDriver) as an alternative to Selenium for dynamic web scraping with potentially better performance.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/gptr/scraping.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport SCRAPER=\"nodriver\"\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring Nginx on Ubuntu\nDESCRIPTION: Commands for installing Nginx web server on Ubuntu, starting and enabling the service, and verifying the installation.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/getting-started/linux-deployment.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install nginx -y\nsudo systemctl start nginx\nsudo systemctl enable nginx\nsudo systemctl status nginx\n```\n\n----------------------------------------\n\nTITLE: Configuring xAI Grok Beta Model\nDESCRIPTION: Configuration for xAI's Grok Beta model across all LLM types.\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/llms/llms.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nFAST_LLM=\"xai:grok-beta\"\nSMART_LLM=\"xai:grok-beta\"\nSTRATEGIC_LLM=\"xai:grok-beta\"\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration\nDESCRIPTION: Example of environment variables that need to be configured in the .env file\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/getting-started.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=your_openai_api_key\nTAVILY_API_KEY=your_tavily_api_key\n```\n\n----------------------------------------\n\nTITLE: Cloning GPT Researcher Repository\nDESCRIPTION: Commands to clone the GPT Researcher repository and navigate to the project directory\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/getting-started.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/assafelovic/gpt-researcher.git\ncd gpt-researcher\n```\n\n----------------------------------------\n\nTITLE: Installing MCP Server Dependencies\nDESCRIPTION: Commands to navigate to the MCP server directory and install required dependencies\nSOURCE: https://github.com/assafelovic/gpt-researcher/blob/master/docs/docs/gpt-researcher/mcp-server/getting-started.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd mcp-server\npip install -r requirements.txt\n```"
  }
]
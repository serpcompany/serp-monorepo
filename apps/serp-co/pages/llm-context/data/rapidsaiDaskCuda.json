[
  {
    "owner": "rapidsai",
    "repo": "dask-cuda",
    "content": "TITLE: Creating Dask-CUDA Cluster with LocalCUDACluster in Python\nDESCRIPTION: This snippet demonstrates how to create a Dask-CUDA cluster using LocalCUDACluster and connect a Dask.distributed Client to it. It utilizes all available GPUs.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/quickstart.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_cuda import LocalCUDACluster\nfrom dask.distributed import Client\n\ncluster = LocalCUDACluster()\nclient = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalCUDACluster with RMM Pool and UCX\nDESCRIPTION: Shows how to set up a LocalCUDACluster with UCX protocol and RMM memory pool configuration for optimal performance in multi-GPU environments.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/best-practices.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"0,1\",\n                               protocol=\"ucx\",\n                               rmm_pool_size=\"30GB\")\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalCUDACluster with Multiple GPUs\nDESCRIPTION: Demonstrates how to initialize a LocalCUDACluster with different GPU configurations, showing both default GPU selection and explicit GPU device specification.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/best-practices.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster(n_workers=2)                                # will use GPUs 0,1\ncluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"3,4\")                 # will use GPUs 3,4\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalCUDACluster with Dask Client in Python\nDESCRIPTION: Basic example showing how to create a local CUDA cluster and initialize a Dask client. This setup enables distributed computing on CUDA-enabled systems.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_cuda import LocalCUDACluster\nfrom dask.distributed import Client\n\ncluster = LocalCUDACluster()\nclient = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Installing Dask-CUDA with conda\nDESCRIPTION: Command to install Dask-CUDA using conda package manager. This installs the latest version along with CUDA Toolkit 12.8 from the rapidsai, conda-forge, and nvidia channels.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/install.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c rapidsai -c conda-forge -c nvidia dask-cuda cuda-version=12.8\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalCUDACluster with Manual UCX Configuration in Python\nDESCRIPTION: This code shows how to connect a client to a cluster with manually configured UCX, enabling all supported transports and an RMM pool. It specifies various UCX-related parameters and RMM pool size.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster(\n    protocol=\"ucx\",\n    interface=\"ib0\",\n    enable_tcp_over_ucx=True,\n    enable_nvlink=True,\n    enable_infiniband=True,\n    enable_rdmacm=True,\n    rmm_pool_size=\"1GB\"\n)\nclient = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalCUDACluster with Automatic UCX Configuration in Python\nDESCRIPTION: This snippet demonstrates how to connect a client to a cluster with automatically-configured UCX and an RMM pool using LocalCUDACluster. It specifies the UCX protocol, network interface, and RMM pool size.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster(\n    protocol=\"ucx\",\n    interface=\"ib0\",\n    rmm_pool_size=\"1GB\"\n)\nclient = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Starting Dask Scheduler and CUDA Worker from Command Line\nDESCRIPTION: This bash snippet shows how to start a Dask scheduler and connect a CUDA worker to it using the command line. This method creates a cluster equivalent to using LocalCUDACluster.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/quickstart.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ dask scheduler\ndistributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n\n$ dask cuda worker 127.0.0.1:8786\n```\n\n----------------------------------------\n\nTITLE: Configuring Device Memory Limit in LocalCUDACluster\nDESCRIPTION: Examples of setting device_memory_limit in LocalCUDACluster to control when spilling from GPU to host memory occurs. The parameter accepts an integer, string memory size, or float representing percentage of GPU memory.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster(device_memory_limit=50000)  # spilling after 50000 bytes\ncluster = LocalCUDACluster(device_memory_limit=\"5GB\")  # spilling after 5 GB\ncluster = LocalCUDACluster(device_memory_limit=0.3)    # spilling after 30% memory utilization\n```\n\n----------------------------------------\n\nTITLE: Specifying GPU Devices for Dask-CUDA Cluster\nDESCRIPTION: Python code to create a LocalCUDACluster with specific GPU devices. The example demonstrates how to handle the DGX Station A100 where the display GPU is commonly the fourth in the PCI Bus ID ordering.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/troubleshooting.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from dask_cuda import LocalCUDACluster\n>>> cluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=[0, 1, 2, 4])\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalCUDACluster with Specific GPUs\nDESCRIPTION: Creates a LocalCUDACluster instance restricted to specific GPUs using CUDA_VISIBLE_DEVICES parameter. Shows how to select GPUs by their indices.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/worker_count.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"0,1\")\n```\n\n----------------------------------------\n\nTITLE: Initializing UCX-Enabled Dask Cluster in Python\nDESCRIPTION: Example of initializing a local Dask cluster with UCX protocol and setting up the environment to avoid CUDA context conflicts.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/ucx.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['RAPIDS_NO_INITIALIZE'] = '1'\nos.environ['DASK_DISTRIBUTED__COMM__UCX__CREATE_CUDA_CONTEXT'] = 'True'\n\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster(protocol=\"ucx\")\n```\n\n----------------------------------------\n\nTITLE: Connecting Dask Client with Manual UCX Configuration in Python\nDESCRIPTION: This Python code connects a client to a cluster with manual UCX configuration. It uses dask_cuda.initialize to set up UCX configuration options before creating the client.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\nfrom dask_cuda.initialize import initialize\n\ninitialize(\n    enable_tcp_over_ucx=True,\n    enable_nvlink=True,\n    enable_infiniband=True,\n    enable_rdmacm=True,\n)\nclient = Client(\"ucx://<scheduler_address>:8786\")\n```\n\n----------------------------------------\n\nTITLE: Starting Dask CUDA Workers with Manual UCX Configuration in Bash\nDESCRIPTION: This bash command starts Dask CUDA workers with manual UCX configuration, enabling all supported transports and a 1GB RMM pool. It specifies various UCX-related options and the scheduler address.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ dask cuda worker ucx://<scheduler_address>:8786 \\\n> --enable-tcp-over-ucx \\\n> --enable-nvlink \\\n> --enable-infiniband \\\n> --enable-rdmacm \\\n> --rmm-pool-size=\"1GB\"\n```\n\n----------------------------------------\n\nTITLE: Using GPU UUIDs with LocalCUDACluster\nDESCRIPTION: Demonstrates how to initialize LocalCUDACluster using GPU UUIDs instead of index numbers.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/worker_count.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"GPU-dae76d0e-3414-958a-8f3e-fc6682b36f31\")\n```\n\n----------------------------------------\n\nTITLE: Connecting Dask Client with Automatic UCX Configuration in Python\nDESCRIPTION: This Python code connects a client to a cluster with automatic UCX configuration. It sets the UCX memory type registration environment variable and creates a CUDA context before initializing the client.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"UCX_MEMTYPE_REG_WHOLE_ALLOC_TYPES\"] = \"cuda\"\n\nimport dask\nfrom dask.distributed import Client\n\nwith dask.config.set({\"distributed.comm.ucx.create_cuda_context\": True}):\n    client = Client(\"ucx://<scheduler_address>:8786\")\n```\n\n----------------------------------------\n\nTITLE: Starting Dask CUDA Workers with Automatic UCX Configuration in Bash\nDESCRIPTION: This bash command starts Dask CUDA workers with automatic UCX configuration and a 14GB RMM pool per GPU. It sets the UCX memory type registration and specifies the scheduler address and network interface.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ UCX_MEMTYPE_REG_WHOLE_ALLOC_TYPES=cuda\n> dask cuda worker ucx://<scheduler_address>:8786 \\\n> --rmm-pool-size=\"14GB\" \\\n> --interface=\"ib0\"\n```\n\n----------------------------------------\n\nTITLE: Starting Dask Scheduler with Automatic UCX Configuration in Bash\nDESCRIPTION: This bash command starts a Dask scheduler using UCX with automatic configuration and 1GB of RMM pool. It sets necessary environment variables and specifies the UCX protocol and network interface.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ DASK_DISTRIBUTED__COMM__UCX__CREATE_CUDA_CONTEXT=True \\\n> DASK_DISTRIBUTED__RMM__POOL_SIZE=1GB \\\n> dask scheduler --protocol ucx --interface ib0\n```\n\n----------------------------------------\n\nTITLE: Enabling cuDF Spilling in LocalCUDACluster\nDESCRIPTION: Example of enabling native cuDF spilling in LocalCUDACluster using the enable_cudf_spill parameter, which allows spilling individual device buffers during task execution.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from distributed import Client​\n>>> from dask_cuda import LocalCUDACluster​\n\n>>> cluster = LocalCUDACluster(n_workers=10, enable_cudf_spill=True)​\n>>> client = Client(cluster)​\n```\n\n----------------------------------------\n\nTITLE: Enabling JIT-Unspill in LocalCUDACluster\nDESCRIPTION: Examples of enabling JIT-Unspill in LocalCUDACluster either directly via constructor parameter or using dask configuration context manager. JIT-Unspill addresses issues with regular spilling like object duplication and wrong spilling order.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import dask​\n>>> from distributed import Client​\n>>> from dask_cuda import LocalCUDACluster​\n\n>>> cluster = LocalCUDACluster(n_workers=10, device_memory_limit=\"1GB\", jit_unspill=True)​\n>>> client = Client(cluster)​\n\n>>> with dask.config.set(jit_unspill=True):​\n...   cluster = LocalCUDACluster(n_workers=10, device_memory_limit=\"1GB\")​\n...   client = Client(cluster)​\n```\n\n----------------------------------------\n\nTITLE: Configuring Worker Count with LocalCUDACluster\nDESCRIPTION: Demonstrates how to limit the number of GPU workers using n_workers parameter, either alone or in combination with CUDA_VISIBLE_DEVICES.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/worker_count.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncluster = LocalCUDACluster(n_workers=2)                                # will use GPUs 0,1\ncluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"3,4,5\", n_workers=2)  # will use GPUs 3,4\n```\n\n----------------------------------------\n\nTITLE: Connecting Python Client to Command Line Dask-CUDA Cluster\nDESCRIPTION: This Python snippet demonstrates how to connect a Dask distributed Client to a Dask-CUDA cluster that was created using the command line method.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/quickstart.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dask.distributed import Client\n\nclient = Client(\"127.0.0.1:8786\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dask-CUDA with pip\nDESCRIPTION: Command to install Dask-CUDA using pip package manager. This installs the latest version after the CUDA Toolkit has been installed separately from NVIDIA's website.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/install.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install dask-cuda\n```\n\n----------------------------------------\n\nTITLE: Starting Dask Scheduler with Manual UCX Configuration in Bash\nDESCRIPTION: This bash command starts a Dask scheduler using UCX with manual configuration for all supported transports and a 1GB RMM pool. It sets various UCX-related environment variables and specifies the UCX protocol and network interface.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/ucx.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ DASK_DISTRIBUTED__COMM__UCX__CUDA_COPY=True \\\n> DASK_DISTRIBUTED__COMM__UCX__TCP=True \\\n> DASK_DISTRIBUTED__COMM__UCX__NVLINK=True \\\n> DASK_DISTRIBUTED__COMM__UCX__INFINIBAND=True \\\n> DASK_DISTRIBUTED__COMM__UCX__RDMACM=True \\\n> DASK_DISTRIBUTED__RMM__POOL_SIZE=1GB \\\n> dask scheduler --protocol ucx --interface ib0\n```\n\n----------------------------------------\n\nTITLE: Starting Dask CUDA Workers via Command Line\nDESCRIPTION: Shows how to start Dask CUDA workers from command line with specific GPU visibility settings using environment variables.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/worker_count.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ dask scheduler\ndistributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n\n$ CUDA_VISIBLE_DEVICES=0,1 dask cuda worker 127.0.0.1:8786\n```\n\n----------------------------------------\n\nTITLE: Configuring Device Memory Limit in dask cuda worker CLI\nDESCRIPTION: Command line examples for controlling memory spilling in dask cuda worker using the --device-memory-limit parameter with different value formats.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ dask scheduler\ndistributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n\n$ dask cuda worker --device-memory-limit 50000\n$ dask cuda worker --device-memory-limit 5GB\n$ dask cuda worker --device-memory-limit 0.3\n$ dask cuda worker --device-memory-limit 0\n```\n\n----------------------------------------\n\nTITLE: Enabling cuDF Spill Statistics\nDESCRIPTION: Examples of enabling statistics collection for cuDF spilling to monitor performance, with both LocalCUDACluster and command line approaches.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> cluster = LocalCUDACluster(n_workers=10, enable_cudf_spill=True, cudf_spill_stats=1)​\n```\n\n----------------------------------------\n\nTITLE: Accessing cuDF Spill Statistics\nDESCRIPTION: Example function to retrieve and print cuDF spill statistics from a running worker, which can be submitted to a Dask client for execution.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef spill_info():\n    from cudf.core.buffer.spill_manager import get_global_manager\n    print(get_global_manager().statistics)\nclient.submit(spill_info)\n```\n\n----------------------------------------\n\nTITLE: Enabling cuDF Spilling via Command Line\nDESCRIPTION: Command line example for enabling native cuDF spilling in dask cuda worker using the --enable-cudf-spill parameter.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ dask scheduler\ndistributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n\n$ dask cuda worker --enable-cudf-spill\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Device Order in Bash Environment\nDESCRIPTION: Commands for setting the CUDA_DEVICE_ORDER environment variable to PCI_BUS_ID to ensure proper GPU device indexing. This is especially important for systems like DGX Station A100 where the display GPU may not be the last GPU according to PCI Bus ID.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/troubleshooting.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ CUDA_DEVICE_ORDER=PCI_BUS_ID python\n$ CUDA_DEVICE_ORDER=PCI_BUS_ID ipython\n$ CUDA_DEVICE_ORDER=PCI_BUS_ID jupyter lab\n$ CUDA_DEVICE_ORDER=PCI_BUS_ID dask-cuda-worker ...\n```\n\n----------------------------------------\n\nTITLE: Starting Workers with GPU UUID via Command Line\nDESCRIPTION: Shows how to start Dask CUDA workers from command line using GPU UUIDs for device selection.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/worker_count.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ CUDA_VISIBLE_DEVICES=\"GPU-dae76d0e-3414-958a-8f3e-fc6682b36f31\" \\\n> dask cuda worker 127.0.0.1:8786\n```\n\n----------------------------------------\n\nTITLE: Installing Dask-CUDA from source\nDESCRIPTION: Commands to clone the Dask-CUDA repository from GitHub and install it from source. This method allows for installation of the latest development version or specific branches.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/install.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/rapidsai/dask-cuda.git\ncd dask-cuda\npython -m pip install .\n```\n\n----------------------------------------\n\nTITLE: Disabling Memory Spilling in LocalCUDACluster\nDESCRIPTION: Example of disabling memory spilling in LocalCUDACluster by setting device_memory_limit to 0.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncluster = LocalCUDACluster(device_memory_limit=0)  # spilling disabled\n```\n\n----------------------------------------\n\nTITLE: Enabling cuDF Spill Statistics via Command Line\nDESCRIPTION: Command line example for enabling cuDF spill statistics in dask cuda worker.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ dask cuda worker --enable-cudf-spill --cudf-spill-stats 1\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiprocessing Method for Fork-Starved Environments in Python\nDESCRIPTION: Setup for running Dask-CUDA with UCX in a fork-starved environment, using the forkserver method and ensuring proper initialization.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/ucx.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dask\n\nif __name__ == \"__main__\":\n    import multiprocessing.forkserver as f\n    f.ensure_running()\n    with dask.config.set(\n        {\"distributed.worker.multiprocessing-method\": \"forkserver\"}\n    ):\n        run_analysis(...)\n```\n\n----------------------------------------\n\nTITLE: Listing GPU UUIDs with nvidia-smi\nDESCRIPTION: Shows how to query GPU UUIDs using the NVIDIA System Management Interface (nvidia-smi) command.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/examples/worker_count.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ nvidia-smi -L\nGPU 0: Tesla V100-SXM2-32GB (UUID: GPU-dae76d0e-3414-958a-8f3e-fc6682b36f31)\nGPU 1: Tesla V100-SXM2-32GB (UUID: GPU-60f2c95a-c564-a078-2a14-b4ff488806ca)\n```\n\n----------------------------------------\n\nTITLE: Enabling JIT-Unspill via Command Line\nDESCRIPTION: Command line example for enabling JIT-Unspill in dask cuda worker using the --enable-jit-unspill parameter.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ dask scheduler\ndistributed.scheduler - INFO - Scheduler at:  tcp://127.0.0.1:8786\n\n$ dask cuda worker --enable-jit-unspill​\n```\n\n----------------------------------------\n\nTITLE: Enabling JIT-Unspill via Environment Variable\nDESCRIPTION: Example of enabling JIT-Unspill in dask cuda worker using the DASK_JIT_UNSPILL environment variable.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ dask scheduler\ndistributed.scheduler - INFO -   Scheduler at:  tcp://127.0.0.1:8786\n\n$ DASK_JIT_UNSPILL=True dask cuda worker​\n```\n\n----------------------------------------\n\nTITLE: JIT-Unspill Type Checking Limitations\nDESCRIPTION: Example demonstrating type checking limitations with ProxyObject in JIT-Unspill. While isinstance() works correctly, direct type comparison does not.\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/spilling.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from dask_cuda.proxy_object import asproxy\n>>> x = np.arange(3)\n>>> isinstance(asproxy(x), type(x))\nTrue\n>>>  type(asproxy(x)) is type(x)\nFalse\n```\n\n----------------------------------------\n\nTITLE: GPU CPU Affinity Diagnostic Script\nDESCRIPTION: Python script to print CPU affinity for each GPU in the system. This helps diagnose CPU affinity issues by showing which CPUs are associated with each GPU according to NVIDIA's management library (NVML).\nSOURCE: https://github.com/rapidsai/dask-cuda/blob/branch-25.06/docs/source/troubleshooting.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# print_affinity.py\nimport math\nfrom multiprocessing import cpu_count\n\nimport pynvml\n\npynvml.nvmlInit()\nfor i in range(pynvml.nvmlDeviceGetCount()):\n    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n    cpu_affinity = pynvml.nvmlDeviceGetCpuAffinity(handle, math.ceil(cpu_count() / 64))\n    print(f\"GPU {i}: list(cpu_affinity)\")\n```"
  }
]
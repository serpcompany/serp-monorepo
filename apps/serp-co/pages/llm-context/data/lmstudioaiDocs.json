[
  {
    "owner": "lmstudio-ai",
    "repo": "docs",
    "content": "TITLE: Implementing Multi-turn Chatbot in Python\nDESCRIPTION: Complete example showing how to implement an interactive multi-turn chatbot with streaming responses.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm()\nchat = lms.Chat(\"You are a task focused AI assistant\")\n\nwhile True:\n    try:\n        user_input = input(\"You (leave blank to exit): \")\n    except EOFError:\n        print()\n        break\n    if not user_input:\n        break\n    chat.add_user_message(user_input)\n    prediction_stream = model.respond_stream(\n        chat,\n        on_message=chat.append,\n    )\n    print(\"Bot: \", end=\"\", flush=True)\n    for fragment in prediction_stream:\n        print(fragment.content, end=\"\", flush=True)\n    print()\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completions with POST Request\nDESCRIPTION: Shows how to use the chat completions API to generate responses based on a conversation history. This example includes system and user messages with parameters for controlling the generation process.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"granite-3.0-2b-instruct\",\n    \"messages\": [\n      { \"role\": \"system\", \"content\": \"Always answer in rhymes.\" },\n      { \"role\": \"user\", \"content\": \"Introduce yourself.\" }\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": -1,\n    \"stream\": false\n  }'\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Responses in Python\nDESCRIPTION: Shows how to stream AI responses in real-time using both API styles. Streams text fragments as they are generated rather than waiting for complete response.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nmodel = lms.llm()\n\nfor fragment in model.respond_stream(\"What is the meaning of life?\"):\n    print(fragment.content, end=\"\", flush=True)\nprint()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    model = client.llm.model()\n\n    for fragment in model.respond_stream(\"What is the meaning of life?\"):\n        print(fragment.content, end=\"\", flush=True)\n    print()\n```\n\n----------------------------------------\n\nTITLE: Chat Completions API Response Format in JSON\nDESCRIPTION: Displays the response format for chat completions. The response includes the generated message, usage statistics, performance metrics like tokens per second and time to first token, and detailed model information.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"chatcmpl-i3gkjwthhw96whukek9tz\",\n  \"object\": \"chat.completion\",\n  \"created\": 1731990317,\n  \"model\": \"granite-3.0-2b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Greetings, I'm a helpful AI, here to assist,\\nIn providing answers, with no distress.\\nI'll keep it short and sweet, in rhyme you'll find,\\nA friendly companion, all day long you'll bind.\"\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 24,\n    \"completion_tokens\": 53,\n    \"total_tokens\": 77\n  },\n  \"stats\": {\n    \"tokens_per_second\": 51.43709529007664,\n    \"time_to_first_token\": 0.111,\n    \"generation_time\": 0.954,\n    \"stop_reason\": \"eosFound\"\n  },\n  \"model_info\": {\n    \"arch\": \"granite\",\n    \"quant\": \"Q4_K_M\",\n    \"format\": \"gguf\",\n    \"context_length\": 4096\n  },\n  \"runtime\": {\n    \"name\": \"llama.cpp-mac-arm64-apple-metal-advsimd\",\n    \"version\": \"1.3.0\",\n    \"supported_formats\": [\"gguf\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Tool-Enhanced Streaming Chatbot in Python with LM Studio API\nDESCRIPTION: Complete Python implementation of a chatbot that uses streaming responses with tool calls. The code demonstrates how to connect to the LM Studio API, define tools, process streaming responses, handle tool calls, and maintain conversation state.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport time\n\nclient = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\nMODEL = \"lmstudio-community/qwen2.5-7b-instruct\"\n\nTIME_TOOL = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_time\",\n        \"description\": \"Get the current time, only if asked\",\n        \"parameters\": {\"type\": \"object\", \"properties\": {}},\n    },\n}\n\ndef get_current_time():\n    return {\"time\": time.strftime(\"%H:%M:%S\")}\n\ndef process_stream(stream, add_assistant_label=True):\n    \"\"\"Handle streaming responses from the API\"\"\"\n    collected_text = \"\"\n    tool_calls = []\n    first_chunk = True\n\n    for chunk in stream:\n        delta = chunk.choices[0].delta\n\n        # Handle regular text output\n        if delta.content:\n            if first_chunk:\n                print()\n                if add_assistant_label:\n                    print(\"Assistant:\", end=\" \", flush=True)\n                first_chunk = False\n            print(delta.content, end=\"\", flush=True)\n            collected_text += delta.content\n\n        # Handle tool calls\n        elif delta.tool_calls:\n            for tc in delta.tool_calls:\n                if len(tool_calls) <= tc.index:\n                    tool_calls.append({\n                        \"id\": \"\", \"type\": \"function\",\n                        \"function\": {\"name\": \"\", \"arguments\": \"\"}\n                    })\n                tool_calls[tc.index] = {\n                    \"id\": (tool_calls[tc.index][\"id\"] + (tc.id or \"\")),\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": (tool_calls[tc.index][\"function\"][\"name\"] + (tc.function.name or \"\")),\n                        \"arguments\": (tool_calls[tc.index][\"function\"][\"arguments\"] + (tc.function.arguments or \"\"))\n                    }\n                }\n    return collected_text, tool_calls\n\ndef chat_loop():\n    messages = []\n    print(\"Assistant: Hi! I am an AI agent empowered with the ability to tell the current time (Type 'quit' to exit)\")\n\n    while True:\n        user_input = input(\"\\nYou: \").strip()\n        if user_input.lower() == \"quit\":\n            break\n\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n        # Get initial response\n        response_text, tool_calls = process_stream(\n            client.chat.completions.create(\n                model=MODEL,\n                messages=messages,\n                tools=[TIME_TOOL],\n                stream=True,\n                temperature=0.2\n            )\n        )\n\n        if not tool_calls:\n            print()\n\n        text_in_first_response = len(response_text) > 0\n        if text_in_first_response:\n            messages.append({\"role\": \"assistant\", \"content\": response_text})\n\n        # Handle tool calls if any\n        if tool_calls:\n            tool_name = tool_calls[0][\"function\"][\"name\"]\n            print()\n            if not text_in_first_response:\n                print(\"Assistant:\", end=\" \", flush=True)\n            print(f\"**Calling Tool: {tool_name}**\")\n            messages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\n\n            # Execute tool calls\n            for tool_call in tool_calls:\n                if tool_call[\"function\"][\"name\"] == \"get_current_time\":\n                    result = get_current_time()\n                    messages.append({\n                        \"role\": \"tool\",\n                        \"content\": str(result),\n                        \"tool_call_id\": tool_call[\"id\"]\n                    })\n\n            # Get final response after tool execution\n            final_response, _ = process_stream(\n                client.chat.completions.create(\n                    model=MODEL,\n                    messages=messages,\n                    stream=True\n                ),\n                add_assistant_label=False\n            )\n\n            if final_response:\n                print()\n                messages.append({\"role\": \"assistant\", \"content\": final_response})\n\nif __name__ == \"__main__\":\n    chat_loop()\n```\n\n----------------------------------------\n\nTITLE: Downloading Models with LM Studio SDK (Python)\nDESCRIPTION: Demonstrates how to search for, select, and download models using the LM Studio SDK in Python. The example shows searching for a Llama 3.2 1B model, filtering by compatibility type, finding appropriate quantization options, and downloading the chosen model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/_download-models.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\"\n\nconst client = new LMStudioClient()\n\n# 1. Search for the model you want\n# Specify any/all of searchTerm, limit, compatibilityTypes\nconst searchResults = client.repository.searchModels({\n  searchTerm: \"llama 3.2 1b\",    # Search for Llama 3.2 1B\n  limit: 5,                      # Get top 5 results\n  compatibilityTypes: [\"gguf\"],  # Only download GGUFs\n})\n\n# 2. Find download options\nconst bestResult = searchResults[0];\nconst downloadOptions = bestResult.getDownloadOptions()\n\n# Let's download Q4_K_M, a good middle ground quantization\nconst desiredModel = downloadOptions.find(option => option.quantization === 'Q4_K_M')\n\n# 3. Download it!\nconst modelKey = desiredModel.download()\n\n# This returns a path you can use to load the model\nconst loadedModel = client.llm.model(modelKey)\n```\n\n----------------------------------------\n\nTITLE: Configuring Inference Parameters in Python\nDESCRIPTION: Demonstrates how to customize inference parameters like temperature and token limits for both streaming and non-streaming responses.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprediction_stream = model.respond_stream(chat, config={\n    \"temperature\": 0.6,\n    \"maxTokens\": 50,\n})\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = model.respond(chat, config={\n    \"temperature\": 0.6,\n    \"maxTokens\": 50,\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Streaming Speculative Decoding with lmstudio-js\nDESCRIPTION: This snippet demonstrates how to use speculative decoding with a draft model in a non-streaming context. It initializes the LMStudioClient, sets up main and draft models, and performs a prediction with speculative decoding. The result includes the generated content and statistics on accepted draft tokens.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/speculative-decoding.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\n\nconst mainModelKey = \"qwen2.5-7b-instruct\";\nconst draftModelKey = \"qwen2.5-0.5b-instruct\";\n\nconst model = await client.llm.model(mainModelKey);\nconst result = await model.respond(\"What are the prime numbers between 0 and 100?\", {\n  draftModel: draftModelKey,\n});\n\nconst { content, stats } = result;\nconsole.info(content);\nconsole.info(`Accepted ${stats.acceptedDraftTokensCount}/${stats.predictedTokensCount} tokens`);\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Responses\nDESCRIPTION: Shows how to generate responses in both streaming and non-streaming modes using the respond() method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/chat-completion.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// The `chat` object is created in the previous step.\nconst prediction = model.respond(chat);\n\nfor await (const { content } of prediction) {\n  process.stdout.write(content);\n}\n\nconsole.info(); // Write a new line to prevent text from being overwritten by your shell.\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// The `chat` object is created in the previous step.\nconst result = await model.respond(chat);\n\nconsole.info(result.content);\n```\n\n----------------------------------------\n\nTITLE: Setting Inference Parameters with respond() and complete()\nDESCRIPTION: Examples of setting inference-time parameters like temperature and maxTokens when making predictions using both respond() and complete() methods.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/parameters.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.respond(chat, {\n  temperature: 0.6,\n  maxTokens: 50,\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.complete(prompt, {\n  temperature: 0.6,\n  maxTokens: 50,\n  stop: [\"\\n\\n\"],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Speculative Decoding with lmstudio-js\nDESCRIPTION: This snippet shows how to use speculative decoding with a draft model in a streaming context. It sets up the LMStudioClient, initializes main and draft models, and performs a streaming prediction. The code demonstrates how to process the streamed content and retrieve statistics on accepted draft tokens.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/speculative-decoding.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\n\nconst mainModelKey = \"qwen2.5-7b-instruct\";\nconst draftModelKey = \"qwen2.5-0.5b-instruct\";\n\nconst model = await client.llm.model(mainModelKey);\nconst prediction = model.respond(\"What are the prime numbers between 0 and 100?\", {\n  draftModel: draftModelKey,\n});\n\nfor await (const { content } of prediction) {\n  process.stdout.write(content);\n}\nprocess.stdout.write(\"\\n\");\n\nconst { stats } = await prediction.result();\nconsole.info(`Accepted ${stats.acceptedDraftTokensCount}/${stats.predictedTokensCount} tokens`);\n```\n\n----------------------------------------\n\nTITLE: Generating Text Completions with LM Studio\nDESCRIPTION: Demonstrates how to generate completions in both non-streaming and streaming modes using a loaded model. Non-streaming returns the complete result at once, while streaming yields text fragments as they are generated.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/completion.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The `chat` object is created in the previous step.\nresult = model.complete(\"My name is\", config={\"maxTokens\": 100})\n\nprint(result)\n```\n\nLANGUAGE: python\nCODE:\n```\n# The `chat` object is created in the previous step.\nprediction_stream = model.complete_stream(\"My name is\", config={\"maxTokens\": 100})\n\nfor fragment in prediction_stream:\n    print(fragment.content, end=\"\", flush=True)\nprint() # Advance to a new line at the end of the response\n```\n\n----------------------------------------\n\nTITLE: Retrieving Model Information in LM Studio using Python APIs\nDESCRIPTION: Demonstrates two methods to access model metadata and information from a loaded model instance: using the convenience API or the scoped resource API. This functionality works with both LLM and embedding models, returning detailed information about the model architecture, context length, format, and other properties.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/6_model-info/get-model-info.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm()\n\nprint(model.get_info())\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model()\n\n    print(model.get_info())\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Responses Using JSON Schema in Python\nDESCRIPTION: Illustrates how to use a JSON schema to generate structured responses from a language model. Includes examples for both non-streaming and streaming approaches.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/structured-response.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult = model.respond(\"Tell me about The Hobbit\", response_format=schema)\nbook = result.parsed\n\nprint(book)\n#     ^\n# Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\nLANGUAGE: python\nCODE:\n```\nprediction_stream = model.respond_stream(\"Tell me about The Hobbit\", response_format=schema)\n\n# Stream the response\nfor fragment in prediction:\n    print(fragment.content, end=\"\", flush=True)\nprint()\n# Note that even for structured responses, the *fragment* contents are still only text\n\n# Get the final structured result\nresult = prediction_stream.result()\nbook = result.parsed\n\nprint(book)\n#     ^\n# Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Responses with Streaming Options\nDESCRIPTION: Shows how to generate responses using both streaming and non-streaming approaches with existing chat context.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# The `chat` object is created in the previous step.\nresult = model.respond(chat)\n\nprint(result)\n```\n\nLANGUAGE: python\nCODE:\n```\n# The `chat` object is created in the previous step.\nprediction_stream = model.respond_stream(chat)\n\nfor fragment in prediction_stream:\n    print(fragment.content, end=\"\", flush=True)\nprint()\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Tools with LLM for Prime Number Calculation in Python\nDESCRIPTION: This example shows how to provide multiple tools (addition and prime number checking) in a single .act() call. It uses the Qwen2.5-7B-Instruct model to determine if the sum of two numbers is prime.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/2_agent/act.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport lmstudio as lms\n\ndef add(a: int, b: int) -> int:\n    \"\"\"Given two numbers a and b, returns the sum of them.\"\"\"\n    return a + b\n\ndef is_prime(n: int) -> bool:\n    \"\"\"Given a number n, returns True if n is a prime number.\"\"\"\n    if n < 2:\n        return False\n    sqrt = int(math.sqrt(n))\n    for i in range(2, sqrt):\n        if n % i == 0:\n            return False\n    return True\n\nmodel = lms.llm(\"qwen2.5-7b-instruct\")\nmodel.act(\n  \"Is the result of 12345 + 45668 a prime? Think step by step.\",\n  [add, is_prime],\n  on_message=print,\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Multiplication Tool Implementation with LMStudio .act() API\nDESCRIPTION: Demonstrates a simple example of using the `.act()` API with a multiplication tool. The code creates a tool that multiplies two numbers and lets the LLM autonomously decide when to use it to solve a mathematical problem.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_agent/act.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient, tool } from \"@lmstudio/sdk\";\nimport { z } from \"zod\";\n\nconst client = new LMStudioClient();\n\nconst multiplyTool = tool({\n  name: \"multiply\",\n  description: \"Given two numbers a and b. Returns the product of them.\",\n  parameters: { a: z.number(), b: z.number() },\n  implementation: ({ a, b }) => a * b,\n});\n\nconst model = await client.llm.model(\"qwen2.5-7b-instruct\");\nawait model.act(\"What is the result of 12345 multiplied by 54321?\", [multiplyTool], {\n  onMessage: (message) => console.info(message.toString()),\n});\n```\n\n----------------------------------------\n\nTITLE: Managing Chat Context in Python\nDESCRIPTION: Shows different ways to create and manage chat context, including creating new chats and building from existing history.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\n# Create a chat with an initial system prompt.\nchat = lms.Chat(\"You are a resident AI philosopher.\")\n\n# Build the chat context by adding messages of relevant types.\nchat.add_user_message(\"What is the meaning of life?\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\n# Create a chat object from a chat history dict\nchat = lms.Chat.from_history({\n    \"messages\": [\n        { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n        { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n    ]\n})\n```\n\n----------------------------------------\n\nTITLE: Generating Streaming Structured Response with JSON Schema in TypeScript\nDESCRIPTION: Shows how to stream a structured response using a JSON schema, displaying content in real-time and parsing the final result after completion.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/structured-response.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.respond(\"Tell me about The Hobbit.\", {\n  structured: {\n    type: \"json\",\n    jsonSchema: schema,\n  },\n  maxTokens: 100, // Recommended to avoid getting stuck\n});\n\nfor await (const { content } of prediction) {\n  process.stdout.write(content);\n}\nprocess.stdout.write(\"\\n\");\n\nconst result = await prediction.result();\nconst book = JSON.parse(result.content);\n\nconsole.info(\"Parsed\", book);\n```\n\n----------------------------------------\n\nTITLE: Interactive Chat Loop with File Creation Tool\nDESCRIPTION: Implements a conversation loop with an LLM agent that can create files on the local machine. This example shows how to maintain chat context while allowing the LLM to create files based on user requests, with real-time response streaming.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_agent/act.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chat, LMStudioClient, tool } from \"@lmstudio/sdk\";\nimport { existsSync } from \"fs\";\nimport { writeFile } from \"fs/promises\";\nimport { createInterface } from \"readline/promises\";\nimport { z } from \"zod\";\n\nconst rl = createInterface({ input: process.stdin, output: process.stdout });\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\nconst chat = Chat.empty();\n\nconst createFileTool = tool({\n  name: \"createFile\",\n  description: \"Create a file with the given name and content.\",\n  parameters: { name: z.string(), content: z.string() },\n  implementation: async ({ name, content }) => {\n    if (existsSync(name)) {\n      return \"Error: File already exists.\";\n    }\n    await writeFile(name, content, \"utf-8\");\n    return \"File created.\";\n  },\n});\n\nwhile (true) {\n  const input = await rl.question(\"You: \");\n  // Append the user input to the chat\n  chat.append(\"user\", input);\n\n  process.stdout.write(\"Bot: \");\n  await model.act(chat, [createFileTool], {\n    // When the model finish the entire message, push it to the chat\n    onMessage: (message) => chat.append(message),\n    onPredictionFragment: ({ content }) => {\n      process.stdout.write(content);\n    },\n  });\n  process.stdout.write(\"\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Tool in LMStudio SDK with TypeScript\nDESCRIPTION: Demonstrates how to define a basic mathematical tool using the `tool()` function from the LMStudio SDK. This example creates an addition tool that takes two numbers as parameters and returns their sum. It uses Zod for parameter validation.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_agent/tools.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from \"@lmstudio/sdk\";\nimport { z } from \"zod\";\n\nconst exampleTool = tool({\n  // The name of the tool\n  name: \"add\",\n\n  // A description of the tool\n  description: \"Given two numbers a and b. Returns the sum of them.\",\n\n  // zod schema of the parameters\n  parameters: { a: z.number(), b: z.number() },\n\n  // The implementation of the tool. Just a regular function.\n  implementation: ({ a, b }) => a + b,\n});\n```\n\n----------------------------------------\n\nTITLE: Chat with Llama Model using Convenience API\nDESCRIPTION: Example demonstrating how to load a Llama 3.2 1B model and query it using the convenience API. This approach uses a default client instance for interactive use in Python prompts or Jupyter notebooks.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm(\"llama-3.2-1b-instruct\")\nresult = model.respond(\"What is the meaning of life?\")\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Passing Images to a VLM in LM Studio\nDESCRIPTION: Complete example of passing an image to a Vision-Language Model using the .respond() method in LM Studio. Shows how to create a chat with an image and get a prediction about it.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/image-input.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nimage_path = \"/path/to/image.jpg\" # Replace with the path to your image\nimage_handle = lms.prepare_image(image_path)\nmodel = lms.llm(\"qwen2-vl-2b-instruct\")\nchat = lms.Chat()\nchat.add_user_message(\"Describe this image please\", images=[image_handle])\nprediction = model.respond(chat)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n    image_handle = client.files.prepare_image(image_path)\n    model = client.llm.model(\"qwen2-vl-2b-instruct\")\n    chat = lms.Chat()\n    chat.add_user_message(\"Describe this image please\", images=[image_handle])\n    prediction = model.respond(chat)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Terminal Simulator using LMStudio SDK Text Completions\nDESCRIPTION: This example creates an interactive terminal simulator that uses the LMStudio SDK to process user commands with a language model. It maintains a history of interactions and streams the model's responses to simulate terminal output.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/completion.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nimport { createInterface } from \"node:readline/promises\";\n\nconst rl = createInterface({ input: process.stdin, output: process.stdout });\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\nlet history = \"\";\n\nwhile (true) {\n  const command = await rl.question(\"$ \");\n  history += \"$ \" + command + \"\\n\";\n\n  const prediction = model.complete(history, { stopStrings: [\"$\"] });\n  for await (const { content } of prediction) {\n    process.stdout.write(content);\n  }\n  process.stdout.write(\"\\n\");\n\n  const { content } = await prediction.result();\n  history += content;\n}\n```\n\n----------------------------------------\n\nTITLE: Chat with Llama Model using Scoped Resource API\nDESCRIPTION: Example showing how to load a Llama 3.2 1B model and query it using the scoped resource API. This approach uses context managers to ensure deterministic resource cleanup.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model(\"llama-3.2-1b-instruct\")\n    result = model.respond(\"What is the meaning of life?\")\n\n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-turn Chat\nDESCRIPTION: Demonstrates a complete example of implementing an interactive multi-turn chat conversation with the model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/chat-completion.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chat, LMStudioClient } from \"@lmstudio/sdk\";\nimport { createInterface } from \"readline/promises\";\n\nconst rl = createInterface({ input: process.stdin, output: process.stdout });\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\nconst chat = Chat.empty();\n\nwhile (true) {\n  const input = await rl.question(\"You: \");\n  // Append the user input to the chat\n  chat.append(\"user\", input);\n\n  const prediction = model.respond(chat, {\n    // When the model finish the entire message, push it to the chat\n    onMessage: (message) => chat.append(message),\n  });\n  process.stdout.write(\"Bot: \");\n  for await (const { content } of prediction) {\n    process.stdout.write(content);\n  }\n  process.stdout.write(\"\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Model Download with Progress Callbacks (Python with scoped resources)\nDESCRIPTION: Shows how to implement progress callbacks when downloading models with the LM Studio SDK using Python with scoped resources. Demonstrates tracking download progress with byte counts and speed reporting, as well as download finalization notification.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/_download-models.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\ndef print_progress_update(update: lmstudio.DownloadProgressUpdate) -> None:\n    print(f\"Downloaded {update.downloaded_bytes} bytes of {update.total_bytes} total \\\n            at {update.speed_bytes_per_second} bytes/sec\")\n\nwith lms.Client() as client:\n    # ... Same code as before ...\n\n    model_key = desired_model.download(\n        on_progress=print_progress_update,\n        on_finalize: lambda: print(\"Finalizing download...\")\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Responses Using Class-Based Schema in Python\nDESCRIPTION: Shows how to use the defined BookSchema to generate structured responses from a language model. Includes examples for both non-streaming and streaming approaches.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/structured-response.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = model.respond(\"Tell me about The Hobbit\", response_format=BookSchema)\nbook = result.parsed\n\nprint(book)\n#           ^\n# Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\nLANGUAGE: python\nCODE:\n```\nprediction_stream = model.respond_stream(\"Tell me about The Hobbit\", response_format=BookSchema)\n\n# Optionally stream the response\n# for fragment in prediction:\n#   print(fragment.content, end=\"\", flush=True)\n# print()\n# Note that even for structured responses, the *fragment* contents are still only text\n\n# Get the final structured result\nresult = prediction_stream.result()\nbook = result.parsed\n\nprint(book)\n#           ^\n# Note that `book` is correctly typed as { title: string, author: string, year: number }\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Loop for Agent Interaction in Python\nDESCRIPTION: This section implements the main chat loop that allows the user to interact with the agent. It processes user input, sends requests to the model, handles function calls, and displays responses.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant with the ability to perform various tasks using the provided tools.\"},\n]\n\nprint(\"Chat with the AI assistant. Type 'quit' to exit.\")\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'quit':\n        break\n\n    messages.append({\"role\": \"user\", \"content\": user_input})\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            tools=tools,\n        )\n\n        assistant_message = response.choices[0].message\n\n        if assistant_message.tool_calls:\n            for tool_call in assistant_message.tool_calls:\n                function_name = tool_call.function.name\n                function_args = json.loads(tool_call.function.arguments)\n\n                if function_name == \"open_url\":\n                    result = open_url(**function_args)\n                elif function_name == \"get_current_time\":\n                    result = get_current_time()\n                elif function_name == \"analyze_directory\":\n                    result = analyze_directory(**function_args)\n                else:\n                    result = f\"Error: Unknown function {function_name}\"\n\n                messages.append(\n                    {\n                        \"role\": \"function\",\n                        \"name\": function_name,\n                        \"content\": result,\n                    }\n                )\n\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n            )\n            assistant_message = response.choices[0].message\n\n        messages.append({\"role\": \"assistant\", \"content\": assistant_message.content})\n        print(f\"Assistant: {assistant_message.content}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\nprint(\"Chat ended.\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Model for Text Completions in LM Studio\nDESCRIPTION: Shows two different approaches to load a model: using the top-level convenience API or the scoped resource API. The example uses Qwen2.5 7B Instruct model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/completion.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nmodel = lms.llm(\"qwen2.5-7b-instruct\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    model = client.llm.model(\"qwen2.5-7b-instruct\")\n```\n\n----------------------------------------\n\nTITLE: Setting Load Parameters with model()\nDESCRIPTION: Examples of setting load-time parameters using the model() method, which retrieves or loads models on demand. Parameters include contextLength and gpuOffload settings.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/parameters.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nmodel = lms.llm(\"qwen2.5-7b-instruct\", config={\n    \"contextLength\": 8192,\n    \"gpuOffload\": 0.5,\n})\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    model = client.llm.model(\n        \"qwen2.5-7b-instruct\",\n        config={\n            \"contextLength\": 8192,\n            \"gpuOffload\": 0.5,\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Speculative Decoding with Streaming API in Python\nDESCRIPTION: This snippet shows how to use speculative decoding with a draft model in lmstudio-python using the streaming API. It sets up the main and draft models, performs a streaming prediction with speculative decoding, and prints the results in real-time.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/speculative-decoding.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmain_model_key = \"qwen2.5-7b-instruct\"\ndraft_model_key = \"qwen2.5-0.5b-instruct\"\n\nmodel = lms.llm(main_model_key)\nprediction_stream = model.respond_stream(\n    \"What are the prime numbers between 0 and 100?\",\n    config={\n        \"draftModel\": draft_model_key,\n    }\n)\nfor fragment in prediction_stream:\n    print(fragment.content, end=\"\", flush=True)\nprint() # Advance to a new line at the end of the response\n\nstats = prediction_stream.result().stats\nprint(f\"Accepted {stats.accepted_draft_tokens_count}/{stats.predicted_tokens_count} tokens\")\n```\n\n----------------------------------------\n\nTITLE: Multiple Tools Integration with Addition and Prime Number Checking\nDESCRIPTION: Shows how to provide multiple tools in a single `.act()` call. This example implements an addition tool and a prime number checking tool, allowing the LLM to solve a complex mathematical problem by combining both tools.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_agent/act.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient, tool } from \"@lmstudio/sdk\";\nimport { z } from \"zod\";\n\nconst client = new LMStudioClient();\n\nconst additionTool = tool({\n  name: \"add\",\n  description: \"Given two numbers a and b. Returns the sum of them.\",\n  parameters: { a: z.number(), b: z.number() },\n  implementation: ({ a, b }) => a + b,\n});\n\nconst isPrimeTool = tool({\n  name: \"isPrime\",\n  description: \"Given a number n. Returns true if n is a prime number.\",\n  parameters: { n: z.number() },\n  implementation: ({ n }) => {\n    if (n < 2) return false;\n    const sqrt = Math.sqrt(n);\n    for (let i = 2; i <= sqrt; i++) {\n      if (n % i === 0) return false;\n    }\n    return true;\n  },\n});\n\nconst model = await client.llm.model(\"qwen2.5-7b-instruct\");\nawait model.act(\n  \"Is the result of 12345 + 45668 a prime? Think step by step.\",\n  [additionTool, isPrimeTool],\n  { onMessage: (message) => console.info(message.toString()) },\n);\n```\n\n----------------------------------------\n\nTITLE: Representing Chats with Array of Messages in TypeScript\nDESCRIPTION: Demonstrates how to use an array of messages to represent a chat conversation, including examples for text-only and image-inclusive interactions using the .respond() method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/working-with-chats.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.respond([\n  { role: \"system\", content: \"You are a resident AI philosopher.\" },\n  { role: \"user\", content: \"What is the meaning of life?\" },\n]);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst image = await client.files.prepareImage(\"/path/to/image.jpg\");\n\nconst prediction = model.respond([\n  { role: \"system\", content: \"You are a state-of-art object recognition system.\" },\n  { role: \"user\", content: \"What is this object?\", images: [image] },\n]);\n```\n\n----------------------------------------\n\nTITLE: Using Chat Helper Class in Python with LMStudio SDK\nDESCRIPTION: This example shows how to use the Chat helper class for more complex tasks. It initializes a chat instance with a system prompt and adds a user message.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/working-with-chats.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nchat = Chat(\"You are a resident AI philosopher.\")\nchat.add_user_message(\"What is the meaning of life?\")\n\nprediction = llm.respond(chat)\n```\n\n----------------------------------------\n\nTITLE: Generating a Prediction with Image Input in TypeScript\nDESCRIPTION: This code shows how to generate a prediction by passing an image to a Vision-Language Model using the `.respond()` method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/image-input.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.respond([\n  { role: \"user\", content: \"Describe this image please\", images: [image] },\n]);\n```\n\n----------------------------------------\n\nTITLE: Generating Basic Chat Response in Python\nDESCRIPTION: Demonstrates how to obtain a simple chat response using both convenience and scoped resource APIs. Shows basic model initialization and response generation.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nmodel = lms.llm()\nprint(model.respond(\"What is the meaning of life?\"))\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    model = client.llm.model()\n    print(model.respond(\"What is the meaning of life?\"))\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from listDownloadedModels\nDESCRIPTION: This JSON shows the expected output format when listing models, which includes detailed metadata for each model like type, format, size, architecture and capabilities. The example shows both LLM and embedding model types.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/list-downloaded.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"type\": \"llm\",\n    \"modelKey\": \"qwen2.5-7b-instruct\",\n    \"format\": \"gguf\",\n    \"displayName\": \"Qwen2.5 7B Instruct\",\n    \"path\": \"lmstudio-community/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n    \"sizeBytes\": 4683073952,\n    \"paramsString\": \"7B\",\n    \"architecture\": \"qwen2\",\n    \"vision\": false,\n    \"trainedForToolUse\": true,\n    \"maxContextLength\": 32768\n  },\n  {\n    \"type\": \"embedding\",\n    \"modelKey\": \"text-embedding-nomic-embed-text-v1.5@q4_k_m\",\n    \"format\": \"gguf\",\n    \"displayName\": \"Nomic Embed Text v1.5\",\n    \"path\": \"nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q4_K_M.gguf\",\n    \"sizeBytes\": 84106624,\n    \"architecture\": \"nomic-bert\",\n    \"maxContextLength\": 2048\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with Python\nDESCRIPTION: Example showing how to generate text embeddings using the LMStudio Python API. Demonstrates loading an embedding model and converting a text string into a vector representation.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/3_embedding/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.embedding_model(\"nomic-embed-text-v1.5\")\n\nembedding = model.embed(\"Hello, world!\")\n```\n\n----------------------------------------\n\nTITLE: Generating Non-Streaming Structured Response with JSON Schema in TypeScript\nDESCRIPTION: Demonstrates how to use a JSON schema with model.respond() to enforce a structured response about a book. The result needs to be parsed with JSON.parse().\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/structured-response.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await model.respond(\"Tell me about The Hobbit.\", {\n  structured: {\n    type: \"json\",\n    jsonSchema: schema,\n  },\n  maxTokens: 100, // Recommended to avoid getting stuck\n});\n\nconst book = JSON.parse(result.content);\nconsole.info(book);\n```\n\n----------------------------------------\n\nTITLE: Using Chat Helper Class for Complex Tasks in TypeScript\nDESCRIPTION: Illustrates the use of the Chat helper class for managing more complex chat interactions, including examples for text-only and image-inclusive conversations.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/working-with-chats.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst chat = Chat.empty();\nchat.append(\"system\", \"You are a resident AI philosopher.\");\nchat.append(\"user\", \"What is the meaning of life?\");\n\nconst prediction = model.respond(chat);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst image = await client.files.prepareImage(\"/path/to/image.jpg\");\n\nconst chat = Chat.empty();\nchat.append(\"system\", \"You are a state-of-art object recognition system.\");\nchat.append(\"user\", \"What is this object?\", { images: [image] });\n\nconst prediction = model.respond(chat);\n```\n\n----------------------------------------\n\nTITLE: Constructing Chat Object from History in Python with LMStudio SDK\nDESCRIPTION: This snippet demonstrates two ways to construct a Chat object using the Chat.from_history method: one with a full chat history and another with a single string.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/working-with-chats.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nchat = Chat.from_history({\"messages\": [\n  { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n  { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n]})\n\n# This constructs a chat with a single user message\nchat = Chat.from_history(\"What is the meaning of life?\")\n```\n\n----------------------------------------\n\nTITLE: Setting Inference Parameters with respond() and complete()\nDESCRIPTION: Examples of setting inference-time parameters like temperature and maxTokens using both respond() and complete() methods. These parameters can be configured per request.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/parameters.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nresult = model.respond(chat, config={\n    \"temperature\": 0.6,\n    \"maxTokens\": 50,\n})\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = model.complete(chat, config={\n    \"temperature\": 0.6,\n    \"maxTokens\": 50,\n    \"stop\": [\"\\n\\n\"],\n  })\n```\n\n----------------------------------------\n\nTITLE: Defining Tools Using Python Functions in LMStudio SDK\nDESCRIPTION: Three approaches to define tools for LLMs in LMStudio: using type-hinted functions directly, converting functions with poor naming/documentation using ToolFunctionDef.from_callable, or explicitly defining parameters with ToolFunctionDef. Each approach allows passing the tool definition to the act() method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/2_agent/tools.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Type hinted functions with clear names and docstrings\n# may be used directly as tool definitions\ndef add(a: int, b: int) -> int:\n    \"\"\"Given two numbers a and b, returns the sum of them.\"\"\"\n    # The SDK ensures arguments are coerced to their specified types\n    return a + b\n\n# Pass `add` directly to `act()` as a tool definition\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lmstudio import ToolFunctionDef\n\ndef cryptic_name(a: int, b: int) -> int:\n    return a + b\n\n# Type hinted functions with cryptic names and missing or poor docstrings\n# can be turned into clear tool definitions with `from_callable`\ntool_def = ToolFunctionDef.from_callable(\n  cryptic_name,\n  name=\"add\",\n  description=\"Given two numbers a and b, returns the sum of them.\"\n)\n# Pass `tool_def` to `act()` as a tool definition\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lmstudio import ToolFunctionDef\n\ndef cryptic_name(a, b):\n    return a + b\n\n# Functions without type hints can be used without wrapping them\n# at runtime by defining a tool function directly.\ntool_def = ToolFunctionDef(\n  name=\"add\",\n  description=\"Given two numbers a and b, returns the sum of them.\",\n  parameters={\n    \"a\": int,\n    \"b\": int,\n  },\n  implementation=cryptic_name,\n)\n# Pass `tool_def` to `act()` as a tool definition\n```\n\n----------------------------------------\n\nTITLE: Generating Streaming Text Completions with LMStudio SDK\nDESCRIPTION: This snippet shows how to generate text completions in streaming mode, where tokens are processed and displayed as they are generated, with a maximum token limit of 100.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/completion.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst completion = model.complete(\"My name is\", {\n  maxTokens: 100,\n});\n\nfor await (const { content } of completion) {\n  process.stdout.write(content);\n}\n\nconsole.info(); // Write a new line for cosmetic purposes\n```\n\n----------------------------------------\n\nTITLE: Using a File Creation Tool with LMStudio Model\nDESCRIPTION: Example code demonstrating how to use the create_file tool with an LLM in LMStudio. It imports the tool function, initializes a Qwen 2.5 7B model, and prompts it to create a file with its understanding of the meaning of life.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/2_agent/tools.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nfrom create_file_tool import create_file\n\nmodel = lms.llm(\"qwen2.5-7b-instruct\")\nmodel.act(\n  \"Please create a file named output.txt with your understanding of the meaning of life.\",\n  [create_file],\n)\n```\n\n----------------------------------------\n\nTITLE: Chat Implementation with Llama Model\nDESCRIPTION: Example TypeScript code demonstrating how to initialize the LM Studio client, load a Llama model, and perform a basic chat interaction.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/index.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model(\"llama-3.2-1b-instruct\");\nconst result = await model.respond(\"What is the meaning of life?\");\n\nconsole.info(result.content);\n```\n\n----------------------------------------\n\nTITLE: Defining Class-Based Schema for Structured Responses in Python\nDESCRIPTION: Demonstrates how to define a class-based schema for a book using either Pydantic's BaseModel or LMStudio's custom BaseModel. This schema enforces a structure for the model's output.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/structured-response.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\n# A class based schema for a book\nclass BookSchema(BaseModel):\n    title: str\n    author: str\n    year: int\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lmstudio import BaseModel\n\n# A class based schema for a book\nclass BookSchema(BaseModel):\n    title: str\n    author: str\n    year: int\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Model in Python with LM Studio SDK\nDESCRIPTION: Shows how to load or access a specific model by providing its key. This method will load the model if it's not already loaded, or return the existing instance if it is.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/loading.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm(\"llama-3.2-1b-instruct\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model(\"llama-3.2-1b-instruct\")\n```\n\n----------------------------------------\n\nTITLE: Using Chat Completions API with Python OpenAI Client in LM Studio\nDESCRIPTION: An example demonstrating how to create a chat completion using the OpenAI Python client with LM Studio. Shows how to set up system and user messages, and adjust temperature.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n# Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"model-identifier\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Checking Chat Context Fit with LM Studio SDK in Python\nDESCRIPTION: This example demonstrates how to determine if a given conversation fits into a model's context. It converts the conversation to a string using the prompt template, counts the tokens, and compares it to the model's context length.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/4_tokenization/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\ndef does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -> bool:\n    # Convert the conversation to a string using the prompt template.\n    formatted = model.apply_prompt_template(chat)\n    # Count the number of tokens in the string.\n    token_count = len(model.tokenize(formatted))\n    # Get the current loaded context length of the model\n    context_length = model.get_context_length()\n    return token_count < context_length\n\nmodel = lms.llm()\n\nchat = lms.Chat.from_history({\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"What is the meaning of life.\" },\n        { \"role\": \"assistant\", \"content\": \"The meaning of life is...\" },\n        # ... More messages\n    ]\n})\n\nprint(\"Fits in context:\", does_chat_fit_in_context(model, chat))\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Client and Defining Tools for LM Studio in Python\nDESCRIPTION: This snippet sets up the OpenAI client to use a local LM Studio server and defines the tools (functions) that the model can use, including open_url, get_current_time, and analyze_directory.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\nmodel = \"lmstudio-community/qwen2.5-7b-instruct\"\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"open_url\",\n            \"description\": \"Opens a URL in the default web browser. Only use for safe, user-requested URLs.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"url\": {\"type\": \"string\", \"description\": \"The URL to open.\"}\n                },\n                \"required\": [\"url\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_time\",\n            \"description\": \"Returns the current date and time.\",\n            \"parameters\": {\"type\": \"object\", \"properties\": {}}\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_directory\",\n            \"description\": \"Analyzes the contents of a directory and returns statistics.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"path\": {\"type\": \"string\", \"description\": \"The path to the directory to analyze.\"}\n                },\n                \"required\": [\"path\"]\n            }\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Managing Chat Context\nDESCRIPTION: Demonstrates two approaches to managing chat context: using an array of messages or constructing a Chat object incrementally.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/chat-completion.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chat } from \"@lmstudio/sdk\";\n\n// Create a chat object from an array of messages.\nconst chat = Chat.from([\n  { role: \"system\", content: \"You are a resident AI philosopher.\" },\n  { role: \"user\", content: \"What is the meaning of life?\" },\n]);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chat } from \"@lmstudio/sdk\";\n\n// Create an empty chat object.\nconst chat = Chat.empty();\n\n// Build the chat context by appending messages.\nchat.append(\"system\", \"You are a resident AI philosopher.\");\nchat.append(\"user\", \"What is the meaning of life?\");\n```\n\n----------------------------------------\n\nTITLE: Loading New Model Instances with LM Studio SDK in TypeScript\nDESCRIPTION: This snippet illustrates how to load new instances of models using the .load() method, allowing multiple instances of the same or different models to be loaded simultaneously.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/loading.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst llama = await client.llm.load(\"llama-3.2-1b-instruct\");\nconst another_llama = await client.llm.load(\"llama-3.2-1b-instruct\", {\n  identifier: \"second-llama\"\n});\n```\n\n----------------------------------------\n\nTITLE: Multiplying Numbers with LLM and Custom Function in Python\nDESCRIPTION: This snippet demonstrates how to use the .act() API with a custom multiplication function. It uses the Qwen2.5-7B-Instruct model to perform a multiplication task.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/2_agent/act.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\ndef multiply(a: float, b: float) -> float:\n    \"\"\"Given two numbers a and b. Returns the product of them.\"\"\"\n    return a * b\n\nmodel = lms.llm(\"qwen2.5-7b-instruct\")\nmodel.act(\n  \"What is the result of 12345 multiplied by 54321?\",\n  [multiply],\n  on_message=print,\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Prediction Statistics in LM Studio\nDESCRIPTION: Shows how to retrieve and display metadata about a model's completion, including the model name, token count, latency metrics, and stop reason for both streaming and non-streaming approaches.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/completion.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# `result` is the response from the model.\nprint(\"Model used:\", result.model_info.display_name)\nprint(\"Predicted tokens:\", result.stats.predicted_tokens_count)\nprint(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\nprint(\"Stop reason:\", result.stats.stop_reason)\n```\n\nLANGUAGE: python\nCODE:\n```\n# After iterating through the prediction fragments,\n# the overall prediction result may be obtained from the stream\nresult = prediction_stream.result()\n\nprint(\"Model used:\", result.model_info.display_name)\nprint(\"Predicted tokens:\", result.stats.predicted_tokens_count)\nprint(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\nprint(\"Stop reason:\", result.stats.stop_reason)\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Loop with File Creation Tool using LLM in Python\nDESCRIPTION: This code creates an interactive conversation loop with an LLM agent that can create files. It demonstrates how to use the .act() API in a chat context with a custom file creation tool.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/2_agent/act.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport readline # Enables input line editing\nfrom pathlib import Path\n\nimport lmstudio as lms\n\ndef create_file(name: str, content: str):\n    \"\"\"Create a file with the given name and content.\"\"\"\n    dest_path = Path(name)\n    if dest_path.exists():\n        return \"Error: File already exists.\"\n    try:\n        dest_path.write_text(content, encoding=\"utf-8\")\n    except Exception as exc:\n        return \"Error: {exc!r}\"\n    return \"File created.\"\n\ndef print_fragment(fragment, round_index=0):\n    # .act() supplies the round index as the second parameter\n    # Setting a default value means the callback is also\n    # compatible with .complete() and .respond().\n    print(fragment.content, end=\"\", flush=True)\n\nmodel = lms.llm()\nchat = lms.Chat(\"You are a task focused AI assistant\")\n\nwhile True:\n    try:\n        user_input = input(\"You (leave blank to exit): \")\n    except EOFError:\n        print()\n        break\n    if not user_input:\n        break\n    chat.add_user_message(user_input)\n    print(\"Bot: \", end=\"\", flush=True)\n    model.act(\n        chat,\n        [create_file],\n        on_message=chat.append,\n        on_prediction_fragment=print_fragment,\n    )\n    print()\n\n```\n\n----------------------------------------\n\nTITLE: Setting Auto Unload Timer for Model in Python with LM Studio SDK\nDESCRIPTION: Demonstrates how to set a time-to-live (TTL) for a model, specifying the idle time in seconds after the last request until the model unloads automatically.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/loading.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nllama = lms.llm(\"llama-3.2-1b-instruct\", ttl=3600)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    llama = client.llm.model(\"llama-3.2-1b-instruct\", ttl=3600)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Prediction Statistics from LMStudio SDK Completions\nDESCRIPTION: This snippet shows how to access and display metadata about the text completion process, including the model used, number of tokens generated, time to first token, and the reason the generation stopped.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/completion.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.info(\"Model used:\", completion.modelInfo.displayName);\nconsole.info(\"Predicted tokens:\", completion.stats.predictedTokensCount);\nconsole.info(\"Time to first token (seconds):\", completion.stats.timeToFirstTokenSec);\nconsole.info(\"Stop reason:\", completion.stats.stopReason);\n```\n\n----------------------------------------\n\nTITLE: Accessing Prediction Statistics in Python\nDESCRIPTION: Shows how to access and display prediction metadata including model info, token counts, and timing statistics.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# After iterating through the prediction fragments,\n# the overall prediction result may be obtained from the stream\nresult = prediction_stream.result()\n\nprint(\"Model used:\", result.model_info.display_name)\nprint(\"Predicted tokens:\", result.stats.predicted_tokens_count)\nprint(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\nprint(\"Stop reason:\", result.stats.stop_reason)\n```\n\nLANGUAGE: python\nCODE:\n```\n# `result` is the response from the model.\nprint(\"Model used:\", result.model_info.display_name)\nprint(\"Predicted tokens:\", result.stats.predicted_tokens_count)\nprint(\"Time to first token (seconds):\", result.stats.time_to_first_token_sec)\nprint(\"Stop reason:\", result.stats.stop_reason)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tools with LLM Models in LMStudio SDK\nDESCRIPTION: Shows how to integrate custom tools with LLM models using the LMStudio SDK. This example initializes an LLM client, loads the Qwen2.5-7b-instruct model, and passes the previously defined file creation tool to the model's `act()` method with a natural language prompt.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_agent/tools.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nimport { createFileTool } from \"./createFileTool\";\n\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model(\"qwen2.5-7b-instruct\");\nawait model.act(\n  \"Please create a file named output.txt with your understanding of the meaning of life.\",\n  [createFileTool],\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing Single-Turn Tool Use in Python\nDESCRIPTION: This Python script demonstrates a single-turn interaction with LM Studio, defining a simple 'say_hello' function as a tool and requesting the AI to use it. It shows how to set up the OpenAI client, define tools, make a chat completion request, and handle the response.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n# Connect to LM Studio\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\n# Define a simple function\ndef say_hello(name: str) -> str:\n    print(f\"Hello, {name}!\")\n\n# Tell the AI about our function\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"say_hello\",\n            \"description\": \"Says hello to someone\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\n                        \"type\": \"string\",\n                        \"description\": \"The person's name\"\n                    }\n                },\n                \"required\": [\"name\"]\n            }\n        }\n    }\n]\n\n# Ask the AI to use our function\nresponse = client.chat.completions.create(\n    model=\"lmstudio-community/qwen2.5-7b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"Can you say hello to Bob the Builder?\"}],\n    tools=tools\n)\n\n# Get the name the AI wants to use a tool to say hello to\n# (Assumes the AI has requested a tool call and that tool call is say_hello)\ntool_call = response.choices[0].message.tool_calls[0]\nname = eval(tool_call.function.arguments)[\"name\"]\n```\n\n----------------------------------------\n\nTITLE: Generating Quick Chat Response with LMStudio\nDESCRIPTION: Demonstrates how to initialize an LMStudio client and stream a response to a simple chat prompt using the respond() method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/chat-completion.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model();\n\nfor await (const fragment of model.respond(\"What is the meaning of life?\")) {\n  process.stdout.write(fragment.content);\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Chat Fit in Model Context Window using LMStudio\nDESCRIPTION: This example demonstrates how to determine if a given conversation fits into a model's context window. It involves converting the conversation to a string, counting tokens, and comparing the count to the model's context length.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/6_model-info/get-context-length.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\ndef does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -> bool:\n    # Convert the conversation to a string using the prompt template.\n    formatted = model.apply_prompt_template(chat)\n    # Count the number of tokens in the string.\n    token_count = len(model.tokenize(formatted))\n    # Get the current loaded context length of the model\n    context_length = model.get_context_length()\n    return token_count < context_length\n\nmodel = lms.llm()\n\nchat = lms.Chat.from_history({\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"What is the meaning of life.\" },\n        { \"role\": \"assistant\", \"content\": \"The meaning of life is...\" },\n        # ... More messages\n    ]\n})\n\nprint(\"Fits in context:\", does_chat_fit_in_context(model, chat))\n```\n\n----------------------------------------\n\nTITLE: Generating Non-Streaming Structured Response with Zod in TypeScript\nDESCRIPTION: Demonstrates how to use a Zod schema with the model.respond() method to enforce a structured response about a book. The result contains a parsed field with the properly typed data.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/structured-response.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await model.respond(\"Tell me about The Hobbit.\",\n  { structured: bookSchema },\n  maxTokens: 100, // Recommended to avoid getting stuck\n);\n\nconst book = result.parsed;\nconsole.info(book);\n//           ^\n// Note that `book` is now correctly typed as { title: string, author: string, year: number }\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings with POST Request\nDESCRIPTION: Shows how to generate vector embeddings for text using the embeddings API. This converts text input into a vector representation that can be used for semantic search, clustering, or other NLP tasks.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:1234/api/v0/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-nomic-embed-text-v1.5\",\n    \"input\": \"Some text to embed\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Downloading Models with TypeScript SDK\nDESCRIPTION: Demonstrates how to search for models, select download options based on quantization, and download a model using the LM Studio TypeScript SDK. The example searches for 'llama 3.2 1b' models, filters for GGUF compatibility, and selects the Q4_K_M quantization option.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/_download-models.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\n\n// 1. Search for the model you want\n// Specify any/all of searchTerm, limit, compatibilityTypes\nconst searchResults = await client.repository.searchModels({\n  searchTerm: \"llama 3.2 1b\",    // Search for Llama 3.2 1B\n  limit: 5,                      // Get top 5 results\n  compatibilityTypes: [\"gguf\"],  // Only download GGUFs\n});\n\n// 2. Find download options\nconst bestResult = searchResults[0];\nconst downloadOptions = await bestResult.getDownloadOptions();\n\n// Let's download Q4_K_M, a good middle ground quantization\nconst desiredModel = downloadOptions.find(option => option.quantization === 'Q4_K_M');\n\n// 3. Download it!\nconst modelKey = await desiredModel.download();\n\n// This returns a path you can use to load the model\nconst loadedModel = await client.llm.model(modelKey);\n```\n\n----------------------------------------\n\nTITLE: Configuring Server API Host and Port in Python\nDESCRIPTION: Examples of how to customize the server API host and port when creating a client instance using both the convenience API and scoped resource API approaches.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_getting-started/project-setup.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nSERVER_API_HOST = \"localhost:1234\"\n\n# This must be the *first* SDK interaction (otherwise the SDK will\n# implicitly attempt to access the default server instance)\nlms.get_default_client(SERVER_API_HOST)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nSERVER_API_HOST = \"localhost:1234\"\n\n# When using the scoped resource API, each client instance\n# can be configured to use a specific server instance\nwith lms.Client(SERVER_API_HOST) as client:\n    model = client.llm.model()\n\n    for fragment in model.respond_stream(\"What is the meaning of life?\"):\n        print(fragment.content, end=\"\", flush=True)\n    print() # Advance to a new line at the end of the response\n```\n\n----------------------------------------\n\nTITLE: Interactive Chat Session Using Python REPL with lmstudio\nDESCRIPTION: Demonstrates how to use lmstudio-python to list loaded models, create a chat session, and conduct an interactive conversation. The example shows model listing, chat initialization with system prompt, sending user messages, and receiving model responses with proper message handling.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_getting-started/repl.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import lmstudio as lms\n>>> loaded_models = lms.list_loaded_models()\n>>> for idx, model in enumerate(loaded_models):\n...     print(f\"{idx:>3} {model}\")\n...\n  0 LLM(identifier='qwen2.5-7b-instruct')\n>>> model = loaded_models[0]\n>>> chat = lms.Chat(\"You answer questions concisely\")\n>>> chat.add_user_message(\"Tell me three fruits\")\nChatMessageDataUser(content=[ChatMessagePartTextData(text='Tell me three fruits')])\n>>> print(model.respond(chat, on_message=chat.append))\nBanana, apple, orange.\n>>> chat.add_user_message(\"Tell me three more fruits\")\nChatMessageDataUser(content=[ChatMessagePartTextData(text='Tell me three more fruits')])\n>>> print(model.respond(chat, on_message=chat.append))\nPineapple, avocado, strawberry.\n>>> chat.add_user_message(\"How many fruits have you told me?\")\nChatMessageDataUser(content=[ChatMessagePartTextData(text='How many fruits have you told me?')])\n>>> print(model.respond(chat, on_message=chat.append))\nYou asked for three fruits initially, then three more, so I've told you six fruits in total: banana, apple, orange, pineapple, avocado, and strawberry.\n```\n\n----------------------------------------\n\nTITLE: Instantiating an LLM Model with LMStudio SDK in TypeScript\nDESCRIPTION: This code demonstrates how to import the LMStudioClient and load a specific language model (qwen2.5-7b-instruct) for text completion tasks.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/completion.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model(\"qwen2.5-7b-instruct\");\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with LMStudio SDK in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the LMStudio SDK in TypeScript to create an embedding from input text. It initializes the LMStudio client, loads the embedding model, and generates an embedding for the text 'Hello, world!'.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_embedding/index.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst model = await client.embedding.model(\"nomic-embed-text-v1.5\");\n\nconst { embedding } = await model.embed(\"Hello, world!\");\n```\n\n----------------------------------------\n\nTITLE: Building a Terminal Simulator with LM Studio Text Completions\nDESCRIPTION: A complete example showing how to create an interactive terminal simulator. It maintains a history of commands, uses streaming completions to generate responses, and stops generation when it encounters a '$' character.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/completion.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm()\nconsole_history = []\n\nwhile True:\n    try:\n        user_command = input(\"$ \")\n    except EOFError:\n        print()\n        break\n    if user_command.strip() == \"exit\":\n        break\n    console_history.append(f\"$ {user_command}\")\n    history_prompt = \"\\n\".join(console_history)\n    prediction_stream = model.complete_stream(\n        history_prompt,\n        config={ \"stopStrings\": [\"$\"] },\n    )\n    for fragment in prediction_stream:\n        print(fragment.content, end=\"\", flush=True)\n    print()\n    console_history.append(prediction_stream.result().content)\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Text with LM Studio SDK in Python\nDESCRIPTION: This snippet demonstrates how to tokenize a string using a loaded LLM or embedding model in LM Studio. It returns an array of token IDs.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/4_tokenization/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm()\n\ntokens = model.tokenize(\"Hello, world!\")\n\nprint(tokens) # Array of token IDs.\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Vision-Language Model in TypeScript\nDESCRIPTION: This snippet demonstrates how to connect to LM Studio and obtain a handle to a Vision-Language Model using TypeScript.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/image-input.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model(\"qwen2-vl-2b-instruct\");\n```\n\n----------------------------------------\n\nTITLE: Responding to Single String Input in Python with LMStudio SDK\nDESCRIPTION: This snippet demonstrates how to use a single string to represent a chat with one user message using the .respond method of the LMStudio SDK.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/working-with-chats.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprediction = llm.respond(\"What is the meaning of life?\")\n```\n\n----------------------------------------\n\nTITLE: Applying Prompt Template with Chat Object in Python\nDESCRIPTION: Demonstrates how to apply a prompt template to a Chat object using LM Studio SDK. Creates a new chat, adds system and user messages, and formats them using the model's prompt template.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_more/_apply-prompt-template.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { Chat, LMStudioClient } from \"@lmstudio/sdk\"\n\nclient = new LMStudioClient()\nmodel = client.llm.model() # Use any loaded LLM\n\nchat = Chat.createEmpty()\nchat.append(\"system\", \"You are a helpful assistant.\")\nchat.append(\"user\", \"What is LM Studio?\")\n\nformatted = model.applyPromptTemplate(chat)\nprint(formatted)\n```\n\n----------------------------------------\n\nTITLE: Implementing Speculative Decoding with Non-Streaming API in Python\nDESCRIPTION: This snippet demonstrates how to use speculative decoding with a draft model in lmstudio-python using the non-streaming API. It initializes the main model, sets up the draft model, and performs a prediction with speculative decoding.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/speculative-decoding.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmain_model_key = \"qwen2.5-7b-instruct\"\ndraft_model_key = \"qwen2.5-0.5b-instruct\"\n\nmodel = lms.llm(main_model_key)\nresult = model.respond(\n    \"What are the prime numbers between 0 and 100?\",\n    config={\n        \"draftModel\": draft_model_key,\n    }\n)\n\nprint(result)\nstats = result.stats\nprint(f\"Accepted {stats.accepted_draft_tokens_count}/{stats.predicted_tokens_count} tokens\")\n```\n\n----------------------------------------\n\nTITLE: Querying Loaded Models with LMStudio SDK in TypeScript\nDESCRIPTION: Demonstrates how to use the LMStudioClient to query both LLM and embedding models currently loaded in memory. The code shows how to initialize the client and use the listLoaded method from both llm and embedding namespaces.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/list-loaded.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\n\nconst llmOnly = await client.llm.listLoaded();\nconst embeddingOnly = await client.embedding.listLoaded();\n```\n\n----------------------------------------\n\nTITLE: Preparing an Image for VLM Input in LM Studio\nDESCRIPTION: Code to prepare an image for input to a Vision-Language Model using LM Studio's prepare_image() function. Supports JPEG, PNG, and WebP formats from file paths or binary data.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/image-input.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nimage_path = \"/path/to/image.jpg\" # Replace with the path to your image\nimage_handle = lms.prepare_image(image_path)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    image_path = \"/path/to/image.jpg\" # Replace with the path to your image\n    image_handle = client.files.prepare_image(image_path)\n```\n\n----------------------------------------\n\nTITLE: Setting Load Parameters with load()\nDESCRIPTION: Example of setting load-time parameters like contextLength and GPU settings when creating a new model instance using the load() method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/parameters.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = await client.llm.load(\"qwen2.5-7b-instruct\", {\n  config: {\n    contextLength: 8192,\n    gpu: {\n      ratio: 0.5,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Structured Output Request using Python OpenAI Client\nDESCRIPTION: Python example demonstrating how to request structured output from LM Studio using the OpenAI client. This example creates fictional characters conforming to a specific JSON schema.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/structured-output.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport json\n\n# Initialize OpenAI client that points to the local LM Studio server\nclient = OpenAI(\n    base_url=\"http://localhost:1234/v1\",\n    api_key=\"lm-studio\"\n)\n\n# Define the conversation with the AI\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n    {\"role\": \"user\", \"content\": \"Create 1-3 fictional characters\"}\n]\n\n# Define the expected response structure\ncharacter_schema = {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n        \"name\": \"characters\",\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"characters\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\"type\": \"string\"},\n                            \"occupation\": {\"type\": \"string\"},\n                            \"personality\": {\"type\": \"string\"},\n                            \"background\": {\"type\": \"string\"}\n                        },\n                        \"required\": [\"name\", \"occupation\", \"personality\", \"background\"]\n                    },\n                    \"minItems\": 1,\n                }\n            },\n            \"required\": [\"characters\"]\n        },\n    }\n}\n\n# Get response from AI\nresponse = client.chat.completions.create(\n    model=\"your-model\",\n    messages=messages,\n    response_format=character_schema,\n)\n\n# Parse and display the results\nresults = json.loads(response.choices[0].message.content)\nprint(json.dumps(results, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Querying Specific Model Information with GET Request\nDESCRIPTION: Shows how to get detailed information about a specific model using the model ID parameter. This endpoint returns the same model details as the list endpoint but for a single model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:1234/api/v0/models/qwen2-vl-7b-instruct\n```\n\n----------------------------------------\n\nTITLE: Providing Chat History Data Directly in Python with LMStudio SDK\nDESCRIPTION: This example shows how to provide chat history data directly as a dictionary to the LMStudio SDK's respond method, without using the Chat helper class.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/working-with-chats.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprediction = llm.respond({\"messages\": [\n  { \"role\": \"system\", \"content\": \"You are a resident AI philosopher.\" },\n  { \"role\": \"user\", \"content\": \"What is the meaning of life?\" },\n]})\n```\n\n----------------------------------------\n\nTITLE: Preparing a Base64 Image for VLM Input in TypeScript\nDESCRIPTION: This snippet demonstrates how to prepare a base64-encoded image for input to a Vision-Language Model using the `prepareImageBase64()` method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/image-input.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst imageBase64 = \"Your base64 string here\";\nconst image = await client.files.prepareImageBase64(imageBase64);\n```\n\n----------------------------------------\n\nTITLE: Generating Streaming Structured Response with Zod in TypeScript\nDESCRIPTION: Shows how to stream a structured response using a Zod schema, allowing for real-time display of content while still obtaining the final typed result after completion.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/structured-response.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.respond(\"Tell me about The Hobbit.\",\n  { structured: bookSchema },\n  maxTokens: 100, // Recommended to avoid getting stuck\n);\n\nfor await (const { content } of prediction) {\n  process.stdout.write(content);\n}\nprocess.stdout.write(\"\\n\");\n\n// Get the final structured result\nconst result = await prediction.result();\nconst book = result.parsed;\n\nconsole.info(book);\n//           ^\n// Note that `book` is now correctly typed as { title: string, author: string, year: number }\n```\n\n----------------------------------------\n\nTITLE: Loading New Model Instance in Python with LM Studio SDK\nDESCRIPTION: Demonstrates how to load a new instance of a model, even if one already exists. This allows multiple instances of the same or different models to be loaded simultaneously.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/loading.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nclient = lms.get_default_client()\nllama = client.llm.load_new_instance(\"llama-3.2-1b-instruct\")\nanother_llama = client.llm.load_new_instance(\"llama-3.2-1b-instruct\", \"second-llama\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    llama = client.llm.load_new_instance(\"llama-3.2-1b-instruct\")\n    another_llama = client.llm.load_new_instance(\"llama-3.2-1b-instruct\", \"second-llama\")\n```\n\n----------------------------------------\n\nTITLE: Checking Chat Fit in Context Window in TypeScript\nDESCRIPTION: This example shows how to determine if a given conversation fits into a model's context window by converting the chat to a string, counting tokens, and comparing to the model's context length.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/6_model-info/get-context-length.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chat, type LLM, LMStudioClient } from \"@lmstudio/sdk\";\n\nasync function doesChatFitInContext(model: LLM, chat: Chat) {\n  // Convert the conversation to a string using the prompt template.\n  const formatted = await model.applyPromptTemplate(chat);\n  // Count the number of tokens in the string.\n  const tokenCount = await model.countTokens(formatted);\n  // Get the current loaded context length of the model\n  const contextLength = await model.getContextLength();\n  return tokenCount < contextLength;\n}\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\n\nconst chat = Chat.from([\n  { role: \"user\", content: \"What is the meaning of life.\" },\n  { role: \"assistant\", content: \"The meaning of life is...\" },\n  // ... More messages\n]);\n\nconsole.info(\"Fits in context:\", await doesChatFitInContext(model, chat));\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM Model in Python\nDESCRIPTION: Demonstrates how to initialize a specific LLM model (Qwen2.5 7B Instruct) using both API styles.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/chat-completion.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nmodel = lms.llm(\"qwen2.5-7b-instruct\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    model = client.llm.model(\"qwen2.5-7b-instruct\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Models with Progress Callbacks in Python\nDESCRIPTION: Shows how to download models with progress monitoring in Python using the LM Studio SDK. The example includes a callback function that prints download progress information such as bytes downloaded, total size, and download speed.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/_download-models.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio\n\ndef print_progress_update(update: lmstudio.DownloadProgressUpdate) -> None:\n    print(f\"Downloaded {update.downloaded_bytes} bytes of {update.total_bytes} total \\\n            at {update.speed_bytes_per_second} bytes/sec\")\n\nwith lmstudio.Client() as client:\n    # ... Same code as before ...\n\n    model_key = desired_model.download(\n        on_progress=print_progress_update,\n        on_finalize: lambda: print(\"Finalizing download...\")\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Text Completions with POST Request\nDESCRIPTION: Demonstrates how to use the completions API for text continuation. This example shows how to provide a prompt with parameters like temperature, max tokens, and stop sequences to control generation.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:1234/api/v0/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"granite-3.0-2b-instruct\",\n    \"prompt\": \"the meaning of life is\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 10,\n    \"stream\": false,\n    \"stop\": \"\\n\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Getting Specific Model with LM Studio SDK in TypeScript\nDESCRIPTION: This code shows how to get a specific model using the .model() method with a model key argument. It will load the model if not already loaded or return the existing instance.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/loading.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model(\"llama-3.2-1b-instruct\");\n```\n\n----------------------------------------\n\nTITLE: Retrieving Context Length in Python using LMStudio\nDESCRIPTION: This snippet shows how to use the get_context_length() function on a model object to retrieve the maximum context length. The model object is an instance of a loaded model obtained from the llm.model method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/6_model-info/get-context-length.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncontext_length = model.get_context_length()\n```\n\n----------------------------------------\n\nTITLE: Generating Non-streaming Text Completions with LMStudio SDK\nDESCRIPTION: This code demonstrates how to generate text completions in non-streaming mode, where the complete response is returned only after the entire generation is finished.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/completion.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst completion = await model.complete(\"My name is\", {\n  maxTokens: 100,\n});\n\nconsole.info(completion.content);\n```\n\n----------------------------------------\n\nTITLE: Retrieving LLM Model Information in TypeScript\nDESCRIPTION: This snippet demonstrates how to access information about a loaded LLM model using the getInfo method. It shows how to retrieve properties such as the model key, context length, and whether the model is trained for tool use.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/6_model-info/get-model-info.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\n\nconst modelInfo = await model.getInfo();\n\nconsole.info(\"Model Key\", modelInfo.modelKey);\nconsole.info(\"Current Context Length\", model.contextLength);\nconsole.info(\"Model Trained for Tool Use\", modelInfo.trainedForToolUse);\n// etc.\n```\n\n----------------------------------------\n\nTITLE: Adding Messages to Chat in TypeScript\nDESCRIPTION: Demonstrates how to add new messages to an existing Chat instance using the addMessage method. Messages can be added for different roles such as system, user, or assistant.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_7_api-reference/chat.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nchat.addMessage({ role: \"user\", content: \"What's the weather like today?\" });\nchat.addMessage({ role: \"assistant\", content: \"I'm sorry, but I don't have access to real-time weather information. Is there anything else I can help you with?\" });\n```\n\n----------------------------------------\n\nTITLE: Starting the REST API Server with Bash Command\nDESCRIPTION: Shows how to start the LM Studio REST API server using the command line interface. This is the initial step required before making any API calls.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms server start\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Text with LM Studio SDK\nDESCRIPTION: Demonstrates how to tokenize a string using a loaded LLM or embedding model. Uses the LMStudioClient to initialize a model and convert text into token IDs.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/4_tokenization/index.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\n\nconst tokens = await model.tokenize(\"Hello, world!\");\n\nconsole.info(tokens); // Array of token IDs.\n```\n\n----------------------------------------\n\nTITLE: Customizing Inference Parameters\nDESCRIPTION: Demonstrates how to customize model parameters like temperature and maximum tokens for both streaming and non-streaming responses.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/chat-completion.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.respond(chat, {\n  temperature: 0.6,\n  maxTokens: 50,\n});\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await model.respond(chat, {\n  temperature: 0.6,\n  maxTokens: 50,\n});\n```\n\n----------------------------------------\n\nTITLE: Applying Prompt Template with Message Array in Python\nDESCRIPTION: Shows how to apply a prompt template to an array of messages using LM Studio SDK. Uses a direct array of message objects instead of a Chat instance.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_more/_apply-prompt-template.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\"\n\nclient = new LMStudioClient()\nmodel = client.llm.model() # Use any loaded LLM\n\nformatted = model.applyPromptTemplate([\n  { role: \"system\", content: \"You are a helpful assistant.\" },\n  { role: \"user\", content: \"What is LM Studio?\" },\n])\nprint(formatted)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Embedding Model Information in TypeScript\nDESCRIPTION: This snippet shows how to access information about a loaded embedding model using the getInfo method. It demonstrates how to retrieve properties such as the model key and context length.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/6_model-info/get-model-info.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst model = await client.embedding.model();\n\nconst modelInfo = await model.getInfo();\n\nconsole.info(\"Model Key\", modelInfo.modelKey);\nconsole.info(\"Current Context Length\", modelInfo.contextLength);\n// etc.\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Completion in TypeScript\nDESCRIPTION: Shows how to generate a completion for the chat conversation using the generate method. This method returns a Promise that resolves to a ChatMessage object representing the model's response.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_7_api-reference/chat.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst response = await chat.generate();\nconsole.log(response.content);\n```\n\n----------------------------------------\n\nTITLE: Getting Current Model with LM Studio SDK in TypeScript\nDESCRIPTION: This snippet demonstrates how to get the currently loaded model in LM Studio using the .model() method without arguments.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/loading.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model();\n```\n\n----------------------------------------\n\nTITLE: Instantiating a VLM Model in LM Studio\nDESCRIPTION: Code to connect to LM Studio and obtain a handle to a Vision-Language Model (VLM). Provides both convenience API and scoped resource API approaches.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/image-input.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nmodel = lms.llm(\"qwen2-vl-2b-instruct\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    model = client.llm.model(\"qwen2-vl-2b-instruct\")\n```\n\n----------------------------------------\n\nTITLE: Downloading a VLM Model with LM Studio CLI\nDESCRIPTION: Command to download a Vision-Language Model (VLM) like qwen2-vl-2b-instruct using the LM Studio CLI.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/image-input.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms get qwen2-vl-2b-instruct\n```\n\n----------------------------------------\n\nTITLE: Applying Prompt Template to Chat Object in TypeScript\nDESCRIPTION: Demonstrates how to create a Chat object, append messages, and apply a prompt template using the LM Studio SDK. The method takes a Chat object as input and returns a formatted string suitable for LLM processing.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/_more/_apply-prompt-template.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chat, LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst llm = await client.llm.model(); // Use any loaded LLM\n\nconst chat = Chat.createEmpty();\nchat.append(\"system\", \"You are a helpful assistant.\");\nchat.append(\"user\", \"What is LM Studio?\");\nconst formatted = await llm.applyPromptTemplate(chat);\nconsole.info(formatted);\n```\n\n----------------------------------------\n\nTITLE: Accessing Chat History in TypeScript\nDESCRIPTION: Illustrates how to access the full conversation history of a Chat instance using the messages property. This returns an array of all ChatMessage objects in the conversation.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_7_api-reference/chat.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst history = chat.messages;\nconsole.log(history);\n```\n\n----------------------------------------\n\nTITLE: Counting Tokens with LM Studio SDK\nDESCRIPTION: Shows how to count the number of tokens in a text string using the countTokens method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/4_tokenization/index.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst tokenCount = await model.countTokens(\"Hello, world!\");\nconsole.info(\"Token count:\", tokenCount);\n```\n\n----------------------------------------\n\nTITLE: Downloading a Vision-Language Model with LM Studio CLI\nDESCRIPTION: This command downloads the 'qwen2-vl-2b-instruct' model using the LM Studio CLI.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/image-input.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms get qwen2-vl-2b-instruct\n```\n\n----------------------------------------\n\nTITLE: Downloading Local Models with CLI\nDESCRIPTION: Command to download the Llama 3.2 1B instructional model using the LM Studio CLI, which is required for the example code to work.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlms get llama-3.2-1b-instruct\n```\n\n----------------------------------------\n\nTITLE: Structured Output Request using curl\nDESCRIPTION: Example of making a structured output request to LM Studio's API using curl. The request includes a JSON schema that enforces the response format as a joke object.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/structured-output.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://{{hostname}}:{{port}}/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"{{model}}\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful jokester.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Tell me a joke.\"\n      }\n    ],\n    \"response_format\": {\n      \"type\": \"json_schema\",\n      \"json_schema\": {\n        \"name\": \"joke_response\",\n        \"strict\": \"true\",\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"joke\": {\n              \"type\": \"string\"\n            }\n          },\n        \"required\": [\"joke\"]\n        }\n      }\n    },\n    \"temperature\": 0.7,\n    \"max_tokens\": 50,\n    \"stream\": false\n  }'\n```\n\n----------------------------------------\n\nTITLE: Preparing an Image File for VLM Input in TypeScript\nDESCRIPTION: This code shows how to prepare an image file for input to a Vision-Language Model using the `prepareImage()` method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/image-input.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst imagePath = \"/path/to/image.jpg\"; // Replace with the path to your image\nconst image = await client.files.prepareImage(imagePath);\n```\n\n----------------------------------------\n\nTITLE: Counting Tokens with LM Studio SDK in Python\nDESCRIPTION: This code shows how to count the number of tokens in a given string using LM Studio's tokenizer. It utilizes the length of the tokenized array.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/4_tokenization/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntoken_count = len(model.tokenize(\"Hello, world!\"))\nprint(\"Token count:\", token_count)\n```\n\n----------------------------------------\n\nTITLE: Models API Response Format in JSON\nDESCRIPTION: Shows the response format when querying available models. The response includes a list of models with details like ID, type, publisher, architecture, quantization, state, and maximum context length.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"qwen2-vl-7b-instruct\",\n      \"object\": \"model\",\n      \"type\": \"vlm\",\n      \"publisher\": \"mlx-community\",\n      \"arch\": \"qwen2_vl\",\n      \"compatibility_type\": \"mlx\",\n      \"quantization\": \"4bit\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 32768\n    },\n    {\n      \"id\": \"meta-llama-3.1-8b-instruct\",\n      \"object\": \"model\",\n      \"type\": \"llm\",\n      \"publisher\": \"lmstudio-community\",\n      \"arch\": \"llama\",\n      \"compatibility_type\": \"gguf\",\n      \"quantization\": \"Q4_K_M\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 131072\n    },\n    {\n      \"id\": \"text-embedding-nomic-embed-text-v1.5\",\n      \"object\": \"model\",\n      \"type\": \"embeddings\",\n      \"publisher\": \"nomic-ai\",\n      \"arch\": \"nomic-bert\",\n      \"compatibility_type\": \"gguf\",\n      \"quantization\": \"Q4_0\",\n      \"state\": \"not-loaded\",\n      \"max_context_length\": 2048\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Chat Context Length Compatibility\nDESCRIPTION: Demonstrates how to check if a chat conversation fits within a model's context length by converting the chat to a string, counting tokens, and comparing to the model's context length limit.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/4_tokenization/index.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chat, type LLM, LMStudioClient } from \"@lmstudio/sdk\";\n\nasync function doesChatFitInContext(model: LLM, chat: Chat) {\n  // Convert the conversation to a string using the prompt template.\n  const formatted = await model.applyPromptTemplate(chat);\n  // Count the number of tokens in the string.\n  const tokenCount = await model.countTokens(formatted);\n  // Get the current loaded context length of the model\n  const contextLength = await model.getContextLength();\n  return tokenCount < contextLength;\n}\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\n\nconst chat = Chat.from([\n  { role: \"user\", content: \"What is the meaning of life.\" },\n  { role: \"assistant\", content: \"The meaning of life is...\" },\n  // ... More messages\n]);\n\nconsole.info(\"Fits in context:\", await doesChatFitInContext(model, chat));\n```\n\n----------------------------------------\n\nTITLE: Automated Download Command\nDESCRIPTION: Command for automated downloads that skips all confirmation prompts, useful for scripting\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/get.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nlms get llama-3.1-8b --yes\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with Python using LM Studio API\nDESCRIPTION: Example showing how to get text embeddings using the OpenAI-compatible embeddings endpoint in LM Studio. Demonstrates processing a text string and returning its vector embedding.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Make sure to `pip install openai` first\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ndef get_embedding(text, model=\"model-identifier\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model).data[0].embedding\n\nprint(get_embedding(\"Once upon a time, there was a cat.\"))\n```\n\n----------------------------------------\n\nTITLE: Listing Loaded Models with LMStudio Python API\nDESCRIPTION: Demonstrates two ways to query which models are currently loaded in memory: the convenience API and the scoped resource API. Both methods can retrieve all loaded models or filter by model type (LLM or embedding).\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/list-loaded.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nall_loaded_models = lms.list_loaded_models()\nllm_only = lms.list_loaded_models(\"llm\")\nembedding_only = lms.list_loaded_models(\"embedding\")\n\nprint(all_loaded_models)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lms\n\nwith lms.Client() as client:\n    all_loaded_models = client.list_loaded_models()\n    llm_only = client.llm.list_loaded()\n    embedding_only = client.embedding.list_loaded()\n\n    print(all_loaded_models)\n```\n\n----------------------------------------\n\nTITLE: Basic Model Loading\nDESCRIPTION: Basic command to load a model into memory using its model key.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/load.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlms load <model_key>\n```\n\n----------------------------------------\n\nTITLE: Creating a File Creation Tool in Python for LMStudio\nDESCRIPTION: Defines a create_file tool that creates a file with specified name and content. It includes error handling for existing files and exceptions, returning appropriate status messages.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/2_agent/tools.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\ndef create_file(name: str, content: str):\n    \"\"\"Create a file with the given name and content.\"\"\"\n    dest_path = Path(name)\n    if dest_path.exists():\n        return \"Error: File already exists.\"\n    try:\n        dest_path.write_text(content, encoding=\"utf-8\")\n    except Exception as exc:\n        return \"Error: {exc!r}\"\n    return \"File created.\"\n```\n\n----------------------------------------\n\nTITLE: Setting Context Length\nDESCRIPTION: Command to load a model with a specific context length setting.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/load.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nlms load <model_key> --context-length 4096\n```\n\n----------------------------------------\n\nTITLE: Model Download with Progress Callbacks in TypeScript/JavaScript\nDESCRIPTION: Demonstrates how to implement progress callbacks when downloading models using the LM Studio SDK in TypeScript/JavaScript. Shows how to monitor download progress with byte counts and speed reporting, as well as download finalization notification.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/_download-models.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { LMStudioClient, type DownloadProgressUpdate } from \"@lmstudio/sdk\";\n\nfunction printProgressUpdate(update: DownloadProgressUpdate) {\n  process.stdout.write(`Downloaded ${update.downloadedBytes} bytes of ${update.totalBytes} total \\\n                        at ${update.speed_bytes_per_second} bytes/sec`)\n}\n\nconst client = new LMStudioClient()\n\n# ... Same code as before ...\n\nmodelKey = desiredModel.download({\n  onProgress: printProgressUpdate,\n  onStartFinalizing: () => console.log(\"Finalizing...\"),\n})\n\nconst loadedModel = client.llm.model(modelKey)\n```\n\n----------------------------------------\n\nTITLE: Listing Downloaded Models in LM Studio using Python Convenience API\nDESCRIPTION: Shows how to list all downloaded models, as well as filtering for specific model types (LLM or embedding) using the convenience API. The returned models can be iterated through and each includes methods to convert to a full SDK handle.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/list-downloaded.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\ndownloaded = lms.list_downloaded_models()\nllm_only = lms.list_downloaded_models(\"llm\")\nembedding_only = lms.list_downloaded_models(\"embedding\")\n\nfor model in downloaded:\n    print(model)\n```\n\n----------------------------------------\n\nTITLE: Loading Model with Custom Identifier\nDESCRIPTION: Command to load a model with a custom identifier for API reference.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/load.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nlms load <model_key> --identifier \"my-custom-identifier\"\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Functions for Agent Operations in Python\nDESCRIPTION: This section defines three custom functions: open_url for opening safe URLs, get_current_time for checking the current time, and analyze_directory for examining file system directories.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef open_url(url: str) -> str:\n    parsed_url = urlparse(url)\n    if parsed_url.scheme not in ['http', 'https']:\n        return f\"Error: Invalid URL scheme. Only http and https are allowed.\"\n    try:\n        webbrowser.open(url)\n        return f\"Opened {url} in the default browser.\"\n    except Exception as e:\n        return f\"Error opening {url}: {str(e)}\"\n\ndef get_current_time() -> str:\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef analyze_directory(path: str) -> str:\n    try:\n        if not os.path.isdir(path):\n            return f\"Error: {path} is not a valid directory.\"\n        files = os.listdir(path)\n        file_count = len(files)\n        dir_count = sum(os.path.isdir(os.path.join(path, f)) for f in files)\n        file_types = {}\n        for file in files:\n            _, ext = os.path.splitext(file)\n            ext = ext.lower()\n            file_types[ext] = file_types.get(ext, 0) + 1\n        return json.dumps({\n            \"total_items\": file_count,\n            \"directories\": dir_count,\n            \"files\": file_count - dir_count,\n            \"file_types\": file_types\n        })\n    except Exception as e:\n        return f\"Error analyzing directory {path}: {str(e)}\"\n```\n\n----------------------------------------\n\nTITLE: Defining LM Studio Status Command Parameters\nDESCRIPTION: Parameters definition for the lms server status command, including JSON output flag, verbosity controls, and log level settings.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-status.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: \"--json\"\n  type: \"flag\"\n  optional: true\n  description: \"Output the status in JSON format\"\n- name: \"--verbose\"\n  type: \"flag\"\n  optional: true\n  description: \"Enable detailed logging output\"\n- name: \"--quiet\"\n  type: \"flag\"\n  optional: true\n  description: \"Suppress all logging output\"\n- name: \"--log-level\"\n  type: \"string\"\n  optional: true\n  description: \"The level of logging to use. Defaults to 'info'\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Models with Progress Callbacks in TypeScript\nDESCRIPTION: Demonstrates how to implement progress monitoring when downloading models using the LM Studio TypeScript SDK. The code shows how to track download progress and detect when the download enters the finalizing stage for checksum validation.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/_download-models.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient, type DownloadProgressUpdate } from \"@lmstudio/sdk\";\n\nfunction printProgressUpdate(update: DownloadProgressUpdate) {\n  process.stdout.write(`Downloaded ${update.downloadedBytes} bytes of ${update.totalBytes} total \\\n                        at ${update.speed_bytes_per_second} bytes/sec`);\n}\n\nconst client = new LMStudioClient();\n\n// ... Same code as before ...\n\nmodelKey = await desiredModel.download({\n  onProgress: printProgressUpdate,\n  onStartFinalizing: () => console.log(\"Finalizing...\"),\n});\n\nconst loadedModel = await client.llm.model(modelKey);\n```\n\n----------------------------------------\n\nTITLE: Example Output from Model Information Query\nDESCRIPTION: Shows the structure and content of the model information returned by the get_info() method. The output includes details such as architecture, context length, model format, identifier, and various capabilities of the model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/6_model-info/get-model-info.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nLlmInstanceInfo.from_dict({\n  \"architecture\": \"qwen2\",\n  \"contextLength\": 4096,\n  \"displayName\": \"Qwen2.5 7B Instruct 1M\",\n  \"format\": \"gguf\",\n  \"identifier\": \"qwen2.5-7b-instruct\",\n  \"instanceReference\": \"lpFZPBQjhSZPrFevGyY6Leq8\",\n  \"maxContextLength\": 1010000,\n  \"modelKey\": \"qwen2.5-7b-instruct-1m\",\n  \"paramsString\": \"7B\",\n  \"path\": \"lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf\",\n  \"sizeBytes\": 4683073888,\n  \"trainedForToolUse\": true,\n  \"type\": \"llm\",\n  \"vision\": false\n})\n```\n\n----------------------------------------\n\nTITLE: LMS Get Command Parameters Definition\nDESCRIPTION: Defines the complete set of parameters and flags available for the `lms get` command, including search terms, format filters, and display options\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/get.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: \"[search term]\"\n  type: \"string\"\n  optional: true\n  description: \"The model to download. For specific quantizations, append '@' (e.g., 'llama-3.1-8b@q4_k_m')\"\n- name: \"--mlx\"\n  type: \"flag\"\n  optional: true\n  description: \"Include MLX models in search results\"\n- name: \"--gguf\"\n  type: \"flag\"\n  optional: true\n  description: \"Include GGUF models in search results\"\n- name: \"--limit\"\n  type: \"number\"\n  optional: true\n  description: \"Limit the number of model options shown\"\n- name: \"--always-show-all-results\"\n  type: \"flag\"\n  optional: true\n  description: \"Always show search results, even with exact matches\"\n- name: \"--always-show-download-options\"\n  type: \"flag\"\n  optional: true\n  description: \"Always show quantization options, even with exact matches\"\n- name: \"--yes\"\n  type: \"flag\"\n  optional: true\n  description: \"Skip all confirmations. Uses first match and recommended quantization\"\n```\n\n----------------------------------------\n\nTITLE: Listing Downloaded Models with LMStudioClient in TypeScript\nDESCRIPTION: This code demonstrates how to use the LMStudioClient to list all downloaded models available on the local machine. The listDownloadedModels method is called on the system namespace of the client instance.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/list-downloaded.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconsole.info(await client.system.listDownloadedModels());\n```\n\n----------------------------------------\n\nTITLE: Printing Prediction Statistics\nDESCRIPTION: Shows how to access and print prediction metadata including model info, token counts, and timing statistics.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/chat-completion.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// If you have already iterated through the prediction fragments,\n// doing this will not result in extra waiting.\nconst result = await prediction.result();\n\nconsole.info(\"Model used:\", result.modelInfo.displayName);\nconsole.info(\"Predicted tokens:\", result.stats.predictedTokensCount);\nconsole.info(\"Time to first token (seconds):\", result.stats.timeToFirstTokenSec);\nconsole.info(\"Stop reason:\", result.stats.stopReason);\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// `result` is the response from the model.\nconsole.info(\"Model used:\", result.modelInfo.displayName);\nconsole.info(\"Predicted tokens:\", result.stats.predictedTokensCount);\nconsole.info(\"Time to first token (seconds):\", result.stats.timeToFirstTokenSec);\nconsole.info(\"Stop reason:\", result.stats.stopReason);\n```\n\n----------------------------------------\n\nTITLE: Modifying Python OpenAI Client for LM Studio Integration\nDESCRIPTION: Shows how to modify the base URL in a Python OpenAI client to point to the local LM Studio server instead of OpenAI's servers.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(\n+    base_url=\"http://localhost:1234/v1\"\n)\n\n# ... the rest of your code ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving Context Length in TypeScript\nDESCRIPTION: This snippet demonstrates how to get the maximum context length of a loaded model using the getContextLength() method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/6_model-info/get-context-length.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst contextLength = await model.getContextLength();\n```\n\n----------------------------------------\n\nTITLE: Basic Model Download Command\nDESCRIPTION: Demonstrates how to download a model by specifying its name using the lms get command\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/get.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlms get llama-3.1-8b\n```\n\n----------------------------------------\n\nTITLE: Accessing Current Model in Python with LM Studio SDK\nDESCRIPTION: Demonstrates how to access the currently loaded model using LM Studio's convenience API and scoped resource API. This is useful when a model is already loaded in LM Studio.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/loading.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model()\n```\n\n----------------------------------------\n\nTITLE: Setting Load Parameters with model()\nDESCRIPTION: Example of setting load-time parameters like contextLength and GPU settings when retrieving or loading a model using the model() method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/parameters.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = await client.llm.model(\"qwen2.5-7b-instruct\", {\n  config: {\n    contextLength: 8192,\n    gpu: {\n      ratio: 0.5,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Auto Unload Timer with .load() in LM Studio SDK\nDESCRIPTION: This snippet shows how to set an auto-unload timer (TTL) when loading a model using the .load() method. The TTL specifies the idle time in seconds before the model unloads.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/loading.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\n\nconst model = await client.llm.load(\"llama-3.2-1b-instruct\", {\n  ttl: 300, // 300 seconds\n});\n```\n\n----------------------------------------\n\nTITLE: Defining LLMPredictionConfigInput Fields in LMS Params\nDESCRIPTION: This code snippet defines the fields of the LLMPredictionConfigInput interface using a custom 'lms_params' format. Each field is described with its name, type, optionality, and a detailed description of its purpose and behavior in the context of LLM predictions.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_7_api-reference/llm-prediction-config-input.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: \"maxTokens\"\n  type: \"number | false\"\n  optional: true\n  description: \"Number of tokens to predict at most. If set to false, the model will predict as many tokens as it wants.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `maxPredictedTokensReached`.\"\n\n- name: \"temperature\"\n  type: \"number\"\n  optional: true\n  description: \"The temperature parameter for the prediction model. A higher value makes the predictions more random, while a lower value makes the predictions more deterministic. The value should be between 0 and 1.\"\n\n- name: \"stopStrings\"\n  type: \"Array<string>\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `stopStringFound`.\"\n\n- name: \"toolCallStopStrings\"\n  type: \"Array<string>\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop with the `stopReason` `toolCalls`.\"\n\n- name: \"contextOverflowPolicy\"\n  type: \"LLMContextOverflowPolicy\"\n  optional: true\n  description: \"The behavior for when the generated tokens length exceeds the context window size. The allowed values are:\\n\\n- `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context window size. If the generation is stopped because of this limit, the `stopReason` in the prediction stats will be set to `contextLengthReached`\\n- `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.\\n- `rollingWindow`: Maintain a rolling window and truncate past messages.\"\n\n- name: \"structured\"\n  type: \"ZodType<TStructuredOutputType> | LLMStructuredPredictionSetting\"\n  optional: true\n  description: \"Configures the model to output structured JSON data that follows a specific schema defined using Zod.\\n\\nWhen you provide a Zod schema, the model will be instructed to generate JSON that conforms to that schema rather than free-form text.\\n\\nThis is particularly useful for extracting specific data points from model responses or when you need the output in a format that can be directly used by your application.\"\n\n- name: \"topKSampling\"\n  type: \"number\"\n  optional: true\n  description: \"Controls token sampling diversity by limiting consideration to the K most likely next tokens.\\n\\nFor example, if set to 40, only the 40 tokens with the highest probabilities will be considered for the next token selection. A lower value (e.g., 20) will make the output more focused and conservative, while a higher value (e.g., 100) allows for more creative and diverse outputs.\\n\\nTypical values range from 20 to 100.\"\n\n- name: \"repeatPenalty\"\n  type: \"number | false\"\n  optional: true\n  description: \"Applies a penalty to repeated tokens to prevent the model from getting stuck in repetitive patterns.\\n\\nA value of 1.0 means no penalty. Values greater than 1.0 increase the penalty. For example, 1.2 would reduce the probability of previously used tokens by 20%. This is particularly useful for preventing the model from repeating phrases or getting stuck in loops.\\n\\nSet to false to disable the penalty completely.\"\n\n- name: \"minPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Sets a minimum probability threshold that a token must meet to be considered for generation.\\n\\nFor example, if set to 0.05, any token with less than 5% probability will be excluded from consideration. This helps filter out unlikely or irrelevant tokens, potentially improving output quality.\\n\\nValue should be between 0 and 1. Set to false to disable this filter.\"\n\n- name: \"topPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Implements nucleus sampling by only considering tokens whose cumulative probabilities reach a specified threshold.\\n\\nFor example, if set to 0.9, the model will consider only the most likely tokens that together add up to 90% of the probability mass. This helps balance between diversity and quality by dynamically adjusting the number of tokens considered based on their probability distribution.\\n\\nValue should be between 0 and 1. Set to false to disable nucleus sampling.\"\n\n- name: \"xtcProbability\"\n  type: \"number | false\"\n  optional: true\n  description: \"Controls how often the XTC (Exclude Top Choices) sampling technique is applied during generation.\\n\\nXTC sampling can boost creativity and reduce clichs by occasionally filtering out common tokens. For example, if set to 0.3, there's a 30% chance that XTC sampling will be applied when generating each token.\\n\\nValue should be between 0 and 1. Set to false to disable XTC completely.\"\n\n- name: \"xtcThreshold\"\n  type: \"number | false\"\n  optional: true\n  description: \"Defines the lower probability threshold for the XTC (Exclude Top Choices) sampling technique.\\n\\nWhen XTC sampling is activated (based on xtcProbability), the algorithm identifies tokens with probabilities between this threshold and 0.5, then removes all such tokens except the least probable one. This helps introduce more diverse and unexpected tokens into the generation.\\n\\nOnly takes effect when xtcProbability is enabled.\"\n\n- name: \"cpuThreads\"\n  type: \"number\"\n  optional: true\n  description: \"Specifies the number of CPU threads to allocate for model inference.\\n\\nHigher values can improve performance on multi-core systems but may compete with other processes. For example, on an 8-core system, a value of 4-6 might provide good performance while leaving resources for other tasks.\\n\\nIf not specified, the system will use a default value based on available hardware.\"\n\n- name: \"draftModel\"\n  type: \"string\"\n  optional: true\n  description: \"The draft model to use for speculative decoding. Speculative decoding is a technique that can drastically increase the generation speed (up to 3x for larger models) by paring a main model with a smaller draft model.\\n\\nSee here for more information: https://lmstudio.ai/docs/advanced/speculative-decoding\\n\\nYou do not need to load the draft model yourself. Simply specifying its model key here is enough.\"\n```\n\n----------------------------------------\n\nTITLE: Quantized Model Download Command\nDESCRIPTION: Shows how to download a specific quantization of a model using the @ notation\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/get.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nlms get llama-3.1-8b@q4_k_m\n```\n\n----------------------------------------\n\nTITLE: Downloading Embedding Model with LMStudio CLI\nDESCRIPTION: This command downloads the 'nomic-ai/nomic-embed-text-v1.5' embedding model using the LMStudio CLI. This step is a prerequisite for generating embeddings.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_embedding/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms get nomic-ai/nomic-embed-text-v1.5\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Functions in JSON\nDESCRIPTION: Example JSON structure for defining tool functions that can be used by the language model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_delivery_date\",\n      \"description\": \"Get the delivery date for a customer's order\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"order_id\": {\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\"order_id\"]\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Load Parameters with load_new_instance()\nDESCRIPTION: Examples of setting load-time parameters using load_new_instance() method, which creates and loads a new model instance with specified configuration.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/parameters.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nclient = lms.get_default_client()\nmodel = client.llm.load_new_instance(\"qwen2.5-7b-instruct\", config={\n    \"contextLength\": 8192,\n    \"gpuOffload\": 0.5,\n})\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\nwith lms.Client() as client:\n    model = client.llm.load_new_instance(\n        \"qwen2.5-7b-instruct\",\n        config={\n            \"contextLength\": 8192,\n            \"gpuOffload\": 0.5,\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Auto Unload Timer with .model() in LM Studio SDK\nDESCRIPTION: This code demonstrates setting an auto-unload timer (TTL) when accessing a model using the .model() method. Note that the TTL is only set if the model is loaded from this call.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/loading.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model(\"llama-3.2-1b-instruct\", {\n  // Note: specifying ttl in `.model` will only set the TTL for the model if the model is\n  // loaded from this call. If the model was already loaded, the TTL will not be updated.\n  ttl: 300, // 300 seconds\n});\n```\n\n----------------------------------------\n\nTITLE: Single Model API Response Format in JSON\nDESCRIPTION: Displays the response format when requesting information about a specific model. The response includes model ID, type, architecture, compatibility, quantization state, and context length.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"qwen2-vl-7b-instruct\",\n  \"object\": \"model\",\n  \"type\": \"vlm\",\n  \"publisher\": \"mlx-community\",\n  \"arch\": \"qwen2_vl\",\n  \"compatibility_type\": \"mlx\",\n  \"quantization\": \"4bit\",\n  \"state\": \"not-loaded\",\n  \"max_context_length\": 32768\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting to Remote LM Studio Instance with lms ls in Shell\nDESCRIPTION: This command allows listing models on a remote LM Studio instance, useful for managing multiple installations.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ls.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nlms ls --host <host>\n```\n\n----------------------------------------\n\nTITLE: Loading Models with lms\nDESCRIPTION: Commands to load models with various options, including GPU usage and context length. Also shows how to assign a custom identifier to a loaded model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nlms load [--gpu=max|auto|0.0-1.0] [--context-length=1-N]\n\nlms load TheBloke/phi-2-GGUF --identifier=\"gpt-4-turbo\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Model Load Configuration using TypeScript SDK\nDESCRIPTION: Demonstrates how to use the LM Studio SDK to fetch the load configuration of a loaded model. This code initializes a client connection and retrieves the load configuration settings for either an LLM or embedding model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/6_model-info/_get-load-config.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst model = await client.llm.model();\n\nloadConfig = await model.getLoadConfig()\n```\n\n----------------------------------------\n\nTITLE: Streaming ChoiceDeltaToolCall Response Format in Python\nDESCRIPTION: Demonstrates the format of streamed tool call chunks from the /v1/chat/completions endpoint. Shows how function calls are broken into pieces during streaming, with function names and arguments sent incrementally.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nChoiceDeltaToolCall(index=0, id='814890118', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='San Francisco', name=None), type=None)\nChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)\n```\n\n----------------------------------------\n\nTITLE: Supported LM Studio API Parameters\nDESCRIPTION: List of parameters supported by LM Studio's OpenAI-compatible API for controlling text generation behavior, including temperature, max tokens, sampling parameters, and more.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel\ntop_p\ntop_k\nmessages\ntemperature\nmax_tokens\nstream\nstop\npresence_penalty\nfrequency_penalty\nlogit_bias\nrepeat_penalty\nseed\n```\n\n----------------------------------------\n\nTITLE: Filtering LLM Models with lms ls in Shell\nDESCRIPTION: This command filters the list to show only Large Language Models (LLMs) in LM Studio.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ls.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlms ls --llm\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema for Structured Responses in Python\nDESCRIPTION: Demonstrates how to define a JSON schema for a book, which can be used to enforce structured responses from a language model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_llm-prediction/structured-response.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# A JSON schema for a book\nschema = {\n  \"type\": \"object\",\n  \"properties\": {\n    \"title\": { \"type\": \"string\" },\n    \"author\": { \"type\": \"string\" },\n    \"year\": { \"type\": \"integer\" },\n  },\n  \"required\": [\"title\", \"author\", \"year\"],\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Available Models with GET Request\nDESCRIPTION: Demonstrates how to list all loaded and downloaded models using the /api/v0/models endpoint. Returns information about model state, type, and technical specifications.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:1234/api/v0/models\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Advanced Agent in Python\nDESCRIPTION: This snippet imports necessary Python libraries for URL parsing, browser interaction, datetime handling, file system operations, and OpenAI API integration.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom urllib.parse import urlparse\nimport webbrowser\nfrom datetime import datetime\nimport os\nfrom openai import OpenAI\n```\n\n----------------------------------------\n\nTITLE: Modifying TypeScript OpenAI Client for LM Studio Integration\nDESCRIPTION: Shows how to modify the base URL in a TypeScript OpenAI client to point to the local LM Studio server instead of OpenAI's servers.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n+  baseUrl: \"http://localhost:1234/v1\"\n});\n\n// ... the rest of your code ...\n```\n\n----------------------------------------\n\nTITLE: Filtering Embedding Models with lms ls in Shell\nDESCRIPTION: This command filters the list to show only embedding models in LM Studio.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ls.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nlms ls --embedding\n```\n\n----------------------------------------\n\nTITLE: Defining LLMPredictionConfigInput Fields in LMS Params\nDESCRIPTION: This code snippet defines the fields for the LLMPredictionConfigInput, including their types, optionality, and descriptions. It covers various aspects of LLM prediction configuration such as token limits, temperature, stopping conditions, and advanced sampling techniques.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/7_api-reference/llm-prediction-config-input.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: \"maxTokens\"\n  type: \"number | false\"\n  optional: true\n  description: \"Number of tokens to predict at most. If set to false, the model will predict as many tokens as it wants.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `maxPredictedTokensReached`.\"\n\n- name: \"temperature\"\n  type: \"number\"\n  optional: true\n  description: \"The temperature parameter for the prediction model. A higher value makes the predictions more random, while a lower value makes the predictions more deterministic. The value should be between 0 and 1.\"\n\n- name: \"stopStrings\"\n  type: \"Array<string>\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop.\\n\\nWhen the prediction is stopped because of this limit, the `stopReason` in the prediction stats will be set to `stopStringFound`.\"\n\n- name: \"toolCallStopStrings\"\n  type: \"Array<string>\"\n  optional: true\n  description: \"An array of strings. If the model generates one of these strings, the prediction will stop with the `stopReason` `toolCalls`.\"\n\n- name: \"contextOverflowPolicy\"\n  type: \"LLMContextOverflowPolicy\"\n  optional: true\n  description: \"The behavior for when the generated tokens length exceeds the context window size. The allowed values are:\\n\\n- `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context window size. If the generation is stopped because of this limit, the `stopReason` in the prediction stats will be set to `contextLengthReached`\\n- `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.\\n- `rollingWindow`: Maintain a rolling window and truncate past messages.\"\n\n- name: \"structured\"\n  type: \"ZodType<TStructuredOutputType> | LLMStructuredPredictionSetting\"\n  optional: true\n  description: \"Configures the model to output structured JSON data that follows a specific schema defined using Zod.\\n\\nWhen you provide a Zod schema, the model will be instructed to generate JSON that conforms to that schema rather than free-form text.\\n\\nThis is particularly useful for extracting specific data points from model responses or when you need the output in a format that can be directly used by your application.\"\n\n- name: \"topKSampling\"\n  type: \"number\"\n  optional: true\n  description: \"Controls token sampling diversity by limiting consideration to the K most likely next tokens.\\n\\nFor example, if set to 40, only the 40 tokens with the highest probabilities will be considered for the next token selection. A lower value (e.g., 20) will make the output more focused and conservative, while a higher value (e.g., 100) allows for more creative and diverse outputs.\\n\\nTypical values range from 20 to 100.\"\n\n- name: \"repeatPenalty\"\n  type: \"number | false\"\n  optional: true\n  description: \"Applies a penalty to repeated tokens to prevent the model from getting stuck in repetitive patterns.\\n\\nA value of 1.0 means no penalty. Values greater than 1.0 increase the penalty. For example, 1.2 would reduce the probability of previously used tokens by 20%. This is particularly useful for preventing the model from repeating phrases or getting stuck in loops.\\n\\nSet to false to disable the penalty completely.\"\n\n- name: \"minPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Sets a minimum probability threshold that a token must meet to be considered for generation.\\n\\nFor example, if set to 0.05, any token with less than 5% probability will be excluded from consideration. This helps filter out unlikely or irrelevant tokens, potentially improving output quality.\\n\\nValue should be between 0 and 1. Set to false to disable this filter.\"\n\n- name: \"topPSampling\"\n  type: \"number | false\"\n  optional: true\n  description: \"Implements nucleus sampling by only considering tokens whose cumulative probabilities reach a specified threshold.\\n\\nFor example, if set to 0.9, the model will consider only the most likely tokens that together add up to 90% of the probability mass. This helps balance between diversity and quality by dynamically adjusting the number of tokens considered based on their probability distribution.\\n\\nValue should be between 0 and 1. Set to false to disable nucleus sampling.\"\n\n- name: \"xtcProbability\"\n  type: \"number | false\"\n  optional: true\n  description: \"Controls how often the XTC (Exclude Top Choices) sampling technique is applied during generation.\\n\\nXTC sampling can boost creativity and reduce clichs by occasionally filtering out common tokens. For example, if set to 0.3, there's a 30% chance that XTC sampling will be applied when generating each token.\\n\\nValue should be between 0 and 1. Set to false to disable XTC completely.\"\n\n- name: \"xtcThreshold\"\n  type: \"number | false\"\n  optional: true\n  description: \"Defines the lower probability threshold for the XTC (Exclude Top Choices) sampling technique.\\n\\nWhen XTC sampling is activated (based on xtcProbability), the algorithm identifies tokens with probabilities between this threshold and 0.5, then removes all such tokens except the least probable one. This helps introduce more diverse and unexpected tokens into the generation.\\n\\nOnly takes effect when xtcProbability is enabled.\"\n\n- name: \"cpuThreads\"\n  type: \"number\"\n  optional: true\n  description: \"Specifies the number of CPU threads to allocate for model inference.\\n\\nHigher values can improve performance on multi-core systems but may compete with other processes. For example, on an 8-core system, a value of 4-6 might provide good performance while leaving resources for other tasks.\\n\\nIf not specified, the system will use a default value based on available hardware.\"\n\n- name: \"draftModel\"\n  type: \"string\"\n  optional: true\n  description: \"The draft model to use for speculative decoding. Speculative decoding is a technique that can drastically increase the generation speed (up to 3x for larger models) by paring a main model with a smaller draft model.\\n\\nSee here for more information: https://lmstudio.ai/docs/advanced/speculative-decoding\\n\\nYou do not need to load the draft model yourself. Simply specifying its model key here is enough.\"\n```\n\n----------------------------------------\n\nTITLE: Text Embeddings API Response Format in JSON\nDESCRIPTION: Displays the response format for text embeddings. The response includes a vector representation of the input text (embedding) along with model information and usage statistics.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"embedding\": [\n        -0.016731496900320053,\n        0.028460891917347908,\n        -0.1407836228609085,\n        ... (truncated for brevity) ...,\n        0.02505224384367466,\n        -0.0037634256295859814,\n        -0.04341062530875206\n      ],\n      \"index\": 0\n    }\n  ],\n  \"model\": \"text-embedding-nomic-embed-text-v1.5@q4_k_m\",\n  \"usage\": {\n    \"prompt_tokens\": 0,\n    \"total_tokens\": 0\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Tool-Enhanced Chatbot Interaction in Console\nDESCRIPTION: Shows a sample interaction with the streaming chatbot implementation, demonstrating how the assistant responds to a user query, calls the time tool, and provides a natural response with the tool's output.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\n-> % python tool-streaming-chatbot.py\nAssistant: Hi! I am an AI agent empowered with the ability to tell the current time (Type 'quit' to exit)\n\nYou: Tell me a joke, then tell me the current time\n\nA: Sure! Here's a light joke for you: Why don't scientists trust atoms? Because they make up everything.\n\nNow, let me get the current time for you.\n\n**Calling Tool: get_current_time**\n\nThe current time is 18:49:31. Enjoy your day!\n\nYou:\n```\n\n----------------------------------------\n\nTITLE: Retrieving Load Config using LM Studio Python SDK (Convenience API)\nDESCRIPTION: This snippet demonstrates how to get the load configuration of a model using the LM Studio Python SDK's convenience API. It creates an LLM instance and calls the get_load_config() method to retrieve the configuration.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/6_model-info/get-load-config.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm()\n\nprint(model.get_load_config())\n```\n\n----------------------------------------\n\nTITLE: Listing Loaded Models with LM Studio CLI\nDESCRIPTION: Shows how to use the `lms ps` command to display information about all models currently loaded in memory. The output includes model identifiers, types, paths, sizes, and architectures.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ps.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlms ps\n```\n\n----------------------------------------\n\nTITLE: Defining LLMLoadModelConfig Parameters in LMS\nDESCRIPTION: This code snippet defines the parameters for the LLMLoadModelConfig interface. It includes options for GPU settings, context length, RoPE adjustments, evaluation settings, performance optimizations, and memory management.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_7_api-reference/llm-load-model-config.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: gpu\n  description: |\n    How to distribute the work to your GPUs. See {@link GPUSetting} for more information.\n  public: true\n  type: GPUSetting\n  optional: true\n\n- name: contextLength\n  description: |\n    The size of the context length in number of tokens. This will include both the prompts and the\n    responses. Once the context length is exceeded, the value set in\n    {@link LLMPredictionConfigBase#contextOverflowPolicy} is used to determine the behavior.\n\n    See {@link LLMContextOverflowPolicy} for more information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyBase\n  description: |\n    Custom base frequency for rotary positional embeddings (RoPE).\n\n    This advanced parameter adjusts how positional information is embedded in the model's\n    representations. Increasing this value may enable better performance at high context lengths by\n    modifying how the model processes position-dependent information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyScale\n  description: |\n    Scaling factor for RoPE (Rotary Positional Encoding) frequency.\n\n    This factor scales the effective context window by modifying how positional information is\n    encoded. Higher values allow the model to handle longer contexts by making positional encoding\n    more granular, which can be particularly useful for extending a model beyond its original\n    training context length.\n  type: number\n  optional: true\n\n- name: evalBatchSize\n  description: |\n    Number of input tokens to process together in a single batch during evaluation.\n\n    Increasing this value typically improves processing speed and throughput by leveraging\n    parallelization, but requires more memory. Finding the optimal batch size often involves\n    balancing between performance gains and available hardware resources.\n  type: number\n  optional: true\n\n- name: flashAttention\n  description: |\n    Enables Flash Attention for optimized attention computation.\n\n    Flash Attention is an efficient implementation that reduces memory usage and speeds up\n    generation by optimizing how attention mechanisms are computed. This can significantly\n    improve performance on compatible hardware, especially for longer sequences.\n  type: boolean\n  optional: true\n\n- name: keepModelInMemory\n  description: |\n    When enabled, prevents the model from being swapped out of system memory.\n\n    This option reserves system memory for the model even when portions are offloaded to GPU,\n    ensuring faster access times when the model needs to be used. Improves performance\n    particularly for interactive applications, but increases overall RAM requirements.\n  type: boolean\n  optional: true\n\n- name: seed\n  description: |\n    Random seed value for model initialization to ensure reproducible outputs.\n\n    Setting a specific seed ensures that random operations within the model (like sampling)\n    produce the same results across different runs, which is important for reproducibility\n    in testing and development scenarios.\n  type: number\n  optional: true\n\n- name: useFp16ForKVCache\n  description: |\n    When enabled, stores the key-value cache in half-precision (FP16) format.\n\n    This option significantly reduces memory usage during inference by using 16-bit floating\n    point numbers instead of 32-bit for the attention cache. While this may slightly reduce\n    numerical precision, the impact on output quality is generally minimal for most applications.\n  type: boolean\n  optional: true\n\n- name: tryMmap\n  description: |\n    Attempts to use memory-mapped (mmap) file access when loading the model.\n\n    Memory mapping can improve initial load times by mapping model files directly from disk to\n    memory, allowing the operating system to handle paging. This is particularly beneficial for\n    quick startup, but may reduce performance if the model is larger than available system RAM,\n    causing frequent disk access.\n  type: boolean\n  optional: true\n\n- name: numExperts\n  description: |\n    Specifies the number of experts to use for models with Mixture of Experts (MoE) architecture.\n\n    MoE models contain multiple \"expert\" networks that specialize in different aspects of the task.\n    This parameter controls how many of these experts are active during inference, affecting both\n    performance and quality of outputs. Only applicable for models designed with the MoE architecture.\n  type: number\n  optional: true\n\n- name: llamaKCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's key cache.\n\n    This option determines the precision level used to store the key component of the attention\n    mechanism's cache. Lower precision values (e.g., 4-bit or 8-bit quantization) significantly\n    reduce memory usage during inference but may slightly impact output quality. The effect varies\n    between different models, with some being more robust to quantization than others.\n\n    Set to false to disable quantization and use full precision.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n\n- name: llamaVCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's value cache.\n\n    Similar to the key cache quantization, this option controls the precision used for the value\n    component of the attention mechanism's cache. Reducing precision saves memory but may affect\n    generation quality. This option requires Flash Attention to be enabled to function properly.\n\n    Different models respond differently to value cache quantization, so experimentation may be\n    needed to find the optimal setting for a specific use case. Set to false to disable quantization.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n```\n\n----------------------------------------\n\nTITLE: Text Completions API Response Format in JSON\nDESCRIPTION: Shows the response format for text completions. The response includes the generated text, usage statistics, performance metrics, and detailed model information similar to the chat completions response.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/rest.md#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"cmpl-p9rtxv6fky2v9k8jrd8cc\",\n  \"object\": \"text_completion\",\n  \"created\": 1731990488,\n  \"model\": \"granite-3.0-2b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" to find your purpose, and once you have\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"completion_tokens\": 9,\n    \"total_tokens\": 14\n  },\n  \"stats\": {\n    \"tokens_per_second\": 57.69230769230769,\n    \"time_to_first_token\": 0.299,\n    \"generation_time\": 0.156,\n    \"stop_reason\": \"maxPredictedTokensReached\"\n  },\n  \"model_info\": {\n    \"arch\": \"granite\",\n    \"quant\": \"Q4_K_M\",\n    \"format\": \"gguf\",\n    \"context_length\": 4096\n  },\n  \"runtime\": {\n    \"name\": \"llama.cpp-mac-arm64-apple-metal-advsimd\",\n    \"version\": \"1.3.0\",\n    \"supported_formats\": [\"gguf\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: LLMLoadModelConfig Parameters Documentation\nDESCRIPTION: Comprehensive configuration parameters for loading and initializing language models, including GPU distribution, context handling, memory management, and model-specific optimizations.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/7_api-reference/llm-load-model-config.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: gpu\n  description: |\n    How to distribute the work to your GPUs. See {@link GPUSetting} for more information.\n  public: true\n  type: GPUSetting\n  optional: true\n\n- name: contextLength\n  description: |\n    The size of the context length in number of tokens. This will include both the prompts and the\n    responses. Once the context length is exceeded, the value set in\n    {@link LLMPredictionConfigBase#contextOverflowPolicy} is used to determine the behavior.\n\n    See {@link LLMContextOverflowPolicy} for more information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyBase\n  description: |\n    Custom base frequency for rotary positional embeddings (RoPE).\n\n    This advanced parameter adjusts how positional information is embedded in the model's\n    representations. Increasing this value may enable better performance at high context lengths by\n    modifying how the model processes position-dependent information.\n  type: number\n  optional: true\n\n- name: ropeFrequencyScale\n  description: |\n    Scaling factor for RoPE (Rotary Positional Encoding) frequency.\n\n    This factor scales the effective context window by modifying how positional information is\n    encoded. Higher values allow the model to handle longer contexts by making positional encoding\n    more granular, which can be particularly useful for extending a model beyond its original\n    training context length.\n  type: number\n  optional: true\n\n- name: evalBatchSize\n  description: |\n    Number of input tokens to process together in a single batch during evaluation.\n\n    Increasing this value typically improves processing speed and throughput by leveraging\n    parallelization, but requires more memory. Finding the optimal batch size often involves\n    balancing between performance gains and available hardware resources.\n  type: number\n  optional: true\n\n- name: flashAttention\n  description: |\n    Enables Flash Attention for optimized attention computation.\n\n    Flash Attention is an efficient implementation that reduces memory usage and speeds up\n    generation by optimizing how attention mechanisms are computed. This can significantly\n    improve performance on compatible hardware, especially for longer sequences.\n  type: boolean\n  optional: true\n\n- name: keepModelInMemory\n  description: |\n    When enabled, prevents the model from being swapped out of system memory.\n\n    This option reserves system memory for the model even when portions are offloaded to GPU,\n    ensuring faster access times when the model needs to be used. Improves performance\n    particularly for interactive applications, but increases overall RAM requirements.\n  type: boolean\n  optional: true\n\n- name: seed\n  description: |\n    Random seed value for model initialization to ensure reproducible outputs.\n\n    Setting a specific seed ensures that random operations within the model (like sampling)\n    produce the same results across different runs, which is important for reproducibility\n    in testing and development scenarios.\n  type: number\n  optional: true\n\n- name: useFp16ForKVCache\n  description: |\n    When enabled, stores the key-value cache in half-precision (FP16) format.\n\n    This option significantly reduces memory usage during inference by using 16-bit floating\n    point numbers instead of 32-bit for the attention cache. While this may slightly reduce\n    numerical precision, the impact on output quality is generally minimal for most applications.\n  type: boolean\n  optional: true\n\n- name: tryMmap\n  description: |\n    Attempts to use memory-mapped (mmap) file access when loading the model.\n\n    Memory mapping can improve initial load times by mapping model files directly from disk to\n    memory, allowing the operating system to handle paging. This is particularly beneficial for\n    quick startup, but may reduce performance if the model is larger than available system RAM,\n    causing frequent disk access.\n  type: boolean\n  optional: true\n\n- name: numExperts\n  description: |\n    Specifies the number of experts to use for models with Mixture of Experts (MoE) architecture.\n\n    MoE models contain multiple \"expert\" networks that specialize in different aspects of the task.\n    This parameter controls how many of these experts are active during inference, affecting both\n    performance and quality of outputs. Only applicable for models designed with the MoE architecture.\n  type: number\n  optional: true\n\n- name: llamaKCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's key cache.\n\n    This option determines the precision level used to store the key component of the attention\n    mechanism's cache. Lower precision values (e.g., 4-bit or 8-bit quantization) significantly\n    reduce memory usage during inference but may slightly impact output quality. The effect varies\n    between different models, with some being more robust to quantization than others.\n\n    Set to false to disable quantization and use full precision.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n\n- name: llamaVCacheQuantizationType\n  description: |\n    Quantization type for the Llama model's value cache.\n\n    Similar to the key cache quantization, this option controls the precision used for the value\n    component of the attention mechanism's cache. Reducing precision saves memory but may affect\n    generation quality. This option requires Flash Attention to be enabled to function properly.\n\n    Different models respond differently to value cache quantization, so experimentation may be\n    needed to find the optimal setting for a specific use case. Set to false to disable quantization.\n  type: LLMLlamaCacheQuantizationType | false\n  optional: true\n```\n\n----------------------------------------\n\nTITLE: System Prompt with Tool Definition\nDESCRIPTION: Example of how tool definitions are injected into the system prompt for the Qwen2.5-Instruct model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{\"type\": \"function\", \"function\": {\"name\": \"get_delivery_date\", \"description\": \"Get the delivery date for a customer's order\", \"parameters\": {\"type\": \"object\", \"properties\": {\"order_id\": {\"type\": \"string\"}}, \"required\": [\"order_id\"]}}}\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n```\n\n----------------------------------------\n\nTITLE: Generating JSON Output of Loaded Models in LM Studio\nDESCRIPTION: Demonstrates how to get the list of loaded models in a machine-readable JSON format using the `--json` flag with the `lms ps` command.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ps.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlms ps --json\n```\n\n----------------------------------------\n\nTITLE: Handling Tool Calls in Python\nDESCRIPTION: Pseudocode showing how to handle tool calls in Python, including executing functions and updating the conversation.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# pseudocode, see examples for copy-paste snippets\nif response.has_tool_calls:\n    for each tool_call:\n        # Extract function name & args\n        function_to_call = tool_call.name     # e.g. \"get_delivery_date\"\n        args = tool_call.arguments            # e.g. {\"order_id\": \"123\"}\n\n        # Execute the function\n        result = execute_function(function_to_call, args)\n\n        # Add result to conversation\n        add_to_messages([\n            ASSISTANT_TOOL_CALL_MESSAGE,      # The request to use the tool\n            TOOL_RESULT_MESSAGE               # The tool's response\n        ])\nelse:\n    # Normal response without tools\n    add_to_messages(response.content)\n```\n\n----------------------------------------\n\nTITLE: Listing Supported OpenAI-like API Endpoints in LM Studio\nDESCRIPTION: Shows the available OpenAI-compatible API endpoints supported by LM Studio, including models, chat completions, embeddings, and completions.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nGET  /v1/models\nPOST /v1/chat/completions\nPOST /v1/embeddings\nPOST /v1/completions\n```\n\n----------------------------------------\n\nTITLE: Defining a Zod Schema for Book Object in TypeScript\nDESCRIPTION: Creates a Zod schema that defines the structure of a book object with title, author, and year properties. This schema will be used to enforce the model's response format.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/structured-response.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\n\n// A zod schema for a book\nconst bookSchema = z.object({\n  title: z.string(),\n  author: z.string(),\n  year: z.number().int(),\n});\n```\n\n----------------------------------------\n\nTITLE: Starting LM Studio Server via CLI\nDESCRIPTION: Command line instruction for starting the LM Studio server programmatically using the lms CLI tool. Requires prior setup of the lms command line interface.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/headless.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms server start\n```\n\n----------------------------------------\n\nTITLE: GPU Offload Configuration\nDESCRIPTION: Examples of loading a model with different GPU offload settings.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/load.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nlms load <model_key> --gpu 0.5    # Offload 50% of layers to GPU\nlms load <model_key> --gpu max    # Offload all layers to GPU\nlms load <model_key> --gpu off    # Disable GPU offloading\n```\n\n----------------------------------------\n\nTITLE: Unloading Models with lms\nDESCRIPTION: Command to unload models, with an option to unload all loaded models at once.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nlms unload [--all]\n```\n\n----------------------------------------\n\nTITLE: Creating a File Tool with External Effects in TypeScript\nDESCRIPTION: Defines a tool that creates files on the local filesystem. This example shows how to implement tools with external effects, using Node.js file system operations to create a new file with specified content. It includes error handling for existing files.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_agent/tools.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from \"@lmstudio/sdk\";\nimport { existsSync } from \"fs\";\nimport { writeFile } from \"fs/promises\";\nimport { z } from \"zod\";\n\nconst createFileTool = tool({\n  name: \"createFile\",\n  description: \"Create a file with the given name and content.\",\n  parameters: { name: z.string(), content: z.string() },\n  implementation: async ({ name, content }) => {\n    if (existsSync(name)) {\n      return \"Error: File already exists.\";\n    }\n    await writeFile(name, content, \"utf-8\");\n    return \"File created.\";\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Starting LM Studio Server on Custom Port\nDESCRIPTION: This command demonstrates how to start the LM Studio server on a specific port (3000 in this example) using the --port parameter.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-start.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nlms server start --port 3000\n```\n\n----------------------------------------\n\nTITLE: Starting LM Studio Server via CLI\nDESCRIPTION: Command to start LM Studio as a local server using the lms CLI tool.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms server start\n```\n\n----------------------------------------\n\nTITLE: Unloading Model from Memory with LM Studio SDK in TypeScript\nDESCRIPTION: This code demonstrates how to unload a model from memory using the .unload() method on the model handle when it's no longer needed.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/5_manage-models/loading.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model();\nawait model.unload();\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters for LM Studio Server Start Command\nDESCRIPTION: This snippet defines the parameters for the `lms server start` command, including the optional port number and CORS flag.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-start.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: \"--port\"\n  type: \"number\"\n  optional: true\n  description: \"Port to run the server on. If not provided, uses the last used port\"\n- name: \"--cors\"\n  type: \"flag\"\n  optional: true\n  description: \"Enable CORS support for web application development. When not set, CORS is disabled\"\n```\n\n----------------------------------------\n\nTITLE: Connecting to Remote LM Studio Instance for Model Listing\nDESCRIPTION: Shows how to use the `--host` flag with `lms ps` to connect to a remote LM Studio instance and list its loaded models. The remote instance must be running and accessible on the same subnet.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ps.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nlms ps --host <host>\n```\n\n----------------------------------------\n\nTITLE: Starting LM Studio Server via CLI\nDESCRIPTION: Command to start LM Studio as a local server using the lms CLI tool. This enables programmatic interaction with LM Studio through an OpenAI-like REST API.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/structured-output.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms server start\n```\n\n----------------------------------------\n\nTITLE: Loading Models with LMS CLI in Bash\nDESCRIPTION: This command uses the LMS CLI to load a specified model. It requires two parameters: the model name and the path to the model file. This allows users to dynamically load models into the LMStudio environment.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/_lms-load.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms load --model <model-name> --path <path-to-model>\n```\n\n----------------------------------------\n\nTITLE: Starting LM Studio Server with Default Settings\nDESCRIPTION: This command starts the LM Studio server using default settings without specifying any additional parameters.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-start.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlms server start\n```\n\n----------------------------------------\n\nTITLE: Retrieving Load Config using LM Studio Python SDK (Scoped Resource API)\nDESCRIPTION: This snippet shows how to get the load configuration of a model using the LM Studio Python SDK's scoped resource API. It creates a client context, initializes an LLM model, and calls the get_load_config() method to retrieve the configuration.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/6_model-info/get-load-config.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model()\n\n    print(model.get_load_config())\n```\n\n----------------------------------------\n\nTITLE: Obtaining LLM Model Handle\nDESCRIPTION: Shows how to obtain a specific model handle (Qwen2.5 7B Instruct) using the LMStudio client.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/chat-completion.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\nconst client = new LMStudioClient();\n\nconst model = await client.llm.model(\"qwen2.5-7b-instruct\");\n```\n\n----------------------------------------\n\nTITLE: Loading a Model via CLI\nDESCRIPTION: Command to load a model using the lms CLI tool.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlms load\n```\n\n----------------------------------------\n\nTITLE: Loading Model with TTL Using LM Studio CLI\nDESCRIPTION: This command demonstrates how to load a model using the LM Studio CLI (lms) with a specified time-to-live (TTL) of 1 hour (3600 seconds). This sets an expiration time for the loaded model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/ttl-and-auto-evict.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlms load <model> --ttl 3600\n```\n\n----------------------------------------\n\nTITLE: Basic Server Status Check Command\nDESCRIPTION: Simple command to check the basic status of the LM Studio server.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-status.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlms server status\n```\n\n----------------------------------------\n\nTITLE: Unloading Model from Memory in Python with LM Studio SDK\nDESCRIPTION: Shows how to unload a model from memory when it's no longer needed by calling the unload() method on its handle.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/loading.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nmodel = lms.llm()\nmodel.unload()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    model = client.llm.model()\n    model.unload()\n```\n\n----------------------------------------\n\nTITLE: Example Log Stream Output\nDESCRIPTION: Shows sample output from the log stream command, including timestamp, prediction type, model identifier, and the exact input prompt sent to the model.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/log-stream.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ lms log stream\nI Streaming logs from LM Studio\n\ntimestamp: 5/2/2024, 9:49:47 PM\ntype: llm.prediction.input\nmodelIdentifier: TheBloke/TinyLlama-1.1B-1T-OpenOrca-GGUF/tinyllama-1.1b-1t-openorca.Q2_K.gguf\nmodelPath: TheBloke/TinyLlama-1.1B-1T-OpenOrca-GGUF/tinyllama-1.1b-1t-openorca.Q2_K.gguf\ninput: \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nHello, what's your name?\n### Response:\n\"\n```\n\n----------------------------------------\n\nTITLE: LM Studio Load Command Parameters\nDESCRIPTION: Definition of available parameters for the lms load command including path, ttl, gpu settings, context length and identifier options.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/load.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: \"[path]\"\n  type: \"string\"\n  optional: true\n  description: \"The path of the model to load. If not provided, you will be prompted to select one\"\n- name: \"--ttl\"\n  type: \"number\"\n  optional: true\n  description: \"If provided, when the model is not used for this number of seconds, it will be unloaded\"\n- name: \"--gpu\"\n  type: \"string\"\n  optional: true\n  description: \"How much to offload to the GPU. Values: 0-1, off, max\"\n- name: \"--context-length\"\n  type: \"number\"\n  optional: true\n  description: \"The number of tokens to consider as context when generating text\"\n- name: \"--identifier\"\n  type: \"string\"\n  optional: true\n  description: \"The identifier to assign to the loaded model for API reference\"\n```\n\n----------------------------------------\n\nTITLE: Managing Local Server with lms\nDESCRIPTION: Commands to start and stop the local LM Studio server using the `lms` CLI.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlms server start\nlms server stop\n```\n\n----------------------------------------\n\nTITLE: JSON Status Output Command\nDESCRIPTION: Command to get the server status in JSON format with quiet logging.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-status.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nlms server status --json --quiet\n```\n\n----------------------------------------\n\nTITLE: Remote Instance Connection\nDESCRIPTION: Command to load a model on a remote LM Studio instance using the host flag.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/load.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nlms load <model_key> --host <host>\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Tool Use with Curl in Bash\nDESCRIPTION: This snippet shows how to make a chat completion request with tool use to LM Studio using curl. It includes the request payload with a tool definition and explains the expected response format.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"lmstudio-community/qwen2.5-7b-instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What dell products do you have under $50 in electronics?\"}],\n    \"tools\": [\n      {\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"search_products\",\n          \"description\": \"Search the product catalog by various criteria. Use this whenever a customer asks about product availability, pricing, or specifications.\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"query\": {\n                \"type\": \"string\",\n                \"description\": \"Search terms or product name\"\n              },\n              \"category\": {\n                \"type\": \"string\",\n                \"description\": \"Product category to filter by\",\n                \"enum\": [\"electronics\", \"clothing\", \"home\", \"outdoor\"]\n              },\n              \"max_price\": {\n                \"type\": \"number\",\n                \"description\": \"Maximum price in dollars\"\n              }\n            },\n            \"required\": [\"query\"],\n            \"additionalProperties\": false\n          }\n        }\n      }\n    ]\n  }'\n```\n\n----------------------------------------\n\nTITLE: LM Studio Pro Tip for Setting Default Load Parameters\nDESCRIPTION: A markdown block explaining the reasons why users might want to set default load parameters for models, including GPU offload settings, context size configuration, and Flash Attention utilization.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/3_advanced/per-model.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n#### Reasons to set default load parameters (not required, totally optional)\n\n- Set a particular GPU offload settings for a given model\n- Set a particular context size for a given model\n- Whether or not to utilize Flash Attention for a given model\n```\n\n----------------------------------------\n\nTITLE: Listing All Models with lms ls in Shell\nDESCRIPTION: This command lists all downloaded models in LM Studio, showing their parameters, architecture, and size.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ls.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlms ls\n```\n\n----------------------------------------\n\nTITLE: Remote Model Unloading\nDESCRIPTION: Shell command example for unloading a model from a remote LM Studio instance using the --host flag.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/unload.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nlms unload <model_key> --host <host>\n```\n\n----------------------------------------\n\nTITLE: Basic Log Streaming Command\nDESCRIPTION: The basic command to start streaming logs from LM Studio.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/log-stream.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlms log stream\n```\n\n----------------------------------------\n\nTITLE: Setting Auto-unload Timer\nDESCRIPTION: Command to load a model with an automatic unload timer (TTL).\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/load.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nlms load <model_key> --ttl 3600   # Unload after 1 hour of inactivity\n```\n\n----------------------------------------\n\nTITLE: Installing lms on macOS/Linux\nDESCRIPTION: Command to bootstrap the `lms` CLI tool on macOS or Linux systems by adding it to the system path.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n~/.lmstudio/bin/lms bootstrap\n```\n\n----------------------------------------\n\nTITLE: Listing Local Models with lms\nDESCRIPTION: Command to list all local models available in the LM Studio models directory.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nlms ls\n```\n\n----------------------------------------\n\nTITLE: LM Studio Server Status Example Usage\nDESCRIPTION: Complete example showing server start and status check commands with their outputs.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-status.md#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n  ~ lms server start\nStarting server...\nWaking up LM Studio service...\nSuccess! Server is now running on port 1234\n\n  ~ lms server status\nThe server is running on port 1234.\n```\n\n----------------------------------------\n\nTITLE: Format Filter Commands\nDESCRIPTION: Commands to filter search results by model format (MLX or GGUF)\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/get.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nlms get --mlx\nlms get --gguf\n```\n\n----------------------------------------\n\nTITLE: Defining LMS Unload Command Parameters\nDESCRIPTION: Parameter specifications for the lms unload command, including model_key for specific model unloading, --all flag for unloading all models, and --host for remote connections.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/unload.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_params\nCODE:\n```\n- name: \"[model_key]\"\n  type: \"string\"\n  optional: true\n  description: \"The key of the model to unload. If not provided, you will be prompted to select one\"\n- name: \"--all\"\n  type: \"flag\"\n  optional: true\n  description: \"Unload all currently loaded models\"\n- name: \"--host\"\n  type: \"string\"\n  optional: true\n  description: \"The host address of a remote LM Studio instance to connect to\"\n```\n\n----------------------------------------\n\nTITLE: Downloading models with lms get command\nDESCRIPTION: Shows how to download models directly from the terminal using the 'lms get' command with a keyword. This feature was introduced in LM Studio 0.3.5.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/api-changelog.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlms get deepseek-r1\n```\n\n----------------------------------------\n\nTITLE: Listing Loaded Models with lms\nDESCRIPTION: Command to list all currently loaded models in LM Studio.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nlms ps\n```\n\n----------------------------------------\n\nTITLE: Logging Control Commands\nDESCRIPTION: Commands demonstrating different logging verbosity options for status checks.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-status.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nlms server status --verbose\nlms server status --quiet\nlms server status --log-level debug\n```\n\n----------------------------------------\n\nTITLE: Display Option Commands\nDESCRIPTION: Commands to modify the display behavior of search results and download options\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/get.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nlms get --always-show-all-results\nlms get --always-show-download-options\n```\n\n----------------------------------------\n\nTITLE: LM Studio Client Integration\nDESCRIPTION: TypeScript example showing how to initialize LM Studio client, load a model and generate predictions using streaming.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/_template_dont_edit.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// index.ts\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\n// Create a client to connect to LM Studio, then load a model\nasync function main() {\n  const client = new LMStudioClient();\n  const model = await client.llm.load(\"meta-llama-3-8b\");\n\n  const prediction = model.predict(\"Once upon a time, there was a\");\n\n  for await (const text of prediction) {\n    process.stdout.write(text);\n  }\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Setting TTL for API-loaded models with lms CLI\nDESCRIPTION: Demonstrates how to set a Time-To-Live (TTL) for models using the lms command-line interface. This determines how long a model stays loaded when idle.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/api-changelog.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlms load --ttl <seconds>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Available Models with cURL in LM Studio\nDESCRIPTION: A cURL example to fetch the list of currently loaded models from the LM Studio API using the /v1/models endpoint.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:1234/v1/models\n```\n\n----------------------------------------\n\nTITLE: Result Limit Command\nDESCRIPTION: Command to limit the number of search results displayed\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/get.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nlms get --limit 5\n```\n\n----------------------------------------\n\nTITLE: Creating New LMStudio Node Projects\nDESCRIPTION: Commands to create new Node.js projects with TypeScript or JavaScript support using the LMStudio CLI.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/project-setup.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms create node-typescript\n```\n\nLANGUAGE: bash\nCODE:\n```\nlms create node-javascript\n```\n\n----------------------------------------\n\nTITLE: Installing LM Studio SDK using Package Managers\nDESCRIPTION: Commands for installing the @lmstudio/sdk package using different Node.js package managers (npm, yarn, and pnpm).\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @lmstudio/sdk --save\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @lmstudio/sdk\n```\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @lmstudio/sdk\n```\n\n----------------------------------------\n\nTITLE: Representing Single-Message Chats with a String in TypeScript\nDESCRIPTION: Shows how to use a single string to represent a chat with only one user message using the .respond method.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/working-with-chats.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst prediction = model.respond(\"What is the meaning of life?\");\n```\n\n----------------------------------------\n\nTITLE: Displaying Detailed Model Information with lms ls in Shell\nDESCRIPTION: This command provides detailed information about each model in LM Studio.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ls.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nlms ls --detailed\n```\n\n----------------------------------------\n\nTITLE: Downloading Embedding Model with LMStudio CLI\nDESCRIPTION: Command to download the nomic-ai/nomic-embed-text-v1.5 embedding model using the LMStudio CLI tool.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/3_embedding/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms get nomic-ai/nomic-embed-text-v1.5\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama Model using CLI\nDESCRIPTION: Command line instruction for downloading the required Llama 3.2 1B model using the LM Studio CLI.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlms get llama-3.2-1b-instruct\n```\n\n----------------------------------------\n\nTITLE: Installing lmstudio Python SDK with pip\nDESCRIPTION: Command to install the lmstudio Python SDK package using pip package manager.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lmstudio\n```\n\n----------------------------------------\n\nTITLE: Applying Prompt Template to Message Array in TypeScript\nDESCRIPTION: Shows how to apply a prompt template to an array of messages using the LM Studio SDK. This approach demonstrates working with message arrays that can be converted to Chat objects for template application.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/_more/_apply-prompt-template.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LMStudioClient } from \"@lmstudio/sdk\";\n\nconst client = new LMStudioClient();\nconst llm = await client.llm.model(); // Use any loaded LLM\n\nconst formatted = await llm.applyPromptTemplate([\n  { role: \"system\", content: \"You are a helpful assistant.\" },\n  { role: \"user\", content: \"What is LM Studio?\" },\n]);\nconsole.info(formatted);\n```\n\n----------------------------------------\n\nTITLE: Accessing Conversations Directory Path on Mac/Linux\nDESCRIPTION: Shows the filesystem path where LM Studio stores chat conversations on Mac and Linux systems.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/chat.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n~/.lmstudio/conversations/\n```\n\n----------------------------------------\n\nTITLE: Outputting Model List in JSON Format with lms ls in Shell\nDESCRIPTION: This command outputs the list of models in JSON format for easy parsing and integration with other tools.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/ls.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nlms ls --json\n```\n\n----------------------------------------\n\nTITLE: Markdown Header Definition for System Client Reference\nDESCRIPTION: YAML frontmatter metadata block defining the page title, sidebar title, description and index for the client.system API reference documentation\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_7_api-reference/system-namespace.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: \"`client.system` Reference\"\nsidebar_title: \"`client.system` namespace\"\ndescription: \"`client.system` - API reference for the system namespace in an `LMStudioClient` instance\"\nindex: 6\n---\n```\n\n----------------------------------------\n\nTITLE: Modifying cURL Request for LM Studio OpenAI API\nDESCRIPTION: Shows how to modify a cURL request intended for OpenAI to work with LM Studio, by changing the endpoint URL and model identifier.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/1_endpoints/openai.md#2025-04-22_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- curl https://api.openai.com/v1/chat/completions \\\n+ curl http://localhost:1234/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n-     \"model\": \"gpt-4o-mini\",\n+     \"model\": \"use the model identifier from LM Studio here\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Installing LMStudio SDK in Existing Projects\nDESCRIPTION: Package manager commands to install the @lmstudio/sdk package in existing Node.js projects using npm, yarn, or pnpm.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/project-setup.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @lmstudio/sdk --save\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @lmstudio/sdk\n```\n\nLANGUAGE: bash\nCODE:\n```\npnpm add @lmstudio/sdk\n```\n\n----------------------------------------\n\nTITLE: LM Studio Offline Notice\nDESCRIPTION: Notification explaining that LM Studio's core functions like chatting with models, document interactions, and local server operations do not require internet connectivity.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/offline.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_notice\nCODE:\n```\nIn general, LM Studio does not require the internet in order to work. This includes core functions like chatting with models, chatting with documents, or running a local server, none of which require the internet.\n```\n\n----------------------------------------\n\nTITLE: Unloading a Specific Model\nDESCRIPTION: Shell command example for unloading a single model from memory using its model key.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/unload.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlms unload <model_key>\n```\n\n----------------------------------------\n\nTITLE: Parameter List Configuration\nDESCRIPTION: Shows how to format a list of parameters using the lms_params component with name, type, optional status, and descriptions.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n```lms_params\n- name: \"[path]\"\n  type: \"string\"\n  optional: true\n  description: \"The path of the model to load. If not provided, you will be prompted to select one\"\n- name: \"--ttl\"\n  type: \"number\"\n  optional: true\n  description: \"If provided, when the model is not used for this number of seconds, it will be unloaded\"\n- name: \"--gpu\"\n  type: \"string\"\n  optional: true\n  description: \"How much to offload to the GPU. Values: 0-1, off, max\"\n- name: \"--context-length\"\n  type: \"number\"\n  optional: true\n  description: \"The number of tokens to consider as context when generating text\"\n- name: \"--identifier\"\n  type: \"string\"\n  optional: true\n  description: \"The identifier to assign to the loaded model for API reference\"\n```\n```\n\n----------------------------------------\n\nTITLE: Downloading models with lms get using Hugging Face URL\nDESCRIPTION: Demonstrates how to download models from a specific Hugging Face URL using the 'lms get' command in the terminal.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/api-changelog.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlms get <hugging face url>\n```\n\n----------------------------------------\n\nTITLE: Stopping LM Studio Server using CLI Command\nDESCRIPTION: This command gracefully stops the running LM Studio server instance. Any active requests will be terminated when the server is stopped. The server can be restarted using the 'lms server start' command.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-stop.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlms server stop\n```\n\n----------------------------------------\n\nTITLE: Accessing Settings with Keyboard Shortcut in LM Studio\nDESCRIPTION: This code snippet provides a tip for quickly accessing the Settings tab from anywhere in the LM Studio application using keyboard shortcuts, which vary by operating system.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/2_user-interface/themes.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```lms_protip\nYou can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.\n```\n```\n\n----------------------------------------\n\nTITLE: Installing lms on Windows\nDESCRIPTION: PowerShell command to bootstrap the `lms` CLI tool on Windows systems by adding it to the system path.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncmd /c %USERPROFILE%/.lmstudio/bin/lms.exe bootstrap\n```\n\n----------------------------------------\n\nTITLE: Configuring Article Frontmatter in YAML\nDESCRIPTION: Example frontmatter configuration for a new documentation article, showing how to set title, description and index properties.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Keyboard Shortcuts\ndescription: Learn about the keyboard shortcuts in LM Studio\nindex: 2\n---\n```\n\n----------------------------------------\n\nTITLE: Prompting LLM with Updated Messages\nDESCRIPTION: Example of how to prompt the LLM again with updated messages after tool use, without giving access to tools.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Example messages\nmessages = [\n    {\"role\": \"user\", \"content\": \"When will order 123 be delivered?\"},\n    {\"role\": \"assistant\", \"function_call\": {\n        \"name\": \"get_delivery_date\",\n        \"arguments\": {\"order_id\": \"123\"}\n    }},\n    {\"role\": \"tool\", \"content\": \"2024-03-15\"},\n]\nresponse = client.chat.completions.create(\n    model=\"lmstudio-community/qwen2.5-7b-instruct\",\n    messages=messages\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying lms Installation\nDESCRIPTION: Sample output of running the `lms` command to verify installation and view available subcommands.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ lms\nlms - LM Studio CLI - v0.2.22\nGitHub: https://github.com/lmstudio-ai/lmstudio-cli\n\nUsage\nlms <subcommand>\n\nwhere <subcommand> can be one of:\n\n- status - Prints the status of LM Studio\n- server - Commands for managing the local server\n- ls - List all downloaded models\n- ps - List all loaded models\n- load - Load a model\n- unload - Unload a model\n- create - Create a new project with scaffolding\n- log - Log operations. Currently only supports streaming logs from LM Studio via `lms log stream`\n- version - Prints the version of the CLI\n- bootstrap - Bootstrap the CLI\n\nFor more help, try running `lms <subcommand> --help`\n```\n\n----------------------------------------\n\nTITLE: Horizontal Stack Layout Example\nDESCRIPTION: Demonstrates how to create a two-column layout using the lms_hstack component for side-by-side content display.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n```lms_hstack\n\n# Column 1\n\n~~~js\nconsole.log(\"Hello from the code block\");\n~~~\n\n:::split:::\n\n# Column 2\nSecond column markdown content here\n\n```\n```\n\n----------------------------------------\n\nTITLE: Importing GGUF Models via Command Line in LM Studio\nDESCRIPTION: Command line interface for importing external GGUF model files into LM Studio using the experimental lms import command.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/import-model.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlms import <path/to/model.gguf>\n```\n\n----------------------------------------\n\nTITLE: Example Output of Downloaded Models Listing in LM Studio\nDESCRIPTION: Shows the formatted output when listing downloaded models. The output includes model keys, display names, architecture information, and capabilities such as vision support.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/list-downloaded.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDownloadedLlm(model_key='qwen2.5-7b-instruct-1m', display_name='Qwen2.5 7B Instruct 1M', architecture='qwen2', vision=False)\nDownloadedEmbeddingModel(model_key='text-embedding-nomic-embed-text-v1.5', display_name='Nomic Embed Text v1.5', architecture='nomic-bert')\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Instance in TypeScript\nDESCRIPTION: Creates a new Chat instance with optional initial messages. The constructor takes an array of ChatMessage objects and a ChatOptions object for configuration.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/_7_api-reference/chat.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst chat = new Chat([\n  { role: \"system\", content: \"You are a helpful assistant.\" },\n  { role: \"user\", content: \"Hello!\" }\n], {\n  maxTokens: 100,\n  temperature: 0.7\n});\n```\n\n----------------------------------------\n\nTITLE: Setting TTL for API-loaded models with cURL\nDESCRIPTION: Shows how to set a Time-To-Live (TTL) in seconds for models loaded via API requests using cURL. The TTL parameter determines how long a model stays loaded when idle before being automatically evicted.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/api-changelog.md#2025-04-22_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-qwen-7b\",\n    \"messages\": [ ... ]\n+   \"ttl\": 300,\n}'\n```\n\n----------------------------------------\n\nTITLE: Locating Presets Storage Directory on Windows\nDESCRIPTION: Shows the file path where Config Presets are stored on Windows systems.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/presets.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n%USERPROFILE%\\.lmstudio\\config-presets\n```\n\n----------------------------------------\n\nTITLE: LM Studio Example Directory Structure\nDESCRIPTION: Demonstrates a specific example of the directory structure using the ocelot-v1 model from infra-ai publisher.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/import-model.md#2025-04-22_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n~/.lmstudio/models/\n infra-ai/\n     ocelot-v1/\n         ocelot-v1-instruct-q4_0.gguf\n```\n\n----------------------------------------\n\nTITLE: Installing lmstudio-python with Package Managers\nDESCRIPTION: Examples of how to install the lmstudio package using different Python package managers (pip, pdm, and uv).\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/1_getting-started/project-setup.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lmstudio\n```\n\nLANGUAGE: bash\nCODE:\n```\npdm add lmstudio\n```\n\nLANGUAGE: bash\nCODE:\n```\nuv add lmstudio\n```\n\n----------------------------------------\n\nTITLE: Constructing Chat Objects with Chat.from Method in TypeScript\nDESCRIPTION: Demonstrates how to quickly construct Chat objects using the Chat.from method, with examples for both array of messages and single string inputs.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/working-with-chats.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst chat = Chat.from([\n  { role: \"system\", content: \"You are a resident AI philosopher.\" },\n  { role: \"user\", content: \"What is the meaning of life?\" },\n]);\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// This constructs a chat with a single user message\nconst chat = Chat.from(\"What is the meaning of life?\");\n```\n\n----------------------------------------\n\nTITLE: Filtering for MLX models when downloading\nDESCRIPTION: Shows how to filter for MLX models only when downloading models using the 'lms get' command by adding the --mlx flag.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/0_root/api-changelog.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nlms get deepseek-r1 --mlx\n```\n\n----------------------------------------\n\nTITLE: Setting TTL in OpenAI API Request for JIT Model Loading in LM Studio\nDESCRIPTION: This snippet demonstrates how to set a time-to-live (TTL) value of 5 minutes (300 seconds) for a model when making a request to the OpenAI compatibility API in LM Studio. The TTL is specified in the request payload.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/ttl-and-auto-evict.md#2025-04-22_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\ncurl http://localhost:1234/api/v0/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"deepseek-r1-distill-qwen-7b\",\n+   \"ttl\": 300,\n    \"messages\": [ ... ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Frontmatter for Inference Parameters Page\nDESCRIPTION: YAML frontmatter block defining the title and description metadata for a documentation page about inference parameters in LM Studio.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/_configuration/inference.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Inference parameters\ndescription: Configurable parameters for inference in LM Studio\n---\n```\n\n----------------------------------------\n\nTITLE: Multi-Language Code Example with TypeScript and Python\nDESCRIPTION: Demonstrates how to create multi-language code snippets using the lms_code_snippet component with TypeScript and Python variants.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```lms_code_snippet\n  variants:\n    TypeScript:\n      language: typescript\n      code: |\n        // Multi-line TypeScript code\n        function hello() {\n          console.log(\"hey\")\n          return \"world\"\n        }\n\n    Python:\n      language: python\n      code: |\n        # Multi-line Python code\n        def hello():\n            print(\"hey\")\n            return \"world\"\n```\n```\n\n----------------------------------------\n\nTITLE: LM Studio Base Directory Structure\nDESCRIPTION: Shows the expected base directory structure for models in LM Studio, following the publisher/model/file hierarchy pattern.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/import-model.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n~/.lmstudio/models/\n publisher/\n     model/\n         model-file.gguf\n```\n\n----------------------------------------\n\nTITLE: Python Generator Implementation\nDESCRIPTION: Python implementation of a hello function that prints 'hey' and returns 'world'.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/_template_dont_edit.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Multi-line Python code\ndef hello():\n    print(\"hey\")\n    return \"world\"\n```\n\n----------------------------------------\n\nTITLE: Single Language Code Example with Python\nDESCRIPTION: Shows how to create a single-language code snippet with a title using the lms_code_snippet component.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n```lms_code_snippet\n  title: \"generator.py\"\n  variants:\n    Python:\n      language: python\n      code: |\n        # Multi-line Python code\n        def hello():\n            print(\"hey\")\n            return \"world\"\n```\n```\n\n----------------------------------------\n\nTITLE: Displaying Informational Box in LM Studio Documentation\nDESCRIPTION: This snippet demonstrates how to create an informational box in the LM Studio documentation using a custom 'lms_info' code block. It explains terminology related to open-source and open-weights models, and their file formats.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```lms_info\nYou might sometimes see terms such as `open-source models` or `open-weights models`. Different models might be released under different licenses and varying degrees of 'openness'. In order to run a model locally, you need to be able to get access to its \"weights\", often distributed as one or more files that end with `.gguf`, `.safetensors` etc.\n```\n```\n\n----------------------------------------\n\nTITLE: Multi-language Function Implementation\nDESCRIPTION: Example function implementation showing equivalent code in TypeScript and Python that prints 'hey' and returns 'world'.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/_template_dont_edit.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Multi-line TypeScript code\nfunction hello() {\n  console.log(\"hey\")\n  return \"world\"\n}\n```\n\nLANGUAGE: python\nCODE:\n```\n# Multi-line Python code\ndef hello():\n    print(\"hey\")\n    return \"world\"\n```\n\n----------------------------------------\n\nTITLE: Locating Presets Storage Directory on macOS or Linux\nDESCRIPTION: Shows the file path where Config Presets are stored on macOS or Linux systems.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/presets.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n~/.lmstudio/config-presets\n```\n\n----------------------------------------\n\nTITLE: Simple JavaScript Console Log\nDESCRIPTION: Basic JavaScript example showing console output.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/_template_dont_edit.md#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(\"Hello from the code block\");\n```\n\n----------------------------------------\n\nTITLE: Accessing Conversations Directory Path on Windows\nDESCRIPTION: Shows the filesystem path where LM Studio stores chat conversations on Windows systems.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/chat.md#2025-04-22_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\n%USERPROFILE%\\.lmstudio\\conversations\n```\n\n----------------------------------------\n\nTITLE: Accessing Settings with Keyboard Shortcuts in LM Studio\nDESCRIPTION: Provides a keyboard shortcut tip for quickly accessing the Settings tab from anywhere in the LM Studio application. The shortcut is cmd + , on macOS or ctrl + , on Windows/Linux systems.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/1_basics/_connect-apps.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_protip\nCODE:\n```\nYou can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.\n```\n\n----------------------------------------\n\nTITLE: Model Tool Call Example\nDESCRIPTION: Example of how a model might request a tool call in its output.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/4_api/tools.md#2025-04-22_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<tool_call>\n{\"name\": \"get_delivery_date\", \"arguments\": {\"order_id\": \"123\"}}\n</tool_call>\n```\n\n----------------------------------------\n\nTITLE: Starting LM Studio Server with CORS Support\nDESCRIPTION: This command shows how to start the LM Studio server with CORS support enabled, which may be necessary for web applications or certain VS Code extensions.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-start.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nlms server start --cors\n```\n\n----------------------------------------\n\nTITLE: Unloading All Models\nDESCRIPTION: Shell command example for unloading all currently loaded models at once using the --all flag.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/unload.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nlms unload --all\n```\n\n----------------------------------------\n\nTITLE: Defining a JSON Schema for Book Object in TypeScript\nDESCRIPTION: Creates a JSON schema that defines the structure of a book object with title, author, and year properties. This schema will be used to enforce the model's response format.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/2_llm-prediction/structured-response.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// A JSON schema for a book\nconst schema = {\n  type: \"object\",\n  properties: {\n    title: { type: \"string\" },\n    author: { type: \"string\" },\n    year: { type: \"integer\" },\n  },\n  required: [\"title\", \"author\", \"year\"],\n};\n```\n\n----------------------------------------\n\nTITLE: Listing Downloaded Models in LM Studio using Python Scoped Resource API\nDESCRIPTION: Demonstrates how to list downloaded models using the scoped resource API with a client context. Shows methods for listing all models or filtering by model type (LLM or embedding models).\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/1_python/5_manage-models/list-downloaded.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lmstudio as lms\n\nwith lms.Client() as client:\n    downloaded = client.list_downloaded_models()\n    llm_only = client.llm.list_downloaded()\n    embedding_only = client.embedding.list_downloaded()\n\nfor model in downloaded:\n    print(model)\n```\n\n----------------------------------------\n\nTITLE: Settings Shortcut Tip for LM Studio\nDESCRIPTION: Shows keyboard shortcuts for quickly accessing the Settings menu in LM Studio across different operating systems. Press cmd + , on macOS or ctrl + , on Windows/Linux to open Settings.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/0_app/2_user-interface/languages.md#2025-04-22_snippet_0\n\nLANGUAGE: lms_protip\nCODE:\n```\nYou can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.\n```\n\n----------------------------------------\n\nTITLE: Example Output of LM Studio Server Stop Command\nDESCRIPTION: This snippet shows the expected output when successfully stopping the LM Studio server. It indicates the port number on which the server was running before being stopped.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/3_cli/server-stop.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nStopped the server on port 1234.\n```\n\n----------------------------------------\n\nTITLE: Defining Frontmatter in Markdown Documentation\nDESCRIPTION: YAML frontmatter block that defines metadata for a documentation page. It specifies the title as 'Overview', includes a placeholder for description, and sets the index value to 1.\nSOURCE: https://github.com/lmstudio-ai/docs/blob/main/2_typescript/3_agent/_index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Overview\ndescription: TODO...\nindex: 1\n---\n```"
  }
]
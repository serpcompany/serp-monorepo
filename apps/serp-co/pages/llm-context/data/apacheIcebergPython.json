[
  {
    "owner": "apache",
    "repo": "iceberg-python",
    "content": "TITLE: Querying Table with Filter, Columns, and Limit in PyIceberg\nDESCRIPTION: This snippet demonstrates how to perform a table scan with a filter, selected columns, and a limit using PyIceberg. It loads a table from a catalog and applies a filter to select rows where 'trip_distance' is greater than or equal to 10.0, selects specific fields, and limits the number of rows returned.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\nfrom pyiceberg.expressions import GreaterThanOrEqual\n\ncatalog = load_catalog(\"default\")\ntable = catalog.load_table(\"nyc.taxis\")\n\nscan = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n    limit=100,\n)\n\n# Or filter using a string predicate\nscan = table.scan(\n    row_filter=\"trip_distance > 10.0\",\n)\n\n[task.file.file_path for task in scan.plan_files()]\n```\n\n----------------------------------------\n\nTITLE: Adding Partition Fields in Iceberg\nDESCRIPTION: Demonstrates adding new partition fields to a table using various transforms (BucketTransform, DayTransform, IdentityTransform). Includes examples of using the `add_field` API and the `identity` shortcut.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_spec() as update:\n    update.add_field(\"id\", BucketTransform(16), \"bucketed_id\")\n    update.add_field(\"event_ts\", DayTransform(), \"day_ts\")\n    # identity is a shortcut API for adding an IdentityTransform\n    update.identity(\"some_field\")\n```\n\n----------------------------------------\n\nTITLE: Creating Iceberg Table Schema\nDESCRIPTION: This snippet creates an Iceberg table schema with city, latitude, and longitude fields using the PyIceberg API.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\n\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, DoubleType\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntbl = catalog.create_table(\"default.cities\", schema=schema)\n```\n\n----------------------------------------\n\nTITLE: Moving Columns in Iceberg Schema\nDESCRIPTION: Illustrates how to change the order of columns in an Iceberg schema. Includes moving a column to the first position, moving a column after another, and moving a column within a struct.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_schema() as update:\n    update.move_first(\"symbol\")\n    # This will move `bid` after `ask`\n    update.move_after(\"bid\", \"ask\")\n    # This will move `confirmed_by` before `exchange` in the `details` struct\n    update.move_before((\"details\", \"confirmed_by\"), (\"details\", \"exchange\"))\n```\n\n----------------------------------------\n\nTITLE: Appending Data to an Iceberg Table with PyArrow\nDESCRIPTION: This snippet illustrates how to append data to an existing Iceberg table using a PyArrow Table.  The `tbl.append(df)` operation adds the data from the DataFrame `df` to the existing data in the table, creating a new buffer for this data.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pa.Table.from_pylist(\n    [{\"city\": \"Groningen\", \"lat\": 53.21917, \"long\": 6.56667}],\n)\n\ntbl.append(df)\n```\n\n----------------------------------------\n\nTITLE: Appending data to Iceberg table\nDESCRIPTION: This Python code appends the data from the PyArrow dataframe to the newly created Iceberg table. The `append` method is used to add the data. After appending, it validates the number of rows written to the table by scanning it.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntable.append(df)\nlen(table.scan().to_arrow())\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Iceberg Table with Data using Python\nDESCRIPTION: This code creates an Iceberg table named 'default.product_support_issues' with a predefined schema, and populates it with sample data using pyarrow. The schema defines fields for ticket ID, customer ID, issue description, and creation timestamp.  It assumes that `catalog` is an existing Iceberg catalog instance.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nschema = Schema(\n    NestedField(field_id=1, name='ticket_id', field_type=LongType(), required=True),\n    NestedField(field_id=2, name='customer_id', field_type=LongType(), required=True),\n    NestedField(field_id=3, name='issue', field_type=StringType(), required=False),\n    NestedField(field_id=4, name='created_at', field_type=TimestampType(), required=True),\n  required=True\n)\n\niceberg_table = catalog.create_table(\n    identifier='default.product_support_issues',\n    schema=schema\n)\n\npa_table_data = pa.Table.from_pylist(\n    [\n        {'ticket_id': 1, 'customer_id': 546, 'issue': 'User Login issue', 'created_at': 1650020000000000},\n        {'ticket_id': 2, 'customer_id': 547, 'issue': 'Payment not going through', 'created_at': 1650028640000000},\n        {'ticket_id': 3, 'customer_id': 548, 'issue': 'Error on checkout', 'created_at': 1650037280000000},\n        {'ticket_id': 4, 'customer_id': 549, 'issue': 'Unable to reset password', 'created_at': 1650045920000000},\n        {'ticket_id': 5, 'customer_id': 550, 'issue': 'Account locked', 'created_at': 1650054560000000},\n        {'ticket_id': 6, 'customer_id': 551, 'issue': 'Order not received', 'created_at': 1650063200000000},\n        {'ticket_id': 7, 'customer_id': 552, 'issue': 'Refund not processed', 'created_at': 1650071840000000},\n        {'ticket_id': 8, 'customer_id': 553, 'issue': 'Shipping address issue', 'created_at': 1650080480000000},\n        {'ticket_id': 9, 'customer_id': 554, 'issue': 'Product damaged', 'created_at': 1650089120000000},\n        {'ticket_id': 10, 'customer_id': 555, 'issue': 'Unable to apply discount code', 'created_at': 1650097760000000},\n        {'ticket_id': 11, 'customer_id': 556, 'issue': 'Website not loading', 'created_at': 1650106400000000},\n        {'ticket_id': 12, 'customer_id': 557, 'issue': 'Incorrect order received', 'created_at': 1650115040000000},\n        {'ticket_id': 13, 'customer_id': 558, 'issue': 'Unable to track order', 'created_at': 1650123680000000},\n        {'ticket_id': 14, 'customer_id': 559, 'issue': 'Order delayed', 'created_at': 1650132320000000},\n        {'ticket_id': 15, 'customer_id': 560, 'issue': 'Product not as described', 'created_at': 1650140960000000},\n        {'ticket_id': 16, 'customer_id': 561, 'issue': 'Unable to contact support', 'created_at': 1650149600000000},\n        {'ticket_id': 17, 'customer_id': 562, 'issue': 'Duplicate charge', 'created_at': 1650158240000000},\n        {'ticket_id': 18, 'customer_id': 563, 'issue': 'Unable to update profile', 'created_at': 1650166880000000},\n        {'ticket_id': 19, 'customer_id': 564, 'issue': 'App crashing', 'created_at': 1650175520000000},\n        {'ticket_id': 20, 'customer_id': 565, 'issue': 'Unable to download invoice', 'created_at': 1650184160000000},\n        {'ticket_id': 21, 'customer_id': 566, 'issue': 'Incorrect billing amount', 'created_at': 1650192800000000},\n    ], schema=iceberg_table.schema().as_arrow()\n)\n\niceberg_table.append(\n    df=pa_table_data\n)\n```\n\n----------------------------------------\n\nTITLE: Scanning Iceberg table with a row filter\nDESCRIPTION: This Python code scans the Iceberg table, filtering rows where the `tip_per_mile` column is greater than 0, effectively retrieving rows where a tip per mile value exists, demonstrating basic data exploration capabilities.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = table.scan(row_filter=\"tip_per_mile > 0\").to_arrow()\nlen(df)\n```\n\n----------------------------------------\n\nTITLE: Overwriting Iceberg Table Data with PyArrow\nDESCRIPTION: This snippet demonstrates how to overwrite the entire contents of an Iceberg table with data from a PyArrow Table. The `tbl.overwrite(df)` operation replaces all existing data in the table with the provided DataFrame `df`. Subsequent reads will only return data from the new DataFrame.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntbl.overwrite(df)\n```\n\n----------------------------------------\n\nTITLE: Convert Iceberg Table to Polars DataFrame/LazyFrame in Python\nDESCRIPTION: This snippet demonstrates how to convert an Iceberg table to a Polars DataFrame or LazyFrame. `iceberg_table.to_polars()` utilizes Polars' functionalities, while `iceberg_table.scan().to_polars()` leverages the Apache Iceberg data scanning and retrieval API before analyzing the resulting DataFrame in Polars. Requires `polars` to be installed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\n# Get LazyFrame\niceberg_table.to_polars()\n\n# Get Data Frame\niceberg_table.scan().to_polars()\n```\n\n----------------------------------------\n\nTITLE: Transaction with Schema Update in Iceberg\nDESCRIPTION: Illustrates performing a schema update within a transaction. This allows combining schema changes with other table operations within a single atomic transaction, ensuring consistency.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nwith table.transaction() as transaction:\n    with transaction.update_schema() as update_schema:\n        update.add_column(\"some_other_field\", IntegerType(), \"doc\")\n    # ... Update properties etc\n```\n\n----------------------------------------\n\nTITLE: Loading catalog and configuring for SQLite\nDESCRIPTION: This Python snippet loads the Iceberg catalog and configures it to use a SQLite database for storing metadata and a local filesystem for storing data. It uses the `load_catalog` function from the `pyiceberg.catalog` module. The `uri` parameter specifies the path to the SQLite database, and the `warehouse` parameter specifies the path to the local filesystem.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\n\nwarehouse_path = \"/tmp/warehouse\"\ncatalog = load_catalog(\n    \"default\",\n    **{\n        'type': 'sql',\n        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n        \"warehouse\": f\"file://{warehouse_path}\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Enforcing Schema When Appending Data to Iceberg\nDESCRIPTION: This snippet demonstrates how to enforce the Iceberg table schema when appending data from a PyArrow Table to avoid type errors. It loads the table schema, applies it to the DataFrame, and then appends the DataFrame to the table.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\nimport pyarrow as pa\n\ncatalog = load_catalog(\"default\")\ntable = catalog.load_table(\"default.cities\")\nschema = table.schema().as_arrow()\n\ndf = pa.Table.from_pylist(\n    [{\"city\": \"Groningen\", \"lat\": 53.21917, \"long\": 6.56667}], schema=schema\n)\n\ntable.append(df)\n```\n\n----------------------------------------\n\nTITLE: Appending Initial Data for Upsert Example\nDESCRIPTION: This snippet appends initial data to the table defined previously for the upsert operation to work.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Write some data\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"inhabitants\": 921402},\n        {\"city\": \"San Francisco\", \"inhabitants\": 808988},\n        {\"city\": \"Drachten\", \"inhabitants\": 45019},\n        {\"city\": \"Paris\", \"inhabitants\": 2103000},\n    ],\n    schema=arrow_schema\n)\ntbl.append(df)\n```\n\n----------------------------------------\n\nTITLE: Appending Initial Data to Created Iceberg Table\nDESCRIPTION: This snippet shows how to add initial data to the created Iceberg table using append operation with pyarrow.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014},\n    ],\n)\ntbl.append(df)\n```\n\n----------------------------------------\n\nTITLE: Show First Few Rows of Daft DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to display the first few rows of a Daft DataFrame, showing materialized data.  It leverages the optimized query engine and Iceberg features for efficient reads, such as hidden partitioning and file-level statistics.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndf.show(2)\n```\n\nLANGUAGE: python\nCODE:\n```\n╭──────────┬───────────────────────────────┬───────────────────────────────╮\n│ VendorID ┆ tpep_pickup_datetime          ┆ tpep_dropoff_datetime         │\n│ ---      ┆ ---                           ┆ ---                           │\n│ Int64    ┆ Timestamp(Microseconds, None) ┆ Timestamp(Microseconds, None) │\n╞══════════╪═══════════════════════════════╪═══════════════════════════════╡\n│ 2        ┆ 2008-12-31T23:23:50.000000    ┆ 2009-01-01T00:34:31.000000    │\n├╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n│ 2        ┆ 2008-12-31T23:05:03.000000    ┆ 2009-01-01T16:10:18.000000    │\n╰──────────┴───────────────────────────────┴───────────────────────────────╯\n\n(Showing first 2 rows)\n```\n\n----------------------------------------\n\nTITLE: Loading a Table from Catalog\nDESCRIPTION: This Python snippet demonstrates how to load an existing Iceberg table from a catalog using the `load_table` method. It loads the 'bids' table from the 'docs_example' namespace, returning a `Table` object. It requires a catalog instance to be loaded and the table to exist in the specified namespace.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntable = catalog.load_table(\"docs_example.bids\")\n# Equivalent to:\ntable = catalog.load_table((\"docs_example\", \"bids\"))\n# The tuple syntax can be used if the namespace or table contains a dot.\n```\n\n----------------------------------------\n\nTITLE: Convert PyIceberg Scan to Apache Arrow Table in Python\nDESCRIPTION: This snippet demonstrates how to convert a PyIceberg table scan into an Apache Arrow table. It filters the data based on 'trip_distance' and selects specific fields. This requires `pyarrow` to be installed. The resulting Arrow table is returned.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ntable.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_arrow()\n```\n\n----------------------------------------\n\nTITLE: Adding Existing Parquet Files to Iceberg Table (Python)\nDESCRIPTION: This snippet demonstrates how to add existing parquet files to an Iceberg table without rewriting them. It assumes that the parquet files have a schema consistent with the Iceberg table schema.  The file paths are provided as a list to the `add_files` method of the table object.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n# Given that these parquet files have schema consistent with the Iceberg table\n\nfile_paths = [\n    \"s3a://warehouse/default/existing-1.parquet\",\n    \"s3a://warehouse/default/existing-2.parquet\",\n]\n\n# They can be added to the table without rewriting them\n\ntbl.add_files(file_paths=file_paths)\n```\n\n----------------------------------------\n\nTITLE: Querying Iceberg Table to Polars DataFrame with Filter in Python\nDESCRIPTION: This code snippet demonstrates how to scan an Iceberg table, apply a filter to select rows where 'ticket_id' is greater than 10, and convert the result into a Polars DataFrame. This leverages Iceberg's ability to push down the filter to the storage layer, only fetching relevant data into the DataFrame.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ntable.scan(\n    row_filter=\"ticket_id > 10\",\n).to_polars()\n```\n\n----------------------------------------\n\nTITLE: Listing Tables in a Namespace\nDESCRIPTION: This Python snippet demonstrates how to list tables within a specific namespace using the `list_tables` method of a catalog instance. It lists tables in the 'docs_example' namespace. It requires a catalog instance to be loaded and a namespace with tables to be present.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncatalog.list_tables(\"docs_example\")\n```\n\n----------------------------------------\n\nTITLE: Updating Column Properties in Iceberg Schema\nDESCRIPTION: Demonstrates updating the type, nullability, or documentation of columns in an Iceberg schema.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_schema() as update:\n    # Promote a float to a double\n    update.update_column(\"bid\", field_type=DoubleType())\n    # Make a field optional\n    update.update_column(\"symbol\", required=False)\n    # Update the documentation\n    update.update_column(\"symbol\", doc=\"Name of the share on the exchange\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting Data Files in Iceberg Table (Python)\nDESCRIPTION: This code snippet demonstrates how to inspect the data files within the current snapshot of an Iceberg table by invoking the `files()` method. The returned `pyarrow.Table` includes data file metadata such as content, file path, file format, spec ID, record count, and file size.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.files()\n```\n\n----------------------------------------\n\nTITLE: Creating PyArrow Table\nDESCRIPTION: This Python code creates a PyArrow table from a list of dictionaries, each representing a row of data. This requires the `pyarrow` library. The table is then used to demonstrate the write support.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Parquet data into PyArrow dataframe\nDESCRIPTION: This Python code loads the downloaded Parquet file into a PyArrow table.  The `pyarrow.parquet` module is used for reading the parquet file.  The resulting PyArrow table will be used to create an Iceberg table.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.parquet as pq\n\ndf = pq.read_table(\"/tmp/yellow_tripdata_2023-01.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Configuring In-Memory Catalog in YAML\nDESCRIPTION: This snippet configures an in-memory catalog, which uses a SQLite in-memory database. The `type` is set to `in-memory`, and a `warehouse` location is specified for storing data files.  This catalog is useful for testing and demos, but not for production due to lack of concurrency support.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: in-memory\n    warehouse: /tmp/pyiceberg/warehouse\n```\n\n----------------------------------------\n\nTITLE: Convert PyIceberg Scan to Pandas DataFrame in Python\nDESCRIPTION: This snippet converts a PyIceberg table scan to a Pandas DataFrame. It filters data where 'trip_distance' is greater than or equal to 10.0 and selects specific fields. This approach improves performance by fetching only relevant Parquet files. Requires `pandas` to be installed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ntable.scan(\n    row_filter=\"trip_distance >= 10.0\",\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Evolving schema of Iceberg table\nDESCRIPTION: This Python code evolves the schema of the Iceberg table to include the newly created `tip_per_mile` column. It uses a context manager to update the schema and uses `union_by_name` to add the new column while ensuring existing columns remain intact.  The new schema is derived from the PyArrow dataframe.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_schema() as update_schema:\n    update_schema.union_by_name(df.schema)\n```\n\n----------------------------------------\n\nTITLE: Convert PyIceberg Scan to DuckDB Table in Python\nDESCRIPTION: This snippet demonstrates converting a PyIceberg table scan to an in-memory DuckDB table.  It filters the data based on 'trip_distance' and selects specific fields. Requires DuckDB to be installed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ncon = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_duckdb(table_name=\"distant_taxi_trips\")\n```\n\n----------------------------------------\n\nTITLE: Convert PyIceberg Scan to Arrow RecordBatchReader in Python\nDESCRIPTION: This snippet converts a PyIceberg table scan into an Apache Arrow RecordBatchReader. It filters data based on 'trip_distance' and selects specific fields, allowing data to be read one record batch at a time. Requires `pyarrow` to be installed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ntable.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_arrow_batch_reader()\n```\n\n----------------------------------------\n\nTITLE: Creating and Filtering Polars LazyFrame from Iceberg Table in Python\nDESCRIPTION: This code creates a Polars LazyFrame from an Iceberg table and applies a filter using Polars' expression language (`pl.col`).  The filter selects rows where 'ticket_id' is greater than 10. The `.collect()` method triggers the computation and returns a Polars DataFrame.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nlf = iceberg_table.to_polars().filter(pl.col(\"ticket_id\") > 10)\nprint(lf.collect())\n```\n\n----------------------------------------\n\nTITLE: Inspecting Table Entries with Iceberg Python\nDESCRIPTION: This code snippet shows how to retrieve and display all the table's current manifest entries for both data and delete files using `table.inspect.entries()`. The returned pyarrow table contains detailed information about each entry, including status, snapshot ID, file path, and metrics.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.entries()\n```\n\n----------------------------------------\n\nTITLE: Appending Data before Dynamic Partition Overwrite\nDESCRIPTION: This demonstrates how to append data to an Iceberg table before performing a dynamic partition overwrite. This populates initial data for the example.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": -48.864716, \"long\": -2.349014},\n    ],\n)\ntbl.append(df)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Metadata Log Entries in Iceberg Table (Python)\nDESCRIPTION: This code snippet demonstrates how to retrieve and display the metadata log entries of an Iceberg table using the `metadata_log_entries()` method. The result is a pyarrow.Table containing timestamp, file path, latest snapshot ID, latest schema ID, and latest sequence number.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.metadata_log_entries()\n```\n\n----------------------------------------\n\nTITLE: Describing a Table (Text Output)\nDESCRIPTION: This command describes the 'nyc.taxis' table and outputs the metadata in a human-readable text format. It displays details such as table format version, metadata location, UUID, schema, partition spec, and properties.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/cli.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n➜  pyiceberg describe nyc.taxis\nTable format version  1\nMetadata location     file:/.../nyc.db/taxis/metadata/00000-aa3a3eac-ea08-4255-b890-383a64a94e42.metadata.json\nTable UUID            6cdfda33-bfa3-48a7-a09e-7abb462e3460\nLast Updated          1661783158061\nPartition spec        []\nSort order            []\nCurrent schema        Schema, id=0\n├── 1: VendorID: optional long\n├── 2: tpep_pickup_datetime: optional timestamptz\n├── 3: tpep_dropoff_datetime: optional timestamptz\n├── 4: passenger_count: optional double\n├── 5: trip_distance: optional double\n├── 6: RatecodeID: optional double\n├── 7: store_and_fwd_flag: optional string\n├── 8: PULocationID: optional long\n├── 9: DOLocationID: optional long\n├── 10: payment_type: optional long\n├── 11: fare_amount: optional double\n├── 12: extra: optional double\n├── 13: mta_tax: optional double\n├── 14: tip_amount: optional double\n├── 15: tolls_amount: optional double\n├── 16: improvement_surcharge: optional double\n├── 17: total_amount: optional double\n├── 18: congestion_surcharge: optional double\n└── 19: airport_fee: optional double\nCurrent snapshot      Operation.APPEND: id=5937117119577207079, schema_id=0\nSnapshots             Snapshots\n└── Snapshot 5937117119577207079, schema 0: file:/.../nyc.db/taxis/metadata/snap-5937117119577207079-1-94656c4f-4c66-4600-a4ca-f30377300527.avro\nProperties            owner                 root\nwrite.format.default  parquet\n```\n\n----------------------------------------\n\nTITLE: Querying DuckDB Table and Fetching Results in Python\nDESCRIPTION: This snippet demonstrates how to query a DuckDB table created from a PyIceberg scan. It executes a SQL query to calculate the duration of taxi trips and fetches the first four results. It depends on the previous snippet where a connection `con` to the DuckDB is established.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nprint(\n    con.execute(\n        \"SELECT tpep_dropoff_datetime - tpep_pickup_datetime AS duration FROM distant_taxi_trips LIMIT 4\"\n    ).fetchall()\n)\n[\n    (datetime.timedelta(seconds=1194),),\n    (datetime.timedelta(seconds=1118),),\n    (datetime.timedelta(seconds=1697),),\n    (datetime.timedelta(seconds=1581),),\n]\n```\n\n----------------------------------------\n\nTITLE: Inspecting Iceberg Table Snapshots\nDESCRIPTION: This snippet demonstrates how to inspect the snapshots of an Iceberg table using `table.inspect.snapshots()`. Snapshots provide a history of changes made to the table over time.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.snapshots()\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Catalog Implementation\nDESCRIPTION: This YAML snippet shows how to configure PyIceberg to use a custom catalog implementation.  It sets the 'py-catalog-impl' property to the fully qualified name of the Python class implementing the catalog, allowing custom logic and behavior. Custom key-value pairs can also be included.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    py-catalog-impl: mypackage.mymodule.MyCatalog\n    custom-key1: value1\n    custom-key2: value2\n```\n\n----------------------------------------\n\nTITLE: Adding Nested Columns to Struct in Iceberg\nDESCRIPTION: Demonstrates how to add a column within a struct. The struct type must exist before columns can be added to it. This shows adding fields to a `details` struct, including adding a string field.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_schema() as update:\n    update.add_column(\"retries\", IntegerType(), \"Number of retries to place the bid\")\n    # In a struct\n    update.add_column(\"details\", StructType())\n\nwith table.update_schema() as update:\n    update.add_column((\"details\", \"confirmed_by\"), StringType(), \"Name of the exchange\")\n```\n\n----------------------------------------\n\nTITLE: Low-Level API to retrieve file paths in PyIceberg\nDESCRIPTION: This code retrieves the file paths that contain matching rows using the `plan_files` method. The method returns a set of tasks, each providing the file path of a file that might contain relevant data. It's a low-level API that requires the engine itself to filter the file.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_54\n\nLANGUAGE: json\nCODE:\n```\n[\n  \"s3://warehouse/wh/nyc/taxis/data/00003-4-42464649-92dd-41ad-b83b-dea1a2fe4b58-00001.parquet\"\n]\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns in Iceberg Schema\nDESCRIPTION: Shows how to rename columns in an Iceberg table's schema. This includes renaming top-level columns and columns within structs.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_schema() as update:\n    update.rename_column(\"retries\", \"num_retries\")\n    # This will rename `confirmed_by` to `processed_by` in the `details` struct\n    update.rename_column((\"details\", \"confirmed_by\"), \"processed_by\")\n```\n\n----------------------------------------\n\nTITLE: Pandas DataFrame Output Example in Python\nDESCRIPTION: This code provides an example of the Pandas DataFrame output, including column names and a sample of the data.  It shows the structure of the returned DataFrame containing VendorID, tpep_pickup_datetime, and tpep_dropoff_datetime.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\n        VendorID      tpep_pickup_datetime     tpep_dropoff_datetime\n0              2 2021-04-01 00:28:05+00:00 2021-04-01 00:47:59+00:00\n1              1 2021-04-01 00:39:01+00:00 2021-04-01 00:57:39+00:00\n2              2 2021-04-01 00:14:42+00:00 2021-04-01 00:42:59+00:00\n3              1 2021-04-01 00:17:17+00:00 2021-04-01 00:43:38+00:00\n4              1 2021-04-01 00:24:04+00:00 2021-04-01 00:56:20+00:00\n...          ...\n116976         2 2021-04-30 23:56:18+00:00 2021-05-01 00:29:13+00:00\n116977         2 2021-04-30 23:07:41+00:00 2021-04-30 23:37:18+00:00\n116978         2 2021-04-30 23:38:28+00:00 2021-05-01 00:12:04+00:00\n116979         2 2021-04-30 23:33:00+00:00 2021-04-30 23:59:00+00:00\n116980         2 2021-04-30 23:44:25+00:00 2021-05-01 00:14:47+00:00\n\n[116981 rows x 3 columns]\n```\n\n----------------------------------------\n\nTITLE: Convert PyIceberg Scan to Ray Dataset in Python\nDESCRIPTION: This snippet demonstrates how to convert a PyIceberg table scan into a Ray dataset.  It filters data based on 'trip_distance' and selects specific fields. Requires Ray to be installed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nray_dataset = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_ray()\n```\n\n----------------------------------------\n\nTITLE: Merging Schemas with Union by Name in Iceberg\nDESCRIPTION: Shows how to merge another schema into an existing schema based on field names using `union_by_name()`. This simplifies schema evolution by avoiding manual field-ID management.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, DoubleType, LongType\n\ncatalog = load_catalog()\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntable = catalog.create_table(\"default.locations\", schema)\n\nnew_schema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n    NestedField(10, \"population\", LongType(), required=False),\n)\n\nwith table.update_schema() as update:\n    update.union_by_name(new_schema)\n```\n\n----------------------------------------\n\nTITLE: Describing a Table (JSON Output)\nDESCRIPTION: This command describes the 'nyc.taxis' table and outputs the metadata in JSON format, using `jq` to format the output for readability.  The `--output json` flag is crucial for obtaining the JSON format, enabling automation and programmatic parsing of the metadata.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/cli.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n➜  pyiceberg --output json describe nyc.taxis | jq\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"identifier\": [\n    \"nyc\",\n    \"taxis\"\n  ],\n  \"metadata_location\": \"file:/.../nyc.db/taxis/metadata/00000-aa3a3eac-ea08-4255-b890-383a64a94e42.metadata.json\",\n  \"metadata\": {\n    \"location\": \"file:/.../nyc.db/taxis\",\n    \"table-uuid\": \"6cdfda33-bfa3-48a7-a09e-7abb462e3460\",\n    \"last-updated-ms\": 1661783158061,\n    \"last-column-id\": 19,\n    \"schemas\": [\n      {\n        \"type\": \"struct\",\n        \"fields\": [\n          {\n            \"id\": 1,\n            \"name\": \"VendorID\",\n            \"type\": \"long\",\n            \"required\": false\n          },\n...\n          {\n            \"id\": 19,\n            \"name\": \"airport_fee\",\n            \"type\": \"double\",\n            \"required\": false\n          }\n        ],\n        \"schema-id\": 0,\n        \"identifier-field-ids\": []\n      }\n    ],\n    \"current-schema-id\": 0,\n    \"partition-specs\": [\n      {\n        \"spec-id\": 0,\n        \"fields\": []\n      }\n    ],\n    \"default-spec-id\": 0,\n    \"last-partition-id\": 999,\n    \"properties\": {\n      \"owner\": \"root\",\n      \"write.format.default\": \"parquet\"\n    },\n    \"current-snapshot-id\": 5937117119577207000,\n    \"snapshots\": [\n      {\n        \"snapshot-id\": 5937117119577207000,\n        \"timestamp-ms\": 1661783158061,\n        \"manifest-list\": \"file:/.../nyc.db/taxis/metadata/snap-5937117119577207079-1-94656c4f-4c66-4600-a4ca-f30377300527.avro\",\n        \"summary\": {\n          \"operation\": \"append\",\n          \"spark.app.id\": \"local-1661783139151\",\n          \"added-data-files\": \"1\",\n          \"added-records\": \"2979431\",\n          \"added-files-size\": \"46600777\",\n          \"changed-partition-count\": \"1\",\n          \"total-records\": \"2979431\",\n          \"total-files-size\": \"46600777\",\n          \"total-data-files\": \"1\",\n          \"total-delete-files\": \"0\",\n          \"total-position-deletes\": \"0\",\n          \"total-equality-deletes\": \"0\"\n        },\n        \"schema-id\": 0\n      }\n    ],\n    \"snapshot-log\": [\n      {\n        \"snapshot-id\": \"5937117119577207079\",\n        \"timestamp-ms\": 1661783158061\n      }\n    ],\n    \"metadata-log\": [],\n    \"sort-orders\": [\n      {\n        \"order-id\": 0,\n        \"fields\": []\n      }\n    ],\n    \"default-sort-order-id\": 0,\n    \"refs\": {\n      \"main\": {\n        \"snapshot-id\": 5937117119577207000,\n        \"type\": \"branch\"\n      }\n    },\n    \"format-version\": 1,\n    \"schema\": {\n      \"type\": \"struct\",\n      \"fields\": [\n        {\n          \"id\": 1,\n          \"name\": \"VendorID\",\n          \"type\": \"long\",\n          \"required\": false\n        },\n...\n        {\n          \"id\": 19,\n          \"name\": \"airport_fee\",\n          \"type\": \"double\",\n          \"required\": false\n        }\n      ],\n      \"schema-id\": 0,\n      \"identifier-field-ids\": []\n    },\n    \"partition-spec\": []\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Columns in Iceberg Schema\nDESCRIPTION: Shows how to delete columns from an Iceberg schema. Requires `allow_incompatible_changes` because deleting a column can break existing readers and writers.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_schema(allow_incompatible_changes=True) as update:\n    update.delete_column(\"some_field\")\n    # In a struct\n    update.delete_column((\"details\", \"confirmed_by\"))\n```\n\n----------------------------------------\n\nTITLE: Setting Table Properties in Iceberg\nDESCRIPTION: Demonstrates setting and removing table properties using the `Transaction` API, both with and without a context manager.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nwith table.transaction() as transaction:\n    transaction.set_properties(abc=\"def\")\n\nassert table.properties == {\"abc\": \"def\"}\n\nwith table.transaction() as transaction:\n    transaction.remove_properties(\"abc\")\n\nassert table.properties == {}\n```\n\nLANGUAGE: python\nCODE:\n```\ntable = table.transaction().set_properties(abc=\"def\").commit_transaction()\n\nassert table.properties == {\"abc\": \"def\"}\n\ntable = table.transaction().remove_properties(\"abc\").commit_transaction()\n\nassert table.properties == {}\n```\n\n----------------------------------------\n\nTITLE: Creating new column in PyArrow dataframe\nDESCRIPTION: This Python code creates a new column named `tip_per_mile` in the PyArrow dataframe by dividing the `tip_amount` column by the `trip_distance` column using the `pyarrow.compute` module.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.compute as pc\n\ndf = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Unified AWS Credentials\nDESCRIPTION: This YAML configuration demonstrates how to set AWS credentials for both Glue/DynamoDB Catalog and S3 FileIO using the `client.*` properties. This allows sharing the same credentials for both services, simplifying configuration. Overrides are possible using service-specific properties.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: glue\n    client.access-key-id: <ACCESS_KEY_ID>\n    client.secret-access-key: <SECRET_ACCESS_KEY>\n    client.region: <REGION_NAME>\n```\n\n----------------------------------------\n\nTITLE: Convert Iceberg Table to Daft DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to convert an Iceberg table to a Daft DataFrame and apply filtering and selection operations. It filters for rows where 'trip_distance' is greater than or equal to 10.0 and selects specific columns. Requires Daft to be installed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndf = table.to_daft()  # equivalent to `daft.read_iceberg(table)`\ndf = df.where(df[\"trip_distance\"] >= 10.0)\ndf = df.select(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n```\n\n----------------------------------------\n\nTITLE: Adding a deprecation notice for properties or behavior change (Python)\nDESCRIPTION: This code snippet demonstrates how to add a deprecation notice for a property or behavior change using the `deprecation_message` function. It's helpful for notifying users about deprecated properties or changes in behavior, suggesting an alternative approach or property to use.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.utils.deprecated import deprecation_message\n\ndeprecation_message(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"The old_property is deprecated. Please use the something_else property instead.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Snapshots in Iceberg\nDESCRIPTION: Demonstrates creating tags and branches to manage snapshots with operations through the `Table` API, with and without a context manager.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n# To run a specific operation\ntable.manage_snapshots().create_tag(snapshot_id, \"tag123\").commit()\n# To run multiple operations\ntable.manage_snapshots()\n    .create_tag(snapshot_id1, \"tag123\")\n    .create_tag(snapshot_id2, \"tag456\")\n    .commit()\n# Operations are applied on commit.\n```\n\nLANGUAGE: python\nCODE:\n```\nwith table.manage_snapshots() as ms:\n    ms.create_branch(snapshot_id1, \"Branch_A\").create_tag(snapshot_id2, \"tag789\")\n```\n\n----------------------------------------\n\nTITLE: REST Catalog Configuration in YAML\nDESCRIPTION: Example YAML configuration for a REST catalog in PyIceberg. This configuration specifies the URI, credentials, and SSL settings for connecting to the REST catalog.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n\n  default-mtls-secured-catalog:\n    uri: https://rest-catalog/ws/\n    ssl:\n      client:\n        cert: /absolute/path/to/client.crt\n        key: /absolute/path/to/client.key\n      cabundle: /absolute/path/to/cabundle.pem\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLite SQLCatalog in YAML\nDESCRIPTION: This snippet demonstrates configuring a SQL catalog with a SQLite backend. It specifies the path to the SQLite database file using the `uri` property.  `init_catalog_tables` is set to false. It is recommended to use this catalog for development purposes only.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: sql\n    uri: sqlite:////tmp/pyiceberg.db\n    init_catalog_tables: false\n```\n\n----------------------------------------\n\nTITLE: Apache Arrow Table Schema and Data Example in Python\nDESCRIPTION: This snippet showcases the schema and a sample of data structure of an Apache Arrow table when using the `.to_arrow()` method.  The schema includes VendorID (int64), tpep_pickup_datetime (timestamp), and tpep_dropoff_datetime (timestamp).\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\npyarrow.Table\nVendorID: int64\ntpep_pickup_datetime: timestamp[us, tz=+00:00]\ntpep_dropoff_datetime: timestamp[us, tz=+00:00]\n----\nVendorID: [[2,1,2,1,1,...,2,2,2,2,2],[2,1,1,1,2,...,1,1,2,1,2],...,[2,2,2,2,2,...,2,6,6,2,2],[2,2,2,2,2,...,2,2,2,2,2]]\ntpep_pickup_datetime: [[2021-04-01 00:28:05.000000,...,2021-04-30 23:44:25.000000]]\ntpep_dropoff_datetime: [[2021-04-01 00:47:59.000000,...,2021-05-01 00:14:47.000000]]\n```\n\n----------------------------------------\n\nTITLE: Removing Partition Fields in Iceberg\nDESCRIPTION: Illustrates removing a partition field from a table using the `remove_field` API.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_spec() as update:\n    # Remove the partition field with the name\n    update.remove_field(\"some_partition_name\")\n```\n\n----------------------------------------\n\nTITLE: Listing Namespaces in Default Catalog\nDESCRIPTION: This command lists the namespaces in the default catalog. It assumes that you have a default catalog configured, typically through a `~/.pyiceberg.yaml` file.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/cli.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n➜  pyiceberg list\ndefault\nnyc\n```\n\n----------------------------------------\n\nTITLE: Listing Tables in a Namespace\nDESCRIPTION: This command lists the tables within a specific namespace, in this case the 'nyc' namespace.  It requires the namespace to exist in the configured catalog.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/cli.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n➜  pyiceberg list nyc\nnyc.taxis\n```\n\n----------------------------------------\n\nTITLE: Check if a View Exists in Iceberg\nDESCRIPTION: Demonstrates how to check if a view exists in an Iceberg catalog.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\ncatalog.view_exists(\"default.bar\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Parquet data\nDESCRIPTION: This command downloads a Parquet file containing taxi trip data from a publicly accessible URL and saves it to a temporary file. The data will be used to create and populate an Iceberg table.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o /tmp/yellow_tripdata_2023-01.parquet\n```\n\n----------------------------------------\n\nTITLE: SimpleLocationProvider Partitioned Table Path\nDESCRIPTION: Example of a data file path generated by the SimpleLocationProvider for a partitioned Iceberg table. The path includes Hive-style partition paths like 'category=orders'.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\ns3://bucket/ns/table/data/category=orders/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Schema and Partitioning\nDESCRIPTION: This Python snippet shows how to create a table with a defined schema, partitioning, and sorting. It defines a schema with various data types, a partition specification using a DayTransform, and a sort order based on a symbol. It uses `pyiceberg.schema`, `pyiceberg.types`, `pyiceberg.partitioning`, and `pyiceberg.table.sorting` modules.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import (\n    TimestampType,\n    FloatType,\n    DoubleType,\n    StringType,\n    NestedField,\n    StructType,\n)\n\nschema = Schema(\n    NestedField(field_id=1, name=\"datetime\", field_type=TimestampType(), required=True),\n    NestedField(field_id=2, name=\"symbol\", field_type=StringType(), required=True),\n    NestedField(field_id=3, name=\"bid\", field_type=FloatType(), required=False),\n    NestedField(field_id=4, name=\"ask\", field_type=DoubleType(), required=False),\n    NestedField(\n        field_id=5,\n        name=\"details\",\n        field_type=StructType(\n            NestedField(\n                field_id=4, name=\"created_by\", field_type=StringType(), required=False\n            ),\n        ),\n        required=False,\n    ),\n)\n\nfrom pyiceberg.partitioning import PartitionSpec, PartitionField\nfrom pyiceberg.transforms import DayTransform\n\npartition_spec = PartitionSpec(\n    PartitionField(\n        source_id=1, field_id=1000, transform=DayTransform(), name=\"datetime_day\"\n    )\n)\n\nfrom pyiceberg.table.sorting import SortOrder, SortField\nfrom pyiceberg.transforms import IdentityTransform\n\n# Sort on the symbol\nsort_order = SortOrder(SortField(source_id=2, transform=IdentityTransform()))\n\ncatalog.create_table(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n    location=\"s3://pyiceberg\",\n    partition_spec=partition_spec,\n    sort_order=sort_order,\n)\n```\n\n----------------------------------------\n\nTITLE: ObjectStoreLocationProvider Example Path\nDESCRIPTION: Example data file path using ObjectStoreLocationProvider for a partitioned table. The path includes binary directories inserted for distribution across object store prefixes.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\ns3://bucket/ns/table/data/0101/0110/1001/10110010/category=orders/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n```\n\n----------------------------------------\n\nTITLE: PyIceberg CLI Help\nDESCRIPTION: This command displays the help message for the PyIceberg CLI, listing available commands and options such as specifying a catalog with `--catalog`, setting verbosity with `--verbose`, and selecting output format with `--output`.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/cli.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n➜  pyiceberg --help\nUsage: pyiceberg [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n--catalog TEXT\n--verbose BOOLEAN\n--output [text|json]\n--ugi TEXT\n--uri TEXT\n--credential TEXT\n--help                Show this message and exit.\n\nCommands:\ndescribe    Describes a namespace xor table\ndrop        Operations to drop a namespace or table\nlist        Lists tables or namespaces\nlocation    Returns the location of the table\nproperties  Properties on tables/namespaces\nrename      Renames a table\nschema      Gets the schema of the table\nspec        Returns the partition spec of the table\nuuid        Returns the UUID of the table\n```\n\n----------------------------------------\n\nTITLE: Uploading PyIceberg release to PyPi (Bash)\nDESCRIPTION: This script checks out the released artifact from Apache SVN, navigates to the versioned directory, and uploads the wheel and tar.gz files to PyPi using `twine`. It requires `twine` and potentially a PyPi API token for authentication.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsvn checkout https://dist.apache.org/repos/dist/release/iceberg /tmp/iceberg-dist-release/\ncd /tmp/iceberg-dist-release/pyiceberg-${VERSION}\ntwine upload pyiceberg-*.whl pyiceberg-*.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Setting up a virtual environment (Bash)\nDESCRIPTION: These commands set up an up-to-date virtual environment using virtualenv. This isolates the project's dependencies, preventing conflicts with system-wide packages and ensuring consistent behavior across different environments. The virtual environment is activated to use the project's specific dependencies.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade virtualenv pip\npython -m venv ./venv\nsource ./venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Installing PyIceberg Nightly Build\nDESCRIPTION: This command installs the pre-release version of pyiceberg from the testpypi repository. It utilizes pip, the Python package installer, to fetch and install the package, specifying the index URL to testpypi and using the --pre flag to allow installation of pre-release versions.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/nightly-build.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -i https://test.pypi.org/simple/ --pre pyiceberg\n```\n\n----------------------------------------\n\nTITLE: Cloning the repository and installing with extra dependencies (Shell)\nDESCRIPTION: Clones the Apache Iceberg Python repository from GitHub for local development, then installs it with extra dependencies such as s3fs and hive. The -e flag makes it an editable install.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/apache/iceberg-python.git\ncd iceberg-python\npip3 install -e \".[s3fs,hive]\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Signed Tag - Bash\nDESCRIPTION: This script creates a signed tag for a release candidate, which is then pushed to the remote repository. It sets environment variables for the version and release candidate number and constructs the tag name.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport VERSION=0.7.0\nexport RC=1\n\nexport VERSION_WITH_RC=${VERSION}rc${RC}\nexport GIT_TAG=pyiceberg-${VERSION_WITH_RC}\n\ngit tag -s ${GIT_TAG} -m \"PyIceberg ${VERSION_WITH_RC}\"\ngit push git@github.com:apache/iceberg-python.git ${GIT_TAG}\n```\n\n----------------------------------------\n\nTITLE: Running tests (Bash)\nDESCRIPTION: This command runs the project's tests using pytest. It also enforces a code coverage of 90% or higher, ensuring that the codebase is well-tested.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Verifying Signatures of PyIceberg Release Artifacts\nDESCRIPTION: This snippet checks out the release candidate files from the Apache dist repository, navigates to the verification directory, and then iterates through the `.whl` and `.tar.gz` files, verifying their signatures against the corresponding `.asc` files. The verification is done using `gpg --verify` command.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsvn checkout https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-${PYICEBERG_VERSION}/ ${PYICEBERG_VERIFICATION_DIR}\n\ncd ${PYICEBERG_VERIFICATION_DIR}\n\nfor name in $(ls pyiceberg-*.whl pyiceberg-*.tar.gz)\ndo\n    gpg --verify ${name}.asc ${name}\ndone\n```\n\n----------------------------------------\n\nTITLE: Cleaning the build environment (Bash)\nDESCRIPTION: This command removes old cached files generated during the Cython build process. It helps prevent build failures and ensures that the build environment is clean and up-to-date.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake clean\n```\n\n----------------------------------------\n\nTITLE: Verifying Checksums of PyIceberg Release Artifacts\nDESCRIPTION: This snippet changes the directory to the verification directory and verifies the checksums of the `.whl` and `.tar.gz` files against their corresponding `.sha512` files.  `shasum -a 512 --check` is used to perform the checksum verification. This ensures that the downloaded files have not been tampered with.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd ${PYICEBERG_VERIFICATION_DIR}\nfor name in $(ls pyiceberg-*.whl.sha512 pyiceberg-*.tar.gz.sha512)\ndo\n    shasum -a 512 --check ${name}\ndone\n```\n\n----------------------------------------\n\nTITLE: Downloading Artifacts from GitHub Actions - Bash\nDESCRIPTION: This command downloads the artifacts generated by the GitHub Action. It depends on the GitHub CLI (`gh`) being installed and configured.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngh run download $RUN_ID --repo apache/iceberg-python\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry for dependency management (Bash)\nDESCRIPTION: This command installs Poetry, a tool for dependency management and packaging in Python projects. Poetry simplifies dependency resolution and project packaging, making it easier to manage project dependencies.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install poetry\n```\n\n----------------------------------------\n\nTITLE: Watching GitHub Action Progress - Bash\nDESCRIPTION: This series of commands helps to watch the progress of the GitHub Action using `gh`. It first retrieves the run ID based on workflow name, branch and event, then waits for the workflow to complete, finally it watches the workflow to provide real time updates.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nRUN_ID=$(gh run list --repo apache/iceberg-python --workflow \"Python Build Release Candidate\" --branch \"${GIT_TAG}\" --event push --json databaseId -q '.[0].databaseId')\necho \"Waiting for workflow to complete, this will take several minutes...\"\ngh run watch $RUN_ID --repo apache/iceberg-python\n```\n\n----------------------------------------\n\nTITLE: Creating a Table Transactionally\nDESCRIPTION: This Python snippet demonstrates how to create a table and perform subsequent schema and spec updates atomically within a transaction. It uses `create_table_transaction` context manager to ensure that all changes are applied together or rolled back in case of failure. It requires a catalog and necessary parameters such as schema, location, partition spec, and sort order.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith catalog.create_table_transaction(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n    location=\"s3://pyiceberg\",\n    partition_spec=partition_spec,\n    sort_order=sort_order,\n) as txn:\n    with txn.update_schema() as update_schema:\n        update_schema.add_column(path=\"new_column\", field_type=StringType())\n\n    with txn.update_spec() as update_spec:\n        update_spec.add_identity(\"symbol\")\n\n    txn.set_properties(test_a=\"test_aa\", test_b=\"test_b\", test_c=\"test_c\")\n```\n\n----------------------------------------\n\nTITLE: Installing PyIceberg with optional dependencies\nDESCRIPTION: This command installs PyIceberg with optional dependencies for S3FS and Hive support.  Other optional dependencies can be used based on the intended environment and catalog needs.  Dependencies like s3fs, adlfs, gcsfs, or pyarrow are required to fetch files from an object store.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install \"pyiceberg[s3fs,hive]\"\n```\n\n----------------------------------------\n\nTITLE: Building and Serving Documentation Locally\nDESCRIPTION: These commands install the necessary dependencies for building the documentation and then starts a local server to serve the built documentation. The `make docs-install` command installs the required Python packages and other dependencies. The `make docs-serve` command builds the documentation and starts a local web server to view it.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmake docs-install\nmake docs-serve\n```\n\n----------------------------------------\n\nTITLE: Configuring RESTCatalog Headers in YAML\nDESCRIPTION: This snippet demonstrates how to configure custom headers for a RESTCatalog in PyIceberg by including them in the catalog properties with the `header.` prefix.  All HTTP requests to the REST service will include these headers.  The example sets the `content-type` header.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n    header.content-type: application/vnd.api+json\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Release Verification\nDESCRIPTION: This snippet sets environment variables for the PyIceberg version being verified and the directory to store the verification files.  It's crucial for managing the verification process and ensuring that paths are correct.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport PYICEBERG_VERSION=<version> # e.g. 0.6.1rc3\nexport PYICEBERG_VERIFICATION_DIR=/tmp/pyiceberg/${PYICEBERG_VERSION}\n```\n\n----------------------------------------\n\nTITLE: Running linting (Bash)\nDESCRIPTION: This command runs `pre-commit` to autoformat and lint the codebase. It automatically fixes violations related to import orders, formatting, etc., according to the project's linting rules. Pylint errors must be fixed manually.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Running ADLS tests (Bash)\nDESCRIPTION: This command runs the ADLS (Azure Data Lake Storage)-specific tests, which are typically ignored by default because they require Azurite to be running. These tests verify the functionality of the project when interacting with ADLS storage.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake test-adls\n```\n\n----------------------------------------\n\nTITLE: Renaming Partition Fields in Iceberg\nDESCRIPTION: Shows how to rename a partition field using the `rename_field` API.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_spec() as update:\n    # Rename the partition field with the name bucketed_id to sharded_id\n    update.rename_field(\"bucketed_id\", \"sharded_id\")\n```\n\n----------------------------------------\n\nTITLE: Running Test Coverage for PyIceberg\nDESCRIPTION: This snippet executes `make test-coverage` to run the full test suite, including both unit and integration tests.  This validates the functionality of the release candidate, including spinning up Docker containers to facilitate the tests.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nmake test-coverage\n```\n\n----------------------------------------\n\nTITLE: Running integration tests (Shell)\nDESCRIPTION: Runs the integration tests with Apache Spark. Spark will create a new database and provision some tables that PyIceberg can query against.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nmake test-integration\n```\n\n----------------------------------------\n\nTITLE: Ray Dataset Output Example in Python\nDESCRIPTION: This snippet presents an example of the Ray dataset schema and data information. It indicates the number of blocks and rows, along with the schema of the dataset, including column names and data types.  It shows a snippet of the schema obtained using the `.to_ray()` method.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nDataset(\n    num_blocks=1,\n    num_rows=1168798,\n    schema={\n        VendorID: int64,\n        tpep_pickup_datetime: timestamp[us, tz=UTC],\n        tpep_dropoff_datetime: timestamp[us, tz=UTC]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Loading a Catalog in Python\nDESCRIPTION: This Python snippet demonstrates how to load a catalog using the `load_catalog` function from the `pyiceberg.catalog` module. It initializes a catalog named 'docs' with connection parameters like URI, S3 endpoint, access key, and secret key. It requires the `pyiceberg` library and proper configuration of the catalog connection parameters.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\n    \"docs\",\n    **{\n        \"uri\": \"http://127.0.0.1:8181\",\n        \"s3.endpoint\": \"http://127.0.0.1:9000\",\n        \"py-io-impl\": \"pyiceberg.io.pyarrow.PyArrowFileIO\",\n        \"s3.access-key-id\": \"admin\",\n        \"s3.secret-access-key\": \"password\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Running License Checks with RAT\nDESCRIPTION: This snippet executes the `dev/check-license` script to validate the license headers in the source code. This ensures that all files have the correct Apache license headers.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./dev/check-license\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Catalogs\nDESCRIPTION: This YAML snippet shows how to define multiple catalogs (hive and rest) within the same `.pyiceberg.yaml` file, each with its own specific connection details such as URI, S3 endpoint, and credentials. These can then be loaded individually by calling `load_catalog(name=\"hive\")` and `load_catalog(name=\"rest\")`.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  hive:\n    uri: thrift://127.0.0.1:9083\n    s3.endpoint: http://127.0.0.1:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n  rest:\n    uri: https://rest-server:8181/\n    warehouse: my-warehouse\n```\n\n----------------------------------------\n\nTITLE: Configuring DynamoDB Catalog with Explicit Credentials\nDESCRIPTION: This YAML configuration shows how to configure PyIceberg to use AWS DynamoDB as the catalog, explicitly providing the AWS access key ID, secret access key, session token, and region. This avoids relying on environment variables or shared credentials files and includes S3 endpoint configuration.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: dynamodb\n    table-name: iceberg\n    dynamodb.access-key-id: <ACCESS_KEY_ID>\n    dynamodb.secret-access-key: <SECRET_ACCESS_KEY>\n    dynamodb.session-token: <SESSION_TOKEN>\n    dynamodb.region: <REGION_NAME>\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n```\n\n----------------------------------------\n\nTITLE: Defining Schema with Identifier Field for Upsert\nDESCRIPTION: This snippet defines a schema with an identifier field for an Iceberg table, which is essential for upsert operations. The identifier field is used to determine which rows should be updated or inserted during the upsert process. The example sets the 'city' field as the identifier field.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import IntegerType, NestedField, StringType\n\nimport pyarrow as pa\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=True),\n    NestedField(2, \"inhabitants\", IntegerType(), required=True),\n    # Mark City as the identifier field, also known as the primary-key\n    identifier_field_ids=[1]\n)\n\ntbl = catalog.create_table(\"default.cities\", schema=schema)\n\narrow_schema = pa.schema(\n    [\n        pa.field(\"city\", pa.string(), nullable=False),\n        pa.field(\"inhabitants\", pa.int32(), nullable=False),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Vendoring FB303 Thrift Client (Bash)\nDESCRIPTION: This script downloads the fb303.thrift file from the Apache Thrift repository, generates the corresponding Python code using the Thrift compiler, and moves the generated fb303 directory to the project's vendor directory. Dependencies include curl and the thrift compiler.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/vendor/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nrm -f /tmp/fb303.thrift\nrm -rf fb303\ncurl -s https://raw.githubusercontent.com/apache/thrift/master/contrib/fb303/if/fb303.thrift > /tmp/fb303.thrift\nrm -rf /tmp/gen-py/\nthrift -gen py -o /tmp/ /tmp/fb303.thrift\nmv /tmp/gen-py/fb303 fb303\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg Table for Write Support\nDESCRIPTION: This Python code snippet demonstrates how to create an Iceberg table based on a defined schema for writing data.  It loads a catalog, defines a schema using `NestedField`, `StringType`, and `DoubleType`, and creates a table using `catalog.create_table`.  It requires `pyiceberg` and a catalog configured.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\n\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, DoubleType\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntbl = catalog.create_table(\"default.cities\", schema=schema)\n```\n\n----------------------------------------\n\nTITLE: Upserting Data into Iceberg Table\nDESCRIPTION: Demonstrates how to perform an upsert operation on an Iceberg table. It merges an Arrow table into the Iceberg table, updating existing rows based on the identifier field and inserting new rows. The example verifies the number of updated and inserted rows.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf = pa.Table.from_pylist(\n    [\n        # Will be updated, the inhabitants has been updated\n        {\"city\": \"Drachten\", \"inhabitants\": 45505},\n\n        # New row, will be inserted\n        {\"city\": \"Berlin\", \"inhabitants\": 3432000},\n\n        # Ignored, already exists in the table\n        {\"city\": \"Paris\", \"inhabitants\": 2103000},\n    ],\n    schema=arrow_schema\n)\nupd = tbl.upsert(df)\n\nassert upd.rows_updated == 1\nassert upd.rows_inserted == 1\n```\n\n----------------------------------------\n\nTITLE: Generating Signature and Checksum Files - Bash\nDESCRIPTION: This script generates GPG signatures and SHA-512 checksums for the release artifacts. It navigates to the directory containing the artifacts, then creates `.asc` and `.sha512` files for each `.whl` and `.tar.gz` file.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n(\n    cd svn-release-candidate-${VERSION}rc${RC}\n\n    for name in $(ls pyiceberg-*.whl pyiceberg-*.tar.gz)\n    do\n        gpg --yes --armor --output \"${name}.asc\" --detach-sig \"${name}\"\n        shasum -a 512 \"${name}\" > \"${name}.sha512\"\n    done\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Configuration via Environment Variables (Bash)\nDESCRIPTION: This snippet demonstrates how to set PyIceberg configuration values using environment variables.  It sets the catalog URI, S3 access key ID, and secret access key for the 'default' catalog. The double underscore `__` represents a nested field, and the underscore `_` is converted into a dash `-`.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport PYICEBERG_CATALOG__DEFAULT__URI=thrift://localhost:9083\nexport PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID=username\nexport PYICEBERG_CATALOG__DEFAULT__S3__SECRET_ACCESS_KEY=password\n```\n\n----------------------------------------\n\nTITLE: Interact with Ray Dataset and Retrieve Data in Python\nDESCRIPTION: This snippet demonstrates how to interact with a Ray dataset and retrieve the first two rows of data. It requires a Ray dataset object, which is created with the `.to_ray()` method from the pyiceberg scan API.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nprint(ray_dataset.take(2))\n[\n    {\n        \"VendorID\": 2,\n        \"tpep_pickup_datetime\": datetime.datetime(2008, 12, 31, 23, 23, 50),\n        \"tpep_dropoff_datetime\": datetime.datetime(2009, 1, 1, 0, 34, 31),\n    },\n    {\n        \"VendorID\": 2,\n        \"tpep_pickup_datetime\": datetime.datetime(2008, 12, 31, 23, 5, 3),\n        \"tpep_dropoff_datetime\": datetime.datetime(2009, 1, 1, 16, 10, 18),\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Glue Catalog with Static Credentials in YAML\nDESCRIPTION: This snippet illustrates how to configure a Glue catalog by specifying AWS credentials (access key ID, secret access key, and session token) directly within the catalog properties. It also includes configurations for S3 access. The region for the Glue service is also provided.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: glue\n    glue.access-key-id: <ACCESS_KEY_ID>\n    glue.secret-access-key: <SECRET_ACCESS_KEY>\n    glue.session-token: <SESSION_TOKEN>\n    glue.region: <REGION_NAME>\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n```\n\n----------------------------------------\n\nTITLE: Vendoring Hive Metastore Thrift Definition (Bash)\nDESCRIPTION: This script downloads the hive_metastore.thrift definition along with its fb303 dependency, generates Python code for the Hive Metastore using the Thrift compiler, and moves the resulting hive_metastore directory. It requires curl and the thrift compiler.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/vendor/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf /tmp/hive\nmkdir -p /tmp/hive/share/fb303/if/\ncurl -s https://raw.githubusercontent.com/apache/thrift/master/contrib/fb303/if/fb303.thrift > /tmp/hive/share/fb303/if/fb303.thrift\ncurl -s https://raw.githubusercontent.com/apache/hive/master/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift > /tmp/hive/hive_metastore.thrift\nthrift -gen py -o /tmp/hive /tmp/hive/hive_metastore.thrift\nmv /tmp/hive/gen-py/hive_metastore hive_metastore\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL SQLCatalog in YAML\nDESCRIPTION: This snippet illustrates how to configure a SQL catalog with a PostgreSQL backend using psycopg2. The connection details (username, password, host, and database name) are specified in the `uri` property. The `init_catalog_tables` property is set to `false` to prevent the creation of catalog tables during initialization.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: sql\n    uri: postgresql+psycopg2://username:password@localhost/mydatabase\n    init_catalog_tables: false\n```\n\n----------------------------------------\n\nTITLE: Uploading Artifacts to Apache Dev SVN - Bash\nDESCRIPTION: This script uploads the release artifacts to the Apache development SVN repository. It checks out the SVN repository, creates a versioned directory, copies the artifacts, and commits the changes.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport SVN_TMP_DIR=/tmp/iceberg-${VERSION}/\nsvn checkout https://dist.apache.org/repos/dist/dev/iceberg $SVN_TMP_DIR\n\nexport SVN_TMP_DIR_VERSIONED=${SVN_TMP_DIR}pyiceberg-$VERSION_WITH_RC/\nmkdir -p $SVN_TMP_DIR_VERSIONED\ncp svn-release-candidate-${VERSION}rc${RC}/* $SVN_TMP_DIR_VERSIONED\nsvn add $SVN_TMP_DIR_VERSIONED\nsvn ci -m \"PyIceberg ${VERSION_WITH_RC}\" ${SVN_TMP_DIR_VERSIONED}\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with PyArrow Schema\nDESCRIPTION: This Python snippet demonstrates how to create an Iceberg table using a pyarrow schema. It initializes a pyarrow schema with string, integer, and boolean fields, and then creates a table in the catalog using that schema. It depends on `pyarrow` and `pyiceberg` libraries being installed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"foo\", pa.string(), nullable=True),\n        pa.field(\"bar\", pa.int32(), nullable=False),\n        pa.field(\"baz\", pa.bool_(), nullable=True),\n    ]\n)\n\ncatalog.create_table(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Vote Email - Bash\nDESCRIPTION: This script generates the text for a vote email announcement. It retrieves the Git tag hash and last commit ID, then constructs the email body with relevant information for voting on the release candidate.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport GIT_TAG_REF=$(git show-ref ${GIT_TAG})\nexport GIT_TAG_HASH=${GIT_TAG_REF:0:40}\nexport LAST_COMMIT_ID=$(git rev-list ${GIT_TAG} 2> /dev/null | head -n 1)\n\ncat << EOF > release-announcement-email.txt\nTo: dev@iceberg.apache.org\nSubject: [VOTE] Release Apache PyIceberg $VERSION_WITH_RC\nHi Everyone,\n\nI propose that we release the following RC as the official PyIceberg $VERSION release.\n\nA summary of the high level features:\n\n* <Add summary by hand>\n\nThe commit ID is $LAST_COMMIT_ID\n\n* This corresponds to the tag: $GIT_TAG ($GIT_TAG_HASH)\n* https://github.com/apache/iceberg-python/releases/tag/$GIT_TAG\n* https://github.com/apache/iceberg-python/tree/$LAST_COMMIT_ID\n\nThe release tarball, signature, and checksums are here:\n\n* https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-$VERSION_WITH_RC/\n\nYou can find the KEYS file here:\n\n* https://downloads.apache.org/iceberg/KEYS\n\nConvenience binary artifacts are staged on pypi:\n\nhttps://pypi.org/project/pyiceberg/$VERSION_WITH_RC/\n\nAnd can be installed using: pip3 install pyiceberg==$VERSION_WITH_RC\n\nInstructions for verifying a release can be found here:\n\n* https://py.iceberg.apache.org/verify-release/\n\nPlease download, verify, and test.\n\nPlease vote in the next 72 hours.\n[ ] +1 Release this as PyIceberg $VERSION\n[ ] +0\n[ ] -1 Do not release this because...\nEOF\n```\n\n----------------------------------------\n\nTITLE: Release Announcement Email Template (Text)\nDESCRIPTION: This is a template for the release announcement email to be sent to the dev mailing list. It includes information about Apache Iceberg and the Python release, with a link to the PyPi project page.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nTo: dev@iceberg.apache.org\nSubject: [ANNOUNCE] Apache PyIceberg release <VERSION>\n\nI'm pleased to announce the release of Apache PyIceberg <VERSION>!\n\nApache Iceberg is an open table format for huge analytic datasets. Iceberg\ndelivers high query performance for tables with tens of petabytes of data,\nalong with atomic commits, concurrent writes, and SQL-compatible table\nevolution.\n\nThis Python release can be downloaded from: https://pypi.org/project/pyiceberg/<VERSION>/\n\nThanks to everyone for contributing!\n```\n\n----------------------------------------\n\nTITLE: Creating a Patch Branch from the Latest Release Tag - Bash\nDESCRIPTION: This series of commands demonstrates how to create a patch branch from the latest release tag.  First, it fetches all tags, then it creates a new branch from the specified tag and finally it cherry-picks commits for the upcoming patch release.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Fetch all tags\ngit fetch --tags\n\n# Assuming 0.8.0 is the latest release tag\ngit checkout -b pyiceberg-0.8.x pyiceberg-0.8.0\n\n# Cherry-pick commits for the upcoming patch release\ngit cherry-pick <commit>\n```\n\n----------------------------------------\n\nTITLE: Configuring a Catalog\nDESCRIPTION: This YAML snippet demonstrates how to define a catalog called `prod` in the `.pyiceberg.yaml` configuration file, including its URI and credentials for accessing an Iceberg catalog. The file must be located in a supported directory such as `$HOME`, `%USERPROFILE%`, the current working directory or `$PYICEBERG_HOME`.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  prod:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n```\n\n----------------------------------------\n\nTITLE: Loading a Static Table\nDESCRIPTION: This Python snippet demonstrates how to load an Iceberg table directly from a metadata file without using a catalog. It uses `StaticTable.from_metadata` and requires the full path to the table's metadata file.  It allows for loading a read-only view of the table.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.table import StaticTable\n\nstatic_table = StaticTable.from_metadata(\n    \"s3://warehouse/wh/nyc.db/taxis/metadata/00002-6ea51ce3-62aa-4197-9cf8-43d07c3440ca.metadata.json\"\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.table import StaticTable\n\nstatic_table = StaticTable.from_metadata(\n    \"s3://warehouse/wh/nyc.db/taxis\"\n)\n```\n\n----------------------------------------\n\nTITLE: Removing Deprecated API Marked with @deprecated - Python\nDESCRIPTION: This code snippet demonstrates how to remove an API that has been marked as deprecated using the `@deprecated` decorator. The snippet shows an example where an API deprecated in version 0.1.0 and scheduled for removal in 0.2.0 should be removed when preparing for the 0.2.0 release. It also mentions the usage of the `deprecation_message` function and how to update it.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@deprecated(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"Please use load_something_else() instead\",\n)\n```\n\n----------------------------------------\n\nTITLE: Appending Data to an Iceberg Table\nDESCRIPTION: This Python snippet showcases how to append data (in the form of a PyArrow Table) to an existing Iceberg table using the `append` method. It depends on the `tbl` variable representing an Iceberg table created with the appropriate schema, and a PyArrow table `df` containing the data to be written. This operation uses fast append.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntbl.append(df)\n```\n\n----------------------------------------\n\nTITLE: Adding GPG key to Apache Iceberg KEYS file (Bash)\nDESCRIPTION: This script adds a new GPG key to the Apache Iceberg KEYS file. It first checks out the KEYS file from SVN, appends the new key information, and then commits the changes. Requires having a GPG key set up and the `svn` command line tool.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nsvn co https://dist.apache.org/repos/dist/release/iceberg icebergsvn\ncd icebergsvn\necho \"\" >> KEYS # append a newline\ngpg --list-sigs <YOUR KEY ID HERE> >> KEYS # append signatures\ngpg --armor --export <YOUR KEY ID HERE> >> KEYS # append public key block\nsvn commit -m \"add key for <YOUR NAME HERE>\"\n```\n\n----------------------------------------\n\nTITLE: Preparing for License Documentation Verification\nDESCRIPTION: This snippet sets an environment variable removing the 'rc' qualifier from the version, then extracts the source tarball and changes the directory to the extracted source directory. This prepares for running license checks on the source code.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport PYICEBERG_RELEASE_VERSION=${PYICEBERG_VERSION/rc?/}\n\ntar xzf pyiceberg-${PYICEBERG_RELEASE_VERSION}.tar.gz\ncd pyiceberg-${PYICEBERG_RELEASE_VERSION}\n```\n\n----------------------------------------\n\nTITLE: Importing Apache Iceberg Keys using GPG\nDESCRIPTION: This snippet downloads the Apache Iceberg KEYS file and imports it into GPG for signature verification. This allows the user to trust the signatures of the release artifacts.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://downloads.apache.org/iceberg/KEYS -o KEYS\ngpg --import KEYS\n```\n\n----------------------------------------\n\nTITLE: Adding a deprecation notice to a function (Python)\nDESCRIPTION: This code snippet demonstrates how to add a deprecation notice to a function using the `@deprecated` decorator. This informs users that the function is deprecated and will be removed in a future version, suggesting an alternative method to use.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.utils.deprecated import deprecated\n\n\n@deprecated(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"Please use load_something_else() instead\",\n)\ndef load_something():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Inspecting File Manifests with Iceberg Python\nDESCRIPTION: This code snippet shows how to retrieve a table's current file manifests using `table.inspect.manifests()`. The returned pyarrow table contains details about each manifest, including its path, length, partition spec ID, snapshot ID, and file counts.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.manifests()\n```\n\n----------------------------------------\n\nTITLE: Configuring Glue Catalog with Profile Name in YAML\nDESCRIPTION: This snippet shows how to configure a Glue catalog using a named AWS profile. The `glue.profile-name` property specifies the name of the profile configured in the AWS CLI. The snippet also shows configurations for region and S3 access.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: glue\n    glue.profile-name: <PROFILE_NAME>\n    glue.region: <REGION_NAME>\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Catalog in YAML\nDESCRIPTION: This snippet showcases how to configure a Hive catalog in PyIceberg.  It sets the `uri` to the Thrift endpoint of the Hive metastore and includes example configurations for S3 access, including endpoint, access key ID, and secret access key.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    uri: thrift://localhost:9083\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n```\n\n----------------------------------------\n\nTITLE: Installing from GitHub directly (Shell)\nDESCRIPTION: This command installs the PyIceberg package directly from the GitHub repository. This method is less recommended but can be useful for quick testing. The `#egg=pyiceberg[pyarrow]` specifies the package name and an optional dependency (pyarrow).\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install \"git+https://github.com/apache/iceberg-python.git#egg=pyiceberg[pyarrow]\"\n```\n\n----------------------------------------\n\nTITLE: Custom LocationProvider Implementation in Python\nDESCRIPTION: Example implementation of a custom LocationProvider in Python that uses UUIDs to generate data file paths. The class inherits from LocationProvider and overrides the new_data_location method to generate custom paths.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n\nclass UUIDLocationProvider(LocationProvider):\n    def __init__(self, table_location: str, table_properties: Properties):\n        super().__init__(table_location, table_properties)\n\n    def new_data_location(self, data_file_name: str, partition_key: Optional[PartitionKey] = None) -> str:\n        # Can use any custom method to generate a file path given the partitioning information and file name\n        prefix = f\"{self.table_location}/{uuid.uuid4()}\"\n        return f\"{prefix}/{partition_key.to_path()}/{data_file_name}\" if partition_key else f\"{prefix}/{data_file_name}\"\n```\n\n----------------------------------------\n\nTITLE: Inspecting Snapshot References with Iceberg Python\nDESCRIPTION: This code snippet demonstrates how to retrieve a table's known snapshot references using `table.inspect.refs()`. The method returns a pyarrow table containing information about each reference, such as its name, type, snapshot ID, and retention policies.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.refs()\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from Iceberg Table with Filter\nDESCRIPTION: This snippet demonstrates how to delete data from an Iceberg table based on a filter condition. The `tbl.delete()` operation removes records that match the provided filter. In this case, it deletes any records where the 'city' field equals 'Paris'.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntbl.delete(delete_filter=\"city == 'Paris'\")\n```\n\n----------------------------------------\n\nTITLE: Moving PyIceberg artifacts to Apache Release SVN (Bash)\nDESCRIPTION: This script moves the approved release candidate from the dev directory to the release directory in the Apache SVN repository. It requires setting environment variables for the versioned dev and release directory URLs. Only a PMC member is authorized to upload artifacts to SVN.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport SVN_DEV_DIR_VERSIONED=\"https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-${VERSION_WITH_RC}\"\nexport SVN_RELEASE_DIR_VERSIONED=\"https://dist.apache.org/repos/dist/release/iceberg/pyiceberg-${VERSION}\"\n\nsvn mv ${SVN_DEV_DIR_VERSIONED} ${SVN_RELEASE_DIR_VERSIONED} -m \"PyIceberg: Add release ${VERSION}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring DynamoDB Catalog with Table Name\nDESCRIPTION: This YAML snippet configures PyIceberg to use AWS DynamoDB as the catalog. It specifies the catalog type as 'dynamodb' and the name of the table to use as 'iceberg'. It assumes that AWS credentials are set up through environment variables or a shared credentials file.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n    type: dynamodb\n    table-name: iceberg\n```\n\n----------------------------------------\n\nTITLE: Adding a Column to an Iceberg Table\nDESCRIPTION: Demonstrates adding a new column to an Iceberg table's schema using the `update_schema()` method. This example shows how to add a simple field with a data type and documentation string, as well as how to add nested fields within a struct.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_schema() as update:\n    update.add_column(\"some_field\", IntegerType(), \"doc\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting Table Partitions with Iceberg Python\nDESCRIPTION: This code snippet demonstrates how to inspect the partitions of an Iceberg table using the `table.inspect.partitions()` method. It returns a pyarrow table with information about each partition including the partition values, spec ID, record count, file count, and size.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.partitions()\n```\n\n----------------------------------------\n\nTITLE: Listing Namespaces\nDESCRIPTION: This Python snippet demonstrates how to list all namespaces within a catalog using the `list_namespaces` method. It retrieves a list of namespaces and asserts that 'docs_example' is present.  It depends on a catalog instance having been loaded and a namespace potentially having been created.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nns = catalog.list_namespaces()\n\nassert ns == [(\"docs_example\",)]\n```\n\n----------------------------------------\n\nTITLE: Removing Old Artifacts From Apache Dev SVN - Bash\nDESCRIPTION: This command removes old release candidate artifacts from the Apache development SVN repository.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsvn delete https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-<OLD_RC_VERSION> -m \"Remove old RC artifacts\"\n```\n\n----------------------------------------\n\nTITLE: Running pytest with pdb enabled (Shell)\nDESCRIPTION: This command runs pytest with the Python debugger (pdb) enabled. When a test fails or an exception occurs, the debugger will be launched, allowing you to inspect the code and variables at the point of failure.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nmake test PYTEST_ARGS=\"--pdb\"\n```\n\n----------------------------------------\n\nTITLE: Overwriting with Filter in PyIceberg\nDESCRIPTION: Demonstrates a partial overwrite in PyIceberg using an overwrite_filter. It overwrites only the rows that match the provided filter. Here, it replaces the entry for 'Paris' with 'New York'.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.expressions import EqualTo\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"New York\", \"lat\": 40.7128, \"long\": 74.0060},\n    ]\n)\ntbl.overwrite(df, overwrite_filter=EqualTo('city', \"Paris\"))\n```\n\n----------------------------------------\n\nTITLE: Checking if a Table Exists\nDESCRIPTION: This Python snippet demonstrates how to check if a table exists in a catalog using the `table_exists` method. It checks for the existence of the 'bids' table in the 'docs_example' namespace and returns a boolean value. This depends on a catalog instance being loaded.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncatalog.table_exists(\"docs_example.bids\")\n```\n\n----------------------------------------\n\nTITLE: Deleting old PyIceberg artifacts from Apache Release SVN (Bash)\nDESCRIPTION: This script deletes old release artifacts from the Apache SVN repository to keep only the latest release. The script requires knowing the URL of the old release artifact.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsvn delete https://dist.apache.org/repos/dist/release/iceberg/pyiceberg-<OLD_RELEASE_VERSION> -m \"Remove old release artifacts\"\n```\n\n----------------------------------------\n\nTITLE: Creating Iceberg namespace and table\nDESCRIPTION: This Python code creates a namespace and then an Iceberg table using the loaded PyArrow dataframe's schema. The `create_namespace` and `create_table` methods of the `catalog` object are used. The table is named `default.taxi_dataset`.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncatalog.create_namespace(\"default\")\n\ntable = catalog.create_table(\n    \"default.taxi_dataset\",\n    schema=df.schema,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing the library locally (Bash)\nDESCRIPTION: This command installs the library in editable mode, allowing changes in the source code to be immediately reflected without reinstalling the package. This is useful for local development and testing. `pip3 install -e .` installs the package.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -e .\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table in Iceberg\nDESCRIPTION: Demonstrates how to create an iceberg table with a partition specified on the 'city' field.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import DoubleType, NestedField, StringType\nfrom pyiceberg.transforms import IdentityTransform\nfrom pyiceberg.partitioning import PartitionSpec, PartitionField\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntbl = catalog.create_table(\n    \"default.cities\",\n    schema=schema,\n    partition_spec=PartitionSpec(PartitionField(source_id=1, field_id=1001, transform=IdentityTransform(), name=\"city_identity\"))\n)\n```\n\n----------------------------------------\n\nTITLE: Overwriting Iceberg table with new data\nDESCRIPTION: This Python code overwrites the existing data in the Iceberg table with the updated PyArrow dataframe, which now includes the new `tip_per_mile` column.  The `overwrite` method is used. After overwriting the table the schema is printed.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntable.overwrite(df)\nprint(table.scan().to_arrow())\n```\n\n----------------------------------------\n\nTITLE: Running S3 tests (Bash)\nDESCRIPTION: This command runs the S3-specific tests, which are typically ignored by default because they require MinIO to be running. These tests verify the functionality of the project when interacting with S3 storage.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmake test-s3\n```\n\n----------------------------------------\n\nTITLE: Upgrading pip in shell\nDESCRIPTION: This command upgrades the pip package manager to the latest version. This is a recommended first step before installing PyIceberg to ensure you have the most recent version of pip.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Creating warehouse directory\nDESCRIPTION: This command creates a temporary directory to be used as the warehouse for the Iceberg catalog.  This warehouse is used for storing the data files and metadata for the Iceberg tables. This is only suitable for local testing and should not be used in production.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmkdir /tmp/warehouse\n```\n\n----------------------------------------\n\nTITLE: Inspecting Iceberg Table History (Python)\nDESCRIPTION: This code snippet demonstrates how to retrieve and display the history of an Iceberg table using the `history()` method. The output `pyarrow.Table` shows made_current_at timestamp, snapshot ID, parent ID and ancestor information.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ntable.inspect.history()\n```\n\n----------------------------------------\n\nTITLE: ObjectStoreLocationProvider Path with Partition Exclusion\nDESCRIPTION: Example data file path using ObjectStoreLocationProvider with partition exclusion enabled. The path omits the 'category=orders' partition key and value to reduce key size.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_4\n\nLANGUAGE: txt\nCODE:\n```\ns3://bucket/ns/table/data/1101/0100/1011/00111010-00000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n```\n\n----------------------------------------\n\nTITLE: Updating Deprecation Message - Python\nDESCRIPTION: This code snippet demonstrates how to update the deprecation message when removing an API. It changes the behavior according to what is noted in the message of that deprecation.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndeprecation_message(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"The old_property is deprecated. Please use the something_else property instead.\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Namespace\nDESCRIPTION: This Python snippet shows how to create a namespace within a catalog using the `create_namespace` method. It creates a namespace named 'docs_example' in the previously loaded catalog. It depends on a catalog instance having been loaded.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncatalog.create_namespace(\"docs_example\")\n```\n\n----------------------------------------\n\nTITLE: Daft DataFrame Schema Output Example in Python\nDESCRIPTION: This code snippet demonstrates the schema output of a Daft DataFrame, including the column names and their corresponding data types. The key is that the Dataframe is not materialized yet.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n╭──────────┬───────────────────────────────┬───────────────────────────────╮\n│ VendorID ┆ tpep_pickup_datetime          ┆ tpep_dropoff_datetime         │\n│ ---      ┆ ---                           ┆ ---                           │\n│ Int64    ┆ Timestamp(Microseconds, None) ┆ Timestamp(Microseconds, None) │\n╰──────────┴───────────────────────────────┴───────────────────────────────╯\n\n(No data to display: Dataframe not materialized)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Compatibility Mode in YAML\nDESCRIPTION: This snippet demonstrates setting the `hive.hive2-compatible` property to `true` for Hive 2.x compatibility. This ensures the catalog interacts correctly with older Hive metastore versions.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ncatalog:\n  default:\n...\n    hive.hive2-compatible: true\n```\n\n----------------------------------------\n\nTITLE: Running make install (Bash)\nDESCRIPTION: The command `make install` will install Poetry and all the dependencies of the Iceberg library, including development dependencies. It helps get the project ready by installing all needed dependencies.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Managing Table Statistics in Iceberg\nDESCRIPTION: Demonstrates setting and removing table statistics using the `update_statistics` API, with and without a context manager.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n# To run a specific operation\ntable.update_statistics().set_statistics(statistics_file=statistics_file).commit()\n# To run multiple operations\ntable.update_statistics()\n  .set_statistics(statistics_file1)\n  .remove_statistics(snapshot_id2)\n  .commit()\n# Operations are applied on commit.\n```\n\nLANGUAGE: python\nCODE:\n```\nwith table.update_statistics() as update:\n    update.set_statistics(statistics_file)\n    update.remove_statistics(snapshot_id2)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Overwrite in PyIceberg\nDESCRIPTION: Illustrates dynamic partition overwrite in PyIceberg where partitions from arrow table replace partitions in the iceberg table. The partitions to be replaced are automatically derived from dataframe.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf_corrected = pa.Table.from_pylist([\n    {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014}\n])\ntbl.dynamic_partition_overwrite(df_corrected)\n```\n\n----------------------------------------\n\nTITLE: Installing PyIceberg for Testing\nDESCRIPTION: This snippet uses `make install` to install the PyIceberg package from the source distribution. This allows the user to run the tests against the installed package.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/verify-release.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Uploading Artifacts to PyPi - Bash\nDESCRIPTION: This command uploads the release artifacts to PyPi using `twine`. It assumes that the artifacts have been downloaded and are located in the specified directory.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/how-to-release.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntwine upload pypi-release-candidate-${VERSION}rc${RC}/*\n```\n\n----------------------------------------\n\nTITLE: Rebuilding integration test containers (Shell)\nDESCRIPTION: Rebuilds the containers from scratch before running the integration tests. It's useful when there are changes in the Dockerfile or the provision script.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nmake test-integration-rebuild\n```\n\n----------------------------------------\n\nTITLE: SimpleLocationProvider Example Path\nDESCRIPTION: Example of a data file path generated by the SimpleLocationProvider for a non-partitioned Iceberg table. The path is prefixed with the table location followed by '/data/'.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/configuration.md#_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\ns3://bucket/ns/table/data/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n```\n\n----------------------------------------\n\nTITLE: Running pytest in verbose mode (Shell)\nDESCRIPTION: This command runs pytest in verbose mode, providing more detailed output during the test execution.  It is useful for debugging.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/contributing.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nmake test PYTEST_ARGS=\"-v\"\n```\n\n----------------------------------------\n\nTITLE: Setting Snapshot Properties in Iceberg\nDESCRIPTION: Illustrates how to set snapshot properties while writing to a table using the `append` or `overwrite` API.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/api.md#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ntbl.append(df, snapshot_properties={\"abc\": \"def\"})\n\n# or\n\ntbl.overwrite(df, snapshot_properties={\"abc\": \"def\"})\n\nassert tbl.metadata.snapshots[-1].summary[\"abc\"] == \"def\"\n```\n\n----------------------------------------\n\nTITLE: Finding data files and metadata in warehouse\nDESCRIPTION: This shell command finds all files and directories within the `/tmp/warehouse` directory. This demonstrates how Iceberg saves data and metadata files in the local filesystem.\nSOURCE: https://github.com/apache/iceberg-python/blob/main/mkdocs/docs/index.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nfind /tmp/warehouse/\n```"
  }
]
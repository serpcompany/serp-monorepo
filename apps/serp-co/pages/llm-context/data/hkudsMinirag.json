[
  {
    "owner": "hkuds",
    "repo": "minirag",
    "content": "TITLE: MiniRAG Environment Variables Configuration Sample\nDESCRIPTION: Sample .env file with all available configuration options for MiniRAG. This includes server settings, directory configurations, RAG parameters, LLM and embedding settings, security options, logging, and SSL configuration.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_3\n\nLANGUAGE: env\nCODE:\n```\n# Server Configuration\nHOST=0.0.0.0\nPORT=9721\n\n# Directory Configuration\nWORKING_DIR=/app/data/rag_storage\nINPUT_DIR=/app/data/inputs\n\n# RAG Configuration\nMAX_ASYNC=4\nMAX_TOKENS=32768\nEMBEDDING_DIM=1024\nMAX_EMBED_TOKENS=8192\n#HISTORY_TURNS=3\n#CHUNK_SIZE=1200\n#CHUNK_OVERLAP_SIZE=100\n#COSINE_THRESHOLD=0.4\n#TOP_K=50\n\n# LLM Configuration\nLLM_BINDING=ollama\nLLM_BINDING_HOST=http://localhost:11434\nLLM_MODEL=mistral-nemo:latest\n\n# must be set if using OpenAI LLM (LLM_MODEL must be set or set by command line parms)\nOPENAI_API_KEY=you_api_key\n\n# Embedding Configuration\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\nEMBEDDING_MODEL=bge-m3:latest\n\n# Security\n#MINIRAG_API_KEY=you-api-key-for-accessing-MiniRAG\n\n# Logging\nLOG_LEVEL=INFO\n\n# Optional SSL Configuration\n#SSL=true\n#SSL_CERTFILE=/path/to/cert.pem\n#SSL_KEYFILE=/path/to/key.pem\n\n# Optional Timeout\n#TIMEOUT=30\n```\n\n----------------------------------------\n\nTITLE: Installing MiniRAG from Source for Development\nDESCRIPTION: Series of commands to clone the MiniRAG repository and install it in development mode with API support. This allows for local development and modification of the codebase.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/HKUDS/minirag.git\n\n# Change to the repository directory\ncd minirag\n\n# create a Python virtual enviroment if neccesary\n# Install in editable mode with API support\npip install -e \".[api]\"\n```\n\n----------------------------------------\n\nTITLE: Running MiniRAG Indexing and QA Pipeline\nDESCRIPTION: Commands to execute the MiniRAG pipeline, starting with indexing the dataset and then running the question answering process.\nSOURCE: https://github.com/hkuds/minirag/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ./reproduce/Step_0_index.py\npython ./reproduce/Step_1_QA.py\n```\n\n----------------------------------------\n\nTITLE: Querying MiniRAG API\nDESCRIPTION: cURL command to send a POST request to the MiniRAG query endpoint, demonstrating how to query the RAG system with options for different search modes.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9721/query\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"query\": \"Your question here\", \"mode\": \"hybrid\", \"\"}'\n```\n\n----------------------------------------\n\nTITLE: Running MiniRAG Server with LoLLMs Backend\nDESCRIPTION: Example commands for running MiniRAG server with LoLLMs as the backend for both LLM and embedding. Shows how to specify different models, use authentication, and mix backends like LoLLMs for LLM and OpenAI for embedding.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Run minirag with lollms, mistral-nemo:latest for llm, and bge-m3:latest for embedding, use lollms for both llm and embedding\nminirag-server --llm-binding lollms --embedding-binding lollms\n\n# Using specific models (ensure they are installed in your ollama instance)\nminirag-server --llm-binding lollms --llm-model adrienbrault/nous-hermes2theta-llama3-8b:f16 --embedding-binding lollms --embedding-model nomic-embed-text --embedding-dim 1024\n\n# Using an authentication key\nminirag-server --key my-key\n\n# Using lollms for llm and openai for embedding\nminirag-server --llm-binding lollms --embedding-binding openai --embedding-model text-embedding-3-small\n```\n\n----------------------------------------\n\nTITLE: Running MiniRAG Server with Azure OpenAI Backend\nDESCRIPTION: Commands to start a MiniRAG server using Azure OpenAI for LLM and embedding, with options for authentication and different model combinations.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Run minirag with lollms, GPT-4o-mini  for llm, and text-embedding-3-small for embedding, use openai for both llm and embedding\nminirag-server --llm-binding azure_openai --llm-model GPT-4o-mini --embedding-binding openai --embedding-model text-embedding-3-small\n\n# Using an authentication key\nminirag-server --llm-binding azure_openai --llm-model GPT-4o-mini --embedding-binding azure_openai --embedding-model text-embedding-3-small --key my-key\n\n# Using lollms for llm and azure_openai for embedding\nminirag-server --llm-binding lollms --embedding-binding azure_openai --embedding-model text-embedding-3-small\n```\n\n----------------------------------------\n\nTITLE: Running MiniRAG Server with OpenAI Backend\nDESCRIPTION: Commands to start a MiniRAG server using OpenAI for both LLM and embedding, with options for authentication and different model combinations.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Run minirag with lollms, GPT-4o-mini  for llm, and text-embedding-3-small for embedding, use openai for both llm and embedding\nminirag-server --llm-binding openai --llm-model GPT-4o-mini --embedding-binding openai --embedding-model text-embedding-3-small\n\n# Using an authentication key\nminirag-server --llm-binding openai --llm-model GPT-4o-mini --embedding-binding openai --embedding-model text-embedding-3-small --key my-key\n\n# Using lollms for llm and openai for embedding\nminirag-server --llm-binding lollms --embedding-binding openai --embedding-model text-embedding-3-small\n```\n\n----------------------------------------\n\nTITLE: Running MiniRAG Server with Ollama Backend\nDESCRIPTION: Example commands for running MiniRAG server with Ollama as the default backend for both LLM and embedding. Shows variations for specifying different models, using authentication, and mixing backends.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Run minirag with ollama, mistral-nemo:latest for llm, and bge-m3:latest for embedding\nminirag-server\n\n# Using specific models (ensure they are installed in your ollama instance)\nminirag-server --llm-model adrienbrault/nous-hermes2theta-llama3-8b:f16 --embedding-model nomic-embed-text --embedding-dim 1024\n\n# Using an authentication key\nminirag-server --key my-key\n\n# Using lollms for llm and ollama for embedding\nminirag-server --llm-binding lollms\n```\n\n----------------------------------------\n\nTITLE: Running MiniRAG in Development Mode\nDESCRIPTION: Commands to run different MiniRAG server backends (LoLLMs, Ollama, OpenAI, Azure OpenAI) in development mode using Uvicorn.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn lollms_minirag_server:app --reload --port 9721\n\nuvicorn ollama_minirag_server:app --reload --port 9721\n\nuvicorn openai_minirag_server:app --reload --port 9721\n\nuvicorn azure_openai_minirag_server:app --reload --port 9721\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses from MiniRAG API\nDESCRIPTION: cURL command to send a POST request to the MiniRAG streaming query endpoint, allowing for streaming responses from the RAG system.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9721/query/stream\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"query\": \"Your question here\", \"mode\": \"hybrid\"}'\n```\n\n----------------------------------------\n\nTITLE: Installing MiniRAG with API Support from PyPI\nDESCRIPTION: Command to install MiniRAG with API support from PyPI. The package provides RAG capabilities through FastAPI servers that can be added to existing LLM services.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"lightrag-hku[api]\"\n```\n\n----------------------------------------\n\nTITLE: Installing MiniRAG from PyPI\nDESCRIPTION: Command to install MiniRAG from PyPI using pip. This installs the LightRAG package which serves as the foundation for MiniRAG.\nSOURCE: https://github.com/hkuds/minirag/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install lightrag-hku\n```\n\n----------------------------------------\n\nTITLE: Installing MiniRAG from Source\nDESCRIPTION: Instructions for installing MiniRAG directly from the source code repository. This approach is recommended for development and testing purposes.\nSOURCE: https://github.com/hkuds/minirag/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd MiniRAG\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Running MiniRAG with Command-line Arguments\nDESCRIPTION: Examples of running MiniRAG with different command-line arguments to override default or environment-set configuration values. Shows how to specify server port and use environment variables simultaneously.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# This command-line argument will override both the environment variable and default value\npython minirag.py --port 8080\n\n# The environment variable will override the default value but not the command-line argument\nPORT=7000 python minirag.py\n```\n\n----------------------------------------\n\nTITLE: Creating Azure OpenAI API Resources Using Azure CLI\nDESCRIPTION: Azure CLI commands to create an Azure OpenAI API resource. These commands create a resource group, an OpenAI service, and deploy GPT-4o and text-embedding models, then retrieve the endpoint and API key information.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Change the resource group name, location and OpenAI resource name as needed\nRESOURCE_GROUP_NAME=MiniRAG\nLOCATION=swedencentral\nRESOURCE_NAME=MiniRAG-OpenAI\n\naz login\naz group create --name $RESOURCE_GROUP_NAME --location $LOCATION\naz cognitiveservices account create --name $RESOURCE_NAME --resource-group $RESOURCE_GROUP_NAME  --kind OpenAI --sku S0 --location swedencentral\naz cognitiveservices account deployment create --resource-group $RESOURCE_GROUP_NAME  --model-format OpenAI --name $RESOURCE_NAME --deployment-name gpt-4o --model-name gpt-4o --model-version \"2024-08-06\"  --sku-capacity 100 --sku-name \"Standard\"\naz cognitiveservices account deployment create --resource-group $RESOURCE_GROUP_NAME  --model-format OpenAI --name $RESOURCE_NAME --deployment-name text-embedding-3-large --model-name text-embedding-3-large --model-version \"1\"  --sku-capacity 80 --sku-name \"Standard\"\naz cognitiveservices account show --name $RESOURCE_NAME --resource-group $RESOURCE_GROUP_NAME --query \"properties.endpoint\"\naz cognitiveservices account keys list --name $RESOURCE_NAME -g $RESOURCE_GROUP_NAME\n```\n\n----------------------------------------\n\nTITLE: Handling Chat Completion Requests\nDESCRIPTION: cURL command to send a chat completion request to the MiniRAG API, demonstrating how to interact with the chat functionality.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ncurl -N -X POST http://localhost:9721/api/chat -H \"Content-Type: application/json\" -d \\\n  '{\"model\":\"minirag:latest\",\"messages\":[{\"role\":\"user\",\"content\":\"猪八戒是谁\"}],\"stream\":true}'\n```\n\n----------------------------------------\n\nTITLE: Batch Uploading Files to MiniRAG System\nDESCRIPTION: cURL command to upload multiple files at once to the MiniRAG system via the documents/batch endpoint.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9721/documents/batch\" \\\n    -F \"files=@/path/to/doc1.txt\" \\\n    -F \"files=@/path/to/doc2.txt\"\n```\n\n----------------------------------------\n\nTITLE: Inserting Text into MiniRAG System\nDESCRIPTION: cURL command to insert text directly into the MiniRAG system via the documents/text endpoint, with options for content and description.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9721/documents/text\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"text\": \"Your text content here\", \"description\": \"Optional description\"}'\n```\n\n----------------------------------------\n\nTITLE: Uploading Single File to MiniRAG System\nDESCRIPTION: cURL command to upload a single file to the MiniRAG system via the documents/file endpoint, with options for file path and description.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9721/documents/file\" \\\n    -F \"file=@/path/to/your/document.txt\" \\\n    -F \"description=Optional description\"\n```\n\n----------------------------------------\n\nTITLE: Triggering Document Scan in MiniRAG System\nDESCRIPTION: cURL command to trigger a document scan for new files in the Input directory of the MiniRAG system, with an adjustable timeout.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9721/documents/scan\" --max-time 1800\n```\n\n----------------------------------------\n\nTITLE: MiniRAG Code Structure Overview\nDESCRIPTION: Directory structure of the MiniRAG repository, showing the organization of the dataset, core implementation modules, and reproduction scripts.\nSOURCE: https://github.com/hkuds/minirag/blob/main/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n├── dataset\n│   └── LiHua-World\n│       ├── README.md\n│       ├── README_CN.md\n│       ├── data\n│       │   ├── LiHuaWorld.zip\n│       └── qa\n│           ├── query_set.csv\n│           └── query_set.json\n├── minirag\n│   ├── kg\n│   │   ├── __init__.py\n│   │   ├── neo4j_impl.py\n│   │   └── oracle_impl.py\n│   ├── __init__.py\n│   ├── base.py\n│   ├── exceptions.py\n│   ├── llm.py\n│   ├── minirag.py\n│   ├── operate.py\n│   ├── prompt.py\n│   ├── storage.py\n│   └── utils.py\n├── reproduce\n│   ├── Step_0_index.py\n│   └── Step_1_QA.py\n├── LICENSE\n├── main.py\n├── README.md\n├── README_CN.md\n├── requirements.txt\n├── setup.py\n```\n\n----------------------------------------\n\nTITLE: Checking MiniRAG Server Health\nDESCRIPTION: cURL command to check the health and configuration of the MiniRAG server, useful for monitoring and diagnostics.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://localhost:9721/health\"\n```\n\n----------------------------------------\n\nTITLE: Clearing All Documents from MiniRAG System\nDESCRIPTION: cURL command to delete all documents from the MiniRAG system, providing a way to reset the document database.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X DELETE \"http://localhost:9721/documents\"\n```\n\n----------------------------------------\n\nTITLE: Installing MiniRAG as a Linux Service\nDESCRIPTION: Shell commands to install MiniRAG as a Linux service, including creating a service file, startup script, and configuring systemd.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\n#!/bin/bash\n\n# python virtual environment activation\nsource /home/netman/minirag-xyj/venv/bin/activate\n# start lightrag api server\nlightrag-server\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG without API Support\nDESCRIPTION: Command to install the base LightRAG package without API functionality, suitable for users who don't require the API features.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install lightrag-hku\n```\n\n----------------------------------------\n\nTITLE: Retrieving Available Ollama Models\nDESCRIPTION: cURL command to get a list of available Ollama models from the MiniRAG API endpoint.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:9721/api/tags\n```\n\n----------------------------------------\n\nTITLE: Retrieving Ollama Version Information\nDESCRIPTION: cURL command to get Ollama version information from the MiniRAG API endpoint.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:9721/api/version\n```\n\n----------------------------------------\n\nTITLE: Displaying MiniRAG Server Help Information\nDESCRIPTION: Command to display help information for the MiniRAG server, providing details on available options and usage.\nSOURCE: https://github.com/hkuds/minirag/blob/main/minirag/api/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nminirag-server --help\n```\n\n----------------------------------------\n\nTITLE: Preparing Knowledge Base in Markdown\nDESCRIPTION: Guidance on using the chat records as the knowledge base for RAG testing. This step involves utilizing all the data in the ./data directory.\nSOURCE: https://github.com/hkuds/minirag/blob/main/dataset/LiHua-World/README.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nStep 2. Use all the chat records in the `./data` directory as the knowledge base.\n```\n\n----------------------------------------\n\nTITLE: Conducting RAG Testing in Markdown\nDESCRIPTION: Instructions for using the provided question set to conduct RAG testing. This step involves using either the CSV or JSON format of the query set.\nSOURCE: https://github.com/hkuds/minirag/blob/main/dataset/LiHua-World/README.md#2025-04-21_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nStep 3. Use `query_set.csv` or `query_set.json` in the `./qa` directory as the question set to conduct RAG testing.\n```\n\n----------------------------------------\n\nTITLE: Unzipping Dataset Files in Markdown\nDESCRIPTION: Instructions for unzipping the LiHuaWorld.zip file to obtain the original chat records. This step is crucial for setting up the knowledge base for RAG testing.\nSOURCE: https://github.com/hkuds/minirag/blob/main/dataset/LiHua-World/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nStep 1. Unzip the `LiHuaWorld.zip` file in the `./data` directory to obtain the original chat records.\n```\n\n----------------------------------------\n\nTITLE: MiniRAG Citation Format\nDESCRIPTION: BibTeX citation for the MiniRAG paper, providing the correct format for academic references to this research work.\nSOURCE: https://github.com/hkuds/minirag/blob/main/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@article{fan2025minirag,\n  title={MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation},\n  author={Fan, Tianyu and Wang, Jingyuan and Ren, Xubin and Huang, Chao},\n  journal={arXiv preprint arXiv:2501.06713},\n  year={2025}\n}\n```"
  }
]
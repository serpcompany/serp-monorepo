[
  {
    "owner": "openai",
    "repo": "whisper",
    "content": "TITLE: Transcribing audio using Whisper in Python\nDESCRIPTION: Python code snippet demonstrating how to load a Whisper model and transcribe an audio file.\nSOURCE: https://github.com/openai/whisper/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\n```\n\n----------------------------------------\n\nTITLE: Advanced Whisper usage in Python\nDESCRIPTION: Python code example showing low-level access to Whisper model for language detection and audio decoding.\nSOURCE: https://github.com/openai/whisper/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport whisper\n\nmodel = whisper.load_model(\"turbo\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n\n# make log-Mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n\n# detect the spoken language\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\n\n# decode the audio\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\n\n# print the recognized text\nprint(result.text)\n```\n\n----------------------------------------\n\nTITLE: Running Inference on LibriSpeech Dataset\nDESCRIPTION: Processes the dataset through the Whisper model, collecting transcription hypotheses and reference texts for later evaluation.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhypotheses = []\nreferences = []\n\nfor mels, texts in tqdm(loader):\n    results = model.decode(mels, options)\n    hypotheses.extend([result.text for result in results])\n    references.extend(texts)\n```\n\n----------------------------------------\n\nTITLE: Calculating Word Error Rate (WER)\nDESCRIPTION: Computes the Word Error Rate between normalized reference and hypothesis transcriptions to evaluate Whisper model performance.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n\nprint(f\"WER: {wer * 100:.2f} %\")\n```\n\n----------------------------------------\n\nTITLE: Setting Decoding Options\nDESCRIPTION: Configures the decoding options for Whisper, specifying English language and disabling timestamps for short-form transcription.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# predict without timestamps for short-form transcription\noptions = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n```\n\n----------------------------------------\n\nTITLE: Transcribing audio files using Whisper CLI\nDESCRIPTION: Command-line examples for transcribing audio files using Whisper, including specifying the model, language, and translation task.\nSOURCE: https://github.com/openai/whisper/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwhisper audio.flac audio.mp3 audio.wav --model turbo\n```\n\nLANGUAGE: bash\nCODE:\n```\nwhisper japanese.wav --language Japanese\n```\n\nLANGUAGE: bash\nCODE:\n```\nwhisper japanese.wav --language Japanese --task translate\n```\n\n----------------------------------------\n\nTITLE: Loading Whisper Model\nDESCRIPTION: Loads the base English Whisper model and prints information about whether it's multilingual and its parameter count.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = whisper.load_model(\"base.en\")\nprint(\n    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Normalizing Transcriptions\nDESCRIPTION: Applies text normalization to both hypotheses and references to ensure fair comparison, standardizing text format, capitalization, and punctuation.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata[\"hypothesis_clean\"] = [normalizer(text) for text in data[\"hypothesis\"]]\ndata[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\ndata\n```\n\n----------------------------------------\n\nTITLE: Creating a LibriSpeech Dataset Wrapper Class\nDESCRIPTION: Implements a custom PyTorch dataset class that wraps the LibriSpeech dataset, handling audio preprocessing including trimming or padding audio to 30 seconds and converting to mel spectrograms.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LibriSpeech(torch.utils.data.Dataset):\n    \"\"\"\n    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n    It will drop the last few seconds of a very small portion of the utterances.\n    \"\"\"\n    def __init__(self, split=\"test-clean\", device=DEVICE):\n        self.dataset = torchaudio.datasets.LIBRISPEECH(\n            root=os.path.expanduser(\"~/.cache\"),\n            url=split,\n            download=True,\n        )\n        self.device = device\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        audio, sample_rate, text, _, _, _ = self.dataset[item]\n        assert sample_rate == 16000\n        audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n        mel = whisper.log_mel_spectrogram(audio)\n        \n        return (mel, text)\n```\n\n----------------------------------------\n\nTITLE: Initializing Dataset and DataLoader\nDESCRIPTION: Creates an instance of the LibriSpeech wrapper class with the 'test-clean' split and sets up a PyTorch DataLoader with batch size 16.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset = LibriSpeech(\"test-clean\")\nloader = torch.utils.data.DataLoader(dataset, batch_size=16)\n```\n\n----------------------------------------\n\nTITLE: Installing Whisper using pip\nDESCRIPTION: Commands to install the latest release of Whisper or the latest commit from the repository using pip.\nSOURCE: https://github.com/openai/whisper/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U openai-whisper\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/openai/whisper.git\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Dependencies\nDESCRIPTION: Imports necessary libraries and sets up the device for processing (CUDA if available, otherwise CPU).\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\n\ntry:\n    import tensorflow  # required in Colab to avoid protobuf compatibility issues\nexcept ImportError:\n    pass\n\nimport torch\nimport pandas as pd\nimport whisper\nimport torchaudio\n\nfrom tqdm.notebook import tqdm\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n```\n\n----------------------------------------\n\nTITLE: Importing Text Normalization Tools\nDESCRIPTION: Imports the jiwer library for WER calculation and Whisper's English text normalizer to standardize text before evaluation.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport jiwer\nfrom whisper.normalizers import EnglishTextNormalizer\n\nnormalizer = EnglishTextNormalizer()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Results\nDESCRIPTION: Organizes the transcription hypotheses and reference texts into a pandas DataFrame for analysis.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndata = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\ndata\n```\n\n----------------------------------------\n\nTITLE: Installing Whisper and Evaluation Packages\nDESCRIPTION: Installs the Whisper model from GitHub repository and jiwer package for calculating Word Error Rate (WER) metrics.\nSOURCE: https://github.com/openai/whisper/blob/main/notebooks/LibriSpeech.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install git+https://github.com/openai/whisper.git\n! pip install jiwer\n```\n\n----------------------------------------\n\nTITLE: Installing ffmpeg on various platforms\nDESCRIPTION: Commands to install ffmpeg, a required command-line tool, on different operating systems using their respective package managers.\nSOURCE: https://github.com/openai/whisper/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Whisper Model Size Specifications Table\nDESCRIPTION: Markdown table displaying the various Whisper model sizes, their parameter counts, and language support capabilities for both English-only and multilingual versions.\nSOURCE: https://github.com/openai/whisper/blob/main/model-card.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|  Size  | Parameters | English-only model | Multilingual model |\n|:------:|:----------:|:------------------:|:------------------:|\n|  tiny  |    39 M    |         ✓          |         ✓          |\n|  base  |    74 M    |         ✓          |         ✓          |\n| small  |   244 M    |         ✓          |         ✓          |\n| medium |   769 M    |         ✓          |         ✓          |\n| large  |   1550 M   |                    |         ✓          |\n| turbo  |   798 M    |                    |         ✓          |\n```\n\n----------------------------------------\n\nTITLE: Installing setuptools-rust\nDESCRIPTION: Command to install setuptools-rust, which may be required if the installation fails due to missing setuptools_rust module.\nSOURCE: https://github.com/openai/whisper/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install setuptools-rust\n```\n\n----------------------------------------\n\nTITLE: Markdown Changelog Documentation\nDESCRIPTION: Detailed version history for the Whisper project, documenting releases from the first versioned release in January 2023 through September 2024. Includes pull request references and descriptions of changes.\nSOURCE: https://github.com/openai/whisper/blob/main/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# CHANGELOG\n\n## [v20240930](https://github.com/openai/whisper/releases/tag/v20240930)\n\n* allowing numpy 2 in tests ([#2362](https://github.com/openai/whisper/pull/2362))\n* large-v3-turbo model ([#2361](https://github.com/openai/whisper/pull/2361))\n* test on python/pytorch versions up to 3.12 and 2.4.1 ([#2360](https://github.com/openai/whisper/pull/2360))\n* using sdpa if available ([#2359](https://github.com/openai/whisper/pull/2359))\n```\n\n----------------------------------------\n\nTITLE: Converting wav.scp to WAV files for CallHome & Switchboard datasets\nDESCRIPTION: This bash script converts the wav.scp file entries to individual WAV files for the CallHome and Switchboard datasets. It creates a 'wav' directory and processes each entry in the wav.scp file.\nSOURCE: https://github.com/openai/whisper/blob/main/data/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p wav\nwhile read name cmd; do\n    echo $name\n    echo ${cmd/\\|/} wav/$name.wav | bash\ndone < wav.scp\n```"
  }
]
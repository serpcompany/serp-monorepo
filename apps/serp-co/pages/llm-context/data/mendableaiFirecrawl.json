[
  {
    "owner": "mendableai",
    "repo": "firecrawl",
    "content": "TITLE: Extract Data using Groq and LLama 3 Model\nDESCRIPTION: This Python code snippet demonstrates how to extract specific information from the scraped website content using the Groq API and the Llama 3 model. It initializes the Groq client with an API key, defines a list of fields to extract, and sends a request to the Groq chat completion API. The response format is set to JSON. Replace `gsk_YOUR_GROQ_API_KEY` with your actual Groq API key.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_extraction/web-data-extraction-using-llms.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom groq import Groq\n\nclient = Groq(\n    api_key=\"gsk_YOUR_GROQ_API_KEY\",  # Note: Replace 'API_KEY' with your actual Groq API key\n)\n\n# Here we define the fields we want to extract from the page content\nextract = [\"summary\",\"date\",\"companies_building_with_quest\",\"title_of_the_article\",\"people_testimonials\"]\n\ncompletion = client.chat.completions.create(\n    model=\"llama3-8b-8192\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a legal advisor who extracts information from documents in JSON.\"\n        },\n        {\n            \"role\": \"user\",\n            # Here we pass the page content and the fields we want to extract\n            \"content\": f\"Extract the following information from the provided documentation:\\Page content:\\n\\n{page_content}\\n\\nInformation to extract: {extract}\"\n        }\n    ],\n    temperature=0,\n    max_tokens=1024,\n    top_p=1,\n    stream=False,\n    stop=None,\n    # We set the response format to JSON object\n    response_format={\"type\": \"json_object\"}\n)\n\n\n# Pretty print the JSON response\ndataExtracted = json.dumps(str(completion.choices[0].message.content), indent=4)\n\nprint(dataExtracted)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with LLM (Python)\nDESCRIPTION: This snippet demonstrates how to extract structured data from a URL using LLM extraction with Pydantic schemas. It defines Pydantic schemas for article data and a list of top articles, then uses the `scrape_url` method with `extractorOptions` to extract data according to the schema.  Only the main content is scraped.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ArticleSchema(BaseModel):\n    title: str\n    points: int\n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\n\ndata = app.scrape_url('https://news.ycombinator.com', {\n    'extractorOptions': {\n        'extractionSchema': TopArticlesSchema.model_json_schema(),\n        'mode': 'llm-extraction'\n    },\n    'pageOptions':{\n        'onlyMainContent': True\n    }\n})\nprint(data[\"llm_extraction\"])\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Batch Scraping (Python)\nDESCRIPTION: This code shows how to asynchronously batch scrape URLs using the `async_batch_scrape_urls` method. It specifies a list of URLs and desired output formats, the result is printed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nbatch_scrape_result = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\nprint(batch_scrape_result)\n```\n\n----------------------------------------\n\nTITLE: Initializing APIs with Credentials\nDESCRIPTION: This snippet loads environment variables for Google and Firecrawl API keys, configures the Google Generative AI module, and initializes the FirecrawlApp with the appropriate API keys. It is a necessary first step before using the Firecrawl or Gemini APIs.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_caching.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport datetime\nimport time\nimport google.generativeai as genai\nfrom google.generativeai import caching\nfrom dotenv import load_dotenv\nfrom firecrawl import FirecrawlApp\nimport json\n\n# Load environment variables\nload_dotenv()\n\n# Retrieve API keys from environment variables\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\nfirecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n\n# Configure the Google Generative AI module with the API key\ngenai.configure(api_key=google_api_key)\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key=firecrawl_api_key)\n```\n\n----------------------------------------\n\nTITLE: Generating response with Groq\nDESCRIPTION: This code snippet showcases how to generate a response to a user's question using the Groq API. It initializes the Groq client with an API key (replace 'YOUR_GROQ_API_KEY' with the correct key), constructs a prompt for the LLM including relevant context from the retrieved documents and the user's question, and uses the Groq chat completion endpoint to generate the response.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_rag_with_llama3/web-data-rag--with-llama3.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom groq import Groq\n\nclient = Groq(\n    api_key=\"YOUR_GROQ_API_KEY\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"llama3-8b-8192\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"You are a friendly assistant. Your job is to answer the users question based on the documentation provided below:\\nDocs:\\n\\n{docs}\\n\\nQuestion: {question}\"\n        }\n    ],\n    temperature=1,\n    max_tokens=1024,\n    top_p=1,\n    stream=False,\n    stop=None,\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Crawling a Website with Firecrawl\nDESCRIPTION: This snippet crawls a specified website using the Firecrawl API. It defines the URL to crawl and optional parameters for the crawling process, such as the limit on the number of pages to crawl. The crawled data is then processed to exclude the 'content' field and saved as a JSON file.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_flash_caching.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Crawl a website\ncrawl_url = 'https://dify.ai/'\nparams = {\n   \n    'crawlOptions': {\n        'limit': 100\n    }\n}\ncrawl_result = app.crawl_url(crawl_url, params=params)\n\nif crawl_result is not None:\n    # Convert crawl results to JSON format, excluding 'content' field from each entry\n    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]\n\n    # Save the modified results as a text file containing JSON data\n    with open('crawl_result.txt', 'w') as file:\n        file.write(json.dumps(cleaned_crawl_result, indent=4))\nelse:\n    print(\"No data returned from crawl.\")\n```\n\n----------------------------------------\n\nTITLE: Proxy Rotation Implementation - Python\nDESCRIPTION: This Python code implements proxy rotation to avoid IP blocking during web scraping. It defines a list of proxies and cycles through them using `itertools.cycle`. The scraper attempts to make a request using each proxy in the pool, with a maximum of 3 retries per URL. It also implements timeout handling.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom itertools import cycle\n\ndef get_proxy_pool():\n    proxies = [\n        'http://proxy1:port',\n        'http://proxy2:port',\n        'http://proxy3:port'\n    ]\n    return cycle(proxies)\n\ndef scrape_with_proxies(url, proxy_pool):\n    for _ in range(3):  # Max 3 retries\n        try:\n            proxy = next(proxy_pool)\n            response = requests.get(\n                url,\n                proxies={'http': proxy, 'https': proxy},\n                timeout=10\n            )\n            return response\n        except requests.RequestException:\n            continue\n    return None\n```\n\n----------------------------------------\n\nTITLE: Python: Validate Environment Variables with Pydantic\nDESCRIPTION: This Python code snippet uses Pydantic to create a `Settings` class for validating environment variables. It defines the expected variables (api_key, database_url, debug_mode), their types, and a `Config` class to specify the `.env` file and encoding. It also uses `SecretStr` for sensitive values like API keys.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseSettings, SecretStr\n\n\nclass Settings(BaseSettings):\n    api_key: SecretStr\n    database_url: str\n    debug_mode: bool = False\n\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n```\n\n----------------------------------------\n\nTITLE: LLM Extraction with Firecrawl API (cURL)\nDESCRIPTION: This snippet demonstrates how to use the Firecrawl API to scrape a website and extract structured data using an LLM, with a defined schema. It sends a POST request to the /v1/scrape endpoint with a URL and JSON options including a schema. Requires a valid API key in the Authorization header. The output contains the scraped content, metadata, and extracted JSON data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://www.mendable.ai/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"company_mission\": {\n                      \"type\": \"string\"\n            },\n            \"supports_sso\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_open_source\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_in_yc\": {\n                      \"type\": \"boolean\"\n            }\n          },\n          \"required\": [\n            \"company_mission\",\n            \"supports_sso\",\n            \"is_open_source\",\n            \"is_in_yc\"\n          ]\n        }\n      }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Creating a Cached Content Model\nDESCRIPTION: This snippet creates a cached content model in the Gemini API.  It sets a Time To Live (TTL) for the cache and configures a system instruction to guide the model's responses. The cache uses the uploaded crawled content, and the model is built from this cached content for faster retrieval and querying.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_flash_caching.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a cache with a 5 minute TTL\ncache = caching.CachedContent.create(\n    model=\"models/gemini-1.5-flash-002\",\n    display_name=\"website crawl testing again\", # used to identify the cache\n    system_instruction=\"You are an expert at this website, and your job is to answer user's query based on the website you have access to.\",\n    contents=[text_file],\n    ttl=datetime.timedelta(minutes=15),\n)\n# Construct a GenerativeModel which uses the created cache.\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow for Scheduled Web Scraping in YAML\nDESCRIPTION: This YAML file defines a GitHub Actions workflow that runs a Hacker News scraper every 6 hours. The workflow sets up Python, installs dependencies, executes the scraper script, and automatically commits any new data to the repository. It can also be triggered manually.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_37\n\nLANGUAGE: yaml\nCODE:\n```\nname: Run Hacker News Scraper\n\npermissions:\n  contents: write\n\non:\n  schedule:\n    - cron: \"0 */6 * * *\"\n  workflow_dispatch:\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n    \n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n          \n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r hacker-news-scraper/requirements.txt\n          \n      - name: Run scraper\n        run: python hacker-news-scraper/scraper.py\n        \n      - name: Commit and push if changes\n        run: |\n          git config --local user.email \"github-actions[bot]@users.noreply.github.com\"\n          git config --local user.name \"github-actions[bot]\"\n          git add .\n          git commit -m \"Update scraped data\" -a || exit 0\n          git push\n```\n\n----------------------------------------\n\nTITLE: Crawling Website with WebSockets (Python)\nDESCRIPTION: This code snippet demonstrates how to crawl a website using WebSockets with the `crawl_url_and_watch` method. It defines event handlers for 'document', 'error', and 'done' events, connects to the WebSocket, and initiates the crawl job. `nest_asyncio` is used because this is intended to run in a notebook environment.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# inside an async function...\nnest_asyncio.apply()\n\n# Define event handlers\ndef on_document(detail):\n    print(\"DOC\", detail)\n\ndef on_error(detail):\n    print(\"ERR\", detail['error'])\n\ndef on_done(detail):\n    print(\"DONE\", detail['status'])\n\n    # Function to start the crawl and watch process\nasync def start_crawl_and_watch():\n    # Initiate the crawl job and get the watcher\n    watcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })\n\n    # Add event listeners\n    watcher.add_event_listener(\"document\", on_document)\n    watcher.add_event_listener(\"error\", on_error)\n    watcher.add_event_listener(\"done\", on_done)\n\n    # Start the watcher\n    await watcher.connect()\n\n# Run the event loop\nawait start_crawl_and_watch()\n```\n\n----------------------------------------\n\nTITLE: Saving Scraped Data to CSV/JSON - Python\nDESCRIPTION: This `DataManager` class provides methods for saving lists of dictionaries to either CSV or JSON files.  It uses `pandas` for CSV writing and the built-in `json` module for JSON writing. The class also includes a cleanup method to remove old files, keeping the last 7 days of data. The base path defaults to a 'data' directory, and the filename includes a timestamp.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nimport json\nimport pandas as pd\n\nclass DataManager:\n    def __init__(self, base_path='data'):\n        self.base_path = Path(base_path)\n        self.base_path.mkdir(exist_ok=True)\n    \n    def save_data(self, data, format='csv'):\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        \n        if format == 'csv':\n            filename = self.base_path / f'data_{timestamp}.csv'\n            pd.DataFrame(data).to_csv(filename, index=False)\n        elif format == 'json':\n            filename = self.base_path / f'data_{timestamp}.json'\n            with open(filename, 'w') as f:\n                json.dump(data, f, indent=2)\n        \n        # Cleanup old files (keep last 7 days)\n        self._cleanup_old_files(days=7)\n        \n        return filename\n    \n    def _cleanup_old_files(self, days):\n        # Implementation for cleaning up old files\n        pass\n```\n\n----------------------------------------\n\nTITLE: Crawling a Website\nDESCRIPTION: Crawls a website using the `crawl_url` method.  Crawl options, such as excluding specific paths, can be configured using the `CrawlOptions` struct.  The method waits for the crawl to complete before returning the results.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_4\n\nLANGUAGE: Rust\nCODE:\n```\nlet crawl_options = CrawlOptions {\n    exclude_paths: vec![ \"blog/*\".into() ].into(),\n    ..Default::default()\n};\n\nlet crawl_result = app\n    .crawl_url(\"https://mendable.ai\", crawl_options)\n    .await;\n\nmatch crawl_result {\n    Ok(data) => println!(\"Crawl Result (used {} credits):\\n{:#?}\", data.credits_used, data.data),\n    Err(e) => eprintln!(\"Crawl failed: {}\", e),\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping Data with Firecrawl in Python\nDESCRIPTION: This function uses Firecrawl to scrape Hacker News. It creates a `FirecrawlApp` instance and calls `scrape_url()` with the base URL and parameters specifying that data should be extracted according to the `NewsData` schema. The schema helps Firecrawl automatically identify and extract the relevant HTML elements.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef get_news_data():\n    app = FirecrawlApp()\n\n    data = app.scrape_url(\n        BASE_URL,\n        params={\n            \"formats\": [\"extract\"],\n            \"extract\": {\"schema\": NewsData.model_json_schema()},\n        },\n    )\n\n    return data\n```\n\n----------------------------------------\n\nTITLE: Run O3 Web Crawler\nDESCRIPTION: Executes the main Python script for the O3 Web Crawler. This script will prompt the user for a website URL and an objective, then crawl the site to extract relevant information.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/o3-web-crawler/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython o3-web-crawler.py\n```\n\n----------------------------------------\n\nTITLE: Scraping a URL using Firecrawl API (cURL)\nDESCRIPTION: This snippet demonstrates how to scrape a single URL using the Firecrawl API. It sends a POST request to the /v1/scrape endpoint, specifying the URL to scrape and the desired output formats (markdown and html). An API key is required for authorization.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Batch Scrape with Firecrawl\nDESCRIPTION: Illustrates how to initiate an asynchronous batch scrape using the asyncBatchScrapeUrls method. This method allows you to scrape multiple URLs asynchronously, returning an ID to check the status.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst asyncBatchScrapeResult = await app.asyncBatchScrapeUrls(['https://firecrawl.dev', 'https://mendable.ai'], { formats: ['markdown', 'html'] });\n```\n\n----------------------------------------\n\nTITLE: Initializing FirecrawlApp and Crawling URL (Python)\nDESCRIPTION: This snippet initializes the FirecrawlApp with an API key and then crawls a website URL using the `crawl_url` method. The `params` argument sets a limit on the number of pages to crawl and specifies the desired output formats (markdown and html). The `poll_interval` is set to 30 seconds. The crawl status is printed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev', \n  params={\n    'limit': 100, \n    'scrapeOptions': {'formats': ['markdown', 'html']}\n  }, \n  poll_interval=30\n)\nprint(crawl_status)\n```\n\n----------------------------------------\n\nTITLE: Crawling Asynchronously\nDESCRIPTION: Crawls a website asynchronously using the `crawl_url_async` method.  This method returns a `CrawlAsyncResponse` containing the crawl's ID. The `check_crawl_status` method can be used to check the status of the crawl at any time. Completed crawls are deleted after 24 hours.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_5\n\nLANGUAGE: Rust\nCODE:\n```\nlet crawl_id = app.crawl_url_async(\"https://mendable.ai\", None).await?.id;\n\n// ... later ...\n\nlet status = app.check_crawl_status(crawl_id).await?;\n\nif status.status == CrawlStatusTypes::Completed {\n    println!(\"Crawl is done: {:#?}\", status.data);\n} else {\n    // ... wait some more ...\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Python Tests Workflow YAML\nDESCRIPTION: This workflow automatically executes on each `git push` command and pull request.  It checks out the repository code, sets up a Python environment, and specifies the Python version to use.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# test.yaml\nname: Python Tests\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Workflow with Multiple Triggers YAML\nDESCRIPTION: This workflow demonstrates how to configure multiple triggers, including `push`, `pull_request`, `schedule`, and `workflow_dispatch`. It runs on code changes on `git push`, executes daily scheduled tasks with cron, and can be triggered manually through the GitHub UI. Sensitive data like API keys are handled securely.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: Comprehensive Workflow\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n  schedule:\n    - cron: '0 0 * * *'  # Daily at midnight\n  workflow_dispatch:  # Manual trigger\n\njobs:\n  process:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run daily tasks\n        run: python daily_tasks.py\n        env:\n          API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n```\n\n----------------------------------------\n\nTITLE: Initializing Settings with Pydantic in Python\nDESCRIPTION: This code snippet demonstrates how to initialize settings using Pydantic's BaseSettings class. It shows how to define default values, handle secrets, and load environment variables from a .env file. The Settings class automatically validates types and converts values from environment variables.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nsettings = Settings()\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Firecrawl and Zod\nDESCRIPTION: This example showcases how to extract structured data from a URL using Firecrawl and Zod for schema definition. It defines a Zod schema for extracting specific fields from the scraped content and logs the extracted data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\n  apiKey: \"fc-YOUR_API_KEY\",\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://firecrawl.dev\", {\n  extractorOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"llm_extraction\"]);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Crawl with Firecrawl\nDESCRIPTION: Demonstrates how to initiate an asynchronous crawl using the asyncCrawlUrl method. It sets parameters such as excluded paths and a crawl limit, returning a job ID for status tracking.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst asyncCrawlResult = await app.asyncCrawlUrl('mendable.ai', { excludePaths: ['blog/*'], limit: 5});\n```\n\n----------------------------------------\n\nTITLE: Scraping URL with Firecrawl\nDESCRIPTION: Demonstrates how to scrape a URL using the Firecrawl Node SDK. It initializes the FirecrawlApp with an API key, then uses the scrapeUrl method to fetch content from a specified URL and prints the response.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n  formats: ['markdown', 'html'],\n});\n\nif (scrapeResponse) {\n  console.log(scrapeResponse)\n}\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n})\n\nconsole.log(crawlResponse)\n```\n\n----------------------------------------\n\nTITLE: Crawling Website with Exclusion Paths (Python)\nDESCRIPTION: This snippet shows how to crawl a website using the `crawl_url` method while excluding specific paths.  An optional idempotency key is created. It also shows how to optionally specify the `depth` parameter. The crawl result is printed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nidempotency_key = str(uuid.uuid4()) # optional idempotency key\ncrawl_result = app.crawl_url('firecrawl.dev', {'excludePaths': ['blog/*']}, 2, idempotency_key)\nprint(crawl_result)\n```\n\n----------------------------------------\n\nTITLE: Crawling a URL using Firecrawl API (cURL)\nDESCRIPTION: This snippet demonstrates how to initiate a crawl job using the Firecrawl API. It sends a POST request to the /v1/crawl endpoint with the URL to crawl, a limit on the number of pages to crawl, and the desired output formats (markdown and html). An API key is required for authorization.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer fc-YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"limit\": 10,\n      \"scrapeOptions\": {\n        \"formats\": [\"markdown\", \"html\"]\n      }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Crawling Website with Firecrawl\nDESCRIPTION: This snippet demonstrates how to crawl a website using the crawlUrl method of the FirecrawlApp. It sets a limit on the number of pages to crawl and specifies the output formats for the scraped content.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up a Python Project on PythonAnywhere (Bash)\nDESCRIPTION: This sequence of commands clones a Git repository, creates a virtual environment, activates it, installs dependencies, and recreates the `.env` file on PythonAnywhere.  Requires Git to be installed and configured, as well as a `requirements.txt` file in the repository.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/your-username/your-repo.git\n$ cd your-repo\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip install -r requirements.txt\n\n# Recreate your .env file\n$ touch .env\n$ echo \"FIRECRAWL_API_KEY='your-api-key-here'\" >> .env\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Firecrawl API (cURL)\nDESCRIPTION: This snippet shows how to extract structured data from multiple URLs using the Firecrawl API with cURL. It sends a POST request to the /v1/extract endpoint with an array of URLs, a prompt, and a schema defining the desired data structure. It requires a valid API key in the Authorization header. The output is a JSON object containing a job ID and URL trace.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/extract \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\n        \"https://firecrawl.dev/*\", \n        \"https://docs.firecrawl.dev/\", \n        \"https://www.ycombinator.com/companies\"\n      ],\n      \"prompt\": \"Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"company_mission\": {\n            \"type\": \"string\"\n          },\n          \"is_open_source\": {\n            \"type\": \"boolean\"\n          },\n          \"is_in_yc\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"required\": [\n          \"company_mission\",\n          \"is_open_source\",\n          \"is_in_yc\"\n        ]\n      }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Saving Product Data to AWS S3 with Boto3 (Python)\nDESCRIPTION: This code snippet demonstrates how to save scraped product data to an AWS S3 bucket using the boto3 library. It initializes an S3 client, creates a filename based on the current date, and uploads the product data as a JSON file to the specified bucket. Requires the `boto3` package and AWS credentials configured.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport boto3  # pip install boto3\nfrom datetime import datetime\n\ndef save_yesterday_top_products():\n    products = get_yesterday_top_products()\n    \n    # Initialize S3 client\n    s3 = boto3.client('s3')\n    \n    # Create filename with date\n    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n    filename = f\"ph_top_products_{date_str}.json\"\n    \n    # Upload to S3\n    s3.put_object(\n        Bucket='your-bucket-name',\n        Key=filename,\n        Body=json.dumps(products)\n    )\n```\n\n----------------------------------------\n\nTITLE: Crawling a Website with Firecrawl in Python\nDESCRIPTION: This snippet demonstrates how to use the Firecrawl library to crawl a specific website URL. It initializes the FirecrawlApp with an API key, then uses the crawl_url method to retrieve the website's content, excluding specific paths like 'blog/*' and 'usecases/*'.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/contradiction_testing/web-data-contradiction-testing-using-llms.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"YOUR-KEY\")\n\ncrawl_result = app.crawl_url('mendable.ai', {'crawlerOptions': {'excludes': ['blog/*','usecases/*']}})\n\nprint(crawl_result)\n```\n\n----------------------------------------\n\nTITLE: Scraping URL with FirecrawlApp (Python)\nDESCRIPTION: This code snippet demonstrates how to scrape a single URL using the `scrape_url` method of the FirecrawlApp. It takes a URL as input and prints the scraped data. No additional parameters are specified.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nurl = 'https://example.com'\nscraped_data = app.scrape_url(url)\n```\n\n----------------------------------------\n\nTITLE: Scrape Website Content using Firecrawl\nDESCRIPTION: This Python code snippet demonstrates how to scrape a website URL using Firecrawl. It initializes the FirecrawlApp with an API key, and then uses the scrape_url method to retrieve the content of the specified URL. The pageOptions parameter is set to extract only the main content of the page, excluding navigation and footer elements. Replace `fc-YOUR_FIRECRAWL_API_KEY` with your actual Firecrawl API key.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_extraction/web-data-extraction-using-llms.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom firecrawl import FirecrawlApp  # Importing the FireCrawlLoader\n\nurl = \"https://about.fb.com/news/2024/04/introducing-our-open-mixed-reality-ecosystem/\"\n\nfirecrawl = FirecrawlApp(\n    api_key=\"fc-YOUR_FIRECRAWL_API_KEY\",\n)\npage_content = firecrawl.scrape_url(url=url,  # Target URL to crawl\n    params={\n        \"pageOptions\":{\n            \"onlyMainContent\": True # Ignore navs, footers, etc.\n        }\n    })\nprint(page_content)\n```\n\n----------------------------------------\n\nTITLE: Build and Push Docker Container with GitHub Actions\nDESCRIPTION: This workflow builds and pushes a Docker container to Docker Hub when changes are made to the Dockerfile or files in the `src` directory. It sets up Docker Buildx, logs in to Docker Hub using stored credentials, builds the image, and pushes it with the `latest` tag and a tag based on the commit hash. Requires a Docker Hub account and repository secrets for username and token.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_39\n\nLANGUAGE: yaml\nCODE:\n```\nname: Build and Push Container\non:\n  push:\n    branches: [main]\n    paths:\n      - 'Dockerfile'\n      - 'src/**'\n  workflow_dispatch:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n        \n      - name: Login to Docker Hub\n        uses: docker/login-action@v2\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n          \n      - name: Build and push\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          push: true\n          tags: |\n            user/app:latest\n            user/app:${{ github.sha }}\n```\n\n----------------------------------------\n\nTITLE: Create Project Directory\nDESCRIPTION: Creates a new directory named 'learn-scheduling' and changes the current directory to it.  This prepares the environment for the scraping project.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir learn-scheduling\ncd learn-scheduling\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow - YAML\nDESCRIPTION: This YAML file defines a GitHub Actions workflow that schedules and runs a web scraper. It is configured to run every five minutes using a cron schedule and can also be triggered manually. The workflow sets up a Python environment, installs dependencies, executes the scraper script, and commits any changes back to the repository. It uses the FIRCRAWL_API_KEY secret stored in GitHub repository settings.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_31\n\nLANGUAGE: yaml\nCODE:\n```\nname: Run Firecrawl Scraper\n\non:\n  schedule:\n    - cron: \"0/5 * * * *\" # Runs every five minute\n  workflow_dispatch: # Allows manual trigger\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.9\"\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install pydantic firecrawl-py\n\n      - name: Run scraper\n        run: python firecrawl_scraper.py\n        env:\n          # Add any environment variables your scraper needs\n          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n\n      - name: Commit and push if changes\n        run: |\n          git config --global user.name 'GitHub Actions Bot'\n          git config --global user.email 'actions@github.com'\n          git add .\n          git commit -m \"Update scraped data\" || exit 0\n          git push\n```\n\n----------------------------------------\n\nTITLE: Batch Scraping Multiple URLs (cURL)\nDESCRIPTION: This snippet demonstrates how to batch scrape multiple URLs at once using the Firecrawl API. It sends a POST request to the /v1/batch/scrape endpoint with an array of URLs and the desired output formats.  It requires a valid API key in the Authorization header. The output is a job ID for tracking the batch scraping process.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Scraping with Retry Mechanism (Python)\nDESCRIPTION: This code implements a retry mechanism for the scraping function using the `tenacity` library.  It retries the `get_yesterday_top_products` function up to 3 times with exponential backoff.  Requires the `tenacity` library to be installed. Any exceptions raised during scraping are logged and re-raised to trigger the retry.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n    reraise=True\n)\ndef get_yesterday_top_products():\n    try:\n        app = FirecrawlApp()\n        data = app.scrape_url(\n            BASE_URL,\n            params={\n                \"formats\": [\"extract\"],\n                \"extract\": {\n                    \"schema\": YesterdayTopProducts.model_json_schema(),\n                    \"prompt\": \"Extract the top products listed under the 'Yesterday's Top Products' section.\"\n                },\n            },\n        )\n        return data[\"extract\"][\"products\"]\n    except Exception as e:\n        logger.error(f\"Scraping failed: {str(e)}\")\n        raise\n```\n\n----------------------------------------\n\nTITLE: Scraping a URL using curl\nDESCRIPTION: This curl command sends a POST request to the /scrape endpoint with a JSON payload specifying the URL to scrape and various options, such as wait time, timeout, headers, and a selector to check for.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/playwright-service-ts/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:3000/scrape \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"url\": \"https://example.com\",\n  \"wait_after_load\": 1000,\n  \"timeout\": 15000,\n  \"headers\": {\n    \"Custom-Header\": \"value\"\n  },\n  \"check_selector\": \"#content\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Query Claude Python\nDESCRIPTION: Queries the Claude model with the prepared prompt. It specifies the model, max tokens, and the prompt content. The response from Claude is stored in the response variable.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Query Claude\nresponse = anthropic_client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1000,\n    messages=[\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n)\n\nprint(\"Claude response received.\")\n```\n\n----------------------------------------\n\nTITLE: Scheduling Patterns with Schedule\nDESCRIPTION: This code showcases different scheduling patterns using the `schedule` library. It includes examples for scheduling a job every 10 minutes, every hour, every day at a specific time, every Monday, every Wednesday at a specific time, and every minute at a specific second.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nschedule.every(10).minutes.do(job)\nschedule.every().hour.do(job)\nschedule.every().day.at(\"10:30\").do(job)\nschedule.every().monday.do(job)\nschedule.every().wednesday.at(\"13:15\").do(job)\nschedule.every().day.at(\"12:42\", \"Europe/Amsterdam\").do(job)\nschedule.every().minute.at(\":17\").do(job)  # 17th second of a minute\n```\n\n----------------------------------------\n\nTITLE: LLM Extraction without Schema (cURL)\nDESCRIPTION: This snippet shows how to extract data without providing a specific schema to the LLM extraction endpoint. It passes a prompt that provides the LLM with the information to generate the output structure.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"prompt\": \"Extract the company mission from the page.\"\n      }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Scrape URL with Firecrawl Python\nDESCRIPTION: Scrapes the specified URL using the FirecrawlApp's scrape_url method.  It uses a parameter to only get the main content of the page.  The scraped content is stored in the page_content variable, and its length is printed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Scrape the URL using Firecrawl\npage_content = firecrawl_app.scrape_url(url, params={\"pageOptions\": {\"onlyMainContent\": True}})\n\nprint(f\"Page content scraped. Length: {len(page_content['content'])} characters\")\n```\n\n----------------------------------------\n\nTITLE: Basic Firecrawl URL Mapping\nDESCRIPTION: Demonstrates the basic usage of the Firecrawl Python SDK to map a URL. It initializes the FirecrawlApp, loads the API key from environment variables, and calls the `map_url()` method.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv; load_dotenv()\n\napp = FirecrawlApp()\n\nresponse = app.map_url(url=\"https://firecrawl.dev\")\n```\n\n----------------------------------------\n\nTITLE: Scraping a URL\nDESCRIPTION: Scrapes a single URL using the `scrape_url` method.  This method takes the URL as a parameter and returns a `Document` containing the scraped data. Error handling is included to catch and print any errors during the scraping process.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_2\n\nLANGUAGE: Rust\nCODE:\n```\nlet scrape_result = app.scrape_url(\"https://firecrawl.dev\", None).await;\nmatch scrape_result {\n    Ok(data) => println!(\"Scrape result:\\n{}\", data.markdown),\n    Err(e) => eprintln!(\"Scrape failed: {}\", e),\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables\nDESCRIPTION: This command copies the `.env.example` file to `.env` so you can add your API keys there.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-web-crawler/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Crawling a Website with Firecrawl\nDESCRIPTION: This snippet uses the FirecrawlApp to crawl a specified website ('https://dify.ai/').  It defines crawl options to limit the number of pages crawled.  The resulting crawl data is cleaned to remove the 'content' field, converted to JSON, and saved to a file named 'crawl_result.txt'.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_caching.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Crawl a website\ncrawl_url = 'https://dify.ai/'\nparams = {\n   \n    'crawlOptions': {\n        'limit': 100\n    }\n}\ncrawl_result = app.crawl_url(crawl_url, params=params)\n\nif crawl_result is not None:\n    # Convert crawl results to JSON format, excluding 'content' field from each entry\n    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]\n\n    # Save the modified results as a text file containing JSON data\n    with open('crawl_result.txt', 'w') as file:\n        file.write(json.dumps(cleaned_crawl_result, indent=4))\nelse:\n    print(\"No data returned from crawl.\")\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: This command installs the required Python packages for the Gemini 2.5 Web Extractor using pip.  It relies on the `requirements.txt` file, which lists all necessary dependencies.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-web-extractor/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: System Monitoring Workflow Steps Definition (YAML)\nDESCRIPTION: Defines the steps to run within the `monitor` job in the System Monitoring workflow. It checks out the repository, sets up Python, installs dependencies from `requirements.txt`, runs tests using `pytest`, and collects system metrics using a Python script.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_17\n\nLANGUAGE: YAML\nCODE:\n```\njobs:\n  monitor:\n    runs-on: ubuntu-latest\n    \n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n      \n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n      \n      - name: Run tests\n        run: pytest tests/\n      \n      - name: Collect system metrics\n        run: python main.py\n```\n\n----------------------------------------\n\nTITLE: Batch Scraping URLs with Firecrawl\nDESCRIPTION: Demonstrates how to batch scrape multiple URLs using the batchScrapeUrls method. This allows you to efficiently scrape content from several URLs in a single request.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst batchScrapeResponse = await app.batchScrapeUrls(['https://firecrawl.dev', 'https://mendable.ai'], {\n  formats: ['markdown', 'html'],\n})\n```\n\n----------------------------------------\n\nTITLE: Scraping and Crawling with Firecrawl Python SDK\nDESCRIPTION: This snippet shows how to scrape and crawl a website using the Firecrawl Python SDK. It initializes the FirecrawlApp with an API key and then uses the scrape_url and crawl_url methods.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n  'https://firecrawl.dev', \n  formats=[\"markdown\", \"html\"]\n)\nprint(scrape_status)\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev',\n  limit=100,\n  scrapeOptions'={'formats': ['markdown', 'html']}\n  poll_interval=30\n)\nprint(crawl_status)\n```\n\n----------------------------------------\n\nTITLE: Checking Crawl Status with Firecrawl\nDESCRIPTION: Shows how to check the status of a crawl job using the checkCrawlStatus method. It takes a job ID as input and returns the current status of the crawl job.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst status = await app.checkCrawlStatus(id);\n```\n\n----------------------------------------\n\nTITLE: Mapping a URL using Firecrawl API (cURL)\nDESCRIPTION: This snippet shows how to map a website and get all the URLs. It sends a POST request to the /v1/map endpoint, providing the URL of the website to map. An API key is required for authorization.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Setting up a Web Scraper Project\nDESCRIPTION: These commands create a project directory, initialize a virtual environment, install dependencies, and configure the API key using environment variables.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nmkdir product-hunt-scraper\ncd product-hunt-scraper\ntouch scraper.py .env\npython -m venv venv\nsource venv/bin/activate\npip install pydantic firecrawl-py\necho \"FIRECRAWL_API_KEY='your-api-key-here' >> .env\"\n```\n\n----------------------------------------\n\nTITLE: Querying the Cached Gemini Model\nDESCRIPTION: This snippet queries the Gemini model using the previously created cached model, asking the question: \"What powers website scraping with Dify?\". The response is extracted from the model's output, converted to a dictionary, and then the relevant text is printed to the console.  Error handling (e.g. checking for empty responses) is not included.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_caching.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Query the model\nresponse = model.generate_content([\"What powers website scraping with Dify?\"])\nresponse_dict = response.to_dict()\nresponse_text = response_dict['candidates'][0]['content']['parts'][0]['text']\nprint(response_text)\n```\n\n----------------------------------------\n\nTITLE: YAML: Use GitHub Secrets in Workflow\nDESCRIPTION: This YAML code snippet shows how to use GitHub Secrets to pass sensitive information as environment variables to a workflow job. It demonstrates referencing secrets in the `secrets` context within the `env` section of a step, assigning them to environment variables such as `API_KEY` and `DATABASE_URL`.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\nname: Web Scraping Pipeline\n# ... the rest of the file\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n    \n    steps:\n      # ... the rest of the steps\n          \n      - name: Run scraper\n        env:\n          API_KEY: ${{ secrets.API_KEY }}\n          DATABASE_URL: ${{ secrets.DATABASE_URL }}\n        run: python scraper.py\n```\n\n----------------------------------------\n\nTITLE: Install pipreqs and Generate requirements.txt (Bash)\nDESCRIPTION: This snippet demonstrates using the `pipreqs` package to automatically generate a `requirements.txt` file by scanning the project's Python scripts for dependencies.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install pipreqs\npipreqs .\n```\n\n----------------------------------------\n\nTITLE: Counting Links Found by /crawl\nDESCRIPTION: Iterates through the crawl response to extract and count the number of unique links found using the /crawl endpoint, demonstrating how to process the data returned by the endpoint.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncrawl_links = set()\n\nfor page in crawl_response['data']:\n    crawl_links.update(page[\"links\"])\n\nlen(crawl_links)\n```\n\n----------------------------------------\n\nTITLE: Scraping and Crawling with Firecrawl Node.js SDK\nDESCRIPTION: This snippet shows how to scrape and crawl a website using the Firecrawl Node.js SDK. It initializes the FirecrawlApp with an API key and then uses the scrapeUrl and crawlUrl methods.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_25\n\nLANGUAGE: javascript\nCODE:\n```\nimport FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n  formats: ['markdown', 'html'],\n});\n\nif (scrapeResponse) {\n  console.log(scrapeResponse)\n}\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;\n\nif (crawlResponse) {\n  console.log(crawlResponse)\n}\n```\n\n----------------------------------------\n\nTITLE: Python: Load Environment Variables with dotenv\nDESCRIPTION: This Python code snippet demonstrates how to load environment variables from a `.env` file using the `python-dotenv` library. It loads the variables using `load_dotenv()` and accesses them using `os.getenv()`. A simple validation ensures that the required API key and database URL are present.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access variables\napi_key = os.getenv('API_KEY')\ndatabase_url = os.getenv('DATABASE_URL')\n\nif not api_key or not database_url:\n    raise ValueError(\"Missing required environment variables\")\n```\n\n----------------------------------------\n\nTITLE: Verify Heroku Configuration (Bash)\nDESCRIPTION: This command retrieves and displays the configuration variables set for the Heroku application.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nheroku config\n```\n\n----------------------------------------\n\nTITLE: Running the Crawler\nDESCRIPTION: This command executes the DeepSeek V3 web crawler script. It initiates the crawling process, prompting the user for a website URL and objective, then extracting relevant information based on the specified parameters. The script outputs the results in JSON format.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-crawler/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython deepseek-v3-crawler.py\n```\n\n----------------------------------------\n\nTITLE: Setting up the Vectorstore\nDESCRIPTION: This code sets up the vectorstore using Ollama embeddings and the FAISS vectorstore.  It splits the documents into chunks of 1000 characters with a 200 character overlap using RecursiveCharacterTextSplitter, then creates a FAISS index from the document splits using the Ollama embeddings model.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_rag_with_llama3/web-data-rag--with-llama3.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = FAISS.from_documents(documents=splits, embedding=OllamaEmbeddings())\n```\n\n----------------------------------------\n\nTITLE: Running the Crawler (bash)\nDESCRIPTION: This command executes the `gemini-2.5-crawler.py` script using the Python interpreter. This will start the web crawling process and prompt the user for the website URL and search objective.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-crawler/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython gemini-2.5-crawler.py\n```\n\n----------------------------------------\n\nTITLE: Scraping with Extract\nDESCRIPTION: Scrapes a URL and extracts structured data based on a JSON schema.  The `serde_json::json!` macro is used to define the JSON schema. `ScrapeOptions` are configured to use the `Extract` format with the specified schema.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_3\n\nLANGUAGE: Rust\nCODE:\n```\nlet json_schema = json!({\n    \"type\": \"object\",\n    \"properties\": {\n        \"top\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"title\": {\"type\": \"string\"},\n                    \"points\": {\"type\": \"number\"},\n                    \"by\": {\"type\": \"string\"},\n                    \"commentsURL\": {\"type\": \"string\"}\n                },\n                \"required\": [\"title\", \"points\", \"by\", \"commentsURL\"]\n            },\n            \"minItems\": 5,\n            \"maxItems\": 5,\n            \"description\": \"Top 5 stories on Hacker News\"\n        }\n    },\n    \"required\": [\"top\"]\n});\n\nlet llm_extraction_options = ScrapeOptions {\n    formats: vec![ ScrapeFormats::Extract ].into(),\n    extract: ExtractOptions {\n        schema: json_schema.into(),\n        ..Default::default()\n    }.into(),\n    ..Default::default()\n};\n\nlet llm_extraction_result = app\n    .scrape_url(\"https://news.ycombinator.com\", llm_extraction_options)\n    .await;\n\nmatch llm_extraction_result {\n    Ok(data) => println!(\"LLM Extraction Result:\\n{:#?}\", data.extract.unwrap()),\n    Err(e) => eprintln!(\"LLM Extraction failed: {}\", e),\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing FirecrawlApp and Scraping URL (Python)\nDESCRIPTION: This snippet initializes the FirecrawlApp with an API key and then scrapes a website URL using the `scrape_url` method. The `params` argument specifies the desired output formats (markdown and html).  The scrape status is then printed to the console.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n  'https://firecrawl.dev', \n  params={'formats': ['markdown', 'html']}\n)\nprint(scrape_status)\n```\n\n----------------------------------------\n\nTITLE: Import Python Packages\nDESCRIPTION: Imports necessary Python libraries for web scraping. The code is the starting point for the `firecrawl_scraper.py` script. The subsequent code would use these libraries to build the web scraper.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Committing and Pushing Changes to Heroku (Bash)\nDESCRIPTION: These commands commit the changes made to the project and push them to both the Heroku and origin repositories. This deploys the updated code to the Heroku platform. Requires Git to be initialized and configured.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\ngit commit -m \"Switch data persistence to S3\"\ngit push heroku main\ngit push origin main\n```\n\n----------------------------------------\n\nTITLE: Copy .env.example to .env\nDESCRIPTION: Copies the `.env.example` file to `.env`. This creates the environment file which will hold API keys and other sensitive information for the application.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-crawler/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Retrieval using similarity search\nDESCRIPTION: This code demonstrates how to perform a similarity search on the vectorstore based on a user's question. It uses the `similarity_search` method to retrieve the most relevant documents from the vectorstore based on the given question, which are then used as context for the LLM.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_rag_with_llama3/web-data-rag--with-llama3.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"What is firecrawl?\"\ndocs = vectorstore.similarity_search(query=question)\n```\n\n----------------------------------------\n\nTITLE: Create Procfile (Bash)\nDESCRIPTION: This snippet creates a `Procfile` and defines the command to execute the scraper script using Python when deployed to Heroku.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ntouch Procfile\necho \"worker: python scraper.py\" > Procfile\n```\n\n----------------------------------------\n\nTITLE: Cron Schedule Examples\nDESCRIPTION: Examples of cron expressions used to schedule tasks in GitHub Actions.  These examples cover running jobs every 6 hours, at specific minutes of the hour, on weekdays, and on specific days of the week at certain times.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_6\n\nLANGUAGE: Text\nCODE:\n```\n# Every 6 hours at the first minute\n- cron: '0 */6 * * *'\n\n# At minute 15 of every hour\n- cron: '15 * * * *'\n\n# Every weekday (Monday through Friday)\n- cron: '0 0 * * 1-5'\n\n# Each day at 12am, 6am, 12pm, 6pm on Tuesday, Thursday, Saturday\n- cron: '0 0,6,12,18 * * 1,3,5'\n```\n\n----------------------------------------\n\nTITLE: Loading website with Firecrawl\nDESCRIPTION: This code snippet demonstrates how to load website data using Firecrawl and Langchain. It initializes the FireCrawlLoader with an API key, a target URL, and sets the mode to 'crawl' to crawl all accessible subpages.  Replace 'YOUR_API_KEY' with a valid Firecrawl API key.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_rag_with_llama3/web-data-rag--with-llama3.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.document_loaders import FireCrawlLoader  # Importing the FireCrawlLoader\n\nurl = \"https://firecrawl.dev\"\nloader = FireCrawlLoader(\n    api_key=\"fc-YOUR_API_KEY\", # Note: Replace 'YOUR_API_KEY' with your actual FireCrawl API key\n    url=url,  # Target URL to crawl\n    mode=\"crawl\"  # Mode set to 'crawl' to crawl all accessible subpages\n)\ndocs = loader.load()\n```\n\n----------------------------------------\n\nTITLE: Crawling Website with WebSockets\nDESCRIPTION: Shows how to crawl a website using WebSockets to receive real-time updates. It configures event listeners for document, error, and done events to process the crawl data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Crawl a website with WebSockets:\nconst watch = await app.crawlUrlAndWatch('mendable.ai', { excludePaths: ['blog/*'], limit: 5});\n\nwatch.addEventListener(\"document\", doc => {\n console.log(\"DOC\", doc.detail);\n});\n\nwatch.addEventListener(\"error\", err => {\n console.error(\"ERR\", err.detail.error);\n});\n\nwatch.addEventListener(\"done\", state => {\n console.log(\"DONE\", state.detail.status);\n});\n```\n\n----------------------------------------\n\nTITLE: Add API keys to .env file\nDESCRIPTION: Sets the API keys in the .env file. Replace `your_firecrawl_api_key_here` and `your_together_api_key_here` with your actual API keys from Firecrawl and Together AI, respectively. This is crucial for authenticating with the services.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-crawler/README.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nFIRECRAWL_API_KEY=your_firecrawl_api_key_here\nTOGETHER_API_KEY=your_together_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Installing Firecrawl on Kubernetes (Bash)\nDESCRIPTION: This script applies a series of Kubernetes manifests to install Firecrawl on a Kubernetes cluster. It creates configmaps, secrets, and deployments for the playwright service, API, worker, and Redis. Ensure that the manifests are located in the current directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes/cluster-install/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f configmap.yaml\nkubectl apply -f secret.yaml\nkubectl apply -f playwright-service.yaml\nkubectl apply -f api.yaml\nkubectl apply -f worker.yaml\nkubectl apply -f redis.yaml\n```\n\n----------------------------------------\n\nTITLE: Run Company Researcher Script\nDESCRIPTION: Executes the main Python script for the GPT-4.1 Company Researcher. This command initiates the research process, prompting the user for company name and information requests, and then proceeds to gather and structure data accordingly.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-company-researcher/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython gpt-4.1-company-researcher.py\n```\n\n----------------------------------------\n\nTITLE: Mapping a URL with Search using Firecrawl API (cURL)\nDESCRIPTION: This snippet shows how to map a website and search for specific URLs. It sends a POST request to the /v1/map endpoint with a search parameter to find URLs containing specific keywords. An API key is required for authorization.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\",\n      \"search\": \"docs\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Run Firecrawl Tests\nDESCRIPTION: This command executes the test suite using npm. The tests are designed to cover various aspects of the system, including crawling accuracy, response time, and error handling.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm run test\n```\n\n----------------------------------------\n\nTITLE: Save Firecrawl News Data to JSON\nDESCRIPTION: This code defines a function save_firecrawl_news_data() that handles saving the scraped Hacker News data to a JSON file. It generates a filename using the current timestamp in the format YYYY_MM_DD_HH_MM and saves the data to this timestamped JSON file.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef save_firecrawl_news_data():\n    # Get the data\n    data = get_firecrawl_news_data()\n    # Format current date for filename\n    date_str = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n    filename = f\"firecrawl_hacker_news_data_{date_str}.json\"\n\n    # Save the news items to JSON file\n    with open(filename, \"w\") as f:\n        json.dump(data[\"extract\"][\"news_items\"], f, indent=4)\n\n    return filename\n```\n\n----------------------------------------\n\nTITLE: Searching and Scraping with Firecrawl API (cURL)\nDESCRIPTION: This snippet demonstrates how to search and scrape web content using the Firecrawl API with cURL. It sends a POST request to the /v1/search endpoint with a query, limit, and scrape options to retrieve content in markdown and links formats.  It requires a valid API key in the Authorization header. The output is a JSON object containing search results with scraped content.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5,\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\", \"links\"]\n    }\n  }'\n```\n\n----------------------------------------\n\nTITLE: Saving Scraped Data to JSON in Python\nDESCRIPTION: This function saves the scraped Hacker News data to a JSON file. It calls `get_news_data()` to fetch the latest data, generates a filename using the current timestamp, and saves the data to a JSON file with proper indentation. The filename contains the current date, ensuring uniqueness.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef save_firecrawl_news_data():\n    \"\"\"\n    Save the scraped news data to a JSON file with the current date in the filename.\n    \"\"\"\n    # Get the data\n    data = get_news_data()\n    # Format current date for filename\n    date_str = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n    filename = f\"firecrawl_hacker_news_data_{date_str}.json\"\n\n    # Save the news items to JSON file\n    with open(filename, \"w\") as f:\n        json.dump(data[\"extract\"][\"news_items\"], f, indent=4)\n\n    print(f\"{datetime.now()}: Successfully saved the news data.\")\n    \nif __name__ == \"__main__\":\n    save_firecrawl_news_data()\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys\nDESCRIPTION: Sets the API keys for Together AI, SerpAPI, and Firecrawl in the `.env` file. Replace `your_together_ai_key`, `your_serpapi_key`, and `your_firecrawl_key` with your actual API keys. These API keys are essential for the script to access the respective services.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-extractor/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nTOGETHER_API_KEY=your_together_ai_key\nSERP_API_KEY=your_serpapi_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies\nDESCRIPTION: Installs the required Python packages for the GPT-4.1 Company Researcher tool using pip and the requirements.txt file. This ensures that all necessary libraries are available for the script to function correctly.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-company-researcher/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Up Logging - Python\nDESCRIPTION: This function, `setup_logging`, configures a logger using Python's `logging` module. It creates a log directory if it doesn't exist and sets up both a file handler for detailed logs (DEBUG level) and a console handler for important messages (INFO level). The log messages include timestamps, logger names, log levels, and the actual message. The log files are named based on the current date.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom pathlib import Path\n\ndef setup_logging():\n    log_dir = Path('logs')\n    log_dir.mkdir(exist_ok=True)\n    \n    # File handler for detailed logs\n    file_handler = logging.FileHandler(\n        log_dir / f'scraper_{datetime.now().strftime(\"%Y%m%d\")}.log'\n    )\n    file_handler.setLevel(logging.DEBUG)\n    \n    # Console handler for important messages\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.INFO)\n    \n    # Configure logging\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[file_handler, console_handler]\n    )\n```\n\n----------------------------------------\n\nTITLE: Prepare Prompt for Claude Python\nDESCRIPTION: Prepares a prompt for the Claude model. The prompt instructs Claude to analyze webpage content and extract specific information (page title, whether the company is part of YC, and whether the product is open source). The prompt specifies the desired JSON format for the output.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Prepare the prompt for Claude\nprompt = f\"\"\"Analyze the following webpage content and extract the following information:\n1. The title of the page\n2. Whether the company is part of Y Combinator (YC)\n3. Whether the company/product is open source\n\nReturn the information in JSON format with the following schema:\n{{\n    \"main_header_title\": string,\n    \"is_yc_company\": boolean,\n    \"is_open_source\": boolean\n}}\n\nWebpage content:\n{page_content['content']}\n\nReturn only the JSON, nothing else.\"\"\"\n\nprint(\"Prompt prepared for Claude.\")\n```\n\n----------------------------------------\n\nTITLE: Running a Python Scraper on PythonAnywhere (Bash)\nDESCRIPTION: This command navigates to the project directory, activates the virtual environment, and executes the scraper.py script. This is intended for scheduling via PythonAnywhere's task scheduler. Requires a virtual environment named `venv` and a script named `scraper.py` in the specified directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\ncd /home/your-username/your-repo && source venv/bin/activate && python scraper.py\n```\n\n----------------------------------------\n\nTITLE: Batch Scraping with WebSockets (Python)\nDESCRIPTION: This code demonstrates how to batch scrape URLs using WebSockets with the `batch_scrape_urls_and_watch` method. It defines event handlers for 'document', 'error', and 'done' events, connects to the WebSocket, and initiates the batch scrape job. `nest_asyncio` is used because this is intended to run in a notebook environment.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# inside an async function...\nnest_asyncio.apply()\n\n# Define event handlers\ndef on_document(detail):\n    print(\"DOC\", detail)\n\ndef on_error(detail):\n    print(\"ERR\", detail['error'])\n\ndef on_done(detail):\n    print(\"DONE\", detail['status'])\n\n# Function to start the crawl and watch process\nasync def start_crawl_and_watch():\n    # Initiate the crawl job and get the watcher\n    watcher = app.batch_scrape_urls_and_watch(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\n\n    # Add event listeners\n    watcher.add_event_listener(\"document\", on_document)\n    watcher.add_event_listener(\"error\", on_error)\n    watcher.add_event_listener(\"done\", on_done)\n\n    # Start the watcher\n    await watcher.connect()\n\n# Run the event loop\nawait start_crawl_and_watch()\n```\n\n----------------------------------------\n\nTITLE: Handling Different Environments with .env Files in Bash\nDESCRIPTION: This snippet shows how to manage different environments (development, staging, production) using environment-specific .env files. It demonstrates the creation of multiple .env files and outlines the strategy of configuring variables to prevent use of production resources during development and testing.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\n.env                # Default environment variables\n.env.development   # Development-specific variables\n.env.staging       # Staging-specific variables\n.env.production    # Production-specific variables\n```\n\n----------------------------------------\n\nTITLE: Create python files using bash\nDESCRIPTION: Creates `main.py` and `requirements.txt` files using `touch` command. These files will contain the main python script logic and the dependencies.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\ntouch main.py requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running the Gemini 2.5 Web Extractor Script\nDESCRIPTION: This command executes the main Python script for the Gemini 2.5 Web Extractor.  It requires Python 3.8 or higher and the environment variables set up correctly.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-web-extractor/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython gemini-2.5-web-extractor.py\n```\n\n----------------------------------------\n\nTITLE: Running the Crawler\nDESCRIPTION: This command executes the main Python script for the web crawler, prompting the user for the website URL and search objective.  Requires Python 3.8+ and the installed dependencies.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-web-crawler/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython gpt-4.1-web-crawler.py\n```\n\n----------------------------------------\n\nTITLE: Saving Scraped Data to JSON\nDESCRIPTION: This function calls the get_yesterday_top_products function to get the scraped data and then saves it to a JSON file with the current date in the filename.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef save_yesterday_top_products():\n    products = get_yesterday_top_products()\n    \n    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n    filename = f\"ph_top_products_{date_str}.json\"\n    \n    with open(filename, \"w\") as f:\n        json.dump(products, f)\n        \nif __name__ == \"__main__\":\n    save_yesterday_top_products()\n```\n\n----------------------------------------\n\nTITLE: Batch Scraping URLs (Python)\nDESCRIPTION: This snippet demonstrates how to batch scrape multiple URLs using the `batch_scrape_urls` method. It specifies a list of URLs and desired output formats, including an idempotency key. A depth parameter is passed. The result is printed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nidempotency_key = str(uuid.uuid4()) # optional idempotency key\nbatch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']}, 2, idempotency_key)\nprint(batch_scrape_result)\n```\n\n----------------------------------------\n\nTITLE: Creating a Batch File for Windows Task Scheduler - Python\nDESCRIPTION: This batch file automates the execution of a Python web scraper script. It navigates to the project directory, activates the virtual environment, runs the scraper, and then deactivates the environment.  This allows scheduled execution of the Python scraper by the Windows Task Scheduler.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@echo off\ncd /d \"C:\\path\\to\\your\\project\"\ncall venv\\Scripts\\activate\npython cron_scraper.py\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Collect and Save System Metrics (Python)\nDESCRIPTION: Python script (`main.py`) that collects system metrics such as CPU usage, memory usage, disk usage, and active process count using the `psutil` library. The script then saves these metrics to a JSON file, organized by date, in the `system_metrics` directory. It also handles potential errors during execution.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nimport psutil\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\n\ndef get_system_metrics():\n    \"\"\"Collect key system metrics\"\"\"\n    metrics = {\n        \"cpu_percent\": psutil.cpu_percent(interval=1),\n        \"memory_percent\": psutil.virtual_memory().percent,\n        \"disk_usage\": psutil.disk_usage('/').percent,\n        \"timestamp\": datetime.now().isoformat()\n    }\n    \n    # Add running processes count\n    metrics[\"active_processes\"] = len(psutil.pids())\n    \n    return metrics\n\ndef save_metrics(metrics):\n    \"\"\"Save metrics to a JSON file with today's date\"\"\"\n    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n    reports_dir = Path(\"system_metrics\")\n    reports_dir.mkdir(exist_ok=True)\n    \n    # Save to daily file\n    file_path = reports_dir / f\"metrics_{date_str}.json\"\n    \n    # Load existing metrics if file exists\n    if file_path.exists():\n        with open(file_path) as f:\n            daily_metrics = json.load(f)\n    else:\n        daily_metrics = []\n    \n    # Append new metrics\n    daily_metrics.append(metrics)\n    \n    # Save updated metrics\n    with open(file_path, 'w') as f:\n        json.dump(daily_metrics, f, indent=2)\n\ndef main():\n    try:\n        metrics = get_system_metrics()\n        save_metrics(metrics)\n        print(f\"System metrics collected at {metrics['timestamp']}\")\n        print(f\"CPU: {metrics['cpu_percent']}% | Memory: {metrics['memory_percent']}%\")\n        return True\n    except Exception as e:\n        print(f\"Error collecting metrics: {str(e)}\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Containers\nDESCRIPTION: These commands are used to build and run the Docker containers for Firecrawl. `docker compose build` builds the Docker images defined in the `docker-compose.yml` file. `docker compose up` starts the containers, creating a local instance of Firecrawl accessible at `http://localhost:3002`.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose build\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow YAML Configuration\nDESCRIPTION: This YAML configuration defines a GitHub Actions workflow to automate the execution of the Product Hunt scraper. It includes scheduled and manual triggers, defines the execution environment (Ubuntu, Python 3.10), installs dependencies, runs the scraper, and commits/pushes any changes to the repository.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nname: Product Hunt Scraper\n\non:\n  schedule:\n    - cron: '0 1 * * *'  # Runs at 1 AM UTC daily\n  workflow_dispatch:  # Allows manual trigger\n\npermissions:\n  contents: write\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v3\n      with:\n          persist-credentials: true\n      \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.10'\n        \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n        \n    - name: Run scraper\n      env:\n        FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n      run: python scraper.py\n        \n    - name: Commit and push if changes\n      run: |\n        git config --local user.email \"github-actions[bot]@users.noreply.github.com\"\n        git config --local user.name \"github-actions[bot]\"\n        git add *.json\n        git diff --quiet && git diff --staged --quiet || git commit -m \"Update ProductHunt data [skip ci]\"\n        git push\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs the required Python packages for the DeepSeek V3 web crawler. It uses pip, the Python package installer, and the requirements.txt file to install all necessary dependencies such as FireCrawl and Hugging Face libraries.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-crawler/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies with Pip\nDESCRIPTION: Installs the necessary Python packages from the requirements.txt file. This step is crucial for setting up the environment before running the web crawler.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/o4-mini-web-crawler/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: System Monitoring Workflow Definition (name and triggers)\nDESCRIPTION: Defines the name and triggers for the system monitoring GitHub Actions workflow in YAML format. It sets the workflow to run every 30 minutes via a cron schedule and also allows manual triggering using `workflow_dispatch`.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_15\n\nLANGUAGE: YAML\nCODE:\n```\nname: System Monitoring\non:\n  schedule:\n    - cron: '*/30 * * * *'  # Run every 30 minutes\n  workflow_dispatch:        # Enables manual trigger\n```\n\n----------------------------------------\n\nTITLE: Querying the Gemini Model\nDESCRIPTION: This code snippet queries the Gemini model with a specific question. It generates content based on the crawled website data and prints the model's response, extracted from the generated content.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_flash_caching.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Query the model\nresponse = model.generate_content([\"What powers website scraping with Dify?\"])\nresponse_dict = response.to_dict()\nresponse_text = response_dict['candidates'][0]['content']['parts'][0]['text']\nprint(response_text)\n```\n\n----------------------------------------\n\nTITLE: Running the Application in Development Mode\nDESCRIPTION: This command starts the application in development mode, typically using a tool like nodemon for automatic restarts upon code changes.  This is useful during development.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/playwright-service-ts/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables from .env Files in Python\nDESCRIPTION: This code shows how to load environment variables from environment-specific .env files using the `dotenv` library.  It retrieves the environment name from the `ENVIRONMENT` environment variable (defaulting to 'development'), constructs the path to the corresponding .env file, and loads the variables into the environment.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nimport os\n\nenv = os.getenv('ENVIRONMENT', 'development')\nenv_file = f'.env.{env}'\n\nload_dotenv(env_file)\n```\n\n----------------------------------------\n\nTITLE: Parse and Display Result Python\nDESCRIPTION: Parses the JSON response from Claude using json.loads() and prints the parsed result to the console using json.dumps() with an indent of 2 for better readability.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Parse and print the result\nresult = json.loads(response.content[0].text)\nprint(json.dumps(result, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Install Groq and Firecrawl Python Dependencies\nDESCRIPTION: This bash command installs the necessary Python packages for interacting with Groq and Firecrawl. It installs the 'groq' and 'firecrawl-py' libraries using pip, the Python package installer. These packages provide the necessary functions and classes for interacting with the Groq and Firecrawl APIs.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_extraction/web-data-extraction-using-llms.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install groq firecrawl-py\n```\n\n----------------------------------------\n\nTITLE: Install Required Packages with pip\nDESCRIPTION: Installs the necessary Python packages listed in the requirements.txt file.  This command ensures that all dependencies, such as firecrawl, together, and python-dotenv, are installed before running the web crawler.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-crawler/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Create .gitignore File\nDESCRIPTION: Creates a `.gitignore` file and adds `.env` to it. This ensures that the environment variables file is excluded from version control, preventing sensitive information from being accidentally committed to a public repository.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntouch .gitignore\necho \".env\" >> .gitignore\n```\n\n----------------------------------------\n\nTITLE: Run Web Crawler Script\nDESCRIPTION: Executes the main Python script for the web crawler. This script prompts the user for a website URL and an objective, then crawls the site, analyzes the content, and returns structured data if the objective is met.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/o4-mini-web-crawler/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython o4-mini-web-crawler.py\n```\n\n----------------------------------------\n\nTITLE: Cron Schedule Examples YAML\nDESCRIPTION: These are examples of cron syntax for GitHub Actions scheduling. The first example runs daily at 3:30 AM UTC. The second example runs every Monday at 1:00 PM UTC.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# Daily at 3:30 AM UTC\n- cron: '30 3 * * *'\n\n# Every Monday at 1:00 PM UTC\n- cron: '0 13 * * 1'\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys\nDESCRIPTION: This snippet demonstrates setting up environment variables for the FireCrawl and Hugging Face API keys. These keys are essential for authenticating with the respective services and enabling the crawler to function correctly. The keys should be replaced with the user's actual API keys.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-crawler/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nHUGGINGFACE_API_KEY=your_huggingface_api_key\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs the required Python packages for the web crawler using pip. It reads the list of dependencies from the `requirements.txt` file.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-web-crawler/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Rate Limiting with Random Delays - Python\nDESCRIPTION: This Python code snippet demonstrates how to implement rate limiting in a web scraper by adding random delays between requests. It loops through a list of URLs, waits a random amount of time (between 2 and 5 seconds), and then makes a request. This helps prevent the scraper from overloading the server and getting blocked.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport random\n\ndef scrape_with_delays(urls):\n    for url in urls:\n        try:\n            # Random delay between 2-5 seconds\n            delay = random.uniform(2, 5)\n            time.sleep(delay)\n            \n            # Your scraping code here\n            response = requests.get(url)\n            \n        except requests.RequestException as e:\n            logging.error(f\"Error scraping {url}: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Run DeepSeek V3 Extraction Script\nDESCRIPTION: This command executes the `deepseek-v3-extract.py` script, initiating the company research process. It requires the API keys to be set up beforehand in the environment.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-company-researcher/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython deepseek-v3-extract.py\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository (bash)\nDESCRIPTION: This command clones the specified repository URL using Git and then changes the current directory to the newly cloned repository directory. This is the first step in setting up the web crawler project.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-crawler/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <your-repo-url>\ncd <your-repo-directory>\n```\n\n----------------------------------------\n\nTITLE: Mounting a Fly.io Volume in fly.toml\nDESCRIPTION: Configures the `fly.toml` file to mount the created volume to the `/data` directory. This allows Redis to persist data to the volume.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/redis/README.md#_snippet_3\n\nLANGUAGE: TOML\nCODE:\n```\n[mounts]\nsource      = \"redis_server\"\ndestination = \"/data\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Firecrawl and Gemini APIs\nDESCRIPTION: This code snippet initializes the Firecrawl application and configures the Google Generative AI module. It loads API keys from environment variables, sets up the Gemini API, and initializes the Firecrawl application with the API key.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_flash_caching.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport datetime\nimport time\nimport google.generativeai as genai\nfrom google.generativeai import caching\nfrom dotenv import load_dotenv\nfrom firecrawl import FirecrawlApp\nimport json\n\n# Load environment variables\nload_dotenv()\n\n# Retrieve API keys from environment variables\ngoogle_api_key = os.getenv(\"GOOGLE_API_KEY\")\nfirecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n\n# Configure the Google Generative AI module with the API key\ngenai.configure(api_key=google_api_key)\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key=firecrawl_api_key)\n```\n\n----------------------------------------\n\nTITLE: Install Python Schedule Library\nDESCRIPTION: This code shows how to install the Python `schedule` library using pip.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip install schedule\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies\nDESCRIPTION: Installs the required Python packages for the O3 Web Crawler using pip. This command reads the 'requirements.txt' file, which lists all the necessary libraries and their versions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/o3-web-crawler/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: YAML: System Monitor GitHub Actions Workflow\nDESCRIPTION: This YAML code defines a GitHub Actions workflow for system monitoring. The workflow is triggered by a schedule (cron job every 30 minutes) and manual dispatch. It checks out the repository, sets up Python, installs dependencies, runs tests, collects metrics, and commits and pushes the changes back to the repository.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nname: System Monitor\n\non:\n  schedule:\n    - cron: '*/30 * * * *'\n  workflow_dispatch:\n\npermissions:\n  contents: write\n\njobs:\n  monitor:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          \n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          \n      - name: Run tests\n        run: python -m pytest\n        \n      - name: Collect metrics\n        run: python main.py\n        \n      - name: Commit and push changes\n        run: |\n          git config --global user.name 'github-actions[bot]'\n          git config --global user.email 'github-actions[bot]@users.noreply.github.com'\n          git add metrics.json\n          git commit -m \"Update metrics\" || exit 0\n          git push\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys (text)\nDESCRIPTION: This snippet shows how to populate the `.env` file with the Firecrawl API key and the Gemini API key. These keys are necessary for the crawler to authenticate with the respective APIs.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-crawler/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nGEMINI_API_KEY=your_gemini_api_key\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository\nDESCRIPTION: This command clones the repository containing the DeepSeek V3 web crawler. It allows users to download the necessary files to their local machine. Replace <repository-url> with the actual URL of the repository.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-crawler/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <repository-url>\ncd <repository-directory>\n```\n\n----------------------------------------\n\nTITLE: Bash: Commit Workflow Changes\nDESCRIPTION: This bash code snippet commits the changes made to the GitHub Actions workflow file. It adds all changed files to the staging area and commits them with the message \"Add a commit step to the workflow file\".\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\ngit commit -m \"Add a commit step to the workflow file\"\ngit push origin main\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Setup\nDESCRIPTION: This code snippet shows how to set environment variables for API keys required by the script. It needs OpenRouter, Firecrawl, and SerpAPI keys to access the services.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-company-researcher/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nOPENROUTER_API_KEY=your_openrouter_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nSERP_API_KEY=your_serpapi_key\n```\n\n----------------------------------------\n\nTITLE: View data metadata\nDESCRIPTION: This code calls the function get_firecrawl_news_data and prints the metadata key of the dictionary returned by the function.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndata['metadata']\n```\n\n----------------------------------------\n\nTITLE: Copying Environment File (bash)\nDESCRIPTION: This command duplicates the `.env.example` file to create a new file named `.env`. The `.env` file is used to store sensitive information such as API keys, preventing them from being directly embedded in the code.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-crawler/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Install Python Libraries\nDESCRIPTION: Installs the required Python libraries: `requests` for making HTTP requests, `beautifulsoup4` for parsing HTML, `firecrawl-py` for using the Firecrawl API, and `python-dotenv` for managing environment variables.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install requests beautifulsoup4 firecrawl-py python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Price Scraper Schedules Workflow YAML\nDESCRIPTION: YAML configuration for a GitHub Actions workflow that defines schedules for a price scraper. It demonstrates using cron expressions to run the scraper every 4 hours, daily at 1:30 AM UTC, and on weekdays at 9 AM UTC. It also includes a job definition with steps for checking out the repository and running the Firecrawl scraper.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\nname: Price Scraper Schedules\non:\n  schedule:\n    - cron: '0 */4 * * *'    # Every 4 hours\n    - cron: '30 1 * * *'     # Daily at 1:30 AM UTC\n    - cron: '0 9 * * 1-5'    # Weekdays at 9 AM UTC\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run Firecrawl scraper\n        env:\n          API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n        run: python scraper.py\n```\n\n----------------------------------------\n\nTITLE: Example Usage Input\nDESCRIPTION: Demonstrates how to interact with the crawler by providing a website URL and a specific objective (finding the company's contact information). This input will be used by the crawler to extract the specified information.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-crawler/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nEnter the website to crawl: https://example.com\nEnter your objective: Find the company's contact information\n```\n\n----------------------------------------\n\nTITLE: Initialize Clients Python\nDESCRIPTION: Initializes the FirecrawlApp and Anthropic client using the API keys retrieved from environment variables.  This step prepares the Firecrawl and Anthropic services for use in subsequent steps.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Initialize FirecrawlApp and Anthropic client\nfirecrawl_app = FirecrawlApp(api_key=firecrawl_api_key)\nanthropic_client = Anthropic(api_key=anthropic_api_key)\n\nprint(\"Firecrawl and Anthropic clients initialized.\")\n```\n\n----------------------------------------\n\nTITLE: List of Python Dependencies\nDESCRIPTION: This snippet shows the project's dependencies. python-dotenv manages environment variables, google-generativeai provides access to Google's generative AI models, requests is for making HTTP requests, and serpapi is for interacting with the SerpApi service.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-web-extractor/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\npython-dotenv==1.0.0\ngoogle-generativeai==0.3.2\nrequests==2.31.0\nserpapi==0.1.5\n```\n\n----------------------------------------\n\nTITLE: Running the Apartment Finder Script\nDESCRIPTION: This command executes the `apartment_finder.py` script. This will start the interactive prompt, guiding the user through the apartment search process.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deep-research-apartment-finder/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython apartment_finder.py\n```\n\n----------------------------------------\n\nTITLE: Heroku CLI Installation (Linux)\nDESCRIPTION: These commands install the Heroku CLI on Linux using a shell script.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://cli-assets.heroku.com/install.sh | sh  # Linux\n```\n\n----------------------------------------\n\nTITLE: Run Price Scraper Workflow YAML\nDESCRIPTION: This workflow automatically runs a price scraper every 12 hours. It uses the `schedule` trigger to run every 12 hours and the `workflow_dispatch` trigger to allow manual triggers. It sets up Python, manages environment variables, and securely uses API keys.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: Run Price Scraper\n\non:\n  schedule:\n    - cron: '0 */12 * * *'  # Runs every 12 hours\n  workflow_dispatch:  # Allows manual triggers\n\njobs:\n  scrape:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n        \n    - name: Run scraper\n      env:\n        API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n      run: python scraper.py\n```\n\n----------------------------------------\n\nTITLE: Scraping with Actions (cURL)\nDESCRIPTION: This snippet shows how to use the Firecrawl API to scrape a website with actions. Actions are steps to follow to navigate the page to the content that is required.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n        \"url\": \"google.com\",\n        \"formats\": [\"markdown\"],\n        \"actions\": [\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"click\", \"selector\": \"textarea[title=\\\"Search\\\"]\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"write\", \"text\": \"firecrawl\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"press\", \"key\": \"ENTER\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"click\", \"selector\": \"h3\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"screenshot\"}\n        ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Analyzing Website Content for Contradictions with Claude in Python\nDESCRIPTION: This snippet processes the crawled data to identify contradictions using the Claude API. It iterates through all page combinations, creates a prompt containing the combined page content, and sends it to the Claude model for analysis. The API response containing any identified contradictions or inconsistencies is then appended to a final output list.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/contradiction_testing/web-data-contradiction-testing-using-llms.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom itertools import combinations\n\npage_combinations = []\n\nfor first_page, second_page in combinations(crawl_result, 2):\n    combined_string = \"First Page:\\n\" + first_page['markdown'] + \"\\n\\nSecond Page:\\n\" + second_page['markdown']\n    page_combinations.append(combined_string)\n\nimport anthropic\n\nclient = anthropic.Anthropic(\n    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n    api_key=\"YOUR-KEY\",\n)\n\nfinal_output = []\n\nfor page_combination in page_combinations:\n\n    prompt = \"Here are two pages from a companies website, your job is to find any contradictions or differences in opinion between the two pages, this could be caused by outdated information or other. If you find any contradictions, list them out and provide a brief explanation of why they are contradictory or differing. Make sure the explanation is specific and concise. It is okay if you don't find any contradictions, just say 'No contradictions found' and nothing else. Here are the pages: \" + \"\\n\\n\".join(page_combination)\n\n    message = client.messages.create(\n        model=\"claude-3-opus-20240229\",\n        max_tokens=1000,\n        temperature=0.0,\n        system=\"You are an assistant that helps find contradictions or differences in opinion between pages in a company website and knowledge base. This could be caused by outdated information in the knowledge base.\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    final_output.append(message.content)\n```\n\n----------------------------------------\n\nTITLE: Copy Environment File\nDESCRIPTION: Copies the `.env.example` file to `.env`. The `.env` file stores sensitive information like API keys and should be kept separate from version control. This step allows the user to configure the necessary API keys for the tool to function.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-company-researcher/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Cron Schedule Examples (Python)\nDESCRIPTION: These snippets demonstrate how to set up cron jobs to run the `cron_scraper.py` script at different intervals, specifically every minute and every hour. They include commands for changing the directory, executing the script with the correct Python interpreter from the virtual environment, and redirecting output and errors to a log file.  The `cd`, `&&`, and redirection operators `>>` and `2>&1` are critical for proper cron execution.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Run every minute\n*/1 * * * * cd /absolute/path/to/project && /absolute/path/to/.venv/bin/python cron_scraper.py >> ~/cron.log 2>&1\n\n# Run every hour\n*/1 * * * * cd /absolute/path/to/project && /absolute/path/to/.venv/bin/python cron_scraper.py >> ~/cron.log 2>&1\n```\n\n----------------------------------------\n\nTITLE: Checking Batch Scrape Status (Python)\nDESCRIPTION: This snippet demonstrates how to check the status of an asynchronous batch scrape job using the `check_batch_scrape_status` method. It takes the job ID and prints the status.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nid = batch_scrape_result['id']\nstatus = app.check_batch_scrape_status(id)\n```\n\n----------------------------------------\n\nTITLE: Load Test Configuration - Phases YAML\nDESCRIPTION: Defines the load test phases with varying arrival rates. Each phase specifies a duration and an arrival rate, simulating different load levels on the system. The configuration includes an initial load, increased load, peak load, and cool-down phase.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-6-7/load-test-7.md#_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\nphases:\n  - duration: 60\n    arrivalRate: 1  # Initial load\n  - duration: 120\n    arrivalRate: 2  # Increased load\n  - duration: 180\n    arrivalRate: 3  # Peak load\n  - duration: 60\n    arrivalRate: 1  # Cool down\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies\nDESCRIPTION: This command installs the required Python packages listed in the `requirements.txt` file. These packages are essential for the Swarm Firecrawl Marketing Agent to function correctly. Ensure `pip` is installed and configured properly.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/openai_swarm_firecrawl/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Firecrawl Node SDK\nDESCRIPTION: Command to install the Firecrawl Node SDK using npm. This allows you to use the Firecrawl library in your Node.js projects for scraping and crawling websites.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @mendable/firecrawl-js\n```\n\n----------------------------------------\n\nTITLE: Scheduling Scraper with 'schedule' Library (Python)\nDESCRIPTION: This snippet uses the 'schedule' library to run the `save_firecrawl_news_data` function every hour.  It imports the schedule and time modules and defines a while loop that continuously checks and runs any pending scheduled tasks, sleeping for 1 second between checks. Requires the `schedule` library.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport schedule\nimport time\n\n# Assuming save_firecrawl_news_data is defined elsewhere\n# e.g., from firecrawl_scraper import save_firecrawl_news_data\n\n# Schedule the scraper to run every hour\nschedule.every().hour.do(save_firecrawl_news_data)\n\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Setting Heroku Configuration Variables (Bash)\nDESCRIPTION: This snippet shows how to set AWS credentials as Heroku configuration variables using the Heroku CLI. These variables are used to authenticate with AWS S3. Requires the Heroku CLI to be installed and configured.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nheroku config:set AWS_ACCESS_KEY_ID='your-key'\nheroku config:set AWS_SECRET_ACCESS_KEY='your-secret'\n```\n\n----------------------------------------\n\nTITLE: Scraping ProductHunt Data\nDESCRIPTION: This function initializes a Firecrawl app, then uses it to scrape the data from ProductHunt using the previously defined Pydantic schema and a prompt.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nBASE_URL = \"https://www.producthunt.com\"\n\n\ndef get_yesterday_top_products():\n    app = FirecrawlApp()\n\n    data = app.scrape_url(\n        BASE_URL,\n        params={\n            \"formats\": [\"extract\"],\n            \"extract\": {\n                \"schema\": YesterdayTopProducts.model_json_schema(),\n                \n                \"prompt\": \"Extract the top products listed under the 'Yesterday's Top Products' section. There will be exactly 5 products.\",\n            },\n        },\n    )\n\n    return data[\"extract\"][\"products\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing FirecrawlApp\nDESCRIPTION: Initializes the `FirecrawlApp` with your API key.  The `FirecrawlApp::new` method takes the API key as a string.  The `#[tokio::main]` macro allows the main function to be asynchronous.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_1\n\nLANGUAGE: Rust\nCODE:\n```\nuse firecrawl::FirecrawlApp;\n\n#[tokio::main]\nasync fn main() {\n    // Initialize the FirecrawlApp with the API key\n    let app = FirecrawlApp::new(\"fc-YOUR-API-KEY\").expect(\"Failed to initialize FirecrawlApp\");\n\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Using the XML Sitemap Function and Saving to File\nDESCRIPTION: This snippet demonstrates how to use the `create_xml_sitemap` function with a base URL and a list of links obtained from the `/map` endpoint (`map_response[\"links\"]`). It then saves the generated XML sitemap string to a file named `sitemap.xml`, ensuring proper XML declaration and UTF-8 encoding.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nbase_url = \"https://books.toscrape.com\"\nlinks = map_response[\"links\"]\n\nxml_sitemap = create_xml_sitemap(links, base_url)\n\n# Save to file\nwith open(\"sitemap.xml\", \"w\", encoding=\"utf-8\") as f:\n    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n    f.write(xml_sitemap)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies (bash)\nDESCRIPTION: This command uses pip, the Python package installer, to install all the necessary dependencies listed in the `requirements.txt` file. These dependencies are required for the proper functioning of the web crawler script.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-crawler/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Firecrawl Python SDK\nDESCRIPTION: This snippet shows how to install the Firecrawl Python SDK using pip.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\npip install firecrawl-py\n```\n\n----------------------------------------\n\nTITLE: Set API Keys and URL Python\nDESCRIPTION: Retrieves API keys for Anthropic and Firecrawl from environment variables using os.getenv(). It then sets the URL to be scraped and prints it to the console. The URL is set to 'https://mendable.ai'.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Retrieve API keys from environment variables\nanthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\nfirecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n\n# Set the URL to scrape\nurl = \"https://mendable.ai\"  # Replace with the actual URL you want to scrape\n\nprint(f\"URL to scrape: {url}\")\n```\n\n----------------------------------------\n\nTITLE: Bash: Push to GitHub Repository\nDESCRIPTION: This bash code snippet pushes the local git repository to a remote GitHub repository. It adds the remote origin, sets the default branch to main, and pushes the local main branch to the remote repository, setting up tracking.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add origin https://github.com/your-username/your-repository.git\ngit branch -M main\ngit push -u origin main\n```\n\n----------------------------------------\n\nTITLE: Mapping URL with Firecrawl\nDESCRIPTION: Demonstrates how to generate a list of URLs from a website using the mapUrl method. This is useful for creating a sitemap of the target domain.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst mapResult = await app.mapUrl('https://example.com') as MapResponse;\nconsole.log(mapResult)\n```\n\n----------------------------------------\n\nTITLE: Install python-dotenv\nDESCRIPTION: Installs the python-dotenv package, enabling automatic loading of environment variables from .env files within Python scripts.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Uploading Crawl Results to Gemini API\nDESCRIPTION: This code snippet uploads the crawled data stored in a text file to the Gemini API. It uses the genai.upload_file function to upload the file and then waits for the file to finish processing before proceeding.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_flash_caching.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Upload the video using the Files API\ntext_file = genai.upload_file(path=\"crawl_result.txt\")\n\n# Wait for the file to finish processing\nwhile text_file.state.name == \"PROCESSING\":\n    print('Waiting for file to be processed.')\n    time.sleep(2)\n    text_file = genai.get_file(text_file.name)\n```\n\n----------------------------------------\n\nTITLE: Listing Python Project Dependencies\nDESCRIPTION: This snippet specifies the required Python packages and their versions for the firecrawl project. These dependencies must be installed to ensure the project runs correctly. Each line indicates a package name and its version number.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-company-researcher/requirements.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython-dotenv==1.0.1\nrequests==2.31.0\nserpapi-python==0.1.5\nopenai==1.12.0\nfirecrawl==0.1.2\n```\n\n----------------------------------------\n\nTITLE: Redis URL Configuration in ConfigMap (YAML)\nDESCRIPTION: This snippet shows how to configure the `REDIS_URL` and `REDIS_RATE_LIMIT_URL` in the ConfigMap when a `REDIS_PASSWORD` is configured in the secret. It is important to replace the placeholders `password`, `host`, and `port` with the actual values.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes/cluster-install/README.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nREDIS_URL: \"redis://:password@host:port\"\nREDIS_RATE_LIMIT_URL: \"redis://:password@host:port\"\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Scraping\nDESCRIPTION: This code snippet imports necessary libraries for the scraper including json for handling JSON data, FirecrawlApp for interacting with the Firecrawl API, dotenv for loading environment variables, Pydantic for defining data models, and datetime for handling dates.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport json\n\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\n\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Setting Firecrawl API Key\nDESCRIPTION: This code snippet shows where to configure the Firecrawl API key within the `ingestion.tsx` component.  It involves replacing the placeholder string with your actual API key to enable communication with the Firecrawl API. This is a client-side implementation and should be moved to the server for security in production.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/ui/ingestion-ui/README.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst FIRECRAWL_API_KEY = \"your-api-key-here\";\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing API Docker Image (Bash)\nDESCRIPTION: This bash script builds a Docker image for the Firecrawl API, tags it, and pushes it to a Docker registry. Replace `ghcr.io/winkk-dev/firecrawl:latest` with your own Docker registry. The `--no-cache` flag ensures a fresh build.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes/cluster-install/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --no-cache --platform linux/amd64 -t ghcr.io/winkk-dev/firecrawl:latest ../../../apps/api\ndocker push ghcr.io/winkk-dev/firecrawl:latest\n```\n\n----------------------------------------\n\nTITLE: Checking Crawl Status (Python)\nDESCRIPTION: This snippet retrieves the status of a crawl job using the `check_crawl_status` method. It takes the job ID as input, retrieves the status and no output is returned.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nid = crawl_result['id']\nstatus = app.check_crawl_status(id)\n```\n\n----------------------------------------\n\nTITLE: Defining Load Test Phases in YAML\nDESCRIPTION: This YAML snippet defines the phases of a load test, specifying the duration and arrival rate for each phase.  It simulates increasing load over time, starting with an initial load of 10 requests per second, increasing to 20 and then 30, before cooling down back to 10 requests per second. These phases are used to simulate user behavior and measure system performance under different load conditions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-1-5/load-test-3.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n- duration: 60\narrivalRate: 10  # Initial load\n- duration: 120\narrivalRate: 20  # Increased load\n- duration: 180\narrivalRate: 30  # Peak load\n- duration: 60\narrivalRate: 10  # Cool down\n```\n\n----------------------------------------\n\nTITLE: Scheduling Scraper with asyncio (Python)\nDESCRIPTION: This snippet demonstrates scheduling a web scraper using the `asyncio` library for concurrent execution. It defines an asynchronous function `schedule_scraper` that runs indefinitely, executing the `save_firecrawl_news_data` function at a specified interval and includes error handling.  The `main` function sets up multiple scraper tasks with different intervals and runs them concurrently using `asyncio.gather`. Requires the `asyncio`, `time`, and `firecrawl_scraper` modules.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport time\nfrom firecrawl_scraper import save_firecrawl_news_data\n\n\nasync def schedule_scraper(interval_hours: float = 1):\n    while True:\n        try:\n            print(f\"Starting scrape at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n            # Run the scraper\n            filename = save_firecrawl_news_data()\n            print(f\"Data saved to {filename}\")\n\n        except Exception as e:\n            print(f\"Error during scraping: {e}\")\n\n        # Wait for the specified interval\n        await asyncio.sleep(interval_hours * 3600)  # Convert hours to seconds\n\n\nasync def main():\n    # Create tasks for different scheduling intervals\n    tasks = [\n        schedule_scraper(interval_hours=1),  # Run every hour\n        # Add more tasks with different intervals if needed\n        # schedule_scraper(interval_hours=0.5),  # Run every 30 minutes\n        # schedule_scraper(interval_hours=2),    # Run every 2 hours\n    ]\n\n    # Run all tasks concurrently\n    await asyncio.gather(*tasks)\n\n\nif __name__ == \"__main__\":\n    # Run the async scheduler\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Load Test Configuration\nDESCRIPTION: This YAML snippet configures a load test with a duration of 60 seconds and an arrival rate of 10 requests per second. It defines the parameters for a basic load test to measure the system's performance under a specific load.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-1-5/load-test-1.md#_snippet_0\n\nLANGUAGE: yml\nCODE:\n```\n# load-test.yml\n- duration: 60\n  arrivalRate: 10  # Initial load\n```\n\n----------------------------------------\n\nTITLE: Run the Web Crawler\nDESCRIPTION: Executes the main Python script for the Llama 4 Maverick web crawler. This command initiates the crawling and analysis process, prompting the user for the website URL and the objective.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-crawler/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython llama4-maverick-web-crawler.py\n```\n\n----------------------------------------\n\nTITLE: Add dependencies to requirements.txt\nDESCRIPTION: Adds the `psutil` and `pytest` packages to the `requirements.txt` file, specifying minimum version requirements. These are the dependencies required by the Python script and tests.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_13\n\nLANGUAGE: Text\nCODE:\n```\npsutil>=5.9.0\npytest>=7.0.0\n```\n\n----------------------------------------\n\nTITLE: Mapping Website URLs (Python)\nDESCRIPTION: This code demonstrates how to generate a list of URLs from a website using the `map_url` method. The result is printed to the console.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Map a website:\nmap_result = app.map_url('https://example.com')\nprint(map_result)\n```\n\n----------------------------------------\n\nTITLE: Initialize git repo using bash\nDESCRIPTION: Initializes a Git repository in the current directory, adds all files to the staging area, and creates an initial commit with the message \"Initial commit\".\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\ngit init \ngit add .\ngit commit -m \"Initial commit\"\n```\n\n----------------------------------------\n\nTITLE: Firecrawl /map Endpoint Usage and Timing\nDESCRIPTION: Demonstrates using the Firecrawl Python SDK to map a URL using the `/map` endpoint and measures the execution time using the `%%time` magic command.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nurl = \"books.toscrape.com\"\n\nmap_response = app.map_url(url=url)\n```\n\n----------------------------------------\n\nTITLE: Fly.io Volume Creation Output\nDESCRIPTION: Example output from creating a Fly.io volume for Redis.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/redis/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nName: redis_server\nRegion: ord\nSize GB: 10\nCreated at: 02 Nov 20 19:55 UTC\n```\n\n----------------------------------------\n\nTITLE: Defining Data Models with Pydantic in Python\nDESCRIPTION: This code defines two Pydantic models: `NewsItem` and `NewsData`. `NewsItem` represents a single news item with fields for title, URL, author, rank, upvotes, and date. `NewsData` contains a list of `NewsItem` objects. These models structure the scraped data and ensure it matches the expected schema.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nclass NewsItem(BaseModel):\n    title: str = Field(description=\"The title of the news item\")\n    source_url: str = Field(description=\"The URL of the news item\")\n    author: str = Field(\n        description=\"The URL of the post author's profile concatenated with the base URL.\"\n    )\n    rank: str = Field(description=\"The rank of the news item\")\n    upvotes: str = Field(description=\"The number of upvotes of the news item\")\n    date: str = Field(description=\"The date of the news item.\")\n\n\nclass NewsData(BaseModel):\n    news_items: List[NewsItem]\n```\n\n----------------------------------------\n\nTITLE: Mapping Response (JSON)\nDESCRIPTION: This JSON snippet shows the expected response from the Firecrawl API after mapping a URL. It contains a status indicator and an array of links found on the website.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://firecrawl.dev\",\n    \"https://www.firecrawl.dev/pricing\",\n    \"https://www.firecrawl.dev/blog\",\n    \"https://www.firecrawl.dev/playground\",\n    \"https://www.firecrawl.dev/smart-crawl\",\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Git Repository - Bash\nDESCRIPTION: These bash commands initialize a Git repository in the project directory, add all files to the staging area, and create an initial commit. This is the first step in setting up a GitHub repository for use with GitHub Actions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n# Initialize git in your project directory\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n```\n\n----------------------------------------\n\nTITLE: Checking Crawl Job Status using Firecrawl API (cURL)\nDESCRIPTION: This snippet shows how to check the status of a previously submitted crawl job using the Firecrawl API. It sends a GET request to the /v1/crawl/{job_id} endpoint, including the API key for authorization.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Set Heroku Environment Variables (Bash)\nDESCRIPTION: This command sets the `FIRECRAWL_API_KEY` environment variable in Heroku using the CLI. This makes the API key available to the scraper when it's deployed on Heroku.  Replace 'your-api-key-here' with the actual API key.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nheroku config:set FIRECRAWL_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Installing Firecrawl Node.js SDK\nDESCRIPTION: This snippet shows how to install the Firecrawl Node.js SDK using npm.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @mendable/firecrawl-js\n```\n\n----------------------------------------\n\nTITLE: Scrape Hacker News Data with Firecrawl\nDESCRIPTION: This code defines a function get_firecrawl_news_data() that initializes a FirecrawlApp instance and uses it to scrape data from Hacker News. It passes the BASE_URL and parameters specifying that data should be extracted according to the NewsData schema.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_firecrawl_news_data():\n    app = FirecrawlApp()\n\n    data = app.scrape_url(\n        BASE_URL,\n        params={\n            \"formats\": [\"extract\"],\n            \"extract\": {\"schema\": NewsData.model_json_schema()},\n        },\n    )\n\n    return data\n```\n\n----------------------------------------\n\nTITLE: Define NewsData Pydantic Model\nDESCRIPTION: This code defines a Pydantic model named NewsData that contains a list of NewsItem objects. This model serves as a container for all the news items scraped from Hacker News.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass NewsData(BaseModel):\n    news_items: List[NewsItem]\n```\n\n----------------------------------------\n\nTITLE: Create runtime.txt (Bash)\nDESCRIPTION: This snippet creates a `runtime.txt` file that specifies the Python version to use on Heroku.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ntouch runtime.txt\necho \"python-3.10.12\" > runtime.txt\n```\n\n----------------------------------------\n\nTITLE: Creating a Cached Gemini Model\nDESCRIPTION: This snippet creates a cached content model using the Google Generative AI API.  It defines the model to use ('models/gemini-1.5-pro-002'), a display name for the cache, system instructions for the model's behavior, the uploaded text file as content, and a time-to-live (TTL) of 15 minutes.  Finally, it constructs a GenerativeModel from the cached content.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_caching.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a cache with a 5 minute TTL\ncache = caching.CachedContent.create(\n    model=\"models/gemini-1.5-pro-002\",\n    display_name=\"website crawl testing again\", # used to identify the cache\n    system_instruction=\"You are an expert at this website, and your job is to answer user's query based on the website you have access to.\",\n    contents=[text_file],\n    ttl=datetime.timedelta(minutes=15),\n)\n# Construct a GenerativeModel which uses the created cache.\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\n```\n\n----------------------------------------\n\nTITLE: Running End-to-End Tests\nDESCRIPTION: Executes end-to-end tests using `cargo`.  This requires setting the environment variables from the `.env` file using `xargs` before running the tests.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\n$ export $(xargs < ./tests/.env)\n$ cargo test --test e2e_with_auth\n```\n\n----------------------------------------\n\nTITLE: Defining a Pydantic Model for Product Data\nDESCRIPTION: This defines a Pydantic model named 'Product' to represent the structure of the data to be scraped from each Product Hunt product. It uses type hints and Field descriptions for clear extraction.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass Product(BaseModel):\n    name: str = Field(description=\"The name of the product\")\n    description: str = Field(description=\"A short description of the product\")\n    url: str = Field(description=\"The URL of the product\")\n    \n    topics: list[str] = Field(\n        description=\"A list of topics the product belongs to. Can be found below the product description.\"\n    )\n    \n    n_upvotes: int = Field(description=\"The number of upvotes the product has\")\n    n_comments: int = Field(description=\"The number of comments the product has\")\n    \n    rank: int = Field(\n        description=\"The rank of the product on Product Hunt's Yesterday's Top Products section.\"\n    )\n    logo_url: str = Field(description=\"The URL of the product's logo.\")\n```\n\n----------------------------------------\n\nTITLE: Publish Python Package to PyPI with GitHub Actions\nDESCRIPTION: This workflow publishes a new version of a Python package to PyPI when a new GitHub release is created. It checks out the code, sets up Python, installs dependencies, builds the package, and uploads it to PyPI using a stored API token. The package requires a `setup.py` or `pyproject.toml` file, and a PyPI API token stored as a repository secret.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_38\n\nLANGUAGE: yaml\nCODE:\n```\nname: Publish Python Package\non:\n  release:\n    types: [created]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          \n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install build twine\n          \n      - name: Build package\n        run: python -m build\n        \n      - name: Publish to PyPI\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n        run: |\n          python -m twine upload dist/*\n```\n\n----------------------------------------\n\nTITLE: Cron Scraper Script (Python)\nDESCRIPTION: This script is designed to be executed by cron. It imports necessary modules, configures logging to a file and console, defines a `main` function to run the `save_firecrawl_news_data` scraping function with error handling, and executes the `main` function when the script is run directly. Requires `sys`, `logging`, `datetime`, `pathlib`, and `firecrawl_scraper` modules.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# cron_scraper.py\nimport sys\nimport logging\nfrom datetime import datetime\nfrom pathlib import Path\nfrom firecrawl_scraper import save_firecrawl_news_data\n\n# Set up logging\nlog_dir = Path(\"logs\")\nlog_dir.mkdir(exist_ok=True)\nlog_file = log_dir / f\"scraper_{datetime.now().strftime('%Y_%m')}.log\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)],\n)\n\n\ndef main():\n    try:\n        logging.info(\"Starting scraping job\")\n        filename = save_firecrawl_news_data()  # Actual scraping function\n        logging.info(f\"Successfully saved data to {filename}\")\n    except Exception as e:\n        logging.error(f\"Scraping failed: {str(e)}\", exc_info=True)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with Firecrawl Python SDK\nDESCRIPTION: This snippet shows how to extract structured data from a URL using the Firecrawl Python SDK with Pydantic schemas.  It defines a schema for the article data and then uses the scrape_url method with the schema to extract the data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass ArticleSchema(BaseModel):\n    title: str\n    points: int \n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., description=\"Top 5 stories\")\n\njson_config = JsonConfig(schema=TopArticlesSchema.model_json_schema())\n\nllm_extraction_result = app.scrape_url('https://news.ycombinator.com', formats=[\"json\"], json=json_config)\n\nprint(llm_extraction_result.json)\n```\n\n----------------------------------------\n\nTITLE: Define NewsItem Pydantic Model\nDESCRIPTION: This code defines a Pydantic model named NewsItem, which specifies the structure of the data to be scraped from each Hacker News post. It includes fields for title, source_url, author, rank, upvotes, and date, along with descriptions for each field.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass NewsItem(BaseModel):\n    title: str = Field(description=\"The title of the news item\")\n    source_url: str = Field(description=\"The URL of the news item\")\n    author: str = Field(\n        description=\"The URL of the post author's profile concatenated with the base URL.\"\n    )\n    rank: str = Field(description=\"The rank of the news item\")\n    upvotes: str = Field(description=\"The number of upvotes of the news item\")\n    date: str = Field(description=\"The date of the news item.\")\n```\n\n----------------------------------------\n\nTITLE: Run the Main Script\nDESCRIPTION: This command executes the `main.py` script, which starts the interactive demo of the Swarm Firecrawl Marketing Agent. Ensure that all dependencies are installed and environment variables are set before running this command.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/openai_swarm_firecrawl/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Add main.py file using bash\nDESCRIPTION: Adds `main.py` file with the metric collection script to Git and creates a commit with a descriptive message.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_19\n\nLANGUAGE: Bash\nCODE:\n```\ngit add .\ngit commit -m \"Add the main.py functionality\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Git Repository\nDESCRIPTION: This command initializes a Git repository for the scraper project. It also creates a .gitignore file and removes the .env file from tracking.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ngit init\ntouch .gitignore\necho \".env\" >> .gitignore  # Remove the .env file from Git indexing\ngit add .\ngit commit -m \"Initial commit\"\n```\n\n----------------------------------------\n\nTITLE: Installing Firecrawl Python SDK with pip\nDESCRIPTION: This command installs the firecrawl-py package using pip, allowing you to use the Firecrawl Python SDK in your projects.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install firecrawl-py\n```\n\n----------------------------------------\n\nTITLE: Run the Airbnb scraping and analysis script\nDESCRIPTION: This command executes the main script of the project, which is responsible for scraping Airbnb data using Firecrawl and analyzing it using E2B's Code Interpreter SDK. It assumes that the necessary environment variables and dependencies have been set up.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/scrape_and_analyze_airbnb_data_e2b/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nnpm run start\n```\n\n----------------------------------------\n\nTITLE: Import Libraries Python\nDESCRIPTION: Imports necessary libraries including os, json, FirecrawlApp, Anthropic, and load_dotenv. It also loads environment variables from a .env file using load_dotenv().\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nfrom firecrawl import FirecrawlApp\nfrom anthropic import Anthropic\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Saving Product Data to Local Storage (Python)\nDESCRIPTION: This code snippet demonstrates how to save scraped product data to a local JSON file. It creates a filename based on the current date and saves the product data as a JSON file in the `data` directory. Requires the `json` and `os` modules. The function also ensures the data directory exists.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef save_yesterday_top_products():\n    \"\"\"\n    Change back to JSON-based storage.\n    \"\"\"\n    products = get_yesterday_top_products()\n    \n    # Local storage (works on PythonAnywhere)\n    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n    filename = f\"data/ph_top_products_{date_str}.json\"\n    \n    # Create data directory if it doesn't exist\n    os.makedirs(\"data\", exist_ok=True)\n    \n    with open(filename, \"w\") as f:\n        json.dump(products, f)\n```\n\n----------------------------------------\n\nTITLE: Python: Test System Metrics Collection\nDESCRIPTION: This code snippet tests the `get_system_metrics()` function to ensure it returns data in the expected format and with valid ranges. It uses `assert` statements to check for CPU and memory percentages within 0-100, the existence of active processes, and the inclusion of a timestamp.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef test_get_system_metrics():\n    \"\"\"Test if system metrics are collected correctly\"\"\"\n    metrics = get_system_metrics()\n    \n    # Check if all required metrics exist and are valid\n    assert 0 <= metrics['cpu_percent'] <= 100\n    assert 0 <= metrics['memory_percent'] <= 100\n    assert metrics['active_processes'] > 0\n    assert 'timestamp' in metrics\n```\n\n----------------------------------------\n\nTITLE: Deploy to Heroku (Bash)\nDESCRIPTION: These commands add all local changes, commits them with a message, and pushes them to the Heroku remote, deploying the application.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\ngit commit -m \"Add Heroku-related files\"\ngit push heroku main\n```\n\n----------------------------------------\n\nTITLE: Basic URL Scraping with Firecrawl\nDESCRIPTION: This snippet shows how to scrape a single URL using the scrapeUrl method of the FirecrawlApp. It initializes the app and then calls scrapeUrl to retrieve the content of the provided URL.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst url = \"https://example.com\";\nconst scrapedData = await app.scrapeUrl(url);\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Testing (Bash)\nDESCRIPTION: This command forwards port 3002 on the local machine to port 3002 on the `api` service in the `dev` namespace. This is useful for testing the Firecrawl API locally.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes/cluster-install/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward svc/api 3002:3002 -n dev\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies\nDESCRIPTION: This command installs the necessary Python packages listed in the `requirements.txt` file. It ensures that all required libraries are present for the script to function correctly.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-company-researcher/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Firecrawl Dependencies\nDESCRIPTION: This snippet defines the Python package dependencies required for the Firecrawl project. These packages are necessary for various functionalities, including interacting with Google Cloud AI Platform, using generative AI models, managing environment variables, making HTTP requests, and utilizing the Firecrawl library. Each package is specified with a minimum version requirement.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-crawler/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ngoogle-cloud-aiplatform>=1.36.0\ngoogle-generativeai>=0.3.2\npython-dotenv>=1.0.0\nrequests>=2.31.0\nfirecrawl>=0.1.0\n```\n\n----------------------------------------\n\nTITLE: Firecrawl /crawl Endpoint Usage and Timing\nDESCRIPTION: Demonstrates using the Firecrawl Python SDK to crawl a URL using the `/crawl` endpoint and measures the execution time using the `%%time` magic command.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nurl = \"books.toscrape.com\"\n\ncrawl_response = app.crawl_url(url=url, params={\"scrapeOptions\": {\"formats\": [\"links\"]}})\n```\n\n----------------------------------------\n\nTITLE: Batch Scraping with WebSockets\nDESCRIPTION: Shows how to batch scrape multiple URLs using WebSockets to receive real-time updates. It configures event listeners for document, error, and done events to process the crawl data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/js-sdk/firecrawl/README.md#_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Batch scrape multiple URLs with WebSockets:\nconst watch = await app.batchScrapeUrlsAndWatch(['https://firecrawl.dev', 'https://mendable.ai'], { formats: ['markdown', 'html'] });\n\nwatch.addEventListener(\"document\", doc => {\n console.log(\"DOC\", doc.detail);\n});\n\nwatch.addEventListener(\"error\", err => {\n console.error(\"ERR\", err.detail.error);\n});\n\nwatch.addEventListener(\"done\", state => {\n console.log(\"DONE\", state.detail.status);\n});\n```\n\n----------------------------------------\n\nTITLE: Install Playwright Browsers\nDESCRIPTION: This command installs the browsers required by Playwright for running end-to-end tests. Playwright is a testing framework that automates browser interactions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx playwright install\n```\n\n----------------------------------------\n\nTITLE: Import Libraries\nDESCRIPTION: This code imports necessary libraries including json, FirecrawlApp from firecrawl, load_dotenv from dotenv, BaseModel and Field from pydantic, List from typing and datetime from datetime. The load_dotenv() function is called to load environment variables from a .env file.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom firecrawl import FirecrawlApp\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom datetime import datetime\n\nload_dotenv()\nBASE_URL = \"https://news.ycombinator.com/\"\n```\n\n----------------------------------------\n\nTITLE: Running the Application\nDESCRIPTION: These commands build and start the Playwright web scraping application, allowing it to be accessed and used for scraping web pages. The `npm run dev` command is used for development.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/playwright-service-ts/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\nnpm start\n```\n\n----------------------------------------\n\nTITLE: Run Scraper from Command Line\nDESCRIPTION: This code adds a `__main__` block to the firecrawl_scraper.py script to allow running the scraper directly from the command line. It calls the `save_firecrawl_news_data` function to save the scraped data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    save_firecrawl_news_data()\n```\n\n----------------------------------------\n\nTITLE: Commit Changes (Python)\nDESCRIPTION: This snippet adds all changes to the git repository and commits them with a descriptive message. This is required to make the changes, including the workflow file, visible to GitHub Actions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ngit add .\ngit commit -m \"Descriptive commit message\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installs the required Python packages listed in the `requirements.txt` file using pip. This ensures that all necessary libraries are available for the script to run correctly. It requires Python and pip to be installed and configured.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-extractor/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Testing the Crawl API Endpoint\nDESCRIPTION: This `curl` command tests the `/v1/crawl` endpoint of the Firecrawl API. It sends a POST request with a JSON payload specifying the URL to crawl (https://firecrawl.dev). It requires `curl` to be installed. The API key is not required for self-hosted instances.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:3002/v1/crawl \\\n      -H 'Content-Type: application/json' \\\n      -d '{\n        \"url\": \"https://firecrawl.dev\"\n      }'\n```\n\n----------------------------------------\n\nTITLE: Defining a Pydantic Model for List of Products\nDESCRIPTION: This code defines a Pydantic model named 'YesterdayTopProducts' which contains a list of 'Product' objects. This is used to define the schema for scraping a collection of products from ProductHunt.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass YesterdayTopProducts(BaseModel):\n    products: list[Product] = Field(\n        description=\"A list of top products from yesterday on Product Hunt.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Uploading Crawl Results to Gemini\nDESCRIPTION: This snippet uploads the 'crawl_result.txt' file to the Gemini API using the `genai.upload_file` function. It then waits for the file to finish processing by repeatedly checking its state, introducing a 2-second delay between checks.  Once processing is complete, the file object is retrieved.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/website_qa_with_gemini_caching/website_qa_with_gemini_caching.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Upload the video using the Files API\ntext_file = genai.upload_file(path=\"crawl_result.txt\")\n\n# Wait for the file to finish processing\nwhile text_file.state.name == \"PROCESSING\":\n    print('Waiting for file to be processed.')\n    time.sleep(2)\n    text_file = genai.get_file(text_file.name)\n```\n\n----------------------------------------\n\nTITLE: Actions in Python Tests Workflow YAML\nDESCRIPTION: This workflow illustrates the use of actions, which are reusable units of code. It includes actions for checking out the repository (`actions/checkout@v3`) and setting up the Python environment (`actions/setup-python@v4`).  The `python-version` is an input parameter.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# test.yaml\nname: Python Tests\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n```\n\n----------------------------------------\n\nTITLE: Searching the Web with Firecrawl API (cURL)\nDESCRIPTION: This snippet shows how to search the web using the Firecrawl API with cURL. It sends a POST request to the /v1/search endpoint with a query and limit.  It requires a valid API key in the Authorization header. The output is a JSON object containing search results.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5\n  }'\n```\n\n----------------------------------------\n\nTITLE: Stripe Documentation Mapping with Search Parameter\nDESCRIPTION: Maps the Stripe documentation website using the `map_url()` method with the `search` parameter to filter URLs containing the term 'tax'.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://docs.stripe.com\"\n\nresponse = app.map_url(url=url, params={\"search\": \"tax\"})\n```\n\n----------------------------------------\n\nTITLE: Bumping Version Script\nDESCRIPTION: Script used to create a release with version bump.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/redis/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nscripts/bump_version.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Firecrawl Rust SDK\nDESCRIPTION: Adds the Firecrawl and Tokio dependencies to your `Cargo.toml` file. Tokio is used for asynchronous operations and requires the \"full\" feature set.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n[dependencies]\nfirecrawl = \"^0.1\"\ntokio = { version = \"^1\", features = [\"full\"] }\n```\n\n----------------------------------------\n\nTITLE: Import Libraries for XML Sitemap Creation in Python\nDESCRIPTION: This code snippet imports the necessary Python libraries for creating an XML sitemap. It imports `datetime` for adding timestamps, `xml.etree.ElementTree` for constructing the XML structure, and `urllib.parse` for parsing and validating URLs.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nimport xml.etree.ElementTree as ET\nfrom urllib.parse import urlparse\n```\n\n----------------------------------------\n\nTITLE: Access Links from Response\nDESCRIPTION: Accesses and displays the first 10 links from the 'links' array within the response received from the Firecrawl API.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse[\"links\"][:10]\n```\n\n----------------------------------------\n\nTITLE: Jobs Definition YAML\nDESCRIPTION: This example shows how to define two jobs, `test` and `deploy`, each running on a different runner (`ubuntu-latest` and `macos-latest` respectively). Each job can contain multiple steps and access shared workflow data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\njobs:\n    test:\n        runs-on: ubuntu-latest\n        ...\n    deploy:\n        runs-on: macos-latest\n        ...\n```\n\n----------------------------------------\n\nTITLE: Pushing Code to GitHub\nDESCRIPTION: This command configures the remote origin for the local Git repository and pushes the code to GitHub. Replace your-repo-link with the actual GitHub repository link.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\ngit remote add origin your-repo-link\ngit push\n```\n\n----------------------------------------\n\nTITLE: Bash: Create and Ignore .env File\nDESCRIPTION: These bash commands create a `.env` file, add example environment variables, and then add `.env` to `.gitignore` to prevent it from being committed to the repository, thereby protecting sensitive data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\ntouch .env\necho \"API_KEY='your-api-key-here'\" >> .env\necho \"DATABASE_URL='postgresql://user:pass@localhost:5432/db'\" >> .env\n```\n\nLANGUAGE: bash\nCODE:\n```\necho \".env\" >> .gitignore\n```\n\n----------------------------------------\n\nTITLE: Stripe Documentation Mapping with Advanced Parameters\nDESCRIPTION: Maps the Stripe documentation website using the `map_url()` method with parameters such as 'search', 'sitemapOnly', and 'includeSubdomains' to customize the crawling behavior.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://docs.stripe.com\"\n\nresponse = app.map_url(url=url, params={\"search\": \"tax\", \"sitemapOnly\": True, \"includeSubdomains\": True})\nlen(response['links'])\n```\n\n----------------------------------------\n\nTITLE: Creating a Fly.io Volume for Redis Persistence\nDESCRIPTION: Creates a persistent volume for Redis data using the Fly.io CLI. This prevents data loss across deploys or restarts. The volume should be in the same region as the app instances.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/redis/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nflyctl volumes create redis_server --region ord\n```\n\n----------------------------------------\n\nTITLE: Dependency Declaration\nDESCRIPTION: This snippet defines the dependencies required for the Firecrawl project. It includes `firecrawl`, `openai`, and `python-dotenv` along with their minimum versions. These dependencies need to be installed using pip or a similar package manager before running the project.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/o3-web-crawler/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfirecrawl>=0.1.0\nopenai>=1.0.0\npython-dotenv>=0.19.0\n```\n\n----------------------------------------\n\nTITLE: Create .env File\nDESCRIPTION: Creates a `.env` file and adds the `FIRECRAWL_API_KEY` to it. Replace `'your-key-here'` with your actual API key.  This securely stores the API key and prevents it from being committed to version control.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntouch .env\necho \"FIRECRAWL_API_KEY='your-key-here'\" >> .env\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository\nDESCRIPTION: This command clones the repository from the provided URL and changes the directory to the project folder. It's the first step in setting up the Apartment Finder CLI.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deep-research-apartment-finder/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <repository-url>\ncd apartment-finder\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pnpm\nDESCRIPTION: This snippet demonstrates how to install project dependencies using pnpm. It assumes you are in the `apps/api/` directory and have pnpm version 9+ installed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# cd apps/api # to make sure you're in the right folder\npnpm install # make sure you have pnpm version 9+!\n```\n\n----------------------------------------\n\nTITLE: Scaling Heroku Worker Dynos (Bash)\nDESCRIPTION: These commands scale the Heroku worker dynos to stop and start the worker process. Scaling to 0 stops the worker, while scaling to 1 starts it.  Requires the Heroku CLI and a configured Heroku application.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nheroku ps:scale worker=0\n```\n\nLANGUAGE: bash\nCODE:\n```\nheroku ps:scale worker=1\n```\n\n----------------------------------------\n\nTITLE: Deleting a Heroku Application (Bash)\nDESCRIPTION: This command permanently deletes a Heroku application. Requires the Heroku CLI, and a configured Heroku application. Use with caution as this action is irreversible.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nheroku apps:destroy --app your-app-name --confirm your-app-name\n```\n\n----------------------------------------\n\nTITLE: Committing Workflow Changes - Bash\nDESCRIPTION: These bash commands add the modified files (including the new workflow file) to the staging area, commit the changes with a descriptive message, and push the commit to the remote GitHub repository. This makes the workflow active in the GitHub Actions tab.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\ngit commit -m \"Add a workflow to scrape on a schedule\"\n```\n\n----------------------------------------\n\nTITLE: Extract API Response (Data)\nDESCRIPTION: This JSON snippet represents the data format response from the Firecrawl extract API, when the data extraction is complete.  It shows the extracted company mission, open-source status, and Y Combinator status.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": true,\n  \"data\": {\n    \"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n    \"supports_sso\": false,\n    \"is_open_source\": true,\n    \"is_in_yc\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Set Environment Variables\nDESCRIPTION: This configuration sets the OpenAI and Firecrawl API keys as environment variables. Replace `your_openai_api_key` and `your_firecrawl_api_key` with your actual API keys. These API keys are necessary for accessing the OpenAI and Firecrawl services.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/openai_swarm_firecrawl/README.md#_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=your_openai_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\n```\n\n----------------------------------------\n\nTITLE: Listing Python package dependencies\nDESCRIPTION: This snippet lists the Python package dependencies required for the firecrawl project, specifying each package's name and version.  These packages need to be installed using pip or a similar package manager.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-crawler/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nfirecrawl==1.13.5\npython-dotenv==1.0.1\nhuggingface-hub>=0.20.0\n```\n\n----------------------------------------\n\nTITLE: Copy Environment File\nDESCRIPTION: Copies the `.env.example` file to `.env`. This allows the user to store sensitive information like API keys locally and securely. The user needs to fill in the API keys after copying.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/o4-mini-web-crawler/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Building go-html-to-md library using go build\nDESCRIPTION: This snippet compiles the go-html-to-markdown.go file into a shared object (.so) file named html-to-markdown.so using the go build command with the c-shared build mode. It also sets execute permissions on the generated shared object file.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/api/sharedLibs/go-html-to-md/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd apps/api/src/lib/go-html-to-md\ngo build -o html-to-markdown.so -buildmode=c-shared html-to-markdown.go\nchmod +x html-to-markdown.so\n```\n\n----------------------------------------\n\nTITLE: Artillery Load Test Configuration YAML\nDESCRIPTION: This YAML snippet configures the load test phases using Artillery. It defines four phases with different arrival rates to simulate varying load conditions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-1-5/load-test-4.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- duration: 60\narrivalRate: 10  # Initial load\n- duration: 120\narrivalRate: 20  # Increased load\n- duration: 180\narrivalRate: 30  # Peak load\n- duration: 60\narrivalRate: 10  # Cool down\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs the necessary npm packages required to run the Firecrawl UI Template. It fetches and installs all dependencies defined in the `package.json` file.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/ui/ingestion-ui/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Install Firecrawl Python SDK\nDESCRIPTION: Installs the Firecrawl Python SDK using pip, allowing interaction with the Firecrawl API from Python scripts.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install firecrawl-py\n```\n\n----------------------------------------\n\nTITLE: Adding Remote Repository - Bash\nDESCRIPTION: These commands add a remote origin to the local Git repository, rename the main branch to 'main', and push the local commits to the remote repository. This associates the local project with a GitHub repository, enabling the use of GitHub Actions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\n# Create a new repo on GitHub.com, then:\ngit remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\ngit branch -M main\ngit push -u origin main\n```\n\n----------------------------------------\n\nTITLE: Deleting Firecrawl from Kubernetes (Bash)\nDESCRIPTION: This script deletes the Kubernetes manifests used to install Firecrawl, effectively removing the deployment from the cluster. Ensure that the manifests are located in the current directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes/cluster-install/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -f configmap.yaml\nkubectl delete -f secret.yaml\nkubectl delete -f playwright-service.yaml\nkubectl delete -f api.yaml\nkubectl delete -f worker.yaml\nkubectl delete -f redis.yaml\n```\n\n----------------------------------------\n\nTITLE: Python: Test Saving and Reading Metrics\nDESCRIPTION: This code tests the `save_metrics()` function by saving metrics to a file and then reading them back to verify the file's existence and the integrity of the saved data. It retrieves system metrics, saves them using `save_metrics()`, and then checks if the file exists and contains data in the expected format.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef test_save_and_read_metrics():\n    \"\"\"Test if metrics are saved and can be read back\"\"\"\n    # Get and save metrics\n    metrics = get_system_metrics()\n    save_metrics(metrics)\n    \n    # Check if file exists and contains data\n    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n    file_path = Path(\"system_metrics\") / f\"metrics_{date_str}.json\"\n    \n    assert file_path.exists()\n    with open(file_path) as f:\n        saved_data = json.load(f)\n    \n    assert isinstance(saved_data, list)\n    assert len(saved_data) > 0\n```\n\n----------------------------------------\n\nTITLE: Running Scrape Scheduler (Bash)\nDESCRIPTION: This snippet shows how to execute the Python script `scrape_scheduler.py` using the python interpreter.  It provides a command to start the scheduler and suggests debugging with a shorter interval before implementing the hourly schedule.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\npython scrape_scheduler.py\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Crawling Website (Python)\nDESCRIPTION: This code demonstrates how to asynchronously crawl a website using the `async_crawl_url` method. It excludes specified paths and prints the crawl result. An empty string is passed as the `idempotency_key`.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncrawl_result = app.async_crawl_url('firecrawl.dev', {'excludePaths': ['blog/*']}, \"\")\nprint(crawl_result)\n```\n\n----------------------------------------\n\nTITLE: Creating Workflow Directory - Bash\nDESCRIPTION: This command creates the directory structure required for storing GitHub Actions workflow files.  The `.github/workflows` directory is where YAML files defining the automation workflows reside.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p .github/workflows\n```\n\n----------------------------------------\n\nTITLE: Counting Links from the response\nDESCRIPTION: Counts the total number of links found in the response from the Firecrawl API when mapping Stripe documentation.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlen(response[\"links\"])\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: These commands install the necessary dependencies for the Playwright web scraping service, including Node.js modules and Playwright browsers.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/playwright-service-ts/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpx playwright install\n```\n\n----------------------------------------\n\nTITLE: Create Heroku App and Set Remote (Bash)\nDESCRIPTION: These commands create a new Heroku application and sets it as a remote repository for the project, enabling deployment to Heroku.  The app name needs to be unique.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nheroku create ph-scraper-your-name  # Make the app name unique\nheroku git:remote -a ph-scraper-your-name\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging (Python)\nDESCRIPTION: This code configures logging with both file and console handlers. It sets up a logger that writes detailed logs to a dated file and simplified logs to the console. It's essential for debugging, monitoring, and maintaining web scrapers. Requires the `logging` and `datetime` modules.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom datetime import datetime\n\ndef setup_logging():\n    \"\"\"Configure logging with both file and console handlers.\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    # Create formatters\n    detailed_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    simple_formatter = logging.Formatter('%(levelname)s: %(message)s')\n\n    # File handler\n    file_handler = logging.FileHandler(\n        f'logs/scraper_{datetime.now().strftime(\"%Y%m%d\")}.log'\n    )\n    file_handler.setFormatter(detailed_formatter)\n    \n    # Console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(simple_formatter)\n\n    # Add handlers\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n\n    return logger\n\nlogger = setup_logging()\n```\n\n----------------------------------------\n\nTITLE: Load Test Configuration in YAML\nDESCRIPTION: This YAML snippet defines the configuration for a load test, specifying the duration and arrival rate of requests. The duration is set to 10 seconds, and the arrival rate is set to 10 requests per second.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-6-7/load-test-6.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n# load-test.yml\n  - duration: 10\n    arrivalRate: 10\n```\n\n----------------------------------------\n\nTITLE: Installing Firecrawl and Anthropic Dependencies\nDESCRIPTION: This command installs the necessary Python packages, including the Firecrawl library (firecrawl-py) and the Anthropic library for accessing the Claude model.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/contradiction_testing/web-data-contradiction-testing-using-llms.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install firecrawl-py anthropic\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Changes from Git (Bash)\nDESCRIPTION: This command navigates to the project directory and pulls the latest changes from the `origin main` branch.  Requires Git to be initialized and configured within the project directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\ncd your-repo\ngit pull origin main\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Playwright Docker Image (Bash)\nDESCRIPTION: This bash script builds a Docker image for the Firecrawl Playwright service, tags it, and pushes it to a Docker registry. Replace `ghcr.io/winkk-dev/firecrawl-playwright:latest` with your own Docker registry. The `--no-cache` flag ensures a fresh build.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes/cluster-install/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --no-cache --platform linux/amd64 -t ghcr.io/winkk-dev/firecrawl-playwright:latest ../../../apps/playwright-service\ndocker push ghcr.io/winkk-dev/firecrawl-playwright:latest\n```\n\n----------------------------------------\n\nTITLE: Scrape Scheduler Imports\nDESCRIPTION: This code imports necessary modules for scheduling the scraper, including `schedule`, `time`, and the `save_firecrawl_news_data` function from the `firecrawl_scraper.py` script.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport schedule\nimport time\nfrom firecrawl_scraper import save_firecrawl_news_data\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository\nDESCRIPTION: Clones the project repository to your local machine using Git. This command downloads the entire project structure and files, allowing you to work with the code locally. It requires Git to be installed and configured on your system.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-extractor/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <your-repo-url>\ncd <your-repo-name>\n```\n\n----------------------------------------\n\nTITLE: Search API Response Example\nDESCRIPTION: This JSON snippet represents a typical response from the Firecrawl search API, containing a success status and an array of search results with URLs, titles, and descriptions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"url\": \"https://firecrawl.dev\",\n      \"title\": \"Firecrawl | Home Page\",\n      \"description\": \"Turn websites into LLM-ready data with Firecrawl\"\n    },\n    {\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"title\": \"Documentation | Firecrawl\",\n      \"description\": \"Learn how to use Firecrawl in your own applications\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping Response (JSON)\nDESCRIPTION: This JSON snippet shows the expected response from the Firecrawl API after scraping a URL. It contains a success indicator and a data object containing the extracted markdown, HTML, and metadata from the URL.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": true,\n  \"data\": {\n    \"markdown\": \"Launch Week I is here! [See our Day 2 Release 🚀](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[💥 Get 2 months free...\",\n    \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"><body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\n    \"metadata\": {\n      \"title\": \"Home - Firecrawl\",\n      \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n      \"language\": \"en\",\n      \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Firecrawl\",\n      \"ogDescription\": \"Turn any website into LLM-ready data.\",\n      \"ogUrl\": \"https://www.firecrawl.dev/\",\n      \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Firecrawl\",\n      \"sourceURL\": \"https://firecrawl.dev\",\n      \"statusCode\": 200\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This command installs the required Python packages from the `requirements.txt` file. This ensures that all necessary libraries for the Apartment Finder CLI are available.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deep-research-apartment-finder/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Enabling/Disabling Heroku Maintenance Mode (Bash)\nDESCRIPTION: These commands enable and disable maintenance mode for a Heroku application.  Maintenance mode prevents access to the application. Requires the Heroku CLI and a configured Heroku application.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nheroku maintenance:on\n```\n\nLANGUAGE: bash\nCODE:\n```\nheroku maintenance:off\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory and Installing Dependencies in Bash\nDESCRIPTION: This script creates a project directory, navigates into it, and installs necessary dependencies using pip.  The dependencies are `firecrawl-py`, `pydantic`, and `python-dotenv`. Also, an API key is added to the .env file.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n# Create project directory and install dependencies\nmkdir hacker-news-scraper && cd hacker-news-scraper\npip install firecrawl-py pydantic python-dotenv\n\n# Create necessary files\ntouch requirements.txt scraper.py .env\n\n# Add dependencies to requirements.txt\necho \"firecrawl-py\\npydantic\\npython-dotenv\" > requirements.txt\n\n# Add Firecrawl API key to .env (get your key at firecrawl.dev/signin/signup)\necho \"FIRECRAWL_API_KEY='your_api_key_here'\" > .env\n```\n\n----------------------------------------\n\nTITLE: Heroku CLI Installation (MacOS)\nDESCRIPTION: These commands install the Heroku CLI on MacOS using brew.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nbrew install heroku/brew/heroku  # macOS\n```\n\n----------------------------------------\n\nTITLE: Crawling Endpoint Test with curl\nDESCRIPTION: This snippet demonstrates how to test the crawl endpoint by sending a POST request with a JSON payload containing a URL to crawl. It specifies the content type as application/json.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:3002/v1/crawl \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n      \"url\": \"https://mendable.ai\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrency Limits in TOML\nDESCRIPTION: This snippet configures the concurrency settings for an HTTP service in a Fly.io application. It defines the concurrency type as 'requests', sets a hard limit of 100 concurrent requests, and a soft limit of 75 concurrent requests. The soft limit triggers autoscaling.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-1-5/load-test-3.md#_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\n[http_service.concurrency]\n  type = \"requests\"\n  hard_limit = 100\n  soft_limit = 75\n```\n\n----------------------------------------\n\nTITLE: Bash: Commit Test Changes\nDESCRIPTION: This bash code snippet commits the changes made in the testing phase.  It adds all changed files to the staging area, then commits them with the message \"Write tests for main.py\".\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ngit add .\ngit commit -m \"Write tests for main.py\"\n```\n\n----------------------------------------\n\nTITLE: Load Test Configuration Phases\nDESCRIPTION: This YAML snippet defines the phases of the load test, specifying the duration and arrival rate (requests per second) for each phase. The test includes phases for initial load, increased load, peak load, and cool down.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-6-7/load-test-8.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nphases:\n    - duration: 60\n      arrivalRate: 1  # Initial load\n    - duration: 120\n      arrivalRate: 2  # Increased load\n    - duration: 180\n      arrivalRate: 3  # Peak load\n    - duration: 60\n      arrivalRate: 1  # Cool down\n```\n\n----------------------------------------\n\nTITLE: Listing Project Dependencies\nDESCRIPTION: This snippet lists the Python dependencies required for the Firecrawl project, specifying the package name and the minimum acceptable version for each.  These dependencies must be installed for the project to run correctly. It includes libraries for environment variable management, making HTTP requests, interacting with OpenAI's API, and performing Google searches.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/deepseek-v3-company-researcher/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npython-dotenv>=1.0.0\nrequests>=2.31.0\nopenai>=1.12.0\ngoogle-search-results>=2.4.2\nserpapi>=0.1.5\n```\n\n----------------------------------------\n\nTITLE: Creating Workflow Directories and File (Bash)\nDESCRIPTION: This snippet creates the necessary directory structure for GitHub Actions workflow files and creates a new YAML file for the Product Hunt scraper.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p .github/workflows\ntouch .github/workflows/ph-scraper.yml\n```\n\n----------------------------------------\n\nTITLE: Crawl Job Response (JSON)\nDESCRIPTION: This JSON snippet shows the expected response from the Firecrawl API after submitting a crawl job. It includes a success indicator, a unique job ID, and the URL to check the status of the crawl job.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": true,\n  \"id\": \"123-456-789\",\n  \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n```\n\n----------------------------------------\n\nTITLE: System Monitoring Workflow Job Definition (YAML)\nDESCRIPTION: Defines the `run_script` job within the System Monitoring workflow, specifying that it should run on the latest version of Ubuntu. This is a single job workflow.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_16\n\nLANGUAGE: YAML\nCODE:\n```\njobs:\n  run_script:\n    runs-on: ubuntu-latest\n```\n\n----------------------------------------\n\nTITLE: Installing Pytest (Bash)\nDESCRIPTION: This command installs the pytest testing framework using pip, which is required to run the end-to-end tests for the Firecrawl Python SDK.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install pytest\n```\n\n----------------------------------------\n\nTITLE: Dependency Versions\nDESCRIPTION: Specifies the versions of required python packages for the firecrawl project. This ensures that the correct versions of libraries are used during development and deployment, preventing compatibility issues.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-web-crawler/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfirecrawl==0.11.0\nopenai==1.14.0\npython-dotenv==1.0.0\n```\n\n----------------------------------------\n\nTITLE: Simple Scheduling Example\nDESCRIPTION: This code demonstrates a simple example of using the Python `schedule` library to schedule a function to run every 3 seconds. It defines a job() function that prints the current time and then schedules it to run using schedule.every(3).seconds.do(job).\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport schedule\nimport time\n\ndef job():\n    current_time = time.strftime(\"%H:%M:%S\")\n    print(f\"{current_time}: I'm working...\")\n\n# Schedule it\nschedule.every(3).seconds.do(job)\n\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Cloning the Repository\nDESCRIPTION: This command clones the GPT-4.1 web crawler repository from GitHub to your local machine. It requires Git to be installed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-web-crawler/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/yourusername/gpt-4.1-web-crawler.git\ncd gpt-4.1-web-crawler\n```\n\n----------------------------------------\n\nTITLE: View first news item\nDESCRIPTION: This code calls the function get_firecrawl_news_data and prints the first news item in the extract field of the data returned by the function.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata['extract']['news_items'][0]\n```\n\n----------------------------------------\n\nTITLE: Counting Links Found by /map\nDESCRIPTION: Counts the number of links returned by the /map endpoint, used for comparison against the number of links returned by /crawl.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlen(map_response[\"links\"])\n```\n\n----------------------------------------\n\nTITLE: Heroku Login (Bash)\nDESCRIPTION: This command logs the user into their Heroku account via the Heroku CLI, opening a web browser for authentication.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nheroku login  # Opens your web browser\n```\n\n----------------------------------------\n\nTITLE: Running Pytest Tests (Bash)\nDESCRIPTION: This command executes the pytest tests located in the specified directory, allowing you to verify the functionality of the Firecrawl Python SDK.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/python-sdk/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npytest firecrawl/__tests__/e2e_withAuth/test.py\n```\n\n----------------------------------------\n\nTITLE: Run Artillery Load Test\nDESCRIPTION: This command executes a load test defined in the `load-test.yml` file using Artillery. The load test measures the system's performance under simulated user load, assessing response times and error rates.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nartillery run load-test.yml\n```\n\n----------------------------------------\n\nTITLE: Create workflow file using bash\nDESCRIPTION: Creates a new workflow file named `system_monitor.yml` inside the `.github/workflows` directory. This file will contain the workflow definition.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\ntouch .github/workflows/system_monitor.yml\n```\n\n----------------------------------------\n\nTITLE: Copying Environment File\nDESCRIPTION: Copies the example environment file `.env.example` to `.env`. This allows you to easily configure environment variables such as API keys without modifying the main script. The `.env` file should then be edited to include the actual API keys.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-extractor/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Install Artillery\nDESCRIPTION: This command installs Artillery globally, which is a load testing tool used to simulate user traffic and measure the performance of the Firecrawl system under stress. This is a prerequisite for running load tests.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g artillery\n```\n\n----------------------------------------\n\nTITLE: Create working directory using bash\nDESCRIPTION: Creates a new directory named `first-workflows` and navigates into the directory using `mkdir` and `cd` commands.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nmkdir first-workflows\ncd first-workflows\n```\n\n----------------------------------------\n\nTITLE: Testing the API with curl\nDESCRIPTION: This snippet provides a curl command to test the API by sending a GET request to the `/test` endpoint.  A successful response should return \"Hello, world!\"\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET http://localhost:3002/test\n```\n\n----------------------------------------\n\nTITLE: Create XML Sitemap Function in Python\nDESCRIPTION: This function `create_xml_sitemap` takes a list of URLs and a base URL as input and generates an XML sitemap string. It creates the root XML element, adds each URL to the sitemap, and includes optional elements like `lastmod`, `changefreq`, and `priority`. It ensures only URLs from the same domain are included in the sitemap.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef create_xml_sitemap(urls, base_url):\n    # Create the root element\n    urlset = ET.Element(\"urlset\")\n    urlset.set(\"xmlns\", \"http://www.sitemaps.org/schemas/sitemap/0.9\")\n\n    # Get current date for lastmod\n    today = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Add each URL to the sitemap\n    for url in urls:\n        # Only include URLs from the same domain\n        if urlparse(url).netloc == urlparse(base_url).netloc:\n            url_element = ET.SubElement(urlset, \"url\")\n            loc = ET.SubElement(url_element, \"loc\")\n            loc.text = url\n\n            # Add optional elements\n            lastmod = ET.SubElement(url_element, \"lastmod\")\n            lastmod.text = today\n\n            changefreq = ET.SubElement(url_element, \"changefreq\")\n            changefreq.text = \"monthly\"\n\n            priority = ET.SubElement(url_element, \"priority\")\n            priority.text = \"0.5\"\n\n    # Create the XML string\n    return ET.tostring(urlset, encoding=\"unicode\", method=\"xml\")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: This snippet shows the required and optional environment variables that need to be set in the `.env` file located in the `/apps/api/` directory. It includes configurations for Redis, Supabase, OpenAI, Playwright, LlamaParse, Slack, and PostHog.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# ===== Required ENVS ======\nNUM_WORKERS_PER_QUEUE=8\nPORT=3002\nHOST=0.0.0.0\nREDIS_URL=redis://localhost:6379\nREDIS_RATE_LIMIT_URL=redis://localhost:6379\n\n## To turn on DB authentication, you need to set up supabase.\nUSE_DB_AUTHENTICATION=false\n\n# ===== Optional ENVS ======\n\n# Supabase Setup (used to support DB authentication, advanced logging, etc.)\nSUPABASE_ANON_TOKEN=\nSUPABASE_URL=\nSUPABASE_SERVICE_TOKEN=\n\n# Other Optionals\nTEST_API_KEY= # use if you've set up authentication and want to test with a real API key\nOPENAI_API_KEY= # add for LLM dependednt features (image alt generation, etc.)\nBULL_AUTH_KEY= @\nPLAYWRIGHT_MICROSERVICE_URL=  # set if you'd like to run a playwright fallback\nLLAMAPARSE_API_KEY= #Set if you have a llamaparse key you'd like to use to parse pdfs\nSLACK_WEBHOOK_URL= # set if you'd like to send slack server health status messages\nPOSTHOG_API_KEY= # set if you'd like to send posthog events like job logs\nPOSTHOG_HOST= # set if you'd like to send posthog events like job logs\n```\n\n----------------------------------------\n\nTITLE: Fire-Engine Worker Count Configuration - YAML\nDESCRIPTION: Configures the number of workers per queue for the fire-engine scraping strategy. This parameter determines the concurrency level of scraping tasks. Setting this value affects resource utilization and the system's ability to handle concurrent requests.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-6-7/load-test-7.md#_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nNUM_WORKERS_PER_QUEUE=8\n```\n\n----------------------------------------\n\nTITLE: Starting Development Server\nDESCRIPTION: This command starts the React development server, allowing you to view and interact with the Firecrawl UI Template in your web browser. It typically runs the application in development mode with hot-reloading enabled.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/ui/ingestion-ui/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Load Test with Artillery\nDESCRIPTION: This YAML snippet configures a load test using Artillery. It defines four phases of varying arrival rates to simulate different load conditions. The 'duration' specifies the length of each phase in seconds, while 'arrivalRate' defines the number of virtual users to start per second.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-1-5/load-test-2.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n# load-test.yml\n- duration: 60\n  arrivalRate: 10  # Initial load\n- duration: 120\n  arrivalRate: 20  # Increased load\n- duration: 180\n  arrivalRate: 30  # Peak load\n- duration: 60\n  arrivalRate: 10  # Cool down\n```\n\n----------------------------------------\n\nTITLE: Install project dependencies using npm\nDESCRIPTION: This command installs the necessary Node.js packages required for the project. It reads the dependencies listed in the `package.json` file and downloads them into the `node_modules` directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/scrape_and_analyze_airbnb_data_e2b/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration\nDESCRIPTION: This snippet defines the environment variables required for configuring a self-hosted Firecrawl instance. It includes settings for the port, host, database authentication, AI features (OpenAI, Ollama), proxy server, search API (SearXNG), Supabase, API keys, Bull Queue, Playwright microservice, Redis, LlamaParse, Slack, and PostHog. Users need to create a `.env` file in the root directory and set these variables accordingly.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# ===== Required ENVS ======\nPORT=3002\nHOST=0.0.0.0\n\n# To turn on DB authentication, you need to set up Supabase.\nUSE_DB_AUTHENTICATION=false\n\n# ===== Optional ENVS ======\n\n## === AI features (JSON format on scrape, /extract API) ===\n# Provide your OpenAI API key here to enable AI features\n# OPENAI_API_KEY=\n\n# Experimental: Use Ollama\n# OLLAMA_BASE_URL=http://localhost:11434/api\n# MODEL_NAME=deepseek-r1:7b\n# MODEL_EMBEDDING_NAME=nomic-embed-text\n\n# Experimental: Use any OpenAI-compatible API\n# OPENAI_BASE_URL=https://example.com/v1\n# OPENAI_API_KEY=\n\n## === Proxy ===\n# PROXY_SERVER can be a full URL (e.g. http://0.1.2.3:1234) or just an IP and port combo (e.g. 0.1.2.3:1234)\n# Do not uncomment PROXY_USERNAME and PROXY_PASSWORD if your proxy is unauthenticated\n# PROXY_SERVER=\n# PROXY_USERNAME=\n# PROXY_PASSWORD=\n\n## === /search API ===\n# By default, the /search API will use Google search.\n\n# You can specify a SearXNG server with the JSON format enabled, if you'd like to use that instead of direct Google.\n# You can also customize the engines and categories parameters, but the defaults should also work just fine.\n# SEARXNG_ENDPOINT=http://your.searxng.server\n# SEARXNG_ENGINES=\n# SEARXNG_CATEGORIES=\n\n## === Other ===\n\n# Supabase Setup (used to support DB authentication, advanced logging, etc.)\n# SUPABASE_ANON_TOKEN=\n# SUPABASE_URL=\n# SUPABASE_SERVICE_TOKEN=\n\n# Use if you've set up authentication and want to test with a real API key\n# TEST_API_KEY=\n\n# This key lets you access the queue admin panel. Change this if your deployment is publicly accessible.\nBULL_AUTH_KEY=CHANGEME\n\n# This is now autoconfigured by the docker-compose.yaml. You shouldn't need to set it.\n# PLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/scrape\n# REDIS_URL=redis://redis:6379\n# REDIS_RATE_LIMIT_URL=redis://redis:6379\n\n# Set if you have a llamaparse key you'd like to use to parse pdfs\n# LLAMAPARSE_API_KEY=\n\n# Set if you'd like to send server health status messages to Slack\n# SLACK_WEBHOOK_URL=\n\n# Set if you'd like to send posthog events like job logs\n# POSTHOG_API_KEY=\n# POSTHOG_HOST=\n```\n\n----------------------------------------\n\nTITLE: Make Cron Script Executable (Bash)\nDESCRIPTION: This snippet shows how to make the `cron_scraper.py` file executable. The `chmod +x` command modifies the file's permissions, allowing it to be executed as a program.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x cron_scraper.py\n```\n\n----------------------------------------\n\nTITLE: Bumping Version Script with Prerelease\nDESCRIPTION: Script used to create a prerelease with version bump.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/redis/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nscripts/bump_version.sh prerel\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Logs\nDESCRIPTION: This command is used for troubleshooting Docker container issues. It displays the logs for a specified container, allowing users to identify errors or unexpected behavior. Replace `[container_name]` with the actual name of the container you want to inspect.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker logs [container_name]\n```\n\n----------------------------------------\n\nTITLE: Running the Extractor\nDESCRIPTION: Executes the main Python script, `llama-4-maverick-extractor.py`. This script prompts the user for a company name and the desired information, then searches, extracts, and structures the data.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-extractor/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython llama-4-maverick-extractor.py\n```\n\n----------------------------------------\n\nTITLE: Running with Docker Compose\nDESCRIPTION: This snippet shows the command to start the Firecrawl project using Docker Compose. This requires Docker and Docker Compose to be installed and configured, and a `.env` file in the `/apps/api/` directory. All services will be started automatically.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies\nDESCRIPTION: This command installs the necessary Python packages, including langchain, groq, faiss-cpu, ollama, and firecrawl-py. The --upgrade flag ensures that the packages are updated to the latest versions, and --quiet minimizes the output.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/web_data_rag_with_llama3/web-data-rag--with-llama3.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade --quiet langchain langchain-community groq faiss-cpu ollama firecrawl-py\n```\n\n----------------------------------------\n\nTITLE: LLM Extraction API Response\nDESCRIPTION: This JSON snippet represents the structured data returned by the Firecrawl scrape API with JSON format specified. It includes raw content, metadata (title, description, etc.), and the extracted JSON data based on the provided schema.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": true,\n  \"data\": {\n    \"content\": \"Raw Content\",\n    \"metadata\": {\n      \"title\": \"Mendable\",\n      \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Mendable\",\n      \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"ogUrl\": \"https://mendable.ai/\",\n      \"ogImage\": \"https://mendable.ai/mendable_new_og1.png\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Mendable\",\n      \"sourceURL\": \"https://mendable.ai/\"\n    },\n    \"json\": {\n      \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n      \"supports_sso\": true,\n      \"is_open_source\": false,\n      \"is_in_yc\": true\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting the Main Server with pnpm\nDESCRIPTION: This snippet shows how to start the main API server using pnpm.  This needs to be run after installing dependencies and you are in the apps/api/ directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npnpm run start\n```\n\n----------------------------------------\n\nTITLE: Setting HTTP Timeout - YAML\nDESCRIPTION: This code snippet shows the configuration setting for HTTP timeout in a YAML file. The timeout is set to 30 seconds, which defines the maximum time allowed for an HTTP request to complete before being considered a failure.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-1-5/load-test-5.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhttp:\n  timeout: 30\n```\n\n----------------------------------------\n\nTITLE: Example Usage\nDESCRIPTION: Demonstrates an example of how to run the script and interact with it. The user is prompted for a company name (Tesla) and the information they want (latest electric vehicle models and their prices).\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-extractor/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ python llama-4-maverick-extractor.py\nEnter the company name: Tesla\nEnter what information you want about the company: latest electric vehicle models and their prices\n```\n\n----------------------------------------\n\nTITLE: Monitor Heroku Logs (Bash)\nDESCRIPTION: This command tails the Heroku application logs, providing real-time output for monitoring the application's health and debugging issues.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nheroku logs --tail\n```\n\n----------------------------------------\n\nTITLE: Declaring Python Package Dependencies\nDESCRIPTION: This snippet declares the Python package dependencies for the Firecrawl project. It specifies minimum versions for 'firecrawl', 'together', and 'python-dotenv'. This ensures that the project has access to the necessary libraries and functionalities with compatible versions.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/llama-4-maverick-web-crawler/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfirecrawl>=0.1.0\ntogether>=0.2.0\npython-dotenv>=0.19.0\n```\n\n----------------------------------------\n\nTITLE: Tail Cron Log (Bash)\nDESCRIPTION: This snippet shows how to monitor the `cron.log` file in real-time. The `tail -f` command displays the last lines of the file and updates the display as new lines are added, allowing you to track the execution of your cron jobs.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ntail -f ~/cron.log\n```\n\n----------------------------------------\n\nTITLE: Create tests directory using bash\nDESCRIPTION: Creates a `tests` directory and a `test_main.py` file within it. This structure is meant to house the python test files. \nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\nmkdir tests\ntouch tests/test_main.py\n```\n\n----------------------------------------\n\nTITLE: Example Input\nDESCRIPTION: This shows example inputs that will be prompted when running the web crawler.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gpt-4.1-web-crawler/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nEnter the website to crawl: https://example.com\nEnter your objective: Find the company's leadership team with their roles and short bios\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: This command installs the necessary npm packages required for running the tests. This is a prerequisite step to ensure all testing dependencies are available before executing the test suite.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Open Heroku Scheduler Dashboard (Bash)\nDESCRIPTION: This command opens the Heroku Scheduler dashboard in a web browser, allowing for the configuration of scheduled tasks.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nheroku addons:open scheduler\n```\n\n----------------------------------------\n\nTITLE: Example requirements.txt Content\nDESCRIPTION: This is an example of a manually created requirements.txt file, listing the project's Python dependencies. Each dependency is on a new line.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\npydantic\nfirecrawl-py\n```\n\n----------------------------------------\n\nTITLE: Running Workers with pnpm\nDESCRIPTION: This snippet shows how to start the worker processes, which are responsible for processing crawl jobs. The command should be executed in the `apps/api/` directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npnpm run workers\n# if you are going to use the [llm-extract feature](https://github.com/mendableai/firecrawl/pull/586/), you should also export OPENAI_API_KEY=sk-______\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment\nDESCRIPTION: Creates a new virtual environment named 'venv' using Python's `venv` module, then activates the environment. Activation differs based on the operating system, with separate commands for Unix/macOS and Windows.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate  # For Unix/macOS\nvenv\\Scripts\\activate     # For Windows\n```\n\n----------------------------------------\n\nTITLE: Example User Input\nDESCRIPTION: This example demonstrates the expected input from the user when running the script. The script prompts the user for a company name and the desired information to extract.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-web-extractor/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nEnter the company name: Tesla\nEnter what information you want about the company: latest electric vehicle models and their specifications\n```\n\n----------------------------------------\n\nTITLE: Running Redis Server\nDESCRIPTION: This snippet shows the command to start the Redis server, which is required for the Firecrawl project to function correctly.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nredis-server\n```\n\n----------------------------------------\n\nTITLE: Mapping a URL\nDESCRIPTION: Maps all associated links from a starting URL using the `map_url` method.  This method returns a list of mapped URLs.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/rust-sdk/README.md#_snippet_6\n\nLANGUAGE: Rust\nCODE:\n```\nlet map_result = app\n    .map_url(\"https://firecrawl.dev\", None)\n    .await;\n\nmatch map_result {\n    Ok(data) => println!(\"Mapped URLs: {:#?}\", data),\n    Err(e) => eprintln!(\"Map failed: {}\", e),\n}\n```\n\n----------------------------------------\n\nTITLE: Get data type\nDESCRIPTION: This code calls the function get_firecrawl_news_data and prints the type of the data returned by the function.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndata = get_firecrawl_news_data()\n\nprint(type(data))\n```\n\n----------------------------------------\n\nTITLE: Crawl Job Status Response (JSON)\nDESCRIPTION: This JSON snippet shows the expected response when checking the status of a crawl job. It includes the status of the job (e.g., 'completed'), the total number of pages crawled, the credits used, the expiration date, and an array of data containing the extracted markdown, HTML, and metadata for each crawled page.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"completed\",\n  \"total\": 36,\n  \"creditsUsed\": 36,\n  \"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n  \"data\": [\n    {\n      \"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\n      \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\\" data-js-focus-visible=\\\"\\\">...\",\n      \"metadata\": {\n        \"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\n        \"language\": \"en\",\n        \"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\n        \"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\n        \"ogLocaleAlternate\": [],\n        \"statusCode\": 200\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Signal Flow Diagram with Mermaid\nDESCRIPTION: This Mermaid diagram illustrates the signal flow of the `scrapeURL` function. It shows the sequence of operations, including building fallback lists, scraping with different engines, parsing Markdown, checking for successful scrapes, and handling scenarios where no engines are left to try.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/api/src/scraper/scrapeURL/README.md#_snippet_0\n\nLANGUAGE: Mermaid\nCODE:\n```\nflowchart TD;\n    scrapeURL-.->buildFallbackList;\n    buildFallbackList-.->scrapeURLWithEngine;\n    scrapeURLWithEngine-.->parseMarkdown;\n    parseMarkdown-.->wasScrapeSuccessful{{Was scrape successful?}};\n    wasScrapeSuccessful-.\"No\".->areEnginesLeft{{Are there engines left to try?}};\n    areEnginesLeft-.\"Yes, try next engine\".->scrapeURLWithEngine;\n    areEnginesLeft-.\"No\".->NoEnginesLeftError[/NoEnginesLeftError/]\n    wasScrapeSuccessful-.\"Yes\".->asd;\n```\n\n----------------------------------------\n\nTITLE: Create Heroku Scheduler Add-on (Bash)\nDESCRIPTION: This command creates a Heroku Scheduler add-on, which allows for scheduled execution of tasks. The 'standard' plan is used.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nheroku addons:create scheduler:standard\n```\n\n----------------------------------------\n\nTITLE: Cloning the Gemini 2.5 Web Extractor Repository\nDESCRIPTION: This command clones the Gemini 2.5 Web Extractor repository from a remote URL to your local machine.  It requires Git to be installed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/gemini-2.5-web-extractor/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <repository-url>\ncd gemini-2.5-web-extractor\n```\n\n----------------------------------------\n\nTITLE: Number of Workers Configuration\nDESCRIPTION: This YAML snippet configures the number of workers per queue. It is used to define the number of worker machines processing the queue.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-6-7/load-test-8.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nNUM_WORKERS_PER_QUEUE=12\n```\n\n----------------------------------------\n\nTITLE: Mapping with Search Response (JSON)\nDESCRIPTION: This JSON snippet shows the expected response from the Firecrawl API after mapping a URL with a search query. It contains a status indicator and an array of links that match the search criteria, ordered from most to least relevant.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://docs.firecrawl.dev\",\n    \"https://docs.firecrawl.dev/sdks/python\",\n    \"https://docs.firecrawl.dev/learn/rag-llama3\",\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Destroying Heroku Scheduler Add-on (Bash)\nDESCRIPTION: This command removes the Heroku scheduler add-on from the application. This prevents scheduled tasks from running. Requires the Heroku CLI and a configured Heroku application with the scheduler add-on.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nheroku addons:destroy scheduler\n```\n\n----------------------------------------\n\nTITLE: Adding boto3 to requirements.txt (Bash)\nDESCRIPTION: This code adds the `boto3` library as a dependency in the `requirements.txt` file. This ensures that boto3 is installed when the Heroku application is deployed. Requires a `requirements.txt` file in the project directory.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/deploying_web_scrapers/notebook.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\necho \"boto3\" >> requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Number of extracted news items\nDESCRIPTION: This code calls the function get_firecrawl_news_data and prints the number of extracted news items.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/scheduling_scrapers/notebook.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlen(data['extract']['news_items'])\n```\n\n----------------------------------------\n\nTITLE: Create directory structure using bash\nDESCRIPTION: Creates a `.github/workflows` directory structure to hold the workflow files. It uses the `-p` flag with `mkdir` to create parent directories as needed.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/github-actions-tutorial/notebook.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nmkdir -p .github/workflows\n```\n\n----------------------------------------\n\nTITLE: Extract API Response (ID)\nDESCRIPTION: A successful response from the extract endpoint returns a success flag, an ID for tracking the job and a URL trace.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/README.md#_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": true,\n  \"id\": \"44aa536d-f1cb-4706-ab87-ed0386685740\",\n  \"urlTrace\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Redis Password on Fly.io\nDESCRIPTION: Sets the Redis password as a Fly.io secret. This is a prerequisite for deploying Redis and ensures secure access. The password won't be visible after deployment.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/redis/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nfly secrets set REDIS_PASSWORD=mypassword\n```\n\n----------------------------------------\n\nTITLE: Create .env file for API Key\nDESCRIPTION: Creates a .env file to securely store the Firecrawl API key and adds the API key as an environment variable. This prevents exposing the API key directly in the code.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntouch .env\necho \"FIRECRAWL_API_KEY='YOUR-API-KEY'\" >> .env\n```\n\n----------------------------------------\n\nTITLE: Fly.io Concurrency Configuration TOML\nDESCRIPTION: This TOML snippet defines the concurrency settings for the Fly.io application. It sets a hard limit of 100 and a soft limit of 50 requests.\nSOURCE: https://github.com/mendableai/firecrawl/blob/main/apps/test-suite/load-test-results/tests-1-5/load-test-4.md#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[http_service.concurrency]\n  type = \"requests\"\n  hard_limit = 100\n  soft_limit = 50\n```"
  }
]
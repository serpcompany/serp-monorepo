[
  {
    "owner": "pyro-ppl",
    "repo": "pyro",
    "content": "TITLE: Creating Stochastic Parameters with PyroSample\nDESCRIPTION: Demonstrates how to make a PyroModule Bayesian by replacing parameters with PyroSample attributes. This example places a Normal prior over the weights, properly shaped with event dimensions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"params before:\", [name for name, _ in linear.named_parameters()])\n\nlinear.weight = PyroSample(dist.Normal(0, 1).expand([5, 2]).to_event(2))\nprint(\"params after:\", [name for name, _ in linear.named_parameters()])\nprint(\"weight:\", linear.weight)\nprint(\"weight:\", linear.weight)\n\nexample_input = torch.randn(100, 5)\nexample_output = linear(example_input)\nassert example_output.shape == (100, 2)\n```\n\n----------------------------------------\n\nTITLE: Configuring SVI Inference for VAE in Python\nDESCRIPTION: Sets up Stochastic Variational Inference (SVI) with the VAE model, guide, optimizer, and Trace_ELBO loss.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsvi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Initializing Inverse Autoregressive Flows in Python\nDESCRIPTION: This snippet shows how to create and initialize Inverse Autoregressive Flows (IAFs) in the constructor of a Deep Markov Model. It creates a list of AffineAutoregressive transforms and wraps them in a PyTorch ModuleList.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\niafs = [AffineAutoregressive(AutoRegressiveNN(z_dim, [iaf_dim])) for _ in range(num_iafs)]\nself.iafs = nn.ModuleList(iafs)\n```\n\n----------------------------------------\n\nTITLE: Initializing SVI for Variational Inference\nDESCRIPTION: Code showing how to set up Stochastic Variational Inference using Pyro's SVI class with a model, guide, optimizer and ELBO loss function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyro\nfrom pyro.infer import SVI, Trace_ELBO\nsvi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Training and Visualizing DPMM Results\nDESCRIPTION: Performs variational inference using SVI and visualizes the learned cluster centers for different concentration parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nT = 6\noptim = Adam({\"lr\": 0.05})\nsvi = SVI(model, guide, optim, loss=Trace_ELBO())\nlosses = []\n\ndef train(num_iterations):\n    pyro.clear_param_store()\n    for j in tqdm(range(num_iterations)):\n        loss = svi.step(data)\n        losses.append(loss)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Generative Model in Pyro\nDESCRIPTION: Implements a stochastic function that models measuring an object's weight with a noisy scale. The model incorporates prior belief (guess) and measurement uncertainty with Gaussian distributions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef scale(guess):\n    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n```\n\n----------------------------------------\n\nTITLE: Implementing Amortized Latent Dirichlet Allocation in Python using Pyro\nDESCRIPTION: This code snippet demonstrates a complete implementation of Amortized Latent Dirichlet Allocation using Pyro. It includes data loading, model definition, guide function, and training loop. The model is designed for topic modeling on text data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/lda.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport itertools\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\nfrom pyro.optim import ClippedAdam\n\nassert pyro.__version__.startswith('1.8.4')\npyro.enable_validation(True)\npyro.set_rng_seed(0)\n# Enable smoke test - run the model by default for 1 iteration\nsmoke_test = 'CI' in os.environ\n\n\ndef get_data(args):\n    with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data/lda-data.txt')) as f:\n        data = f.readlines()\n    data = [x.split() for x in data]\n    data = [[int(y) for y in x] for x in data]\n    data = [x for x in data if len(x) > args.min_doc_length]\n    data = data[:-(len(data) % args.batch_size)]  # make sure the batch size divides the data\n    vocab_size = max(itertools.chain.from_iterable(data)) + 1\n    return data, vocab_size\n\n\ndef model(data, args, vocab_size, batch_size, num_docs, num_words):\n    # Globals\n    with pyro.plate('topics', args.num_topics):\n        topic_weights = pyro.sample('topic_weights', dist.Gamma(1. / args.num_topics, 1.))\n        topic_words = pyro.sample('topic_words',\n                                  dist.Dirichlet(torch.ones(vocab_size) / vocab_size))\n    # Locals\n    with pyro.plate('documents', num_docs) as ind:\n        doc_topics = pyro.sample('doc_topics', dist.Dirichlet(topic_weights))\n        with pyro.plate('words', num_words):\n            # The word_topics variable is marginalized out during inference,\n            # achieved by specifying its distribution and value inside this plate.\n            word_topics = pyro.sample('word_topics', dist.Categorical(doc_topics[ind]),\n                                      infer={\"enumerate\": \"parallel\"})\n            data = pyro.sample('doc_words', dist.Categorical(topic_words[word_topics]),\n                               obs=data)\n    return topic_weights, topic_words, doc_topics\n\n\ndef guide(data, args, vocab_size, batch_size, num_docs, num_words):\n    # Use a conjugate guide for global variables.\n    topic_weights_posterior = pyro.param(\n        'topic_weights_posterior',\n        lambda: torch.ones(args.num_topics) / args.num_topics,\n        constraint=constraints.positive)\n    topic_words_posterior = pyro.param(\n        'topic_words_posterior',\n        lambda: torch.ones(args.num_topics, vocab_size) / vocab_size,\n        constraint=constraints.greater_than(0.5 / vocab_size))\n    # Use an amortized guide for local variables.\n    # We'll use an LSTM here, but a NN, CNN, LSTM, or any PyTorch module will work.\n    lstm = torch.nn.LSTM(vocab_size, args.encoding_dim, args.num_layers,\n                         batch_first=True, bidirectional=False)\n    h0 = torch.zeros(args.num_layers, 1, args.encoding_dim)\n    c0 = torch.zeros(args.num_layers, 1, args.encoding_dim)\n    pyro.module('lstm', lstm)\n    pyro.param('h0', h0)\n    pyro.param('c0', c0)\n\n    with pyro.plate('topics', args.num_topics):\n        pyro.sample('topic_weights',\n                    dist.Gamma(topic_weights_posterior, 1.))\n        pyro.sample('topic_words',\n                    dist.Dirichlet(topic_words_posterior))\n\n    def encode(data):\n        # Embed and run through LSTM\n        with torch.no_grad():\n            data = torch.nn.functional.one_hot(data, vocab_size).float()\n            data = data.unsqueeze(0)\n            post_h, _ = lstm(data, (h0.expand(1, batch_size, -1).contiguous(),\n                                    c0.expand(1, batch_size, -1).contiguous()))\n            post_h = post_h.squeeze(0)\n\n        # Convert LSTM encodings to topic probabilities.\n        doc_topics_w = pyro.param('doc_topics_w',\n                                  lambda: torch.randn(args.encoding_dim, args.num_topics))\n        doc_topics_b = pyro.param('doc_topics_b',\n                                  lambda: torch.zeros(args.num_topics))\n        doc_topic_logits = torch.matmul(post_h, doc_topics_w.t()) + doc_topics_b\n        doc_topic_posterior = torch.softmax(doc_topic_logits, -1)\n        return doc_topic_posterior\n\n    # Sample local variables.\n    with pyro.plate('documents', num_docs) as ind:\n        doc_topic_posterior = encode(data)\n        pyro.sample('doc_topics', dist.Dirichlet(doc_topic_posterior[ind]))\n\n\ndef main(args):\n    # Load data\n    data, vocab_size = get_data(args)\n    print('Data loaded. Vocab size: {}, Num docs: {}'.format(vocab_size, len(data)))\n\n    # Batch data\n    data_split = [\n        data[i:i + args.batch_size] for i in range(0, len(data), args.batch_size)\n    ]\n    data_split = [[torch.tensor(x) for x in y] for y in data_split]\n\n    # Setup Pyro optimizer\n    pyro.clear_param_store()\n    adam_params = {\"lr\": args.learning_rate, \"betas\": (args.beta1, args.beta2),\n                   \"clip_norm\": args.clip_norm, \"lrd\": args.lr_decay,\n                   \"weight_decay\": args.weight_decay}\n    if args.cuda:\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    optimizer = ClippedAdam(adam_params)\n\n    # Setup inference algorithm\n    elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n    svi = SVI(model, guide, optimizer, loss=elbo)\n\n    # Train\n    t_start = time.time()\n    loss_list = []\n    for epoch in range(args.num_epochs):\n        epoch_loss = 0.0\n        word_count = 0\n        for i, document_batch in enumerate(data_split):\n            # Batch documents for training\n            batch_loss = svi.step(document_batch, args, vocab_size,\n                                  args.batch_size, len(document_batch),\n                                  len(document_batch[0]))\n            epoch_loss += batch_loss\n            word_count += sum(map(lambda t: t.shape[0], document_batch))\n        loss_list.append(epoch_loss / word_count)\n        print('[Epoch %d] avg loss: %.4f' % (epoch, loss_list[-1]))\n        if args.plot and epoch % args.plot_every == 0:\n            plot_loss(loss_list)\n        if args.checkpoint and epoch % args.checkpoint_every == 0:\n            save(args.checkpoint_path)\n        if smoke_test and epoch > 1:\n            break\n\n    # Report test set perplexity\n    test_loss = svi.evaluate_loss(data, args, vocab_size, len(data), len(data))\n    print('Test set perplexity: {}'.format(np.exp(test_loss / sum(map(len, data)))))\n\n    # Plot\n    if args.plot:\n        plot_loss(loss_list)\n        plt.title(\"Test set perplexity: {:.4f}\".format(\n            np.exp(test_loss / sum(map(len, data)))))\n\n    # Save\n    if args.checkpoint:\n        save(args.checkpoint_path)\n\n    # Print topics\n    topic_weights = pyro.param('topic_weights_posterior').detach().cpu().numpy()\n    topic_words = pyro.param('topic_words_posterior').detach().cpu().numpy()\n    for i in range(args.num_topics):\n        print('Topic {}: {}'.format(i, topic_weights[i]))\n        print(''.join([' ' + str(j) for j in np.argsort(topic_words[i])[-10:][::-1]]))\n\n    # Wrap up\n    print('Done. Elapsed time: {:.2f}s'.format(time.time() - t_start))\n\n\ndef save(checkpoint_path):\n    model_state = {'topic_weights': pyro.param('topic_weights_posterior').detach().cpu().numpy(),\n                   'topic_words': pyro.param('topic_words_posterior').detach().cpu().numpy()}\n    guide_state = {'lstm': pyro.module('lstm').state_dict(),\n                   'h0': pyro.param('h0').detach().cpu().numpy(),\n                   'c0': pyro.param('c0').detach().cpu().numpy()}\n    with open(checkpoint_path, 'wb') as f:\n        torch.save({'model': model_state, 'guide': guide_state}, f)\n\n\ndef plot_loss(loss_list):\n    plt.figure(figsize=(10, 5))\n    plt.plot(loss_list)\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.tight_layout()\n    plt.show()\n\n\nif __name__ == '__main__':\n    assert pyro.__version__.startswith('1.8.4')\n    parser = argparse.ArgumentParser(description=\"Amortized Latent Dirichlet Allocation\")\n    parser.add_argument(\"-t\", \"--num-topics\", default=8, type=int)\n    parser.add_argument(\"-b\", \"--batch-size\", default=32, type=int)\n    parser.add_argument(\"-n\", \"--num-epochs\", default=50, type=int)\n    parser.add_argument(\"-lr\", \"--learning-rate\", default=0.01, type=float)\n    parser.add_argument(\"--beta1\", default=0.90, type=float)\n    parser.add_argument(\"--beta2\", default=0.999, type=float)\n    parser.add_argument(\"--clip-norm\", default=10.0, type=float)\n    parser.add_argument(\"--lr-decay\", default=0.99996, type=float)\n    parser.add_argument(\"--weight-decay\", default=0.0, type=float)\n    parser.add_argument(\"--num-layers\", default=2, type=int)\n    parser.add_argument(\"--min-doc-length\", default=2, type=int)\n    parser.add_argument(\"--encoding-dim\", default=100, type=int)\n    parser.add_argument(\"--jit\", action=\"store_true\")\n    parser.add_argument(\"--cuda\", action=\"store_true\")\n    parser.add_argument(\"--checkpoint-path\", default=None, type=str)\n    parser.add_argument(\"--checkpoint\", action=\"store_true\")\n    parser.add_argument(\"--checkpoint-every\", default=1, type=int)\n    parser.add_argument(\"--plot\", action=\"store_true\")\n    parser.add_argument(\"--plot-every\", default=1, type=int)\n    args = parser.parse_args()\n    main(args)\n```\n\n----------------------------------------\n\nTITLE: Applying Inverse Autoregressive Flows in Variational Guide\nDESCRIPTION: This code applies the Inverse Autoregressive Flows to the base distribution in the variational guide. It transforms the base Gaussian distribution into a richer non-Gaussian distribution using the TransformedDistribution construct.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nif self.iafs.__len__() > 0:\n    z_dist = TransformedDistribution(z_dist, self.iafs)\n```\n\n----------------------------------------\n\nTITLE: Coin Fairness Model Implementation\nDESCRIPTION: Defines the probabilistic model for coin fairness using Beta prior and Bernoulli likelihood.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pyro.distributions as dist\n\ndef model(data):\n    # define the hyperparameters that control the Beta prior\n    alpha0 = torch.tensor(10.0)\n    beta0 = torch.tensor(10.0)\n    # sample f from the Beta prior\n    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n    # loop over the observed data\n    for i in range(len(data)):\n        # observe datapoint i using the Bernoulli \n        # likelihood Bernoulli(f)\n        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n```\n\n----------------------------------------\n\nTITLE: Creating a Compatible Guide for LogNormal Model\nDESCRIPTION: Implements a valid guide function with a learnable location parameter that maintains the same LogNormal support as the model distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef good_guide():\n    loc = pyro.param(\"loc\", torch.tensor(0.0))\n    pyro.sample(\"x\", dist.LogNormal(loc, 1.0))\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete CVAE Model in Pyro\nDESCRIPTION: Defines the main CVAE class that combines encoder, decoder, and baseline networks. Implements both the model and guide functions required for Pyro's probabilistic programming framework.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass CVAE(nn.Module):\n    def __init__(self, z_dim, hidden_1, hidden_2, pre_trained_baseline_net):\n        super().__init__()\n        # The CVAE is composed of multiple MLPs, such as recognition network\n        # qφ(z|x, y), (conditional) prior network pθ(z|x), and generation\n        # network pθ(y|x, z). Also, CVAE is built on top of the NN: not only\n        # the direct input x, but also the initial guess y_hat made by the NN\n        # are fed into the prior network.\n        self.baseline_net = pre_trained_baseline_net\n        self.prior_net = Encoder(z_dim, hidden_1, hidden_2)\n        self.generation_net = Decoder(z_dim, hidden_1, hidden_2)\n        self.recognition_net = Encoder(z_dim, hidden_1, hidden_2)\n\n    def model(self, xs, ys=None):\n        # register this pytorch module and all of its sub-modules with pyro\n        pyro.module(\"generation_net\", self)\n        batch_size = xs.shape[0]\n        with pyro.plate(\"data\"):\n\n            # Prior network uses the baseline predictions as initial guess.\n            # This is the generative process with recurrent connection\n            with torch.no_grad():\n                # this ensures the training process does not change the\n                # baseline network\n                y_hat = self.baseline_net(xs).view(xs.shape)\n\n            # sample the handwriting style from the prior distribution, which is\n            # modulated by the input xs.\n            prior_loc, prior_scale = self.prior_net(xs, y_hat)\n            zs = pyro.sample('z', dist.Normal(prior_loc, prior_scale).to_event(1))\n\n            # the output y is generated from the distribution pθ(y|x, z)\n            loc = self.generation_net(zs)\n\n            if ys is not None:\n                # In training, we will only sample in the masked image\n                mask_loc = loc[(xs == -1).view(-1, 784)].view(batch_size, -1)\n                mask_ys = ys[xs == -1].view(batch_size, -1)\n                pyro.sample('y', dist.Bernoulli(mask_loc).to_event(1), obs=mask_ys)\n            else:\n                # In testing, no need to sample: the output is already a\n                # probability in [0, 1] range, which better represent pixel\n                # values considering grayscale. If we sample, we will force\n                # each pixel to be  either 0 or 1, killing the grayscale\n                pyro.deterministic('y', loc.detach())\n\n            # return the loc so we can visualize it later\n            return loc\n\n    def guide(self, xs, ys=None):\n        with pyro.plate(\"data\"):\n            if ys is None:\n                # at inference time, ys is not provided. In that case,\n                # the model uses the prior network\n                y_hat = self.baseline_net(xs).view(xs.shape)\n                loc, scale = self.prior_net(xs, y_hat)\n            else:\n                # at training time, uses the variational distribution\n                # q(z|x,y) = normal(loc(x,y),scale(x,y))\n                loc, scale = self.recognition_net(xs, ys)\n\n            pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n```\n\n----------------------------------------\n\nTITLE: Implementing Improved Guide Step Function for AIR\nDESCRIPTION: A function that implements a single step of the guide in the AIR model, processing data with an RNN and sampling latent variables z_pres, z_where, and z_what using Pyro's probabilistic programming interface.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef guide_step_improved(t, data, prev):\n\n    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n    h, c = rnn(rnn_input, (prev.h, prev.c))\n    z_pres_p, z_where_loc, z_where_scale = predict(h)\n\n    z_pres = pyro.sample('z_pres_{}'.format(t),\n                         dist.Bernoulli(z_pres_p * prev.z_pres)\n                             .to_event(1))\n\n    z_where = pyro.sample('z_where_{}'.format(t),\n                          dist.Normal(z_where_loc, z_where_scale)\n                              .to_event(1))\n\n    # New. Crop a small window from the input.\n    x_att = image_to_object(z_where, data)\n\n    # Compute the parameter of the distribution over z_what\n    # by passing the window through the encoder network.\n    z_what_loc, z_what_scale = encode(x_att)\n\n    z_what = pyro.sample('z_what_{}'.format(t),\n                         dist.Normal(z_what_loc, z_what_scale)\n                             .to_event(1))\n\n    return # values for next step\n```\n\n----------------------------------------\n\nTITLE: Implementing Semi-Supervised VAE Model and Guide Functions in Pyro\nDESCRIPTION: Core implementation of the semi-supervised VAE model and guide functions in Pyro. The model handles both observed and unobserved labels, and the guide provides different inference paths depending on whether labels are available. This implements model M2 from the referenced paper.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ss-vae.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef model(self, xs, ys=None):\n    # register this pytorch module and all of its sub-modules with pyro\n    pyro.module(\"ss_vae\", self)\n    batch_size = xs.size(0)\n\n    # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n    with pyro.plate(\"data\"):\n\n        # sample the handwriting style from the constant prior distribution\n        prior_loc = xs.new_zeros([batch_size, self.z_dim])\n        prior_scale = xs.new_ones([batch_size, self.z_dim])\n        zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n\n        # if the label y (which digit to write) is supervised, sample from the\n        # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n        alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n        ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n\n        # finally, score the image (x) using the handwriting style (z) and\n        # the class label y (which digit to write) against the\n        # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n        # where `decoder` is a neural network\n        loc = self.decoder([zs, ys])\n        pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n\ndef guide(self, xs, ys=None):\n    with pyro.plate(\"data\"):\n        # if the class label (the digit) is not supervised, sample\n        # (and score) the digit with the variational distribution\n        # q(y|x) = categorical(alpha(x))\n        if ys is None:\n            alpha = self.encoder_y(xs)\n            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n\n        # sample (and score) the latent handwriting-style with the variational\n        # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n        loc, scale = self.encoder_z([xs, ys])\n        pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n```\n\n----------------------------------------\n\nTITLE: Training a Bayesian Model with SVI and AutoNormal Guide\nDESCRIPTION: Demonstrates how to perform variational inference on a Bayesian PyroModule using SVI with an AutoNormal guide and ELBO loss function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\npyro.set_rng_seed(1)\n\nmodel = Model(5, 2)\nx = torch.randn(100, 5)\ny = model(x)\n\nguide = AutoNormal(model)\nsvi = SVI(model, guide, Adam({\"lr\": 0.01}), Trace_ELBO())\nfor step in range(2 if smoke_test else 501):\n    loss = svi.step(x, y) / y.numel()\n    if step % 100 == 0:\n        print(\"step {} loss = {:0.4g}\".format(step, loss))\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom ELBO with KL Annealing in Python for Pyro\nDESCRIPTION: This function implements a custom ELBO (Evidence Lower BOund) calculation with KL annealing for variational inference in Pyro. It allows scaling of log probabilities for specified latent variables, providing flexibility in balancing reconstruction and KL terms during training.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef simple_elbo_kl_annealing(model, guide, *args, **kwargs):\n    # get the annealing factor and latents to anneal from the keyword\n    # arguments passed to the model and guide\n    annealing_factor = kwargs.pop('annealing_factor', 1.0)\n    latents_to_anneal = kwargs.pop('latents_to_anneal', [])\n    # run the guide and replay the model against the guide\n    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n    model_trace = poutine.trace(\n        poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)\n        \n    elbo = 0.0\n    # loop through all the sample sites in the model and guide trace and\n    # construct the loss; note that we scale all the log probabilities of\n    # samples sites in `latents_to_anneal` by the factor `annealing_factor`\n    for site in model_trace.values():\n        if site[\"type\"] == \"sample\":\n            factor = annealing_factor if site[\"name\"] in latents_to_anneal else 1.0\n            elbo = elbo + factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n    for site in guide_trace.values():\n        if site[\"type\"] == \"sample\":\n            factor = annealing_factor if site[\"name\"] in latents_to_anneal else 1.0        \n            elbo = elbo - factor * site[\"fn\"].log_prob(site[\"value\"]).sum()\n    return -elbo\n\nsvi = SVI(model, guide, optim, loss=simple_elbo_kl_annealing)\nsvi.step(other_args, annealing_factor=0.2, latents_to_anneal=[\"my_latent\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Stochastic Variational Inference (SVI) in Pyro\nDESCRIPTION: Demonstrates how to set up and run Stochastic Variational Inference in Pyro using the SVI class. This code optimizes the guide parameters to approximate the posterior distribution of the model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nguess = 8.5\n\npyro.clear_param_store()\nsvi = pyro.infer.SVI(model=conditioned_scale, \n                     guide=scale_parametrized_guide,\n                     optim=pyro.optim.Adam({\"lr\": 0.003}),\n                     loss=pyro.infer.Trace_ELBO())\n\n\nlosses, a, b = [], [], []\nnum_steps = 2500\nfor t in range(num_steps):\n    losses.append(svi.step(guess))\n    a.append(pyro.param(\"a\").item())\n    b.append(pyro.param(\"b\").item())\n    \nplt.plot(losses)\nplt.title(\"ELBO\")\nplt.xlabel(\"step\")\nplt.ylabel(\"loss\");\nprint('a = ',pyro.param(\"a\").item())\nprint('b = ', pyro.param(\"b\").item())\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Function with Likelihood in Pyro\nDESCRIPTION: Defines the complete model function that combines the prior with a Gaussian likelihood (std=0.3). Uses plate for data subsampling and registers networks for optimization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    # Register network for optimization.\n    pyro.module(\"decode\", decode)\n    with pyro.plate('data', data.size(0)) as indices:\n        batch = data[indices]\n        x = prior(batch.size(0)).view(-1, 50 * 50)\n        sd = (0.3 * torch.ones(1)).expand_as(x)\n        pyro.sample('obs', dist.Normal(x, sd).to_event(1),\n                    obs=batch)\n```\n\n----------------------------------------\n\nTITLE: Implementing MLE and MAP Estimation with AutoGuides in Pyro\nDESCRIPTION: Demonstrates how to use Pyro's AutoGuide machinery for MLE and MAP estimation, including masking for MLE and using AutoDelta for both MLE and MAP.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mle_map.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef masked_model(data):\n    f = pyro.sample(\"latent_fairness\", \n                    dist.Beta(10.0, 10.0).mask(False))\n    with pyro.plate(\"data\", data.size(0)):\n        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n\nautoguide_mle = pyro.infer.autoguide.AutoDelta(masked_model)\ntrain(masked_model, autoguide_mle)\n\nautoguide_map = pyro.infer.autoguide.AutoDelta(original_model)\ntrain(original_model, autoguide_map)\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete Guide with Baselines for AIR Model\nDESCRIPTION: Complete implementation of the guide for the AIR model, including state initialization, guide step function with baseline variance reduction, and the main guide function that iterates through multiple steps of detection.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nGuideState = namedtuple('GuideState', ['h', 'c', 'bl_h', 'bl_c', 'z_pres', 'z_where', 'z_what'])\ndef initial_guide_state(n):\n    return GuideState(h=torch.zeros(n, 256),\n                      c=torch.zeros(n, 256),\n                      bl_h=torch.zeros(n, 256),\n                      bl_c=torch.zeros(n, 256),\n                      z_pres=torch.ones(n, 1),\n                      z_where=torch.zeros(n, 3),\n                      z_what=torch.zeros(n, 50))\n\ndef guide_step(t, data, prev):\n\n    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n    h, c = rnn(rnn_input, (prev.h, prev.c))\n    z_pres_p, z_where_loc, z_where_scale = predict(h)\n\n    # Here we compute the baseline value, and pass it to sample.\n    baseline_value, bl_h, bl_c = baseline_step(data, prev)\n    z_pres = pyro.sample('z_pres_{}'.format(t),\n                         dist.Bernoulli(z_pres_p * prev.z_pres)\n                             .to_event(1),\n                         infer=dict(baseline=dict(baseline_value=baseline_value.squeeze(-1))))\n\n    z_where = pyro.sample('z_where_{}'.format(t),\n                          dist.Normal(z_where_loc, z_where_scale)\n                              .mask(z_pres)\n                              .to_event(1))\n    \n    x_att = image_to_object(z_where, data)\n\n    z_what_loc, z_what_scale = encode(x_att)\n\n    z_what = pyro.sample('z_what_{}'.format(t),\n                         dist.Normal(z_what_loc, z_what_scale)\n                             .mask(z_pres)\n                             .to_event(1))\n\n    return GuideState(h=h, c=c, bl_h=bl_h, bl_c=bl_c, z_pres=z_pres, z_where=z_where, z_what=z_what)\n\ndef guide(data):\n    # Register networks for optimization.\n    pyro.module('rnn', rnn),\n    pyro.module('predict', predict),\n    pyro.module('encode', encode),\n    pyro.module('bl_rnn', bl_rnn)\n    pyro.module('bl_predict', bl_predict)\n\n    with pyro.plate('data', data.size(0), subsample_size=64) as indices:\n        batch = data[indices]\n        state = initial_guide_state(batch.size(0))\n        steps = []\n        for t in range(3):\n            state = guide_step(t, batch, state)\n            steps.append(state)\n        return steps\n```\n\n----------------------------------------\n\nTITLE: Defining Dirichlet Process Mixture Model in Pyro\nDESCRIPTION: Implements the generative model for Dirichlet process mixture using Pyro primitives. Includes beta sampling, cluster parameter sampling, and data generation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    with pyro.plate(\"beta_plate\", T-1):\n        beta = pyro.sample(\"beta\", Beta(1, alpha))\n\n    with pyro.plate(\"mu_plate\", T):\n        mu = pyro.sample(\"mu\", MultivariateNormal(torch.zeros(2), 5 * torch.eye(2)))\n\n    with pyro.plate(\"data\", N):\n        z = pyro.sample(\"z\", Categorical(mix_weights(beta)))\n        pyro.sample(\"obs\", MultivariateNormal(mu[z], torch.eye(2)), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Pyro Bayesian Model Implementation - Python\nDESCRIPTION: Implements the complete Bayesian model using Pyro, including prior distributions and sampling logic.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyro\nimport pyro.distributions as dist\n\ndef model(polling_allocation):\n    with pyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n        alpha = pyro.sample(\"alpha\", dist.MultivariateNormal(\n            prior_mean, covariance_matrix=prior_covariance))\n        \n        poll_results = pyro.sample(\"y\", dist.Binomial(\n            polling_allocation, logits=alpha).to_event(1))\n        \n        dem_win = election_winner(alpha)\n        pyro.sample(\"w\", dist.Delta(dem_win))\n        \n        return poll_results, dem_win, alpha\n```\n\n----------------------------------------\n\nTITLE: Modeling Regional Lineage Growth with Hierarchical Priors in Python\nDESCRIPTION: An alternative regional model using Pyro, allowing growth rates to vary between regions using hierarchical priors. The model captures variability in both initial condition and growth rate for each lineage regionally.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndef regional_model2(counts):\n    T, R, L = counts.shape\n    \n    lineage_plate = pyro.plate(\"lineages\", L, dim=-1)\n    region_plate = pyro.plate(\"region\", R, dim=-2)\n    time_plate = pyro.plate(\"time\", T, dim=-3)\n\n    rate_scale = pyro.sample(\"rate_scale\", dist.LogNormal(-4, 2))\n    init_scale = pyro.sample(\"init_scale\", dist.LogNormal(0, 2))\n    \n    with lineage_plate:\n        rate_loc = pyro.sample(\"rate_loc\", dist.Normal(0, 1))\n        \n    with region_plate, lineage_plate:\n        rate = pyro.sample(\"rate\",  dist.Normal(rate_loc, rate_scale))\n        init = pyro.sample(\"init\", dist.Normal(0, init_scale))\n\n    time = torch.arange(float(T)) * dataset[\"time_step_days\"] / 5.5\n    logits = init + rate * time[:, None, None]\n    \n    with time_plate, region_plate:\n        pyro.sample(\n            \"obs\",\n            dist.Multinomial(logits=logits.unsqueeze(-2), validate_args=False),\n            obs=counts.unsqueeze(-2),\n        )\n```\n\n----------------------------------------\n\nTITLE: Setting up SVI with the Deep Markov Model\nDESCRIPTION: Configures the stochastic variational inference (SVI) process by instantiating the DMM model, setting up a ClippedAdam optimizer with gradient clipping (important for RNNs), and initializing the SVI algorithm with Trace_ELBO as the objective function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# instantiate the dmm\ndmm = DMM(input_dim, z_dim, emission_dim, transition_dim, rnn_dim,\n          args.rnn_dropout_rate, args.num_iafs, args.iaf_dim, args.cuda)\n\n# setup optimizer\nadam_params = {\"lr\": args.learning_rate, \"betas\": (args.beta1, args.beta2),\n               \"clip_norm\": args.clip_norm, \"lrd\": args.lr_decay,\n               \"weight_decay\": args.weight_decay}\noptimizer = ClippedAdam(adam_params)\n```\n\n----------------------------------------\n\nTITLE: Implementing VAE Class with PyTorch\nDESCRIPTION: Implements a VAE class in PyTorch, integrating encoder and decoder networks as submodules. Supports model, guide, and image reconstruction. Allows GPU acceleration based on initialization parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass VAE(nn.Module):\n    # by default our latent space is 50-dimensional\n    # and we use 400 hidden units\n    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n        super().__init__()\n        # create the encoder and decoder networks\n        self.encoder = Encoder(z_dim, hidden_dim)\n        self.decoder = Decoder(z_dim, hidden_dim)\n\n        if use_cuda:\n            # calling cuda() here will put all the parameters of\n            # the encoder and decoder networks into gpu memory\n            self.cuda()\n        self.use_cuda = use_cuda\n        self.z_dim = z_dim\n\n    # define the model p(x|z)p(z)\n    def model(self, x):\n        # register PyTorch module `decoder` with Pyro\n        pyro.module(\"decoder\", self.decoder)\n        with pyro.plate(\"data\", x.shape[0]):\n            # setup hyperparameters for prior p(z)\n            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n            # sample from prior (value will be sampled by guide when computing the ELBO)\n            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n            # decode the latent code z\n            loc_img = self.decoder(z)\n            # score against actual images\n            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n\n    # define the guide (i.e. variational distribution) q(z|x)\n    def guide(self, x):\n        # register PyTorch module `encoder` with Pyro\n        pyro.module(\"encoder\", self.encoder)\n        with pyro.plate(\"data\", x.shape[0]):\n            # use the encoder to get the parameters used to define q(z|x)\n            z_loc, z_scale = self.encoder(x)\n            # sample the latent code z\n            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n\n    # define a helper function for reconstructing images\n    def reconstruct_img(self, x):\n        # encode image x\n        z_loc, z_scale = self.encoder(x)\n        # sample in latent space\n        z = dist.Normal(z_loc, z_scale).sample()\n        # decode the image (note we don't sample in image space)\n        loc_img = self.decoder(z)\n        return loc_img\n```\n\n----------------------------------------\n\nTITLE: Implementing Encoder Network in PyTorch for CVAE\nDESCRIPTION: Defines the encoder network used for both recognition and prior networks in the CVAE. Takes input and output variables, processes them through linear layers to produce mean and scale parameters for the latent distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass Encoder(nn.Module):\n    def __init__(self, z_dim, hidden_1, hidden_2):\n        super().__init__()\n        self.fc1 = nn.Linear(784, hidden_1)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        self.fc31 = nn.Linear(hidden_2, z_dim)\n        self.fc32 = nn.Linear(hidden_2, z_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x, y):\n        # put x and y together in the same image for simplification\n        xc = x.clone()\n        xc[x == -1] = y[x == -1]\n        xc = xc.view(-1, 784)\n        # then compute the hidden units\n        hidden = self.relu(self.fc1(xc))\n        hidden = self.relu(self.fc2(hidden))\n        # then return a mean vector and a (positive) square root covariance\n        # each of size batch_size x z_dim\n        z_loc = self.fc31(hidden)\n        z_scale = torch.exp(self.fc32(hidden))\n        return z_loc, z_scale\n```\n\n----------------------------------------\n\nTITLE: Defining Bayesian Linear Regression Model in Pyro\nDESCRIPTION: This snippet defines a BayesianRegression class using PyroModule. It sets priors on the weights and bias of a linear layer and specifies the forward pass of the model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyro.nn import PyroSample\n\nclass BayesianRegression(PyroModule):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = PyroModule[nn.Linear](in_features, out_features)\n        self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n        self.linear.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n        \n    def forward(self, x, y=None):\n        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n        mean = self.linear(x).squeeze(-1)\n        with pyro.plate(\"data\", x.shape[0]):\n            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n        return mean\n```\n\n----------------------------------------\n\nTITLE: Defining PyroModule for Linear Regression in Python\nDESCRIPTION: This code defines a PyroModule class that subclasses both PyroModule and torch.nn.Linear for use in Bayesian regression with Pyro.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\nfrom pyro.nn import PyroModule\n\nassert issubclass(PyroModule[nn.Linear], nn.Linear)\nassert issubclass(PyroModule[nn.Linear], PyroModule)\n```\n\n----------------------------------------\n\nTITLE: Implementing VAE Evaluation Function in Python\nDESCRIPTION: Defines a function to evaluate the VAE's performance on a test dataset, computing the ELBO loss without gradient steps.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(svi, test_loader, use_cuda=False):\n    # initialize loss accumulator\n    test_loss = 0.\n    # compute the loss over the entire test set\n    for x, _ in test_loader:\n        # if on GPU put mini-batch into CUDA memory\n        if use_cuda:\n            x = x.cuda()\n        # compute ELBO estimate and accumulate loss\n        test_loss += svi.evaluate_loss(x)\n    normalizer_test = len(test_loader.dataset)\n    total_epoch_loss_test = test_loss / normalizer_test\n    return total_epoch_loss_test\n```\n\n----------------------------------------\n\nTITLE: Implementing Normalizing Flow Guide\nDESCRIPTION: Definition of the guide distribution using a conditional neural spline flow from Zuko, configured as an inverse autoregressive flow for efficient sampling.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_flow_guide.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nflow = zuko.flows.NSF(features=3, context=10, transforms=1, hidden_features=(256, 256))\nflow.transform = flow.transform.inv  # inverse autoregressive flow (IAF) are fast to sample from\n\ndef guide(x: Tensor):\n    pyro.module(\"flow\", flow)\n\n    with pyro.plate(\"data\", x.shape[1]):  # amortized\n        pyro.sample(\"z\", ZukoToPyro(flow(x.transpose(0, 1).flatten(-2))))\n```\n\n----------------------------------------\n\nTITLE: Constructing Combiner Module in PyTorch\nDESCRIPTION: The Combiner module parameterizes the distribution q(z_t | z_{t-1}, x_{t:T}). It uses three linear transformations and two non-linearities to compute mean and scale vectors for a diagonal Gaussian distribution. It combines the RNN hidden state with a transformed version of z_{t-1} to output these vectors.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass Combiner(nn.Module):\n    \"\"\"\n    Parameterizes q(z_t | z_{t-1}, x_{t:T}), which is the basic building block\n    of the guide (i.e. the variational distribution). The dependence on x_{t:T} is\n    through the hidden state of the RNN (see the pytorch module `rnn` below)\n    \"\"\"\n    def __init__(self, z_dim, rnn_dim):\n        super().__init__()\n        # initialize the three linear transformations used in the neural network\n        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n        # initialize the two non-linearities used in the neural network\n        self.tanh = nn.Tanh()\n        self.softplus = nn.Softplus()\n\n    def forward(self, z_t_1, h_rnn):\n        \"\"\"\n        Given the latent z at at a particular time step t-1 as well as the hidden\n        state of the RNN h(x_{t:T}) we return the mean and scale vectors that\n        parameterize the (diagonal) gaussian distribution q(z_t | z_{t-1}, x_{t:T})\n        \"\"\"\n        # combine the rnn hidden state with a transformed version of z_t_1\n        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n        # use the combined hidden state to compute the mean used to sample z_t\n        loc = self.lin_hidden_to_loc(h_combined)\n        # use the combined hidden state to compute the scale used to sample z_t\n        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n        # return loc, scale which can be fed into Normal\n        return loc, scale\n```\n\n----------------------------------------\n\nTITLE: Initializing Approximation for Boosting BBVI in Python\nDESCRIPTION: This snippet initializes the approximation for the boosting black box Variational Inference algorithm. It sets up the initial components and weights for the approximation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninitial_approximation = partial(guide, index=0)\ncomponents = [initial_approximation]\nweights = torch.tensor([1.])\nwrapped_approximation = partial(approximation, components=components, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Executing VAE Training Loop in Python\nDESCRIPTION: Implements the main training loop for the VAE, including data loading, model setup, optimization, and periodic evaluation on test data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntrain_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n\n# clear param store\npyro.clear_param_store()\n\n# setup the VAE\nvae = VAE(use_cuda=USE_CUDA)\n\n# setup the optimizer\nadam_args = {\"lr\": LEARNING_RATE}\noptimizer = Adam(adam_args)\n\n# setup the inference algorithm\nsvi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n\ntrain_elbo = []\ntest_elbo = []\n# training loop\nfor epoch in range(NUM_EPOCHS):\n    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n    train_elbo.append(-total_epoch_loss_train)\n    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n\n    if epoch % TEST_FREQUENCY == 0:\n        # report test diagnostics\n        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n        test_elbo.append(-total_epoch_loss_test)\n        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Model with Deterministic Statement\nDESCRIPTION: This function defines a Pyro model using a deterministic statement for the rate parameter and a GammaPoisson distribution for the counts.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef model(features, counts):\n    N, P = features.shape\n    scale = pyro.sample(\"scale\", dist.LogNormal(0, 1))\n    coef = pyro.sample(\"coef\", dist.Normal(0, scale).expand([P]).to_event(1))\n    rate = pyro.deterministic(\"rate\", torch.nn.functional.softplus(coef @ features.T))\n    concentration = pyro.sample(\"concentration\", dist.LogNormal(0, 1))\n    with pyro.plate(\"bins\", N):\n        return pyro.sample(\"counts\", dist.GammaPoisson(concentration, rate), obs=counts)\n```\n\n----------------------------------------\n\nTITLE: Defining a Stochastic Model in Pyro - Python\nDESCRIPTION: This code snippet implements the stochastic model function which samples latent variables and observed data using Pyro's sampling primitives. It uses a loop to recursively sample from the defined distributions, storing the result for each time step. The necessary dependencies include Pyro and PyTorch libraries. Key parameters include `mini_batch`, which contains the input data, and `T_max`, which dictates the time steps for processing sequences.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model(...):\n    z_prev = self.z_0\n\n    # sample the latents z and observed x's one time step at a time\n    for t in range(1, T_max + 1): \n        # the next two lines of code sample z_t ~ p(z_t | z_{t-1}).\n        # first compute the parameters of the diagonal gaussian \n        # distribution p(z_t | z_{t-1})\n        z_loc, z_scale = self.trans(z_prev)\n        # then sample z_t according to dist.Normal(z_loc, z_scale)\n        z_t = pyro.sample(\"z_%d\" % t, dist.Normal(z_loc, z_scale))\n        \n        # compute the probabilities that parameterize the bernoulli likelihood\n        emission_probs_t = self.emitter(z_t)\n        # the next statement instructs pyro to observe x_t according to the\n        # bernoulli distribution p(x_t|z_t)        \n        pyro.sample(\"obs_x_%d\" % t, \n                    dist.Bernoulli(emission_probs_t),\n                    obs=mini_batch[:, t - 1, :])\n        # the latent sampled at this time step will be conditioned upon \n        # in the next time step so keep track of it\n        z_prev = z_t\n```\n\nLANGUAGE: python\nCODE:\n```\ndef model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n\t  mini_batch_seq_lengths, annealing_factor=1.0):\n\n\t# this is the number of time steps we need to process in the mini-batch\n\tT_max = mini_batch.size(1)\n\n\t# register all PyTorch (sub)modules with pyro\n\t# this needs to happen in both the model and guide\n\tpyro.module(\"dmm\", self)\n\n\t# set z_prev = z_0 to setup the recursive conditioning in p(z_t | z_{t-1})\n\tz_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n\n\t# we enclose all the sample statements in the model in a plate.\n\t# this marks that each datapoint is conditionally independent of the others\n\twith pyro.plate(\"z_minibatch\", len(mini_batch)):\n\t\t# sample the latents z and observed x's one time step at a time\n\t\tfor t in range(1, T_max + 1):\n\t\t\t# the next chunk of code samples z_t ~ p(z_t | z_{t-1})\n\t\t\t# note that (both here and elsewhere) we use poutine.scale to take care\n\t\t\t# of KL annealing. we use the mask() method to deal with raggedness\n\t\t\t# in the observed data (i.e. different sequences in the mini-batch\n\t\t\t# have different lengths)\n\n\t\t\t# first compute the parameters of the diagonal gaussian \n\t\t\t# distribution p(z_t | z_{t-1})\n\t\t\tz_loc, z_scale = self.trans(z_prev)\n\n\t\t\t# then sample z_t according to dist.Normal(z_loc, z_scale).\n\t\t\t# note that we use the reshape method so that the univariate \n\t\t\t# Normal distribution is treated as a multivariate Normal \n\t\t\t# distribution with a diagonal covariance.\n\t\t\twith poutine.scale(None, annealing_factor):\n\t\t\t\tz_t = pyro.sample(\"z_%d\" % t,\n\t\t\t\t\t              dist.Normal(z_loc, z_scale)\n\t\t\t\t\t              .mask(mini_batch_mask[:, t - 1:t])\n\t\t\t\t\t              .to_event(1))\n\n\t\t\t# compute the probabilities that parameterize the bernoulli likelihood\n\t\t\temission_probs_t = self.emitter(z_t)\n\t\t\t# the next statement instructs pyro to observe x_t according to the\n\t\t\t# bernoulli distribution p(x_t|z_t)\n\t\t\tpyro.sample(\"obs_x_%d\" % t,\n\t\t\t\t    dist.Bernoulli(emission_probs_t)\n\t\t\t\t\t                .mask(mini_batch_mask[:, t - 1:t])\n\t\t\t\t\t                .to_event(1),\n\t\t\t\t\t    obs=mini_batch[:, t - 1, :])\n\t\t\t# the latent sampled at this time step will be conditioned upon\n\t\t\t# in the next time step so keep track of it\n\t\t\tz_prev = z_t\n```\n\n----------------------------------------\n\nTITLE: Complete Implementation of Boosting BBVI in Python\nDESCRIPTION: This snippet provides the complete implementation of boosting black box Variational Inference, combining all components including the model, guide, RELBO calculation, and the main boosting algorithm.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom collections import defaultdict\nfrom functools import partial\n\nimport numpy as np\nimport pyro\nimport pyro.distributions as dist\nimport scipy.stats\nimport torch\nimport torch.distributions.constraints as constraints\nfrom matplotlib import pyplot\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom pyro.poutine import block, replay, trace\n\n# this is for running the notebook in our testing framework\nn_steps = 2 if smoke_test else 12000\npyro.set_rng_seed(2)\n\n# clear the param store in case we're in a REPL\npyro.clear_param_store()\n\n# Sample observations from a Normal distribution with loc 4 and scale 0.1\nn = torch.distributions.Normal(torch.tensor([4.0]), torch.tensor([0.1]))\ndata = n.sample((100,))\n\n\ndef guide(data, index):\n    scale_q = pyro.param('scale_{}'.format(index), torch.tensor([1.0]), constraints.positive)\n    loc_q = pyro.param('loc_{}'.format(index), torch.tensor([0.0]))\n    pyro.sample(\"z\", dist.Normal(loc_q, scale_q))\n\n\ndef model(data):\n    prior_loc = torch.tensor([0.])\n    prior_scale = torch.tensor([5.])\n    z = pyro.sample('z', dist.Normal(prior_loc, prior_scale))\n    scale = torch.tensor([0.1])\n\n    with pyro.plate('data', len(data)):\n        pyro.sample('x', dist.Normal(z*z, scale), obs=data)\n\n\ndef relbo(model, guide, *args, **kwargs):\n    approximation = kwargs.pop('approximation')\n\n    # We first compute the elbo, but record a guide trace for use below.\n    traced_guide = trace(guide)\n    elbo = pyro.infer.Trace_ELBO(max_plate_nesting=1)\n    loss_fn = elbo.differentiable_loss(model, traced_guide, *args, **kwargs)\n\n    # We do not want to update parameters of previously fitted components\n    # and thus block all parameters in the approximation apart from z.\n    guide_trace = traced_guide.trace\n    replayed_approximation = trace(replay(block(approximation, expose=['z']), guide_trace))\n    approximation_trace = replayed_approximation.get_trace(*args, **kwargs)\n\n    relbo = -loss_fn - approximation_trace.log_prob_sum()\n    \n    # By convention, the negative (R)ELBO is returned.\n    return -relbo\n\n\ndef approximation(data, components, weights):\n    assignment = pyro.sample('assignment', dist.Categorical(weights))\n    result = components[assignment](data)\n    return result\n\n\ndef boosting_bbvi():\n    # T=2\n    n_iterations = 2\n    initial_approximation = partial(guide, index=0)\n    components = [initial_approximation]\n    weights = torch.tensor([1.])\n    wrapped_approximation = partial(approximation, components=components, weights=weights)\n\n    locs = [0]\n    scales = [0]\n\n    for t in range(1, n_iterations + 1):\n        \n        # Create guide that only takes data as argument\n        wrapped_guide = partial(guide, index=t)\n        losses = []\n\n        adam_params = {\"lr\": 0.01, \"betas\": (0.90, 0.999)}\n        optimizer = Adam(adam_params)\n\n        # Pass our custom RELBO to SVI as the loss function.\n        svi = SVI(model, wrapped_guide, optimizer, loss=relbo)\n        for step in range(n_steps):\n            # Pass the existing approximation to SVI.\n            loss = svi.step(data, approximation=wrapped_approximation)\n            losses.append(loss)\n\n            if step % 100 == 0:\n                print('.', end=' ')\n\n        # Update the list of approximation components.\n        components.append(wrapped_guide)\n        \n        # Set new mixture weight.\n        new_weight = 2 / (t + 1)\n\n        # In this specific case, we set the mixture weight of the second component to 0.5.\n        if t == 2:\n            new_weight = 0.5\n        weights = weights * (1-new_weight)\n        weights = torch.cat((weights, torch.tensor([new_weight])))\n\n        # Update the approximation\n        wrapped_approximation = partial(approximation, components=components, weights=weights)\n\n        print('Parameters of component {}:'.format(t))\n        scale = pyro.param(\"scale_{}\".format(t)).item()\n        scales.append(scale)\n        loc = pyro.param(\"loc_{}\".format(t)).item()\n        locs.append(loc)\n        print('loc = {}'.format(loc))\n        print('scale = {}'.format(scale))\n\n\n    # Plot the resulting approximation\n    X = np.arange(-10, 10, 0.1)\n    pyplot.figure(figsize=(10, 4), dpi=100).set_facecolor('white')\n    total_approximation = np.zeros(X.shape)\n    for i in range(1, n_iterations + 1):\n        Y = weights[i].item() * scipy.stats.norm.pdf((X - locs[i]) / scales[i])\n        pyplot.plot(X, Y)\n        total_approximation += Y\n    pyplot.plot(X, total_approximation)\n    pyplot.plot(data.data.numpy(), np.zeros(len(data)), 'k*')\n    pyplot.title('Approximation of posterior over z')\n    pyplot.ylabel('probability density')\n    pyplot.show()\n\nif __name__ == '__main__':\n    boosting_bbvi()\n```\n\n----------------------------------------\n\nTITLE: Defining Guide Function in Pyro\nDESCRIPTION: Example showing how to define a guide function that matches the model's latent variables, which is required for variational inference.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef guide():\n    pyro.sample(\"z_1\", ...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Helper Function for Pyro Models\nDESCRIPTION: Defines a helper function to train Pyro models using Stochastic Variational Inference (SVI) with Adam optimizer and Trace_ELBO as the loss function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mle_map.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train(model, guide, lr=0.005, n_steps=201):\n    pyro.clear_param_store()\n    adam_params = {\"lr\": lr}\n    adam = pyro.optim.Adam(adam_params)\n    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n\n    for step in range(n_steps):\n        loss = svi.step(data)\n        if step % 50 == 0:\n            print('[iter {}]  loss: {:.4f}'.format(step, loss))\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running SVI with Pyro\nDESCRIPTION: This code initializes the data view, sets up the SVI instance with the model, guide, optimizer, and loss function, and runs the inference process for a defined number of steps. The code showcases the use of TraceGraph_ELBO for variance reduction in gradient estimates.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = mnist.view(-1, 50 * 50)\n\nsvi = SVI(model,\n          guide,\n          optim.Adam({'lr': 1e-4}),\n          loss=TraceGraph_ELBO())\n\nfor i in range(5):\n    loss = svi.step(data)\n    print('i={}, elbo={:.2f}'.format(i, loss / data.size(0)))\n```\n\n----------------------------------------\n\nTITLE: Implementing Next Candidate Selection in Bayesian Optimization\nDESCRIPTION: Function that implements the inner loop of Bayesian Optimization to select the next candidate point for evaluation. It generates multiple candidates and selects the one that minimizes the lower confidence bound.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef next_x(lower_bound=0, upper_bound=1, num_candidates=5):\n    candidates = []\n    values = []\n\n    x_init = gpmodel.X[-1:]\n    for i in range(num_candidates):\n        x = find_a_candidate(x_init, lower_bound, upper_bound)\n        y = lower_confidence_bound(x)\n        candidates.append(x)\n        values.append(y)\n        x_init = x.new_empty(1).uniform_(lower_bound, upper_bound)\n\n    argmin = torch.min(torch.cat(values), dim=0)[1].item()\n    return candidates[argmin]\n```\n\n----------------------------------------\n\nTITLE: Configuring the SVI Algorithm with Trace_ELBO\nDESCRIPTION: Sets up the Stochastic Variational Inference (SVI) algorithm using the model and guide methods from the DMM class, the ClippedAdam optimizer, and the Trace_ELBO objective which calculates the Evidence Lower Bound using Monte Carlo estimation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# setup inference algorithm\nsvi = SVI(dmm.model, dmm.guide, optimizer, Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Gaussian Model in Pyro\nDESCRIPTION: Creates a simple Gaussian model with location and scale parameters, and an AutoDiagonalNormal guide for inference. This model will be used to demonstrate JIT compilation in Pyro.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    loc = pyro.sample(\"loc\", dist.Normal(0., 10.))\n    scale = pyro.sample(\"scale\", dist.LogNormal(0., 3.))\n    with pyro.plate(\"data\", data.size(0)):\n        pyro.sample(\"obs\", dist.Normal(loc, scale), obs=data)\n\nguide = AutoDiagonalNormal(model)\n\ndata = dist.Normal(0.5, 2.).sample((100,))\n```\n\n----------------------------------------\n\nTITLE: Visualizing ELBO Convergence and Autocorrelation in Pyro BBVI\nDESCRIPTION: Creates a two-panel plot showing the ELBO (Evidence Lower Bound) value over iterations and its autocorrelation, providing insights into the convergence behavior of Variational Inference on sunspot data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nelbo_plot = plt.figure(figsize=(15, 5))\n\nelbo_ax = elbo_plot.add_subplot(1, 2, 1)\nelbo_ax.set_title(\"ELBO Value vs. Iteration Number for Pyro BBVI on Sunspot Data\")\nelbo_ax.set_ylabel(\"ELBO\")\nelbo_ax.set_xlabel(\"Iteration Number\")\nelbo_ax.plot(np.arange(n_iter), losses)\n\nautocorr_ax = elbo_plot.add_subplot(1, 2, 2)\nautocorr_ax.acorr(np.asarray(losses), detrend=lambda x: x - x.mean(), maxlags=750, usevlines=False, marker=',')\nautocorr_ax.set_xlim(0, 500)\nautocorr_ax.axhline(0, ls=\"--\", c=\"k\", lw=1)\nautocorr_ax.set_title(\"Autocorrelation of ELBO vs. Lag for Pyro BBVI on Sunspot Data\")\nautocorr_ax.set_xlabel(\"Lag\")\nautocorr_ax.set_ylabel(\"Autocorrelation\")\nelbo_plot.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing an Amortized Guide with EasyGuide in Pyro\nDESCRIPTION: This function demonstrates an amortized guide using EasyGuide. It predicts latent variables as an affine function of observed data, allowing the guide to handle new data more effectively. It uses a neural network to learn the mapping from observations to latent variables.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/easyguide.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@easy_guide(model)\ndef guide(self, batch, subsample, full_size):\n    num_time_steps, batch_size = batch.shape\n    self.map_estimate(\"drift\")\n\n    group = self.group(match=\"state_[0-9]*\")\n    cov_diag = pyro.param(\"state_cov_diag\",\n                          lambda: torch.full(group.event_shape, 0.01),\n                          constraint=constraints.positive)\n    cov_factor = pyro.param(\"state_cov_factor\",\n                            lambda: torch.randn(group.event_shape + (rank,)) * 0.01)\n\n    # Predict latent propensity as an affine function of observed data.\n    if not hasattr(self, \"nn\"):\n        self.nn = torch.nn.Linear(group.event_shape.numel(), group.event_shape.numel())\n        self.nn.weight.data.fill_(1.0 / num_time_steps)\n        self.nn.bias.data.fill_(-0.5)\n    pyro.module(\"state_nn\", self.nn)\n    with self.plate(\"data\", full_size, subsample=subsample):\n        loc = self.nn(batch.t())\n        group.sample(\"states\", dist.LowRankMultivariateNormal(loc, cov_factor, cov_diag))\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic HMM Model in Pyro\nDESCRIPTION: Defines a basic Hidden Markov Model with latent states and observations. Uses Dirichlet distributions for transition and emission probabilities, with parallel enumeration for state tracking.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndata_dim = 4\nnum_steps = 10\ndata = dist.Categorical(torch.ones(num_steps, data_dim)).sample()\n\ndef hmm_model(data, data_dim, hidden_dim=10):\n    print(f\"Running for {len(data)} time steps\")\n    # Sample global matrices wrt a Jeffreys prior.\n    with pyro.plate(\"hidden_state\", hidden_dim):\n        transition = pyro.sample(\"transition\", dist.Dirichlet(0.5 * torch.ones(hidden_dim)))\n        emission = pyro.sample(\"emission\", dist.Dirichlet(0.5 * torch.ones(data_dim)))\n\n    x = 0  # initial state\n    for t, y in enumerate(data):\n        x = pyro.sample(f\"x_{t}\", dist.Categorical(transition[x]),\n                        infer={\"enumerate\": \"parallel\"})\n        pyro.sample(f\"  y_{t}\", dist.Categorical(emission[x]), obs=y)\n        print(f\"  x_{t}.shape = {x.shape}\")\n```\n\n----------------------------------------\n\nTITLE: Defining VAE Training Loop Function in Python\nDESCRIPTION: Implements a function to train the VAE using SVI over mini-batches of data, supporting optional CUDA acceleration.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef train(svi, train_loader, use_cuda=False):\n    # initialize loss accumulator\n    epoch_loss = 0.\n    # do a training epoch over each mini-batch x returned\n    # by the data loader\n    for x, _ in train_loader:\n        # if on GPU put mini-batch into CUDA memory\n        if use_cuda:\n            x = x.cuda()\n        # do ELBO gradient and accumulate loss\n        epoch_loss += svi.step(x)\n\n    # return epoch loss\n    normalizer_train = len(train_loader.dataset)\n    total_epoch_loss_train = epoch_loss / normalizer_train\n    return total_epoch_loss_train\n```\n\n----------------------------------------\n\nTITLE: Setting Up GPLVM Priors and Model in Pyro\nDESCRIPTION: This snippet sets up the priors for the GPLVM model, including the latent space dimensions and capture time. It then initializes the SparseGPRegression model with the specified kernel and parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/gplvm.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncapture_time = y.new_tensor([int(cell_name.split(\" \")[0]) for cell_name in df.index.values])\n# we scale the time into the interval [0, 1]\ntime = capture_time.log2() / 6\n\n# we setup the mean of our prior over X\nX_prior_mean = torch.zeros(y.size(1), 2)  # shape: 437 x 2\nX_prior_mean[:, 0] = time\n\nkernel = gp.kernels.RBF(input_dim=2, lengthscale=torch.ones(2))\n\n# we clone here so that we don't change our prior during the course of training\nX = Parameter(X_prior_mean.clone())\n\n# we will use SparseGPRegression model with num_inducing=32;\n# initial values for Xu are sampled randomly from X_prior_mean\nXu = stats.resample(X_prior_mean.clone(), 32)\ngplvm = gp.models.SparseGPRegression(X, y, kernel, Xu, noise=torch.tensor(0.01), jitter=1e-5)\n\n# we use `.to_event()` to tell Pyro that the prior distribution for X has no batch_shape\ngplvm.X = pyro.nn.PyroSample(dist.Normal(X_prior_mean, 0.1).to_event())\ngplvm.autoguide(\"X\", dist.Normal)\n```\n\n----------------------------------------\n\nTITLE: Using Vectorized plate for Conditional Independence in Pyro\nDESCRIPTION: Implementation using vectorized `plate` to mark conditional independence, which processes the entire data tensor at once rather than iterating through individual elements, potentially offering significant speed improvements.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith pyro.plate('observe_data'):\n    pyro.sample('obs', dist.Bernoulli(f), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Generating Data from the Time Series Model\nDESCRIPTION: This snippet generates synthetic data directly from the previously defined time series model. It sets random seed for reproducibility and creates a dataset with specified dimensions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/easyguide.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfull_size = 100\nnum_time_steps = 7\npyro.set_rng_seed(123456789)\ndata = model([None] * num_time_steps, torch.arange(full_size), full_size)\nassert data.shape == (num_time_steps, full_size)\n```\n\n----------------------------------------\n\nTITLE: Configuring ELBO with Multiple Particles\nDESCRIPTION: Configuration of Trace_ELBO with multiple particles to reduce variance in gradient estimates at the cost of increased computation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nelbo = pyro.infer.Trace_ELBO(num_particles=10, \n                             vectorize_particles=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Bernoulli Emission Network in PyTorch\nDESCRIPTION: Defines an Emitter neural network that parameterizes the bernoulli observation likelihood p(x_t | z_t). The network consists of three linear transformations with ReLU and sigmoid activations, outputting probabilities for each note.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Emitter(nn.Module):\n    \"\"\"\n    Parameterizes the bernoulli observation likelihood p(x_t | z_t)\n    \"\"\"\n    def __init__(self, input_dim, z_dim, emission_dim):\n        super().__init__()\n        # initialize the three linear transformations used in the neural network\n        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n        # initialize the two non-linearities used in the neural network\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z_t):\n        \"\"\"\n        Given the latent z at a particular time step t we return the vector of \n        probabilities `ps` that parameterizes the bernoulli distribution p(x_t|z_t)\n        \"\"\"\n        h1 = self.relu(self.lin_z_to_hidden(z_t))\n        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n        ps = self.sigmoid(self.lin_hidden_to_input(h2))\n        return ps\n```\n\n----------------------------------------\n\nTITLE: Constructing PyTorch Optimizer and Dataloader for SVI\nDESCRIPTION: This code snippet showcases how to set up a PyTorch optimizer and dataloader using argparse. It requires the PyTorch library and the argparse module to run optimizations based on user-defined parameters such as size, batch size, and number of epochs. The expected inputs are command-line arguments that configure these parameters, and the output is an initialized SVI setup ready for training.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_torch.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n$ python examples/svi_torch.py --size 10000 --batch-size 100 --num-epochs 100\n```\n\n----------------------------------------\n\nTITLE: Creating a Parametrized Guide Function in Pyro\nDESCRIPTION: Implements a guide function using pyro.param to create learnable parameters. This guide can be optimized during variational inference to approximate the posterior distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef scale_parametrized_guide(guess):\n    a = pyro.param(\"a\", torch.tensor(guess))\n    b = pyro.param(\"b\", torch.tensor(1.))\n    return pyro.sample(\"weight\", dist.Normal(a, torch.abs(b)))\n```\n\n----------------------------------------\n\nTITLE: Performing SIR Model Inference\nDESCRIPTION: Demonstrates inference on the SIR model using both SVI (Stochastic Variational Inference) and MCMC methods. Creates a new model instance from synthetic observations and performs parameter estimation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nobs = synth_data[\"obs\"]\nmodel = SimpleSIRModel(population, recovery_time, obs)\n\n%%time\nlosses = model.fit_svi(num_steps=101 if smoke_test else 2001,\n                       jit=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Learning Rate Decay with ClippedAdam\nDESCRIPTION: Configures the ClippedAdam optimizer with built-in learning rate decay, gradually reducing the learning rate from initial_lr to gamma*initial_lr over num_steps.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnum_steps = 1000\ninitial_lr = 0.001\ngamma = 0.1  # final learning rate will be gamma * initial_lr\nlrd = gamma ** (1 / num_steps)\noptim = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n```\n\n----------------------------------------\n\nTITLE: Implementing EnumMessenger for Discrete Variable Enumeration\nDESCRIPTION: Creates a custom messenger class that handles parallel enumeration of discrete latent variables by converting between funsor and PyTorch tensors.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.contrib.funsor.handlers.named_messenger import NamedMessenger\n\nclass EnumMessenger(NamedMessenger):\n    \n    @pyroapi.pyro_backend(\"contrib.funsor\")  # necessary since we invoke pyro.to_data and pyro.to_funsor\n    def _pyro_sample(self, msg):\n        if msg[\"done\"] or msg[\"is_observed\"] or msg[\"infer\"].get(\"enumerate\") != \"parallel\":\n            return\n\n        raw_value = msg[\"fn\"].enumerate_support(expand=False)\n        \n        funsor_value = pyro.to_funsor(\n            raw_value,\n            output=funsor.Bint[raw_value.shape[0]],\n            dim_to_name={-raw_value.dim(): msg[\"name\"]},\n        )\n\n        msg[\"value\"] = pyro.to_data(funsor_value)\n        msg[\"done\"] = True\n```\n\n----------------------------------------\n\nTITLE: Defining a Probabilistic Model for Log Returns\nDESCRIPTION: This snippet defines a probabilistic model using Pyro that parameterizes the stability, skew, and scale of a Stable distribution to fit log returns. The model encapsulates the observed data, allowing for parameter optimization and inference.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    stability = pyro.param(\"stability\", torch.tensor(1.9),\n                           constraint=constraints.interval(0, 2))\n    skew = 0.\n    scale = pyro.param(\"scale\", torch.tensor(0.1), constraint=constraints.positive)\n    loc = pyro.param(\"loc\", torch.tensor(0.))\n    with pyro.plate(\"data\", len(r)):\n        return pyro.sample(\"r\", dist.Stable(stability, skew, scale, loc), obs=r)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Pyro Model with Parallel Enumeration\nDESCRIPTION: This snippet demonstrates a Pyro model (model3) that uses parallel enumeration for discrete latent variables. It shows how different variables are sampled and how their shapes are affected by enumeration.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@config_enumerate\ndef model3():\n    p = pyro.param(\"p\", torch.arange(6.) / 6)\n    locs = pyro.param(\"locs\", torch.tensor([-1., 1.]))\n\n    a = pyro.sample(\"a\", Categorical(torch.ones(6) / 6))\n    b = pyro.sample(\"b\", Bernoulli(p[a]))  # Note this depends on a.\n    with pyro.plate(\"c_plate\", 4):\n        c = pyro.sample(\"c\", Bernoulli(0.3))\n        with pyro.plate(\"d_plate\", 5):\n            d = pyro.sample(\"d\", Bernoulli(0.4))\n            e_loc = locs[d.long()].unsqueeze(-1)\n            e_scale = torch.arange(1., 8.)\n            e = pyro.sample(\"e\", Normal(e_loc, e_scale)\n                            .to_event(1))  # Note this depends on d.\n\n    #                   enumerated|batch|event dims\n    assert a.shape == (         6, 1, 1   )  # Six enumerated values of the Categorical.\n    assert b.shape == (      2, 1, 1, 1   )  # Two enumerated Bernoullis, unexpanded.\n    assert c.shape == (   2, 1, 1, 1, 1   )  # Only two Bernoullis, unexpanded.\n    assert d.shape == (2, 1, 1, 1, 1, 1   )  # Only two Bernoullis, unexpanded.\n    assert e.shape == (2, 1, 1, 1, 5, 4, 7)  # This is sampled and depends on d.\n\n    assert e_loc.shape   == (2, 1, 1, 1, 1, 1, 1,)\n    assert e_scale.shape == (                  7,)\n            \ntest_model(model3, model3, TraceEnum_ELBO(max_plate_nesting=2))\n```\n\n----------------------------------------\n\nTITLE: Basic Logistic Growth Model Implementation\nDESCRIPTION: Implements a basic logistic growth model using Pyro for modeling lineage proportions over time. Uses multinomial likelihood and includes parameters for growth rate and initial conditions per lineage.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef basic_model(counts):\n    T, L = counts.shape\n\n    # Define plates over lineage and time\n    lineage_plate = pyro.plate(\"lineages\", L, dim=-1)\n    time_plate = pyro.plate(\"time\", T, dim=-2)\n\n    # Define a growth rate (i.e. slope) and an init (i.e. intercept) for each lineage\n    with lineage_plate:\n        rate = pyro.sample(\"rate\", dist.Normal(0, 1))\n        init = pyro.sample(\"init\", dist.Normal(0, 1))\n\n    # We measure time in units of the SARS-CoV-2 generation time of 5.5 days\n    time = torch.arange(float(T)) * dataset[\"time_step_days\"] / 5.5\n    \n    # Assume lineages grow linearly in logit space\n    logits = init + rate * time[:, None]\n    \n    # We use the softmax function to define normalized probabilities from the logits\n    probs = torch.softmax(logits, dim=-1)\n    assert probs.shape == (T, L)\n    \n    # Observe counts via a multinomial likelihood.\n    with time_plate:\n        pyro.sample(\n            \"obs\",\n            dist.Multinomial(probs=probs.unsqueeze(-2), validate_args=False),\n            obs=counts.unsqueeze(-2),\n        )\n```\n\n----------------------------------------\n\nTITLE: Forecasting with SIR Model\nDESCRIPTION: Shows how to generate forecasts using the trained SIR model. Uses the predict method with a forecast window to generate future predictions and confidence intervals.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%time\nsamples = model.predict(forecast=30)\n```\n\n----------------------------------------\n\nTITLE: Gaussian Mixture Model with Enumeration\nDESCRIPTION: Implementation of a Gaussian mixture model using enumeration and plates. Shows shared variance and different means across components.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@config_enumerate\ndef model(data, num_components=3):\n    print(f\"  Running model with {len(data)} data points\")\n    p = pyro.sample(\"p\", dist.Dirichlet(0.5 * torch.ones(num_components)))\n    scale = pyro.sample(\"scale\", dist.LogNormal(0, num_components))\n    with pyro.plate(\"components\", num_components):\n        loc = pyro.sample(\"loc\", dist.Normal(0, 10))\n    with pyro.plate(\"data\", len(data)):\n        x = pyro.sample(\"x\", dist.Categorical(p))\n        print(\"    x.shape = {}\".format(x.shape))\n        pyro.sample(\"obs\", dist.Normal(loc[x], scale), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Training Loop Implementation for DMM\nDESCRIPTION: Main training loop that processes mini-batches, tracks loss, and reports training diagnostics. Includes epoch timing and loss normalization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntimes = [time.time()]\nfor epoch in range(args.num_epochs):\n    # accumulator for our estimate of the negative log likelihood \n    # (or rather -elbo) for this epoch\n    epoch_nll = 0.0 \n    # prepare mini-batch subsampling indices for this epoch\n    shuffled_indices = np.arange(N_train_data)\n    np.random.shuffle(shuffled_indices)\n\n    # process each mini-batch; this is where we take gradient steps\n    for which_mini_batch in range(N_mini_batches):\n        epoch_nll += process_minibatch(epoch, which_mini_batch, shuffled_indices)\n\n    # report training diagnostics\n    times.append(time.time())\n    epoch_time = times[-1] - times[-2]\n    log(\"[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)\" %\n        (epoch, epoch_nll / N_train_time_slices, epoch_time))\n```\n\n----------------------------------------\n\nTITLE: Performing SVI Optimization\nDESCRIPTION: This code sets up and runs Stochastic Variational Inference optimization using the Adam optimizer and the defined guide.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npyro.clear_param_store()\n\nadam_params = {\"lr\": 0.005, \"betas\": (0.90, 0.999)}\noptimizer = Adam(adam_params)\n\nsvi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n\n%%time\n\nn_steps = 5001\n\nfor step in range(n_steps):\n    loss = svi.step(X_, y_)\n    if step % 1000 == 0:\n        print('Loss: ', loss)\n```\n\n----------------------------------------\n\nTITLE: Implementing an Analytically Derived Guide Function\nDESCRIPTION: Creates a perfect guide function that represents the analytically derived posterior distribution for the weight given a measurement. This guide can be used with Pyro's inference algorithms to approximate the true posterior.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef perfect_guide(guess):\n    loc = (0.75**2 * guess + 9.5) / (1 + 0.75**2)  # 9.14\n    scale = np.sqrt(0.75**2 / (1 + 0.75**2))  # 0.6\n    return pyro.sample(\"weight\", dist.Normal(loc, scale))\n```\n\n----------------------------------------\n\nTITLE: Implementing a Toy Mixture Model with Discrete Enumeration in Pyro\nDESCRIPTION: Complete implementation of a toy mixture model using discrete enumeration in Pyro. The code demonstrates model definition, inference with enumeration, training, and evaluation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/toy_mixture_model_discrete_enumeration.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"\nThis example illustrates how to use discrete enumeration in a mixture model.\n\"\"\"\n\nimport argparse\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.ops.indexing import Vindex\n\n\n# Parser to allow for command-line specification of seed, etc.\ndef set_args():\n    parser = argparse.ArgumentParser(description=\"Toy mixture model\")\n    parser.add_argument(\"-n\", \"--num-steps\", nargs=\"?\", default=1000, type=int)\n    parser.add_argument(\"--cuda\", action=\"store_true\")\n    parser.add_argument(\"-s\", \"--seed\", nargs=\"?\", default=42, type=int)\n    parser.add_argument(\"-d\", \"--dataset\", nargs=\"?\", default=\"1\", type=str)\n    parser.add_argument(\n        \"--tmc\", action=\"store_true\", help=\"Use Tensor Monte Carlo instead of exact enumeration\"\n    )\n    args = parser.parse_args()\n    args.device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n    return args\n\n\n# Our data x consists of two non-enumerable continuous random variables\n# and a label y that takes on one of two values, 0 or 1, that we will\n# enumerate over in the guide. Each label value has a distinctive pattern\n# on the x values.\n#\n# Imagine that the two label values are 'shopper' and 'robber',\n# and the two continuous values are (time_in_shop, time_in_carpark).\n# For shoppers, the pattern is to spend some time in the shop and\n# less time in the carpark. For robbers, the pattern is to spend little\n# or no time in the shop and some time in the carpark.\n#\n# We want to learn a model of the form p(x, y) that gives the joint\n# probabilities.\n# First we generate the dataset p(y)p(x|y).\n# Then we use enumeration to compute the exact\n# marginal likelihood p(x) used in p(y|x) = p(x|y)p(y)/p(x) and\n# in the ELBO.\ndef setup_data(dataset_id, device=None):\n    if dataset_id == \"1\":  # shorter parking time for shoppers\n        shopper_time = torch.tensor([[2.0, 1.0], [4.0, 2.0], [3.0, 1.0], [2.0, 2.0], [4.0, 1.0]])\n        robber_time = torch.tensor([[0.5, 3.0], [0.0, 3.0], [0.7, 2.0], [0.3, 3.0], [0.2, 3.0]])\n    elif dataset_id == \"2\":  # longer parking time for shoppers\n        shopper_time = torch.tensor([[2.0, 3.0], [4.0, 6.0], [3.0, 3.0], [2.0, 4.0], [4.0, 5.0]])\n        robber_time = torch.tensor([[0.5, 0.8], [0.0, 1.0], [0.7, 0.3], [0.3, 0.4], [0.2, 1.0]])\n    else:\n        argparse.error(\"Invalid dataset id: %s. Expecting 1 or 2.\" % args.dataset)\n\n    # Put all the data together\n    data = {}\n    data[\"x\"] = torch.cat([shopper_time, robber_time]).to(device)\n    # Create labels\n    data[\"y\"] = torch.tensor([0] * len(shopper_time) + [1] * len(robber_time), device=device)\n    return data\n\n\n# We'll define a helper function to generate samples from the\n# posterior over y's. We'll use this to illustrate enumeration.\ndef get_posterior_pred(model, guide, x, y):\n    trace = pyro.poutine.trace(model).get_trace(x, None)\n    posterior_predictive = []\n    for i in range(len(x)):\n        # get the marginal distribution over y\n        d = trace.nodes[\"y\"][\"fn\"]\n        # sample from the posterior marginal\n        probs_y = (d.probs * torch.tensor([int(j == y[i].item()) for j in range(2)]))\n        probs_y = probs_y / probs_y.sum()\n        posterior_predictive.append(probs_y)\n    return torch.stack(posterior_predictive)\n\n\n# We'll define a simple mixture model where the mixture components\n# are Gaussians with known variance. The latent variable in this case\n# corresponds to the mixture component id. Since it takes on two values\n# 0 and 1 we'll use a Bernoulli distribution for it.\ndef model(data, idx=None):\n    # Create the parameters for the mixture model\n    if idx is not None:\n        x = torch.stack([data[\"x\"][i] for i in idx])\n        y = torch.stack([data[\"y\"][i] for i in idx]) if data[\"y\"] is not None else None\n    else:\n        x = data[\"x\"]\n        y = data[\"y\"]\n\n    # The prior on the mixture weight, a Beta distribution\n    mixture_weight = pyro.sample(\"mixture_weight\", dist.Beta(1.0, 1.0))\n\n    # The mixture assignments, a Bernoulli distribution with no plate\n    y = pyro.sample(\"y\", dist.Bernoulli(mixture_weight), obs=y)\n\n    # Conditionally independent, within the mixture components, over data points\n    with pyro.plate(\"data\", x.shape[0]):\n        assignment = y\n\n        # We'll make some 'shopper' and 'robber' parameters.\n        # In particular, the time in the shop and in the parking lot.\n        # We'll use the mixture component indicator to select the appropriate\n        # parameters and thus, e.g., robbers spend less time in the shop\n        # and more time in the parking lot.\n        # (These will be estimated from the data using probabilistic inference.)\n        shop_time_mu = pyro.param(\n            \"shop_time_mu\",\n            torch.tensor(\n                [\n                    2.0,  # mean time in shop for shoppers\n                    0.6,  # mean time in shop for robbers\n                ]\n            ),\n        )\n        shop_time_sigma = 0.3\n\n        # Vindex helps us select elements from the shop_time_mu tensor:\n        # Vindex selects in the 0th dimension to get the right mu.\n        # Note that in the following line, assignment is a tensor with n elements,\n        # but shop_time_mu only has 2 elements.\n        # Vindex handles the correct mapping, i.e. the equivalent of:\n        # torch.tensor([shop_time_mu[0] if y_i==0 else shop_time_mu[1] for y_i in assignment])\n        # This can allow for a cleaner API (instead of torch index_select).\n        # It also helps the backend enumeration handling.\n        mu_shop = Vindex(shop_time_mu)[assignment]\n\n        park_time_mu = pyro.param(\n            \"park_time_mu\",\n            torch.tensor(\n                [\n                    1.0,  # mean time in park for shoppers\n                    2.0,  # mean time in park for robbers\n                ]\n            ),\n        )\n        park_time_sigma = 0.3\n\n        mu_park = Vindex(park_time_mu)[assignment]\n\n        # Both shoppers and robbers spend some time in the shop\n        x_shop = pyro.sample(\"tshop\", dist.Normal(mu_shop, shop_time_sigma), obs=x[:, 0])\n        # Both shoppers and robbers spend some time in the parking lot\n        x_park = pyro.sample(\"tpark\", dist.Normal(mu_park, park_time_sigma), obs=x[:, 1])\n\n\n# Now we can turn this model into a marginal guided model over y,\n# where the guide is a parameterized Beta distribution.\n# We'll use the original model but parameterize the probability of y directly.\ndef main(args):\n    pyro.set_rng_seed(args.seed)\n\n    # Clear param store\n    pyro.clear_param_store()\n\n    # Dataset\n    data = setup_data(args.dataset, device=args.device)\n\n    # training set - all the unlabeled portions of the data\n    full_training_data = {\"x\": data[\"x\"], \"y\": data[\"y\"]}\n\n    # Setup the optimizer\n    adam_params = {\"lr\": 0.0005, \"betas\": (0.9, 0.999)}\n    optimizer = pyro.optim.Adam(adam_params)\n\n    # Setup the inference algorithm\n    elbos = {}\n    if args.tmc:\n        elbo = TraceTMC_ELBO(max_plate_nesting=1)\n        tmc_model = model\n        guide = AutoDelta(tmc_model)\n        elbos[\"tmc\"] = elbo.differentiable_loss\n    else:\n        from functools import partial\n\n        def enum_model(*args, **kwargs):\n            return pyro.infer.config_enumerate(model, \"parallel\")(*args, **kwargs)\n\n        elbo = TraceEnum_ELBO(max_plate_nesting=1)\n        guide = AutoDelta(enum_model)\n        elbos[\"enum\"] = elbo.differentiable_loss\n\n    # Do the training\n    for name, elbo in elbos.items():\n        losses = []\n        print(\"Inference method: {}\".format(name))\n        svi = SVI(model, guide, optimizer, loss=elbo)\n\n        for step in range(args.num_steps):\n            loss = svi.step(full_training_data)\n            losses.append(loss)\n            if step % 50 == 0:\n                print(\"[{}] Elbo loss: {}\".format(step, loss))\n\n        # Report the final parameters\n        print(\"\\nloss={}\".format(loss))\n        print(\n            \"\\nshop time for shoppers = {}\"\n            .format(pyro.param(\"shop_time_mu\")[0])\n        )\n        print(\n            \"shop time for robbers = {}\"\n            .format(pyro.param(\"shop_time_mu\")[1])\n        )\n        print(\n            \"park time for shoppers = {}\"\n            .format(pyro.param(\"park_time_mu\")[0])\n        )\n        print(\n            \"park time for robbers = {}\"\n            .format(pyro.param(\"park_time_mu\")[1])\n        )\n\n        # Plot the loss\n        plt.figure(figsize=(10, 3), dpi=100).set_facecolor(\"white\")\n        plt.plot(losses)\n        plt.xlabel(\"iters\")\n        plt.ylabel(\"loss\")\n        plt.yscale(\"log\")\n        plt.title(\"Convergence of SVI\")\n        plt.savefig(\"{}.png\".format(name))\n        plt.close()\n\n        # Compute summary statistics\n        preds = get_posterior_pred(model, guide, data[\"x\"], data[\"y\"])\n        shoppers = preds[:5, 0].sum() / 5\n        robbers = preds[5:, 1].sum() / 5\n\n        # accuracy\n        print(\"probability of shoppers (should be close to 1) = {}\".format(shoppers))\n        print(\"probability of robbers (should be close to 1) = {}\".format(robbers))\n\n\nif __name__ == \"__main__\":\n    assert pyro.__version__.startswith(\"1.8.6\")\n    args = set_args()\n    main(args)\n\n```\n\n----------------------------------------\n\nTITLE: Visualizing GPLVM Results on Single-Cell qPCR Data in Python\nDESCRIPTION: This snippet creates a scatter plot to visualize the learned latent space from the GPLVM model. It uses different colors for different cell stages and shows how the model has captured the structure in the data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/gplvm.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(8, 6))\ncolors = plt.get_cmap(\"tab10\").colors[::-1]\nlabels = df.index.unique()\n\nX = gplvm.X_loc.detach().numpy()\nfor i, label in enumerate(labels):\n    X_i = X[df.index == label]\n    plt.scatter(X_i[:, 0], X_i[:, 1], c=[colors[i]], label=label)\n\nplt.legend()\nplt.xlabel(\"pseudotime\", fontsize=14)\nplt.ylabel(\"branching\", fontsize=14)\nplt.title(\"GPLVM on Single-Cell qPCR data\", fontsize=16)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Training Loop for Bayesian Regression with Pyro SVI\nDESCRIPTION: This snippet shows the training loop for the Bayesian regression model using SVI. It iterates through the data, calculates the loss, and takes gradient steps to optimize the ELBO.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\npyro.clear_param_store()\nfor j in range(num_iterations):\n    # calculate the loss and take a gradient step\n    loss = svi.step(x_data, y_data)\n    if j % 100 == 0:\n        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))\n```\n\n----------------------------------------\n\nTITLE: Updating Gaussian Process Posterior with New Evaluations\nDESCRIPTION: Defines a helper function that updates the Gaussian Process model with new evaluation points, retrains the model to optimize hyperparameters using Adam optimizer.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef update_posterior(x_new):\n    y = f(x_new) # evaluate f at new point.\n    X = torch.cat([gpmodel.X, x_new]) # incorporate new evaluation \n    y = torch.cat([gpmodel.y, y])     \n    gpmodel.set_data(X, y)\n    # optimize the GP hyperparameters using Adam with lr=0.001\n    optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.001)\n    gp.util.train(gpmodel, optimizer)\n```\n\n----------------------------------------\n\nTITLE: Extended Response Time Model\nDESCRIPTION: Implements an extended model that includes response time modeling using LogNormal distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntime_intercept = 0.5\ntime_scale = 0.5\n\ndef model(l):\n    theta = pyro.sample(\"theta\", dist.Normal(prior_mean, prior_sd))\n    logit_p = sensitivity * (theta - l)\n    correct = pyro.sample(\"correct\", dist.Bernoulli(logits=logit_p))\n    mean_log_time = time_intercept + time_scale * (theta - l)\n    time = pyro.sample(\"time\", dist.LogNormal(mean_log_time, 1.0))\n    return correct, time\n```\n\n----------------------------------------\n\nTITLE: Defining Forrester Objective Function in Python\nDESCRIPTION: Implements the Forrester et al. (2008) function which serves as the objective function to be optimized. This function has both local and global minima, making it a good test case for Bayesian optimization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return (6 * x - 2)**2 * torch.sin(12 * x - 4)\n```\n\n----------------------------------------\n\nTITLE: Weather Model Implementation\nDESCRIPTION: A simple weather model using PyTorch distributions to generate cloudy/sunny conditions and associated temperatures.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef weather():\n    cloudy = torch.distributions.Bernoulli(0.3).sample()\n    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n    temp = torch.distributions.Normal(mean_temp, scale_temp).rsample()\n    return cloudy, temp.item()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for VAE Implementation\nDESCRIPTION: Import required libraries including Pyro, PyTorch, Zuko for normalizing flows, and utilities for MNIST dataset handling.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae_flow_prior.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyro\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport zuko\n\nfrom pyro.contrib.zuko import ZukoToPyro\nfrom pyro.optim import Adam\nfrom pyro.infer import SVI, Trace_ELBO\nfrom torch import Tensor\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms.functional import to_tensor, to_pil_image\nfrom tqdm import tqdm\n```\n\n----------------------------------------\n\nTITLE: Optimizing a Gaussian Mixture Model with Conditional Independence in Pyro\nDESCRIPTION: This code snippet demonstrates how to improve the previous mixture model implementation by explicitly marking conditional independence using pyro.plate, which enables Rao-Blackwellization for variance reduction in gradient estimation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# mark conditional independence \n# (assumed to be along the rightmost tensor dimension)\nwith pyro.plate(\"foo\", data.size(-1)):\n    ks = pyro.sample(\"k\", dist.Categorical(probs))\n    pyro.sample(\"obs\", dist.Normal(locs[ks], scale),\n                obs=data)\n```\n\n----------------------------------------\n\nTITLE: Batching Distributions Using expand() Method\nDESCRIPTION: Demonstrates another way to batch distributions using the expand() method. This works when parameters are identical along the leftmost dimensions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nd = Bernoulli(torch.tensor([0.1, 0.2, 0.3, 0.4])).expand([3, 4])\nassert d.batch_shape == (3, 4)\nassert d.event_shape == ()\nx = d.sample()\nassert x.shape == (3, 4)\nassert d.log_prob(x).shape == (3, 4)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictive Samples with Pyro\nDESCRIPTION: This snippet defines a summary function and uses Pyro's Predictive class to generate samples from the trained model. It captures both observed and latent variables for further analysis.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.infer import Predictive\n\n\ndef summary(samples):\n    site_stats = {}\n    for k, v in samples.items():\n        site_stats[k] = {\n            \"mean\": torch.mean(v, 0),\n            \"std\": torch.std(v, 0),\n            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n        }\n    return site_stats\n\n\npredictive = Predictive(model, guide=guide, num_samples=800, \n                        return_sites=(\"linear.weight\", \"obs\", \"_RETURN\"))\nsamples = predictive(x_data)\npred_summary = summary(samples)\n```\n\n----------------------------------------\n\nTITLE: Literal Listener Implementation\nDESCRIPTION: Implements the literal listener that assigns probability based on whether an utterance is true for a given state.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@Marginal\ndef literal_listener(utterance):\n    state = state_prior()\n    pyro.factor(\"literal_meaning\", 0. if meaning(utterance, state) else -999999.)\n    return state\n```\n\n----------------------------------------\n\nTITLE: Speaker Model Implementation\nDESCRIPTION: Implements the cooperative speaker model that chooses utterances to convey states to the literal listener with softmax decision rule.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@Marginal\ndef speaker(state):\n    alpha = 1.\n    with poutine.scale(scale=torch.tensor(alpha)):\n        utterance = utterance_prior()\n        pyro.sample(\"listener\", literal_listener(utterance), obs=state)\n    return utterance\n```\n\n----------------------------------------\n\nTITLE: Implementing Guide Function\nDESCRIPTION: Defines the guide function using MarginalAssignmentPersistent for efficient inference of assignments.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tracking_1d.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef guide(args, observations):\n    # Initialize states randomly from the prior.\n    states_loc = pyro.param(\"states_loc\", lambda: torch.randn(args.max_num_objects, 2))\n    states_scale = pyro.param(\"states_scale\",\n                              lambda: torch.ones(states_loc.shape) * args.emission_noise_scale,\n                              constraint=constraints.positive)\n    positions = get_dynamics(args.num_frames).mm(states_loc.t())\n\n    # Solve soft assignment problem.\n    real_dist = dist.Normal(positions.unsqueeze(-2), args.emission_noise_scale)\n    spurious_dist = dist.Normal(0., 1.)\n    is_observed = (observations[..., -1] > 0)\n    observed_positions = observations[..., 0].unsqueeze(-1)\n    assign_logits = (real_dist.log_prob(observed_positions) -\n                     spurious_dist.log_prob(observed_positions) +\n                     math.log(args.expected_num_objects * args.emission_prob /\n                              args.expected_num_spurious))\n    assign_logits[~is_observed] = -float('inf')\n    exists_logits = torch.empty(args.max_num_objects).fill_(\n        math.log(args.max_num_objects / args.expected_num_objects))\n    assignment = MarginalAssignmentPersistent(exists_logits, assign_logits)\n\n    with pyro.plate(\"objects\", args.max_num_objects):\n        exists = pyro.sample(\"exists\", assignment.exists_dist, infer={\"enumerate\": \"parallel\"})\n        with poutine.mask(mask=exists.bool()):\n            pyro.sample(\"states\", dist.Normal(states_loc, states_scale).to_event(1))\n    with pyro.plate(\"detections\", observations.shape[1]):\n        with poutine.mask(mask=is_observed):\n            with pyro.plate(\"time\", args.num_frames):\n                assign = pyro.sample(\"assign\", assignment.assign_dist, infer={\"enumerate\": \"parallel\"})\n\n    return assignment\n```\n\n----------------------------------------\n\nTITLE: Implementing a Guide with Properly Constrained Parameters\nDESCRIPTION: Shows the correct way to implement a guide with constrained parameters, ensuring the scale parameter remains positive using Pyro's constraint system.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.distributions import constraints\n\ndef good_guide():\n    scale = pyro.param(\"scale\", torch.tensor(0.05),               \n                        constraint=constraints.positive)\n    pyro.sample(\"x\", dist.Normal(0.0, scale))\n```\n\n----------------------------------------\n\nTITLE: Prior Probability Setup\nDESCRIPTION: Defines prior probabilities for states and utterances in a simple world with 4 objects that can be blue or red.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntotal_number = 4\n\ndef state_prior():\n    n = pyro.sample(\"state\", dist.Categorical(probs=torch.ones(total_number+1) / total_number+1))\n    return n\n\ndef utterance_prior():\n    ix = pyro.sample(\"utt\", dist.Categorical(probs=torch.ones(3) / 3))\n    return [\"none\",\"some\",\"all\"][ix]\n```\n\n----------------------------------------\n\nTITLE: Configuring SVI for ELBO Optimization in Pyro\nDESCRIPTION: This snippet sets up Stochastic Variational Inference (SVI) with the Adam optimizer and Trace_ELBO loss for optimizing the Evidence Lower Bound (ELBO) in the Bayesian regression model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyro.infer import SVI, Trace_ELBO\n\nadam = pyro.optim.Adam({\"lr\": 0.03})\nsvi = SVI(model, guide, adam, loss=Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Mixing Different PyTorch Optimizers in Pyro Training Loop\nDESCRIPTION: Demonstrates how to use different optimization algorithms for different sets of parameters by directly manipulating the differentiable loss function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nadam = torch.optim.Adam(adam_parameters, {\"lr\": 0.001, \"betas\": (0.90, 0.999)})\nsgd = torch.optim.SGD(sgd_parameters, {\"lr\": 0.0001})\nloss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n# compute loss\nloss = loss_fn(model, guide)\nloss.backward()\n# take a step and zero the parameter gradients\nadam.step()\nsgd.step()\nadam.zero_grad()\nsgd.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Defining a Bimodal Posterior Model in Pyro\nDESCRIPTION: Implements a Pyro model where z follows a normal distribution and x follows a normal distribution with mean z², creating a bimodal posterior when conditioned on observed data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    prior_loc = torch.tensor([0.])\n    prior_scale = torch.tensor([5.])\n    z = pyro.sample('z', dist.Normal(prior_loc, prior_scale))\n    scale = torch.tensor([0.1])\n\n    with pyro.plate('data', len(data)):\n        pyro.sample('x', dist.Normal(z*z, scale), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Implementing LogJointMessenger for lazy Funsor expression building in Python\nDESCRIPTION: This class extends Pyro's Messenger to automatically build lazy Funsor expressions for the logarithm of the joint probability density of a model. It uses pyro.to_funsor to convert sample values and distributions to Funsor objects.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass LogJointMessenger(Messenger):\n\n    def __enter__(self):\n        self.log_joint = funsor.Number(0.)\n        return super().__enter__()\n\n    @pyroapi.pyro_backend(\"contrib.funsor\")\n    def _pyro_post_sample(self, msg):\n        \n        # for Monte Carlo-sampled variables, we don't include a log-density term:\n        if not msg[\"is_observed\"] and not msg[\"infer\"].get(\"enumerate\"):\n            return\n        \n        with funsor.interpreter.interpretation(funsor.terms.lazy):\n            funsor_dist = pyro.to_funsor(msg[\"fn\"], output=funsor.Real)\n            funsor_value = pyro.to_funsor(msg[\"value\"], output=funsor_dist.inputs[\"value\"])\n            self.log_joint += funsor_dist(value=funsor_value)\n```\n\n----------------------------------------\n\nTITLE: Executing Bayesian Optimization Loop\nDESCRIPTION: Main execution loop that performs Bayesian Optimization by iteratively selecting new points, updating the model, and visualizing the results. Includes GP hyperparameter optimization using Adam optimizer.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(12, 30))\nouter_gs = gridspec.GridSpec(5, 2)\noptimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.001)\ngp.util.train(gpmodel, optimizer)\nfor i in range(8):\n    xmin = next_x() \n    gs = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=outer_gs[i])\n    plot(gs, xmin, xlabel=i+1, with_title=(i % 2 == 0))\n    update_posterior(xmin)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: SVI Model Fitting Helper Function\nDESCRIPTION: Implements stochastic variational inference (SVI) fitting procedure with AutoNormal guide and ClippedAdam optimizer. Includes loss tracking and convergence plotting.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fit_svi(model, lr=0.1, num_steps=1001, log_every=250):\n    pyro.clear_param_store()  # clear parameters from previous runs\n    pyro.set_rng_seed(20211214)\n    if smoke_test:\n        num_steps = 2\n        \n    # Define a mean field guide (i.e. variational distribution)\n    guide = AutoNormal(model, init_scale=0.01)\n    optim = ClippedAdam({\"lr\": lr, \"lrd\": 0.1 ** (1 / num_steps)})\n    svi = SVI(model, guide, optim, Trace_ELBO())\n    \n    # Train (i.e. do ELBO optimization) for num_steps iterations\n    losses = []\n    for step in range(num_steps):\n        loss = svi.step()\n        losses.append(loss)\n        if step % log_every == 0:\n            print(f\"step {step: >4d} loss = {loss:0.6g}\")\n            \n    # Plot to assess convergence.\n    plt.figure(figsize=(6, 3))\n    plt.plot(losses)\n    plt.xlabel(\"SVI step\", fontsize=18)\n    plt.ylabel(\"ELBO loss\", fontsize=18)\n    plt.tight_layout()\n\n    return guide\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Custom ELBO in Pyro\nDESCRIPTION: Demonstrates how to create a custom ELBO implementation that can be passed into the SVI machinery, showing the core components of constructing a variational objective function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# note that simple_elbo takes a model, a guide, and their respective arguments as inputs\ndef simple_elbo(model, guide, *args, **kwargs):\n    # run the guide and trace its execution\n    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n    # run the model and replay it against the samples from the guide\n    model_trace = poutine.trace(\n        poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)\n    # construct the elbo loss function\n    return -1*(model_trace.log_prob_sum() - guide_trace.log_prob_sum())\n\nsvi = SVI(model, guide, optim, loss=simple_elbo)\n```\n\n----------------------------------------\n\nTITLE: Training the Kalman Filter Model\nDESCRIPTION: Performs optimization using SVI with Adam optimizer to train the Kalman filter model. Includes iteration loop with loss tracking.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ekf.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noptim = pyro.optim.Adam({'lr': 2e-2})\nsvi = SVI(model, guide, optim, loss=Trace_ELBO(retain_graph=True))\n\npyro.set_rng_seed(0)\npyro.clear_param_store()\n\nfor i in range(250 if not smoke_test else 2):\n    loss = svi.step(zs)\n    if not i % 10:\n        print('loss: ', loss)\n```\n\n----------------------------------------\n\nTITLE: Stan Model Implementation for Eight Schools Analysis\nDESCRIPTION: Stan model specification for hierarchical Bayesian analysis of treatment effects across schools. The model includes parameters for overall mean (mu), variance (tau), and school-specific effects (eta), with transformed parameters for individual school effects (theta).\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/examples/eight_schools/README.md#2025-04-16_snippet_0\n\nLANGUAGE: stan\nCODE:\n```\ndata {\n  int<lower=0> J; // number of schools\n  real y[J]; // estimated treatment effects\n  real<lower=0> sigma[J]; // s.e. of effect estimates\n}\nparameters {\n  real mu;\n  real<lower=0> tau;\n  real eta[J];\n}\ntransformed parameters {\n  real theta[J];\n  for (j in 1:J)\n    theta[j] <- mu + tau * eta[j];\n}\nmodel {\n  eta ~ normal(0, 1);\n  y ~ normal(theta, sigma);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Auxiliary Classification Loss in Pyro\nDESCRIPTION: Defines model and guide functions for adding an auxiliary classification loss term to the objective function, with configurable loss multiplier scaling.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ss-vae.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model_classify(self, xs, ys=None):\n    pyro.module(\"ss_vae\", self)\n    with pyro.plate(\"data\"):\n        # this here is the extra term to yield an auxiliary loss\n        # that we do gradient descent on\n        if ys is not None:\n            alpha = self.encoder_y(xs)\n            with pyro.poutine.scale(scale=self.aux_loss_multiplier):\n                pyro.sample(\"y_aux\", dist.OneHotCategorical(alpha), obs=ys)\n\ndef guide_classify(xs, ys):\n    # the guide is trivial, since there are no \n    # latent random variables\n    pass\n\nsvi_aux = SVI(model_classify, guide_classify, optimizer, loss=Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Implementing Guide Function with Pyro and RNN\nDESCRIPTION: The `guide` function samples latent variables based on observed data using an RNN. It registers modules with Pyro, initializes RNN hidden states, processes the mini-batch in reverse order, and samples latents using the Combiner module while maintaining conditional independence for each data point.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n          mini_batch_seq_lengths, annealing_factor=1.0):\n\n    # this is the number of time steps we need to process in the mini-batch\n    T_max = mini_batch.size(1)\n    # register all PyTorch (sub)modules with pyro\n    pyro.module(\"dmm\", self)\n\n    # if on gpu we need the fully broadcast view of the rnn initial state\n    # to be in contiguous gpu memory\n    h_0_contig = self.h_0.expand(1, mini_batch.size(0), \n                                 self.rnn.hidden_size).contiguous()\n    # push the observed x's through the rnn;\n    # rnn_output contains the hidden state at each time step\n    rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n    # reverse the time-ordering in the hidden state and un-pack it\n    rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n    # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n    z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n\n    # we enclose all the sample statements in the guide in a plate.\n    # this marks that each datapoint is conditionally independent of the others.\n    with pyro.plate(\"z_minibatch\", len(mini_batch)):\n        # sample the latents z one time step at a time\n        for t in range(1, T_max + 1):\n            # the next two lines assemble the distribution q(z_t | z_{t-1}, x_{t:T})\n            z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n            z_dist = dist.Normal(z_loc, z_scale)\n\n            # sample z_t from the distribution z_dist\n            with pyro.poutine.scale(None, annealing_factor):\n                z_t = pyro.sample(\"z_%d\" % t,\n                                  z_dist.mask(mini_batch_mask[:, t - 1:t])\n                                        .to_event(1))\n            # the latent sampled at this time step will be conditioned \n            # upon in the next time step so keep track of it\n            z_prev = z_t\n```\n\n----------------------------------------\n\nTITLE: Implementing Poisson DPMM for Sunspot Data\nDESCRIPTION: Defines the model and guide for a Dirichlet process mixture of Poisson distributions for analyzing sunspot count data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    with pyro.plate(\"beta_plate\", T-1):\n        beta = pyro.sample(\"beta\", Beta(1, alpha))\n\n    with pyro.plate(\"lambda_plate\", T):\n        lmbda = pyro.sample(\"lambda\", Gamma(3, 0.05))\n    \n    with pyro.plate(\"data\", N):\n        z = pyro.sample(\"z\", Categorical(mix_weights(beta)))\n        pyro.sample(\"obs\", Poisson(lmbda[z]), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Parameters\nDESCRIPTION: Sets up configuration parameters for the tracking model including number of frames, objects, and noise scales.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tracking_1d.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nargs = type('Args', (object,), {})  # A fake ArgumentParser.parse_args() result.\n\nargs.num_frames = 5\nargs.max_num_objects = 3\nargs.expected_num_objects = 2.\nargs.expected_num_spurious = 1.\nargs.emission_prob = 0.8\nargs.emission_noise_scale = 0.1\n\nassert args.max_num_objects >= args.expected_num_objects\n```\n\n----------------------------------------\n\nTITLE: Defining Encoder Network in PyTorch\nDESCRIPTION: Creates an Encoder class in PyTorch which processes an image and outputs parameters for a Gaussian distribution in latent space. The network uses linear layers and softplus activation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass Encoder(nn.Module):\n    def __init__(self, z_dim, hidden_dim):\n        super().__init__()\n        # setup the three linear transformations used\n        self.fc1 = nn.Linear(784, hidden_dim)\n        self.fc21 = nn.Linear(hidden_dim, z_dim)\n        self.fc22 = nn.Linear(hidden_dim, z_dim)\n        # setup the non-linearities\n        self.softplus = nn.Softplus()\n\n    def forward(self, x):\n        # define the forward computation on the image x\n        # first shape the mini-batch to have pixels in the rightmost dimension\n        x = x.reshape(-1, 784)\n        # then compute the hidden units\n        hidden = self.softplus(self.fc1(x))\n        # then return a mean vector and a (positive) square root covariance\n        # each of size batch_size x z_dim\n        z_loc = self.fc21(hidden)\n        z_scale = torch.exp(self.fc22(hidden))\n        return z_loc, z_scale\n```\n\n----------------------------------------\n\nTITLE: Running Parallel Pyro Tests\nDESCRIPTION: Commands to run all tests in parallel using the pytest-xdist package, which can significantly speed up the testing process.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install pytest-xdist\n```\n\nLANGUAGE: sh\nCODE:\n```\npytest -vs -n auto\n```\n\n----------------------------------------\n\nTITLE: Implementing Regional SIR Model with Coupled Compartments in Pyro\nDESCRIPTION: This code defines a RegionalSIRModel class that extends CompartmentalModel to model SIR dynamics across multiple regions with inter-region infection coupling. It implements region-specific parameters through Pyro plates, handles hierarchical priors for response rates, and computes coupled infection vectors accounting for both intra-region and inter-region transmission.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass RegionalSIRModel(CompartmentalModel):\n    def __init__(self, population, coupling, recovery_time, data):\n        duration = len(data)\n        num_regions, = population.shape\n        assert coupling.shape == (num_regions, num_regions)\n        assert (0 <= coupling).all()\n        assert (coupling <= 1).all()\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        if isinstance(data, torch.Tensor):\n            # Data tensors should be oriented as (time, region).\n            assert data.shape == (duration, num_regions)\n        compartments = (\"S\", \"I\")  # R is implicit.\n\n        # We create a regional model by passing a vector of populations.\n        super().__init__(compartments, duration, population, approximate=(\"I\",))\n\n        self.coupling = coupling\n        self.recovery_time = recovery_time\n        self.data = data\n\n    def global_model(self):\n        # Assume recovery time is a known constant.\n        tau = self.recovery_time\n\n        # Assume reproductive number is unknown but homogeneous.\n        R0 = pyro.sample(\"R0\", dist.LogNormal(0., 1.))\n\n        # Assume response rate is heterogeneous and model it with a\n        # hierarchical Gamma-Beta prior.\n        rho_c1 = pyro.sample(\"rho_c1\", dist.Gamma(10, 1))\n        rho_c0 = pyro.sample(\"rho_c0\", dist.Gamma(10, 1))\n        with self.region_plate:\n            rho = pyro.sample(\"rho\", dist.Beta(rho_c1, rho_c0))\n\n        return R0, tau, rho\n\n    def initialize(self, params):\n        # Start with a single infection in region 0.\n        I = torch.zeros_like(self.population)\n        I[0] += 1\n        S = self.population - I\n        return {\"S\": S, \"I\": I}\n\n    def transition(self, params, state, t):\n        R0, tau, rho = params\n\n        # Account for infections from all regions. This uses approximate (point\n        # estimate) counts I_approx for infection from other regions, but uses\n        # the exact (enumerated) count I for infections from one's own region.\n        I_coupled = state[\"I_approx\"] @ self.coupling\n        I_coupled = I_coupled + (state[\"I\"] - state[\"I_approx\"]) * self.coupling.diag()\n        I_coupled = I_coupled.clamp(min=0)  # In case I_approx is negative.\n        pop_coupled = self.population @ self.coupling\n\n        with self.region_plate:\n            # Sample flows between compartments.\n            S2I = pyro.sample(\"S2I_{}\".format(t),\n                              infection_dist(individual_rate=R0 / tau,\n                                             num_susceptible=state[\"S\"],\n                                             num_infectious=I_coupled,\n                                             population=pop_coupled))\n            I2R = pyro.sample(\"I2R_{}\".format(t),\n                              binomial_dist(state[\"I\"], 1 / tau))\n\n            # Update compartments with flows.\n            state[\"S\"] = state[\"S\"] - S2I\n            state[\"I\"] = state[\"I\"] + S2I - I2R\n\n            # Condition on observations.\n            t_is_observed = isinstance(t, slice) or t < self.duration\n            pyro.sample(\"obs_{}\".format(t),\n                        binomial_dist(S2I, rho),\n                        obs=self.data[t] if t_is_observed else None)\n```\n\n----------------------------------------\n\nTITLE: Using Sequential plate for Marking Conditional Independence in Pyro\nDESCRIPTION: Modified version of the model that uses `pyro.plate` to mark the conditional independence of observations given the latent variable, enabling Pyro to leverage this structure for efficient inference.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    # sample f from the beta prior\n    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n    # loop over the observed data [WE ONLY CHANGE THE NEXT LINE]\n    for i in pyro.plate(\"data_loop\", len(data)):  \n        # observe datapoint i using the bernoulli likelihood\n        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n```\n\n----------------------------------------\n\nTITLE: Visualizing Results and Comparison\nDESCRIPTION: Plots and compares results from CSIS, importance sampling, and analytic posterior.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/csis.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport scipy.stats\nimport matplotlib.pyplot as plt\n\nwith torch.no_grad():\n    # Draw samples from empirical marginal for plotting\n    csis_samples = torch.stack([marginal() for _ in range(1000)])\n\n    # Calculate empirical marginal with importance sampling\n    is_posterior = pyro.infer.Importance(model, num_samples=50).run(\n        prior_mean, observations={\"x1\": torch.tensor(8.),\n                                  \"x2\": torch.tensor(9.)})\n    is_marginal = pyro.infer.EmpiricalMarginal(is_posterior, \"z\")\n    is_samples = torch.stack([is_marginal() for _ in range(1000)])\n\n# Calculate true prior and posterior over z\ntrue_posterior_z = torch.arange(-10, 10, 0.05)\ntrue_posterior_p = dist.Normal(7.25, (5/6)**0.5).log_prob(true_posterior_z).exp()\nprior_z = true_posterior_z\nprior_p = dist.Normal(1., 5**0.5).log_prob(true_posterior_z).exp()\n\nplt.rcParams['figure.figsize'] = [30, 15]\nplt.rcParams.update({'font.size': 30})\nfig, ax = plt.subplots()\nplt.plot(prior_z, prior_p, 'k--', label='Prior')\nplt.plot(true_posterior_z, true_posterior_p, color='k', label='Analytic Posterior')\nplt.hist(csis_samples.numpy(), range=(-10, 10), bins=100, color='r', density=1,\n         label=\"Inference Compilation\")\nplt.hist(is_samples.numpy(), range=(-10, 10), bins=100, color='b', density=1,\n         label=\"Importance Sampling\")\nplt.xlim(-8, 10)\nplt.ylim(0, 5)\nplt.xlabel(\"z\")\nplt.ylabel(\"Estimated Posterior Probability Density\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing VAE with MAF Prior\nDESCRIPTION: Define the complete VAE model incorporating a Masked Autoregressive Flow (MAF) prior, along with the model and guide functions for Pyro's inference framework.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae_flow_prior.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass VAE(nn.Module):\n    def __init__(self, features: int, latent: int = 16):\n        super().__init__()\n\n        self.encoder = GaussianEncoder(features, latent)\n        self.decoder = BernoulliDecoder(features, latent)\n\n        self.prior = zuko.flows.MAF(\n            features=latent,\n            transforms=3,\n            hidden_features=(256, 256),\n        )\n\n    def model(self, x: Tensor):\n        pyro.module(\"prior\", self.prior)\n        pyro.module(\"decoder\", self.decoder)\n\n        with pyro.plate(\"batch\", len(x)):\n            z = pyro.sample(\"z\", ZukoToPyro(self.prior()))\n            x = pyro.sample(\"x\", self.decoder(z), obs=x)\n\n    def guide(self, x: Tensor):\n        pyro.module(\"encoder\", self.encoder)\n\n        with pyro.plate(\"batch\", len(x)):\n            z = pyro.sample(\"z\", self.encoder(x))\n\nvae = VAE(784, 16).cuda()\nvae\n```\n\n----------------------------------------\n\nTITLE: Fitting Pyro Model with SVI and AutoDiagonalNormal Guide\nDESCRIPTION: Defines a function to fit the model using Stochastic Variational Inference (SVI) with an AutoDiagonalNormal guide. It performs optimization steps and collects loss and parameter statistics.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\npyro.set_rng_seed(1234567890)\n\ndef fit_model(model):\n    num_steps = 1 if smoke_test else 3001\n    optim = ClippedAdam({\"lr\": 0.05, \"betas\": (0.9, 0.99), \"lrd\": 0.1 ** (1 / num_steps)})\n    guide = AutoDiagonalNormal(model)\n    svi = SVI(model, guide, optim, Trace_ELBO())\n    losses = []\n    stats = []\n    for step in range(num_steps):\n        loss = svi.step(r) / len(r)\n        losses.append(loss)\n        stats.append(guide.quantiles([0.325, 0.675]).items())\n        if step % 200 == 0:\n            median = guide.median()\n            print(\"step {} loss = {:0.6g}\".format(step, loss))\n\n    return guide, losses, stats\n\nguide, losses, stats = fit_model(reparam_model)\n\nprint(\"-\" * 20)\nfor name, (lb, ub) in sorted(stats[-1]):\n    if lb.numel() == 1:\n        lb = lb.squeeze().item()\n        ub = ub.squeeze().item()\n        print(\"{} = {:0.4g} ± {:0.4g}\".format(name, (lb + ub) / 2, (ub - lb) / 2))\n\npyplot.figure(figsize=(9, 3))\npyplot.plot(losses)\npyplot.ylabel(\"loss\")\npyplot.xlabel(\"SVI step\")\npyplot.xlim(0, len(losses))\npyplot.ylim(min(losses), 20)\n```\n\n----------------------------------------\n\nTITLE: Running SVI with JitTrace_ELBO\nDESCRIPTION: Shows how to use JitTrace_ELBO for JIT-compiled inference in Pyro. This version is expected to run faster than the standard Trace_ELBO.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\n\nguide(data)  # Do any lazy initialization before compiling.\n\nelbo = JitTrace_ELBO()\nsvi = SVI(model, guide, Adam({'lr': 0.01}), elbo)\nfor i in range(2 if smoke_test else 1000):\n    svi.step(data)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Parallelizable Pyro Model\nDESCRIPTION: This snippet demonstrates how to write a Pyro model that works both with and without enumeration. It uses broadcasting and ellipsis slicing to handle parallelized sample sites correctly.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nwidth = 8\nheight = 10\nsparse_pixels = torch.LongTensor([[3, 2], [3, 5], [3, 9], [7, 1]])\nenumerated = None  # set to either True or False below\n\ndef fun(observe):\n    p_x = pyro.param(\"p_x\", torch.tensor(0.1), constraint=constraints.unit_interval)\n    p_y = pyro.param(\"p_y\", torch.tensor(0.1), constraint=constraints.unit_interval)\n    x_axis = pyro.plate('x_axis', width, dim=-2)\n    y_axis = pyro.plate('y_axis', height, dim=-1)\n\n    # Note that the shapes of these sites depend on whether Pyro is enumerating.\n    with x_axis:\n        x_active = pyro.sample(\"x_active\", Bernoulli(p_x))\n    with y_axis:\n        y_active = pyro.sample(\"y_active\", Bernoulli(p_y))\n    if enumerated:\n        assert x_active.shape  == (2, 1, 1)\n        assert y_active.shape  == (2, 1, 1, 1)\n    else:\n        assert x_active.shape  == (width, 1)\n        assert y_active.shape  == (height,)\n\n    # The first trick is to broadcast. This works with or without enumeration.\n    p = 0.1 + 0.5 * x_active * y_active\n    if enumerated:\n        assert p.shape == (2, 2, 1, 1)\n    else:\n        assert p.shape == (width, height)\n    dense_pixels = p.new_zeros(broadcast_shape(p.shape, (width, height)))\n\n    # The second trick is to index using ellipsis slicing.\n    # This allows Pyro to add arbitrary dimensions on the left.\n    for x, y in sparse_pixels:\n        dense_pixels[..., x, y] = 1\n    if enumerated:\n        assert dense_pixels.shape == (2, 2, width, height)\n    else:\n        assert dense_pixels.shape == (width, height)\n\n    with x_axis, y_axis:    \n        if observe:\n            pyro.sample(\"pixels\", Bernoulli(p), obs=dense_pixels)\n\ndef model4():\n    fun(observe=True)\n\ndef guide4():\n    fun(observe=False)\n\n# Test without enumeration.\nenumerated = False\ntest_model(model4, guide4, Trace_ELBO())\n\n# Test with enumeration.\nenumerated = True\ntest_model(model4, config_enumerate(guide4, \"parallel\"),\n           TraceEnum_ELBO(max_plate_nesting=2))\n```\n\n----------------------------------------\n\nTITLE: Importing Pyro and Setting Random Seed\nDESCRIPTION: Imports necessary Pyro modules and sets a random seed for reproducibility.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\n\nfrom pyro.poutine.runtime import effectful\n\npyro.set_rng_seed(101)\n```\n\n----------------------------------------\n\nTITLE: Calculating EIG for Different Polling Strategies using Pyro\nDESCRIPTION: This code computes the Expected Information Gain (EIG) for each polling strategy using Pyro's posterior_eig function. It trains a neural network for each strategy and selects the best one based on the highest EIG.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.contrib.oed.eig import posterior_eig\nfrom pyro.optim import Adam\n\neigs = {}\nbest_strategy, best_eig = None, 0\n\nfor strategy, allocation in poll_strategies.items():\n    print(strategy, end=\" \")\n    guide = OutcomePredictor()\n    pyro.clear_param_store()\n    # To reduce noise when comparing designs, we will use the precomputed value of H(p(w))\n    # By passing eig=False, we tell Pyro not to estimate the prior entropy on each run\n    # The return value of `posterior_eig` is then -E_p(w,y)[log q(w|y)]\n    ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 12500, guide, \n                        Adam({\"lr\": 0.001}), eig=False, final_num_samples=10000)\n    eigs[strategy] = prior_entropy - ape\n    print(eigs[strategy].item())\n    if eigs[strategy] > best_eig:\n        best_strategy, best_eig = strategy, eigs[strategy]\n```\n\n----------------------------------------\n\nTITLE: Pyro Model Definition in Python\nDESCRIPTION: Defines a probabilistic model using Pyro that describes data generation for images given a latent variable. Integrates PyTorch decoder and specifies a unit normal Gaussian prior distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# define the model p(x|z)p(z)\ndef model(self, x):\n    # register PyTorch module `decoder` with Pyro\n    pyro.module(\"decoder\", self.decoder)\n    with pyro.plate(\"data\", x.shape[0]):\n        # setup hyperparameters for prior p(z)\n        z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n        z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n        # sample from prior (value will be sampled by guide when computing the ELBO)\n        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n        # decode the latent code z\n        loc_img = self.decoder(z)\n        # score against actual images\n        pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n```\n\n----------------------------------------\n\nTITLE: Performing MCMC Inference with NUTS Kernel\nDESCRIPTION: This code sets up and runs MCMC inference using the NUTS kernel for the defined model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnuts_kernel = NUTS(model)\nmcmc = MCMC(nuts_kernel, num_samples=500)\n\n%%time\n\nmcmc.run(X_, y_);\n```\n\n----------------------------------------\n\nTITLE: Implementing Phylogenetic Likelihood in SuperspreadingSEIR Model\nDESCRIPTION: Implementation of a SuperspreadingSEIR model that incorporates phylogenetic likelihood computation. The code shows initialization and transition methods for handling coalescent rates and likelihood calculations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SuperspreadingSEIRModel(CompartmentalModel):\n    def __init__(self, population, incubation_time, recovery_time, data, *,\n                 leaf_times=None, coal_times=None):\n        compartments = (\"S\", \"E\", \"I\")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n        ...\n        self.coal_likelihood = dist.CoalescentRateLikelihood(\n            leaf_times, coal_times, duration)\n    ...\n    \n    def transition(self, params, state, t):\n        ...\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        R = R0 * state[\"S\"] / self.population\n        coal_rate = R * (1. + 1. / k) / (tau_i * state[\"I\"] + 1e-8)\n        pyro.factor(\"coalescent_{}\".format(t),\n                    self.coal_likelihood(coal_rate, t)\n                    if t_is_observed else torch.tensor(0.))\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Model with Plates and Shape Assertions\nDESCRIPTION: This code defines a Pyro model (`model1`) that uses `pyro.plate` to declare independent dimensions and `pyro.sample` to draw samples from probability distributions. The snippet includes shape assertions to verify the shapes of the sampled variables. The `test_model` function is called to test the model using `Trace_ELBO`.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"def model1():\\n    a = pyro.sample(\\\"a\\\", Normal(0, 1))\\n    b = pyro.sample(\\\"b\\\", Normal(torch.zeros(2), 1).to_event(1))\\n    with pyro.plate(\\\"c_plate\\\", 2):\\n        c = pyro.sample(\\\"c\\\", Normal(torch.zeros(2), 1))\\n    with pyro.plate(\\\"d_plate\\\", 3):\\n        d = pyro.sample(\\\"d\\\", Normal(torch.zeros(3,4,5), 1).to_event(2))\\n    assert a.shape == ()       # batch_shape == ()     event_shape == ()\\n    assert b.shape == (2,)     # batch_shape == ()     event_shape == (2,)\\n    assert c.shape == (2,)     # batch_shape == (2,)   event_shape == ()\\n    assert d.shape == (3,4,5)  # batch_shape == (3,)   event_shape == (4,5) \\n\\n    x_axis = pyro.plate(\\\"x_axis\\\", 3, dim=-2)\\ny_axis = pyro.plate(\\\"y_axis\\\", 2, dim=-3)\\n    with x_axis:\\n        x = pyro.sample(\\\"x\\\", Normal(0, 1))\\n    with y_axis:\\n        y = pyro.sample(\\\"y\\\", Normal(0, 1))\\n    with x_axis, y_axis:\\n        xy = pyro.sample(\\\"xy\\\", Normal(0, 1))\\n        z = pyro.sample(\\\"z\\\", Normal(0, 1).expand([5]).to_event(1))\\n    assert x.shape == (3, 1)        # batch_shape == (3,1)     event_shape == ()\\n    assert y.shape == (2, 1, 1)     # batch_shape == (2,1,1)   event_shape == ()\\n    assert xy.shape == (2, 3, 1)    # batch_shape == (2,3,1)   event_shape == ()\\n    assert z.shape == (2, 3, 1, 5)  # batch_shape == (2,3,1)   event_shape == (5,)\\n    \\ntest_model(model1, model1, Trace_ELBO())\"\n```\n\n----------------------------------------\n\nTITLE: Training the Probabilistic Model Using SVI\nDESCRIPTION: This code snippet sets up and runs Stochastic Variational Inference (SVI) to optimize the parameters of the previously defined model. It logs the loss over training steps and visualizes the parameter estimates after optimization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\npyro.set_rng_seed(1234567890)\nnum_steps = 1 if smoke_test else 201\noptim = ClippedAdam({\"lr\": 0.1, \"lrd\": 0.1 ** (1 / num_steps)})\nsvi = SVI(model, lambda: None, optim, EnergyDistance())\nlosses = []\nfor step in range(num_steps):\n    loss = svi.step()\n    losses.append(loss)\n    if step % 20 == 0:\n        print(\"step {} loss = {}\".format(step, loss))\n\nprint(\"-\" * 20)\npyplot.figure(figsize=(9, 3))\npyplot.plot(losses)\npyplot.yscale(\"log\")\npyplot.ylabel(\"loss\")\npyplot.xlabel(\"SVI step\")\nfor name, value in sorted(pyro.get_param_store().items()):\n    if value.numel() == 1:\n        print(\"{} = {:0.4g}\".format(name, value.squeeze().item()))\n```\n\n----------------------------------------\n\nTITLE: Managing Global Messenger Stack Operations\nDESCRIPTION: Implementation of stack management methods in the Messenger class, showing how messengers are pushed and popped from the global stack.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Messenger:\n    ...\n    # __enter__ pushes a Messenger onto the stack\n    def __enter__(self):\n        ...\n        _PYRO_STACK.append(self)\n        ...\n    \n    # __exit__ removes a Messenger from the stack\n    def __exit__(self, ...):\n        ...\n        assert _PYRO_STACK[-1] is self\n        _PYRO_STACK.pop()\n        ...\n```\n\n----------------------------------------\n\nTITLE: Implementing RSA Speaker and Listener Models\nDESCRIPTION: Defines the probabilistic models for literal listener, speaker, and project functions that handle relevant information based on QUD.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-hyperbole.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@Marginal\ndef project(dist,qud):\n    v = pyro.sample(\"proj\",dist)\n    return qud_fns[qud](v)\n\n@Marginal\ndef literal_listener(utterance):\n    state=state_prior()\n    pyro.factor(\"literal_meaning\", 0. if meaning(utterance, state.price) else -999999.)\n    return state\n\n@Marginal\ndef speaker(state, qud):\n    alpha = 1.\n    qudValue = qud_fns[qud](state)\n    with poutine.scale(scale=torch.tensor(alpha)):\n        utterance = utterance_prior()\n        literal_marginal = literal_listener(utterance)\n        projected_literal = project(literal_marginal, qud)\n        pyro.sample(\"listener\", projected_literal, obs=qudValue)\n    return utterance\n```\n\n----------------------------------------\n\nTITLE: LazyValue Class Implementation for Computation Graphs\nDESCRIPTION: A class that implements lazy evaluation by building a computation graph. Stores function calls and arguments until evaluation is explicitly requested.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass LazyValue:\n    def __init__(self, fn, *args, **kwargs):\n        self._expr = (fn, args, kwargs)\n        self._value = None\n        \n    def __str__(self):\n        return \"({} {})\".format(str(self._expr[0]), \" \".join(map(str, self._expr[1])))\n        \n    def evaluate(self):\n        if self._value is None:\n            fn, args, kwargs = self._expr\n            fn = fn.evaluate() if isinstance(fn, LazyValue) else fn\n            args = tuple(arg.evaluate() if isinstance(arg, LazyValue) else arg\n                         for arg in args)\n            kwargs = {k: v.evaluate() if isinstance(v, LazyValue) else v\n                      for k, v in kwargs.items()}\n            self._value = fn(*args, **kwargs)\n        return self._value\n```\n\n----------------------------------------\n\nTITLE: Training Function for SVI in Pyro\nDESCRIPTION: This function implements the training loop for Stochastic Variational Inference (SVI) using the Trace_ELBO loss and Adam optimizer. It processes data in mini-batches and prints the loss every 10 epochs.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/easyguide.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef train(guide, num_epochs=1 if smoke_test else 101, batch_size=20):\n    full_size = data.size(-1)\n    pyro.get_param_store().clear()\n    pyro.set_rng_seed(123456789)\n    svi = SVI(model, guide, Adam({\"lr\": 0.02}), Trace_ELBO())\n    for epoch in range(num_epochs):\n        pos = 0\n        losses = []\n        while pos < full_size:\n            subsample = torch.arange(pos, pos + batch_size)\n            batch = data[:, pos:pos + batch_size]\n            pos += batch_size\n            losses.append(svi.step(batch, subsample, full_size=full_size))\n        epoch_loss = sum(losses) / len(losses)\n        if epoch % 10 == 0:\n            print(\"epoch {} loss = {}\".format(epoch, epoch_loss / data.numel()))\n```\n\n----------------------------------------\n\nTITLE: Training VAE Model\nDESCRIPTION: Implement the training loop using Stochastic Variational Inference (SVI) with Adam optimizer and ELBO loss function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae_flow_prior.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npyro.clear_param_store()\n\nsvi = SVI(vae.model, vae.guide, Adam({'lr': 1e-3}), loss=Trace_ELBO())\n\nfor epoch in (bar := tqdm(range(96))):\n    losses = []\n\n    for x, _ in trainloader:\n        x = x.round().flatten(-3).cuda()\n\n        losses.append(svi.step(x))\n\n    losses = torch.tensor(losses)\n\n    bar.set_postfix(loss=losses.sum().item() / len(trainset))\n```\n\n----------------------------------------\n\nTITLE: Running SVI with JitTraceEnum_ELBO on Time Series Model\nDESCRIPTION: Shows how to use JitTraceEnum_ELBO for JIT-compiled inference on the time series model with varying sequence lengths, demonstrating improved performance.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\n\n# Do any lazy initialization before compiling.\nguide(sequences[0], num_sequences=len(sequences), length=len(sequences[0]))\n\nelbo = JitTraceEnum_ELBO(max_plate_nesting=1)\nsvi = SVI(model, guide, Adam({'lr': 0.01}), elbo)\nfor i in range(1 if smoke_test else 10):\n    for sequence in sequences:\n        svi.step(sequence,                                            # tensor args\n                 num_sequences=len(sequences), length=len(sequence))  # non-tensor args\n```\n\n----------------------------------------\n\nTITLE: Pyro Guide Definition in Python\nDESCRIPTION: Defines a variational distribution using Pyro. It generates latent variable samples given images using a PyTorch encoder to parameterize Gaussian latent distributions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# define the guide (i.e. variational distribution) q(z|x)\ndef guide(self, x):\n    # register PyTorch module `encoder` with Pyro\n    pyro.module(\"encoder\", self.encoder)\n    with pyro.plate(\"data\", x.shape[0]):\n        # use the encoder to get the parameters used to define q(z|x)\n        z_loc, z_scale = self.encoder(x)\n        # sample the latent code z\n        pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n```\n\n----------------------------------------\n\nTITLE: Mini-batch Processing Function for DMM Training\nDESCRIPTION: Implements mini-batch processing with KL annealing, sequence preparation, and gradient steps. Handles batch masking and CUDA support.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef process_minibatch(epoch, which_mini_batch, shuffled_indices):\n    if args.annealing_epochs > 0 and epoch < args.annealing_epochs:\n        # compute the KL annealing factor appropriate \n        # for the current mini-batch in the current epoch\n        min_af = args.minimum_annealing_factor\n        annealing_factor = min_af + (1.0 - min_af) * \\\n            (float(which_mini_batch + epoch * N_mini_batches + 1) /\n             float(args.annealing_epochs * N_mini_batches))\n    else:\n        # by default the KL annealing factor is unity\n        annealing_factor = 1.0 \n\n    # compute which sequences in the training set we should grab\n    mini_batch_start = (which_mini_batch * args.mini_batch_size)\n    mini_batch_end = np.min([(which_mini_batch + 1) * args.mini_batch_size,\n                             N_train_data])\n    mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n    # grab the fully prepped mini-batch using the helper function in the data loader\n    mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n        = poly.get_mini_batch(mini_batch_indices, training_data_sequences,\n                              training_seq_lengths, cuda=args.cuda)\n    # do an actual gradient step\n    loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n                     mini_batch_seq_lengths, annealing_factor)\n    # keep track of the training loss\n    return loss\n```\n\n----------------------------------------\n\nTITLE: Interactive Visualization of Multiple Trajectories\nDESCRIPTION: Uses ipywidgets.interact to create an interactive plot of multiple trajectories with sliders for adjusting prior parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninteract(\n    plot_trajectories,\n    df0=FloatSlider(value=0.0, min=-5, max=5),\n    df1=FloatSlider(value=1.0, min=0.1, max=10),\n    p0=FloatSlider(value=0.0, min=-5, max=5),\n    p1=FloatSlider(value=1.0, min=0.1, max=10),\n    m0=FloatSlider(value=0.0, min=-5, max=5),\n    m1=FloatSlider(value=1.0, min=0.1, max=10),\n);\n```\n\n----------------------------------------\n\nTITLE: Serving Models from C++ with Container nn.Module\nDESCRIPTION: This code snippet shows how to create a container nn.Module to store a model and guide pair for serving from C++. The container should *not* be a PyroModule to avoid name conflicts. The forward method demonstrates a typical trace-replay pattern for model serving.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass Container(nn.Module):            # This cannot be a PyroModule.\n    def __init__(self, model, guide):  # These may be PyroModules.\n        super().__init__()\n        self.model = model\n        self.guide = guide\n    # This is a typical trace-replay pattern seen in model serving.\n    def forward(self, data):\n        tr = poutine.trace(self.guide).get_trace(data)\n        return poutine.replay(model, tr)(data)\n```\n\n----------------------------------------\n\nTITLE: Defining a Time Series Model in Pyro\nDESCRIPTION: This function defines a time-series model with a slowly-varying continuous latent state and Bernoulli observations using a logistic link function. It uses Pyro's probabilistic programming constructs to define the model structure.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/easyguide.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model(batch, subsample, full_size):\n    batch = list(batch)\n    num_time_steps = len(batch)\n    drift = pyro.sample(\"drift\", dist.LogNormal(-1, 0.5))\n    with pyro.plate(\"data\", full_size, subsample=subsample):\n        z = 0.\n        for t in range(num_time_steps):\n            z = pyro.sample(\"state_{}\".format(t),\n                            dist.Normal(z, drift))\n            batch[t] = pyro.sample(\"obs_{}\".format(t),\n                                   dist.Bernoulli(logits=z),\n                                   obs=batch[t])\n    return torch.stack(batch)\n```\n\n----------------------------------------\n\nTITLE: Modeling Regional Lineage Growth with Pyro in Python\nDESCRIPTION: Defines a Pyro model for multivariate logistic growth of SARS-CoV-2 lineages in different regions. It uses Pyro plates to handle dimensions for time, lineage, and region, allowing for region-specific initial conditions while sharing growth rate parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef regional_model(counts):\n    T, R, L = counts.shape\n        \n    lineage_plate = pyro.plate(\"lineages\", L, dim=-1)\n    region_plate = pyro.plate(\"region\", R, dim=-2)\n    time_plate = pyro.plate(\"time\", T, dim=-3)\n\n    with lineage_plate:\n        rate = pyro.sample(\"rate\", dist.Normal(0, 1))\n        \n    init_scale = pyro.sample(\"init_scale\", dist.LogNormal(0, 2))\n    with region_plate, lineage_plate:\n        init = pyro.sample(\"init\", dist.Normal(0, init_scale))\n\n    time = torch.arange(float(T)) * dataset[\"time_step_days\"] / 5.5\n\n    logits = init + rate * time[:, None, None]\n    \n    with time_plate, region_plate:\n        pyro.sample(\n            \"obs\",\n            dist.Multinomial(logits=logits.unsqueeze(-2), validate_args=False),\n            obs=counts.unsqueeze(-2),\n        )\n```\n\n----------------------------------------\n\nTITLE: Rendering Pyro Model with Overlapping Plates\nDESCRIPTION: Demonstrates how to render a Pyro model with overlapping non-nested plates.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npyro.render_model(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Effect Handler\nDESCRIPTION: Demonstrates how to create a custom effect handler (LogJointMessenger) that computes the log joint probability during model execution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LogJointMessenger(poutine.messenger.Messenger):\n    \n    def __init__(self, cond_data):\n        self.data = cond_data\n    \n    def __call__(self, fn):\n        def _fn(*args, **kwargs):\n            with self:\n                fn(*args, **kwargs)\n                return self.logp.clone()\n        return _fn\n    \n    def __enter__(self):\n        self.logp = torch.tensor(0.)\n        return super().__enter__()\n    \n    def __exit__(self, exc_type, exc_value, traceback):\n        self.logp = torch.tensor(0.)\n        return super().__exit__(exc_type, exc_value, traceback)\n    \n    def _pyro_sample(self, msg):\n        assert msg[\"name\"] in self.data\n        msg[\"value\"] = self.data[msg[\"name\"]]\n        msg[\"is_observed\"] = True\n        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n\nwith LogJointMessenger(cond_data={\"measurement\": 9.5, \"weight\": 8.23}) as m:\n    scale(8.5)\n    print(m.logp.clone())\n    \nscale_log_joint = LogJointMessenger(cond_data={\"measurement\": 9.5, \"weight\": 8.23})(scale)\nprint(scale_log_joint(8.5))\n```\n\n----------------------------------------\n\nTITLE: Training Variational Inference with Pyro's SVI in Python\nDESCRIPTION: Fits a Pyro model using stochastic variational inference. The snippet executes this process for 3001 steps, employing a guide function derived by partially applying the regional model and counts data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n%%time\nguide = fit_svi(partial(regional_model, counts), num_steps=3001)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Baseline Neural Network for MNIST Quadrant Prediction\nDESCRIPTION: This neural network serves as a baseline model for predicting missing MNIST quadrants. It consists of a simple feed-forward architecture with two hidden layers and sigmoid activation for the output layer to generate pixel values.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass BaselineNet(nn.Module):\n    def __init__(self, hidden_1, hidden_2):\n        super().__init__()\n        self.fc1 = nn.Linear(784, hidden_1)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        self.fc3 = nn.Linear(hidden_2, 784)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = x.view(-1, 784)\n        hidden = self.relu(self.fc1(x))\n        hidden = self.relu(self.fc2(hidden))\n        y = torch.sigmoid(self.fc3(hidden))\n        return y\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Probabilistic Model\nDESCRIPTION: Defines a simple probabilistic model called 'scale' with two random variables: weight and measurement.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef scale(guess):\n    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n```\n\n----------------------------------------\n\nTITLE: Conditioning a Pyro Model on Observations\nDESCRIPTION: Demonstrates how to condition a generative model on observed data using pyro.condition(). This creates a new model that fixes the measurement value to 9.5 while preserving the original input-output structure.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconditioned_scale = pyro.condition(scale, data={\"measurement\": torch.tensor(9.5)})\n```\n\n----------------------------------------\n\nTITLE: Implementing SimpleSIRModel Class in Python\nDESCRIPTION: Defines a basic SIR epidemiological model class that inherits from CompartmentalModel. Implements required methods for global parameter sampling, state initialization, and state transitions. Includes functionality for handling both individual timesteps and sliced time periods.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleSIRModel(CompartmentalModel):\n    def __init__(self, population, recovery_time, data):\n        compartments = (\"S\", \"I\")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n        self.data = data\n\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(\"R0\", dist.LogNormal(0., 1.))\n        rho = pyro.sample(\"rho\", dist.Beta(100, 100))\n        return R0, tau, rho\n\n    def initialize(self, params):\n        # Start with a single infection.\n        return {\"S\": self.population - 1, \"I\": 1}\n\n    def transition(self, params, state, t):\n        R0, tau, rho = params\n\n        # Sample flows between compartments.\n        S2I = pyro.sample(\"S2I_{}\".format(t),\n                          infection_dist(individual_rate=R0 / tau,\n                                         num_susceptible=state[\"S\"],\n                                         num_infectious=state[\"I\"],\n                                         population=self.population))\n        I2R = pyro.sample(\"I2R_{}\".format(t),\n                          binomial_dist(state[\"I\"], 1 / tau))\n\n        # Update compartments with flows.\n        state[\"S\"] = state[\"S\"] - S2I\n        state[\"I\"] = state[\"I\"] + S2I - I2R\n\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(\"obs_{}\".format(t),\n                    binomial_dist(S2I, rho),\n                    obs=self.data[t] if t_is_observed else None)\n```\n\n----------------------------------------\n\nTITLE: Implementing VectorizeMessenger for Multiple Sample Vectorization\nDESCRIPTION: Creates a messenger class that enables parallel sampling across multiple instances using a global dimension, inheriting from GlobalNameMessenger for proper dimension management.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.contrib.funsor.handlers.named_messenger import GlobalNamedMessenger\nfrom pyro.contrib.funsor.handlers.runtime import DimRequest, DimType\n\nclass VectorizeMessenger(GlobalNamedMessenger):\n    \n    def __init__(self, size, name=\"_PARTICLES\"):\n        super().__init__()\n        self.name = name\n        self.size = size\n\n    @pyroapi.pyro_backend(\"contrib.funsor\")\n    def _pyro_sample(self, msg):\n        if msg[\"is_observed\"] or msg[\"done\"] or msg[\"infer\"].get(\"enumerate\") == \"parallel\":\n            return\n        \n        if self.name in pyro.to_funsor(msg[\"fn\"], funsor.Real).inputs:\n            raw_value = msg[\"fn\"].rsample()\n        else:\n            raw_value = msg[\"fn\"].rsample(sample_shape=(self.size,))\n        \n        fresh_dim = len(msg[\"fn\"].event_shape) - raw_value.dim()\n        funsor_value = pyro.to_funsor(\n            raw_value,\n            output=funsor.Reals[tuple(msg[\"fn\"].event_shape)],\n            dim_to_name={fresh_dim: DimRequest(value=self.name, dim_type=DimType.GLOBAL)},\n        )\n        \n        msg[\"value\"] = pyro.to_data(funsor_value)\n        msg[\"done\"] = True\n```\n\n----------------------------------------\n\nTITLE: Defining Guide with Subsampling in Pyro\nDESCRIPTION: This snippet defines a guide function in Pyro that samples both global and local random variables, making use of the subsampling technique to improve efficiency.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef guide(data):\n    beta = pyro.sample(\"beta\", ...) # sample the global RV\n    for i in pyro.plate(\"locals\", len(data), subsample_size=5):\n        # sample the local RVs\n        pyro.sample(\"z_{}\".format(i), ..., lambda_i)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PyroModule Tutorial\nDESCRIPTION: Imports necessary Python libraries including PyTorch, Pyro, and related modules for the PyroModule tutorial.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torch.nn as nn\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom torch.distributions import constraints\nfrom pyro.nn import PyroModule, PyroParam, PyroSample\nfrom pyro.nn.module import to_pyro_module_\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.optim import Adam\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Processing in Messenger Class\nDESCRIPTION: Core implementation of the message processing logic in the Messenger base class. Shows how different message types are dispatched to appropriate handler methods.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Messenger:\n    ...\n    def _process_message(self, msg):\n        method_name = \"_pyro_{}\".format(msg[\"type\"])  # e.g. _pyro_sample when msg[\"type\"] == \"sample\"\n        if hasattr(self, method_name):\n            getattr(self, method_name)(msg)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Optimizing the Acquisition Function with LBFGS\nDESCRIPTION: Implements a function to find the minimum of the acquisition function using the LBFGS optimizer. It handles domain constraints by transforming the optimization problem to an unconstrained domain and back.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef find_a_candidate(x_init, lower_bound=0, upper_bound=1):\n    # transform x to an unconstrained domain\n    constraint = constraints.interval(lower_bound, upper_bound)\n    unconstrained_x_init = transform_to(constraint).inv(x_init)\n    unconstrained_x = unconstrained_x_init.clone().detach().requires_grad_(True)\n    minimizer = optim.LBFGS([unconstrained_x], line_search_fn='strong_wolfe')\n\n    def closure():\n        minimizer.zero_grad()\n        x = transform_to(constraint)(unconstrained_x)\n        y = lower_confidence_bound(x)\n        autograd.backward(unconstrained_x, autograd.grad(y, unconstrained_x))\n        return y\n    \n    minimizer.step(closure)\n    # after finding a candidate in the unconstrained domain, \n    # convert it back to original domain.\n    x = transform_to(constraint)(unconstrained_x)\n    return x.detach()\n```\n\n----------------------------------------\n\nTITLE: Creating Parametrized Conditional Models with Lambda Functions\nDESCRIPTION: Shows how to defer conditioning by defining a function that takes both the measurement value and guess as inputs. This approach allows for more flexible model conditioning at runtime.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef deferred_conditioned_scale(measurement, guess):\n    return pyro.condition(scale, data={\"measurement\": measurement})(guess)\n```\n\n----------------------------------------\n\nTITLE: Implementing Log Joint Probability with Explicit Handlers\nDESCRIPTION: Shows a more detailed implementation of the log joint probability computation using TraceMessenger and ConditionMessenger explicitly.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.poutine.trace_messenger import TraceMessenger\nfrom pyro.poutine.condition_messenger import ConditionMessenger\n\ndef make_log_joint_2(model):\n    def _log_joint(cond_data, *args, **kwargs):\n        with TraceMessenger() as tracer:\n            with ConditionMessenger(data=cond_data):\n                model(*args, **kwargs)\n        \n        trace = tracer.trace\n        logp = 0.\n        for name, node in trace.nodes.items():\n            if node[\"type\"] == \"sample\":\n                if node[\"is_observed\"]:\n                    assert node[\"value\"] is cond_data[name]\n                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n        return logp\n    return _log_joint\n\nscale_log_joint = make_log_joint_2(scale)\nprint(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))\n```\n\n----------------------------------------\n\nTITLE: Complete Coin Fairness Implementation\nDESCRIPTION: Full implementation combining model, guide, optimizer setup, and training loop for coin fairness inference.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport os\nimport torch\nimport torch.distributions.constraints as constraints\nimport pyro\nfrom pyro.optim import Adam\nfrom pyro.infer import SVI, Trace_ELBO\nimport pyro.distributions as dist\n\n# this is for running the notebook in our testing framework\nsmoke_test = ('CI' in os.environ)\nn_steps = 2 if smoke_test else 2000\n\nassert pyro.__version__.startswith('1.9.1')\n\n# clear the param store in case we're in a REPL\npyro.clear_param_store()\n\n# create some data with 6 observed heads and 4 observed tails\ndata = []\nfor _ in range(6):\n    data.append(torch.tensor(1.0))\nfor _ in range(4):\n    data.append(torch.tensor(0.0))\n\ndef model(data):\n    # define the hyperparameters that control the Beta prior\n    alpha0 = torch.tensor(10.0)\n    beta0 = torch.tensor(10.0)\n    # sample f from the Beta prior\n    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n    # loop over the observed data\n    for i in range(len(data)):\n        # observe datapoint i using the ernoulli likelihood\n        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n\ndef guide(data):\n    # register the two variational parameters with Pyro\n    # - both parameters will have initial value 15.0. \n    # - because we invoke constraints.positive, the optimizer \n    # will take gradients on the unconstrained parameters\n    # (which are related to the constrained parameters by a log)\n    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0), \n                         constraint=constraints.positive)\n    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0), \n                        constraint=constraints.positive)\n    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n\n# setup the optimizer\nadam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\noptimizer = Adam(adam_params)\n\n# setup the inference algorithm\nsvi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n\n# do gradient steps\nfor step in range(n_steps):\n    svi.step(data)\n    if step % 100 == 0:\n        print('.', end='')\n\n# grab the learned variational parameters\nalpha_q = pyro.param(\"alpha_q\").item()\nbeta_q = pyro.param(\"beta_q\").item()\n\n# here we use some facts about the Beta distribution\n# compute the inferred mean of the coin's fairness\ninferred_mean = alpha_q / (alpha_q + beta_q)\n```\n\n----------------------------------------\n\nTITLE: Low-Level SVI Implementation Pattern in Pyro\nDESCRIPTION: Demonstrates the equivalent low-level implementation of the SVI optimization loop, providing more control over the inference process by directly manipulating the differentiable loss.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nloss_fn = lambda model, guide: pyro.infer.Trace_ELBO().differentiable_loss(model, guide, X_train, y_train)\nwith pyro.poutine.trace(param_only=True) as param_capture:\n    loss = loss_fn(model, guide)\nparams = set(site[\"value\"].unconstrained()\n                for site in param_capture.trace.nodes.values())\noptimizer = torch.optim.Adam(params, lr=0.001, betas=(0.90, 0.999))\nfor i in range(n_iter):\n    # compute loss\n    loss = loss_fn(model, guide)\n    loss.backward()\n    # take a step and zero the parameter gradients\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Connecting PyroModules with Attribute Links\nDESCRIPTION: This code snippet demonstrates how to connect PyroModules using attribute links. By nesting one PyroModule within another, you ensure that they are connected to the root model, which avoids name conflicts.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_21\n\nLANGUAGE: diff\nCODE:\n```\n  class Model(PyroModule):\n      def __init__(self):\n-         self.x = nn.Module()    # Could lead to name conflict.\n+         self.x = PyroModule()   # Ensures y is conected to root Model.\n          self.x.y = PyroModule()\n```\n\n----------------------------------------\n\nTITLE: Defensive Parameter Constraints Example\nDESCRIPTION: Shows how to implement defensive parameter constraints to avoid numerical instabilities, using the concentration parameter of a Gamma distribution as an example.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.distributions import constraints\n\nconcentration = pyro.param(\"concentration\", torch.tensor(0.5),\n                           constraints.greater_than(0.001))\n```\n\n----------------------------------------\n\nTITLE: Implementing a Guide Function without EasyGuide in Pyro\nDESCRIPTION: This function defines a guide for the time series model without using EasyGuide. It uses Delta distributions for point estimates and LowRankMultivariateNormal for modeling local states with shared uncertainty and local means.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/easyguide.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrank = 3\n    \ndef guide(batch, subsample, full_size):\n    num_time_steps, batch_size = batch.shape\n\n    # MAP estimate the drift.\n    drift_loc = pyro.param(\"drift_loc\", lambda: torch.tensor(0.1),\n                           constraint=constraints.positive)\n    pyro.sample(\"drift\", dist.Delta(drift_loc))\n\n    # Model local states using shared uncertainty + local mean.\n    cov_diag = pyro.param(\"state_cov_diag\",\n                          lambda: torch.full((num_time_steps,), 0.01),\n                         constraint=constraints.positive)\n    cov_factor = pyro.param(\"state_cov_factor\",\n                            lambda: torch.randn(num_time_steps, rank) * 0.01)\n    with pyro.plate(\"data\", full_size, subsample=subsample):\n        # Sample local mean.\n        loc = pyro.param(\"state_loc\",\n                         lambda: torch.full((full_size, num_time_steps), 0.5),\n                         event_dim=1)\n        states = pyro.sample(\"states\",\n                             dist.LowRankMultivariateNormal(loc, cov_factor, cov_diag),\n                             infer={\"is_auxiliary\": True})\n        # Unpack the joint states into one sample site per time step.\n        for t in range(num_time_steps):\n            pyro.sample(\"state_{}\".format(t), dist.Delta(states[:, t]))\n```\n\n----------------------------------------\n\nTITLE: Defining Probabilistic Model\nDESCRIPTION: Implements the probabilistic model for object tracking with plates for objects and detections.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tracking_1d.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model(args, observations):\n    with pyro.plate(\"objects\", args.max_num_objects):\n        exists = pyro.sample(\"exists\",\n                             dist.Bernoulli(args.expected_num_objects / args.max_num_objects))\n        with poutine.mask(mask=exists.bool()):\n            states = pyro.sample(\"states\", dist.Normal(0., 1.).expand([2]).to_event(1))\n            positions = get_dynamics(args.num_frames).mm(states.t())\n    with pyro.plate(\"detections\", observations.shape[1]):\n        with pyro.plate(\"time\", args.num_frames):\n            # The combinatorial part of the log prob is approximated to allow independence.\n            is_observed = (observations[..., -1] > 0)\n            with poutine.mask(mask=is_observed):\n                assign = pyro.sample(\"assign\",\n                                     dist.Categorical(torch.ones(args.max_num_objects + 1)))\n            is_spurious = (assign == args.max_num_objects)\n            is_real = is_observed & ~is_spurious\n            num_observed = is_observed.float().sum(-1, True)\n            pyro.sample(\"is_real\",\n                        dist.Bernoulli(args.expected_num_objects / num_observed),\n                        obs=is_real.float())\n            pyro.sample(\"is_spurious\",\n                        dist.Bernoulli(args.expected_num_spurious / num_observed),\n                        obs=is_spurious.float())\n\n            # The remaining continuous part is exact.\n            observed_positions = observations[..., 0]\n            with poutine.mask(mask=is_real):\n                bogus_position = positions.new_zeros(args.num_frames, 1)\n                augmented_positions = torch.cat([positions, bogus_position], -1)\n                predicted_positions = gather(augmented_positions, assign, -1)\n                pyro.sample(\"real_observations\",\n                            dist.Normal(predicted_positions, args.emission_noise_scale),\n                            obs=observed_positions)\n            with poutine.mask(mask=is_spurious):\n                pyro.sample(\"spurious_observations\", dist.Normal(0., 1.),\n                            obs=observed_positions)\n```\n\n----------------------------------------\n\nTITLE: Invalid Downstream Coupling Example\nDESCRIPTION: Shows an invalid model where variables outside a vectorized plate depend on enumerated variables inside the plate.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@config_enumerate\ndef invalid_model(data):\n    with pyro.plate(\"plate\", 10):\n        x = pyro.sample(\"x\", dist.Bernoulli(0.5))\n    assert x.shape == (10,)\n    pyro.sample(\"obs\", dist.Normal(x.sum(), 1.), data)\n```\n\n----------------------------------------\n\nTITLE: Evaluating DPMM Concentration Parameter with Log Predictive Scores\nDESCRIPTION: Performs model criticism by evaluating different concentration parameter (alpha) values for a Dirichlet Process Mixture Model using log predictive scoring on held-out test data. This helps optimize the model's hyperparameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Hold out 10% of our original data to test upon\ndf_test = df.sample(frac=0.1)\ndata = torch.tensor(df.drop(df_test.index)['sunspot.year'].values, dtype=torch.float).round()\ndata_test = torch.tensor(df_test['sunspot.year'].values, dtype=torch.float).round()\nN = data.shape[0]\nN_test = data_test.shape[0]\n\nalphas = [0.05, 0.1, 0.5, 0.75, 0.9, 1., 1.25, 1.5, 2, 2.5, 3]\nlog_predictives = []\n\nfor val in alphas:\n    alpha = val\n    T = 20\n    svi = SVI(model, guide, optim, loss=Trace_ELBO())\n    train(500)\n    \n    S = 100 # number of Monte Carlo samples to use in posterior predictive computations\n\n    # Using pyro's built in posterior predictive class:\n    posterior = Predictive(guide, num_samples=S, return_sites=[\"beta\", \"lambda\"])(data)\n    post_pred_weights = mix_weights(posterior[\"beta\"])\n    post_pred_clusters = posterior[\"lambda\"]\n\n    # log_prob shape = N_test x S\n    log_prob = (post_pred_weights.log() + Poisson(post_pred_clusters).log_prob(data.reshape(-1, 1, 1))).logsumexp(-1)\n    mean_log_prob = log_prob.logsumexp(-1) - np.log(S)\n    log_posterior_predictive = mean_log_prob.sum(-1)\n    log_predictives.append(log_posterior_predictive)\n\nplt.figure(figsize=(10, 5))\nplt.plot(alphas, log_predictives)\nplt.title(\"Value of the Log Predictive at Varying Alpha\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Model for Time Series Simulation\nDESCRIPTION: Creates a Pyro model function that simulates a time series with parameters sampled from prior distributions. It includes process noise and measurement noise.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef model(T: int = 1000, data=None):\n    # Sample parameters from the prior.\n    df = pyro.sample(\"df\", dist.LogNormal(0, 1))\n    p_scale = pyro.sample(\"p_scale\", dist.LogNormal(0, 1))  # process noise\n    m_scale = pyro.sample(\"m_scale\", dist.LogNormal(0, 1))  # measurement noise\n    \n    # Simulate a time series.\n    with pyro.plate(\"dt\", T):\n        process_noise = pyro.sample(\"process_noise\", dist.StudentT(df, 0, p_scale))\n    trend = pyro.deterministic(\"trend\", process_noise.cumsum(-1))\n    with pyro.plate(\"t\", T):\n        return pyro.sample(\"obs\", dist.Normal(trend, m_scale), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Implementing Prior Step Function with Mini-batch Support in Pyro\nDESCRIPTION: Defines a prior step function that handles mini-batches by incorporating masking to handle completed samples and conditional object addition based on Bernoulli sampling. Uses z_pres, z_where, and z_what variables for object presence, position, and features.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef prior_step(n, t, prev_x, prev_z_pres):\n\n    # Sample variable indicating whether to add this object to the output.\n\n    # We multiply the success probability of 0.5 by the value sampled for this\n    # choice in the previous step. By doing so we add objects to the output until\n    # the first 0 is sampled, after which we add no further objects.\n    z_pres = pyro.sample('z_pres_{}'.format(t), \n                         dist.Bernoulli(0.5 * prev_z_pres)\n                             .to_event(1))\n    \n    z_where = pyro.sample('z_where_{}'.format(t),\n                          dist.Normal(z_where_prior_loc.expand(n, -1),\n                                      z_where_prior_scale.expand(n, -1))\n                              .mask(z_pres)\n                              .to_event(1))\n\n    z_what = pyro.sample('z_what_{}'.format(t),\n                         dist.Normal(z_what_prior_loc.expand(n, -1),\n                                     z_what_prior_scale.expand(n, -1))\n                             .mask(z_pres)\n                             .to_event(1))\n\n    y_att = decode(z_what)\n    y = object_to_image(z_where, y_att)\n\n    # Combine the image generated at this step with the image so far.\n    x = prev_x + y * z_pres.view(-1, 1, 1)\n\n    return x, z_pres\n```\n\n----------------------------------------\n\nTITLE: Variational Guide Implementation\nDESCRIPTION: Implements the guide function that defines the variational distribution using a Beta distribution with trainable parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef guide(data):\n    # register the two variational parameters with Pyro.\n    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0), \n                         constraint=constraints.positive)\n    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0), \n                        constraint=constraints.positive)\n    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n```\n\n----------------------------------------\n\nTITLE: Implementing Heterogeneous SIR Model with Time-Varying Parameters\nDESCRIPTION: Implementation of a HeterogeneousSIR model that handles time-varying epidemiological parameters through Brownian motion in log-space, demonstrating initialization and transition methods.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass HeterogeneousSIRModel(CompartmentalModel):\n    ...\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(\"R0\", dist.LogNormal(0., 1.))\n        rho = ...\n        return R0, tau, rho\n\n    def initialize(self, params):\n        # Start with a single infection.\n        # We also store the initial beta value in the state dict.\n        return {\"S\": self.population - 1, \"I\": 1, \"beta\": torch.tensor(1.)}\n\n    def transition(self, params, state, t):\n        R0, tau, rho = params\n        # Sample heterogeneous variables.\n        # This assumes beta slowly drifts via Brownian motion in log space.\n        beta = pyro.sample(\"beta_{}\".format(t),\n                           dist.LogNormal(state[\"beta\"].log(), 0.1))\n        Rt = pyro.deterministic(\"Rt_{}\".format(t), R0 * beta)\n\n        # Sample flows between compartments.\n        S2I = pyro.sample(\"S2I_{}\".format(t),\n                          infection_dist(individual_rate=Rt / tau,\n                                         num_susceptible=state[\"S\"],\n                                         num_infectious=state[\"I\"],\n                                         population=self.population))\n        ...\n        # Update compartments and heterogeneous variables.\n        state[\"S\"] = state[\"S\"] - S2I\n        state[\"I\"] = state[\"I\"] + S2I - I2R\n        state[\"beta\"] = beta  # We store the latest beta value in the state dict.\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring SVI with Enumerated Discrete Latents\nDESCRIPTION: Modifies the SVI setup to automatically sum out discrete latent variables using config_enumerate, replacing Monte Carlo sampling with exact enumeration.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ss-vae.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsvi = SVI(model, config_enumerate(guide), optimizer, loss=TraceEnum_ELBO(max_plate_nesting=1))\n```\n\n----------------------------------------\n\nTITLE: Implementing Deep Markov Model as a PyTorch Module\nDESCRIPTION: Defines a PyTorch Module class (DMM) that encapsulates both the model and variational guide for a Deep Markov Model. It initializes neural network components for both the generative model (Emitter, GatedTransition) and inference network (Combiner, RNN), along with trainable parameters for initial states.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass DMM(nn.Module):\n    \"\"\"\n    This PyTorch Module encapsulates the model as well as the \n    variational distribution (the guide) for the Deep Markov Model\n    \"\"\"\n    def __init__(self, input_dim=88, z_dim=100, emission_dim=100, \n                 transition_dim=200, rnn_dim=600, rnn_dropout_rate=0.0, \n                 num_iafs=0, iaf_dim=50, use_cuda=False):\n        super().__init__()\n        # instantiate pytorch modules used in the model and guide below\n        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n        self.trans = GatedTransition(z_dim, transition_dim)\n        self.combiner = Combiner(z_dim, rnn_dim)\n        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, \n                          nonlinearity='relu', batch_first=True, \n                          bidirectional=False, num_layers=1, dropout=rnn_dropout_rate)\n\n        # define a (trainable) parameters z_0 and z_q_0 that help define \n        # the probability distributions p(z_1) and q(z_1)\n        # (since for t = 1 there are no previous latents to condition on)\n        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n        # define a (trainable) parameter for the initial hidden state of the rnn\n        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n\n        self.use_cuda = use_cuda\n        # if on gpu cuda-ize all pytorch (sub)modules\n        if use_cuda:\n            self.cuda()\n\n    # the model p(x_{1:T} | z_{1:T}) p(z_{1:T})\n    def model(...):\n\n        # ... as above ...\n\n    # the guide q(z_{1:T} | x_{1:T}) (i.e. the variational distribution)\n    def guide(...):\n        \n        # ... as above ...\n```\n\n----------------------------------------\n\nTITLE: Defining Probabilistic Model\nDESCRIPTION: Implements a simple probabilistic model with normal distributions where observations must be passed as a dictionary argument.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/csis.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model(prior_mean, observations={\"x1\": 0, \"x2\": 0}):\n    x = pyro.sample(\"z\", dist.Normal(prior_mean, torch.tensor(5**0.5)))\n    y1 = pyro.sample(\"x1\", dist.Normal(x, torch.tensor(2**0.5)), obs=observations[\"x1\"])\n    y2 = pyro.sample(\"x2\", dist.Normal(x, torch.tensor(2**0.5)), obs=observations[\"x2\"])\n    return x\n```\n\n----------------------------------------\n\nTITLE: Implementing a Guide Function with EasyGuide in Pyro\nDESCRIPTION: This function demonstrates how to use the @easy_guide decorator to simplify the guide implementation. It uses map_estimate for drift and group sampling for local states, showcasing EasyGuide's features for more concise guide definitions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/easyguide.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@easy_guide(model)\ndef guide(self, batch, subsample, full_size):\n    # MAP estimate the drift.\n    self.map_estimate(\"drift\")\n\n    # Model local states using shared uncertainty + local mean.\n    group = self.group(match=\"state_[0-9]*\")  # Selects all local variables.\n    cov_diag = pyro.param(\"state_cov_diag\",\n                          lambda: torch.full(group.event_shape, 0.01),\n                          constraint=constraints.positive)\n    cov_factor = pyro.param(\"state_cov_factor\",\n                            lambda: torch.randn(group.event_shape + (rank,)) * 0.01)\n    with self.plate(\"data\", full_size, subsample=subsample):\n        # Sample local mean.\n        loc = pyro.param(\"state_loc\",\n                         lambda: torch.full((full_size,) + group.event_shape, 0.5),\n                         event_dim=1)\n        # Automatically sample the joint latent, then unpack and replay model sites.\n        group.sample(\"states\", dist.LowRankMultivariateNormal(loc, cov_factor, cov_diag))\n```\n\n----------------------------------------\n\nTITLE: Implementing Probabilistic PCA with FactorMuE in Python\nDESCRIPTION: A Python implementation example showing how to combine Probabilistic PCA with MuE using the FactorMuE class. The code demonstrates factor analysis techniques within the Pyro probabilistic programming framework.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mue_factor.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Referenced from github: pyro-ppl/pyro/examples/contrib/mue/FactorMuE.py\n```\n\n----------------------------------------\n\nTITLE: Subsampling Tensors Inside a Pyro Plate\nDESCRIPTION: This snippet shows how to subsample data inside a `pyro.plate` by specifying both the original data size and the subsample size. Pyro selects a random subset of data indices.  The code demonstrates accessing a minibatch of data and the relevant per-datum parameters using the indices generated by the plate. The `test_model` function is called to test the model with a dummy guide and the `Trace_ELBO` loss.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"data = torch.arange(100.)\\n\\ndef model2():\\n    mean = pyro.param(\\\"mean\\\", torch.zeros(len(data)))\\n    with pyro.plate(\\\"data\\\", len(data), subsample_size=10) as ind:\\n        assert len(ind) == 10    # ind is a LongTensor that indexes the subsample.\\n        batch = data[ind]        # Select a minibatch of data.\\n        mean_batch = mean[ind]   # Take care to select the relevant per-datum parameters.\\n        # Do stuff with batch:\\n        x = pyro.sample(\\\"x\\\", Normal(mean_batch, 1), obs=batch)\\n        assert len(x) == 10\\n        \\ntest_model(model2, guide=lambda: None, loss=Trace_ELBO())\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Object Generation\nDESCRIPTION: Defines the decoder neural network and prior distribution functions for generating single objects in the scene.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(50, 200)\n        self.l2 = nn.Linear(200, 400)\n\n    def forward(self, z_what):\n        h = relu(self.l1(z_what))\n        return sigmoid(self.l2(h))\n\ndecode = Decoder()\n\nz_where_prior_loc = torch.tensor([3., 0., 0.])\nz_where_prior_scale = torch.tensor([0.1, 1., 1.])\nz_what_prior_loc = torch.zeros(50)\nz_what_prior_scale = torch.ones(50)\n\ndef prior_step_sketch(t):\n    z_where = pyro.sample('z_where_{}'.format(t),\n                          dist.Normal(z_where_prior_loc.expand(1, -1),\n                                      z_where_prior_scale.expand(1, -1))\n                              .to_event(1))\n\n    z_what = pyro.sample('z_what_{}'.format(t),\n                         dist.Normal(z_what_prior_loc.expand(1, -1),\n                                     z_what_prior_scale.expand(1, -1))\n                             .to_event(1))\n    \n    y_att = decode(z_what)\n    y = object_to_image(z_where, y_att)\n    return y\n```\n\n----------------------------------------\n\nTITLE: Running SVI with Standard Trace_ELBO\nDESCRIPTION: Demonstrates running Stochastic Variational Inference (SVI) using the standard Trace_ELBO for comparison with the JIT-compiled version.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\nelbo = Trace_ELBO()\nsvi = SVI(model, guide, Adam({'lr': 0.01}), elbo)\nfor i in range(2 if smoke_test else 1000):\n    svi.step(data)\n```\n\n----------------------------------------\n\nTITLE: Defining Model and Guide for SVI\nDESCRIPTION: Implements the probabilistic model and guide for Stochastic Variational Inference (SVI). Uses AutoDelta for MAP estimation of position and measurement covariances.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ekf.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    # a HalfNormal can be used here as well\n    R = pyro.sample('pv_cov', dist.HalfCauchy(2e-6)) * torch.eye(4)\n    Q = pyro.sample('measurement_cov', dist.HalfCauchy(1e-6)) * torch.eye(2)\n    # observe the measurements\n    pyro.sample('track_{}'.format(i), EKFDistribution(xs_truth[0], R, ncv,\n                                                      Q, time_steps=num_frames),\n                obs=data)\n    \nguide = AutoDelta(model)  # MAP estimation\n```\n\n----------------------------------------\n\nTITLE: Validation and Test Data Preparation\nDESCRIPTION: Prepares validation and test data for evaluation by creating vectorized copies for parallel processing. Includes sequence packing and CUDA support.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# package repeated copies of val/test data for faster evaluation\n# (i.e. set us up for vectorization)\ndef rep(x):\n    return np.repeat(x, n_eval_samples, axis=0)\n\n# get the validation/test data ready for the dmm: pack into sequences, etc.\nval_seq_lengths = rep(val_seq_lengths)\ntest_seq_lengths = rep(test_seq_lengths)\nval_batch, val_batch_reversed, val_batch_mask, val_seq_lengths = poly.get_mini_batch(\n    np.arange(n_eval_samples * val_data_sequences.shape[0]), rep(val_data_sequences),\n    val_seq_lengths, cuda=args.cuda)\ntest_batch, test_batch_reversed, test_batch_mask, test_seq_lengths = \\\n    poly.get_mini_batch(np.arange(n_eval_samples * test_data_sequences.shape[0]),             \n                        rep(test_data_sequences),\n                        test_seq_lengths, cuda=args.cuda)\n```\n\n----------------------------------------\n\nTITLE: Implementing MCMC with LKJ Prior for Covariances in Pyro\nDESCRIPTION: This script demonstrates how to use MCMC sampling with an LKJ prior over covariances in Pyro. It imports necessary libraries, defines a model with LKJ prior, sets up MCMC sampling, and visualizes the results.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/lkj.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.mcmc import MCMC, NUTS\nimport matplotlib.pyplot as plt\n\n\n# this is for running the notebook in our testing framework\nsmoke_test = ('CI' in os.environ)\nnum_samples = 20 if smoke_test else 1000\n\n\ndef model(K, n_data=100):\n    # K is the dimension of correlation matrix\n    # n_data is the number of data points we're going to sample\n    theta = pyro.sample('theta', dist.LKJCorrCholesky(K, concentration=2.))\n    # the LKJ distribution generates correlation matrices, not covariance matrices\n    # to get a distribution over covariance matrices we need to multiply by a\n    # scaling factor on each dimension.\n    scaling = pyro.sample('scaling', dist.LogNormal(0., 2.).expand([K]).to_event(1))\n    sigma = torch.mm(torch.mm(torch.diag(scaling), theta), theta.t() * torch.diag(scaling))\n    # now let's sample data from a multivariate normal with the covariance\n    pyro.sample('data', dist.MultivariateNormal(torch.zeros(K), sigma).expand([n_data]).to_event(1))\n\n\ndef main(num_samples=1000):\n    nuts_kernel = NUTS(model, adapt_step_size=True)\n    mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=num_samples)\n    mcmc.run(4)\n    samples = mcmc.get_samples()\n    theta = samples['theta']\n    scaling = samples['scaling']\n    # let's compute the average covariance matrix\n    sigma = torch.mean(torch.matmul(torch.matmul(torch.diag_embed(scaling), theta),\n                                   torch.matmul(theta.transpose(-1, -2),\n                                                torch.diag_embed(scaling))), 0)\n    plt.imshow(sigma)\n    plt.colorbar()\n    plt.savefig('covmat.pdf')\n\n\nif __name__ == '__main__':\n    main(num_samples)\n```\n\n----------------------------------------\n\nTITLE: Ice Cream Sales Model\nDESCRIPTION: Example of model composition by extending the weather model to predict ice cream sales.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef ice_cream_sales():\n    cloudy, temp = weather()\n    expected_sales = 200. if cloudy == 'sunny' and temp > 80.0 else 50.\n    ice_cream = pyro.sample('ice_cream', pyro.distributions.Normal(expected_sales, 10.0))\n    return ice_cream\n```\n\n----------------------------------------\n\nTITLE: Setting Up VAE Training Configuration in Python\nDESCRIPTION: Defines training parameters including learning rate, CUDA usage, number of epochs, and test frequency for the VAE model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Run options\nLEARNING_RATE = 1.0e-3\nUSE_CUDA = False\n\n# Run only for a single iteration for testing\nNUM_EPOCHS = 1 if smoke_test else 100\nTEST_FREQUENCY = 5\n```\n\n----------------------------------------\n\nTITLE: Creating a Complex Nested PyroModule Model\nDESCRIPTION: Builds a complex probabilistic model by combining PyroModule classes, creating a regression model with a BayesianLinear layer and observation noise parameter.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass Model(PyroModule):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.linear = BayesianLinear(in_size, out_size)  # this is a PyroModule\n        self.obs_scale = PyroSample(dist.LogNormal(0, 1))\n\n    def forward(self, input, output=None):\n        obs_loc = self.linear(input)  # this samples linear.bias and linear.weight\n        obs_scale = self.obs_scale    # this samples self.obs_scale\n        with pyro.plate(\"instances\", len(input)):\n            return pyro.sample(\"obs\", dist.Normal(obs_loc, obs_scale).to_event(1),\n                               obs=output)\n```\n\n----------------------------------------\n\nTITLE: Implementing SIR Epidemiological Model in Pyro\nDESCRIPTION: This code snippet defines and implements the SIR (Susceptible, Infected, Recovered) model using Pyro. It includes functions for generating synthetic data, defining the model, and performing inference. The model uses stochastic differential equations to simulate the spread of an infectious disease in a population.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_sir.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport math\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.contrib.epidemiology import LocalAndGlobalSIRModel\nfrom pyro.contrib.epidemiology.util import infection_dist, discretize\nfrom pyro.infer import MCMC, NUTS\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.optim import ClippedAdam\nfrom pyro.util import jit\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Helper for creating masks of nan at the end of a time series\ndef make_mask(data):\n    return torch.arange(len(data)) < torch.sum(~torch.isnan(data))\n\n# Helper for plotting time series\ndef plot_series(data, label, color):\n    plt.plot(data.numpy(), \".-\", label=label, color=color)\n\n# Helper for plotting one sample\ndef plot_samples(samples, truth, legend=None):\n    plt.figure(figsize=(6, 3), dpi=150)\n    t = torch.arange(samples.shape[-1])\n    mean = samples.mean(0)\n    p5 = samples.kthvalue(int(round(0.05 * len(samples))))[0]\n    p95 = samples.kthvalue(int(round(0.95 * len(samples))))[0]\n    plt.fill_between(t, p5, p95, alpha=0.3)\n    plt.plot(t, mean, \".-\")\n    if truth is not None:\n        plt.plot(t, truth, \".-k\")\n    if legend is not None:\n        plt.legend(legend)\n\ndef generate_data(args, duration=20, population=1000, R0=1.5, gamma=0.2, rho=0.5, k=3.0):\n    # Generate ground truth data from a known SIR model.\n    dist_inf = infection_dist(\"dirac\")\n    model = LocalAndGlobalSIRModel(\n        duration, population, dist_inf,\n        beta=R0 * gamma / population, gamma=gamma,\n        rho=rho, k=k,\n    )\n\n    # Generate fully-observed data.\n    sites = (\"S\", \"I\", \"R\")\n    data = {}\n    for attempt in range(100):\n        data = model.generate({\"E\": 10.})\n        data = {site: data[\"local\"][site] for site in sites}\n        if data[\"I\"].max() > 30:\n            break\n\n    # Optionally thin the data.\n    assert 7 <= args.interval <= duration\n    data = {site: data[site][::args.interval] for site in sites}\n\n    # Optionally add noise.\n    if args.obs == \"S\": # observe only S\n        data[\"I\"] = torch.full_like(data[\"I\"], float(\"nan\"))\n        data[\"R\"] = torch.full_like(data[\"R\"], float(\"nan\"))\n    elif args.obs == \"I\": # observe only I\n        data[\"S\"] = torch.full_like(data[\"S\"], float(\"nan\"))\n        data[\"R\"] = torch.full_like(data[\"R\"], float(\"nan\"))\n    elif args.obs == \"IR\": # observe I and R\n        data[\"S\"] = torch.full_like(data[\"S\"], float(\"nan\"))\n    elif args.obs != \"SIR\":\n        raise ValueError(\"Invalid --obs \")\n    for site in (\"S\", \"I\", \"R\"):\n        noise = torch.rand(data[site].shape) < args.superspreader\n        data[site][noise] = float(\"nan\")\n\n    # Delay I by INCUBATION_PERIOD.\n    INCUBATION_PERIOD = 2\n    nan = float(\"nan\")\n    data[\"I\"] = torch.nn.functional.pad(data[\"I\"], (INCUBATION_PERIOD, 0), value=nan)[:len(data[\"I\"])]\n    assert data[\"S\"].shape == data[\"I\"].shape == data[\"R\"].shape\n\n    return data\n\nclass SIRModel(pyro.nn.PyroModule):\n    def __init__(self, duration, population):\n        super().__init__()\n        self.duration = duration\n        self.population = population\n        self.R0 = pyro.nn.PyroParam(torch.tensor(1.5),\n                                    constraint=constraints.interval(0.5, 5.))\n        self.gamma = pyro.nn.PyroParam(torch.tensor(0.2),\n                                       constraint=constraints.interval(0., 1.))\n        self.rho = pyro.nn.PyroParam(torch.tensor(0.5),\n                                     constraint=constraints.interval(0., 1.))\n        self.k = pyro.nn.PyroParam(torch.tensor(3.),\n                                   constraint=constraints.positive)\n\n    def forward(self, data):\n        # Unpack data.\n        assert \"S\" in data or \"I\" in data\n        S_obs = data.get(\"S\")\n        I_obs = data.get(\"I\")\n        R_obs = data.get(\"R\")\n        mask = make_mask(I_obs)\n        assert S_obs is None or mask.shape == S_obs.shape\n        assert I_obs is None or mask.shape == I_obs.shape\n        assert R_obs is None or mask.shape == R_obs.shape\n\n        # Sample initial state.\n        I0 = pyro.sample(\"I0\", dist.LogNormal(0., 1.).mask(False))\n        E0 = pyro.sample(\"E0\", dist.LogNormal(0., 1.).mask(False))\n        S0 = self.population - I0 - E0\n        R0 = torch.zeros(1)\n\n        # Run ODE\n        beta = self.R0 * self.gamma / self.population\n        dE = lambda S, E, I, R, t: S * beta * I\n        dI = lambda S, E, I, R, t: E * self.gamma\n        ode = (dE, dI)\n        x0 = torch.stack([S0, E0, I0, R0]).squeeze(-1)\n        solution = discretize(ode, t=torch.arange(float(self.duration)), x0=x0)\n        S, E, I, R = solution.unbind(dim=-1)\n\n        # Observe data with noise.\n        with poutine.scale(scale=0.1):\n            with pyro.plate(\"data\", self.duration, dim=-1):\n                if S_obs is not None:\n                    pyro.sample(\"S_obs\", dist.GammaPoisson(self.k, self.k / S).mask(mask),\n                                obs=S_obs)\n                if I_obs is not None:\n                    pyro.sample(\"I_obs\", dist.GammaPoisson(self.k, self.k / I).mask(mask),\n                                obs=I_obs)\n                if R_obs is not None:\n                    pyro.sample(\"R_obs\", dist.GammaPoisson(self.k, self.k / R).mask(mask),\n                                obs=R_obs)\n\n        # Return model variables.\n        return S, E, I, R\n\ndef fit_mcmc(args, data):\n    print(\"---- Fitting SIR model using MCMC ----\")\n    model = SIRModel(data[\"S\"].shape[-1], int(data[\"S\"][0] + data[\"I\"][0] + data[\"R\"][0]))\n    model = model.cuda() if args.cuda else model\n\n    nuts_kernel = NUTS(model)\n    mcmc = MCMC(nuts_kernel,\n                num_samples=args.num_samples,\n                warmup_steps=args.warmup_steps,\n                num_chains=args.num_chains)\n    mcmc.run(data)\n    return mcmc\n\nif __name__ == \"__main__\":\n    assert pyro.__version__.startswith('1.6.0')\n    parser = argparse.ArgumentParser(description=\"SIR\")\n    parser.add_argument(\"-n\", \"--num-samples\", default=1000, type=int)\n    parser.add_argument(\"--num-chains\", default=1, type=int)\n    parser.add_argument(\"--warmup-steps\", default=200, type=int)\n    parser.add_argument(\"--interval\", default=7, type=int)\n    parser.add_argument(\"--obs\", default=\"SIR\")\n    parser.add_argument(\"--superspreader\", default=0.1, type=float)\n    parser.add_argument(\"--INCUBATION-PERIOD\", default=2, type=int)\n    parser.add_argument(\"--cuda\", action=\"store_true\")\n    args = parser.parse_args()\n\n    pyro.set_rng_seed(20200420)\n    if args.cuda:\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n\n    # Generate data.    \n    data = generate_data(args)\n\n    # Fit.\n    mcmc = fit_mcmc(args, data)\n\n    for name, value in mcmc.get_samples().items():\n        print(\"{} = {:0.3g} ± {:0.3g}\".format(name, value.mean(), value.std()))\n\n    # Plot\n    fig, axes = plt.subplots(3, 1, figsize=(6, 6), sharex=True, dpi=150)\n    for ax, site in zip(axes, (\"S\", \"I\", \"R\")):\n        if site in data:\n            x = data[site][~torch.isnan(data[site])]\n            ax.scatter(range(len(x)), x, c=\"k\")\n        samples = mcmc.get_samples()[site]\n        ax.plot(samples.numpy().T, alpha=0.1, c=\"k\")\n        ax.scatter(range(len(x)), samples.mean(0), c=\"w\")\n        ax.set_ylabel(site)\n    plt.tight_layout()\n    plt.savefig(\"mcmc_fit.png\")\n```\n\n----------------------------------------\n\nTITLE: Computing Inferred Standard Deviation for Coin Fairness\nDESCRIPTION: Calculates the statistical standard deviation and mean for coin fairness estimation using beta distribution parameters and mathematical computation\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfactor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\ninferred_std = inferred_mean * math.sqrt(factor)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\nBased on the data and our prior belief, the fairness \" +\n      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))\n```\n\n----------------------------------------\n\nTITLE: Enumerating Multiple Discrete Latent Variables in Pyro\nDESCRIPTION: Demonstrates how Pyro handles enumeration of multiple discrete latent variables, allocating different tensor dimensions for each variable. Uses a simple chain model with three categorical variables.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@config_enumerate\ndef model():\n    p = pyro.param(\"p\", torch.randn(3, 3).exp(), constraint=constraints.simplex)\n    x = pyro.sample(\"x\", dist.Categorical(p[0]))\n    y = pyro.sample(\"y\", dist.Categorical(p[x]))\n    z = pyro.sample(\"z\", dist.Categorical(p[y]))\n    print(f\"  model x.shape = {x.shape}\")\n    print(f\"  model y.shape = {y.shape}\")\n    print(f\"  model z.shape = {z.shape}\")\n    return x, y, z\n    \ndef guide():\n    pass\n\npyro.clear_param_store()\nprint(\"Sampling:\")\nmodel()\nprint(\"Enumerated Inference:\")\nelbo = TraceEnum_ELBO(max_plate_nesting=0)\nelbo.loss(model, guide);\n```\n\n----------------------------------------\n\nTITLE: Optimized HMM Model with Markov Property\nDESCRIPTION: Complete implementation of HMM with pyro.markov annotation to handle extended enumeration efficiently using dimension recycling.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef hmm_model(data, data_dim, hidden_dim=10):\n    with pyro.plate(\"hidden_state\", hidden_dim):\n        transition = pyro.sample(\"transition\", dist.Dirichlet(0.5 * torch.ones(hidden_dim)))\n        emission = pyro.sample(\"emission\", dist.Dirichlet(0.5 * torch.ones(data_dim)))\n\n    x = 0  # initial state\n    for t, y in pyro.markov(enumerate(data)):\n        x = pyro.sample(f\"x_{t}\", dist.Categorical(transition[x]),\n                        infer={\"enumerate\": \"parallel\"})\n        pyro.sample(f\"y_{t}\", dist.Categorical(emission[x]), obs=y)\n        print(f\"x_{t}.shape = {x.shape}\")\n\n# We'll reuse the same guide and elbo.\nelbo.loss(hmm_model, hmm_guide, data, data_dim=data_dim);\n```\n\n----------------------------------------\n\nTITLE: Neural Network Baseline Implementation in Pyro\nDESCRIPTION: Demonstrates how to implement a neural network-based baseline for variance reduction, including baseline network definition and usage in a guide function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BaselineNN(nn.Module):\n    def __init__(self, dim_input, dim_hidden):\n        super().__init__()\n        self.linear = nn.Linear(dim_input, dim_hidden)\n        # ... finish initialization ...\n\n    def forward(self, x):\n        hidden = self.linear(x)\n        # ... do more computations ...\n        return baseline\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of plate Context Manager in Pyro\nDESCRIPTION: An example of incorrect usage of `pyro.plate` where the context is created and exited before any sample statements are executed, failing to mark the conditional independence properly.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# WARNING do not do this!\nmy_reified_list = list(pyro.plate(\"data_loop\", len(data)))\nfor i in my_reified_list:  \n    pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n```\n\n----------------------------------------\n\nTITLE: Implementing MLE Model and Guide in Pyro\nDESCRIPTION: Defines the model and guide for Maximum Likelihood Estimation (MLE) in Pyro, using a parameter instead of a latent variable for fairness.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mle_map.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model_mle(data):\n    f = pyro.param(\"latent_fairness\", torch.tensor(0.5), \n                   constraint=constraints.unit_interval)\n    with pyro.plate(\"data\", data.size(0)):\n        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n\ndef guide_mle(data):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining Semisupervised Pyro Model\nDESCRIPTION: Creates a semisupervised Pyro model that can handle both labeled and unlabeled data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef model(x, y=None):\n    with pyro.plate(\"N\", 2):\n        z = pyro.sample(\"z\", dist.Normal(0, 1))\n        y = pyro.sample(\"y\", dist.Normal(0, 1), obs=y)\n        pyro.sample(\"x\", dist.Normal(y + z, 1), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Mixture Model with Pyro Autoname\nDESCRIPTION: This snippet demonstrates how to use pyro.contrib.autoname to simplify the implementation of a mixture model. It shows the reduction in boilerplate code compared to a standard implementation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/autoname_examples.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# mixture.py\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib import autoname\n\n# A Pyro model with the usual boilerplate\ndef model(data):\n    p = pyro.sample(\"p\", dist.Beta(1, 1))\n    with pyro.plate(\"data\", len(data)):\n        z = pyro.sample(\"z\", dist.Bernoulli(p))\n        pyro.sample(\"x\", dist.Normal(2 * z - 1, 1), obs=data)\n\n# The same model, with a ``name`` block to reduce boilerplate\n@autoname.name_scope\ndef model(data):\n    p = pyro.sample(\"p\", dist.Beta(1, 1))\n    with autoname.plate(\"data\", len(data)):\n        z = pyro.sample(\"z\", dist.Bernoulli(p))\n        pyro.sample(\"x\", dist.Normal(2 * z - 1, 1), obs=data)\n\n# The same model, with random variable names inferred\n@autoname.name_scope\ndef model(data):\n    p = dist.Beta(1, 1)\n    with autoname.plate(data):\n        z = dist.Bernoulli(p)\n        dist.Normal(2 * z - 1, 1).obs(data)\n\n```\n\n----------------------------------------\n\nTITLE: Computing Prior Entropy for EIG Calculation in Python\nDESCRIPTION: This snippet calculates the prior entropy H(p(w)) which is a component of the Expected Information Gain (EIG) formula used for optimal experimental design.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprior_entropy = dist.Bernoulli(prior_w_prob).entropy()\n```\n\n----------------------------------------\n\nTITLE: Setting Up AutoGuide for Bayesian Regression in Pyro\nDESCRIPTION: This snippet demonstrates how to set up an AutoDiagonalNormal guide for the Bayesian regression model. The guide is used for approximate inference in the model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyro.infer.autoguide import AutoDiagonalNormal\n\nmodel = BayesianRegression(3, 1)\nguide = AutoDiagonalNormal(model)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating PyroModule Effects and Param Store Synchronization\nDESCRIPTION: Illustrates how PyroModule attributes synchronize with Pyro's param store and can be recorded in traces, unlike regular nn.Module attributes.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npyro.clear_param_store()\n\n# This is not traced:\nlinear = Linear(5, 2)\nwith poutine.trace() as tr:\n    linear(example_input)\nprint(type(linear).__name__)\nprint(list(tr.trace.nodes.keys()))\nprint(list(pyro.get_param_store().keys()))\n\n# Now this is traced:\nto_pyro_module_(linear)\nwith poutine.trace() as tr:\n    linear(example_input)\nprint(type(linear).__name__)\nprint(list(tr.trace.nodes.keys()))\nprint(list(pyro.get_param_store().keys()))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for Pyro Inference\nDESCRIPTION: Sets up the Python environment by importing necessary libraries for probabilistic programming with Pyro, including PyTorch, NumPy, and Matplotlib. Also sets a random seed for reproducibility.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nimport pyro\nimport pyro.infer\nimport pyro.optim\nimport pyro.distributions as dist\n\npyro.set_rng_seed(101)\n```\n\n----------------------------------------\n\nTITLE: Implementing Working Memory Model in Pyro\nDESCRIPTION: Defines a Bayesian model for working memory experiments using Pyro. The model takes sequence length as input and outputs whether a participant successfully remembered the sequence based on their memory capacity.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\n\n\nsensitivity = 1.0\nprior_mean = torch.tensor(7.0)\nprior_sd = torch.tensor(2.0)\n\n\ndef model(l):\n    # Dimension -1 of `l` represents the number of rounds\n    # Other dimensions are batch dimensions: we indicate this with a plate_stack\n    with pyro.plate_stack(\"plate\", l.shape[:-1]):\n        theta = pyro.sample(\"theta\", dist.Normal(prior_mean, prior_sd))\n        # Share theta across the number of rounds of the experiment\n        # This represents repeatedly testing the same participant\n        theta = theta.unsqueeze(-1)\n        # This define a *logistic regression* model for y\n        logit_p = sensitivity * (theta - l)\n        # The event shape represents responses from the same participant\n        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n        return y\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Predict Class for TorchScript\nDESCRIPTION: This code implements a custom Predict class that can be used with TorchScript. It uses Pyro's effect handling library to capture execution traces and replay them for prediction. This allows the model to be served without a Python runtime.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\nfrom pyro import poutine\nfrom pyro.poutine.util import prune_subsample_sites\nimport warnings\n\n\nclass Predict(torch.nn.Module):\n    def __init__(self, model, guide):\n        super().__init__()\n        self.model = model\n        self.guide = guide\n        \n    def forward(self, *args, **kwargs):\n        samples = {}\n        guide_trace = poutine.trace(self.guide).get_trace(*args, **kwargs)\n        model_trace = poutine.trace(poutine.replay(self.model, guide_trace)).get_trace(*args, **kwargs)\n        for site in prune_subsample_sites(model_trace).stochastic_nodes:\n            samples[site] = model_trace.nodes[site]['value']\n        return tuple(v for _, v in sorted(samples.items()))\n\npredict_fn = Predict(model, guide)\npredict_module = torch.jit.trace_module(predict_fn, {\"forward\": (x_data,)}, check_trace=False)\n```\n\n----------------------------------------\n\nTITLE: Defining a Stochastic Volatility Model\nDESCRIPTION: This snippet defines a more complex Bayesian model for stochastic volatility which incorporates time-dependent volatility, using a Stable distribution to capture dependencies in log returns over time. It samples from various distributions to characterize the model's underlying behavior.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    # Note we avoid plates because we'll later reparameterize along the time axis using\n    # DiscreteCosineReparam, breaking independence. This requires .unsqueeze()ing scalars.\n    h_0 = pyro.sample(\"h_0\", dist.Normal(0, 1)).unsqueeze(-1)\n    sigma = pyro.sample(\"sigma\", dist.LogNormal(0, 1)).unsqueeze(-1)\n    v = pyro.sample(\"v\", dist.Normal(0, 1).expand(data.shape).to_event(1))\n    log_h = pyro.deterministic(\"log_h\", h_0 + sigma * v.cumsum(dim=-1))\n    sqrt_h = log_h.mul(0.5).exp().clamp(min=1e-8, max=1e8)\n\n    # Observed log returns, assumed to be a Stable distribution scaled by sqrt(h).\n    r_loc = pyro.sample(\"r_loc\", dist.Normal(0, 1e-2)).unsqueeze(-1)\n    r_skew = pyro.sample(\"r_skew\", dist.Uniform(-1, 1)).unsqueeze(-1)\n    r_stability = pyro.sample(\"r_stability\", dist.Uniform(0, 2)).unsqueeze(-1)\n    pyro.sample(\"r\", dist.Stable(r_stability, r_skew, sqrt_h, r_loc * sqrt_h).to_event(1),\n                obs=data)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Results with Matplotlib\nDESCRIPTION: Creates a scatter plot of the original data points and overlays the model predictions with confidence intervals.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nplt.scatter(X_, y_, c='r')\nplt.ylabel('y')\nplt.xlabel('x')\nplt.plot(counts_df['feat'], counts_df['mean'])\nplt.fill_between(counts_df['feat'], counts_df['high'], counts_df['low'], alpha=0.5);\n```\n\n----------------------------------------\n\nTITLE: Visualizing Slope Distribution for GDP vs Terrain Ruggedness\nDESCRIPTION: This snippet creates a plot to visualize the distribution of the slope of log GDP given terrain ruggedness for African and non-African nations. It helps validate the hypothesis about the relationship between terrain ruggedness and GDP.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nweight = samples[\"linear.weight\"]\nweight = weight.reshape(weight.shape[0], 3)\ngamma_within_africa = weight[:, 1] + weight[:, 2]\ngamma_outside_africa = weight[:, 1]\nfig = plt.figure(figsize=(10, 6))\nsns.distplot(gamma_within_africa, kde_kws={\"label\": \"African nations\"},)\nsns.distplot(gamma_outside_africa, kde_kws={\"label\": \"Non-African nations\"})\nfig.suptitle(\"Density of Slope : log(GDP) vs. Terrain Ruggedness\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom L2 Regularizer in Pyro\nDESCRIPTION: Defines a custom L2 regularization function that can be added to the SVI loss to penalize large parameter values.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef my_custom_L2_regularizer(my_parameters):\n    reg_loss = 0.0\n    for param in my_parameters:\n        reg_loss = reg_loss + param.pow(2.0).sum()\n    return reg_loss  \n```\n\n----------------------------------------\n\nTITLE: Implementing Monte Carlo ELBO\nDESCRIPTION: Implementation of variational inference using Monte Carlo ELBO with existing effect handlers.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef monte_carlo_elbo(model, guide, batch, *args, **kwargs):\n    # assuming batch is a dictionary, we use poutine.condition to fix values of observed variables\n    conditioned_model = poutine.condition(model, data=batch)\n    \n    # we'll approximate the expectation in the ELBO with a single sample:\n    # first, we run the guide forward unmodified and record values and distributions\n    # at each sample site using poutine.trace\n    guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n    \n    # we use poutine.replay to set the values of latent variables in the model\n    # to the values sampled above by our guide, and use poutine.trace\n    # to record the distributions that appear at each sample site in in the model\n    model_trace = poutine.trace(\n        poutine.replay(conditioned_model, trace=guide_trace)\n    ).get_trace(*args, **kwargs)\n    \n    elbo = 0.\n    for name, node in model_trace.nodes.items():\n        if node[\"type\"] == \"sample\":\n            elbo = elbo + node[\"fn\"].log_prob(node[\"value\"]).sum()\n            if not node[\"is_observed\"]:\n                elbo = elbo - guide_trace.nodes[name][\"fn\"].log_prob(node[\"value\"]).sum()\n    return -elbo\n```\n\n----------------------------------------\n\nTITLE: Including Python Example File Reference\nDESCRIPTION: A file reference to the inclined_plane.py example that demonstrates importance sampling in Pyro. The example is accessed via a GitHub link and included using literalinclude directive.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/inclined_plane.rst#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. literalinclude:: ../../examples/inclined_plane.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Model Function in Pyro\nDESCRIPTION: Example showing how to define a basic model function in Pyro with a single sample statement for a latent variable z_1.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    pyro.sample(\"z_1\", ...)\n```\n\n----------------------------------------\n\nTITLE: Implementing an Intractable Scale Function in Pyro\nDESCRIPTION: Defines a stochastic function where the exact posterior becomes intractable due to a nonlinear transformation. This demonstrates a case where variational inference is needed instead of exact inference.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef intractable_scale(guess):\n    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n    return pyro.sample(\"measurement\", dist.Normal(some_nonlinear_function(weight), 0.75))\n```\n\n----------------------------------------\n\nTITLE: Initializing Gaussian Process Regression Model in Pyro\nDESCRIPTION: Sets up a Gaussian Process Regression model with a Matern52 kernel using initial evaluation points. This model serves as the surrogate for the objective function in the Bayesian optimization process.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# initialize the model with four input points: 0.0, 0.33, 0.66, 1.0\nX = torch.tensor([0.0, 0.33, 0.66, 1.0])\ny = f(X)\ngpmodel = gp.models.GPRegression(X, y, gp.kernels.Matern52(input_dim=1),\n                                 noise=torch.tensor(0.1), jitter=1.0e-4)\n```\n\n----------------------------------------\n\nTITLE: Adding Markov Property for Extended Enumeration\nDESCRIPTION: Shows how to modify the HMM loop to use pyro.markov for handling more than 25 variables.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: diff\nCODE:\n```\n- for t, y in enumerate(data):\n+ for t, y in pyro.markov(enumerate(data)):\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for BBBVI in Pyro\nDESCRIPTION: Imports necessary Python libraries for implementing Boosting Black Box Variational Inference, including PyTorch, Pyro inference tools, NumPy, and visualization libraries.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom collections import defaultdict\nfrom functools import partial\n\nimport numpy as np\nimport pyro\nimport pyro.distributions as dist\nimport scipy.stats\nimport torch\nimport torch.distributions.constraints as constraints\nfrom matplotlib import pyplot\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom pyro.poutine import block, replay, trace\n```\n\n----------------------------------------\n\nTITLE: Examining Discrete Latent States with infer_discrete in Pyro\nDESCRIPTION: Shows how to use the infer_discrete handler to access predicted values of discrete latent variables after enumeration. Demonstrates both sampling and MAP inference.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nserving_model = infer_discrete(model, first_available_dim=-1)\nx, y, z = serving_model()  # takes the same args as model(), here no args\nprint(f\"x = {x}\")\nprint(f\"y = {y}\")\nprint(f\"z = {z}\")\n```\n\n----------------------------------------\n\nTITLE: Constraining PyroModule Parameters with PyroParam\nDESCRIPTION: Demonstrates how to constrain PyroModule parameters by replacing nn.Parameter with PyroParam. This example ensures the bias parameter is positive by applying a constraint.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"params before:\", [name for name, _ in linear.named_parameters()])\n\nlinear.bias = PyroParam(torch.randn(2).exp(), constraint=constraints.positive)\nprint(\"params after:\", [name for name, _ in linear.named_parameters()])\nprint(\"bias:\", linear.bias)\n\nexample_input = torch.randn(100, 5)\nexample_output = linear(example_input)\nassert example_output.shape == (100, 2)\n```\n\n----------------------------------------\n\nTITLE: Creating a PyroModule by Subclassing\nDESCRIPTION: Demonstrates how to create a PyroModule by subclassing both the original module and PyroModule. This method allows for custom PyroModule creation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass PyroLinear(Linear, PyroModule):\n    pass\n\nlinear = PyroLinear(5, 2)\nassert isinstance(linear, nn.Module)\nassert isinstance(linear, Linear)\nassert isinstance(linear, PyroModule)\n\nexample_input = torch.randn(100, 5)\nexample_output = linear(example_input)\nassert example_output.shape == (100, 2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Beta VAE with Scaled KL Divergence in Pyro\nDESCRIPTION: Shows how to implement a Beta VAE in Pyro using poutine.scale to adjust the weight of the KL divergence term in the ELBO, allowing for control over the regularization strength.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model(data, beta=0.5):\n    z_loc, z_scale = ...\n    with pyro.poutine.scale(scale=beta)\n        z = pyro.sample(\"z\", dist.Normal(z_loc, z_scale))\n    pyro.sample(\"obs\", dist.Bernoulli(...), obs=data)\n   \ndef guide(data, beta=0.5):\n    with pyro.poutine.scale(scale=beta)\n        z_loc, z_scale = ...\n        z = pyro.sample(\"z\", dist.Normal(z_loc, z_scale))\n```\n\n----------------------------------------\n\nTITLE: Implementing Stick-Breaking Weight Function\nDESCRIPTION: Defines a function to generate mixture weights using the stick-breaking construction for Dirichlet process mixture models.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef mix_weights(beta):\n    beta1m_cumprod = (1 - beta).cumprod(-1)\n    return F.pad(beta, (0, 1), value=1) * F.pad(beta1m_cumprod, (1, 0), value=1)\n```\n\n----------------------------------------\n\nTITLE: Correct Indexing with Enumerated Variables in Pyro\nDESCRIPTION: Demonstrates the correct way to perform indexing with enumerated variables in Pyro models, using the Vindex helper for enumeration-compatible advanced indexing semantics.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\np = pyro.param(\"p\", torch.randn(5, 4, 3, 2).exp(),\n               constraint=constraints.simplex)\nx = pyro.sample(\"x\", dist.Categorical(torch.ones(4)))\ny = pyro.sample(\"y\", dist.Categorical(torch.ones(3)))\nwith pyro.plate(\"z_plate\", 5):\n    p_xy = p[..., x, y, :]  # Not compatible with enumeration!\n    z = pyro.sample(\"z\", dist.Categorical(p_xy)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Compatible with enumeration, but not recommended:\np_xy = p[torch.arange(5, device=p.device).reshape(5, 1),\n         x.unsqueeze(-1),\n         y.unsqueeze(-1),\n         torch.arange(2, device=p.device)]\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Gaussian Mixture Data in Python\nDESCRIPTION: Creates synthetic dataset by combining samples from 4 different 2D multivariate normal distributions. Visualizes the generated data using matplotlib.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.cat((MultivariateNormal(-8 * torch.ones(2), torch.eye(2)).sample([50]),\n                  MultivariateNormal(8 * torch.ones(2), torch.eye(2)).sample([50]),\n                  MultivariateNormal(torch.tensor([1.5, 2]), torch.eye(2)).sample([50]),\n                  MultivariateNormal(torch.tensor([-0.5, 1]), torch.eye(2)).sample([50])))\n\nplt.scatter(data[:, 0], data[:, 1])\nplt.title(\"Data Samples from Mixture of 4 Gaussians\")\nplt.show()\nN = data.shape[0]\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Environment for GPLVM in Python\nDESCRIPTION: This snippet imports necessary libraries for GPLVM implementation, including Pyro, PyTorch, and visualization tools. It also sets up the random seed and checks the Pyro version.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/gplvm.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom torch.nn import Parameter\n\nimport pyro\nimport pyro.contrib.gp as gp\nimport pyro.distributions as dist\nimport pyro.ops.stats as stats\n\nsmoke_test = ('CI' in os.environ)  # ignore; used to check code integrity in the Pyro repo\nassert pyro.__version__.startswith('1.9.1')\npyro.set_rng_seed(1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Guide\nDESCRIPTION: Creates a neural network-based guide that learns proposal distributions for the latent variables based on observations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/csis.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Guide(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.neural_net = nn.Sequential(\n            nn.Linear(2, 10),\n            nn.ReLU(),\n            nn.Linear(10, 20),\n            nn.ReLU(),\n            nn.Linear(20, 10),\n            nn.ReLU(),\n            nn.Linear(10, 5),\n            nn.ReLU(),\n            nn.Linear(5, 2))\n\n    def forward(self, prior_mean, observations={\"x1\": 0, \"x2\": 0}):\n        pyro.module(\"guide\", self)\n        x1 = observations[\"x1\"]\n        x2 = observations[\"x2\"]\n        v = torch.cat((x1.view(1, 1), x2.view(1, 1)), 1)\n        v = self.neural_net(v)\n        mean = v[0, 0]\n        std = v[0, 1].exp()\n        pyro.sample(\"z\", dist.Normal(mean, std))\n\nguide = Guide()\n```\n\n----------------------------------------\n\nTITLE: Implementing SIRD Model with Complex Compartment Flow\nDESCRIPTION: Implementation of a SIRD model demonstrating complex compartmental flows with branching paths, including methods for computing flows between compartments and state transitions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass SIRDModel(CompartmentalModel):\n    def __init__(self, population, data):\n        compartments = (\"S\", \"I\", \"D\")\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n        self.data = data\n\n    def compute_flows(self, prev, curr, t):\n        S2I = prev[\"S\"] - curr[\"S\"]  # S can only go in one direction.\n        I2D = curr[\"D\"] - prev[\"D\"]  # D can only have come from one direction.\n        # Now by conservation at I, change + inflows + outflows = 0,\n        # so we can solve for the single unknown I2R.\n        I2R = prev[\"I\"] - curr[\"I\"] + S2I - I2D\n        return {\n            \"S2I_{}\".format(t): S2I,\n            \"I2D_{}\".format(t): I2D,\n            \"I2R_{}\".format(t): I2R,\n        }\n    ...\n    def transition(self, params, state, t):\n        ...\n        # Sample flows between compartments.\n        S2I = pyro.sample(\"S2I_{}\".format(t), ...)\n        I2D = pyro.sample(\"I2D_{}\".format(t), ...)\n        I2R = pyro.sample(\"I2R_{}\".format(t), ...)\n\n        # Update compartments with flows.\n        state[\"S\"] = state[\"S\"] - S2I\n        state[\"I\"] = state[\"I\"] + S2I - I2D - I2R\n        state[\"D\"] = state[\"D\"] + I2D\n        ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Exact Inference via Sequential Enumeration in Pyro\nDESCRIPTION: Function that performs exact inference through enumeration over discrete variables in a model. Uses poutine.queue for breadth-first enumeration and returns an Empirical distribution of samples with corresponding weights.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef sequential_discrete_marginal(model, data, site_name=\"_RETURN\"):\n    \n    from six.moves import queue  # queue data structures\n    q = queue.Queue()  # Instantiate a first-in first-out queue\n    q.put(poutine.Trace())  # seed the queue with an empty trace\n    \n    # as before, we fix the values of observed random variables with poutine.condition\n    # assuming data is a dictionary whose keys are names of sample sites in model\n    conditioned_model = poutine.condition(model, data=data)\n    \n    # we wrap the conditioned model in a poutine.queue,\n    # which repeatedly pushes and pops partially completed executions from a Queue()\n    # to perform breadth-first enumeration over the set of values of all discrete sample sites in model\n    enum_model = poutine.queue(conditioned_model, queue=q)\n    \n    # actually perform the enumeration by repeatedly tracing enum_model\n    # and accumulate samples and trace log-probabilities for postprocessing\n    samples, log_weights = [], []\n    while not q.empty():\n        trace = poutine.trace(enum_model).get_trace()\n        samples.append(trace.nodes[site_name][\"value\"])\n        log_weights.append(trace.log_prob_sum())\n        \n    # we take the samples and log-joints and turn them into a histogram:\n    samples = torch.stack(samples, 0)\n    log_weights = torch.stack(log_weights, 0)\n    log_weights = log_weights - dist.util.logsumexp(log_weights, dim=0)\n    return dist.Empirical(samples, log_weights)\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Named Data Structures with Sphinx\nDESCRIPTION: This code snippet uses Sphinx automodule directives to generate documentation for the pyro.contrib.autoname.named module. It includes all members, undocumented members, and inheritance information, ordering members by source.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.autoname.rst#2025-04-16_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: pyro.contrib.autoname.named\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Combining pyro.to_funsor and pyro.to_data in Probabilistic Computations\nDESCRIPTION: This code snippet illustrates a typical usage pattern of pyro.to_funsor and pyro.to_data in probabilistic computations. It demonstrates creating Funsor terms, converting them to PyTorch tensors for distribution operations, and then converting the results back to Funsor terms.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    \n    probs = funsor.Tensor(torch.tensor([0.5, 0.4, 0.7]), OrderedDict(batch=funsor.Bint[3]))\n    print(type(probs), probs.inputs, probs.output)\n    \n    x = funsor.Tensor(torch.tensor([0., 1., 0., 1.]), OrderedDict(x=funsor.Bint[4]))\n    print(type(x), x.inputs, x.output)\n    \n    dx = dist.Bernoulli(pyro.to_data(probs))\n    print(type(dx), dx.shape())\n    \n    px = pyro.to_funsor(dx.log_prob(pyro.to_data(x)), output=funsor.Real)\n    print(type(px), px.inputs, px.output)\n```\n\n----------------------------------------\n\nTITLE: Initializing SVI with Basic ELBO in Pyro\nDESCRIPTION: Sets up the basic Stochastic Variational Inference (SVI) with Adam optimizer and Trace_ELBO loss for the semi-supervised VAE model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ss-vae.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\nfrom pyro.optim import Adam\n\n# setup the optimizer\nadam_params = {\"lr\": 0.0003}\noptimizer = Adam(adam_params)\n\n# setup the inference algorithm\nsvi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Registering Funsor Backend with PyroAPI\nDESCRIPTION: Registers the contrib.funsor backend with pyroapi and sets up the necessary imports for handlers, inference, and distributions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyro.contrib.funsor\nimport pyroapi\nfrom pyroapi import handlers, infer, ops, optim, pyro\nfrom pyroapi import distributions as dist\n\n# this is already done in pyro.contrib.funsor, but we repeat it here\npyroapi.register_backend(\"contrib.funsor\", dict(\n    distributions=\"pyro.distributions\",\n    handlers=\"pyro.contrib.funsor.handlers\",\n    infer=\"pyro.contrib.funsor.infer\",\n    ops=\"torch\",\n    optim=\"pyro.optim\",\n    pyro=\"pyro.contrib.funsor\",\n))\n```\n\n----------------------------------------\n\nTITLE: Using Predictive for Post-inference Analysis\nDESCRIPTION: This snippet demonstrates how to use the Predictive class to analyze the results of MCMC inference, including deterministic variables.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictive = Predictive(model, samples)(X_, None)\nfor k, v in predictive.items():\n    print(f\"{k}: {tuple(v.shape)}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Guide Step Function with RNN in Pyro\nDESCRIPTION: Defines a basic guide step function using an RNN to generate parameters for the variational distributions of z_pres, z_where, and z_what based on previous choices and input data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef guide_step_basic(t, data, prev):\n\n    # The RNN takes the images and choices from the previous step as input.\n    rnn_input = torch.cat((data, prev.z_where, prev.z_what, prev.z_pres), 1)\n    h, c = rnn(rnn_input, (prev.h, prev.c))\n\n    # Compute parameters for all choices made this step, by passing\n    # the RNN hidden state through another neural network.\n    z_pres_p, z_where_loc, z_where_scale, z_what_loc, z_what_scale = predict_basic(h)\n\n    z_pres = pyro.sample('z_pres_{}'.format(t),\n                         dist.Bernoulli(z_pres_p * prev.z_pres))\n\n    z_where = pyro.sample('z_where_{}'.format(t),\n                          dist.Normal(z_where_loc, z_where_scale))\n\n    z_what = pyro.sample('z_what_{}'.format(t),\n                         dist.Normal(z_what_loc, z_what_scale))\n\n    return # values for next step\n```\n\n----------------------------------------\n\nTITLE: Creating a Tensor Conversion Transform for CVAE MNIST Data\nDESCRIPTION: This transform converts the MNIST images and labels to PyTorch tensors for processing. It handles the conversion of both the digit images to float tensors and the digit labels to integer tensors.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass ToTensor:\n    def __call__(self, sample):\n        sample['original'] = functional.to_tensor(sample['original'])\n        sample['digit'] = torch.as_tensor(np.asarray(sample['digit']),\n                                          dtype=torch.int64)\n        return sample\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Standard Sampling vs Enumeration in Pyro\nDESCRIPTION: Defines simple model and guide functions to illustrate the difference between standard sampling and enumeration in Pyro. Uses Trace_ELBO for standard sampling and TraceEnum_ELBO for enumeration.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    z = pyro.sample(\"z\", dist.Categorical(torch.ones(5)))\n    print(f\"model z = {z}\")\n\ndef guide():\n    z = pyro.sample(\"z\", dist.Categorical(torch.ones(5)))\n    print(f\"guide z = {z}\")\n\nelbo = Trace_ELBO()\nelbo.loss(model, guide);\n\nelbo = TraceEnum_ELBO(max_plate_nesting=0)\nelbo.loss(model, config_enumerate(guide, \"parallel\"));\n\nelbo = TraceEnum_ELBO(max_plate_nesting=0)\nelbo.loss(model, config_enumerate(guide, \"sequential\"));\n```\n\n----------------------------------------\n\nTITLE: Implementing Encoder Network for Object Features in Pyro\nDESCRIPTION: Defines an encoder network that takes pixel intensities from an attention window and outputs parameters for the distribution over the latent code z_what using a two-layer neural network.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrnn = nn.LSTMCell(2554, 256)\n\n# Takes pixel intensities of the attention window to parameters (mean,\n# standard deviation) of the distribution over the latent code,\n# z_what.\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(400, 200)\n        self.l2 = nn.Linear(200, 100)\n\n    def forward(self, data):\n        h = relu(self.l1(data))\n        a = self.l2(h)\n        return a[:, 0:50], softplus(a[:, 50:])\n\nencode = Encoder()\n```\n\n----------------------------------------\n\nTITLE: Higher-Order Stochastic Functions\nDESCRIPTION: Demonstration of higher-order stochastic functions that can accept or return other stochastic functions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef normal_product(loc, scale):\n    z1 = pyro.sample(\"z1\", pyro.distributions.Normal(loc, scale))\n    z2 = pyro.sample(\"z2\", pyro.distributions.Normal(loc, scale))\n    y = z1 * z2\n    return y\n\ndef make_normal_normal():\n    mu_latent = pyro.sample(\"mu_latent\", pyro.distributions.Normal(0, 1))\n    fn = lambda scale: normal_product(mu_latent, scale)\n    return fn\n\nprint(make_normal_normal()(1.))\n```\n\n----------------------------------------\n\nTITLE: Creating Data Loaders for CVAE MNIST Experiment\nDESCRIPTION: This function sets up the dataset and data loaders for both training and validation sets. It applies the necessary transformations to prepare the MNIST data for CVAE training with the specified number of quadrants as input.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef get_data(num_quadrant_inputs, batch_size):\n    transforms = Compose([\n        ToTensor(),\n        MaskImages(num_quadrant_inputs=num_quadrant_inputs)\n    ])\n    datasets, dataloaders, dataset_sizes = {}, {}, {}\n    for mode in ['train', 'val']:\n        datasets[mode] = CVAEMNIST(\n            '../data',\n            download=True,\n            transform=transforms,\n            train=mode == 'train'\n        )\n        dataloaders[mode] = DataLoader(\n            datasets[mode],\n            batch_size=batch_size,\n            shuffle=mode == 'train',\n            num_workers=0\n        )\n        dataset_sizes[mode] = len(datasets[mode])\n\n    return datasets, dataloaders, dataset_sizes\n```\n\n----------------------------------------\n\nTITLE: Pyro Weather Model\nDESCRIPTION: Weather model rewritten using Pyro's primitives and named sampling statements.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef weather():\n    cloudy = pyro.sample('cloudy', pyro.distributions.Bernoulli(0.3))\n    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n    temp = pyro.sample('temp', pyro.distributions.Normal(mean_temp, scale_temp))\n    return cloudy, temp.item()\n\nfor _ in range(3):\n    print(weather())\n```\n\n----------------------------------------\n\nTITLE: Defining Probabilistic Model with Gaussian Prior\nDESCRIPTION: Implementation of a non-linear model with standard Gaussian prior over latent variables. Includes likelihood function and model definition using Pyro's primitives.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_flow_guide.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprior = pyro.distributions.Normal(torch.zeros(3), torch.ones(3)).to_event(1)\n\ndef likelihood(z: Tensor):\n    mu = z[..., :2]\n    rho = z[..., 2].tanh() * 0.99\n\n    cov = 1e-2 * torch.stack([\n        torch.ones_like(rho), rho,\n        rho, torch.ones_like(rho),\n    ], dim=-1).unflatten(-1, (2, 2))\n\n    return pyro.distributions.MultivariateNormal(mu, cov)\n\ndef model(x: Tensor):\n    with pyro.plate(\"data\", x.shape[1]):\n        z = pyro.sample(\"z\", prior)\n\n        with pyro.plate(\"obs\", 5):\n            pyro.sample(\"x\", likelihood(z), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for SVI with Normalizing Flows\nDESCRIPTION: Setup code importing necessary packages including Pyro, PyTorch, Zuko for flows, and visualization tools.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_flow_guide.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyro\nimport torch\nimport zuko  # pip install zuko\n\nfrom corner import corner, overplot_points  # pip install corner\nfrom pyro.contrib.zuko import ZukoToPyro\nfrom pyro.optim import ClippedAdam\nfrom pyro.infer import SVI, Trace_ELBO\nfrom torch import Tensor\n```\n\n----------------------------------------\n\nTITLE: Computing stochastic gradients for ELBO optimization in Python using Pyro and PyTorch\nDESCRIPTION: This snippet demonstrates how to use the implemented log_z function to compute stochastic gradients with respect to the ELBO. It initializes parameters, sets up an Adam optimizer, and performs gradient steps to optimize the ELBO.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith pyroapi.pyro_backend(\"contrib.funsor\"):\n    model(data, verbose=False)  # initialize parameters\n    params = [pyro.param(\"probs\").unconstrained(), pyro.param(\"locs_mean\").unconstrained()]\n\noptimizer = torch.optim.Adam(params, lr=0.1)\nfor step in range(5):\n    optimizer.zero_grad()\n    log_marginal = log_z(model, (data, False))\n    (-log_marginal).backward()\n    optimizer.step()\n    print(log_marginal)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Bayesian Optimization in Python\nDESCRIPTION: Sets up the required libraries for implementing Bayesian optimization including PyTorch, Pyro, and visualization tools. Also sets a random seed for reproducibility.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.autograd as autograd\nimport torch.optim as optim\nfrom torch.distributions import constraints, transform_to\n\nimport pyro\nimport pyro.contrib.gp as gp\n\nassert pyro.__version__.startswith('1.9.1')\npyro.set_rng_seed(1)\n```\n\n----------------------------------------\n\nTITLE: Estimate Lineage Advantage with Hierarchical Model in Python\nDESCRIPTION: Prints the estimated multiplicative advantage of the Delta lineage over the Alpha lineage from the hierarchical model using the Pyro guide's median values and exponential transformation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"Multiplicative advantage: {:.2f}\".format(\n      (guide.median()['rate_loc'][1] - guide.median()['rate_loc'][0]).exp()))\n```\n\n----------------------------------------\n\nTITLE: Automatic Subsampling with Vectorized plate in Pyro\nDESCRIPTION: Implementation of subsampling with vectorized `plate`, which returns indices of the subsampled data points that must be used to select the corresponding elements from the data tensor.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith pyro.plate('observe_data', size=10, subsample_size=5) as ind:\n    pyro.sample('obs', dist.Bernoulli(f), \n                obs=data.index_select(0, ind))\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data\nDESCRIPTION: Creates synthetic data with object states, positions and observations including noise and spurious detections.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tracking_1d.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef generate_data(args):\n    # Object model.\n    num_objects = int(round(args.expected_num_objects))  # Deterministic.\n    states = dist.Normal(0., 1.).sample((num_objects, 2))\n\n    # Detection model.\n    emitted = dist.Bernoulli(args.emission_prob).sample((args.num_frames, num_objects))\n    num_spurious = dist.Poisson(args.expected_num_spurious).sample((args.num_frames,))\n    max_num_detections = int((num_spurious + emitted.sum(-1)).max())\n    observations = torch.zeros(args.num_frames, max_num_detections, 1+1) # position+confidence\n    positions = get_dynamics(args.num_frames).mm(states.t())\n    noisy_positions = dist.Normal(positions, args.emission_noise_scale).sample()\n    for t in range(args.num_frames):\n        j = 0\n        for i, e in enumerate(emitted[t]):\n            if e:\n                observations[t, j, 0] = noisy_positions[t, i]\n                observations[t, j, 1] = 1\n                j += 1\n        n = int(num_spurious[t])\n        if n:\n            observations[t, j:j+n, 0] = dist.Normal(0., 1.).sample((n,))\n            observations[t, j:j+n, 1] = 1\n\n    return states, positions, observations\n```\n\n----------------------------------------\n\nTITLE: Tracing a Pyro Model for Shape Analysis\nDESCRIPTION: This code snippet demonstrates how to trace a Pyro model using `pyro.poutine.trace` and analyze the shapes of sample sites using `Trace.format_shapes()`. The trace allows examining the distribution shape, value shape, and log probability shape for each sample site within the model. `trace.compute_log_prob()` is called to compute the log probabilities and enable printing of `log_prob` shapes.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"trace = poutine.trace(model1).get_trace()\\ntrace.compute_log_prob()  # optional, but allows printing of log_prob shapes\\nprint(trace.format_shapes())\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Posterior Predictive Distribution\nDESCRIPTION: This snippet samples from the unconditioned model and plots the resulting posterior predictive distribution of log returns. It aims to visualize how well the fitted model captures the observed data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsamples = poutine.uncondition(model)().detach()\npyplot.figure(figsize=(9, 3))\npyplot.hist(samples.numpy(), bins=200)\npyplot.yscale(\"log\")\npyplot.xlabel(\"daily log returns\")\npyplot.ylabel(\"count\")\npyplot.title(\"Posterior predictive distribution\");\n```\n\n----------------------------------------\n\nTITLE: Initializing CSIS Instance\nDESCRIPTION: Sets up the CSIS object with model, guide, optimizer, and inference parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/csis.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\noptimiser = pyro.optim.Adam({'lr': 1e-3})\ncsis = pyro.infer.CSIS(model, guide, optimiser, num_inference_samples=50)\nprior_mean = torch.tensor(1.)\n```\n\n----------------------------------------\n\nTITLE: Defining Decoder Network in PyTorch\nDESCRIPTION: Defines a Decoder class in PyTorch that transforms a latent variable into an image space representation. It uses fully connected layers with non-linear activations. Outputs parameters for a Bernoulli distribution representing images.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass Decoder(nn.Module):\n    def __init__(self, z_dim, hidden_dim):\n        super().__init__()\n        # setup the two linear transformations used\n        self.fc1 = nn.Linear(z_dim, hidden_dim)\n        self.fc21 = nn.Linear(hidden_dim, 784)\n        # setup the non-linearities\n        self.softplus = nn.Softplus()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        # define the forward computation on the latent z\n        # first compute the hidden units\n        hidden = self.softplus(self.fc1(z))\n        # return the parameter for the output Bernoulli\n        # each is of size batch_size x 784\n        loc_img = self.sigmoid(self.fc21(hidden))\n        return loc_img\n```\n\n----------------------------------------\n\nTITLE: Implementing the Residual ELBO (RELBO) for BBBVI\nDESCRIPTION: Computes the Residual ELBO which extends standard ELBO with a cross-entropy term between the current approximation component and the mixture approximation. Uses Pyro's trace and replay functionality.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef relbo(model, guide, *args, **kwargs):\n    approximation = kwargs.pop('approximation')\n\n    # We first compute the elbo, but record a guide trace for use below.\n    traced_guide = trace(guide)\n    elbo = pyro.infer.Trace_ELBO(max_plate_nesting=1)\n    loss_fn = elbo.differentiable_loss(model, traced_guide, *args, **kwargs)\n\n    # We do not want to update parameters of previously fitted components\n    # and thus block all parameters in the approximation apart from z.\n    guide_trace = traced_guide.trace\n    replayed_approximation = trace(replay(block(approximation, expose=['z']), guide_trace))\n    approximation_trace = replayed_approximation.get_trace(*args, **kwargs)\n\n    relbo = -loss_fn - approximation_trace.log_prob_sum()\n    \n    # By convention, the negative (R)ELBO is returned.\n    return -relbo\n```\n\n----------------------------------------\n\nTITLE: Implementing MAP Estimation with Uniform Prior in Pyro\nDESCRIPTION: Demonstrates MAP estimation using a uniform prior, showing that it results in the same estimate as MLE when using a uniform prior.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mle_map.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef original_model_uniform_prior(data):\n    f = pyro.sample(\"latent_fairness\", dist.Uniform(low=0.0, high=1.0))\n    with pyro.plate(\"data\", data.size(0)):\n        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n\nautoguide_map_uniform_prior = pyro.infer.autoguide.AutoDelta(original_model_uniform_prior)\ntrain(original_model_uniform_prior, autoguide_map_uniform_prior)\n```\n\n----------------------------------------\n\nTITLE: Training GPLVM Model with Pyro\nDESCRIPTION: This code trains the GPLVM model using Pyro's gp.util.train function. It performs 4000 optimization steps and plots the loss curve.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/gplvm.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# note that training is expected to take a minute or so\nlosses = gp.util.train(gplvm, num_steps=4000)\n\n# let's plot the loss curve after 4000 steps of training\nplt.plot(losses)\nplt.show()\n\ngplvm.mode = \"guide\"\nX = gplvm.X  # draw a sample from the guide of the variable X\n```\n\n----------------------------------------\n\nTITLE: Visualizing Linear Regression Fit in Python\nDESCRIPTION: This code creates plots to visualize the linear regression fit for African and non-African nations, showing the relationship between terrain ruggedness and GDP.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfit = df.copy()\nfit[\"mean\"] = linear_reg_model(x_data).detach().cpu().numpy()\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\nafrican_nations = fit[fit[\"cont_africa\"] == 1]\nnon_african_nations = fit[fit[\"cont_africa\"] == 0]\nfig.suptitle(\"Regression Fit\", fontsize=16)\nax[0].plot(non_african_nations[\"rugged\"], non_african_nations[\"rgdppc_2000\"], \"o\")\nax[0].plot(non_african_nations[\"rugged\"], non_african_nations[\"mean\"], linewidth=2)\nax[0].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"Non African Nations\")\nax[1].plot(african_nations[\"rugged\"], african_nations[\"rgdppc_2000\"], \"o\")\nax[1].plot(african_nations[\"rugged\"], african_nations[\"mean\"], linewidth=2)\nax[1].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"African Nations\");\n```\n\n----------------------------------------\n\nTITLE: Visualizing Estimated Volatility in Pyro Model\nDESCRIPTION: Generates plots to visualize the estimated log volatility and log returns from the fitted model. It uses the Predictive class to generate samples and calculate statistics.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig, axes = pyplot.subplots(2, figsize=(9, 5), sharex=True)\npyplot.subplots_adjust(hspace=0)\naxes[1].plot(r, \"k\", lw=0.2)\naxes[1].set_ylabel(\"log returns\")\naxes[1].set_xlim(0, len(r))\n\n# We will pull out median log returns using the autoguide's .median() and poutines.\nnum_samples = 200\nwith torch.no_grad():\n    pred = Predictive(reparam_model, guide=guide, num_samples=num_samples, parallel=True)(r)\nlog_h = pred[\"log_h\"]\naxes[0].plot(log_h.median(0).values, lw=1)\naxes[0].fill_between(torch.arange(len(log_h[0])),\n                     log_h.kthvalue(int(num_samples * 0.1), dim=0).values,\n                     log_h.kthvalue(int(num_samples * 0.9), dim=0).values,\n                     color='red', alpha=0.5)\naxes[0].set_ylabel(\"log volatility\")\n\nstability = pred[\"r_stability\"].median(0).values.item()\naxes[0].set_title(\"Estimated index of stability = {:0.4g}\".format(stability))\naxes[1].set_xlabel(\"trading day\");\n```\n\n----------------------------------------\n\nTITLE: Visualizing Bayesian Optimization Progress\nDESCRIPTION: Function to create visualization plots showing the Gaussian Process regression model and acquisition function. Displays observed data points, predictive mean, uncertainty intervals, and the next candidate point.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef plot(gs, xmin, xlabel=None, with_title=True):\n    xlabel = \"xmin\" if xlabel is None else \"x{}\".format(xlabel)\n    Xnew = torch.linspace(-0.1, 1.1, 100)\n    ax1 = plt.subplot(gs[0])\n    ax1.plot(gpmodel.X.numpy(), gpmodel.y.numpy(), \"kx\")  # plot all observed data\n    with torch.no_grad():\n        loc, var = gpmodel(Xnew, full_cov=False, noiseless=False)\n        sd = var.sqrt()\n        ax1.plot(Xnew.numpy(), loc.numpy(), \"r\", lw=2)  # plot predictive mean\n        ax1.fill_between(Xnew.numpy(), loc.numpy() - 2*sd.numpy(), loc.numpy() + 2*sd.numpy(),\n                         color=\"C0\", alpha=0.3)  # plot uncertainty intervals\n    ax1.set_xlim(-0.1, 1.1)\n    ax1.set_title(\"Find {}\".format(xlabel))\n    if with_title:\n        ax1.set_ylabel(\"Gaussian Process Regression\")\n\n    ax2 = plt.subplot(gs[1])\n    with torch.no_grad():\n        # plot the acquisition function\n        ax2.plot(Xnew.numpy(), lower_confidence_bound(Xnew).numpy())\n        # plot the new candidate point\n        ax2.plot(xmin.numpy(), lower_confidence_bound(xmin).numpy(), \"^\", markersize=10,\n                 label=\"{} = {:.5f}\".format(xlabel, xmin.item()))  \n    ax2.set_xlim(-0.1, 1.1)\n    if with_title:\n        ax2.set_ylabel(\"Acquisition Function\")\n    ax2.legend(loc=1)\n```\n\n----------------------------------------\n\nTITLE: Executing a Single SVI Step\nDESCRIPTION: Shows how to perform a single gradient step in the SVI optimization process using a mini-batch of data. This is the core operation that would be repeated many times during training to optimize the model parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsvi.step(mini_batch, ...)\n```\n\n----------------------------------------\n\nTITLE: Loading and Batching MNIST Dataset in Python\nDESCRIPTION: This function sets up data loaders for the MNIST dataset using PyTorch. It transforms the images to tensors and normalizes pixel values. It also supports GPU acceleration with optional parameters. Returns loaders for training and testing data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# for loading and batching MNIST dataset\ndef setup_data_loaders(batch_size=128, use_cuda=False):\n    root = './data'\n    download = True\n    trans = transforms.ToTensor()\n    train_set = MNIST(root=root, train=True, transform=trans,\n                      download=download)\n    test_set = MNIST(root=root, train=False, transform=trans)\n\n    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n        batch_size=batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n        batch_size=batch_size, shuffle=False, **kwargs)\n    return train_loader, test_loader\n```\n\n----------------------------------------\n\nTITLE: Using pyro.markov for Efficient Variable Management\nDESCRIPTION: This code demonstrates the use of pyro.markov annotation to manage a large number of variables efficiently. It shows how pyro.markov helps in reusing dimensions and computing shape information with time complexity independent of the number of variables.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    for i in pyro.markov(range(10)):\n        x = pyro.to_data(funsor.Tensor(torch.tensor([0., 1.]), OrderedDict({\"x{}\".format(i): funsor.Bint[2]})))\n        print(\"Shape of x[{}]: \".format(str(i)), x.shape)\n```\n\n----------------------------------------\n\nTITLE: Marginal Distribution Helper Function\nDESCRIPTION: Creates a helper function that wraps inference by constructing distribution over execution traces using Search and creating marginal distribution on return values.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef Marginal(fn):\n    return memoize(lambda *args: HashingMarginal(Search(fn).run(*args)))\n```\n\n----------------------------------------\n\nTITLE: Implementing Variational Guide in Pyro\nDESCRIPTION: Defines the variational family for inference with parameterized distributions for beta, cluster parameters, and assignments.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef guide(data):\n    kappa = pyro.param('kappa', lambda: Uniform(0, 2).sample([T-1]), constraint=constraints.positive)\n    tau = pyro.param('tau', lambda: MultivariateNormal(torch.zeros(2), 3 * torch.eye(2)).sample([T]))\n    phi = pyro.param('phi', lambda: Dirichlet(1/T * torch.ones(T)).sample([N]), constraint=constraints.simplex)\n\n    with pyro.plate(\"beta_plate\", T-1):\n        q_beta = pyro.sample(\"beta\", Beta(torch.ones(T-1), kappa))\n\n    with pyro.plate(\"mu_plate\", T):\n        q_mu = pyro.sample(\"mu\", MultivariateNormal(tau, torch.eye(2)))\n\n    with pyro.plate(\"data\", N):\n        z = pyro.sample(\"z\", Categorical(phi))\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Helper Functions\nDESCRIPTION: Defines utility functions for loading and manipulating SARS-CoV-2 sequence data, including functions to get lineage IDs, location IDs, and aggregate counts across locations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_lineage_id(s):\n    \"\"\"Get lineage id from string name\"\"\"\n    return np.argmax(np.array([s]) == dataset['lineages'])\n\ndef get_location_id(s):\n    \"\"\"Get location id from string name\"\"\"\n    return np.argmax(np.array([s]) == dataset['locations'])\n\ndef get_aggregated_counts_from_locations(locations):\n    \"\"\"Get aggregated counts from a list of locations\"\"\"\n    return sum([dataset['counts'][:, get_location_id(loc)] for loc in locations])\n\nstart = dataset[\"start_date\"]\nstep = datetime.timedelta(days=dataset[\"time_step_days\"])\ndate_range = np.array([start + step * t for t in range(len(dataset[\"counts\"]))])\n```\n\n----------------------------------------\n\nTITLE: Examining Optimized Parameters in Pyro's Parameter Store\nDESCRIPTION: This snippet demonstrates how to retrieve and print the optimized parameter values from Pyro's parameter store after training the Bayesian regression model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nguide.requires_grad_(False)\n\nfor name, value in pyro.get_param_store().items():\n    print(name, pyro.param(name))\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Pyro Model\nDESCRIPTION: Creates a basic Pyro model with normal distributions for mean, standard deviation, and observations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    m = pyro.sample(\"m\", dist.Normal(0, 1))\n    sd = pyro.sample(\"sd\", dist.LogNormal(m, 1))\n    with pyro.plate(\"N\", len(data)):\n        pyro.sample(\"obs\", dist.Normal(m, sd), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Posterior Predictive Distribution\nDESCRIPTION: This code creates a plot to visualize the posterior predictive distribution with 90% confidence intervals for African and non-African nations. It shows the relationship between terrain ruggedness and GDP, including model uncertainty.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\nfig.suptitle(\"Posterior predictive distribution with 90% CI\", fontsize=16)\nax[0].plot(non_african_nations[\"rugged\"], \n           non_african_nations[\"y_mean\"])\nax[0].fill_between(non_african_nations[\"rugged\"], \n                   non_african_nations[\"y_perc_5\"],\n                   non_african_nations[\"y_perc_95\"],\n                   alpha=0.5)\nax[0].plot(non_african_nations[\"rugged\"], \n           non_african_nations[\"true_gdp\"],\n           \"o\")\nax[0].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"Non African Nations\")\nidx = np.argsort(african_nations[\"rugged\"])\n\nax[1].plot(african_nations[\"rugged\"], \n           african_nations[\"y_mean\"])\nax[1].fill_between(african_nations[\"rugged\"],\n                   african_nations[\"y_perc_5\"],\n                   african_nations[\"y_perc_95\"],\n                   alpha=0.5)\nax[1].plot(african_nations[\"rugged\"], \n           african_nations[\"true_gdp\"],\n           \"o\")\nax[1].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"African Nations\");\n```\n\n----------------------------------------\n\nTITLE: Defining and Converting Tensors with Different Dimension Types in Pyro\nDESCRIPTION: This code snippet demonstrates how to define tensors with local, global, and visible dimensions using Funsor and convert them to PyTorch tensors using `pyro.to_data`. It showcases the use of `DimType.LOCAL`, `DimType.GLOBAL`, and `DimType.VISIBLE` to specify the dimension type. It requires `pyroapi`, `funsor`, `torch`, and `pyro` libraries. The snippet also shows how to temporarily change and then reset the `first_available_dim` to manage dimension allocation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprev_first_available_dim = _DIM_STACK.set_first_available_dim(-2)\n\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    \n    funsor_local_ids = funsor.Tensor(torch.arange(9), OrderedDict(k=funsor.Bint[9]))\n    tensor_local_ids = pyro.to_data(funsor_local_ids, dim_type=DimType.LOCAL)\n    print(\"Tensor with new local dimension: \", funsor_local_ids.inputs, tensor_local_ids.shape)\n    \n    funsor_global_ids = funsor.Tensor(torch.arange(10), OrderedDict(n=funsor.Bint[10]))\n    tensor_global_ids = pyro.to_data(funsor_global_ids, dim_type=DimType.GLOBAL)\n    print(\"Tensor with new global dimension: \", funsor_global_ids.inputs, tensor_global_ids.shape)\n    \n    funsor_data_ids = funsor.Tensor(torch.arange(11), OrderedDict(m=funsor.Bint[11]))\n    tensor_data_ids = pyro.to_data(funsor_data_ids, dim_type=DimType.VISIBLE)\n    print(\"Tensor with new visible dimension: \", funsor_data_ids.inputs, tensor_data_ids.shape)\n    \n# we also need to reset the first_available_dim after we're done\n_DIM_STACK.set_first_available_dim(prev_first_available_dim)\n```\n\n----------------------------------------\n\nTITLE: Implementing SVI with PyTorch Lightning for Distributed Training in Python\nDESCRIPTION: This example demonstrates how to implement Stochastic Variational Inference (SVI) using PyTorch Lightning for distributed training in Pyro. It shows how to pass argparse arguments to PyTorch Lightning's Trainer for controlling training parameters like GPU acceleration, number of devices, epochs, and distributed data parallel strategy.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_lightning.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# This script passes argparse arguments to PyTorch Lightning ``Trainer`` automatically_, for example::\n#\n#     $ python examples/svi_lightning.py --accelerator gpu --devices 2 --max_epochs 100 --strategy ddp\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic SIR Model Data\nDESCRIPTION: Creates synthetic epidemiological data by instantiating the SimpleSIRModel with empty data and using the generate method. Includes logic to retry generation until desired infection count is achieved.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npopulation = 10000\nrecovery_time = 10.\nempty_data = [None] * 90\nmodel = SimpleSIRModel(population, recovery_time, empty_data)\n\n# We'll repeatedly generate data until a desired number of infections is found.\npyro.set_rng_seed(20200709)\nfor attempt in range(100):\n    synth_data = model.generate({\"R0\": 2.0})\n    total_infections = synth_data[\"S2I\"].sum().item()\n    if 4000 <= total_infections <= 6000:\n        break\nprint(\"Simulated {} infections after {} attempts\".format(total_infections, 1 + attempt))\n```\n\n----------------------------------------\n\nTITLE: Implementing Marginal Guide for Experimental Design\nDESCRIPTION: Defines a marginal guide for optimal experimental design that approximates the marginal distribution of observations given the experimental design.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef marginal_guide(design, observation_labels, target_labels):\n    # This shape allows us to learn a different parameter for each candidate design l\n    q_logit = pyro.param(\"q_logit\", torch.zeros(design.shape[-2:]))\n    pyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))\n```\n\n----------------------------------------\n\nTITLE: Creating a Mixture Approximation Function for BBBVI\nDESCRIPTION: Implements the mixture distribution approximation that samples from component guides according to their mixture weights. The function first samples a component index and then samples from that component.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef approximation(data, components, weights):\n    assignment = pyro.sample('assignment', dist.Categorical(weights))\n    result = components[assignment](data)\n    return result        \n```\n\n----------------------------------------\n\nTITLE: Implementing Lower Confidence Bound Acquisition Function\nDESCRIPTION: Creates the Lower Confidence Bound acquisition function, which balances exploitation (using the mean prediction) and exploration (using the uncertainty) with a kappa parameter that controls this trade-off.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef lower_confidence_bound(x, kappa=2):\n    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n    sigma = variance.sqrt()\n    return mu - kappa * sigma\n```\n\n----------------------------------------\n\nTITLE: Visualizing Regression Line with Confidence Intervals\nDESCRIPTION: This snippet creates a plot to visualize the regression line with 90% confidence intervals for African and non-African nations. It shows the relationship between terrain ruggedness and GDP.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\nafrican_nations = predictions[predictions[\"cont_africa\"] == 1]\nnon_african_nations = predictions[predictions[\"cont_africa\"] == 0]\nafrican_nations = african_nations.sort_values(by=[\"rugged\"])\nnon_african_nations = non_african_nations.sort_values(by=[\"rugged\"])\nfig.suptitle(\"Regression line 90% CI\", fontsize=16)\nax[0].plot(non_african_nations[\"rugged\"],\n           non_african_nations[\"mu_mean\"])\nax[0].fill_between(non_african_nations[\"rugged\"], \n                   non_african_nations[\"mu_perc_5\"],\n                   non_african_nations[\"mu_perc_95\"],\n                   alpha=0.5)\nax[0].plot(non_african_nations[\"rugged\"], \n           non_african_nations[\"true_gdp\"],\n           \"o\")\nax[0].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"Non African Nations\")\nidx = np.argsort(african_nations[\"rugged\"])\nax[1].plot(african_nations[\"rugged\"], \n           african_nations[\"mu_mean\"])\nax[1].fill_between(african_nations[\"rugged\"],\n                   african_nations[\"mu_perc_5\"],\n                   african_nations[\"mu_perc_95\"],\n                   alpha=0.5)\nax[1].plot(african_nations[\"rugged\"], \n           african_nations[\"true_gdp\"],\n           \"o\")\nax[1].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"African Nations\");\n```\n\n----------------------------------------\n\nTITLE: Using the Obs Parameter for Direct Conditioning in Sample Statements\nDESCRIPTION: Demonstrates an alternative approach to conditioning by using the obs parameter directly in a pyro.sample() statement. This example conditions on measurement=9.5 inside the model definition.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef scale_obs(guess):  # equivalent to conditioned_scale above\n    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.))\n    # here we condition on measurement == 9.5\n    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75), obs=torch.tensor(9.5))\n```\n\n----------------------------------------\n\nTITLE: Initializing SVI with TraceGraph_ELBO for Non-reparameterizable Variables in Pyro\nDESCRIPTION: This code snippet demonstrates how to initialize Stochastic Variational Inference (SVI) with TraceGraph_ELBO loss, which is designed to handle models with non-reparameterizable random variables by automatically tracking dependency structures to reduce variance.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsvi = SVI(model, guide, optimizer, TraceGraph_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Implementing Variational Guide for Working Memory Model\nDESCRIPTION: Defines a Normal guide for variational inference that approximates the posterior distribution of the working memory capacity parameter.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributions.constraints import positive\n\ndef guide(l):\n    # The guide is initialised at the prior\n    posterior_mean = pyro.param(\"posterior_mean\", prior_mean.clone())\n    posterior_sd = pyro.param(\"posterior_sd\", prior_sd.clone(), constraint=positive)\n    pyro.sample(\"theta\", dist.Normal(posterior_mean, posterior_sd))\n```\n\n----------------------------------------\n\nTITLE: Enumerating Variables with Vindex in Pyro\nDESCRIPTION: Shows how to use Vindex for tensor indexing in enumerated variables with plate context. Demonstrates parameter sampling and categorical distributions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@config_enumerate\ndef model():\n    p = pyro.param(\"p\", torch.randn(5, 4, 3, 2).exp(), constraint=constraints.simplex)\n    x = pyro.sample(\"x\", dist.Categorical(torch.ones(4)))\n    y = pyro.sample(\"y\", dist.Categorical(torch.ones(3)))\n    with pyro.plate(\"z_plate\", 5):\n        p_xy = Vindex(p)[..., x, y, :]\n        z = pyro.sample(\"z\", dist.Categorical(p_xy))\n    print(f\"     p.shape = {p.shape}\")\n    print(f\"     x.shape = {x.shape}\")\n    print(f\"     y.shape = {y.shape}\")\n    print(f\"  p_xy.shape = {p_xy.shape}\")\n    print(f\"     z.shape = {z.shape}\")\n    return x, y, z\n```\n\n----------------------------------------\n\nTITLE: Building Parameter Constraints into Module Constructor\nDESCRIPTION: Shows how to incorporate parameter constraints directly into a PyroModule constructor by using PyroParam with a positive constraint in the module definition.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n  class Linear(PyroModule):\n      def __init__(self, in_size, out_size):\n          super().__init__()\n          self.weight = ...\n-         self.bias = nn.Parameter(torch.randn(out_size))\n+         self.bias = PyroParam(torch.randn(out_size).exp(),\n+                               constraint=constraints.positive)\n      ...\n```\n\n----------------------------------------\n\nTITLE: Implementing a Mask Transform for CVAE MNIST Quadrant Prediction\nDESCRIPTION: This transform masks specific quadrants of MNIST images for the CVAE task. It prepares input-output pairs by masking different quadrants based on the specified number of quadrants to use as input, allowing for conditional generation of the missing parts.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass MaskImages:\n    \"\"\"This torchvision image transformation prepares the MNIST digits to be\n    used in the tutorial. Depending on the number of quadrants to be used as\n    inputs (1, 2, or 3), the transformation masks the remaining (3, 2, 1)\n    quadrant(s) setting their pixels with -1. Additionally, the transformation\n    adds the target output in the sample dict as the complementary of the input\n    \"\"\"\n    def __init__(self, num_quadrant_inputs, mask_with=-1):\n        if num_quadrant_inputs <= 0 or num_quadrant_inputs >= 4:\n            raise ValueError('Number of quadrants as inputs must be 1, 2 or 3')\n        self.num = num_quadrant_inputs\n        self.mask_with = mask_with\n\n    def __call__(self, sample):\n        tensor = sample['original'].squeeze()\n        out = tensor.detach().clone()\n        h, w = tensor.shape\n\n        # removes the bottom left quadrant from the target output\n        out[h // 2:, :w // 2] = self.mask_with\n        # if num of quadrants to be used as input is 2,\n        # also removes the top left quadrant from the target output\n        if self.num == 2:\n            out[:, :w // 2] = self.mask_with\n        # if num of quadrants to be used as input is 3,\n        # also removes the top right quadrant from the target output\n        if self.num == 3:\n            out[:h // 2, :] = self.mask_with\n\n        # now, sets the input as complementary\n        inp = tensor.clone()\n        inp[out != -1] = self.mask_with\n\n        sample['input'] = inp\n        sample['output'] = out\n        return sample\n```\n\n----------------------------------------\n\nTITLE: Preparing Prediction Data for Visualization\nDESCRIPTION: This code prepares the prediction data for visualization by organizing the samples into a pandas DataFrame. It includes mean predictions and confidence intervals for both African and non-African nations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmu = pred_summary[\"_RETURN\"]\ny = pred_summary[\"obs\"]\npredictions = pd.DataFrame({\n    \"cont_africa\": x_data[:, 0],\n    \"rugged\": x_data[:, 1],\n    \"mu_mean\": mu[\"mean\"],\n    \"mu_perc_5\": mu[\"5%\"],\n    \"mu_perc_95\": mu[\"95%\"],\n    \"y_mean\": y[\"mean\"],\n    \"y_perc_5\": y[\"5%\"],\n    \"y_perc_95\": y[\"95%\"],\n    \"true_gdp\": y_data,\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Decoder Network in PyTorch for CVAE\nDESCRIPTION: Defines the decoder network used as the generation network in the CVAE. Processes latent variables through linear layers to generate output predictions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass Decoder(nn.Module):\n    def __init__(self, z_dim, hidden_1, hidden_2):\n        super().__init__()\n        self.fc1 = nn.Linear(z_dim, hidden_1)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        self.fc3 = nn.Linear(hidden_2, 784)\n        self.relu = nn.ReLU()\n\n    def forward(self, z):\n        y = self.relu(self.fc1(z))\n        y = self.relu(self.fc2(y))\n        y = torch.sigmoid(self.fc3(y))\n        return y\n```\n\n----------------------------------------\n\nTITLE: Examining Model Trace for Automatic Naming\nDESCRIPTION: Demonstrates PyroModule's automatic naming system by tracing a model execution to see how nested attributes are properly named in the Pyro trace.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nwith poutine.trace() as tr:\n    model(x)\nfor site in tr.trace.nodes.values():\n    print(site[\"type\"], site[\"name\"], site[\"value\"].shape)\n```\n\n----------------------------------------\n\nTITLE: Using Direct Baseline Value Specification in Pyro\nDESCRIPTION: Shows how to directly specify baseline values in Pyro's sample statements, useful when implementing custom baseline computations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nb = # do baseline computation\nz = pyro.sample(\"z\", dist.Bernoulli(...), \n                infer=dict(baseline={'baseline_value': b}))\n```\n\n----------------------------------------\n\nTITLE: Recursive Geometric Distribution\nDESCRIPTION: Implementation of a geometric distribution using recursive stochastic functions with dynamic sample naming.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef geometric(p, t=None):\n    if t is None:\n        t = 0\n    x = pyro.sample(\"x_{}\".format(t), pyro.distributions.Bernoulli(p))\n    if x.item() == 1:\n        return 0\n    else:\n        return 1 + geometric(p, t + 1)\n    \nprint(geometric(0.5))\n```\n\n----------------------------------------\n\nTITLE: Declaring Dependencies with to_event() vs Independence with plate\nDESCRIPTION: Shows two approaches to working with multiple variables: either declaring them as dependent using to_event() or declaring them as independent using pyro.plate. Explains the safety of assuming dependence.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nx = pyro.sample(\"x\", Normal(0, 1).expand([10]).to_event(1))\nassert x.shape == (10,)\n```\n\nLANGUAGE: python\nCODE:\n```\nwith pyro.plate(\"x_plate\", 10):\n    x = pyro.sample(\"x\", Normal(0, 1))  # .expand([10]) is automatic\n    assert x.shape == (10,)\n```\n\n----------------------------------------\n\nTITLE: Training CSIS Model\nDESCRIPTION: Trains the guide network to perform inference by minimizing KL divergence between model and guide.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/csis.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor step in range(n_steps):\n    csis.step(prior_mean)\n```\n\n----------------------------------------\n\nTITLE: Synthetic Experiment Implementation\nDESCRIPTION: Implements a synthetic person function for testing the experimental design system.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef synthetic_person(l):\n    # The synthetic person can remember any sequence shorter than 6\n    # They cannot remember any sequence of length 6 or above\n    # (There is no randomness in their responses)\n    y = (l < 6.).float()\n    return y\n```\n\n----------------------------------------\n\nTITLE: Documenting EasyGuide Module in RST\nDESCRIPTION: ReStructuredText documentation showing the module structure and class documentation for Pyro's easyguide components.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.easyguide.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nEasy Custom Guides\n==================\n\n.. automodule:: pyro.contrib.easyguide\n\nEasyGuide\n---------\n.. autoclass:: pyro.contrib.easyguide.EasyGuide\n    :members:\n    :undoc-members:\n    :special-members: __call__\n    :show-inheritance:\n    :member-order: bysource\n\neasy_guide\n----------\n.. autofunction:: pyro.contrib.easyguide.easy_guide\n\nGroup\n-----\n.. autoclass:: pyro.contrib.easyguide.easyguide.Group\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Using MixedMultiOptimizer for Different Parameter Groups in Pyro\nDESCRIPTION: Shows how to use Pyro's MixedMultiOptimizer to apply different optimizers to different groups of parameters in a more integrated way with Pyro's optimization framework.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    pyro.param('a', ...)\n    pyro.param('b', ...)\n    ...\n  \nadam = pyro.optim.Adam({'lr': 0.1})\nsgd = pyro.optim.SGD({'lr': 0.01})\noptim = MixedMultiOptimizer([(['a'], adam), (['b'], sgd)])\nwith pyro.poutine.trace(param_only=True) as param_capture:\n    loss = elbo.differentiable_loss(model, guide)\nparams = {'a': pyro.param('a'), 'b': pyro.param('b')}\noptim.step(loss, params)\n```\n\n----------------------------------------\n\nTITLE: Complex Plate Structure with Dirichlet Distribution\nDESCRIPTION: Example showing multiple nested plates with Dirichlet distribution and nontrivial event dimensions. Uses Vindex with captured plate indices.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@config_enumerate\ndef model():\n    data_plate = pyro.plate(\"data_plate\", 6, dim=-1)\n    feature_plate = pyro.plate(\"feature_plate\", 5, dim=-2)\n    component_plate = pyro.plate(\"component_plate\", 4, dim=-1)\n    with feature_plate: \n        with component_plate:\n            p = pyro.sample(\"p\", dist.Dirichlet(torch.ones(3)))\n    with data_plate:\n        c = pyro.sample(\"c\", dist.Categorical(torch.ones(4)))\n        with feature_plate as vdx:\n            pc = Vindex(p)[vdx[..., None], c, :]\n            x = pyro.sample(\"x\", dist.Categorical(pc),\n                            obs=torch.zeros(5, 6, dtype=torch.long))\n```\n\n----------------------------------------\n\nTITLE: Creating Hierarchical Models with Dependent Priors\nDESCRIPTION: Shows how to create hierarchical models where PyroSample statements depend on other samples or parameters by using callable priors that reference self.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass Model(PyroModule):\n    def __init__(self):\n        super().__init__()\n        self.dof = PyroSample(dist.Gamma(3, 1))\n        self.loc = PyroSample(dist.Normal(0, 1))\n        self.scale = PyroSample(lambda self: dist.InverseGamma(self.dof, 1))\n        self.x = PyroSample(lambda self: dist.Normal(self.loc, self.scale))\n        \n    def forward(self):\n        return self.x\n    \nModel()()\n```\n\n----------------------------------------\n\nTITLE: Implementing Pixel Location Sampling with Different Broadcasting Approaches in Pyro\nDESCRIPTION: This code demonstrates three different approaches to sampling pixel locations using Pyro's plate context: manual broadcasting, full automatic broadcasting, and partial broadcasting. The code includes utility functions for testing each approach and shows how plate contexts handle tensor shapes and broadcasting automatically.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nnum_particles = 100  # Number of samples for the ELBO estimator\nwidth = 8\nheight = 10\nsparse_pixels = torch.LongTensor([[3, 2], [3, 5], [3, 9], [7, 1]])\n\ndef sample_pixel_locations_no_broadcasting(p_x, p_y, x_axis, y_axis):\n    with x_axis:\n        x_active = pyro.sample(\"x_active\", Bernoulli(p_x).expand([num_particles, width, 1]))\n    with y_axis:\n        y_active = pyro.sample(\"y_active\", Bernoulli(p_y).expand([num_particles, 1, height]))\n    return x_active, y_active\n\ndef sample_pixel_locations_full_broadcasting(p_x, p_y, x_axis, y_axis):\n    with x_axis:\n        x_active = pyro.sample(\"x_active\", Bernoulli(p_x))\n    with y_axis:\n        y_active = pyro.sample(\"y_active\", Bernoulli(p_y))\n    return x_active, y_active \n\ndef sample_pixel_locations_partial_broadcasting(p_x, p_y, x_axis, y_axis):\n    with x_axis:\n        x_active = pyro.sample(\"x_active\", Bernoulli(p_x).expand([width, 1]))\n    with y_axis:\n        y_active = pyro.sample(\"y_active\", Bernoulli(p_y).expand([height]))\n    return x_active, y_active \n\ndef fun(observe, sample_fn):\n    p_x = pyro.param(\"p_x\", torch.tensor(0.1), constraint=constraints.unit_interval)\n    p_y = pyro.param(\"p_y\", torch.tensor(0.1), constraint=constraints.unit_interval)\n    x_axis = pyro.plate('x_axis', width, dim=-2)\n    y_axis = pyro.plate('y_axis', height, dim=-1)\n\n    with pyro.plate(\"num_particles\", 100, dim=-3):\n        x_active, y_active = sample_fn(p_x, p_y, x_axis, y_axis)\n        # Indices corresponding to \"parallel\" enumeration are appended \n        # to the left of the \"num_particles\" plate dim.\n        assert x_active.shape  == (2, 1, 1, 1)\n        assert y_active.shape  == (2, 1, 1, 1, 1)\n        p = 0.1 + 0.5 * x_active * y_active\n        assert p.shape == (2, 2, 1, 1, 1)\n\n        dense_pixels = p.new_zeros(broadcast_shape(p.shape, (width, height)))\n        for x, y in sparse_pixels:\n            dense_pixels[..., x, y] = 1\n        assert dense_pixels.shape == (2, 2, 1, width, height)\n\n        with x_axis, y_axis:    \n            if observe:\n                pyro.sample(\"pixels\", Bernoulli(p), obs=dense_pixels)\n\ndef test_model_with_sample_fn(sample_fn):\n    def model():\n        fun(observe=True, sample_fn=sample_fn)\n\n    @config_enumerate\n    def guide():\n        fun(observe=False, sample_fn=sample_fn)\n\n    test_model(model, guide, TraceEnum_ELBO(max_plate_nesting=3))\n\ntest_model_with_sample_fn(sample_pixel_locations_no_broadcasting)\ntest_model_with_sample_fn(sample_pixel_locations_full_broadcasting)\ntest_model_with_sample_fn(sample_pixel_locations_partial_broadcasting)\n```\n\n----------------------------------------\n\nTITLE: Implementing Prior Function for Multi-Object Generation in Pyro\nDESCRIPTION: Defines a prior function that generates complete images with multiple objects by iterating the prior_step function up to three times, supporting the multi-mnist dataset which contains 0-2 digits.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef prior(n):\n    x = torch.zeros(n, 50, 50)\n    z_pres = torch.ones(n, 1)\n    for t in range(3):\n        x, z_pres = prior_step(n, t, x, z_pres)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Implementing LogJointMessenger with Post-Processing\nDESCRIPTION: Extended implementation of LogJointMessenger showing both processing and post-processing of sample sites.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass LogJointMessenger2(poutine.messenger.Messenger):\n    \n    def __init__(self, cond_data):\n        self.data = cond_data\n    \n    def __call__(self, fn):\n        def _fn(*args, **kwargs):\n            with self:\n                fn(*args, **kwargs)\n                return self.logp.clone()\n        return _fn\n    \n    def __enter__(self):\n        self.logp = torch.tensor(0.)\n        return super().__enter__()\n    \n    def __exit__(self, exc_type, exc_value, traceback):\n        self.logp = torch.tensor(0.)\n        return super().__exit__(exc_type, exc_value, traceback)\n\n    def _pyro_sample(self, msg):\n        if msg[\"name\"] in self.data:\n            msg[\"value\"] = self.data[msg[\"name\"]]\n            msg[\"done\"] = True\n            \n    def _pyro_post_sample(self, msg):\n        assert msg[\"done\"]  # the \"done\" flag asserts that no more modifications to value and fn will be performed.\n        self.logp = self.logp + (msg[\"scale\"] * msg[\"fn\"].log_prob(msg[\"value\"])).sum()\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading TorchScript Module\nDESCRIPTION: This snippet demonstrates how to save the TorchScript module and load it back. This allows the model to be used in production environments without requiring the full Pyro framework.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.save(predict_module, '/tmp/reg_predict.pt')\npred_loaded = torch.jit.load('/tmp/reg_predict.pt')\npred_loaded(x_data)\n```\n\n----------------------------------------\n\nTITLE: Defining a Bayesian Neural Network Module from Scratch\nDESCRIPTION: Creates a BayesianLinear PyroModule from scratch with PyroSample attributes for bias and weight, demonstrating how to define proper priors with event dimensions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass BayesianLinear(PyroModule):\n    def __init__(self, in_size, out_size):\n       super().__init__()\n       self.bias = PyroSample(\n           prior=dist.LogNormal(0, 1).expand([out_size]).to_event(1))\n       self.weight = PyroSample(\n           prior=dist.Normal(0, 1).expand([in_size, out_size]).to_event(2))\n\n    def forward(self, input):\n        return self.bias + input @ self.weight  # this line samples bias and weight\n```\n\n----------------------------------------\n\nTITLE: Implementing Linear Regression Training Loop in PyTorch\nDESCRIPTION: This code sets up the dataset, defines the linear regression model, loss function, and optimizer, and implements a training loop using PyTorch.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Dataset: Add a feature to capture the interaction between \"cont_africa\" and \"rugged\"\ndf[\"cont_africa_x_rugged\"] = df[\"cont_africa\"] * df[\"rugged\"]\ndata = torch.tensor(df[[\"cont_africa\", \"rugged\", \"cont_africa_x_rugged\", \"rgdppc_2000\"]].values,\n                        dtype=torch.float)\nx_data, y_data = data[:, :-1], data[:, -1]\n\n# Regression model\nlinear_reg_model = PyroModule[nn.Linear](3, 1)\n\n# Define loss and optimize\nloss_fn = torch.nn.MSELoss(reduction='sum')\noptim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.05)\nnum_iterations = 1500 if not smoke_test else 2\n\ndef train():\n    # run the model forward on the data\n    y_pred = linear_reg_model(x_data).squeeze(-1)\n    # calculate the mse loss\n    loss = loss_fn(y_pred, y_data)\n    # initialize gradients to zero\n    optim.zero_grad()\n    # backpropagate\n    loss.backward()\n    # take a gradient step\n    optim.step()\n    return loss\n\nfor j in range(num_iterations):\n    loss = train()\n    if (j + 1) % 50 == 0:\n        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n\n            \n# Inspect learned parameters\nprint(\"Learned parameters:\")\nfor name, param in linear_reg_model.named_parameters():\n    print(name, param.data.numpy())\n```\n\n----------------------------------------\n\nTITLE: Creating a Log Joint Probability Function\nDESCRIPTION: Demonstrates how to use Poutine's condition and trace handlers to create a function that computes the log joint probability of a model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef make_log_joint(model):\n    def _log_joint(cond_data, *args, **kwargs):\n        conditioned_model = poutine.condition(model, data=cond_data)\n        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n        return trace.log_prob_sum()\n    return _log_joint\n\nscale_log_joint = make_log_joint(scale)\nprint(scale_log_joint({\"measurement\": torch.tensor(9.5), \"weight\": torch.tensor(8.23)}, torch.tensor(8.5)))\n```\n\n----------------------------------------\n\nTITLE: Implementing Multivariate Forecasting with BART in Python using Pyro\nDESCRIPTION: This script demonstrates multivariate forecasting using Bayesian Additive Regression Trees (BART) in Pyro. It includes data loading, model definition, and inference setup for forecasting BART ridership.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/forecast_simple.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport os\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.forecast import ForecastingModel, Forecaster\nfrom pyro.contrib.forecast.util import MarkDuration\nfrom pyro.infer.reparam import StudentTQuantileReparam\nfrom pyro.ops.tensor_utils import periodic_cumsum, periodic_repeat\nfrom pyro.ops.stats import quantile\n\nclass BARTModel(ForecastingModel):\n    def __init__(self, duration, num_series):\n        self.duration = duration\n        self.num_series = num_series\n\n    def model(self, zero_data, covariates):\n        assert zero_data.shape == (self.duration, self.num_series)\n        assert covariates.shape[0] == self.duration\n        num_features = covariates.shape[-1]\n\n        # Global hyperparameters.\n        df = pyro.sample(\"df\", dist.Exponential(1.0))\n\n        # Global tree parameters.\n        feature_axis = pyro.sample(\"feature_axis\",\n                                   dist.Dirichlet(torch.ones(num_features)))\n        depth = pyro.sample(\"depth\", dist.Poisson(3.0))\n        split = pyro.sample(\"split\", dist.Uniform(0, 1).expand([depth]).to_event(1))\n\n        # Series-level parameters.\n        with pyro.plate(\"series\", self.num_series):\n            loc = pyro.sample(\"loc\", dist.Normal(0, 1))\n            scale = pyro.sample(\"scale\", dist.LogNormal(0, 1))\n\n        # Per-timestep noise.\n        with pyro.plate(\"time\", self.duration):\n            with pyro.plate(\"series\", self.num_series):\n                with pyro.poutine.reparam(config={\"noise\": StudentTQuantileReparam()}):\n                    noise = pyro.sample(\"noise\",\n                                        dist.StudentT(df, 0, 1).mask(False))\n\n        # Compute latent function.\n        feature = torch.einsum(\"tf,f->t\", covariates, feature_axis)\n        feature = feature.unsqueeze(-1).expand(-1, self.num_series)\n        value = loc\n        for t in range(depth):\n            left = feature <= split[t]\n            value = torch.where(left, value, value + 2 ** -t)\n\n        value = value * scale + noise\n        return zero_data + value  # Convert from residual to prediction.\n\n    def guide(self, zero_data, covariates):\n        pyro.sample(\"df\", dist.Delta(1.0))\n\n\ndef load_bart_ridership():\n    import pandas as pd\n    url = \"https://raw.githubusercontent.com/pyro-ppl/pyro/dev/examples/contrib/forecast/data/bart_ridership.csv\"\n    df = pd.read_csv(url, parse_dates=[\"date\"])\n    df[\"t\"] = df[\"date\"].dt.dayofweek\n    return (\n        torch.tensor(df[\"rides\"].values, dtype=torch.float),\n        torch.tensor(df[[\"t\"]].values, dtype=torch.float),\n    )\n\ndef main(args):\n    data, covariates = load_bart_ridership()\n    data = data.reshape(-1, 1)  # independently forecast each series\n    covariates = MarkDuration(24)(covariates)\n    duration, num_series = data.shape\n\n    # Split training data into train and forecast.\n    forecast = 30\n    assert args.forecaster_num_steps <= forecast\n    train_data = data[:-forecast]\n\n    # Create forecaster.\n    model = BARTModel(duration, num_series)\n    forecaster = Forecaster(model,\n                            data=train_data,\n                            covariates=covariates,\n                            num_steps=args.forecaster_num_steps)\n\n    # Train.\n    pyro.set_rng_seed(args.seed)\n    pyro.clear_param_store()\n    train_kwargs = {\n        \"num_epochs\": args.num_epochs,\n        \"learning_rate\": args.learning_rate,\n        \"clip_norm\": args.clip_norm,\n        \"num_particles\": args.num_particles,\n        \"num_steps\": args.num_steps,\n        \"log_every\": args.log_every,\n        \"progress_bar\": args.progress_bar,\n    }\n    for key in list(train_kwargs):\n        if train_kwargs[key] is None:\n            del train_kwargs[key]\n    forecaster.fit(**train_kwargs)\n\n    # Forecast.\n    samples = forecaster(covariates[..., -forecast:])\n    print(samples.shape)  # 1000 x 30 x 1\n\n    # Evaluation.\n    truth = data[-forecast:]\n    pred = samples.median(0)\n    mae = abs(truth - pred).mean().item()\n    ci80 = quantile(samples, probs=torch.tensor([0.1, 0.9]))\n    coverage = ((ci80[0] < truth) & (truth < ci80[1])).float().mean().item()\n    print(\"MAE = {:0.3g}\".format(mae))\n    print(\"80% CI coverage = {:0.3f}\".format(coverage))\n\n\nif __name__ == \"__main__\":\n    assert pyro.__version__.startswith('1.8.4')\n    parser = argparse.ArgumentParser(description=\"Causal Effect Variational Autoencoder\")\n    parser.add_argument(\"-n\", \"--num-epochs\", default=50, type=int)\n    parser.add_argument(\"-lr\", \"--learning-rate\", default=0.1, type=float)\n    parser.add_argument(\"-n-steps\", \"--num-steps\", default=10, type=int)\n    parser.add_argument(\"-n-particle\", \"--num-particles\", default=50, type=int)\n    parser.add_argument(\"-clip\", \"--clip-norm\", default=5, type=float)\n    parser.add_argument(\"-f\", \"--forecaster-num-steps\", default=30, type=int)\n    parser.add_argument(\"--seed\", default=1234567890, type=int)\n    parser.add_argument(\"--log-every\", default=5, type=int)\n    parser.add_argument(\"--progress-bar\", default=True, type=bool)\n    args = parser.parse_args()\n    main(args)\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation Function\nDESCRIPTION: Implements evaluation of the model on validation and test sets. Handles RNN mode switching and loss normalization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef do_evaluation():\n    # put the RNN into evaluation mode (i.e. turn off drop-out if applicable)\n    dmm.rnn.eval()\n\n    # compute the validation and test loss\n    val_nll = svi.evaluate_loss(val_batch, val_batch_reversed, val_batch_mask,\n                                 val_seq_lengths) / np.sum(val_seq_lengths)\n    test_nll = svi.evaluate_loss(test_batch, test_batch_reversed, test_batch_mask,\n                                  test_seq_lengths) / np.sum(test_seq_lengths)\n\n    # put the RNN back into training mode (i.e. turn on drop-out if applicable)\n    dmm.rnn.train()\n    return val_nll, test_nll\n```\n\n----------------------------------------\n\nTITLE: Converting an Existing Module to PyroModule In-Place\nDESCRIPTION: Demonstrates how to convert an existing nn.Module instance to a PyroModule in-place using the to_pyro_module_() function. This is useful for modifying third-party modules.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinear = Linear(5, 2)\nassert isinstance(linear, nn.Module)\nassert not isinstance(linear, PyroModule)\n\nto_pyro_module_(linear)  # this operates in-place\nassert isinstance(linear, nn.Module)\nassert isinstance(linear, Linear)\nassert isinstance(linear, PyroModule)\n\nexample_input = torch.randn(100, 5)\nexample_output = linear(example_input)\nassert example_output.shape == (100, 2)\n```\n\n----------------------------------------\n\nTITLE: Applying Constraints to Parameters in Pyro\nDESCRIPTION: Shows how to apply constraints to parameters using PyTorch's constraints module. This example ensures the standard deviation parameter remains positive without using torch.abs.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributions import constraints\n\ndef scale_parametrized_guide_constrained(guess):\n    a = pyro.param(\"a\", torch.tensor(guess))\n    b = pyro.param(\"b\", torch.tensor(1.), constraint=constraints.positive)\n    return pyro.sample(\"weight\", dist.Normal(a, b))  # no more torch.abs\n```\n\n----------------------------------------\n\nTITLE: Implementing a Gaussian Mixture Model without Conditional Independence in Pyro\nDESCRIPTION: This code snippet shows a basic implementation of a Gaussian mixture model in Pyro without explicitly marking conditional independence, which leads to higher variance in gradient estimation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nks = pyro.sample(\"k\", dist.Categorical(probs)\n                          .to_event(1))\npyro.sample(\"obs\", dist.Normal(locs[ks], scale)\n                       .to_event(1),\n            obs=data)\n```\n\n----------------------------------------\n\nTITLE: Simulating Poll Data and Analyzing Results in Python\nDESCRIPTION: This code simulates poll data based on the 2016 election results, then uses the trained neural network to estimate the posterior probability of a Democrat win. It demonstrates the full process of experimental design, data collection, and analysis.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntest_data = pd.read_pickle(urlopen(BASE_URL + \"us_presidential_election_data_test.pickle?raw=true\"))\nresults_2016 = torch.tensor(test_data.values, dtype=torch.float)\ntrue_alpha = torch.log(results_2016[..., 0] / results_2016[..., 1])\n\nconditioned_model = pyro.condition(model, data={\"alpha\": true_alpha})\ny, _, _ = conditioned_model(best_allocation)\n\nq_w = torch.sigmoid(guide.compute_dem_probability(y).squeeze().detach())\n\nprint(\"Prior probability of Democrat win\", prior_w_prob.item())\nprint(\"Posterior probability of Democrat win\", q_w.item())\n```\n\n----------------------------------------\n\nTITLE: Visualizing Parameter Convergence in Pyro SVI\nDESCRIPTION: Creates a visualization of how the guide parameters converge during SVI optimization. The plot shows the parameters approaching the true values (indicated by dotted lines) over training steps.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplt.subplot(1,2,1)\nplt.plot([0,num_steps],[9.14,9.14], 'k:')\nplt.plot(a)\nplt.ylabel('a')\n\nplt.subplot(1,2,2)\nplt.ylabel('b')\nplt.plot([0,num_steps],[0.6,0.6], 'k:')\nplt.plot(b)\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Using PyTorch Normal Distribution\nDESCRIPTION: Demonstrates how to create and use a normal distribution object in PyTorch to draw samples and compute log probabilities.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nloc = 0.   # mean zero\nscale = 1. # unit variance\nnormal = torch.distributions.Normal(loc, scale) # create a normal distribution object\nx = normal.rsample() # draw a sample from N(0,1)\nprint(\"sample\", x)\nprint(\"log prob\", normal.log_prob(x)) # score the sample from N(0,1)\n```\n\n----------------------------------------\n\nTITLE: Plotting Resampled Trajectories\nDESCRIPTION: Defines a function to plot resampled trajectories using the Resampler, allowing for interactive adjustment of prior parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef plot_resampled(df0, df1, p0, p1, m0, m1):\n    partial_model = make_partial_model(df0, df1, p0, p1, m0, m1)\n    samples = resampler.sample(partial_model, num_samples=20)\n    trajectories = samples[\"obs\"]\n    plt.figure(figsize=(8, 5)).patch.set_color(\"white\")\n    plt.plot(trajectories.T)\n    plt.xlabel(\"time\")\n    plt.ylabel(\"obs\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Baseline Step Function for Variance Reduction in AIR\nDESCRIPTION: A helper function that computes baseline values for variance reduction in the AIR model's gradient estimation. It uses an RNN to process the input images and previously sampled values to predict baseline values.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nbl_rnn = nn.LSTMCell(2554, 256)\nbl_predict = nn.Linear(256, 1)\n\n# Use an RNN to compute the baseline value. This network takes the\n# input images and the values samples so far as input.\ndef baseline_step(x, prev):\n    rnn_input = torch.cat((x,\n                           prev.z_where.detach(),\n                           prev.z_what.detach(),\n                           prev.z_pres.detach()), 1)\n    bl_h, bl_c = bl_rnn(rnn_input, (prev.bl_h, prev.bl_c))\n    bl_value = bl_predict(bl_h) * prev.z_pres\n    return bl_value, bl_h, bl_c\n```\n\n----------------------------------------\n\nTITLE: Implementing Hidden Markov Model\nDESCRIPTION: Defines a discrete-state continuous-observation hidden Markov model with learnable transition and emission distributions dependent on a global random variable.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = [torch.tensor(1.)] * 10\n\ndef model(data, verbose):\n\n    p = pyro.param(\"probs\", lambda: torch.rand((3, 3)), constraint=constraints.simplex)\n    locs_mean = pyro.param(\"locs_mean\", lambda: torch.ones((3,)))\n    locs = pyro.sample(\"locs\", dist.Normal(locs_mean, 1.).to_event(1))\n    if verbose:\n        print(\"locs.shape = {}\".format(locs.shape))\n\n    x = 0\n    for i in pyro.markov(range(len(data))):\n        x = pyro.sample(\"x{}\".format(i), dist.Categorical(p[x]), infer={\"enumerate\": \"parallel\"})\n        if verbose:\n            print(\"x{}.shape = \".format(i), x.shape)\n        pyro.sample(\"y{}\".format(i), dist.Normal(Vindex(locs)[..., x], 1.), obs=data[i])\n```\n\n----------------------------------------\n\nTITLE: Creating Partial Model for Resampling\nDESCRIPTION: Defines a function to create partial models with given priors, used for resampling in expensive simulations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef make_partial_model(df0, df1, p0, p1, m0, m1):\n    def partial_model():\n        # Sample parameters from the prior.\n        pyro.sample(\"df\", dist.LogNormal(df0, df1))\n        pyro.sample(\"p_scale\", dist.LogNormal(p0, p1))  # process noise\n        pyro.sample(\"m_scale\", dist.LogNormal(m0, m1))  # measurement noise\n    return partial_model\n```\n\n----------------------------------------\n\nTITLE: Calculating Multiplicative Advantage using NumPy in Python\nDESCRIPTION: This code calculates and prints the multiplicative advantage of the Delta lineage over the Alpha lineage using the posterior median of the growth rate from the Pyro guide. It uses the exponential function from NumPy to compute the difference in growth rates.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"Multiplicative advantage: {:.2f}\".format(\n      np.exp(guide.median()['rate'][1] - guide.median()['rate'][0])))\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Transformation Functions for AIR\nDESCRIPTION: Functions that handle spatial transformations for the AIR model, including z_where_inv which computes inverse transformation parameters and image_to_object which crops a region from a larger image using a spatial transformer network.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef z_where_inv(z_where):\n    # Take a batch of z_where vectors, and compute their \"inverse\".\n    # That is, for each row compute:\n    # [s,x,y] -> [1/s,-x/s,-y/s]\n    # These are the parameters required to perform the inverse of the\n    # spatial transform performed in the generative model.\n    n = z_where.size(0)\n    out = torch.cat((torch.ones([1, 1]).type_as(z_where).expand(n, 1), -z_where[:, 1:]), 1)\n    out = out / z_where[:, 0:1]\n    return out\n\ndef image_to_object(z_where, image):\n    n = image.size(0)\n    theta_inv = expand_z_where(z_where_inv(z_where))\n    grid = affine_grid(theta_inv, torch.Size((n, 1, 20, 20)))\n    out = grid_sample(image.view(n, 1, 50, 50), grid)\n    return out.view(n, -1)\n```\n\n----------------------------------------\n\nTITLE: Running MCMC with Standard NUTS\nDESCRIPTION: Demonstrates using the No-U-Turn Sampler (NUTS) for Markov Chain Monte Carlo (MCMC) inference without JIT compilation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%time\nnuts_kernel = NUTS(model)\npyro.set_rng_seed(1)\nmcmc_run = MCMC(nuts_kernel, num_samples=100).run(data)\n```\n\n----------------------------------------\n\nTITLE: Implementing Greedy Algorithm for Boosting BBVI in Python\nDESCRIPTION: This snippet implements the greedy algorithm for boosting black box Variational Inference. It iteratively finds components of the approximation by maximizing the RELBO at each step, using Pyro's SVI for optimization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# clear the param store in case we're in a REPL\npyro.clear_param_store()\n\n# Sample observations from a Normal distribution with loc 4 and scale 0.1\nn = torch.distributions.Normal(torch.tensor([4.0]), torch.tensor([0.1]))\ndata = n.sample((100,))\n\n#T=2\nsmoke_test = ('CI' in os.environ)\nn_steps = 2 if smoke_test else 12000\npyro.set_rng_seed(2)\nn_iterations = 2\nlocs = [0]\nscales = [0]\nfor t in range(1, n_iterations + 1):\n\n    # Create guide that only takes data as argument\n    wrapped_guide = partial(guide, index=t)\n    losses = []\n\n    adam_params = {\"lr\": 0.01, \"betas\": (0.90, 0.999)}\n    optimizer = Adam(adam_params)\n\n    # Pass our custom RELBO to SVI as the loss function.\n    svi = SVI(model, wrapped_guide, optimizer, loss=relbo)\n    for step in range(n_steps):\n        # Pass the existing approximation to SVI.\n        loss = svi.step(data, approximation=wrapped_approximation)\n        losses.append(loss)\n\n        if step % 100 == 0:\n            print('.', end=' ')\n\n    # Update the list of approximation components.\n    components.append(wrapped_guide)\n\n    # Set new mixture weight.\n    new_weight = 2 / (t + 1)\n\n    # In this specific case, we set the mixture weight of the second component to 0.5.\n    if t == 2:\n        new_weight = 0.5\n    weights = weights * (1-new_weight)\n    weights = torch.cat((weights, torch.tensor([new_weight])))\n\n    # Update the approximation\n    wrapped_approximation = partial(approximation, components=components, weights=weights)\n\n    print('Parameters of component {}:'.format(t))\n    scale = pyro.param(\"scale_{}\".format(t)).item()\n    scales.append(scale)\n    loc = pyro.param(\"loc_{}\".format(t)).item()\n    locs.append(loc)\n    print('loc = {}'.format(loc))\n    print('scale = {}'.format(scale))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Posterior Predictive Samples\nDESCRIPTION: Code for generating and visualizing posterior predictive samples using corner plots.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_flow_guide.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nz = flow(x_star[:, 0].flatten()).sample((4096,))\nx = likelihood(z).sample()\n\nfig = corner(x.numpy())\n\noverplot_points(fig, x_star[:, 0].numpy())\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Pyro Tutorial\nDESCRIPTION: Sets up the necessary imports for working with Pyro, PyTorch, and various distribution classes. Also includes a helper function to test models and verifies the Pyro version.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport pyro\nfrom torch.distributions import constraints\nfrom pyro.distributions import Bernoulli, Categorical, MultivariateNormal, Normal\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.infer import Trace_ELBO, TraceEnum_ELBO, config_enumerate\nimport pyro.poutine as poutine\nfrom pyro.optim import Adam\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\n\n# We'll ue this helper to check our models are correct.\ndef test_model(model, guide, loss):\n    pyro.clear_param_store()\n    loss.loss(model, guide)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Model with Explicit Loop in Pyro\nDESCRIPTION: A basic implementation of a model that samples a latent fairness parameter from a Beta distribution and then samples observations from a Bernoulli distribution conditioned on that parameter.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    # sample f from the beta prior\n    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n    # loop over the observed data using pyro.sample with the obs keyword argument\n    for i in range(len(data)):\n        # observe datapoint i using the bernoulli likelihood\n        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n```\n\n----------------------------------------\n\nTITLE: Automatic Subsampling with Sequential plate in Pyro\nDESCRIPTION: Using `plate` with the `subsample_size` parameter to perform automatic subsampling, where only a random subset of data points are processed in each iteration while maintaining the correct scaling for the log likelihood.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor i in pyro.plate(\"data_loop\", len(data), subsample_size=5):\n    pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n```\n\n----------------------------------------\n\nTITLE: Plotting Lineage Proportion Over Time with Matplotlib in Python\nDESCRIPTION: This code uses Matplotlib to plot the proportion of Delta lineage over combined Alpha and Delta counts over time. It filters data starting at a defined time point, sets plot limits and formats the x-axis with dates, providing a visual representation of lineage proportions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nstart_time = 13\ntotal_counts = (alpha_counts + delta_counts)[start_time:]\ndates = date_range[start_time:]\nplt.figure(figsize=(7, 4))\nplt.plot(dates, delta_counts[start_time:] / total_counts, color=\"C1\", lw=1, alpha=0.5)\nplt.xlim(min(dates), max(dates))\nplt.ylabel(\"Proportion\", fontsize=18)\nplt.xticks(rotation=90)\nplt.gca().xaxis.set_major_locator(mpl.dates.MonthLocator())\nplt.gca().xaxis.set_major_formatter(mpl.dates.DateFormatter(\"%b %Y\"))\nplt.title(f\"Delta/(Alpha+Delta) in {counts.size(1)} regions\", fontsize=18)\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Configuring Adam Optimizer in Pyro\nDESCRIPTION: Demonstrates how to initialize the Adam optimizer with fixed learning parameters for all model parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.optim import Adam\n\nadam_params = {\"lr\": 0.005, \"betas\": (0.95, 0.999)}\noptimizer = Adam(adam_params)\n```\n\n----------------------------------------\n\nTITLE: Running MCMC with JIT-compiled NUTS\nDESCRIPTION: Shows how to use JIT compilation with the NUTS sampler in MCMC, which is expected to increase sampling throughput.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%time\nnuts_kernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True)\npyro.set_rng_seed(1)\nmcmc_run = MCMC(nuts_kernel, num_samples=100).run(data)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Parameter Store Items\nDESCRIPTION: Retrieves and displays the parameters learned by the model from Pyro's parameter store.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlist(pyro.get_param_store().items())\n```\n\n----------------------------------------\n\nTITLE: Ensuring Unique Names with PyroModule\nDESCRIPTION: This code snippet demonstrates how to avoid name conflicts by using a PyroModule as the main model class. This ensures that all attributes within the model have unique names, preventing potential errors during model execution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_20\n\nLANGUAGE: diff\nCODE:\n```\n- class Model(nn.Module):     # Could lead to name conflict.\n+ class Model(PyroModule):    # Ensures names are unique.\n      def __init__(self):\n          self.x = PyroModule()\n          self.y = PyroModule()\n```\n\n----------------------------------------\n\nTITLE: Generating Truth Trajectory for Kalman Filter\nDESCRIPTION: Creates a ground truth trajectory for a particle moving with near-constant velocity in 2D space. Implements the process noise and state transitions over multiple time frames.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ekf.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndt = 1e-2\nnum_frames = 10\ndim = 4\n\n# Continuous model\nncv = NcvContinuous(dim, 2.0)\n\n# Truth trajectory\nxs_truth = torch.zeros(num_frames, dim)\n# initial direction\ntheta0_truth = 0.0\n# initial state\nwith torch.no_grad():\n    xs_truth[0, :] = torch.tensor([0.0, 0.0,  math.cos(theta0_truth), math.sin(theta0_truth)])\n    for frame_num in range(1, num_frames):\n        # sample independent process noise\n        dx = pyro.sample('process_noise_{}'.format(frame_num), ncv.process_noise_dist(dt))\n        xs_truth[frame_num, :] = ncv(xs_truth[frame_num-1, :], dt=dt) + dx\n```\n\n----------------------------------------\n\nTITLE: Hidden Markov Model Example Implementation\nDESCRIPTION: This file contains a complete implementation of Hidden Markov Models using Pyro's probabilistic programming framework. It demonstrates how to define and work with HMMs in Pyro, including state transitions and observations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/hmm.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nExample: Hidden Markov Models\n=============================\n\n`View hmm.py on github`__\n\n.. _github: https://github.com/pyro-ppl/pyro/blob/dev/examples/hmm.py\n\n__ github_\n\n.. literalinclude:: ../../examples/hmm.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Visualizing Regression Data with Seaborn in Python\nDESCRIPTION: This code creates scatter plots to visualize the relationship between terrain ruggedness and GDP for African and non-African nations using Seaborn.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\nafrican_nations = df[df[\"cont_africa\"] == 1]\nnon_african_nations = df[df[\"cont_africa\"] == 0]\nsns.scatterplot(x=non_african_nations[\"rugged\"],\n                y=non_african_nations[\"rgdppc_2000\"],\n                ax=ax[0])\nax[0].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"Non African Nations\")\nsns.scatterplot(x=african_nations[\"rugged\"],\n                y=african_nations[\"rgdppc_2000\"],\n                ax=ax[1])\nax[1].set(xlabel=\"Terrain Ruggedness Index\",\n          ylabel=\"log GDP (2000)\",\n          title=\"African Nations\");\n```\n\n----------------------------------------\n\nTITLE: Retrieving Final States for Visualization\nDESCRIPTION: Extracts the final states from the trained model for visualization purposes. Retrieves the learned covariance matrices and filter states.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ekf.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# retrieve states for visualization\nR = guide()['pv_cov'] * torch.eye(4)\nQ = guide()['measurement_cov'] * torch.eye(2)\nekf_dist = EKFDistribution(xs_truth[0], R, ncv, Q, time_steps=num_frames)\nstates= ekf_dist.filter_states(zs)\n```\n\n----------------------------------------\n\nTITLE: Defining a Time Series Model with Varying Structure\nDESCRIPTION: Creates a Gaussian Hidden Markov Model (HMM) that can handle sequences of different lengths, demonstrating how to structure inputs for JIT compilation with varying model structure.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef model(sequence, num_sequences, length, state_dim=16):\n    # This is a Gaussian HMM model.\n    with pyro.plate(\"states\", state_dim):\n        trans = pyro.sample(\"trans\", dist.Dirichlet(0.5 * torch.ones(state_dim)))\n        emit_loc = pyro.sample(\"emit_loc\", dist.Normal(0., 10.))\n    emit_scale = pyro.sample(\"emit_scale\", dist.LogNormal(0., 3.))\n\n    # We're doing manual data subsampling, so we need to scale to actual data size.\n    with poutine.scale(scale=num_sequences):\n        # We'll use enumeration inference over the hidden x.\n        x = 0\n        for t in pyro.markov(range(length)):\n            x = pyro.sample(\"x_{}\".format(t), dist.Categorical(trans[x]),\n                            infer={\"enumerate\": \"parallel\"})\n            pyro.sample(\"y_{}\".format(t), dist.Normal(emit_loc[x], emit_scale),\n                        obs=sequence[t])\n\nguide = AutoDiagonalNormal(poutine.block(model, expose=[\"trans\", \"emit_scale\", \"emit_loc\"]))\n\n# This is fake data of different lengths.\nlengths = [24] * 50 + [48] * 20 + [72] * 5\nsequences = [torch.randn(length) for length in lengths]\n```\n\n----------------------------------------\n\nTITLE: LazyMessenger Implementation for Pyro Effect Handlers\nDESCRIPTION: A Messenger class that implements lazy evaluation compatible with other Pyro effect handlers. Wraps values in LazyValue objects for delayed computation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass LazyMessenger(pyro.poutine.messenger.Messenger):\n    def _process_message(self, msg):\n        if msg[\"type\"] in (\"apply\", \"sample\") and not msg[\"done\"]:\n            msg[\"done\"] = True\n            msg[\"value\"] = LazyValue(msg[\"fn\"], *msg[\"args\"], **msg[\"kwargs\"])\n```\n\n----------------------------------------\n\nTITLE: Comparing Parameter Access Inside vs Outside Plates\nDESCRIPTION: Demonstrates the importance of where PyroSample attributes are accessed relative to plates, showing how plate contexts affect the shape of samples.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass NormalModel(PyroModule):\n    def __init__(self):\n        super().__init__()\n        self.loc = PyroSample(dist.Normal(0, 1))\n\nclass GlobalModel(NormalModel):\n    def forward(self, data):\n        # If .loc is accessed (for the first time) outside the plate,\n        # then it will have empty shape ().\n        loc = self.loc\n        assert loc.shape == ()\n        with pyro.plate(\"data\", len(data)):\n            pyro.sample(\"obs\", dist.Normal(loc, 1), obs=data)\n        \nclass LocalModel(NormalModel):\n    def forward(self, data):\n        with pyro.plate(\"data\", len(data)):\n            # If .loc is accessed (for the first time) inside the plate,\n            # then it will be expanded by the plate to shape (plate.size,).\n            loc = self.loc\n            assert loc.shape == (len(data),)\n            pyro.sample(\"obs\", dist.Normal(loc, 1), obs=data)\n\ndata = torch.randn(10)\nLocalModel()(data)\nGlobalModel()(data)\n```\n\n----------------------------------------\n\nTITLE: Defining Extended Pyro Model with Prior Parameters\nDESCRIPTION: Creates an extended version of the Pyro model that allows inputting prior parameters for interactive tuning.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef model2(T: int = 1000, data=None, df0=0, df1=1, p0=0, p1=1, m0=0, m1=1):\n    # Sample parameters from the prior.\n    df = pyro.sample(\"df\", dist.LogNormal(df0, df1))\n    p_scale = pyro.sample(\"p_scale\", dist.LogNormal(p0, p1))  # process noise\n    m_scale = pyro.sample(\"m_scale\", dist.LogNormal(m0, m1))  # measurement noise\n    \n    # Simulate a time series.\n    with pyro.plate(\"dt\", T):\n        process_noise = pyro.sample(\"process_noise\", dist.StudentT(df, 0, p_scale))\n    trend = pyro.deterministic(\"trend\", process_noise.cumsum(-1))\n    with pyro.plate(\"t\", T):\n        return pyro.sample(\"obs\", dist.Normal(trend, m_scale), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Implementing Geometric Distribution\nDESCRIPTION: Implements a recursive geometric distribution for determining the number of objects in the scene.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef geom(num_trials=0):\n    p = torch.tensor([0.5])\n    x = pyro.sample('x{}'.format(num_trials), dist.Bernoulli(p))\n    if x[0] == 1:\n        return num_trials\n    else:\n        return geom(num_trials + 1)\n\ndef geom_prior(x, step=0):\n    p = torch.tensor([0.5])\n    i = pyro.sample('i{}'.format(step), dist.Bernoulli(p))\n    if i[0] == 1:\n        return x\n    else: \n        x = x + prior_step_sketch(step)\n        return geom_prior(x, step + 1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Decaying Average Baseline in Pyro\nDESCRIPTION: Shows how to implement a simple decaying average baseline for variance reduction in stochastic variational inference.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nz = pyro.sample(\"z\", dist.Bernoulli(...), \n                infer=dict(baseline={'use_decaying_avg_baseline': True,\n                                     'baseline_beta': 0.95}))\n```\n\n----------------------------------------\n\nTITLE: Extracting and Stacking Lineage Counts with Torch in Python\nDESCRIPTION: This snippet extracts Alpha and Delta lineage counts from a dataset and sums them across axes using PyTorch. It prints the shape of the resultant stacked counts tensor and the number of regions represented, crucial for later stages of modeling.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nalpha_counts = dataset['counts'][:, :, alpha_ids].sum(-1)\ndelta_counts = dataset['counts'][:, :, delta_ids].sum(-1)\ncounts = torch.stack([alpha_counts, delta_counts], dim=-1)\nprint(\"counts.shape: \", counts.shape)\nprint(f\"number of regions: {counts.size(1)}\")\n```\n\n----------------------------------------\n\nTITLE: Probabilistic Model Definition\nDESCRIPTION: Defines a probabilistic model using Pyro for the working memory experiment.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef make_model(mean, sd):\n    def model(l):\n        with pyro.plate_stack(\"plate\", l.shape[:-1]):\n            theta = pyro.sample(\"theta\", dist.Normal(mean, sd))\n            theta = theta.unsqueeze(-1)\n            logit_p = sensitivity * (theta - l)\n            y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n            return y\n    return model\n```\n\n----------------------------------------\n\nTITLE: Setting up SVI Training with AutoNormal Guide\nDESCRIPTION: Configures SVI training using AutoNormal guide and TraceEnum_ELBO loss for learning HMM parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nhmm_guide = AutoNormal(poutine.block(hmm_model, expose=[\"transition\", \"emission\"]))\n\npyro.clear_param_store()\nelbo = TraceEnum_ELBO(max_plate_nesting=1)\nelbo.loss(hmm_model, hmm_guide, data, data_dim=data_dim);\n```\n\n----------------------------------------\n\nTITLE: Running SIR Model with HMC in Command Line\nDESCRIPTION: Command line execution example showing how to run the SIR (Susceptible-Infected-Recovered) model with HMC. The script generates outbreak data for 10000 people over 60 days and forecasts 30 days ahead.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/sir_hmc.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/sir_hmc.py -p 10000 -d 60 -f 30 --plot\n```\n\n----------------------------------------\n\nTITLE: Creating Predictive Distribution\nDESCRIPTION: Generates predictive distribution using the trained model and guide with 500 samples, printing the shape of each parameter.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npredictive_svi = Predictive(model, guide=auto_guide, num_samples=500)(X_, None)\nfor k, v in predictive_svi.items():\n    print(f\"{k}: {tuple(v.shape)}\")\n```\n\n----------------------------------------\n\nTITLE: Declaring independent dimension with pyro.plate and size argument\nDESCRIPTION: This snippet declares an independent dimension using `pyro.plate` with an optional size argument.  The size argument aids in debugging shapes, providing information about the expected length of the data. Within the context, batch dimension -1 is considered independent.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"with pyro.plate(\\\"my_plate\\\", len(my_data)):\\n    # within this context, batch dimension -1 is independent\"\n```\n\n----------------------------------------\n\nTITLE: Nesting plates for per-pixel independence\nDESCRIPTION: This snippet showcases how to nest `pyro.plate` context managers to declare multiple independent dimensions, such as in the case of per-pixel independence in an image.  Each nested `plate` declares a new independent batch dimension. Within the innermost context, batch dimensions -2 and -1 are considered independent.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"with pyro.plate(\\\"x_axis\\\", 320):\\n    # within this context, batch dimension -1 is independent\\n    with pyro.plate(\\\"y_axis\\\", 200):\\n        # within this context, batch dimensions -2 and -1 are independent\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Resampler with Diffuse Guide\nDESCRIPTION: Initializes a Pyro Resampler with a diffuse guide covering a wide parameter space. This is used for efficient resampling in expensive simulations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n%%time\npartial_guide = make_partial_model(0, 10, 0, 10, 0, 10)\nresampler = Resampler(partial_guide, model, num_guide_samples=10000)\n```\n\n----------------------------------------\n\nTITLE: Training SVI Model\nDESCRIPTION: Setup and execution of SVI training loop using ClippedAdam optimizer and Trace_ELBO loss with multiple particles for variance reduction.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_flow_guide.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npyro.clear_param_store()\n\nsvi = SVI(model, guide, optim=ClippedAdam({\"lr\": 1e-3, \"clip_norm\": 10.0}), loss=Trace_ELBO(num_particles=16, vectorize_particles=True))\n\nfor step in range(4096 + 1):\n    elbo = svi.step(x_star)\n\n    if step % 256 == 0:\n        print(f'({step})', elbo)\n```\n\n----------------------------------------\n\nTITLE: Using pyro.to_funsor with Custom Dimension Mapping\nDESCRIPTION: This snippet shows how to use pyro.to_funsor with custom dimension-to-name mappings. It demonstrates converting PyTorch tensors to Funsor terms while specifying the naming of dimensions, which is useful for complex computations involving both PyTorch and Funsor operations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    \n    x = pyro.to_funsor(torch.tensor([0., 1.]), funsor.Real, dim_to_name={-1: \"x\"})\n    print(\"x: \", type(x), x.inputs, x.output)\n    \n    px = pyro.to_funsor(torch.ones(2, 3), funsor.Real, dim_to_name={-2: \"x\", -1: \"y\"})\n    print(\"px: \", type(px), px.inputs, px.output)\n```\n\n----------------------------------------\n\nTITLE: Rendering Pyro Model with Distributions and Constraints\nDESCRIPTION: Shows how to render a Pyro model with distribution and constraint annotations using render_distributions=True.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.ones(10)\npyro.render_model(model, model_args=(data,), render_params=True ,render_distributions=True)\n```\n\n----------------------------------------\n\nTITLE: Pragmatic Listener Implementation\nDESCRIPTION: Implements the pragmatic listener that infers likely states given chosen utterances using speaker model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@Marginal\ndef pragmatic_listener(utterance):\n    state = state_prior()\n    pyro.sample(\"speaker\", speaker(state), obs=utterance)\n    return state\n```\n\n----------------------------------------\n\nTITLE: Extracting Stability and Skew Parameters from Pyro Model Results\nDESCRIPTION: Extracts the stability and skew parameters from the fitting results of both models (with and without reparameterization) for comparison.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nstability_with_log_prob = []\nskew_with_log_prob = []\nfor stat in stats_with_log_prob:\n    stat = dict(stat)\n    stability_with_log_prob.append(stat['r_stability'].mean().item())\n    skew_with_log_prob.append(stat['r_skew'].mean().item())\n\nstability = []\nskew = []\nfor stat in stats:\n    stat = dict(stat)\n    stability.append(stat['r_stability'].mean().item())\n    skew.append(stat['r_skew'].mean().item())\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing JSB Dataset in Python\nDESCRIPTION: Loads musical sequence data from a pickle file and prepares training, validation and test datasets. Handles sequence lengths and mini-batch calculations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\njsb_file_loc = \"./data/jsb_processed.pkl\"\ndata = pickle.load(open(jsb_file_loc, \"rb\"))\ntraining_seq_lengths = data['train']['sequence_lengths']\ntraining_data_sequences = data['train']['sequences']\ntest_seq_lengths = data['test']['sequence_lengths']\ntest_data_sequences = data['test']['sequences']\nval_seq_lengths = data['valid']['sequence_lengths']\nval_data_sequences = data['valid']['sequences']\nN_train_data = len(training_seq_lengths)\nN_train_time_slices = np.sum(training_seq_lengths)\nN_mini_batches = int(N_train_data / args.mini_batch_size +\n                     int(N_train_data % args.mini_batch_size > 0))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Pyro EasyGuide Tutorial\nDESCRIPTION: This snippet imports necessary Python libraries and modules for working with Pyro and EasyGuide. It also sets up a smoke test flag and checks the Pyro version.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/easyguide.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.contrib.easyguide import easy_guide\nfrom pyro.optim import Adam\nfrom torch.distributions import constraints\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Classifier for Posterior Estimation in Python\nDESCRIPTION: This code defines a PyTorch neural network class 'OutcomePredictor' that serves as a variational approximation q(w|y) for estimating the posterior probability of election outcomes given polling data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\n\nclass OutcomePredictor(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.h1 = nn.Linear(51, 64)\n        self.h2 = nn.Linear(64, 64)\n        self.h3 = nn.Linear(64, 1)\n        \n    def compute_dem_probability(self, y):\n        z = nn.functional.relu(self.h1(y))\n        z = nn.functional.relu(self.h2(z))\n        return self.h3(z)\n    \n    def forward(self, y_dict, design, observation_labels, target_labels):\n        \n        pyro.module(\"posterior_guide\", self)\n        \n        y = y_dict[\"y\"]\n        dem_prob = self.compute_dem_probability(y).squeeze()\n        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))\n```\n\n----------------------------------------\n\nTITLE: Implementing Checkpointing for Model and Optimizer States\nDESCRIPTION: This snippet demonstrates how to save and load checkpoints for both the model parameters and optimizer states. It uses PyTorch's state_dict() method for the model and Pyro's optimizer.save() and optimizer.load() methods for the optimizer state.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# saves the model and optimizer states to disk\ndef save_checkpoint():\n    log(\"saving model to %s...\" % args.save_model)\n    torch.save(dmm.state_dict(), args.save_model)\n    log(\"saving optimizer states to %s...\" % args.save_opt)\n    optimizer.save(args.save_opt)\n    log(\"done saving model and optimizer checkpoints to disk.\")\n\n# loads the model and optimizer states from disk\ndef load_checkpoint():\n    assert exists(args.load_opt) and exists(args.load_model), \\\n        \"--load-model and/or --load-opt misspecified\"\n    log(\"loading model from %s...\" % args.load_model)\n    dmm.load_state_dict(torch.load(args.load_model))\n    log(\"loading optimizer states from %s...\" % args.load_opt)\n    optimizer.load(args.load_opt)\n    log(\"done loading model and optimizer states.\")\n```\n\n----------------------------------------\n\nTITLE: Wrapping Custom Effect Handler for Convenience\nDESCRIPTION: Shows how to wrap the custom LogJointMessenger for easier use as a context manager, decorator, or higher-order function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef log_joint(model=None, cond_data=None):\n    msngr = LogJointMessenger(cond_data=cond_data)\n    return msngr(model) if model is not None else msngr\n\nscale_log_joint = log_joint(scale, cond_data={\"measurement\": 9.5, \"weight\": 8.23})\nprint(scale_log_joint(8.5))\n```\n\n----------------------------------------\n\nTITLE: Plotting Multiple Trajectories with Varying Parameters\nDESCRIPTION: Defines a function to plot multiple trajectories using the extended model with variable prior parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef plot_trajectories(**kwargs):\n    pyro.set_rng_seed(12345)\n    with pyro.plate(\"trajectories\", 20, dim=-2):\n        trajectories = model2(**kwargs)\n    plt.figure(figsize=(8, 5)).patch.set_color(\"white\")\n    plt.plot(trajectories.T)\n    plt.xlabel(\"time\")\n    plt.ylabel(\"obs\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting Pyro Effects in Execution Trace\nDESCRIPTION: Shows how to examine all Pyro effects that appear in the execution trace of a PyroModule by using poutine.trace to capture sites.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith poutine.trace() as tr:\n    linear(example_input)\nfor site in tr.trace.nodes.values():\n    print(site[\"type\"], site[\"name\"], site[\"value\"])\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Model with Overlapping Non-nested Plates\nDESCRIPTION: Creates a Pyro model with overlapping non-nested plates to demonstrate how they are rendered.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    plate1 = pyro.plate(\"plate1\", 2, dim=-2)\n    plate2 = pyro.plate(\"plate2\", 3, dim=-1)\n    with plate1:\n        x = pyro.sample(\"x\", dist.Normal(0, 1))\n    with plate1, plate2:\n        y = pyro.sample(\"y\", dist.Normal(x, 1))\n    with plate2:\n        pyro.sample(\"z\", dist.Normal(y.sum(-2, True), 1), obs=torch.zeros(3))\n```\n\n----------------------------------------\n\nTITLE: Declaring independent dimension with pyro.plate\nDESCRIPTION: This snippet demonstrates the simplest way to declare a dimension as independent by declaring the rightmost batch dimension as independent using the `pyro.plate` context manager.  The size argument is provided to aid in debugging shapes. Within the context, batch dimension -1 is considered independent.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"with pyro.plate(\\\"my_plate\\\"):\\n    # within this context, batch dimension -1 is independent\"\n```\n\n----------------------------------------\n\nTITLE: Running SVI on Working Memory Model\nDESCRIPTION: Executes Stochastic Variational Inference (SVI) on the working memory model using example data where a participant remembered sequences of lengths 5 and 7 but failed at length 9.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\nconditioned_model = pyro.condition(model, {\"y\": y_data})\nsvi = SVI(conditioned_model,\n          guide,\n          Adam({\"lr\": .001}),\n          loss=Trace_ELBO(),\n          num_samples=100)\npyro.clear_param_store()\nnum_iters = 5000\nfor i in range(num_iters):\n    elbo = svi.step(l_data)\n    if i % 500 == 0:\n        print(\"Neg ELBO:\", elbo)\n```\n\n----------------------------------------\n\nTITLE: Prior Mean Calculation - Python\nDESCRIPTION: Calculates prior mean using 2012 election results with logistic transformation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults_2012 = torch.tensor(frame[2012].values, dtype=torch.float)\nprior_mean = torch.log(results_2012[..., 0] / results_2012[..., 1])\n```\n\n----------------------------------------\n\nTITLE: Prior Covariance Calculation - Python\nDESCRIPTION: Computes prior covariance matrix using historical election data from 1976-2012.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nidx = 2 * torch.arange(10)\nas_tensor = torch.tensor(frame.values, dtype=torch.float)\nlogits = torch.log(as_tensor[..., idx] / as_tensor[..., idx + 1]).transpose(0, 1)\nmean = logits.mean(0)\nsample_covariance = (1/(logits.shape[0] - 1)) * (\n    (logits.unsqueeze(-1) - mean) * (logits.unsqueeze(-2) - mean)\n).sum(0)\nprior_covariance = sample_covariance + 0.01 * torch.eye(sample_covariance.shape[0])\n```\n\n----------------------------------------\n\nTITLE: Effectful Operation Decorators for Lazy Evaluation\nDESCRIPTION: Decorator implementations for common operations to support lazy evaluation. Uses pyro.poutine.runtime.effectful to expose operations to LazyMessenger.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@effectful(type=\"apply\")\ndef add(x, y):\n    return x + y\n\n@effectful(type=\"apply\")\ndef mul(x, y):\n    return x * y\n\n@effectful(type=\"apply\")\ndef sigmoid(x):\n    return torch.sigmoid(x)\n\n@effectful(type=\"apply\")\ndef normal(loc, scale):\n    return dist.Normal(loc, scale)\n```\n\n----------------------------------------\n\nTITLE: Implementing Profile HMM with Constant + MuE in Python using Pyro\nDESCRIPTION: This code snippet demonstrates the implementation of a Profile Hidden Markov Model using Pyro's Constant and MuE features. It includes imports, model definition, data generation, and inference setup for sequence analysis tasks.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mue_profile.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport os\n\nimport torch\nfrom torch.distributions import Categorical, Dirichlet\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.contrib.examples.bart import analytic_bart\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\nfrom contrib.mue import MuE\n\n\nclass ProfileHMM(MuE):\n    def __init__(self, num_states, alphabet_size, num_domains=1):\n        self.num_states = num_states\n        self.alphabet_size = alphabet_size\n        self.num_domains = num_domains\n\n    def model(self):\n        with pyro.plate(\"domains\", self.num_domains):\n            with pyro.plate(\"states\", self.num_states):\n                emission = pyro.sample(\"emission\", Dirichlet(torch.ones(self.alphabet_size)))\n            insert = pyro.sample(\"insert\", Dirichlet(torch.ones(self.alphabet_size)))\n            with pyro.plate(\"transitions\", self.num_states + 1):\n                transition = pyro.sample(\"transition\", Dirichlet(torch.ones(3)))\n\n    def guide(self):\n        with pyro.plate(\"domains\", self.num_domains):\n            with pyro.plate(\"states\", self.num_states):\n                alpha_emission = pyro.param(\n                    \"alpha_emission\",\n                    torch.ones(self.num_domains, self.num_states, self.alphabet_size),\n                    constraint=torch.distributions.constraints.positive,\n                )\n                pyro.sample(\"emission\", Dirichlet(alpha_emission))\n            alpha_insert = pyro.param(\n                \"alpha_insert\",\n                torch.ones(self.num_domains, self.alphabet_size),\n                constraint=torch.distributions.constraints.positive,\n            )\n            pyro.sample(\"insert\", Dirichlet(alpha_insert))\n            with pyro.plate(\"transitions\", self.num_states + 1):\n                alpha_transition = pyro.param(\n                    \"alpha_transition\",\n                    torch.ones(self.num_domains, self.num_states + 1, 3),\n                    constraint=torch.distributions.constraints.positive,\n                )\n                pyro.sample(\"transition\", Dirichlet(alpha_transition))\n\n    def sample_observations(self, num_sequences, min_length, max_length):\n        with torch.no_grad():\n            with pyro.plate(\"sequences\", num_sequences):\n                length = pyro.sample(\n                    \"length\", Categorical(torch.ones(max_length - min_length + 1))\n                )\n                length = length + min_length\n                observations = []\n                for t in pyro.markov(range(max_length)):\n                    if t < length:\n                        state = pyro.sample(\n                            f\"state_{t}\",\n                            Categorical(torch.ones(self.num_states + 1) / (self.num_states + 1)),\n                        )\n                        if state < self.num_states:\n                            observation = pyro.sample(\n                                f\"obs_{t}\",\n                                Categorical(self.emission[state]),\n                                obs=torch.tensor(self.alphabet_size - 1),\n                            )\n                        else:\n                            observation = pyro.sample(\n                                f\"obs_{t}\",\n                                Categorical(self.insert),\n                                obs=torch.tensor(self.alphabet_size - 1),\n                            )\n                        observations.append(observation)\n        return observations\n\n\nif __name__ == \"__main__\":\n    pyro.set_rng_seed(0)\n\n    num_domains = 2\n    num_states = 5\n    alphabet_size = 4\n    num_sequences = 100\n    min_length = 10\n    max_length = 20\n\n    model = ProfileHMM(num_states, alphabet_size, num_domains)\n\n    optimizer = Adam({\"lr\": 0.01})\n    svi = SVI(model.model, model.guide, optimizer, loss=Trace_ELBO())\n\n    num_iterations = 1000\n    for i in range(num_iterations):\n        loss = svi.step()\n        if i % 100 == 0:\n            print(f\"[iteration {i + 1}] loss: {loss}\")\n\n    print(\"Learned parameters:\")\n    for name, value in pyro.get_param_store().items():\n        print(f\"{name}:\\n{value.detach().cpu().numpy()}\")\n```\n\n----------------------------------------\n\nTITLE: Defining SVI Guide for Manual Inference\nDESCRIPTION: This function defines a guide for use with Stochastic Variational Inference (SVI), specifying learnable parameters for the model variables.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef guide(features, counts):\n    N, P = features.shape\n    \n    scale_param = pyro.param(\"scale_param\", torch.tensor(0.1), constraint=constraints.positive)\n    loc_param = pyro.param(\"loc_param\", torch.tensor(0.0))\n    scale = pyro.sample(\"scale\", dist.Delta(scale_param))\n    coef = pyro.sample(\"coef\", dist.Normal(loc_param, scale).expand([P]).to_event(1))\n    \n    concentration_param = pyro.param(\"concentration_param\", torch.tensor(0.1), constraint=constraints.positive)\n    concentration = pyro.sample(\"concentration\", dist.Delta(concentration_param))\n```\n\n----------------------------------------\n\nTITLE: Implementing MAP Model and Guide in Pyro\nDESCRIPTION: Defines the guide for Maximum a Posteriori (MAP) estimation in Pyro using a Delta distribution, which puts all probability mass at a single learnable value.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mle_map.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef guide_map(data):\n    f_map = pyro.param(\"f_map\", torch.tensor(0.5),\n                       constraint=constraints.unit_interval)\n    pyro.sample(\"latent_fairness\", dist.Delta(f_map))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Parameter Store Behavior with a Simple Dictionary\nDESCRIPTION: Illustrates the basic behavior of a parameter store using a Python dictionary, which helps to understand how pyro.param works under the hood.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_ii.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsimple_param_store = {}\na = simple_param_store.setdefault(\"a\", torch.randn(1))\n```\n\n----------------------------------------\n\nTITLE: Using Neural Baseline in Pyro Guide Function\nDESCRIPTION: Shows how to use a neural network baseline within a guide function, including proper module registration and baseline specification.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef guide(x):  # here x is the current mini-batch of data\n    pyro.module(\"my_baseline\", baseline_module)\n    # ... other computations ...\n    z = pyro.sample(\"z\", dist.Bernoulli(...), \n                    infer=dict(baseline={'nn_baseline': baseline_module,\n                                         'nn_baseline_input': x}))\n```\n\n----------------------------------------\n\nTITLE: Working with Multivariate Normal Distribution\nDESCRIPTION: Shows how multivariate distributions have non-empty event shapes. Demonstrates the difference between sample shapes and log probability shapes for multivariate distributions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nd = MultivariateNormal(torch.zeros(3), torch.eye(3, 3))\nassert d.batch_shape == ()\nassert d.event_shape == (3,)\nx = d.sample()\nassert x.shape == (3,)            # == batch_shape + event_shape\nassert d.log_prob(x).shape == ()  # == batch_shape\n```\n\n----------------------------------------\n\nTITLE: Initializing Pyro Environment\nDESCRIPTION: Basic setup for Pyro environment by importing required packages and setting random seed.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport pyro\n\npyro.set_rng_seed(101)\n```\n\n----------------------------------------\n\nTITLE: Creating a PyroModule Using Bracket Syntax\nDESCRIPTION: Shows how to create a PyroModule using the PyroModule[...] syntax, which automatically creates a mixin class. This method is useful for quick conversion of existing modules.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlinear = PyroModule[Linear](5, 2)\nassert isinstance(linear, nn.Module)\nassert isinstance(linear, Linear)\nassert isinstance(linear, PyroModule)\n\nexample_input = torch.randn(100, 5)\nexample_output = linear(example_input)\nassert example_output.shape == (100, 2)\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Model with Numerically Integrated Log-Probability\nDESCRIPTION: Creates a version of the model that uses the Stable.log_prob() method for calculating log-probability density, without reparameterization of the Stable distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nmodel_with_log_prob = poutine.reparam(model, {\"v\": DiscreteCosineReparam()})\n```\n\n----------------------------------------\n\nTITLE: Inspecting Posterior Means with Pyro in Python\nDESCRIPTION: This snippet iterates over the posterior median values of the latent parameters obtained from a variational inference guide. It prints the parameter names and values converted to NumPy arrays for each parameter in the Pyro guide.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfor k, v in guide.median().items():\n    print(k, v.data.cpu().numpy())\n```\n\n----------------------------------------\n\nTITLE: Visualizing Hierarchical Pyro Model Structure in Python\nDESCRIPTION: Invokes Pyro's rendering capability to visualize the hierarchical model defined in the function 'regional_model2'. This aids in understanding complex model structures before fitting.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\npyro.render_model(partial(regional_model2, counts))\n```\n\n----------------------------------------\n\nTITLE: Improving MACE Model Layout with Unflatten\nDESCRIPTION: Applies the unflatten preprocessor to improve the layout of the MACE model visualization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# layout after processing the layout with unflatten\nmace_graph.unflatten(stagger=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Function\nDESCRIPTION: Defines the training loop using SVI with TraceEnum_ELBO loss and Adam optimizer.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tracking_1d.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninfer = SVI(model, guide, Adam({\"lr\": 0.01}), TraceEnum_ELBO(max_plate_nesting=2))\nlosses = []\nfor epoch in range(101 if not smoke_test else 2):\n    loss = infer.step(args, observations)\n    if epoch % 10 == 0:\n        print(\"epoch {: >4d} loss = {}\".format(epoch, loss))\n    losses.append(loss)\n```\n\n----------------------------------------\n\nTITLE: Using pyro.to_funsor and pyro.to_data with Named Handler\nDESCRIPTION: This snippet demonstrates the use of pyro.to_funsor and pyro.to_data functions within a pyro.contrib.funsor backend context. It shows how these functions automatically manage dimension mappings and can be used to convert between Funsor and PyTorch tensors efficiently.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    funsor_x = funsor.Tensor(torch.ones((2,)), OrderedDict(x=funsor.Bint[2]), 'real')\n    tensor_x = pyro.to_data(funsor_x)\n    print(funsor_x.inputs, tensor_x.shape)\n\n    funsor_y = funsor.Tensor(torch.ones((3, 2)), OrderedDict(y=funsor.Bint[3], x=funsor.Bint[2]), 'real')\n    tensor_y = pyro.to_data(funsor_y)\n    print(funsor_y.inputs, tensor_y.shape)\n\n    funsor_z = funsor.Tensor(torch.ones((2, 3)), OrderedDict(z=funsor.Bint[2], y=funsor.Bint[3]), 'real')\n    tensor_z = pyro.to_data(funsor_z)\n    print(funsor_z.inputs, tensor_z.shape)\n```\n\n----------------------------------------\n\nTITLE: SIR Model HMC Output Example\nDESCRIPTION: Example output showing the inference results including R0 (basic reproduction number) and rho (observation rate) estimates, along with their uncertainty bounds and convergence diagnostics.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/sir_hmc.rst#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nGenerating data...\nObserved 452/871 infections:\n0 0 2 1 2 0 0 3 2 0 1 3 1 3 0 1 0 6 4 3 6 4 4 3 3 3 5 3 3 3 5 1 4 6 4 2 6 8 7 4 11 8 14 9 17 13 9 14 10 15 16 22 20 22 19 20 28 25 23 21\nRunning inference...\nSample: 100%|=========================| 300/300 [02:35,  1.93it/s, step size=9.67e-02, acc. prob=0.878]\n\n                    mean       std    median      5.0%     95.0%     n_eff     r_hat\n            R0      1.40      0.07      1.40      1.28      1.49     26.56      1.06\n           rho      0.47      0.02      0.47      0.44      0.52      7.08      1.22\n      S_aux[0]   9998.74      0.64   9998.75   9997.84   9999.67     28.74      1.00\n      S_aux[1]   9998.37      0.72   9998.38   9997.28   9999.44     52.24      1.02\n      ...\n      I_aux[0]      1.11      0.64      0.99      0.19      2.02     22.01      1.00\n      I_aux[1]      1.55      0.74      1.65      0.05      2.47     10.05      1.10\n      ...\n\nNumber of divergences: 0\nR0: truth = 1.5, estimate = 1.4 ± 0.0654\nrho: truth = 0.5, estimate = 0.475 ± 0.023\n```\n\n----------------------------------------\n\nTITLE: HMC Class Documentation Reference\nDESCRIPTION: Sphinx documentation directive for the Hamiltonian Monte Carlo (HMC) implementation\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.infer.mcmc.HMC\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Implementing Gated Transition Network in PyTorch\nDESCRIPTION: Implements a GatedTransition neural network that parameterizes the gaussian latent transition probability p(z_t | z_{t-1}). Features a gating mechanism to support both linear and non-linear dynamics, outputting mean and scale parameters for a diagonal gaussian distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass GatedTransition(nn.Module):\n    \"\"\"\n    Parameterizes the gaussian latent transition probability p(z_t | z_{t-1})\n    See section 5 in the reference for comparison.\n    \"\"\"\n    def __init__(self, z_dim, transition_dim):\n        super().__init__()\n        # initialize the six linear transformations used in the neural network\n        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n        self.lin_sig = nn.Linear(z_dim, z_dim)\n        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n        # modify the default initialization of lin_z_to_loc\n        # so that it's starts out as the identity function\n        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n        # initialize the three non-linearities used in the neural network\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.softplus = nn.Softplus()\n\n    def forward(self, z_t_1):\n        \"\"\"\n        Given the latent z_{t-1} corresponding to the time step t-1\n        we return the mean and scale vectors that parameterize the\n        (diagonal) gaussian distribution p(z_t | z_{t-1})\n        \"\"\"\n        # compute the gating function\n        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n        gate = self.sigmoid(self.lin_gate_hidden_to_z(_gate))\n        # compute the 'proposed mean'\n        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n        # assemble the actual mean used to sample z_t, which mixes \n        # a linear transformation of z_{t-1} with the proposed mean \n        # modulated by the gating function\n        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean\n        # compute the scale used to sample z_t, using the proposed \n        # mean from above as input. the softplus ensures that scale is positive\n        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n        # return loc, scale which can be fed into Normal\n        return loc, scale\n```\n\n----------------------------------------\n\nTITLE: Implementing Prediction Network Class for AIR in PyTorch\nDESCRIPTION: A PyTorch module that predicts guide distributions for z_where and z_pres parameters in the AIR model. It takes hidden state input and outputs presence probability and location parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass Predict(nn.Module):\n    def __init__(self, ):\n        super().__init__()\n        self.l = nn.Linear(256, 7)\n\n    def forward(self, h):\n        a = self.l(h)\n        z_pres_p = sigmoid(a[:, 0:1]) # Squish to [0,1]\n        z_where_loc = a[:, 1:4]\n        z_where_scale = softplus(a[:, 4:]) # Squish to >0\n        return z_pres_p, z_where_loc, z_where_scale\n\npredict = Predict()\n```\n\n----------------------------------------\n\nTITLE: Fitting Pyro Model with Numerically Integrated Log-Probability\nDESCRIPTION: Fits the model using numerically integrated log-probability, similar to the previous fitting process. It uses the same fit_model function defined earlier.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\npyro.set_rng_seed(1234567890)\n\nguide_with_log_prob, losses_with_log_prob, stats_with_log_prob = fit_model(model_with_log_prob)\n\nprint(\"-\" * 20)\nfor name, (lb, ub) in sorted(stats_with_log_prob[-1]):\n    if lb.numel() == 1:\n        lb = lb.squeeze().item()\n        ub = ub.squeeze().item()\n        print(\"{} = {:0.4g} ± {:0.4g}\".format(name, (lb + ub) / 2, (ub - lb) / 2))\n\npyplot.figure(figsize=(9, 3))\npyplot.plot(losses_with_log_prob)\npyplot.ylabel(\"loss\")\npyplot.xlabel(\"SVI step\")\npyplot.xlim(0, len(losses_with_log_prob))\npyplot.ylim(min(losses_with_log_prob), 20)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Dirichlet Process Mixture Models in Pyro\nDESCRIPTION: Sets up the necessary Python libraries for implementing Dirichlet Process Mixture Models. Includes data manipulation libraries (pandas, numpy), visualization (matplotlib), PyTorch for tensor operations, and Pyro for probabilistic programming. Also sets a random seed for reproducibility.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions import constraints\n\nimport pyro\nfrom pyro.distributions import *\nfrom pyro.infer import Predictive, SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\nassert pyro.__version__.startswith('1.9.1')\npyro.set_rng_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Creating Batched Distributions with Batched Parameters\nDESCRIPTION: Shows how to create a batched distribution by passing batched parameters. This creates a distribution with non-empty batch shape but empty event shape.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nd = Bernoulli(0.5 * torch.ones(3,4))\nassert d.batch_shape == (3, 4)\nassert d.event_shape == ()\nx = d.sample()\nassert x.shape == (3, 4)\nassert d.log_prob(x).shape == (3, 4)\n```\n\n----------------------------------------\n\nTITLE: Implementing log_z function for ELBO computation with variable elimination in Python\nDESCRIPTION: This function applies three effect handlers to compute an expression for the log-density, marginalizes over discrete variables, averages over Monte Carlo samples, and evaluates the final lazy expression using Funsor's optimize interpretation for variable elimination.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@pyroapi.pyro_backend(\"contrib.funsor\")\ndef log_z(model, model_args, size=10):\n    with LogJointMessenger() as tr, \\\n            VectorizeMessenger(size=size) as v, \\\n            EnumMessenger():\n        model(*model_args)\n\n    with funsor.interpreter.interpretation(funsor.terms.lazy):\n        prod_vars = frozenset({v.name})\n        sum_vars = frozenset(tr.log_joint.inputs) - prod_vars\n        \n        # sum over the discrete random variables we enumerated\n        expr = tr.log_joint.reduce(funsor.ops.logaddexp, sum_vars)\n        \n        # average over the sample dimension\n        expr = expr.reduce(funsor.ops.add, prod_vars) - funsor.Number(float(size))\n\n    return pyro.to_data(funsor.optimizer.apply_optimizer(expr))\n```\n\n----------------------------------------\n\nTITLE: Implementing Spatial Transformer Network\nDESCRIPTION: Implements helper functions for spatial transformation to position and scale objects within the larger image using PyTorch's grid_sample and affine_grid.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef expand_z_where(z_where):\n    n = z_where.size(0)\n    expansion_indices = torch.LongTensor([1, 0, 2, 0, 1, 3])\n    out = torch.cat((torch.zeros([1, 1]).expand(n, 1), z_where), 1)\n    return torch.index_select(out, 1, expansion_indices).view(n, 2, 3)\n\ndef object_to_image(z_where, obj):\n    n = obj.size(0)\n    theta = expand_z_where(z_where)\n    grid = affine_grid(theta, torch.Size((n, 1, 50, 50)))\n    out = grid_sample(obj.view(n, 1, 20, 20), grid)\n    return out.view(n, 50, 50)\n```\n\n----------------------------------------\n\nTITLE: Plotting Comparison of Stability Parameters in Pyro Models\nDESCRIPTION: Creates plots to compare the stability parameters from both models (with and without reparameterization) over SVI steps.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef plot_comparison(log_prob_values, reparam_values, xlabel, ylabel):\n    pyplot.plot(log_prob_values, color='b', label='Log-Prob')\n    pyplot.plot(reparam_values, color='r', label='Reparam')\n    pyplot.xlabel(xlabel)\n    pyplot.ylabel(ylabel)\n    pyplot.legend(loc='best')\n    pyplot.grid()\n\npyplot.subplot(2,1,1)\nplot_comparison(stability_with_log_prob, stability, '', 'Stability')\n\npyplot.subplot(2,1,2)\nplot_comparison(stability_with_log_prob, stability, 'SVI step', 'Stability (Zoomed)')\npyplot.ylim(1.8, 2)\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Model with Deterministic Variables\nDESCRIPTION: Creates a Pyro model that includes deterministic variables defined using pyro.deterministic().\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef model_deterministic(data):\n    sigma = pyro.param(\"sigma\", torch.tensor([1.]), constraint=constraints.positive)\n    mu = pyro.param(\"mu\", torch.tensor([0.]))\n    x = pyro.sample(\"x\", dist.Normal(mu, sigma))\n    log_y = pyro.sample(\"y\", dist.Normal(x, 1))\n    y = pyro.deterministic(\"y_deterministic\", log_y.exp())\n    with pyro.plate(\"N\", len(data)):\n        eps_z_loc = pyro.sample(\"eps_z_loc\", dist.Normal(0, 1))\n        z_loc = pyro.deterministic(\"z_loc\", eps_z_loc + x, event_dim=0)\n        pyro.sample(\"z\", dist.Normal(z_loc, y), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop with Parameter Tracking\nDESCRIPTION: Implementation of a training loop that uses effect handlers to track and optimize parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef train(model, guide, data):\n    optimizer = pyro.optim.Adam({})\n    for batch in data:\n        # this poutine.trace will record all of the parameters that appear in the model and guide\n        # during the execution of monte_carlo_elbo\n        with poutine.trace() as param_capture:\n            # we use poutine.block here so that only parameters appear in the trace above\n            with poutine.block(hide_fn=lambda node: node[\"type\"] != \"param\"):\n                loss = monte_carlo_elbo(model, guide, batch)\n        \n        loss.backward()\n        params = set(node[\"value\"].unconstrained()\n                     for node in param_capture.trace.nodes.values())\n        optimizer.step(params)\n        pyro.infer.util.zero_grads(params)\n```\n\n----------------------------------------\n\nTITLE: Rendering Pyro Model with Partial Function in Python\nDESCRIPTION: Uses Pyro's model rendering function to visualize a supplied model function partially applied with the count data. This aids in understanding the model structure and dependencies visually before fitting.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\npyro.render_model(partial(regional_model, counts))\n```\n\n----------------------------------------\n\nTITLE: Bernoulli Beta Model Implementation in Pyro\nDESCRIPTION: This code defines a Bernoulli-Beta model and its variational guide using Pyro. It includes the model, guide, and inference procedure. It uses a non-reparameterized Beta distribution to highlight the benefits of using baselines for variance reduction.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torch.distributions.constraints as constraints\nimport pyro\nimport pyro.distributions as dist\n# Pyro also has a reparameterized Beta distribution so we import\n# the non-reparameterized version to make our point\nfrom pyro.distributions.testing.fakes import NonreparameterizedBeta\nimport pyro.optim as optim\nfrom pyro.infer import SVI, TraceGraph_ELBO\nimport sys\n\nassert pyro.__version__.startswith('1.9.1')\n\n# this is for running the notebook in our testing framework\nsmoke_test = ('CI' in os.environ)\nmax_steps = 2 if smoke_test else 10000\n\n\ndef param_abs_error(name, target):\n    return torch.sum(torch.abs(target - pyro.param(name))).item()\n\n\nclass BernoulliBetaExample:\n    def __init__(self, max_steps):\n        # the maximum number of inference steps we do\n        self.max_steps = max_steps\n        # the two hyperparameters for the beta prior\n        self.alpha0 = 10.0\n        self.beta0 = 10.0\n        # the dataset consists of six 1s and four 0s\n        self.data = torch.zeros(10)\n        self.data[0:6] = torch.ones(6)\n        self.n_data = self.data.size(0)\n        # compute the alpha parameter of the exact beta posterior\n        self.alpha_n = self.data.sum() + self.alpha0\n        # compute the beta parameter of the exact beta posterior\n        self.beta_n = - self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n        # initial values of the two variational parameters\n        self.alpha_q_0 = 15.0\n        self.beta_q_0 = 15.0\n\n    def model(self, use_decaying_avg_baseline):\n        # sample `latent_fairness` from the beta prior\n        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n        # use plate to indicate that the observations are\n        # conditionally independent given f and get vectorization\n        with pyro.plate(\"data_plate\"):\n            # observe all ten datapoints using the bernoulli likelihood\n            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n\n    def guide(self, use_decaying_avg_baseline):\n        # register the two variational parameters with pyro\n        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0),\n                             constraint=constraints.positive)\n        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0),\n                            constraint=constraints.positive)\n        # sample f from the beta variational distribution\n        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline,\n                         'baseline_beta': 0.90}\n        # note that the baseline_dict specifies whether we're using\n        # decaying average baselines or not\n        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q),\n                    infer=dict(baseline=baseline_dict))\n\n    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n        # clear the param store in case we're in a REPL\n        pyro.clear_param_store()\n        # setup the optimizer and the inference algorithm\n        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n\n        # do up to this many steps of inference\n        for k in range(self.max_steps):\n            svi.step(use_decaying_avg_baseline)\n            if k % 100 == 0:\n                print('.', end='')\n                sys.stdout.flush()\n\n            # compute the distance to the parameters of the true posterior\n            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n\n            # stop inference early if we're close to the true posterior\n            if alpha_error < tolerance and beta_error < tolerance:\n                break\n\n        print(\"\\nDid %d steps of inference.\" % k)\n        print((\"Final absolute errors for the two variational parameters \" +\n               \"were %.4f & %.4f\") % (alpha_error, beta_error))\n\n# do the experiment\nbbe = BernoulliBetaExample(max_steps=max_steps)\nbbe.do_inference(use_decaying_avg_baseline=True)\nbbe.do_inference(use_decaying_avg_baseline=False)\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoNormal Guide and SVI\nDESCRIPTION: Sets up an AutoNormal guide with Adam optimizer and Trace_ELBO loss for SVI. Clears parameter store and configures optimization parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npyro.clear_param_store()\n\nadam_params = {\"lr\": 0.005, \"betas\": (0.90, 0.999)}\noptimizer = Adam(adam_params)\n\nauto_guide = AutoNormal(model)\n\nsvi = SVI(model, auto_guide, optimizer, loss=Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Creating a Parameterized Guide Function for BBBVI\nDESCRIPTION: Defines a guide function that parameterizes a normal distribution for the latent variable z. The index parameter enables creating separate guide components for each iteration of the boosting algorithm.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef guide(data, index):\n    scale_q = pyro.param('scale_{}'.format(index), torch.tensor([1.0]), constraints.positive)\n    loc_q = pyro.param('loc_{}'.format(index), torch.tensor([0.0]))\n    pyro.sample(\"z\", dist.Normal(loc_q, scale_q))\n```\n\n----------------------------------------\n\nTITLE: Fitting Hierarchical Variational Model using SVI in Python\nDESCRIPTION: The snippet fits the second regional model to the SARS-CoV-2 data using stochastic variational inference for 3001 iterations, employing Pyro's SVI for guide function learning.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n%%time\nguide = fit_svi(partial(regional_model2, counts), num_steps=3001)\n```\n\n----------------------------------------\n\nTITLE: Implementing Gaussian Encoder and Bernoulli Decoder\nDESCRIPTION: Define neural network architectures for the encoder (using Gaussian distribution) and decoder (using Bernoulli distribution) components of the VAE.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae_flow_prior.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass GaussianEncoder(nn.Module):\n    def __init__(self, features: int, latent: int):\n        super().__init__()\n\n        self.hyper = nn.Sequential(\n            nn.Linear(features, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 2 * latent),\n        )\n\n    def forward(self, x: Tensor):\n        phi = self.hyper(x)\n        mu, log_sigma = phi.chunk(2, dim=-1)\n\n        return pyro.distributions.Normal(mu, log_sigma.exp()).to_event(1)\n\n\nclass BernoulliDecoder(nn.Module):\n    def __init__(self, features: int, latent: int):\n        super().__init__()\n\n        self.hyper = nn.Sequential(\n            nn.Linear(latent, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, features),\n        )\n\n    def forward(self, z: Tensor):\n        phi = self.hyper(z)\n        rho = torch.sigmoid(phi)\n\n        return pyro.distributions.Bernoulli(rho).to_event(1)\n```\n\n----------------------------------------\n\nTITLE: Understanding PyroSample Access Behavior\nDESCRIPTION: Demonstrates that PyroSample attributes are drawn at most once per module invocation, with subsequent accesses returning the same sample value.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass BayesianLinear(PyroModule):\n    ...\n    def forward(self, input):\n        weight1 = self.weight      # Draws a sample.\n        weight2 = self.weight      # Reads previous sample.\n        assert weight2 is weight1  # All accesses should agree.\n        ...\n```\n\n----------------------------------------\n\nTITLE: Rendering Semisupervised Pyro Model\nDESCRIPTION: Shows how to render a semisupervised Pyro model with different sets of arguments for labeled and unlabeled cases.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npyro.render_model(\n    model,\n    model_kwargs=[\n        {\"x\": torch.zeros(2)},\n        {\"x\": torch.zeros(2), \"y\": torch.zeros(2)},\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Trained VAE\nDESCRIPTION: Generate new MNIST images by sampling from the learned prior distribution and decoding through the trained model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae_flow_prior.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nz = vae.prior().sample((16,))\nx = vae.decoder(z).mean.reshape(-1, 28, 28)\n\nto_pil_image(x.movedim(0, 1).reshape(28, -1))\n```\n\n----------------------------------------\n\nTITLE: Reshaping Distributions with to_event()\nDESCRIPTION: Demonstrates how to treat a univariate distribution as multivariate by using the to_event() method, which converts batch dimensions to event dimensions from the right.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nd = Bernoulli(0.5 * torch.ones(3,4)).to_event(1)\nassert d.batch_shape == (3,)\nassert d.event_shape == (4,)\nx = d.sample()\nassert x.shape == (3, 4)\nassert d.log_prob(x).shape == (3,)\n```\n\n----------------------------------------\n\nTITLE: Updating Pyro Model with Tuned Prior Parameters\nDESCRIPTION: Modifies the original Pyro model with updated prior parameters based on the interactive tuning process.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef model(T: int = 1000, data=None):\n    df = pyro.sample(\"df\", dist.LogNormal(4, 1))  # <-- changed 0 to 4\n    p_scale = pyro.sample(\"p_scale\", dist.LogNormal(1, 1))  # <-- changed 0 to 1\n    m_scale = pyro.sample(\"m_scale\", dist.LogNormal(0, 1))\n\n    with pyro.plate(\"dt\", T):\n        process_noise = pyro.sample(\"process_noise\", dist.StudentT(df, 0, p_scale))\n    trend = pyro.deterministic(\"trend\", process_noise.cumsum(-1))\n    with pyro.plate(\"t\", T):\n        return pyro.sample(\"obs\", dist.Normal(trend, m_scale), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Training SVI Model\nDESCRIPTION: Trains the SVI model for 3001 steps, printing loss every 1000 steps. Uses the previously configured guide and optimizer.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nn_steps = 3001\n\nfor step in range(n_steps):\n    loss = svi.step(X_, y_)\n    if step % 1000 == 0:\n        print('Loss: ', loss)\n```\n\n----------------------------------------\n\nTITLE: Calculating and Plotting Daily Log Returns\nDESCRIPTION: This snippet calculates the daily log returns of the S&P 500 index and plots them. The log returns are computed as the logarithm of the price ratio between two consecutive trading days.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npyplot.figure(figsize=(9, 3))\nr = (x[1:] / x[:-1]).log()\npyplot.plot(r, \"k\", lw=0.1)\npyplot.title(\"daily log returns\")\npyplot.xlabel(\"trading day\");\n```\n\n----------------------------------------\n\nTITLE: Converting Funsor Objects to Data\nDESCRIPTION: Shows conversion of Funsor terms back to Python objects and torch tensors using funsor.to_data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata_one = funsor.to_data(funsor.terms.Number(float(1), 'real'))\nprint(data_one, type(data_one))\n\ndata_two = funsor.to_data(funsor.Tensor(torch.tensor(2.), OrderedDict(), 'real'))\nprint(data_two, type(data_two))\n```\n\n----------------------------------------\n\nTITLE: Defining Reparameterized Model in Pyro\nDESCRIPTION: Creates a reparameterized version of the model using StableReparam for the Stable likelihood and DiscreteCosineReparam for the latent Gaussian process.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nreparam_model = poutine.reparam(model, {\"v\": DiscreteCosineReparam(),\n                                        \"r\": StableReparam()})\n```\n\n----------------------------------------\n\nTITLE: Implementing Pragmatic Listener\nDESCRIPTION: Defines the pragmatic listener that jointly reasons about QUD and state based on speaker utterances.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-hyperbole.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@Marginal\ndef pragmatic_listener(utterance):\n    state = state_prior()\n    qud = qud_prior()\n    speaker_marginal = speaker(state, qud)\n    pyro.sample(\"speaker\", speaker_marginal, obs=utterance)\n    return state\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Model with Parameters\nDESCRIPTION: Creates a Pyro model that includes parameters defined using pyro.param().\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    sigma = pyro.param(\"sigma\", torch.tensor([1.]), constraint=constraints.positive)\n    mu = pyro.param(\"mu\", torch.tensor([0.]))\n    x = pyro.sample(\"x\", dist.Normal(mu, sigma))\n    y = pyro.sample(\"y\", dist.LogNormal(x, 1))\n    with pyro.plate(\"N\", len(data)):\n        pyro.sample(\"z\", dist.Normal(x, y), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Initializing Guide Distribution with Low Variance\nDESCRIPTION: Examples of good and bad guide distribution initialization in Pyro. Shows proper low-variance initialization for better ELBO optimization stability.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.distributions import constraints\n\ndef good_guide():\n    scale = pyro.param(\"scale\", torch.tensor(0.05),               \n                       constraint=constraints.positive)\n    pyro.sample(\"x\", dist.Normal(0.0, scale))\n```\n\nLANGUAGE: python\nCODE:\n```\ndef bad_guide():\n    scale = pyro.param(\"scale\", torch.tensor(12345.6),               \n                       constraint=constraints.positive)\n    pyro.sample(\"x\", dist.Normal(0.0, scale))\n```\n\n----------------------------------------\n\nTITLE: Plotting Single Trajectory with Fixed Parameters\nDESCRIPTION: Defines a function to plot a single trajectory using fixed parameter values. This function is used with ipywidgets for interactive visualization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef plot_trajectory(df=1.0, p_scale=1.0, m_scale=1.0):\n    pyro.set_rng_seed(12345)\n    data = {\n        \"df\": torch.as_tensor(df),\n        \"p_scale\": torch.as_tensor(p_scale),\n        \"m_scale\": torch.as_tensor(m_scale),\n    }\n    trajectory = poutine.condition(model, data)()\n    plt.figure(figsize=(8, 4)).patch.set_color(\"white\")\n    plt.plot(trajectory)\n    plt.xlabel(\"time\")\n    plt.ylabel(\"obs\")\n```\n\n----------------------------------------\n\nTITLE: Example Model Using Lazy Evaluation\nDESCRIPTION: Demonstration of using LazyMessenger with a probabilistic model. Shows how lazy evaluation works with sampling operations in Pyro.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef biased_scale(guess):\n    weight = pyro.sample(\"weight\", normal(guess, 1.))\n    tolerance = pyro.sample(\"tolerance\", normal(0., 0.25))\n    return pyro.sample(\"measurement\", normal(add(mul(weight, 0.8), 1.), sigmoid(tolerance)))\n\nwith LazyMessenger():\n    v = biased_scale(8.5)\n    print(v)\n    print(v.evaluate())\n```\n\n----------------------------------------\n\nTITLE: Configuring Parameter-Specific Optimization\nDESCRIPTION: Shows how to create an optimizer with different learning rates for specific parameters using a callable configuration.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_i.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.optim import Adam\n\ndef per_param_callable(param_name):\n    if param_name == 'my_special_parameter':\n        return {\"lr\": 0.010}\n    else:\n        return {\"lr\": 0.001}\n\noptimizer = Adam(per_param_callable)\n```\n\n----------------------------------------\n\nTITLE: TraceGraph_ELBO Dependency Tracking Example\nDESCRIPTION: Demonstrates how TraceGraph_ELBO uses provenance tracking for non-reparameterizable random variables in a probabilistic model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    probs_a = torch.tensor([0.3, 0.7])\n    probs_b = torch.tensor([[0.1, 0.9], [0.8, 0.2]])\n    probs_c = torch.tensor([[0.5, 0.5], [0.6, 0.4]])\n    a = pyro.sample(\"a\", dist.Categorical(probs_a))\n    b = pyro.sample(\"b\", dist.Categorical(probs_b[a]))\n    pyro.sample(\"c\", dist.Categorical(probs_c[b]), obs=torch.tensor(0))\n\nwith TrackNonReparam():\n    model_tr = trace(model).get_trace()\nmodel_tr.compute_log_prob()\n\nassert get_provenance(model_tr.nodes[\"a\"][\"log_prob\"]) == frozenset({'a'})\nassert get_provenance(model_tr.nodes[\"b\"][\"log_prob\"]) == frozenset({'b', 'a'})\nassert get_provenance(model_tr.nodes[\"c\"][\"log_prob\"]) == frozenset({'b', 'a'})\n```\n\n----------------------------------------\n\nTITLE: Random Effects Sampling for Groups and Individuals\nDESCRIPTION: Code showing how random effects are sampled for groups and individuals using categorical distributions, where values are selected from learned parameter vectors.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/examples/mixed_hmm/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\niota_G[a] ~ Categorical(pi_G)\nepsilon_G[a] = Theta_G[iota_G[a]]\niota_I[a,b] ~ Categorical(pi_I[a])\nepsilon_I[a,b] = Theta_I[a][iota_I[a,b]]\n```\n\n----------------------------------------\n\nTITLE: Defining Model with Subsampling in Pyro\nDESCRIPTION: This snippet defines a probabilistic model using Pyro's sampling methods, incorporating both global and local random variables with subsampling integrated through plates.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    beta = pyro.sample(\"beta\", ...) # sample the global RV\n    for i in pyro.plate(\"locals\", len(data)):\n        z_i = pyro.sample(\"z_{}\".format(i), ...)\n        # compute the parameter used to define the observation \n        # likelihood using the local random variable\n        theta_i = compute_something(z_i) \n        pyro.sample(\"obs_{}\".format(i), dist.MyDist(theta_i), obs=data[i])\n```\n\n----------------------------------------\n\nTITLE: Using PyroModule as Attribute\nDESCRIPTION: This code snippet illustrates the correct way to use PyroModule as an attribute of another PyroModule. By making one PyroModule an attribute of the other, Pyro ensures unique names.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_19\n\nLANGUAGE: diff\nCODE:\n```\nclass Model(PyroModule):\n    def __init__(self):\n       self.x = PyroModule()  # ok\n```\n\n----------------------------------------\n\nTITLE: Converting Variables with Domain Information\nDESCRIPTION: Demonstrates conversion of Python strings to Funsor variables using domain type information.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvar_x = funsor.to_funsor(\"x\", output=funsor.Reals[2])\nprint(var_x, var_x.inputs, var_x.output)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data for HMM Analysis\nDESCRIPTION: Initializes data structures with problem sizes, costs for different implementations (einsum, tensordot, tensor), and runtime measurements for optimization and computation phases.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsizes = [\n    3,\n    4,\n    5,\n    6,\n    7,\n    8,\n    9,\n    10,\n    11,\n    12,\n    13,\n    14,\n    15,\n    16,\n    17,\n    18,\n    19,\n    20,\n    21,\n    22,\n    23,\n    24,\n    25,\n    26,\n    27,\n    28,\n    29,\n    30,\n    31,\n    32,\n    33,\n    34,\n    35,\n    36,\n    37,\n    38,\n    39,\n    40,\n    41,\n    42,\n    43,\n    44,\n    45,\n    46,\n    47,\n    48,\n    49,\n    50,\n]\ncosts = {\n    \"einsum\": [\n        1,\n        2,\n        3,\n        4,\n        5,\n        6,\n        7,\n        8,\n        9,\n        10,\n        11,\n        12,\n        13,\n        14,\n        15,\n        16,\n        17,\n        18,\n        19,\n        20,\n        21,\n        22,\n        23,\n        24,\n        25,\n        26,\n        27,\n        28,\n        29,\n        30,\n        31,\n        32,\n        33,\n        34,\n        35,\n        36,\n        37,\n        38,\n        39,\n        40,\n        41,\n        42,\n        43,\n        44,\n        45,\n        46,\n        47,\n        48,\n    ],\n    \"tensordot\": [\n        12,\n        16,\n        20,\n        24,\n        28,\n        32,\n        36,\n        40,\n        44,\n        48,\n        52,\n        56,\n        60,\n        64,\n        68,\n        72,\n        76,\n        80,\n        84,\n        88,\n        92,\n        96,\n        100,\n        104,\n        108,\n        112,\n        116,\n        120,\n        124,\n        128,\n        132,\n        136,\n        140,\n        144,\n        148,\n        152,\n        156,\n        160,\n        164,\n        168,\n        172,\n        176,\n        180,\n        184,\n        188,\n        192,\n        196,\n        200,\n    ],\n    \"tensor\": [\n        22,\n        31,\n        40,\n        49,\n        58,\n        67,\n        76,\n        85,\n        94,\n        103,\n        112,\n        121,\n        130,\n        139,\n        148,\n        157,\n        166,\n        175,\n        184,\n        193,\n        202,\n        211,\n        220,\n        229,\n        238,\n        247,\n        256,\n        265,\n        274,\n        283,\n        292,\n        301,\n        310,\n        319,\n        328,\n        337,\n        346,\n        355,\n        364,\n        373,\n        382,\n        391,\n        400,\n        409,\n        418,\n        427,\n        436,\n        445,\n    ],\n}\ntimes1 = [\n    0.01864790916442871,\n    0.015166997909545898,\n    0.017799854278564453,\n    0.021364927291870117,\n    0.0234529972076416,\n    0.03243708610534668,\n    0.03485298156738281,\n    0.03809309005737305,\n    0.04254293441772461,\n    0.043493032455444336,\n    0.04782605171203613,\n    0.051072120666503906,\n    0.05495715141296387,\n    0.06077980995178223,\n    0.06451010704040527,\n    0.06647181510925293,\n    0.07750391960144043,\n    0.10012388229370117,\n    0.09436392784118652,\n    0.08780503273010254,\n    0.09475111961364746,\n    0.08931398391723633,\n    0.1099538803100586,\n    0.10660696029663086,\n    0.10943722724914551,\n    0.11156201362609863,\n    0.11216998100280762,\n    0.11894893646240234,\n    0.12170791625976562,\n    0.1290268898010254,\n    0.13869500160217285,\n    0.1344318389892578,\n    0.13837814331054688,\n    0.14883112907409668,\n    0.14552593231201172,\n    0.1480569839477539,\n    0.14761590957641602,\n    0.15995121002197266,\n    0.16048288345336914,\n    0.16365408897399902,\n    0.16843199729919434,\n    0.2130718231201172,\n    0.17986297607421875,\n    0.1792001724243164,\n    0.1941969394683838,\n    0.2153019905090332,\n    0.20756793022155762,\n    0.19938111305236816,\n]\ntimes2 = [\n    0.010827064514160156,\n    0.014249086380004883,\n    0.016450166702270508,\n    0.020006895065307617,\n    0.025799989700317383,\n    0.02879500389099121,\n    0.03235912322998047,\n    0.036743879318237305,\n    0.04072308540344238,\n    0.04432511329650879,\n    0.04558587074279785,\n    0.051867008209228516,\n    0.05726289749145508,\n    0.058149099349975586,\n    0.06532096862792969,\n    0.0634920597076416,\n    0.07218098640441895,\n    0.12434697151184082,\n    0.07972311973571777,\n    0.08487296104431152,\n    0.08191704750061035,\n    0.13434886932373047,\n    0.10629105567932129,\n    0.10842609405517578,\n    0.10170793533325195,\n    0.10760092735290527,\n    0.11115694046020508,\n    0.1158750057220459,\n    0.12462496757507324,\n    0.1272139549255371,\n    0.13429498672485352,\n    0.1305849552154541,\n    0.14617490768432617,\n    0.18872499465942383,\n    0.1460709571838379,\n    0.13549304008483887,\n    0.1373729705810547,\n    0.15271997451782227,\n    0.15703701972961426,\n    0.1608130931854248,\n    0.21175909042358398,\n    0.18168210983276367,\n    0.17579412460327148,\n    0.17799592018127441,\n    0.1961660385131836,\n    0.20264911651611328,\n    0.25041794776916504,\n    0.1808319091796875,\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Complex MACE Pyro Model\nDESCRIPTION: Implements a more complex Pyro model called MACE (Multi-Annotator Competence Estimation) with multiple plates and distributions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef mace(positions, annotations):\n    \"\"\"\n    This model corresponds to the plate diagram in Figure 3 of https://www.aclweb.org/anthology/Q18-1040.pdf.\n    \"\"\"\n    num_annotators = int(torch.max(positions)) + 1\n    num_classes = int(torch.max(annotations)) + 1\n    num_items, num_positions = annotations.shape\n\n    with pyro.plate(\"annotator\", num_annotators):\n        epsilon = pyro.sample(\"ε\", dist.Dirichlet(torch.full((num_classes,), 10.)))\n        theta = pyro.sample(\"θ\", dist.Beta(0.5, 0.5))\n\n    with pyro.plate(\"item\", num_items, dim=-2):\n        # NB: using constant logits for discrete uniform prior\n        # (NumPyro does not have DiscreteUniform distribution yet)\n        c = pyro.sample(\"c\", dist.Categorical(logits=torch.zeros(num_classes)))\n\n        with pyro.plate(\"position\", num_positions):\n            s = pyro.sample(\"s\", dist.Bernoulli(1 - theta[positions]))\n            probs = torch.where(\n                s[..., None] == 0, F.one_hot(c, num_classes).float(), epsilon[positions]\n            )\n            pyro.sample(\"y\", dist.Categorical(probs), obs=annotations)\n\n\npositions = torch.tensor([1, 1, 1, 2, 3, 4, 5])\n# fmt: off\nannotations = torch.tensor([\n    [1, 3, 1, 2, 2, 2, 1, 3, 2, 2, 4, 2, 1, 2, 1,\n     1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n     1, 3, 1, 2, 2, 4, 2, 2, 3, 1, 1, 1, 2, 1, 2],\n    [1, 3, 1, 2, 2, 2, 2, 3, 2, 3, 4, 2, 1, 2, 2,\n     1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 3, 1, 1, 1,\n     1, 3, 1, 2, 2, 3, 2, 3, 3, 1, 1, 2, 3, 2, 2],\n    [1, 3, 2, 2, 2, 2, 2, 3, 2, 2, 4, 2, 1, 2, 1,\n     1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2,\n     1, 3, 1, 2, 2, 3, 1, 2, 3, 1, 1, 1, 2, 1, 2],\n    [1, 4, 2, 3, 3, 3, 2, 3, 2, 2, 4, 3, 1, 3, 1,\n     2, 1, 1, 2, 1, 2, 2, 3, 2, 1, 1, 2, 1, 1, 1,\n     1, 3, 1, 2, 3, 4, 2, 3, 3, 1, 1, 2, 2, 1, 2],\n    [1, 3, 1, 1, 2, 3, 1, 4, 2, 2, 4, 3, 1, 2, 1,\n     1, 1, 1, 2, 3, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1,\n     1, 2, 1, 2, 2, 3, 2, 2, 4, 1, 1, 1, 2, 1, 2],\n    [1, 3, 2, 2, 2, 2, 1, 3, 2, 2, 4, 4, 1, 1, 1,\n     1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2,\n     1, 3, 1, 2, 3, 4, 3, 3, 3, 1, 1, 1, 2, 1, 2],\n    [1, 4, 2, 1, 2, 2, 1, 3, 3, 3, 4, 3, 1, 2, 1,\n     1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1,\n     1, 3, 1, 2, 2, 3, 2, 3, 2, 1, 1, 1, 2, 1, 2],\n]).T\n# fmt: on\n\n# we subtract 1 because the first index starts with 0 in Python\npositions -= 1\nannotations -= 1\n\nmace_graph = pyro.render_model(mace, model_args=(positions, annotations))\n```\n\n----------------------------------------\n\nTITLE: Displaying Latent Parameter Distributions using AutoGuide Quantiles\nDESCRIPTION: This snippet shows how to use the AutoDiagonalNormal guide's quantiles method to display the distribution of latent parameters in the Bayesian regression model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nguide.quantiles([0.25, 0.5, 0.75])\n```\n\n----------------------------------------\n\nTITLE: Interactive Visualization of Single Trajectory\nDESCRIPTION: Uses ipywidgets.interact to create an interactive plot of a single trajectory with sliders for adjusting model parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninteract(\n    plot_trajectory,\n    df=FloatSlider(value=1.0, min=0.01, max=10.0),\n    p_scale=FloatSlider(value=0.1, min=0.01, max=1.0),\n    m_scale=FloatSlider(value=1.0, min=0.01, max=10.0),\n);\n```\n\n----------------------------------------\n\nTITLE: Distribution Sample and Log Probability Shape Examples\nDESCRIPTION: Demonstrates the relationships between distribution.sample() and distribution.log_prob() shapes. Shows how sample shapes combine sample_shape, batch_shape, and event_shape.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx = d.sample()\nassert x.shape == d.batch_shape + d.event_shape\n```\n\nLANGUAGE: python\nCODE:\n```\nassert d.log_prob(x).shape == d.batch_shape\n```\n\nLANGUAGE: python\nCODE:\n```\nx2 = d.sample(sample_shape)\nassert x2.shape == sample_shape + batch_shape + event_shape\n```\n\n----------------------------------------\n\nTITLE: Converting Funsor to PyTorch Tensor with Dimension Mapping\nDESCRIPTION: This snippet demonstrates how to convert a Funsor tensor to a PyTorch tensor while specifying the mapping of named dimensions to tensor dimensions. It shows three cases of dimension mapping, including permuting input dimensions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# case 3: permuting the input dimensions\ntensor3 = funsor.to_data(ambiguous_funsor, name_to_dim={\"a\": -1, \"b\": -2})\nprint(\"Case 3: shape = {}\".format(tensor3.shape))\n```\n\n----------------------------------------\n\nTITLE: Neural MCMC Example Code Reference\nDESCRIPTION: Reference to the neutra.py example file demonstrating Neural MCMC with NeuTraReparam implementation. The code is accessible on GitHub and included via literalinclude directive.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/neutra.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../examples/neutra.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Dataset for Regression Analysis in Python\nDESCRIPTION: This code loads a dataset from a URL, selects relevant features, and preprocesses the data by log-transforming the GDP variable.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDATA_URL = \"https://github.com/pyro-ppl/datasets/blob/master/rugged_data.csv?raw=true\"\ndata = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\ndf = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\ndf = df[np.isfinite(df.rgdppc_2000)]\ndf[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n```\n\n----------------------------------------\n\nTITLE: Pyro Initialization and Configuration in Python\nDESCRIPTION: This snippet ensures the Pyro library is correctly initialized by asserting the version and disabling distribution validation. It also sets a random seed for reproducibility and checks if the environment is set for continuous integration (CI).\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nassert pyro.__version__.startswith('1.9.1')\npyro.distributions.enable_validation(False)\npyro.set_rng_seed(0)\n# Enable smoke test - run the notebook cells on CI.\nsmoke_test = 'CI' in os.environ\n```\n\n----------------------------------------\n\nTITLE: Displaying MNIST Samples\nDESCRIPTION: Load and display sample images from the MNIST dataset for visualization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae_flow_prior.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nx = [trainset[i][0] for i in range(16)]\nx = torch.cat(x, dim=-1)\n\nto_pil_image(x)\n```\n\n----------------------------------------\n\nTITLE: Initializing Pyro Funsor Backend Setup\nDESCRIPTION: Initial setup code that imports required dependencies and configures the Funsor backend for Pyro with specific random seed and torch settings.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import OrderedDict\nimport functools\n\nimport torch\nfrom torch.distributions import constraints\n\nimport funsor\n\nfrom pyro import set_rng_seed as pyro_set_rng_seed\nfrom pyro.ops.indexing import Vindex\nfrom pyro.poutine.messenger import Messenger\n\nfunsor.set_backend(\"torch\")\ntorch.set_default_dtype(torch.float32)\npyro_set_rng_seed(101)\n```\n\n----------------------------------------\n\nTITLE: Lower Estimate of Multiplicative Advantage in Python\nDESCRIPTION: Calculates the multiplicative advantage of Delta over Alpha lineage with a lower estimate than previous calculations. It uses the posterior median of the growth rate for each model region and lineage.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"Multiplicative advantage: {:.2f}\".format(\n      np.exp(guide.median()['rate'][1] - guide.median()['rate'][0])))\n```\n\n----------------------------------------\n\nTITLE: Baseball Statistics Analysis with MCMC Implementation in Python\nDESCRIPTION: This script demonstrates how to analyze baseball statistics using Markov Chain Monte Carlo methods in Pyro. The file includes code for loading baseball data and applying MCMC techniques for statistical analysis.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/baseball.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# File reference is provided but actual code is not shown in the input text.\n# Code can be found at: https://github.com/pyro-ppl/pyro/blob/dev/examples/baseball.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Scoped Mixture Model with Pyro Autoname\nDESCRIPTION: This example shows how to use pyro.contrib.autoname with scoping to implement a mixture model. It demonstrates the use of name scopes and plates to simplify the model definition.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/autoname_examples.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# scoping_mixture.py\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib import autoname\n\n# A Pyro model of a Gaussian mixture\ndef model(data):\n    p = pyro.sample(\"p\", dist.Beta(1, 1))\n    with pyro.plate(\"data\", len(data)):\n        z = pyro.sample(\"z\", dist.Bernoulli(p))\n        mu = 2 * z - 1\n        pyro.sample(\"x\", dist.Normal(mu, 1), obs=data)\n\n# The same model, with ``name`` blocks to reduce boilerplate\n@autoname.name_scope\ndef model(data):\n    p = dist.Beta(1, 1)\n    with autoname.plate(data):\n        with autoname.name_scope(\"cluster\"):\n            z = dist.Bernoulli(p)\n            mu = 2 * z - 1\n        dist.Normal(mu, 1).obs(data)\n\n```\n\n----------------------------------------\n\nTITLE: Defining State Priors for Price and Arousal\nDESCRIPTION: Implements prior probability distributions for price and emotional arousal states based on experimental data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-hyperbole.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nState = collections.namedtuple(\"State\", [\"price\", \"arousal\"])\n\ndef price_prior():\n    values = [50, 51, 500, 501, 1000, 1001, 5000, 5001, 10000, 10001]\n    probs = torch.tensor([0.4205, 0.3865, 0.0533, 0.0538, 0.0223, 0.0211, 0.0112, 0.0111, 0.0083, 0.0120])\n    ix = pyro.sample(\"price\", dist.Categorical(probs=probs))\n    return values[ix]\n\ndef arousal_prior(price):\n    probs = {\n        50: 0.3173,\n        51: 0.3173,\n        500: 0.7920,\n        501: 0.7920,\n        1000: 0.8933,\n        1001: 0.8933,\n        5000: 0.9524,\n        5001: 0.9524,\n        10000: 0.9864,\n        10001: 0.9864\n    }\n    return pyro.sample(\"arousal\", dist.Bernoulli(probs=probs[price])).item() == 1\n\ndef state_prior():\n    price = price_prior()\n    state = State(price=price, arousal=arousal_prior(price))\n    return state\n```\n\n----------------------------------------\n\nTITLE: Saving Pyro Model Visualization to File\nDESCRIPTION: Shows how to save the rendered Pyro model visualization to a PDF file.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngraph = pyro.render_model(model, model_args=(data,), filename=\"model.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Hidden Markov Model Implementation in Python using Pyro and Funsor\nDESCRIPTION: Example code showing how to implement Hidden Markov Models using Pyro's funsor contrib module and pyroapi. The code is referenced from the main Pyro-PPL repository at examples/contrib/funsor/hmm.py.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/hmm_funsor.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n../../examples/contrib/funsor/hmm.py\n```\n\n----------------------------------------\n\nTITLE: Working with Univariate Bernoulli Distribution\nDESCRIPTION: Demonstrates the simplest distribution shape using a univariate Bernoulli distribution. Shows the empty batch and event shapes and verifies the shapes of samples and log probabilities.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nd = Bernoulli(0.5)\nassert d.batch_shape == ()\nassert d.event_shape == ()\nx = d.sample()\nassert x.shape == ()\nassert d.log_prob(x).shape == ()\n```\n\n----------------------------------------\n\nTITLE: Setting a Small Learning Rate with Adam Optimizer\nDESCRIPTION: Sets up the Adam optimizer with a small learning rate of 0.001, which is recommended for initial SVI optimization to avoid numerical instability from high-variance ELBO gradients.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimizer = pyro.optim.Adam({\"lr\": 0.001})\n```\n\n----------------------------------------\n\nTITLE: Loading Electoral College Data - Python\nDESCRIPTION: Loads electoral college vote data from a pickle file and converts it to a PyTorch tensor.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport torch\nfrom urllib.request import urlopen\n\nelectoral_college_votes = pd.read_pickle(urlopen(BASE_URL + \"electoral_college_votes.pickle?raw=true\"))\nprint(electoral_college_votes.head())\nec_votes_tensor = torch.tensor(electoral_college_votes.values, dtype=torch.float).squeeze()\n```\n\n----------------------------------------\n\nTITLE: Managing Global Dimensions Manually\nDESCRIPTION: This snippet shows how to manually manage global dimensions, including creation and deallocation. It demonstrates creating multiple global dimensions and recycling a dimension after manual deallocation, highlighting the internal workings of pyro.contrib.funsor.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.contrib.funsor.handlers.runtime import _DIM_STACK, DimType\n\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    \n    funsor_plate1_ids = funsor.Tensor(torch.arange(10), OrderedDict(plate1=funsor.Bint[10]))\n    tensor_plate1_ids = pyro.to_data(funsor_plate1_ids, dim_type=DimType.GLOBAL)\n    print(\"New global dimension: \", funsor_plate1_ids.inputs, tensor_plate1_ids.shape)\n    \n    funsor_plate2_ids = funsor.Tensor(torch.arange(9), OrderedDict(plate2=funsor.Bint[9]))\n    tensor_plate2_ids = pyro.to_data(funsor_plate2_ids, dim_type=DimType.GLOBAL)\n    print(\"Another new global dimension: \", funsor_plate2_ids.inputs, tensor_plate2_ids.shape)\n    \n    del _DIM_STACK.global_frame[\"plate1\"]\n    \n    funsor_plate3_ids = funsor.Tensor(torch.arange(10), OrderedDict(plate3=funsor.Bint[10]))\n    tensor_plate3_ids = pyro.to_data(funsor_plate1_ids, dim_type=DimType.GLOBAL)\n    print(\"A third new global dimension after recycling: \", funsor_plate3_ids.inputs, tensor_plate3_ids.shape)\n```\n\n----------------------------------------\n\nTITLE: Rendering Pyro Model with Deterministic Variables\nDESCRIPTION: Demonstrates how to render a Pyro model including deterministic variables using render_deterministic=True.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.ones(10)\npyro.render_model(\n    model_deterministic,\n    model_args=(data,),\n    render_params=True,\n    render_distributions=True,\n    render_deterministic=True\n)\n```\n\n----------------------------------------\n\nTITLE: Creating SVI Object with Adam Optimizer in Pyro\nDESCRIPTION: Initializes a basic SVI (Stochastic Variational Inference) object using Adam optimizer and Trace_ELBO loss in Pyro.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/custom_objectives.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimizer = pyro.optim.Adam({\"lr\": 0.001, \"betas\": (0.90, 0.999)})\nsvi = pyro.infer.SVI(model, guide, optimizer, loss=pyro.infer.Trace_ELBO())\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Provenance Tracking in Pyro\nDESCRIPTION: Shows how Pyro tracks dependencies through PyTorch operations using provenance sets. Demonstrates tracking for both unary operations and operations with multiple inputs.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.ops.provenance import get_provenance, track_provenance\n\na = track_provenance(torch.randn(3), frozenset({\"a\"}))\nb = track_provenance(torch.randn(3), frozenset({\"b\"}))\nc = torch.randn(3)  # no provenance information\n\n# For a unary operation, the provenance of the output tensor\n# equals the provenace of the input tensor\nassert get_provenance(a.exp()) == frozenset({\"a\"})\n# In general, the provenance of the output tensors of any op\n# is the union of provenances of input tensors.\nassert get_provenance(a * (b + c)) == frozenset({\"a\", \"b\"})\n```\n\n----------------------------------------\n\nTITLE: Importing Mixed-Effect HMM Model Implementation from Pyro Examples\nDESCRIPTION: References the model.py file from Pyro's examples directory that contains the implementation of a hierarchical mixed-effect hidden Markov model. The implementation is part of the mixed_hmm example suite.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mixed_hmm.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Content referenced from ../../examples/mixed_hmm/model.py\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom MNIST Dataset Class for CVAE in Python\nDESCRIPTION: This code creates a custom PyTorch Dataset class that adapts the MNIST dataset for conditional variational autoencoder training. It wraps the original MNIST dataset and allows for applying transformations to prepare the data for CVAE training.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CVAEMNIST(Dataset):\n    def __init__(self, root, train=True, transform=None, download=False):\n        self.original = MNIST(root, train=train, download=download)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.original)\n\n    def __getitem__(self, item):\n        image, digit = self.original[item]\n        sample = {'original': image, 'digit': digit}\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n```\n\n----------------------------------------\n\nTITLE: Configuring Adam Optimizer with Higher Beta Parameters\nDESCRIPTION: Demonstrates how to set custom beta parameters for the Adam optimizer to increase momentum for stochastic models, using higher values like 0.95 instead of the default 0.90.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbetas = (0.95, 0.999)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Pyro Model Rendering\nDESCRIPTION: Imports necessary libraries and modules for Pyro model rendering, including torch, pyro, and their distributions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torch.nn.functional as F\nimport pyro\nimport pyro.distributions as dist\nimport pyro.distributions.constraints as constraints\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro JIT Trace Function in Python\nDESCRIPTION: This code snippet uses a Sphinx autodoc directive to generate documentation for the pyro.ops.jit.trace function, which is likely used for JIT compilation of Pyro models.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/primitives.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: pyro.ops.jit.trace\n```\n\n----------------------------------------\n\nTITLE: Election Winner Determination Function - Python\nDESCRIPTION: Implements logic to determine election winner based on electoral college votes using PyTorch operations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef election_winner(alpha):\n    dem_win_state = (alpha > 0.).float()\n    dem_electoral_college_votes = ec_votes_tensor * dem_win_state\n    w = (dem_electoral_college_votes.sum(-1) / ec_votes_tensor.sum(-1) > .5).float()\n    return w\n```\n\n----------------------------------------\n\nTITLE: Generating Regression Data with Sklearn\nDESCRIPTION: This code generates regression data using sklearn's make_regression function and transforms it into PyTorch tensors.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX, y = make_regression(n_features=1, bias=150., noise=5., random_state=108)\n\nX_ = torch.tensor(X, dtype=torch.float)\ny_ = torch.tensor((y**3)/100000. + 10., dtype=torch.float)\ny_.round_().clamp_(min=0);\n```\n\n----------------------------------------\n\nTITLE: Configuring Baseline Learning Parameters in Pyro\nDESCRIPTION: Demonstrates how to set different learning rates for baseline parameters versus other model parameters using per-parameter configuration.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iii.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef per_param_args(param_name):\n    if 'baseline' in param_name:\n        return {\"lr\": 0.010}\n    else:\n        return {\"lr\": 0.001}\n    \noptimizer = optim.Adam(per_param_args)\n```\n\n----------------------------------------\n\nTITLE: Initializing Experiment Parameters with PyTorch\nDESCRIPTION: Sets up initial experiment parameters including candidate designs and optimizer configuration using PyTorch and Pyro.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncandidate_designs = torch.arange(1, 15, dtype=torch.float).unsqueeze(-1)\npyro.clear_param_store()\nnum_steps, start_lr, end_lr = 1000, 0.1, 0.001\noptimizer = pyro.optim.ExponentialLR({'optimizer': torch.optim.Adam,\n                                      'optim_args': {'lr': start_lr},\n                                      'gamma': (end_lr / start_lr) ** (1 / num_steps)})\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Single-Cell qPCR Data for GPLVM in Python\nDESCRIPTION: This code loads the single-cell qPCR dataset from a CSV file, prints basic information about the data, and prepares it for use in the GPLVM model by converting it to a PyTorch tensor.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/gplvm.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nURL = \"https://raw.githubusercontent.com/sods/ods/master/datasets/guo_qpcr.csv\"\n\ndf = pd.read_csv(URL, index_col=0)\nprint(\"Data shape: {}\\n{}\\n\".format(df.shape, \"-\" * 21))\nprint(\"Data labels: {}\\n{}\\n\".format(df.index.unique().tolist(), \"-\" * 86))\nprint(\"Show a small subset of the data:\")\ndf.head()\n\ndata = torch.tensor(df.values, dtype=torch.get_default_dtype())\n# we need to transpose data to correct its shape\ny = data.t()\n```\n\n----------------------------------------\n\nTITLE: Implementing Capture-Recapture Models (CJS) in Python using Pyro\nDESCRIPTION: This code snippet defines and implements Cormack-Jolly-Seber (CJS) models for capture-recapture analysis. It includes functions for data generation, model definition, and inference using Pyro's probabilistic programming capabilities. The model estimates survival and recapture probabilities for a population over multiple time periods.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/capture_recapture.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport pyro\nimport pyro.distributions as dist\n\n# CJS model implementation\n\n# Data generation\n\n# Model definition\n\n# Inference procedures\n\n# Main execution\n```\n\n----------------------------------------\n\nTITLE: Mixing plates with dim argument for custom dimension allocation\nDESCRIPTION: This snippet demonstrates how to mix and match `pyro.plate`s by declaring multiple `plates` and using them as reusable context managers. A `dim` argument must be provided to explicitly specify the batch dimension. Within each `plate`'s context, the specified dimension is considered independent.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"x_axis = pyro.plate(\\\"x_axis\\\", 3, dim=-2)\\ny_axis = pyro.plate(\\\"y_axis\\\", 2, dim=-3)\\nwith x_axis:\\n    # within this context, batch dimension -2 is independent\\nwith y_axis:\\n    # within this context, batch dimension -3 is independent\\nwith x_axis, y_axis:\\n    # within this context, batch dimensions -3 and -2 are independent\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Dependencies and Loading Data\nDESCRIPTION: Sets up required imports, loads the multi-MNIST dataset, and configures basic parameters for the AIR implementation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pylab inline\nimport os\nfrom collections import namedtuple\nimport pyro\nimport pyro.optim as optim\nfrom pyro.infer import SVI, TraceGraph_ELBO\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nimport pyro.contrib.examples.multi_mnist as multi_mnist\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import relu, sigmoid, softplus, grid_sample, affine_grid\nimport numpy as np\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for MLE and MAP Estimation in Pyro\nDESCRIPTION: Imports necessary Python libraries including torch, pyro, and matplotlib for implementing MLE and MAP estimation and visualization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mle_map.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.distributions import constraints\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Interactive Visualization of Resampled Trajectories\nDESCRIPTION: Uses ipywidgets.interact to create an interactive plot of resampled trajectories with sliders for adjusting prior parameters.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninteract(\n    plot_resampled,\n    df0=FloatSlider(value=0.0, min=-5, max=5),\n    df1=FloatSlider(value=1.0, min=0.1, max=10),\n    p0=FloatSlider(value=0.0, min=-5, max=5),\n    p1=FloatSlider(value=1.0, min=0.1, max=10),\n    m0=FloatSlider(value=0.0, min=-5, max=5),\n    m1=FloatSlider(value=1.0, min=0.1, max=10),\n);\n```\n\n----------------------------------------\n\nTITLE: Defining Plot Function for Data Visualization\nDESCRIPTION: Creates a function to plot the cost and runtime data for different implementations, using matplotlib to generate two separate plots.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsizes = None\ncosts = None\ntimes1 = None\ntimes2 = None\n\n\ndef plot(title):\n    pyplot.figure(figsize=(8, 5)).patch.set_color(\"white\")\n    pyplot.title(\"{} data structures\".format(title))\n    for name, series in sorted(costs.items()):\n        pyplot.plot(sizes, series, label=name)\n    pyplot.xlabel(\"problem size\")\n    pyplot.xlim(0, max(sizes))\n    pyplot.legend(loc=\"best\")\n    pyplot.tight_layout()\n\n    pyplot.figure(figsize=(8, 5)).patch.set_color(\"white\")\n    pyplot.title(\"{} run time\".format(title))\n    pyplot.plot(sizes, times1, label=\"optim + compute\")\n    pyplot.plot(sizes, times2, label=\"compute\")\n    pyplot.xlim(0, max(sizes))\n    pyplot.xlabel(\"problem size\")\n    pyplot.ylabel(\"time (sec)\")\n    pyplot.legend(loc=\"best\")\n    pyplot.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Defining QUD Functions and Prior\nDESCRIPTION: Implements Question Under Discussion (QUD) functions that determine what aspects of state the speaker attends to.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-hyperbole.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nqud_fns = {\n    \"price\": lambda state: State(price=state.price, arousal=None),\n    \"arousal\": lambda state: State(price=None, arousal=state.arousal),\n    \"priceArousal\": lambda state: State(price=state.price, arousal=state.arousal),\n}\n\ndef qud_prior():\n    values = list(qud_fns.keys())\n    ix = pyro.sample(\"qud\", dist.Categorical(probs=torch.ones(len(values)) / len(values)))\n    return values[ix]\n```\n\n----------------------------------------\n\nTITLE: Loading Daily S&P 500 Data\nDESCRIPTION: This code snippet loads the S&P 500 daily closing prices and extracts the dates and closing prices into arrays using PyTorch tensors. It prepares the data for further analysis on log returns.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = load_snp500()\ndates = df.Date.to_numpy()\nx = torch.tensor(df[\"Close\"]).float()\nx.shape\n```\n\n----------------------------------------\n\nTITLE: Plotting S&P 500 Closing Prices\nDESCRIPTION: This snippet generates a plot of the S&P 500 index closing prices over the available time period. The y-axis is set to logarithmic scale to better visualize the price trends over time.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npyplot.figure(figsize=(9, 3))\npyplot.plot(x)\npyplot.yscale('log')\npyplot.ylabel(\"index\")\npyplot.xlabel(\"trading day\")\npyplot.title(\"S&P 500 from {} to {}\".format(dates[0], dates[-1]));\n```\n\n----------------------------------------\n\nTITLE: Implementing Regional Epidemiological Models in Python using Pyro\nDESCRIPTION: This code snippet contains the entire implementation of regional epidemiological models using Pyro. It includes imports, model definitions, data processing, and inference methods for epidemiological analysis across different regions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_regional.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport os\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.distributions import biject_to\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.epidemiology import RegionalSIRModel\nfrom pyro.contrib.epidemiology.models import fit_to_data\nfrom pyro.infer import MCMC, NUTS\n\n\nDURATION = 15  # in days\n\n\ndef generate_data(true_model, population):\n    R0 = 1.5 + population / population.sum() * 1.5\n    with pyro.poutine.condition(data={\"R0\": R0}):\n        return true_model.generate({\"R0\": R0.shape}, duration=DURATION)\n\n\ndef infer(data, *, num_samples, warmup_steps):\n    data = {name: torch.as_tensor(value) for name, value in data.items()}\n    model = RegionalSIRModel(population=data[\"population\"], coupling=data[\"coupling\"])\n    model.set_data(data[\"active\"], data[\"recovered\"])\n\n    num_time_steps = len(data[\"active\"])\n\n    def initialize(seed):\n        pyro.set_rng_seed(seed)\n        proposed_R0 = dist.Uniform(0, 10).sample(data[\"population\"].shape)\n        proposed_rho = dist.Uniform(0, 1).sample(data[\"population\"].shape)\n        return {\n            \"R0\": proposed_R0,\n            \"rho\": proposed_rho,\n            \"gamma\": dist.Uniform(0, 1).sample(),\n        }\n\n    kernel = NUTS(\n        model,\n        full_mass=[\"R0\", \"gamma\"],\n        init_strategy=initialize,\n        max_tree_depth=5,\n        jit_compile=True,\n        ignore_jit_warnings=True,\n    )\n    mcmc = MCMC(kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n    mcmc.run()\n    samples = mcmc.get_samples()\n\n    return {\n        name: value.detach().cpu().numpy()\n        for name, value in samples.items()\n        if not value.shape[-1:]  # exclude time-dependent variables\n    }\n\n\ndef evaluate(pred_samples, true_samples):\n    for name, pred in pred_samples.items():\n        if name in true_samples:\n            true = true_samples[name]\n            error = pred - true\n            if pred.shape == ():\n                print(f\"{name}: true = {true:.3g}, pred = {pred.mean():.3g} +- {pred.std():.3g}\")\n            else:\n                print(\n                    f\"{name}: Root-mean-square error = {error.pow(2).mean().sqrt().item():.3g}, \"\n                    f\"Relative error = {error.pow(2).sum().sqrt() / true.pow(2).sum().sqrt():.3g}\"\n                )\n\n\ndef plot_samples(samples, true_model, data, filename=None):\n    fig, axes = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\n    time = torch.arange(len(data[\"active\"]) + 7)\n    extended_data = {\n        \"population\": data[\"population\"],\n        \"coupling\": data[\"coupling\"],\n    }\n\n    # Resample from the parameteric posterior to construct a nonparametric\n    # posterior over discrete time series.\n    num_samples = 100\n    time_series = []\n    for i in range(num_samples):\n        params = {name: torch.as_tensor(value[i % len(value)])\n                  for name, value in samples.items()}\n        with pyro.poutine.condition(data=params):\n            time_series.append(true_model._generate(extended_data, duration=len(time)))\n\n    active = torch.stack([d[\"active\"] for d in time_series]).numpy()\n    axes[0].set_title(\"Active patients\")\n    axes[0].plot(time, active[0], \"b\", alpha=0.1)\n    axes[0].plot(time, active.mean(0), \"b-\")\n    axes[0].plot(time, active.T, \"b\", alpha=0.1)\n    axes[0].plot(time[:len(data[\"active\"])], data[\"active\"], \"k.\")\n    axes[0].set_yscale(\"log\")\n\n    recovered = torch.stack([d[\"recovered\"] for d in time_series]).numpy()\n    axes[1].set_title(\"Recovered patients\")\n    axes[1].plot(time, recovered[0], \"b\", alpha=0.1)\n    axes[1].plot(time, recovered.mean(0), \"b-\")\n    axes[1].plot(time, recovered.T, \"b\", alpha=0.1)\n    axes[1].plot(time[:len(data[\"recovered\"])], data[\"recovered\"], \"k.\")\n    axes[1].set_yscale(\"log\")\n\n    R0 = torch.stack([d[\"R0\"] for d in time_series]).numpy()\n    axes[2].set_title(\"Basic reproduction number R0\")\n    axes[2].plot(time, R0[0], \"b\", alpha=0.1)\n    axes[2].plot(time, R0.mean(0), \"b-\")\n    axes[2].plot(time, R0.T, \"b\", alpha=0.1)\n    axes[2].set_ylim(0, 5)\n    axes[2].axhline(1, color=\"gray\", zorder=-1)\n\n    for ax in axes:\n        ax.set_xlim(0, len(time) - 1)\n\n    fig.tight_layout()\n    fig.savefig(filename)\n    plt.close(fig)\n\n\ndef main(args):\n    pyro.set_rng_seed(1)\n    filename = \"regional_\"\n\n    population = torch.tensor([8e6, 4e6, 3e6, 1e6, 3e5])\n    coupling = population / population.sum()\n\n    # Generate data.\n    true_model = RegionalSIRModel(population, coupling)\n    data = generate_data(true_model, population)\n    data = {\n        \"active\": data[\"active\"],\n        \"recovered\": data[\"recovered\"],\n        \"population\": data[\"population\"],\n        \"coupling\": data[\"coupling\"],\n    }\n\n    # Infer posterior.\n    pred_samples = infer(\n        data, num_samples=args.num_samples, warmup_steps=args.warmup_steps\n    )\n\n    # Evaluate fit.\n    true_samples = {\"R0\": data[\"R0\"]}\n    evaluate(pred_samples, true_samples)\n\n    # Plot forecasts.\n    filename += \".png\"\n    plot_samples(pred_samples, true_model, data, filename=filename)\n\n\nif __name__ == \"__main__\":\n    assert pyro.__version__.startswith('1.8.6')\n    parser = argparse.ArgumentParser(description=\"Regional epidemiology\")\n    parser.add_argument(\"-n\", \"--num-samples\", default=200, type=int)\n    parser.add_argument(\"-w\", \"--warmup-steps\", default=100, type=int)\n    args = parser.parse_args()\n    main(args)\n```\n\n----------------------------------------\n\nTITLE: Initializing VAE Instance in Python\nDESCRIPTION: Creates an instance of the VAE (Variational Autoencoder) class.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nvae = VAE()\n```\n\n----------------------------------------\n\nTITLE: Initializing Dependencies for Funsor Backend\nDESCRIPTION: Sets up the required imports and configurations for using the Funsor backend with Pyro, including setting random seed and torch backend.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import OrderedDict\n\nimport torch\nimport funsor\nfrom pyro import set_rng_seed as pyro_set_rng_seed\n\nfunsor.set_backend(\"torch\")\ntorch.set_default_dtype(torch.float32)\npyro_set_rng_seed(101)\n```\n\n----------------------------------------\n\nTITLE: Preparing Count Data\nDESCRIPTION: Prepares count data from the predictive distribution for visualization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncounts_df = prepare_counts_df(predictive_svi)\n```\n\n----------------------------------------\n\nTITLE: Plotting Comparison of Skew Parameters in Pyro Models\nDESCRIPTION: Creates a plot to compare the skew parameters from both models (with and without reparameterization) over SVI steps.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nplot_comparison(skew_with_log_prob, skew, 'SVI step', 'Skew')\n```\n\n----------------------------------------\n\nTITLE: Automated Dimension Mapping with OrderedDict\nDESCRIPTION: This code snippet showcases how to automate the process of updating dimension mappings when converting between Funsor and PyTorch tensors. It uses an OrderedDict to maintain the mapping and demonstrates the conversion for multiple tensors with different dimensions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nname_to_dim = OrderedDict()\n\nfunsor_x = funsor.Tensor(torch.ones((2,)), OrderedDict(x=funsor.Bint[2]), 'real')\nname_to_dim.update({\"x\": -1})\ntensor_x = funsor.to_data(funsor_x, name_to_dim=name_to_dim)\nprint(name_to_dim, funsor_x.inputs, tensor_x.shape)\n\nfunsor_y = funsor.Tensor(torch.ones((3, 2)), OrderedDict(y=funsor.Bint[3], x=funsor.Bint[2]), 'real')\nname_to_dim.update({\"y\": -2})\ntensor_y = funsor.to_data(funsor_y, name_to_dim=name_to_dim)\nprint(name_to_dim, funsor_y.inputs, tensor_y.shape)\n\nfunsor_z = funsor.Tensor(torch.ones((2, 3)), OrderedDict(z=funsor.Bint[2], y=funsor.Bint[3]), 'real')\nname_to_dim.update({\"z\": -3})\ntensor_z = funsor.to_data(funsor_z, name_to_dim=name_to_dim)\nprint(name_to_dim, funsor_z.inputs, tensor_z.shape)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset\nDESCRIPTION: Initialize the MNIST dataset loader with batch processing configuration for training.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae_flow_prior.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrainset = MNIST(root='', download=True, train=True, transform=to_tensor)\ntrainloader = data.DataLoader(trainset, batch_size=256, shuffle=True)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Bayesian Regression in Python\nDESCRIPTION: This code snippet imports necessary Python libraries for data manipulation, visualization, and machine learning using PyTorch and Pyro.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom functools import partial\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pyro\nimport pyro.distributions as dist\n\n# for CI testing\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\npyro.set_rng_seed(1)\n\n\n# Set matplotlib settings\n%matplotlib inline\nplt.style.use('default')\n```\n\n----------------------------------------\n\nTITLE: Importing Pyro API Components\nDESCRIPTION: Imports necessary Pyro API components and distributions for model implementation using the standard Pyro syntax.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_ii.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyro.contrib.funsor\nimport pyroapi\nfrom pyroapi import infer, handlers, ops, optim, pyro\nfrom pyroapi import distributions as dist\n```\n\n----------------------------------------\n\nTITLE: Setting up Adam Optimizer for VAE in Python\nDESCRIPTION: Initializes an Adam optimizer with a specified learning rate for training the VAE.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\noptimizer = Adam({\"lr\": 1.0e-3})\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Environment for Pyro\nDESCRIPTION: This snippet imports necessary libraries, including Pyro, and sets up the matplotlib environment for inline plotting.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_regression\nimport pyro.distributions as dist\nfrom pyro.infer import MCMC, NUTS, Predictive\nfrom pyro.infer.mcmc.util import summary\nfrom pyro.distributions import constraints\nimport pyro\nimport torch\n\npyro.set_rng_seed(101)\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n```\n\n----------------------------------------\n\nTITLE: Importing Pyro Distributions Testing Module in ReStructuredText\nDESCRIPTION: RST directive that imports and documents the pyro.distributions.testing.gof module with its members organized by source order.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/testing.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pyro.distributions.testing.gof\n   :members:\n   :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Implementing Stack Application Logic\nDESCRIPTION: Function that applies message processing across the messenger stack, handling both forward and backward passes.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef apply_stack(msg):  # simplified\n    for handler in reversed(_PYRO_STACK):\n        handler._process_message(msg)\n    ...\n    default_process_message(msg)\n    ...\n    for handler in _PYRO_STACK:\n        handler._postprocess_message(msg) \n    ...\n    return msg\n```\n\n----------------------------------------\n\nTITLE: Pyro Sample Example\nDESCRIPTION: Basic example showing how to use pyro.sample with a named distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/intro_part_i.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nx = pyro.sample(\"my_sample\", pyro.distributions.Normal(loc, scale))\nprint(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Guide with Unconstrained Parameters\nDESCRIPTION: Demonstrates a problematic guide that doesn't constrain the scale parameter of a Normal distribution, which could lead to invalid negative values and NaNs.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef bad_guide():\n    scale = pyro.param(\"scale\", torch.tensor(1.0))\n    pyro.sample(\"x\", dist.Normal(0.0, scale))\n```\n\n----------------------------------------\n\nTITLE: Running SVI with Horovod Distributed Training in Python\nDESCRIPTION: Complete implementation of distributed training in Pyro using Horovod for parallel processing. The example demonstrates setting up a simple Bayesian neural network model, using the HorovodOptimizer wrapper, and training across multiple nodes with synchronized gradient updates.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_horovod.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport horovod.torch as hvd\nfrom torch.utils.data import DataLoader\n\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.optim.horovod import HorovodOptimizer\nfrom pyro import poutine\nfrom pyro.infer import SVI, Trace_ELBO\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(28 * 28, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = x.reshape(-1, 28 * 28)\n        hidden = self.relu(self.linear(x))\n        return nn.Softmax(dim=-1)(hidden)\n\n\nclass Guide(nn.Module):\n    def __init__(self):\n        super(Guide, self).__init__()\n        self.linear = nn.Linear(28 * 28, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = x.reshape(-1, 28 * 28)\n        hidden = self.relu(self.linear(x))\n        return nn.LogSoftmax(dim=-1)(hidden)\n\n\ndef model(x, y=None):\n    with pyro.plate(\"data\", len(x)), poutine.scale(scale=1.0 / len(dataloader)):\n        logits = pyro.deterministic(\"logits\", net(x))\n        with pyro.plate(\"output\", 10):\n            return pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=y)\n\n\ndef guide(x, y=None):\n    with pyro.plate(\"data\", len(x)), poutine.scale(scale=1.0 / len(dataloader)):\n        return pyro.deterministic(\"logits\", guide_net(x))\n\n\ndef main(args):\n    global dataloader, net, guide_net\n\n    # Initialize horovod\n    hvd.init()\n\n    # Set cuda device if using GPU\n    if args.cuda:\n        torch.cuda.set_device(hvd.local_rank())\n\n    pyro.clear_param_store()\n    net = Model()\n    guide_net = Guide()\n\n    # Move model to GPU if needed\n    if args.cuda:\n        net.cuda()\n        guide_net.cuda()\n\n    # Configure the optimizer\n    base_optim = torch.optim.Adam\n    optimizer = HorovodOptimizer(base_optim, {\"lr\": args.learning_rate})\n\n    # Set up SVI\n    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n\n    # Load MNIST dataset\n    train_set = MNIST(\n        os.path.expanduser(\"~/.pytorch/datasets/mnist\"),\n        train=True,\n        download=True,\n        transform=ToTensor(),\n    )\n\n    # Use DistributedSampler to partition data among workers\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_set, num_replicas=hvd.size(), rank=hvd.rank()\n    )\n\n    dataloader = DataLoader(\n        train_set, batch_size=args.batch_size, sampler=train_sampler\n    )\n\n    # Main training loop\n    for epoch in range(args.epochs):\n        # Set the dataloader epoch for correct shuffling\n        train_sampler.set_epoch(epoch)\n        running_loss = 0.0\n\n        # Process mini-batches\n        for i, (x, y) in enumerate(dataloader):\n            if args.cuda:\n                x, y = x.cuda(), y.cuda()\n\n            # Do gradient update\n            loss = svi.step(x, y)\n            running_loss += loss / len(dataloader)\n\n        if hvd.rank() == 0:\n            print(f\"[Epoch {epoch + 1}] loss: {running_loss:.3f}\")\n\n    # Test accuracy on rank 0\n    if hvd.rank() == 0:\n        test_set = MNIST(\n            os.path.expanduser(\"~/.pytorch/datasets/mnist\"),\n            train=False,\n            download=True,\n            transform=ToTensor(),\n        )\n        test_loader = DataLoader(test_set, batch_size=1000)\n\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for x, y in test_loader:\n                if args.cuda:\n                    x, y = x.cuda(), y.cuda()\n                logits = net(x)\n                _, predicted = torch.max(logits, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n            print(f\"Test accuracy: {100 * correct / total:.2f}%\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Horovod MNIST Example\")\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=128,\n        help=\"input batch size for training (default: 128)\",\n    )\n    parser.add_argument(\n        \"--epochs\", type=int, default=5, help=\"number of epochs to train (default: 5)\"\n    )\n    parser.add_argument(\n        \"--learning-rate\",\n        type=float,\n        default=0.001,\n        help=\"learning rate (default: 0.001)\",\n    )\n    parser.add_argument(\"--no-cuda\", action=\"store_true\", default=False, help=\"disables CUDA training\")\n    args = parser.parse_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    main(args)\n```\n\n----------------------------------------\n\nTITLE: Defining Polling Strategies for US Presidential Election in Python\nDESCRIPTION: This code sets up four different polling strategies: focusing on Florida, focusing on DC, uniform polling across states, and a strategy that prioritizes swing states. It uses PyTorch tensors to represent poll allocations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import OrderedDict\n\npoll_in_florida = torch.zeros(51)\npoll_in_florida[9] = 1000\n\npoll_in_dc = torch.zeros(51)\npoll_in_dc[8] = 1000\n\nuniform_poll = (1000 // 51) * torch.ones(51)\n\n# The swing score measures how close the state is to 50/50\nswing_score = 1. / (.5 - torch.tensor(prior_prob_dem.sort_values(\"State\").values).squeeze()).abs()\nswing_poll = 1000 * swing_score / swing_score.sum()\nswing_poll = swing_poll.round()\n\npoll_strategies = OrderedDict([(\"Florida\", poll_in_florida),\n                               (\"DC\", poll_in_dc),\n                               (\"Uniform\", uniform_poll),\n                               (\"Swing\", swing_poll)])\n```\n\n----------------------------------------\n\nTITLE: Handling Tensor Dimension Ambiguity\nDESCRIPTION: Shows different ways to convert tensors to Funsor objects by specifying batch dimensions and dimension names.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nambiguous_tensor = torch.zeros((3, 1, 2))\nprint(\"Ambiguous tensor: shape = {}\".format(ambiguous_tensor.shape))\n\n# case 1: treat all dimensions as output/event dimensions\nfunsor1 = funsor.to_funsor(ambiguous_tensor, output=funsor.Reals[3, 1, 2])\nprint(\"Case 1: inputs = {}, output = {}\".format(funsor1.inputs, funsor1.output))\n\n# case 2: treat the leftmost dimension as a batch dimension\nfunsor2 = funsor.to_funsor(ambiguous_tensor, output=funsor.Reals[1, 2], dim_to_name={-1: \"a\"})\nprint(\"Case 2: inputs = {}, output = {}\".format(funsor2.inputs, funsor2.output))\n\n# case 3: treat the leftmost 2 dimensions as batch dimensions\nfunsor3 = funsor.to_funsor(ambiguous_tensor, output=funsor.Reals[2], dim_to_name={-1: \"b\", -2: \"a\"})\nprint(\"Case 3: inputs = {}, output = {}\".format(funsor3.inputs, funsor3.output))\n\n# case 4: treat all dimensions as batch dimensions\nfunsor4 = funsor.to_funsor(ambiguous_tensor, output=funsor.Real, dim_to_name={-1: \"c\", -2: \"b\", -3: \"a\"})\nprint(\"Case 4: inputs = {}, output = {}\".format(funsor4.inputs, funsor4.output))\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro Development Dependencies\nDESCRIPTION: Commands to install all development dependencies for the Pyro project. The first command uses the Makefile, while the second shows the explicit pip command.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmake install\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install -e .[dev]\n```\n\n----------------------------------------\n\nTITLE: Setting up Pyro and Dependencies in Python\nDESCRIPTION: This snippet imports necessary libraries and sets up the environment for creating a VAE in Pyro. It includes importing PyTorch, Pyro, and related utilities for handling the MNIST dataset. No specific parameters are required, but the Pyro library version 1.9.1 or later is required.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/vae.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nimport numpy as np\nimport torch\nfrom pyro.contrib.examples.util import MNIST\nimport torch.nn as nn\nimport torchvision.transforms as transforms\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.contrib.examples.util  # patches torchvision\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\n```\n\n----------------------------------------\n\nTITLE: Initializing Pyro Environment and Dependencies\nDESCRIPTION: Sets up the required imports and environment for Kalman filter implementation in Pyro. Includes necessary imports from torch, pyro, and tracking-specific modules.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ekf.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport math\n\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.optim import Adam\nfrom pyro.infer import SVI, Trace_ELBO, config_enumerate\nfrom pyro.contrib.tracking.extended_kalman_filter import EKFState\nfrom pyro.contrib.tracking.distributions import EKFDistribution\nfrom pyro.contrib.tracking.dynamic_models import NcvContinuous\nfrom pyro.contrib.tracking.measurements import PositionMeasurement\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\n```\n\n----------------------------------------\n\nTITLE: Rendering MACE Model with Default Layout\nDESCRIPTION: Renders the MACE model using the default layout in pyro.render_model().\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# default layout\nmace_graph\n```\n\n----------------------------------------\n\nTITLE: Visualizing EIG Estimates with Matplotlib\nDESCRIPTION: Creates a visualization of Expected Information Gain (EIG) estimates using matplotlib.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/working_memory.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10,5))\nmatplotlib.rcParams.update({'font.size': 22})\nplt.plot(candidate_designs.numpy(), eig.detach().numpy(), marker='o', linewidth=2)\nplt.xlabel(\"$l$\")\nplt.ylabel(\"EIG($l$)\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with CSIS\nDESCRIPTION: Uses the trained guide to perform inference on new observations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/csis.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nposterior = csis.run(prior_mean,\n                     observations={\"x1\": torch.tensor(8.),\n                                   \"x2\": torch.tensor(9.)})\nmarginal = pyro.infer.EmpiricalMarginal(posterior, \"z\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Histogram of Daily Log Returns\nDESCRIPTION: This code creates a histogram displaying the empirical distribution of the calculated daily log returns, utilizing a logarithmic scale for the y-axis to visualize frequency distribution effectively.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npyplot.figure(figsize=(9, 3))\npyplot.hist(r.numpy(), bins=200)\npyplot.yscale('log')\npyplot.ylabel(\"count\")\npyplot.xlabel(\"daily log returns\")\npyplot.title(\"Empirical distribution.  mean={:0.3g}, std={:0.3g}\".format(r.mean(), r.std()));\n```\n\n----------------------------------------\n\nTITLE: Examining Enumerated Trace Shapes in Pyro\nDESCRIPTION: This code snippet shows how to create an enumerated trace of a Pyro model and examine the shapes of its components using the Trace.format_shapes() method.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tensor_shapes.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntrace = poutine.trace(poutine.enum(model3, first_available_dim=-3)).get_trace()\ntrace.compute_log_prob()  # optional, but allows printing of log_prob shapes\nprint(trace.format_shapes())\n```\n\n----------------------------------------\n\nTITLE: Running Jupyter Notebook with GPU-enabled Pyro Docker Image\nDESCRIPTION: Command to start a Jupyter notebook server using a previously built GPU-enabled Pyro Docker image. The notebook can be accessed through a browser using the link provided in the terminal output.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docker/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmake notebook-gpu img=pyro-gpu-dev-3.6\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamics Function\nDESCRIPTION: Defines the dynamics function that generates sinusoidal patterns over time frames.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tracking_1d.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_dynamics(num_frames):\n    time = torch.arange(float(num_frames)) / 4\n    return torch.stack([time.cos(), time.sin()], -1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Utterance Functions\nDESCRIPTION: Defines functions for utterance prior probability and meaning interpretation based on exact number matching.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-hyperbole.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef utterance_prior():\n    utterances = [50, 51, 500, 501, 1000, 1001, 5000, 5001, 10000, 10001]\n    ix = pyro.sample(\"utterance\", dist.Categorical(probs=torch.ones(len(utterances)) / len(utterances)))\n    return utterances[ix]\n\ndef meaning(utterance, price):\n    return utterance == price\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Pyro JIT Compilation\nDESCRIPTION: Imports necessary libraries and modules for using PyTorch JIT compiler with Pyro, including PyTorch, Pyro, and various Pyro inference and distribution modules.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom torch.distributions import constraints\nfrom pyro import poutine\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.infer import Trace_ELBO, JitTrace_ELBO, TraceEnum_ELBO, JitTraceEnum_ELBO, SVI\nfrom pyro.infer.mcmc import MCMC, NUTS\nfrom pyro.infer.autoguide import AutoDiagonalNormal\nfrom pyro.optim import Adam\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\n```\n\n----------------------------------------\n\nTITLE: Handling Tree-Structured Data with Pyro Autoname\nDESCRIPTION: This snippet demonstrates how to use pyro.contrib.autoname to handle tree-structured data in Pyro models. It shows the implementation of a hierarchical model using name scopes and plates.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/autoname_examples.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# tree_data.py\nimport collections\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib import autoname\n\nPerson = collections.namedtuple(\"Person\", [\"name\", \"age\", \"height\", \"weight\", \"friends\"])\n\n# A standard Pyro model for tree-structured data\ndef model(data):\n    height_loc = pyro.sample(\"height_loc\", dist.Normal(170., 5.))\n    height_scale = pyro.sample(\"height_scale\", dist.LogNormal(0., 1.))\n    weight_loc = pyro.sample(\"weight_loc\", dist.Normal(60., 5.))\n    weight_scale = pyro.sample(\"weight_scale\", dist.LogNormal(0., 1.))\n\n    def visit(person, prefix):\n        # model friend relationships\n        if person.friends:\n            friend_probs = pyro.sample(prefix + \"friend_probs\",\n                                       dist.Dirichlet(torch.ones(len(person.friends))))\n            for j, friend in enumerate(person.friends):\n                pyro.sample(prefix + \"friend_{}\".format(j),\n                            dist.Bernoulli(friend_probs[j]),\n                            obs=1)\n\n        pyro.sample(prefix + \"age\", dist.Normal(50., 20.), obs=person.age)\n        pyro.sample(prefix + \"height\", dist.Normal(height_loc, height_scale),\n                    obs=person.height)\n        pyro.sample(prefix + \"weight\", dist.Normal(weight_loc, weight_scale),\n                    obs=person.weight)\n\n        for j, friend in enumerate(person.friends):\n            # Visit each friend, using an automatic prefix.\n            visit(friend, prefix + \"friend_{}_\".format(j))\n\n    visit(data, prefix=\"\")\n\n# A Pyro model for tree-structured data, using pyro.contrib.autoname\n@autoname.name_scope\ndef model(data):\n    height = dist.Normal(170., 5.)\n    height_scale = dist.LogNormal(0., 1.)\n    weight = dist.Normal(60., 5.)\n    weight_scale = dist.LogNormal(0., 1.)\n\n    def visit(person):\n        # model friend relationships\n        if person.friends:\n            with autoname.name_scope(\"friends\"):\n                friend_probs = dist.Dirichlet(torch.ones(len(person.friends)))\n                for j, friend in enumerate(person.friends):\n                    dist.Bernoulli(friend_probs[j]).obs(1)\n\n        dist.Normal(50., 20.).obs(person.age)\n        dist.Normal(height, height_scale).obs(person.height)\n        dist.Normal(weight, weight_scale).obs(person.weight)\n\n        for friend in person.friends:\n            visit(friend)\n\n    visit(data)\n\n```\n\n----------------------------------------\n\nTITLE: Running SVI with TraceEnum_ELBO on Time Series Model\nDESCRIPTION: Demonstrates running SVI with TraceEnum_ELBO on the time series model with varying sequence lengths, without JIT compilation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/jit.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\npyro.clear_param_store()\nelbo = TraceEnum_ELBO(max_plate_nesting=1)\nsvi = SVI(model, guide, Adam({'lr': 0.01}), elbo)\nfor i in range(1 if smoke_test else 10):\n    for sequence in sequences:\n        svi.step(sequence,                                            # tensor args\n                 num_sequences=len(sequences), length=len(sequence))  # non-tensor args\n```\n\n----------------------------------------\n\nTITLE: Examining Guide Trace for Automatic Naming\nDESCRIPTION: Shows how the automatic naming mechanism works in guides by tracing a guide execution to see the named parameters and their shapes.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nwith poutine.trace() as tr:\n    guide(x)\nfor site in tr.trace.nodes.values():\n    print(site[\"type\"], site[\"name\"], site[\"value\"].shape)\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying MNIST Images\nDESCRIPTION: Loads the multi-MNIST dataset and implements a helper function to display sample images using matplotlib.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninpath = '../../examples/air/.data'\nX_np, _ = multi_mnist.load(inpath)\nX_np = X_np.astype(np.float32)\nX_np /= 255.0\nmnist = torch.from_numpy(X_np)\ndef show_images(imgs):\n    figure(figsize=(8, 2))\n    for i, img in enumerate(imgs):\n        subplot(1, len(imgs), i + 1)\n        axis('off')\n        imshow(img.data.numpy(), cmap='gray')\nshow_images(mnist[9:14])\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro Documentation Dependencies\nDESCRIPTION: Command to install the required Python packages for building Pyro documentation using pip. Note that graphviz needs to be installed separately.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Visualizing Boosting BBVI Approximation Results in Python\nDESCRIPTION: This snippet plots the resulting approximation from the boosting black box Variational Inference algorithm. It visualizes the individual components and the total approximation of the posterior distribution.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/boosting_bbvi.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Plot the resulting approximation\nX = np.arange(-10, 10, 0.1)\npyplot.figure(figsize=(10, 4), dpi=100).set_facecolor('white')\ntotal_approximation = np.zeros(X.shape)\nfor i in range(1, n_iterations + 1):\n    Y = weights[i].item() * scipy.stats.norm.pdf((X - locs[i]) / scales[i])\n    pyplot.plot(X, Y)\n    total_approximation += Y\npyplot.plot(X, total_approximation)\npyplot.plot(data.data.numpy(), np.zeros(len(data)), 'k*')\npyplot.title('Approximation of posterior over z')\npyplot.ylabel('probability density')\npyplot.show()\n```\n\n----------------------------------------\n\nTITLE: Rendering Pyro Model with Parameters\nDESCRIPTION: Demonstrates how to render a Pyro model including its parameters using render_params=True.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.ones(10)\npyro.render_model(model, model_args=(data,), render_params=True)\n```\n\n----------------------------------------\n\nTITLE: Meaning Function Implementation\nDESCRIPTION: Defines the meaning function that maps utterances to truth conditions for different states.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmeanings = {\n    \"none\": lambda N: N==0,\n    \"some\": lambda N: N>0,\n    \"all\": lambda N: N==total_number,\n}\n\ndef meaning(utterance, state):\n    return meanings[utterance](state)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Environment for Epidemiological Modeling in Python\nDESCRIPTION: This code snippet imports necessary libraries, sets up matplotlib for inline plotting, checks the Pyro version, sets the default tensor type to double precision, and defines a smoke test flag for CI environments.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/epi_intro.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.epidemiology import CompartmentalModel, binomial_dist, infection_dist\n\n%matplotlib inline\nassert pyro.__version__.startswith('1.9.1')\ntorch.set_default_dtype(torch.double)  # Required for MCMC inference.\nsmoke_test = ('CI' in os.environ)\n```\n\n----------------------------------------\n\nTITLE: Converting Funsor Objects to Tensors\nDESCRIPTION: Demonstrates conversion of Funsor objects back to tensors with explicit dimension mapping.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nambiguous_funsor = funsor.Tensor(torch.zeros((3, 2)), OrderedDict(a=funsor.Bint[3], b=funsor.Bint[2]), 'real')\nprint(\"Ambiguous funsor: inputs = {}, shape = {}\".format(ambiguous_funsor.inputs, ambiguous_funsor.output))\n\n# case 1: the simplest version\ntensor1 = funsor.to_data(ambiguous_funsor, name_to_dim={\"a\": -2, \"b\": -1})\nprint(\"Case 1: shape = {}\".format(tensor1.shape))\n\n# case 2: an empty dimension between a and b\ntensor2 = funsor.to_data(ambiguous_funsor, name_to_dim={\"a\": -3, \"b\": -1})\nprint(\"Case 2: shape = {}\".format(tensor2.shape))\n```\n\n----------------------------------------\n\nTITLE: Implementing Sparse Bayesian Linear Regression in Pyro\nDESCRIPTION: This code demonstrates a complete implementation of sparse Bayesian linear regression using Pyro. It includes model definition, inference setup, and visualization of results with a focus on promoting sparsity in regression coefficients.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/sparse_regression.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"\nAn example of sparse linear regression using the approach described in\n`Bayesian Smoothing Using Fourier Basis Functions`_ by Hans F. Skaug and\nHans J. Skaug. Briefly, the idea of this approach is reparameterize\nthe regression problem in terms of a \"Discrete Cosine Transform\" of\nthe regression coefficients. This DCT is a generalization of the usual\nFourier transform that makes use of cosine basis functions.\n\nFor more details, see section 4 of:\nIsar, C., Corizzo, R., & Kiringa, I. (2020).\nBayesian Smoothing Using Fourier Basis Functions.\nManuscript submitted for publication.\n\n.. _Bayesian Smoothing Using Fourier Basis Functions: https://arxiv.org/abs/2002.01241\n\"\"\"\n\nimport argparse\n\nimport numpy as np\nimport scipy.fftpack\nimport torch\nfrom torch.distributions import constraints\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer import MCMC, NUTS\n\n\n# generating synthetic data\ndef create_smoother_matrix(dim, eps=1e-6):\n    \"\"\"\n    Creates the smoother matrix A of dimension dim. The regression parameters\n    beta are smoothed by minimizing\n        ||y - X beta||^2 + lambda * beta.T A beta\n\n    Because we want to express the prior on beta in terms of a multivariate\n    normal with precision lambda * A, the matrix A must be positive definite.\n    This is why we add a small value eps along the diagonal of the 2nd\n    difference matrix.\n    \"\"\"\n    # define the 2nd difference matrix D\n    D = np.zeros((dim - 2, dim))\n    for i in range(D.shape[0]):\n        for j in range(D.shape[1]):\n            if i == j:\n                D[i, j] = 1.0\n            elif j == i + 1:\n                D[i, j] = -2.0\n            elif j == i + 2:\n                D[i, j] = 1.0\n\n    # define smoother matrix A = D^T D\n    A = np.matmul(D.T, D)\n    # add a small multiple of the identity to ensure positive definiteness\n    A = A + eps * np.eye(dim)\n    return A\n\n\ndef generate_regression_data(num_datapoints, dim, num_nonzero=5):\n    \"\"\"\n    Simulate synthetic regression data with a sparse coefficients vector\n    \"\"\"\n    assert num_nonzero < dim\n    # use a large value for the coefficient magnitude so that the signal\n    # to noise ratio is high.\n    coefficient_mag = 10.0\n    # position the non-zero coefficients throughout the coefficient vector\n    non_zero_idx = np.linspace(0, dim - 1, num=num_nonzero).astype(np.int64)\n    # generate the sparse coefficients\n    beta = np.zeros(dim)\n    beta[non_zero_idx] = coefficient_mag * (-1) ** np.arange(non_zero_idx.shape[0])\n    # generate the design matrix X\n    X = np.random.randn(num_datapoints, dim)\n    # add noise and generate y\n    eps = 0.1 * np.random.randn(num_datapoints)\n    y = np.matmul(X, beta) + eps\n    # convert everything to torch tensors\n    X, y, beta = [torch.tensor(x) for x in [X, y, beta]]\n    return X.float(), y.float(), beta.float(), non_zero_idx\n\n\n# define the DCT model\ndef model(X, y=None, lmbda=None, tau=None, obs_sd=None):\n    dim = X.shape[1]\n    # if DCT is defined in an enclosing scope then\n    # we don't need to evaluate it here\n    global DCT\n    try:\n        dct_matrix = torch.tensor(DCT)\n    except NameError:\n        # construct DCT matrix of shape (dim, dim)\n        c = np.zeros((dim, dim))\n        c[0, :] = 1.0 * np.sqrt(1.0 / dim)\n        for i in range(1, dim):\n            for j in range(dim):\n                c[i, j] = np.sqrt(2.0 / dim) * np.cos(np.pi * i * (2 * j + 1) / (2.0 * dim))\n        # convert to torch\n        dct_matrix = torch.tensor(c)\n\n    # sample observation noise\n    if obs_sd is None:\n        obs_sd = pyro.sample(\"obs_sd\", dist.HalfNormal(scale=torch.tensor(0.05)))\n\n    # sample the hyperprior parameter that controls the smoothness of the regression coefficients\n    if lmbda is None:\n        lmbda = pyro.sample(\"lmbda\", dist.HalfCauchy(scale=torch.tensor(1.0)))\n\n    # sample the hyperprior parameter that controls the sparsity of the regression coefficients\n    if tau is None:\n        tau = pyro.sample(\"tau\", dist.HalfCauchy(scale=torch.tensor(1.0)))\n\n    # standard normal prior for the DCT of coefficients beta\n    xi = pyro.sample(\"xi\", dist.Normal(0.0, tau).expand([dim]).to_event(1))\n    # compute the actual regression coeficients: beta = DCT xi\n    beta = torch.matmul(dct_matrix.t(), xi)\n    # register beta as a deterministic site in the model\n    beta = pyro.deterministic(\"beta\", beta)\n\n    # observe data\n    with pyro.plate(\"data\", X.shape[0]):\n        # compute mean function using linear regression\n        mean = torch.matmul(X, beta)\n        # observe data using a normal likelihood\n        pyro.sample(\"obs\", dist.Normal(mean, obs_sd), obs=y)\n\n    return beta\n\n\n# define a guide with Delta distributions for the \n# hyperparameters lmbda, tau, and obs_sd. we use MAP \n# estimation for the hyperparameters.\ndef guide(X, y=None, lmbda=None, tau=None, obs_sd=None):\n    if lmbda is None:\n        lmbda_loc = pyro.param(\"lmbda_loc\", torch.tensor(1.0), constraint=constraints.positive)\n        lmbda = pyro.sample(\"lmbda\", dist.Delta(lmbda_loc))\n\n    if tau is None:\n        tau_loc = pyro.param(\"tau_loc\", torch.tensor(1.0), constraint=constraints.positive)\n        tau = pyro.sample(\"tau\", dist.Delta(tau_loc))\n\n    if obs_sd is None:\n        obs_sd_loc = pyro.param(\"obs_sd_loc\", torch.tensor(0.05), constraint=constraints.positive)\n        obs_sd = pyro.sample(\"obs_sd\", dist.Delta(obs_sd_loc))\n\n\ndef dct_model(X, y=None, prec=None, obs_sd=None):\n    dim = X.shape[1]\n    # if DCT is defined in an enclosing scope then\n    # we don't need to evaluate it here\n    global DCT\n    try:\n        dct_matrix = torch.tensor(DCT)\n    except NameError:\n        # construct DCT matrix of shape (dim, dim)\n        c = np.zeros((dim, dim))\n        c[0, :] = 1.0 * np.sqrt(1.0 / dim)\n        for i in range(1, dim):\n            for j in range(dim):\n                c[i, j] = np.sqrt(2.0 / dim) * np.cos(np.pi * i * (2 * j + 1) / (2.0 * dim))\n        # convert to torch\n        dct_matrix = torch.tensor(c)\n\n    # sample observation noise\n    if obs_sd is None:\n        obs_sd = pyro.sample(\"obs_sd\", dist.HalfNormal(scale=torch.tensor(0.05)))\n\n    # sample precision hyperparameter for the coefficients\n    if prec is None:\n        prec = pyro.sample(\"prec\", dist.HalfCauchy(scale=torch.tensor(0.05)))\n\n    # define a diagonal precision matrix for the dct of the coefficients\n    weight = torch.ones(dim) * torch.sqrt(prec)\n    # set the precision for the DC component to be small\n    weight[0] = 0.01\n    # emphasize low frencies (equivalent to a prior that prefers smooth coefficients)\n    weight = weight * torch.arange(1, dim + 1).float().sqrt()\n\n    # sample regression coefficients in the dct basis\n    xi = pyro.sample(\"xi\", dist.Normal(0.0, 1.0 / weight).to_event(1))\n    # map the coefficients back to the original basis\n    beta = torch.matmul(dct_matrix.t(), xi)\n    # register beta as a deterministic site in the model\n    beta = pyro.deterministic(\"beta\", beta)\n\n    # observe data\n    with pyro.plate(\"data\", X.shape[0]):\n        # compute mean function using linear regression\n        mean = torch.matmul(X, beta)\n        # observe data using a normal likelihood\n        pyro.sample(\"obs\", dist.Normal(mean, obs_sd), obs=y)\n\n    return beta\n\n\ndef dct_smoothed_model(X, y=None, lmbda=None, prec=None, obs_sd=None):\n    dim = X.shape[1]\n    # if DCT is defined in an enclosing scope then\n    # we don't need to evaluate it here\n    global DCT\n    try:\n        dct_matrix = torch.tensor(DCT)\n    except NameError:\n        # construct DCT matrix of shape (dim, dim)\n        c = np.zeros((dim, dim))\n        c[0, :] = 1.0 * np.sqrt(1.0 / dim)\n        for i in range(1, dim):\n            for j in range(dim):\n                c[i, j] = np.sqrt(2.0 / dim) * np.cos(np.pi * i * (2 * j + 1) / (2.0 * dim))\n        # convert to torch\n        dct_matrix = torch.tensor(c)\n\n    # if A_SMOOTH is defined in an enclosing scope then\n    # we don't need to evaluate it here\n    global A_SMOOTH\n    try:\n        A = torch.tensor(A_SMOOTH)\n    except NameError:\n        A = torch.tensor(create_smoother_matrix(dim))\n\n    # convert precision matrix A to the DCT basis\n    # we have beta^T A beta = xi^T DCT^T A DCT xi\n    A_dct = torch.matmul(torch.matmul(dct_matrix, A), dct_matrix.t())\n\n    # sample observation noise\n    if obs_sd is None:\n        obs_sd = pyro.sample(\"obs_sd\", dist.HalfNormal(scale=torch.tensor(0.05)))\n\n    # sample smoothness hyperparameter for the coefficients\n    if lmbda is None:\n        lmbda = pyro.sample(\"lmbda\", dist.HalfCauchy(scale=torch.tensor(1.0)))\n\n    # sample precision hyperparameter for the coefficients\n    if prec is None:\n        prec = pyro.sample(\"prec\", dist.HalfCauchy(scale=torch.tensor(0.05)))\n\n    # sample regression coefficients in the dct basis\n    xi = pyro.sample(\n        \"xi\", dist.MultivariateNormal(torch.zeros(dim), precision_matrix=prec * torch.eye(dim) + lmbda * A_dct)\n    )\n    # map the coefficients back to the original basis\n    beta = torch.matmul(dct_matrix.t(), xi)\n    # register beta as a deterministic site in the model\n    beta = pyro.deterministic(\"beta\", beta)\n\n    # observe data\n    with pyro.plate(\"data\", X.shape[0]):\n        # compute mean function using linear regression\n        mean = torch.matmul(X, beta)\n        # observe data using a normal likelihood\n        pyro.sample(\"obs\", dist.Normal(mean, obs_sd), obs=y)\n\n    return beta\n\n\ndef horseshoe_model(X, y=None, sigma=None, tau=None, lmbda=None):\n    dim = X.shape[1]\n\n    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(scale=torch.tensor(5.0))) if sigma is None else sigma\n    tau = pyro.sample(\"tau\", dist.HalfCauchy(scale=torch.tensor(5.0))) if tau is None else tau\n\n    with pyro.plate(\"dim\", dim):\n        lmbda = (\n            pyro.sample(\"lmbda\", dist.HalfCauchy(scale=torch.ones(dim))) if lmbda is None else lmbda\n        )\n        beta = pyro.sample(\"beta\", dist.Normal(0.0, tau * lmbda * sigma))\n\n    with pyro.plate(\"data\", X.shape[0]):\n        mean = X.matmul(beta)\n        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n\n    return beta\n\n\ndef run_inference(model, guide, X, y, num_samples=500, warmup_steps=500):\n    pyro.clear_param_store()\n\n    # setup the optimizer and ELBO loss function\n    nuts_kernel = NUTS(model)\n    # perform inference\n    mcmc = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n    mcmc.run(X, y)\n    samples = mcmc.get_samples()\n\n    return samples\n\n\n# this function is taken from https://github.com/pyro-ppl/pyro/blob/dev/examples/vae/utils/mnist_cached.py\ndef _custom_save_dict(fname, dct):\n    with open(fname, \"wb\") as f:\n        torch.save(dct, f)\n\n\n# this function is taken from https://github.com/pyro-ppl/pyro/blob/dev/examples/vae/utils/mnist_cached.py\ndef _custom_load_dict(fname):\n    with open(fname, \"rb\") as f:\n        return torch.load(f)\n\n\ndef main(args):\n    global RESULTS\n    if args.load_results:\n        RESULTS = _custom_load_dict(args.filename)\n        beta_true = RESULTS[\"beta_true\"]\n        X, y = RESULTS[\"X\"], RESULTS[\"y\"]\n    else:\n        # generate synthetic dataset for linear regression\n        X, y, beta_true, nonzero_idx = generate_regression_data(\n            num_datapoints=250, dim=25, num_nonzero=5\n        )\n        # save X, y, beta_true in RESULTS\n        RESULTS = {\n            \"beta_true\": beta_true,\n            \"X\": X,\n            \"y\": y,\n            \"nonzero_idx\": nonzero_idx,\n        }\n\n    print(\"Number of datapoints: \", X.shape[0])\n    print(\"Dimensionality of the problem: \", X.shape[1])\n\n    # compute the dct in advance\n    global DCT\n    dim = X.shape[1]\n    # construct DCT matrix of shape (dim, dim)\n    c = np.zeros((dim, dim))\n    c[0, :] = 1.0 * np.sqrt(1.0 / dim)\n    for i in range(1, dim):\n        for j in range(dim):\n            c[i, j] = np.sqrt(2.0 / dim) * np.cos(np.pi * i * (2 * j + 1) / (2.0 * dim))\n    # convert to torch\n    DCT = c\n\n    # compute the smoother matrix in advance\n    global A_SMOOTH\n    A_SMOOTH = create_smoother_matrix(dim)\n\n    models = {\n        \"dct\": dct_model,\n        \"dct_smoothed\": dct_smoothed_model,\n        \"horseshoe\": horseshoe_model,\n        \"regular\": model,\n    }\n\n    if not args.model in models.keys():\n        raise ValueError(\"Unrecognized model: {}\".format(args.model))\n\n    the_model = models[args.model]\n    print(\"Model: \", args.model)\n\n    # run mcmc\n    if args.run_inference:\n        print(\"Running inference...\")\n        mcmc_samples = run_inference(\n            the_model, None, X, y, num_samples=args.num_samples, warmup_steps=args.warmup_steps\n        )\n\n        # add the samples to RESULTS\n        RESULTS[args.model] = mcmc_samples\n\n        # save RESULTS\n        _custom_save_dict(args.filename, RESULTS)\n\n    # compute statistics over posterior samples\n    beta_posterior_samples = RESULTS[args.model][\"beta\"]\n    beta_mean = beta_posterior_samples.mean(axis=0)\n    beta_std = beta_posterior_samples.std(axis=0)\n\n    # compute posterior estimates of hyperparameters (if available)\n    hypers = [h for h in [\"lmbda\", \"tau\", \"obs_sd\", \"prec\", \"sigma\"] if h in RESULTS[args.model].keys()]\n    for h in hypers:\n        h_mean = RESULTS[args.model][h].mean(axis=0)\n        h_std = RESULTS[args.model][h].std(axis=0)\n        print(\"Posterior estimate of %s: %.3g +- %.3g\" % (h, h_mean, h_std))\n\n    # print true and inferred coefficients\n    print(\"%-10s  %-10s  %-10s\" % (\"truth\", \"estimate\", \"uncertainty\"))\n    for i in range(0, 25, 5):\n        for j in range(5):\n            idx = i + j\n            true_value = float(beta_true[idx]) if idx < len(beta_true) else 0.0\n            print(\n                \"%-10.3g  %-10.3g  %-10.3g\"\n                % (true_value, float(beta_mean[idx]), float(beta_std[idx]))\n            )\n        print(\"-----------------------------\")\n\n    # compute l1 error\n    l1_error = np.sum(np.abs(beta_mean.numpy() - beta_true.numpy()))\n    print(\"L1 error: %.3g\" % l1_error)\n\n    # plot true coefficients, posterior mean, and posterior mean +- 1 std\n    fig, ax = plt.subplots(dpi=100, figsize=(8, 4))\n    plt.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.95)\n\n    x_range = torch.arange(beta_true.shape[0])\n    plt.plot(x_range, beta_true, \"k-\", lw=2, label=\"True coefficient value\")\n    plt.plot(x_range, beta_mean, \"r-\", lw=1, label=\"Posterior mean\")\n    plt.fill_between(\n        x_range,\n        beta_mean - beta_std,\n        beta_mean + beta_std,\n        color=\"red\",\n        alpha=0.3,\n        label=\"Mean +-1 std\",\n    )\n\n    plt.legend(fontsize=10, loc=\"upper right\")\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    plt.title(\"%s (error=%.3g)\" % (args.model, l1_error), fontsize=12)\n    plt.ylim(-15, 15)\n\n    plt.savefig(\"%s.pdf\" % args.model)\n\n\nif __name__ == \"__main__\":\n    assert pyro.__version__.startswith(\"1.7.0\")\n    parser = argparse.ArgumentParser(description=\"Sparse Bayesian regression using DCT\")\n    parser.add_argument(\n        \"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int, help=\"number of samples in inference\"\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--warmup-steps\",\n        nargs=\"?\",\n        default=500,\n        type=int,\n        help=\"number of warmup steps in inference\",\n    )\n    parser.add_argument(\n        \"--model\",\n        nargs=\"?\",\n        default=\"dct\",\n        type=str,\n        help=\"one of: [dct, dct_smoothed, horseshoe, regular]\",\n    )\n    parser.add_argument(\n        \"--filename\", default=\"results_sparse.pkl\", type=str, help=\"optional filename for saving results\"\n    )\n    parser.add_argument(\"--seed\", default=1, type=int, help=\"random seed\")\n    parser.add_argument(\"--run-inference\", action=\"store_true\", help=\"whether to run inference\")\n    parser.add_argument(\"--load-results\", action=\"store_true\", help=\"whether to load results and skip data generation\")\n    args = parser.parse_args()\n\n    pyro.set_rng_seed(args.seed)\n    pyro.clear_param_store()\n\n    main(args)\n```\n\n----------------------------------------\n\nTITLE: Implementing Marginal Distribution Helper\nDESCRIPTION: Creates a Marginal helper function that constructs distribution over execution traces using Search and HashingMarginal.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-hyperbole.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef Marginal(fn):\n    return memoize(lambda *args: HashingMarginal(Search(fn).run(*args)))\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic PyTorch Linear Module\nDESCRIPTION: Creates a simple linear module using PyTorch's nn.Module as a base class. This module will be used to demonstrate conversion to PyroModule.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Linear(nn.Module):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(in_size, out_size))\n        self.bias = nn.Parameter(torch.randn(out_size))\n        \n    def forward(self, input_):\n        return self.bias + input_ @ self.weight\n    \nlinear = Linear(5, 2)\nassert isinstance(linear, nn.Module)\nassert not isinstance(linear, PyroModule)\n\nexample_input = torch.randn(100, 5)\nexample_output = linear(example_input)\nassert example_output.shape == (100, 2)\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container for PyTorch Build\nDESCRIPTION: Commands to launch a Docker container using CUDA 8.0 base image for building PyTorch binaries.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd manywheel\ndocker run -it --ipc=host --rm -v $(pwd):/remote soumith/manylinux-cuda80:latest bash\n```\n\n----------------------------------------\n\nTITLE: Creating a Masked Binary Cross Entropy Loss for CVAE MNIST Task\nDESCRIPTION: This custom loss function computes binary cross-entropy only on the non-masked pixels of the target image. It zeroes out the loss contribution of pixels that are masked with a specified value, allowing the model to focus on predicting the relevant quadrants.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/cvae.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass MaskedBCELoss(nn.Module):\n    def __init__(self, masked_with=-1):\n        super().__init__()\n        self.masked_with = masked_with\n\n    def forward(self, input, target):\n        target = target.view(input.shape)\n        loss = F.binary_cross_entropy(input, target, reduction='none')\n        loss[target == self.masked_with] = 0\n        return loss.sum()\n```\n\n----------------------------------------\n\nTITLE: Rendering a Simple Pyro Model\nDESCRIPTION: Demonstrates how to render a simple Pyro model using pyro.render_model() function.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/model_rendering.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.ones(10)\npyro.render_model(model, model_args=(data,))\n```\n\n----------------------------------------\n\nTITLE: Distribution Plotting Helper\nDESCRIPTION: Implements a helper function to visualize probability distributions using matplotlib.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef plot_dist(d):\n    support = d.enumerate_support()\n    data = [d.log_prob(s).exp().item() for s in d.enumerate_support()]\n    names = list(map(str, support))\n\n    ax = plt.subplot(111)\n    width = 0.3\n    bins = [x-width/2 for x in range(1, len(data) + 1)]\n    ax.bar(bins,data,width=width)\n    ax.set_xticks(list(range(1, len(data) + 1)))\n    ax.set_xticklabels(names, rotation=45, rotation_mode=\"anchor\", ha=\"right\")\n    \ninterp_dist = pragmatic_listener(\"some\")\nplot_dist(interp_dist)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sparse Gamma Deep Exponential Family in Pyro\nDESCRIPTION: This example demonstrates how to implement a Sparse Gamma Deep Exponential Family model in Pyro. It appears to be a literalinclude reference to the actual implementation file located at examples/sparse_gamma_def.py in the Pyro repository.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/sparse_gamma.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# No code snippet is directly provided in the text. The content references an external file using a literalinclude directive.\n```\n\n----------------------------------------\n\nTITLE: Defining Data and Original Model for Fair Coin Example in Pyro\nDESCRIPTION: Sets up the data and defines the original probabilistic model for a fair coin example using Pyro's syntax.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mle_map.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.zeros(10)\ndata[0:6] = 1.0\n\ndef original_model(data):\n    f = pyro.sample(\"latent_fairness\", dist.Beta(10.0, 10.0))\n    with pyro.plate(\"data\", data.size(0)):\n        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Running Individual Pyro Tests\nDESCRIPTION: Commands to run a single test from the command line, including options for running tests in CUDA mode with specific data type and device settings.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npytest -vs {path_to_test}::{test_name}\n```\n\nLANGUAGE: sh\nCODE:\n```\nCUDA_TEST=1 PYRO_DTYPE=float64 PYRO_DEVICE=cuda pytest -vs {path_to_test}::{test_name}\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Libraries for Financial Analysis in Python\nDESCRIPTION: This snippet imports essential libraries like Pyro for probabilistic programming, and PyTorch for tensor computations, along with matplotlib for plotting data visualizations. The snippet also checks the Pyro version and sets up a smoke test environment conditionally.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/stable.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport os\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom matplotlib import pyplot\nfrom torch.distributions import constraints\n\nfrom pyro import poutine\nfrom pyro.contrib.examples.finance import load_snp500\nfrom pyro.infer import EnergyDistance, Predictive, SVI, Trace_ELBO\nfrom pyro.infer.autoguide import AutoDiagonalNormal\nfrom pyro.infer.reparam import DiscreteCosineReparam, StableReparam\nfrom pyro.optim import ClippedAdam\nfrom pyro.ops.tensor_utils import convolve\n\n%matplotlib inline\nassert pyro.__version__.startswith('1.9.1')\nsmoke_test = ('CI' in os.environ)\n```\n\n----------------------------------------\n\nTITLE: RandomWalkKernel Class Documentation Reference\nDESCRIPTION: Sphinx documentation directive for the RandomWalk MCMC kernel implementation\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.infer.mcmc.RandomWalkKernel\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Validating TorchScript Module with Visualization\nDESCRIPTION: This code validates the serialized TorchScript module by generating samples and recreating the previous plot. It ensures that the module was correctly saved and can reproduce the expected results.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bayesian_regression.ipynb#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nweight = []\nfor _ in range(800):\n    # index = 1 corresponds to \"linear.weight\"\n    weight.append(pred_loaded(x_data)[1])\nweight = torch.stack(weight).detach()\nweight = weight.reshape(weight.shape[0], 3)\ngamma_within_africa = weight[:, 1] + weight[:, 2]\ngamma_outside_africa = weight[:, 1]\nfig = plt.figure(figsize=(10, 6))\nsns.distplot(gamma_within_africa, kde_kws={\"label\": \"African nations\"},)\nsns.distplot(gamma_outside_africa, kde_kws={\"label\": \"Non-African nations\"})\nfig.suptitle(\"Loaded TorchScript Module : log(GDP) vs. Terrain Ruggedness\");\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Automatic Name Generation Module with Sphinx\nDESCRIPTION: This code snippet uses Sphinx automodule directives to generate documentation for the pyro.contrib.autoname module. It includes all members, undocumented members, and inheritance information, ordering members by source.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.autoname.rst#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: pyro.contrib.autoname\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Transform Documentation Structure\nDESCRIPTION: RST documentation structure for Pyro distribution transforms showing class and function documentation patterns using autoclass and autofunction directives.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/distributions.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.distributions.transforms.CorrMatrixCholeskyTransform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n.. autofunction:: pyro.distributions.transforms.conditional_spline_autoregressive\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro dev branch from source\nDESCRIPTION: Commands to clone the Pyro repository and install the latest development version from source.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/README.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/pyro-ppl/pyro\ncd pyro\npip install .  # pip install .[extras] for running models in examples/tutorials\n```\n\n----------------------------------------\n\nTITLE: Setting Default Beta Parameters for Adam Optimizer\nDESCRIPTION: Shows the standard beta parameter values used in Adam optimization, which may need to be adjusted for highly stochastic models.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbetas = (0.90, 0.999)\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Environment and Dependencies\nDESCRIPTION: Sets up required imports and configures GPU/CPU device selection for PyTorch. Includes core dependencies like PyTorch, Pyro, NumPy and Matplotlib.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport datetime\nfrom functools import partial\nimport numpy as np\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.optim import ClippedAdam\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nif torch.cuda.is_available():\n    print(\"Using GPU\")\n    torch.set_default_device(\"cuda\")\nelse:\n    print(\"Using CPU\")\n\nsmoke_test = ('CI' in os.environ)\n```\n\n----------------------------------------\n\nTITLE: Generating Reference Data\nDESCRIPTION: Sampling reference latent variables and observations for demonstration purposes.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_flow_guide.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nz_star = prior.sample((64,))\nx_star = likelihood(z_star).sample((5,))\n```\n\n----------------------------------------\n\nTITLE: MCMCKernel Class Documentation Reference\nDESCRIPTION: Sphinx documentation directive for the base MCMCKernel class in pyro.infer.mcmc.mcmc_kernel\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.infer.mcmc.mcmc_kernel.MCMCKernel\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Generating Measurements with Noise\nDESCRIPTION: Creates noisy measurements from the truth trajectory by adding Gaussian noise to the position coordinates. Only measures the positions of the particle.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/ekf.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Measurements\nmeasurements = []\nmean = torch.zeros(2)\n# no correlations\ncov = 1e-5 * torch.eye(2)\nwith torch.no_grad():\n    # sample independent measurement noise\n    dzs = pyro.sample('dzs', dist.MultivariateNormal(mean, cov).expand((num_frames,)))\n    # compute measurement means\n    zs = xs_truth[:, :2] + dzs\n```\n\n----------------------------------------\n\nTITLE: Running Basic Pyro Tests\nDESCRIPTION: Commands for formatting code and running basic tests before submitting a pull request. These commands ensure code meets style guidelines and passes unit tests.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmake format            # runs isort\n```\n\nLANGUAGE: sh\nCODE:\n```\nmake test              # linting and unit tests\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro using pip\nDESCRIPTION: Command to install the stable release of Pyro using pip package manager.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install pyro-ppl\n```\n\n----------------------------------------\n\nTITLE: BlockMassMatrix Class Documentation Reference\nDESCRIPTION: Sphinx documentation directive for the BlockMassMatrix class implementation\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.infer.mcmc.BlockMassMatrix\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro with extra packages\nDESCRIPTION: Command to install Pyro with additional dependencies required for running examples and tutorials.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install pyro-ppl[extras]\n```\n\n----------------------------------------\n\nTITLE: Estimating Mixture Density from Posterior Parameters in Python\nDESCRIPTION: Calculates Bayesian point estimates for latent variables using posterior means of tau and kappa parameters, then creates and plots a mixture of Poisson distributions to model sunspot data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# We make a point-estimate of our latent variables using the posterior means of tau and kappa for the cluster params and weights\nBayes_Rates = (tau0_optimal / tau1_optimal)\nBayes_Weights = mix_weights(1. / (1. + kappa_optimal))\n\ndef mixture_of_poisson(weights, rates, samples):\n    return (weights * Poisson(rates).log_prob(samples.unsqueeze(-1)).exp()).sum(-1)\n\nlikelihood = mixture_of_poisson(Bayes_Weights, Bayes_Rates, samples)\n\nplt.title(\"Number of Years vs. Sunspot Counts\")\nplt.hist(data.numpy(), bins=60, density=True, lw=0, alpha=0.75);\nplt.plot(samples, likelihood, label=\"Estimated Mixture Density\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for CSIS Implementation\nDESCRIPTION: Sets up required Python libraries including PyTorch, Pyro, and testing configurations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/csis.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.functional as F\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.infer\nimport pyro.optim\n\nimport os\nsmoke_test = ('CI' in os.environ)\nn_steps = 2 if smoke_test else 2000\n```\n\n----------------------------------------\n\nTITLE: Avoiding Name Conflict with nn.Module\nDESCRIPTION: This code snippet demonstrates how to avoid name conflicts by using a standard nn.Module instead of a PyroModule when defining attributes within a nn.Module. This prevents automatic Pyro naming and potential conflicts if the attributes are not properly connected.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/modules.ipynb#2025-04-16_snippet_18\n\nLANGUAGE: diff\nCODE:\n```\n  class Model(nn.Module):        # not a PyroModule\n      def __init__(self):\n          self.x = PyroModule()\n-         self.y = PyroModule()  # Could lead to name conflict.\n+         self.y = nn.Module()  # Has no Pyro names, so avoids conflict.\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation for Random Variables Module\nDESCRIPTION: ReStructuredText documentation defining the structure and autodoc directives for the random variables module in Pyro. Uses Sphinx autodoc to generate API documentation from docstrings.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.randomvariable.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nRandom Variables\n================\n\n.. automodule:: pyro.contrib.randomvariable\n\nRandom Variable\n---------------\n.. autoclass:: pyro.contrib.randomvariable.random_variable.RandomVariable\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Plotting the Objective Function with Matplotlib\nDESCRIPTION: Visualizes the Forrester objective function over the input domain [0,1] to show its characteristics and multiple minima.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/bo.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nx = torch.linspace(0, 1, 100)\nplt.figure(figsize=(8, 4))\nplt.plot(x.numpy(), f(x).numpy())\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Running Comprehensive Pyro Tests\nDESCRIPTION: Commands for running more comprehensive tests that cover examples, integration tests, and CUDA tests. These are recommended when modifying core Pyro code.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmake test-examples     # test examples/\n```\n\nLANGUAGE: sh\nCODE:\n```\nmake integration-test  # longer-running tests (may take hours)\n```\n\nLANGUAGE: sh\nCODE:\n```\nmake test-cuda         # runs unit tests in cuda mode\n```\n\n----------------------------------------\n\nTITLE: Referencing Pyro Constraints Module in Python\nDESCRIPTION: This snippet shows how to reference the constraints module in Pyro documentation using the automodule directive.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/distributions.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. automodule:: pyro.distributions.constraints\n```\n\n----------------------------------------\n\nTITLE: Initializing Dependencies for RSA Model\nDESCRIPTION: Sets up required imports including PyTorch, Pyro, and custom search inference utilities. Configures PyTorch to use double precision for numerical stability.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-hyperbole.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ntorch.set_default_dtype(torch.float64)  # double precision for numerical stability\n\nimport collections\nimport argparse\nimport matplotlib.pyplot as plt\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\n\nfrom search_inference import HashingMarginal, memoize, Search\n```\n\n----------------------------------------\n\nTITLE: Running Pyro Profiling Tools\nDESCRIPTION: Commands for using the profiler module to test performance and identify potential bottlenecks in Pyro code, particularly in the distributions library.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython -m profiler.distributions --help\n```\n\nLANGUAGE: sh\nCODE:\n```\npython -m profiler.distributions\n```\n\nLANGUAGE: sh\nCODE:\n```\npython -m profiler.distributions --dist bernoulli normal --batch_sizes 1000 100000\n```\n\nLANGUAGE: sh\nCODE:\n```\npython -m profiler.distributions --dist bernoulli --tool cprofile\n```\n\n----------------------------------------\n\nTITLE: Initializing Data Source URL - Python\nDESCRIPTION: Defines the base URL for accessing election datasets stored in GitHub.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nBASE_URL =  \"https://github.com/pyro-ppl/datasets/blob/master/us_elections/\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Dependencies\nDESCRIPTION: Sets up required imports for object tracking implementation including PyTorch, Pyro, and visualization libraries.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/tracking_1d.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport os\nimport torch\nfrom torch.distributions import constraints\nfrom matplotlib import pyplot\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.contrib.tracking.assignment import MarginalAssignmentPersistent\nfrom pyro.distributions.util import gather\nfrom pyro.infer import SVI, TraceEnum_ELBO\nfrom pyro.optim import Adam\n\n%matplotlib inline\nassert pyro.__version__.startswith('1.9.1')\nsmoke_test = ('CI' in os.environ)\n```\n\n----------------------------------------\n\nTITLE: Including MCMC Module Documentation using reStructuredText\nDESCRIPTION: This snippet demonstrates how to include external documentation content from another file using the reStructuredText include directive. It's specifically including documentation for Pyro's MCMC inference module.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/mcmc.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: pyro.infer.mcmc.txt\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Pyro Model with LogNormal Distribution\nDESCRIPTION: Creates a basic Pyro model that samples a single variable from a LogNormal distribution with location 0.0 and scale 1.0.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    pyro.sample(\"x\", dist.LogNormal(0.0, 1.0))\n```\n\n----------------------------------------\n\nTITLE: Building Pyro Documentation with Make\nDESCRIPTION: Command to build the Pyro documentation from the project's root directory using Make.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake docs\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro dev branch using pip\nDESCRIPTION: Commands to install the latest development version of Pyro directly from GitHub using pip.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/README.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install git+https://github.com/pyro-ppl/pyro.git\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install git+https://github.com/pyro-ppl/pyro.git#egg=project[extras]\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Settings Module with Sphinx\nDESCRIPTION: This reStructuredText (rst) directive instructs Sphinx to automatically generate documentation for the pyro.settings module. The directive includes all members of the module and orders them by source order rather than alphabetically.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/settings.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pyro.settings\n   :members:\n   :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for RSA Implementation\nDESCRIPTION: Sets up required imports including PyTorch for tensors, Pyro for probabilistic programming, and custom search inference utilities.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/RSA-implicature.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ntorch.set_default_dtype(torch.float64)  # double precision for numerical stability\n\nimport collections\nimport argparse\nimport matplotlib.pyplot as plt\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\n\nfrom search_inference import HashingMarginal, memoize, Search\n```\n\n----------------------------------------\n\nTITLE: Sample Message Structure Example\nDESCRIPTION: Example showing the structure of a message dictionary used in Pyro's messaging system.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/effect_handlers.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npyro.sample(\"x\", dist.Bernoulli(0.5), infer={\"enumerate\": \"parallel\"}, obs=None)\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro from source\nDESCRIPTION: Commands to clone the Pyro repository, checkout the master branch (latest stable release), and install it.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/README.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngit clone git@github.com:pyro-ppl/pyro.git\ncd pyro\ngit checkout master  # master is pinned to the latest release\npip install .\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Model Inspection Module in RST\nDESCRIPTION: ReStructuredText documentation for Pyro's model inspection module, which provides tools for examining probabilistic models.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/infer.util.rst#2025-04-16_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pyro.infer.inspect\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Invalid Plate Dependency Example\nDESCRIPTION: Demonstrates an invalid model with dependent variables within a plate, violating conditional independence.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef invalid_model():\n    x = 0\n    for i in pyro.plate(\"invalid\", 10):\n        x = pyro.sample(f\"x_{i}\", dist.Normal(x, 1.))\n```\n\n----------------------------------------\n\nTITLE: Building Pyro HTML Documentation\nDESCRIPTION: Command to build the HTML pages of the Pyro documentation using Make.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure for Bayesian Neural Networks in Pyro using reStructuredText\nDESCRIPTION: This RST code defines the documentation structure for Pyro's Bayesian Neural Networks module. It includes automodule directives to automatically generate documentation from docstrings in the pyro.contrib.bnn package and its hidden_layer submodule, with members ordered by source appearance.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.bnn.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nBayesian Neural Networks\n=========================\n\n.. automodule:: pyro.contrib.bnn\n\nHiddenLayer\n-------------------------\n.. automodule:: pyro.contrib.bnn.hidden_layer\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro OED Module in RST\nDESCRIPTION: RST code for documenting the pyro.contrib.oed module, including sections for Expected Information Gain and Generalised Linear Mixed Models.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.oed.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nOptimal Experiment Design\n=========================\n\n.. automodule:: pyro.contrib.oed\n\nExpected Information Gain\n-------------------------\n.. automodule:: pyro.contrib.oed.eig\n    :members:\n    :member-order: bysource\n\nGeneralised Linear Mixed Models\n-------------------------------\n.. automodule:: pyro.contrib.oed.glmm\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Executing AutoGuide\nDESCRIPTION: Runs the trained auto guide on input data X_ and y_.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/predictive_deterministic.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nauto_guide(X_, y_)\n```\n\n----------------------------------------\n\nTITLE: Building Pyro API Documentation with Sphinx\nDESCRIPTION: Command structure for building API documentation using sphinx-apidoc. This generates documentation from Python docstrings.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/README.md#2025-04-16_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx-apidoc [options] -o <output_path> <module_path> [exclude_pattern, ...]\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Scoping Module with Sphinx\nDESCRIPTION: This code snippet uses Sphinx automodule directives to generate documentation for the pyro.contrib.autoname.scoping module. It includes all members, undocumented members, and inheritance information, ordering members by source.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.autoname.rst#2025-04-16_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: pyro.contrib.autoname.scoping\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formula - Joint Distribution\nDESCRIPTION: Mathematical representation of the joint distribution for the Deep Markov Model over three time steps, showing the relationship between observations (x) and latent variables (z).\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dmm.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\np({\\bf x}_{123} , {\\bf z}_{123})=p({\\bf x}_1|{\\bf z}_1)p({\\bf x}_2|{\\bf z}_2)p({\\bf x}_3|{\\bf z}_3)p({\\bf z}_1)p({\\bf z}_2|{\\bf z}_1)p({\\bf z}_3|{\\bf z}_2)\n```\n\n----------------------------------------\n\nTITLE: DBN Plot Function Call - Python\nDESCRIPTION: A function call to plot data with 'DBN' (Dynamic Bayesian Network) parameter.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nplot(\"DBN\")\n```\n\n----------------------------------------\n\nTITLE: Building Custom Pyro Docker Image with GPU Support\nDESCRIPTION: Command to build a custom Docker image for Pyro using the 'dev' branch on top of PyTorch's 'master' branch with Python 3.6 for GPU execution. The resulting image will be named 'pyro-gpu-dev-3.6'.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docker/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmake build-gpu pyro_branch=dev pytorch_branch=master python_version=3.6\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Resampler Module in RST\nDESCRIPTION: ReStructuredText documentation for Pyro's resampler module, which provides functionality for interactive prior tuning in probabilistic models.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/infer.util.rst#2025-04-16_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pyro.infer.resampler\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Importing ParamStore Module in Python\nDESCRIPTION: This code snippet demonstrates how to import and document the ParamStore module from pyro.params.param_store. It uses automodule to automatically generate documentation for all members, including undocumented members, and shows inheritance information.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/parameters.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: pyro.params.param_store\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Plotting HMM Analysis Results\nDESCRIPTION: Calls the plot function to visualize the HMM analysis results using the previously defined data.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nplot(\"HMM\")\n```\n\n----------------------------------------\n\nTITLE: Creating Tensor Data for Vectorized plate in Pyro\nDESCRIPTION: Preparation of tensor data for use with vectorized `plate` operations, creating a binary tensor with 6 ones (heads) and 4 zeros (tails).\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_ii.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.zeros(10)\ndata[0:6] = torch.ones(6)  # 6 heads and 4 tails\n```\n\n----------------------------------------\n\nTITLE: Referencing Pyro Version Location\nDESCRIPTION: Reference to the location of version prefix constant in Pyro's initialization file.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/RELEASE-MANAGEMENT.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\npyro/__init__.py\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Sunspot Data\nDESCRIPTION: Loads historical sunspot count data from an online source and prepares it for analysis using the DPMM model.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dirichlet_process_mixture.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('http://www.sidc.be/silso/DATA/SN_y_tot_V2.0.csv', sep=';', names=['time', 'sunspot.year'], usecols=[0, 1])\ndata = torch.tensor(df['sunspot.year'].values, dtype=torch.float32).round()\nN = data.shape[0]\n\nplt.hist(df['sunspot.year'].values, bins=40)\nplt.title(\"Number of Years vs. Sunspot Counts\")\nplt.xlabel(\"Sunspot Count\")\nplt.ylabel(\"Number of Years\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring Matplotlib for Visualization\nDESCRIPTION: Imports matplotlib and configures it for inline display in the notebook with SVG output format.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom matplotlib import pyplot\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Pyro and Visualization\nDESCRIPTION: Imports necessary Python libraries including Pyro, PyTorch, matplotlib, and ipywidgets for interactive visualizations. Also sets up environment variables for testing.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom ipywidgets import interact, FloatSlider\nimport matplotlib.pyplot as plt\nimport torch\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer.resampler import Resampler\n\nassert pyro.__version__.startswith('1.9.1')\nsmoke_test = ('CI' in os.environ)  # for CI testing only\n```\n\n----------------------------------------\n\nTITLE: Time Series Data Array - JavaScript\nDESCRIPTION: An array of floating point numbers representing performance timing measurements in seconds.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n[0.19684600830078125, 0.2023460865020752, 0.22677183151245117, 0.23773717880249023, 0.23118090629577637, 0.23914885520935059, 0.2430558204650879, 0.31301093101501465, 0.2789499759674072, 0.26804518699645996, 0.28461790084838867, 0.3887619972229004, 0.31357502937316895, 0.2947719097137451, 0.3141598701477051, 0.4249720573425293, 0.32235097885131836, 0.3292689323425293, 0.32982301712036133, 0.39942502975463867, 0.3410038948059082, 0.3757472038269043, 0.38117194175720215]\n```\n\n----------------------------------------\n\nTITLE: Installing Pyro-PPL Package\nDESCRIPTION: Installs the Pyro probabilistic programming library using pip. This is specifically for running in Google Colab environments.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/prior_predictive.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q pyro-ppl  # for colab\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Pyro Enumeration Tutorial\nDESCRIPTION: Imports necessary libraries and modules for the Pyro enumeration tutorial, including PyTorch, Pyro distributions, and inference algorithms. Also sets random seed and checks Pyro version.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/enumeration.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom torch.distributions import constraints\nfrom pyro import poutine\nfrom pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate, infer_discrete\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.ops.indexing import Vindex\n\nsmoke_test = ('CI' in os.environ)\nassert pyro.__version__.startswith('1.9.1')\npyro.set_rng_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data for Additional Analysis\nDESCRIPTION: Reinitializes data structures with new problem sizes, costs, and runtime measurements for further analysis, possibly for a different model or scenario.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsizes = [\n    3,\n    4,\n    5,\n    6,\n    7,\n    8,\n    9,\n    10,\n    11,\n    12,\n    13,\n    14,\n    15,\n    16,\n    17,\n    18,\n    19,\n    20,\n    21,\n    22,\n    23,\n    24,\n    25,\n    26,\n    27,\n    28,\n    29,\n    30,\n    31,\n    32,\n    33,\n    34,\n    35,\n    36,\n    37,\n    38,\n    39,\n    40,\n    41,\n    42,\n    43,\n    44,\n    45,\n    46,\n    47,\n    48,\n    49,\n    50,\n]\ncosts = {\n    \"einsum\": [\n        7,\n        9,\n        13,\n        15,\n        19,\n        21,\n        25,\n        27,\n        31,\n        33,\n        37,\n        39,\n        43,\n        45,\n        49,\n        51,\n        55,\n        57,\n        61,\n        63,\n        67,\n        69,\n        73,\n        75,\n        79,\n        81,\n        85,\n        87,\n        91,\n        93,\n        97,\n        99,\n        103,\n        105,\n        109,\n        111,\n        115,\n        117,\n        121,\n        123,\n        127,\n        129,\n        133,\n        135,\n        139,\n        141,\n        145,\n        147,\n    ],\n    \"tensordot\": [\n        18,\n        25,\n        30,\n        37,\n        42,\n        49,\n        54,\n        61,\n        66,\n        73,\n        78,\n        85,\n        90,\n        97,\n        102,\n        109,\n        114,\n        121,\n        126,\n        133,\n        138,\n        145,\n        150,\n        157,\n        162,\n        169,\n        174,\n        181,\n        186,\n        193,\n        198,\n        205,\n        210,\n        217,\n        222,\n        229,\n        234,\n        241,\n        246,\n        253,\n        258,\n        265,\n        270,\n        277,\n        282,\n        289,\n        294,\n        301,\n    ],\n    \"tensor\": [\n        46,\n        63,\n        80,\n        97,\n        114,\n        131,\n        148,\n        165,\n        182,\n        199,\n        216,\n        233,\n        250,\n        267,\n        284,\n        301,\n        318,\n        335,\n        352,\n        369,\n        386,\n        403,\n        420,\n        437,\n        454,\n        471,\n        488,\n        505,\n        522,\n        539,\n        556,\n        573,\n        590,\n        607,\n        624,\n        641,\n        658,\n        675,\n        692,\n        709,\n        726,\n        743,\n        760,\n        777,\n        794,\n        811,\n        828,\n        845,\n    ],\n}\ntimes1 = [\n    0.02198004722595215,\n    0.03037405014038086,\n    0.03350090980529785,\n    0.04224896430969238,\n    0.04834318161010742,\n    0.05909299850463867,\n    0.06626009941101074,\n    0.08351302146911621,\n    0.09097099304199219,\n    0.08897876739501953,\n    0.09535503387451172,\n    0.10136294364929199,\n    0.13000011444091797,\n    0.12712597846984863,\n    0.13105392456054688,\n    0.1476750373840332,\n    0.14663481712341309,\n    0.15439701080322266,\n    0.15521693229675293,\n    0.1650080680847168,\n    0.1742238998413086,\n    0.17893004417419434,\n    0.18517208099365234,\n    0.19159197807312012,\n    0.20879316329956055,\n    0.2737429141998291,\n    0.23352789878845215,\n    0.22190213203430176,\n    0.23365497589111328,\n    0.23900103569030762,\n    0.2523791790008545,\n    0.26091718673706055,\n    0.2820899486541748,\n    0.3140451908111572,\n    0.28127598762512207,\n    0.2906830310821533,\n    0.34561610221862793,\n    0.4711790084838867,\n    0.3032550811767578,\n    0.31789112091064453,\n    0.34140491485595703,\n    0.34586501121520996,\n    0.3419170379638672,\n    0.35588693618774414,\n    0.36873412132263184,\n    0.36976003646850586,\n    0.3961608409881592,\n    0.3883850574493408,\n]\ntimes2 = [\n    0.022389888763427734,\n    0.026437997817993164,\n    0.03232693672180176,\n    0.041667938232421875,\n    0.05160379409790039,\n    0.055931806564331055,\n    0.07128310203552246,\n    0.08053183555603027,\n    0.08122706413269043,\n    0.0810542106628418,\n    0.0922250747680664,\n    0.10212492942810059,\n    0.11983704566955566,\n    0.1128089427947998,\n    0.13935494422912598,\n    0.12748098373413086,\n    0.13879609107971191,\n    0.1859588623046875,\n    0.14890193939208984,\n    0.15740394592285156,\n    0.16302895545959473,\n    0.17653393745422363,\n    0.1802539825439453,\n    0.18121719360351562,\n    0.20098400115966797,\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Pyro Documentation Structure with reStructuredText\nDESCRIPTION: This snippet defines the structure of the Pyro documentation using reStructuredText directives. It creates a table of contents (toctree) for both core Pyro components and contributed code modules.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 2\n   :caption: Pyro Core:\n\n   getting_started\n   primitives\n   inference\n   distributions\n   parameters\n   nn\n   optimization\n   poutine\n   ops\n   settings\n   testing\n\n.. toctree::\n   :glob:\n   :maxdepth: 2\n   :caption: Contributed Code:\n\n   contrib.autoname\n   contrib.bnn\n   contrib.cevae\n   contrib.easyguide\n   contrib.epidemiology\n   contrib.examples\n   contrib.forecast\n   contrib.funsor\n   contrib.gp\n   contrib.minipyro\n   contrib.mue\n   contrib.oed\n   contrib.randomvariable\n   contrib.timeseries\n   contrib.tracking\n   contrib.zuko\n```\n\n----------------------------------------\n\nTITLE: Installing CMake3 for PyTorch Build\nDESCRIPTION: Shell commands to update CMake to version 3, required for building PyTorch master branch.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nyum install epel-release\nyum install cmake3\nyum remove cmake \nln -s /usr/bin/cmake3 /usr/bin/cmake\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Parameter Settings\nDESCRIPTION: The parameters used for executing the main inference script are outlined here. These parameters control the number of iterations, learning rates, and various biases for the model's decoder. It is important to understand how these affect the inference results and model performance.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/air.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npython main.py -n 200000 -blr 0.1 --z-pres-prior 0.01 --scale-prior-sd 0.2 --predict-net 200 --bl-predict-net 200 --decoder-output-use-sigmoid --decoder-output-bias -2 --seed 287710\n```\n\n----------------------------------------\n\nTITLE: Mini-Pyro Example Usage\nDESCRIPTION: Example script demonstrating how to use Mini-Pyro functionality. Located in examples/minipyro.py.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/minipyro.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../examples/minipyro.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Converting Numbers to Funsor Objects\nDESCRIPTION: Demonstrates basic conversion of Python numbers and torch tensors to Funsor objects using funsor.to_funsor.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfunsor_one = funsor.to_funsor(float(1))\nprint(funsor_one, type(funsor_one))\n\nfunsor_two = funsor.to_funsor(torch.tensor(2.))\nprint(funsor_two, type(funsor_two))\n```\n\n----------------------------------------\n\nTITLE: Loading Historical Election Data - Python\nDESCRIPTION: Loads and displays historical US presidential election data from 1976-2012.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/elections.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nframe = pd.read_pickle(urlopen(BASE_URL + \"us_presidential_election_data_historical.pickle?raw=true\"))\nprint(frame[[1976, 1980, 1984]].head())\n```\n\n----------------------------------------\n\nTITLE: Loading SMC Filter Example from Pyro Documentation\nDESCRIPTION: Reference to the SMC filter implementation file 'smcfilter.py' from the Pyro examples directory, which demonstrates Sequential Monte Carlo filtering techniques.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/smcfilter.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../examples/smcfilter.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Pyro\nDESCRIPTION: Specifies required Python packages and their minimum versions for the Pyro project. Includes documentation tools (Sphinx, sphinx-rtd-theme), numerical computation libraries (numpy), and other essential dependencies like opt_einsum and pyro-api.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx==4.2.0\nsphinx-rtd-theme==1.0.0\ngraphviz>=0.8\nnumpy>=1.7\nobservations>=0.1.4\nopt_einsum>=2.3.2\npyro-api>=0.1.1\ntqdm>=4.36\nfunsor[torch]\nsetuptools\nsphinx_copybutton\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Pyro AutoGuide\nDESCRIPTION: ReStructuredText documentation defining the structure and class references for Pyro's autoguide module. Contains documentation directives for multiple autoguide classes and their initialization.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/infer.autoguide.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nAutomatic Guide Generation\n==========================\n\n.. automodule:: pyro.infer.autoguide\n\nAutoGuide\n---------\n.. autoclass:: pyro.infer.autoguide.AutoGuide\n    :members:\n    :undoc-members:\n    :member-order: bysource\n    :show-inheritance:\n\nAutoGuideList\n-------------\n.. autoclass:: pyro.infer.autoguide.AutoGuideList\n    :members:\n    :undoc-members:\n    :member-order: bysource\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Building Pyro Documentation\nDESCRIPTION: Command to ensure the documentation builds correctly, which should be run before submitting changes that affect documentation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nmake docs\n```\n\n----------------------------------------\n\nTITLE: Setting up Pyro Tutorial Smoke Tests\nDESCRIPTION: Python code snippet showing how to configure a tutorial for CI testing by adding a smoke_test flag and reducing iteration counts during testing.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/CONTRIBUTING.md#2025-04-16_snippet_6\n\nLANGUAGE: py\nCODE:\n```\nsmoke_test = ('CI' in os.environ)\n```\n\nLANGUAGE: py\nCODE:\n```\nfor epoch in range(200 if not smoke_test else 1):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Primitives Module in Python\nDESCRIPTION: This code snippet uses Sphinx autodoc directives to generate documentation for the pyro.primitives module. It includes all members, shows inheritance, and orders members by source code order.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/primitives.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: pyro.primitives\n    :members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Generating Zuko Module Documentation using Sphinx in reStructuredText\nDESCRIPTION: This snippet uses Sphinx's autodoc extension to automatically generate documentation for the Zuko module in Pyro's contrib package. It includes all members of the module in the generated documentation.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.zuko.rst#2025-04-16_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: pyro.contrib.zuko\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Creating Global Dimensions with pyro.to_data\nDESCRIPTION: This code snippet demonstrates how to create global dimensions that ignore the pyro.markov structure. It uses the dim_type parameter in pyro.to_data to specify a dimension as global, which remains active in nested markov and named contexts.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pyro.contrib.funsor.handlers.runtime import _DIM_STACK, DimType\n\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    funsor_particle_ids = funsor.Tensor(torch.arange(10), OrderedDict(n=funsor.Bint[10]))\n    tensor_particle_ids = pyro.to_data(funsor_particle_ids, dim_type=DimType.GLOBAL)\n    print(\"New global dimension: \", funsor_particle_ids.inputs, tensor_particle_ids.shape)\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Documentation for Minipyro Module\nDESCRIPTION: This reStructuredText snippet configures Sphinx to automatically generate documentation for the pyro.contrib.minipyro module. It includes all members, undocumented members, and the __call__ special method, while also showing inheritance information.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/contrib.minipyro.rst#2025-04-16_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: pyro.contrib.minipyro\n    :members:\n    :undoc-members:\n    :special-members: __call__\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Documenting Pyro Validation Functions in RST\nDESCRIPTION: ReStructuredText documentation for Pyro's validation-related utility functions that control and check validation states in inference operations.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/infer.util.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: pyro.infer.util.enable_validation\n.. autofunction:: pyro.infer.util.is_validation_enabled\n.. autofunction:: pyro.infer.util.validation_enabled\n```\n\n----------------------------------------\n\nTITLE: MCMC Class Documentation Reference\nDESCRIPTION: Sphinx documentation directive for the main MCMC class in pyro.infer.mcmc.api\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.infer.mcmc.api.MCMC\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Using pyro.markov with History Parameter\nDESCRIPTION: This snippet shows the usage of pyro.markov with the 'history' parameter. It demonstrates how the history parameter affects the number of previous pyro.markov contexts considered when building the mapping between names and dimensions.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/contrib_funsor_intro_i.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwith pyroapi.pyro_backend(\"contrib.funsor\"), handlers.named():\n    for i in pyro.markov(range(10), history=2):\n        x = pyro.to_data(funsor.Tensor(torch.tensor([0., 1.]), OrderedDict({\"x{}\".format(i): funsor.Bint[2]})))\n        print(\"Shape of x[{}]: \".format(str(i)), x.shape)\n```\n\n----------------------------------------\n\nTITLE: MCMC Utility Functions Documentation Reference\nDESCRIPTION: Sphinx documentation directives for MCMC utility functions including model initialization, diagnostics, and sample selection\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: pyro.infer.mcmc.util.initialize_model\n\n.. autofunction:: pyro.infer.mcmc.util.diagnostics\n\n.. autofunction:: pyro.infer.mcmc.util.select_samples\n```\n\n----------------------------------------\n\nTITLE: State Transition Probability Calculation in Hierarchical HMM\nDESCRIPTION: Mathematical formula showing how state transition probabilities are calculated using logistic regression with group and individual random effects, and various covariates.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/examples/mixed_hmm/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlogit(p(x[t,a,b] = state i | x[t-1,a,b] = state j)) =\n    (epsilon_G[a] + epsilon_I[a,b] + Z_I[a,b].T @ beta1 + Z_G[a].T @ beta2 + Z_T[t,a,b].T @ beta3)[i,j]\n```\n\n----------------------------------------\n\nTITLE: Creating an Incompatible Guide with Support Mismatch\nDESCRIPTION: Shows an incorrect guide implementation that uses a Normal distribution for a model with LogNormal, leading to potential numerical errors due to support mismatch.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef bad_guide():\n    loc = pyro.param(\"loc\", torch.tensor(0.0))\n    # Normal may sample x < 0\n    pyro.sample(\"x\", dist.Normal(loc, 1.0))  \n```\n\n----------------------------------------\n\nTITLE: MCMC Inference Implementation in Pyro\nDESCRIPTION: Example code showing how to perform Markov Chain Monte Carlo inference in Pyro, specifically for the Eight Schools hierarchical model. The code is linked to from the documentation but not directly included in the snippet.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/mcmc.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Referenced file path: ../../examples/eight_schools/mcmc.py\n```\n\n----------------------------------------\n\nTITLE: Referencing Pyro Distribution Transforms in Python\nDESCRIPTION: This snippet shows how to reference various distribution transforms in Pyro documentation using the autofunction directive. Each transform is documented separately.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/distributions.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autofunction:: pyro.distributions.transforms.polynomial\n\n.. autofunction:: pyro.distributions.transforms.radial\n\n.. autofunction:: pyro.distributions.transforms.spline\n\n.. autofunction:: pyro.distributions.transforms.spline_autoregressive\n\n.. autofunction:: pyro.distributions.transforms.spline_coupling\n\n.. autofunction:: pyro.distributions.transforms.sylvester\n```\n\n----------------------------------------\n\nTITLE: Importing PyroOptim from pyro.optim in Python\nDESCRIPTION: This snippet demonstrates how to import the PyroOptim class from the pyro.optim module. PyroOptim is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters in Pyro.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/optimization.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyro.optim import PyroOptim\n```\n\n----------------------------------------\n\nTITLE: Linear Regression Weight Initialization Examples\nDESCRIPTION: Demonstrates different approaches to initializing weights in linear regression, showing both simple isotropic priors and scale-specific priors.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/svi_part_iv.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npyro.sample(\"W\", dist.Normal(torch.zeros(2), torch.ones(2)))\n```\n\nLANGUAGE: python\nCODE:\n```\nprior_scale = torch.tensor([1.0e-5, 1.0e-2])\npyro.sample(\"W\", dist.Normal(torch.zeros(2), prior_scale))\n```\n\n----------------------------------------\n\nTITLE: Deep Kernel Learning Implementation in Pyro\nDESCRIPTION: Python script demonstrating Deep Kernel Learning implementation using Pyro's Gaussian Process module. The script is referenced from the contrib/gp/sv-dkl.py file in the Pyro examples directory.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/dkl.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../examples/contrib/gp/sv-dkl.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: Running Pytest for Data Generation\nDESCRIPTION: Shell command to generate data for the analysis using pytest on a specific test file.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tests/infer/enum_growth.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nGROWTH_SIZE=50 pytest -s tests/infer/test_enum.py -k growth\n```\n\n----------------------------------------\n\nTITLE: Mini-Pyro Core Implementation\nDESCRIPTION: The main implementation file of Mini-Pyro containing core functionality for probabilistic programming. Located in pyro/contrib/minipyro.py.\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/minipyro.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../../pyro/contrib/minipyro.py\n    :language: python\n```\n\n----------------------------------------\n\nTITLE: NUTS Class Documentation Reference\nDESCRIPTION: Sphinx documentation directive for the No-U-Turn Sampler (NUTS) implementation\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.infer.mcmc.NUTS\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: StreamingMCMC Class Documentation Reference\nDESCRIPTION: Sphinx documentation directive for the StreamingMCMC class in pyro.infer.mcmc.api\nSOURCE: https://github.com/pyro-ppl/pyro/blob/dev/docs/source/pyro.infer.mcmc.txt#2025-04-16_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: pyro.infer.mcmc.api.StreamingMCMC\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```"
  }
]
[
  {
    "owner": "lastmile-ai",
    "repo": "mcp-agent",
    "content": "TITLE: Basic Finder Agent in Python\nDESCRIPTION: This Python code defines a basic 'finder' agent that can read local files or fetch URLs using the `mcp-agent` framework. It initializes the MCPApp, creates an Agent with access to 'fetch' and 'filesystem' servers, attaches an OpenAI LLM, and demonstrates how to use the agent to perform file lookups and URL fetching. Requires the `mcp_agent` library.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\n\nfrom mcp_agent.app import MCPApp\nfrom mcp_agent.agents.agent import Agent\nfrom mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n\napp = MCPApp(name=\"hello_world_agent\")\n\nasync def example_usage():\n    async with app.run() as mcp_agent_app:\n        logger = mcp_agent_app.logger\n        # This agent can read the filesystem or fetch URLs\n        finder_agent = Agent(\n            name=\"finder\",\n            instruction=\"\"\"You can read local files or fetch URLs.\n                Return the requested information when asked.\"\"\",\n            server_names=[\"fetch\", \"filesystem\"], # MCP servers this Agent can use\n        )\n\n        async with finder_agent:\n            # Automatically initializes the MCP servers and adds their tools for LLM use\n            tools = await finder_agent.list_tools()\n            logger.info(f\"Tools available:\", data=tools)\n\n            # Attach an OpenAI LLM to the agent (defaults to GPT-4o)\n            llm = await finder_agent.attach_llm(OpenAIAugmentedLLM)\n\n            # This will perform a file lookup and read using the filesystem server\n            result = await llm.generate_str(\n                message=\"Show me what's in README.md verbatim\"\n            )\n            logger.info(f\"README.md contents: {result}\")\n\n            # Uses the fetch server to fetch the content from URL\n            result = await llm.generate_str(\n                message=\"Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents\"\n            )\n            logger.info(f\"Blog intro: {result}\")\n\n            # Multi-turn interactions by default\n            result = await llm.generate_str(\"Summarize that in a 128-char tweet\")\n            logger.info(f\"Tweet: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Agent Secrets in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the `mcp_agent.secrets.yaml` file.  It shows where to place your OpenAI API key and the websocket URL with the Base64-encoded configuration string for GitHub authentication. Ensure that you replace `openai_api_key` with your actual OpenAI API key and `BASE64_ENCODED_CONFIG` with your Base64 encoded JSON.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_websockets/README.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nopenai:\n  api_key: openai_api_key\n\nmcp:\n  servers:\n    smithery-github:\n      url: \"wss://server.smithery.ai/@smithery-ai/github/ws?config=BASE64_ENCODED_CONFIG\"\n```\n\n----------------------------------------\n\nTITLE: Orchestrator Example in Python\nDESCRIPTION: This example shows how to use the Orchestrator workflow to generate a plan, assign tasks to sub-agents, and synthesize the results. It leverages AnthropicAugmentedLLM and available agents such as finder_agent, writer_agent, proofreader, fact_checker, and style_enforcer.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfinder_agent = Agent(name=\"finder\", server_names=[\"fetch\", \"filesystem\"])\nwriter_agent = Agent(name=\"writer\", server_names=[\"filesystem\"])\nproofreader = Agent(name=\"proofreader\", ...)\nfact_checker = Agent(name=\"fact_checker\", ...)\nstyle_enforcer = Agent(name=\"style_enforcer\", instructions=\"Use APA style guide from ...\", server_names=[\"fetch\"])\n\norchestrator = Orchestrator(\n    llm_factory=AnthropicAugmentedLLM,\n    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],\n)\n\ntask = \"Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects.\"\nresult = await orchestrator.generate_str(task, RequestParams(model=\"gpt-4o\"))\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Swarm Agent Workflow Example in Python\nDESCRIPTION: This code snippet demonstrates how to define and use a Swarm workflow with multiple SwarmAgent instances. It shows how to initialize agents, define context variables, and execute the workflow with a test input. It assumes the existence of `SwarmAgent` and `AnthropicSwarm` classes.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntriage_agent = SwarmAgent(...)\nflight_mod_agent = SwarmAgent(...)\nlost_baggage_agent = SwarmAgent(...)\n\n# The triage agent decides whether to route to flight_mod_agent or lost_baggage_agent\nswarm = AnthropicSwarm(agent=triage_agent, context_variables={...})\n\ntest_input = \"My bag was not delivered!\"\nresult = await swarm.generate_str(test_input)\nprint(\"Result:\", result)\n```\n\n----------------------------------------\n\nTITLE: EvaluatorOptimizerLLM Example in Python\nDESCRIPTION: This example demonstrates the use of EvaluatorOptimizerLLM to refine a response using an optimizer and an evaluator agent. It uses OpenAIAugmentedLLM as the llm_factory and continues iterating until the minimum quality bar is reached (QualityRating.EXCELLENT).\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\noptimizer = Agent(name=\"cover_letter_writer\", server_names=[\"fetch\"], instruction=\"Generate a cover letter ...\")\nevaluator = Agent(name=\"critiquer\", instruction=\"Evaluate clarity, specificity, relevance...\")\n\nllm = EvaluatorOptimizerLLM(\n    optimizer=optimizer,\n    evaluator=evaluator,\n    llm_factory=OpenAIAugmentedLLM,\n    min_rating=QualityRating.EXCELLENT, # Keep iterating until the minimum quality bar is reached\n)\n\nresult = await eo_llm.generate_str(\"Write a job cover letter for an AI framework developer role at LastMile AI.\")\nprint(\"Final refined cover letter:\", result)\n```\n\n----------------------------------------\n\nTITLE: Google API Configuration (YAML)\nDESCRIPTION: This YAML snippet shows how to configure the Google API key and optional VertexAI settings within the `mcp_agent.secrets.yaml` or `mcp_agent.config.yaml` file. The `api_key` is required, while `vertexai`, `project`, and `location` are optional and specific to VertexAI usage. The `api_key` can also be set via the environment variable `GOOGLE_API_KEY`.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_google_agent/README.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ngoogle:\n  api_key: \"your-google-api-key\"\n  vertexai: false\n  # Include these if using VertexAI\n  # project: \"your-google-cloud-project\"\n  # location: \"us-central1\"\n```\n\n----------------------------------------\n\nTITLE: AugmentedLLM Example in Python\nDESCRIPTION: This example demonstrates how to use an AugmentedLLM with an Anthropic model to interact with an agent that has access to filesystem and fetch tools. It shows how to generate a string output from a message and how to conduct a multi-turn conversation.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom mcp_agent.agents.agent import Agent\nfrom mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM\n\nfinder_agent = Agent(\n    name=\"finder\",\n    instruction=\"You are an agent with filesystem + fetch access. Return the requested file or URL contents.\",\n    server_names=[\"fetch\", \"filesystem\"],\n)\n\nasync with finder_agent:\n   llm = await finder_agent.attach_llm(AnthropicAugmentedLLM)\n\n   result = await llm.generate_str(\n      message=\"Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents\",\n      # Can override model, tokens and other defaults\n   )\n   logger.info(f\"Result: {result}\")\n\n   # Multi-turn conversation\n   result = await llm.generate_str(\n      message=\"Summarize those paragraphs in a 128 character tweet\",\n   )\n   logger.info(f\"Result: {result}\")\n```\n\n----------------------------------------\n\nTITLE: LLMRouter Example in Python\nDESCRIPTION: This example demonstrates the usage of LLMRouter for routing a request to different agents or functions based on relevance. It uses OpenAIAugmentedLLM as the LLM and routes to a finder agent, writer agent, or a print_hello_world function.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef print_hello_world:\n     print(\"Hello, world!\")\n\nfinder_agent = Agent(name=\"finder\", server_names=[\"fetch\", \"filesystem\"])\nwriter_agent = Agent(name=\"writer\", server_names=[\"filesystem\"])\n\nllm = OpenAIAugmentedLLM()\nrouter = LLMRouter(\n    llm=llm,\n    agents=[finder_agent, writer_agent],\n    functions=[print_hello_world],\n)\n\nresults = await router.route( # Also available: route_to_agent, route_to_server\n    request=\"Find and print the contents of README.md verbatim\",\n    top_k=1\n)\nchosen_agent = results[0].result\nasync with chosen_agent:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Persistent MCP Server Connections in Python\nDESCRIPTION: This code demonstrates how to establish persistent connections to MCP servers using the `connect` and `disconnect` functions. A `try...finally` block ensures that the server is disconnected even if an exception occurs during usage. This pattern is suitable for multi-step workflows where the server needs to remain active between calls.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp_agent.mcp.gen_client import connect, disconnect\n\nfetch_client = None\ntry:\n     fetch_client = connect(\"fetch\")\n     result = await fetch_client.list_tools()\nfinally:\n     disconnect(\"fetch\")\n```\n\n----------------------------------------\n\nTITLE: Evaluator-Optimizer Workflow Composition in Python\nDESCRIPTION: This snippet demonstrates how to compose an Evaluator-Optimizer workflow as the planner LLM inside an Orchestrator workflow. It initializes `Agent` instances for optimization and evaluation and then integrates them into an `EvaluatorOptimizerLLM`. This `planner_llm` is then passed to the `Orchestrator`.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\noptimizer = Agent(name=\"plan_optimizer\", server_names=[...], instruction=\"Generate a plan given an objective ...\")\nevaluator = Agent(name=\"plan_evaluator\", instruction=\"Evaluate logic, ordering and precision of plan......\")\n\nplanner_llm = EvaluatorOptimizerLLM(\n    optimizer=optimizer,\n    evaluator=evaluator,\n    llm_factory=OpenAIAugmentedLLM,\n    min_rating=QualityRating.EXCELLENT,\n)\n\norchestrator = Orchestrator(\n    llm_factory=AnthropicAugmentedLLM,\n    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],\n    planner=planner_llm # It's that simple\n)\n\n...\n```\n\n----------------------------------------\n\nTITLE: MCPAggregator for Combining Multiple MCP Servers in Python\nDESCRIPTION: This code demonstrates how to use the `MCPAggregator` to create a single interface for interacting with multiple MCP servers. It combines the tools exposed by the 'fetch' and 'filesystem' servers into a single aggregated server. The aggregator also supports namespacing to invoke specific servers and tools.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp_agent.mcp.mcp_aggregator import MCPAggregator\n\naggregator = await MCPAggregator.create(server_names=[\"fetch\", \"filesystem\"])\n\nasync with aggregator:\n   # combined list of tools exposed by 'fetch' and 'filesystem' servers\n   tools = await aggregator.list_tools()\n\n   # namespacing -- invokes the 'fetch' server to call the 'fetch' tool\n   fetch_result = await aggregator.call_tool(name=\"fetch-fetch\", arguments={\"url\": \"https://www.anthropic.com/research/building-effective-agents\"})\n\n   # no namespacing -- first server in the aggregator exposing that tool wins\n   read_file_result = await aggregator.call_tool(name=\"read_file\", arguments={})\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Server in Claude Desktop (JSON)\nDESCRIPTION: This JSON snippet configures the MCP server within the Claude Desktop application. It specifies the command to execute (`uv`), arguments including the directory path and the server script to run (`server.py`).  The `command` may need the full path to the `uv` executable. Ensure `/ABSOLUTE/PATH/TO/PARENT/FOLDER` is replaced with the actual path.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_agent_server/README.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"mcp-agent\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: ParallelLLM Example in Python\nDESCRIPTION: This example showcases the use of ParallelLLM to fan-out tasks to multiple sub-agents (proofreader, fact_checker, style_enforcer) and then fan-in the results using a grader agent. The llm_factory is set to OpenAIAugmentedLLM.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nproofreader = Agent(name=\"proofreader\", instruction=\"Review grammar...\")\nfact_checker = Agent(name=\"fact_checker\", instruction=\"Check factual consistency...\")\nstyle_enforcer = Agent(name=\"style_enforcer\", instruction=\"Enforce style guidelines...\")\n\ngrader = Agent(name=\"grader\", instruction=\"Combine feedback into a structured report.\")\n\nparallel = ParallelLLM(\n    fan_in_agent=grader,\n    fan_out_agents=[proofreader, fact_checker, style_enforcer],\n    llm_factory=OpenAIAugmentedLLM,\n)\n\nresult = await parallel.generate_str(\"Student short story submission: ...\", RequestParams(model=\"gpt4-o\"))\n```\n\n----------------------------------------\n\nTITLE: MCP Server Lifecycle Management with gen_client in Python\nDESCRIPTION: This code shows how to manage the lifecycle of an MCP server using the `gen_client` async context manager.  The `gen_client` function initializes and shuts down the server automatically within the `async with` block.  The code interacts with the server (e.g., calls `list_tools()`) while it is running.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp_agent.mcp.gen_client import gen_client\n\nasync with gen_client(\"fetch\") as fetch_client:\n    # Fetch server is initialized and ready to use\n    result = await fetch_client.list_tools()\n\n# Fetch server is automatically disconnected/shutdown\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Configuration (YAML)\nDESCRIPTION: This YAML snippet configures the Azure OpenAI inference endpoint, including the default model, API key, endpoint URL, and API version. The API version is crucial for structured outputs.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_azure_agent/README.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n# mcp_agent.secrets.yaml\n\n# Azure OpenAI inference endpoint\nazure:\n    default_model: gpt-4o-mini\n    api_key: changethis\n    endpoint: https://<your-resource-name>.cognitiveservices.azure.com/openai/deployments/<your-deployment-name>\n    api_version: \"2025-01-01-preview\" # Azure OpenAI api-version. See https://aka.ms/azsdk/azure-ai-inference/azure-openai-api-versions\n```\n\n----------------------------------------\n\nTITLE: Installing mcp-agent with uv package manager\nDESCRIPTION: This command demonstrates how to install the `mcp-agent` package using the `uv` package manager for Python projects.  `uv` is recommended within the documentation.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv add \"mcp-agent\"\n```\n\n----------------------------------------\n\nTITLE: mcp_agent configuration YAML\nDESCRIPTION: This YAML configuration file defines settings for the `mcp-agent` application, including the execution engine, logger, MCP server configurations, and OpenAI model settings. Secrets like API keys are expected to be in a separate `mcp_agent.secrets.yaml` file.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nexecution_engine: asyncio\nlogger:\n  transports: [console]  # You can use [file, console] for both\n  level: debug\n  path: \"logs/mcp-agent.jsonl\"  # Used for file transport\n  # For dynamic log filenames:\n  # path_settings:\n  #   path_pattern: \"logs/mcp-agent-{unique_id}.jsonl\"\n  #   unique_id: \"timestamp\"  # Or \"session_id\"\n  #   timestamp_format: \"%Y%m%d_%H%M%S\"\n\nmcp:\n  servers:\n    fetch:\n      command: \"uvx\"\n      args: [\"mcp-server-fetch\"]\n    filesystem:\n      command: \"npx\"\n      args:\n        [\n          \"-y\",\n          \"@modelcontextprotocol/server-filesystem\",\n          \"<add_your_directories>\",\n        ]\n\nopenai:\n  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored\n  default_model: gpt-4o\n```\n\n----------------------------------------\n\nTITLE: Running Qdrant Server with Docker\nDESCRIPTION: This command runs the Qdrant server locally using Docker. It maps port 6333 and mounts a local directory for persistent storage of Qdrant data. The volume mount ensures that the data persists between container restarts.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/streamlit_mcp_rag_agent/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant\n```\n\n----------------------------------------\n\nTITLE: Ollama Model Pull\nDESCRIPTION: This snippet pulls the required llama3.2:3b and llama3.1:8b models from Ollama. These models are used by the MCP agent to process queries about files and URLs. Ensure Ollama is installed before running these commands.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_ollama_agent/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3.2:3b\n\nollama run llama3.1:8b\n```\n\n----------------------------------------\n\nTITLE: Running MCP Server and Client (Bash)\nDESCRIPTION: This Bash command executes both the MCP server (`server.py`) and the client (`client.py`) using the `uv run` command. It initiates communication between the server and client processes for MCP operations. Ensure that `server.py` and `client.py` are in the current working directory or the specified directory.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_agent_server/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv run client.py server.py\n```\n\n----------------------------------------\n\nTITLE: Running the Finder Agent (Bash)\nDESCRIPTION: This bash command navigates to the example directory for the MCP basic Azure agent and executes the `main.py` script using `uv run`. The `--extra azure` flag likely provides additional configurations or dependencies specific to the Azure environment.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_azure_agent/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/mcp_basic_azure_agent\n\nuv run --extra azure main.py\n```\n\n----------------------------------------\n\nTITLE: Human Input Configuration for SwarmAgent in Python\nDESCRIPTION: This code shows how to configure a `SwarmAgent` to request human input during a workflow using a `human_input_callback`.  Specifically, it sets the `human_input_callback` to `console_input_callback`, which will prompt the user for input from the console when the agent requests it. This allows the LLM to pause execution and get user feedback mid-workflow.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp_agent.human_input.handler import console_input_callback\n\nlost_baggage = SwarmAgent(\n    name=\"Lost baggage traversal\",\n    instruction=lambda context_variables: f\"\"\"\n        {\n        FLY_AIR_AGENT_PROMPT.format(\n            customer_context=context_variables.get(\"customer_context\", \"None\"),\n            flight_context=context_variables.get(\"flight_context\", \"None\"),\n        )\n    }\\n Lost baggage policy: policies/lost_baggage_policy.md\"\"\",\n    functions=[\n        escalate_to_agent,\n        initiate_baggage_search,\n        transfer_to_triage,\n        case_resolved,\n    ],\n    server_names=[\"fetch\", \"filesystem\"],\n    human_input_callback=console_input_callback, # Request input from the console\n)\n```\n\n----------------------------------------\n\nTITLE: Running Streamlit Application\nDESCRIPTION: These commands install the required dependencies using `uv pip` and then run the Streamlit application `main.py`. The `requirements.txt` file lists the necessary packages for the application to function correctly.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/streamlit_mcp_rag_agent/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -r requirements.txt\nuv run streamlit run main.py\n```\n\n----------------------------------------\n\nTITLE: Run Finder Agent (Bash)\nDESCRIPTION: This bash command navigates to the example directory and executes the `main.py` script using `uv run`. This assumes that `uv` is installed and configured to run Python scripts. It runs the 'Finder' agent, which has access to the `fetch` MCP server.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_google_agent/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/mcp_basic_google_agent\n\nuv run main.py\n```\n\n----------------------------------------\n\nTITLE: Run the Application (uv run)\nDESCRIPTION: Executes the `main.py` script, which is the entry point for the MCP Agent.  The `--owner`, `--repo`, and `--channel` arguments are used to specify the GitHub repository owner, repository name, and Slack channel for the agent to monitor and send summaries to, respectively.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_github_to_slack_agent/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv run main.py --owner <github-owner> --repo <repository-name> --channel <slack-channel>\n```\n\n----------------------------------------\n\nTITLE: Encoding GitHub Personal Access Token in JSON\nDESCRIPTION: This JSON snippet shows the structure for encoding your GitHub Personal Access Token (PAT). Replace `YOUR_GITHUB_PAT` with your actual PAT obtained from GitHub. This JSON needs to be Base64-encoded for use in the `mcp_agent.secrets.yaml` configuration.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_websockets/README.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"githubPersonalAccessToken\": \"YOUR_GITHUB_PAT\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Streamlit with uv\nDESCRIPTION: This bash snippet installs the necessary requirements and then runs the Streamlit application using uv.  It assumes that `requirements.txt` contains the dependencies for the Streamlit app and the main script is `main.py`.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/streamlit_mcp_basic_agent/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -r requirements.txt\nuv run streamlit run main.py\n```\n\n----------------------------------------\n\nTITLE: Running Marimo Notebook with API Key\nDESCRIPTION: This command executes the Marimo notebook in edit mode with sandboxing enabled, using the provided OpenAI API key. The 'uvx' command likely refers to a tool or environment manager used to run the Marimo application. The API key is passed as an environment variable.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/marimo_mcp_basic_agent/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<your-api-key> uvx marimo edit --sandbox notebook.py\n```\n\n----------------------------------------\n\nTITLE: Running mcp_basic_agent example\nDESCRIPTION: These commands outline the steps to run an example application using `mcp-agent`. It involves navigating to the example directory, copying the secrets file, and executing the main script using `uv`.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/mcp_basic_agent # Or any other example\ncp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml # Update API keys\nuv run main.py\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies (uv sync)\nDESCRIPTION: Installs the required Python dependencies for the project using `uv sync`. The `--dev` flag likely installs development dependencies as well.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_github_to_slack_agent/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --dev\n```\n\n----------------------------------------\n\nTITLE: Pulling Qdrant Docker Image\nDESCRIPTION: This command pulls the latest Qdrant image from Dockerhub. It ensures that the Qdrant vector database server can be run locally for the RAG agent example.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/streamlit_mcp_rag_agent/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull qdrant/qdrant\n```\n\n----------------------------------------\n\nTITLE: Running the Bedrock Finder Agent\nDESCRIPTION: This command navigates to the agent's directory and executes the main script to run the \"Finder\" agent. It assumes that the necessary AWS credentials and configurations are already set up. 'uv' is used to run the python script 'main.py'.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_bedrock_agent/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/mcp_basic_bedrock_agent\n\nuv run main.py\n```\n\n----------------------------------------\n\nTITLE: Running interactive mode with uv\nDESCRIPTION: This command executes the interactive mode of the LLM selector using the `uv` command-line tool. The `interactive.py` script is the entry point for the interactive mode.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_model_selector/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv run interactive.py\n```\n\n----------------------------------------\n\nTITLE: Run example.py script to run an example\nDESCRIPTION: This command executes the example.py script to run an example in debug mode.  It configures the virtual environment, installs dependencies and runs the specified example. Dependencies: Python, uv.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv run scripts/example.py run <example_name> --debug\n```\n\n----------------------------------------\n\nTITLE: Declaring mcp-agent dependency\nDESCRIPTION: This snippet defines the mcp-agent dependency, linking it to a local project root. It uses the 'file://' protocol to specify the path to the local mcp-agent project root, allowing for direct inclusion of the local code.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/workflow_router/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nmcp-agent @ file://../../  # Link to the local mcp-agent project root\n```\n\n----------------------------------------\n\nTITLE: VS Code settings.json configuration\nDESCRIPTION: This JSON snippet provides recommended settings for VS Code, including formatting on save, default formatters for Python and other languages, and schema associations for YAML configuration files. It enhances the development experience by automating code formatting and validation.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"editor.formatOnSave\": true,\n  \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n    \"editor.formatOnSave\": true,\n    \"editor.rulers\": []\n  },\n  \"yaml.schemas\": {\n    \"https://raw.githubusercontent.com/lastmile-ai/mcp-agent/main/schema/mcp-agent.config.schema.json\": [\n      \"mcp-agent.config.yaml\",\n      \"mcp_agent.config.yaml\",\n      \"mcp-agent.secrets.yaml\",\n      \"mcp_agent.secrets.yaml\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Run promptify.py script\nDESCRIPTION: This command executes the promptify.py script to bundle the mcp-agent repository content into a single markdown file.  It includes files matching the provided regex patterns and excludes others. Dependencies: Python, uv.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv run scripts/promptify.py -i \"**/agents/**\" -i \"**/context.py\" -x \"**/app.py\"\n```\n\n----------------------------------------\n\nTITLE: MCPConnectionManager for Server Connections in Python\nDESCRIPTION: This code shows how to use the `MCPConnectionManager` for managing MCP server connections with finer-grained control. The `MCPConnectionManager` is initialized with the server registry from the current context, allowing it to manage the lifecycle of MCP servers. It allows for re-using connections.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp_agent.context import get_current_context\nfrom mcp_agent.mcp.mcp_connection_manager import MCPConnectionManager\n\ncontext = get_current_context()\nconnection_manager = MCPConnectionManager(context.server_registry)\n\nasync with connection_manager:\n    fetch_client = await connection_manager.get_server(\"fetch\") # Initializes fetch server\n    result = fetch_client.list_tool()\n    fetch_client2 = await connection_manager.get_server(\"fetch\") # Reuses same server connection\n\n# All servers managed by connection manager are automatically disconnected/shut down\n```\n\n----------------------------------------\n\nTITLE: Run a specific example\nDESCRIPTION: This command runs the `workflow_orchestrator_worker` example in debug mode using the `example.py` script. It prepares the virtual environment, installs dependencies and executes the specified example. Dependencies: Python, uv.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv run scripts/example.py run workflow_orchestrator_worker --debug\n```\n\n----------------------------------------\n\nTITLE: Run format.py script\nDESCRIPTION: This command runs the format.py script to automatically format code according to project style guidelines. Dependencies: Python, uv.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuv run scripts/format.py\n```\n\n----------------------------------------\n\nTITLE: Running script mode with uv\nDESCRIPTION: This command executes the script mode of the LLM selector using the `uv` command-line tool. The `main.py` script is the entry point for the script mode.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_model_selector/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv run main.py\n```\n\n----------------------------------------\n\nTITLE: Defining Package Dependencies\nDESCRIPTION: This snippet defines the required Python packages and their minimum versions.  It ensures the mcp-agent project has access to necessary libraries like mcp-agent, anthropic, and instructor with specific version constraints. These constraints maintain compatibility and prevent unexpected behavior due to version conflicts.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_github_to_slack_agent/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nmcp-agent>=0.0.14\nanthropic>=0.48.0\ninstructor[anthropic]>=1.7.2\n```\n\n----------------------------------------\n\nTITLE: MCP Server Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure an MCP server within the `mcp_agent.config.yaml` file. It defines a server named 'fetch' with its command, arguments, and description. This configuration is used by mcp-agent to manage and connect to MCP servers.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nmcp:\n  servers:\n    fetch:\n      command: \"uvx\"\n      args: [\"mcp-server-fetch\"]\n      description: \"Fetch content at URLs from the world wide web\"\n```\n\n----------------------------------------\n\nTITLE: Installing mcp-agent with pip\nDESCRIPTION: This command demonstrates how to install the `mcp-agent` package using `pip`, the standard package installer for Python. This is provided as an alternative installation method.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mcp-agent\n```\n\n----------------------------------------\n\nTITLE: Update Test Fixtures (Python)\nDESCRIPTION: This Python snippet demonstrates how to update the expected output test fixtures. It imports the `update_test_fixtures` function from the `test_event_progress` module and calls it to regenerate the expected output based on the current event processing logic.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/tests/fixture/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tests.test_event_progress import update_test_fixtures\nupdate_test_fixtures()\n```\n\n----------------------------------------\n\nTITLE: Project Dependency Declaration\nDESCRIPTION: Declares the core framework dependency using a file path linking to the local mcp-agent project root.  This indicates that the current project relies on a locally available version of the mcp-agent framework. It also lists additional dependencies, 'anthropic' and 'openai'.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/workflow_parallel/requirements.txt#_snippet_0\n\nLANGUAGE: TEXT\nCODE:\n```\nmcp-agent @ file://../../  # Link to the local mcp-agent project root\n\n# Additional dependencies specific to this example\nanthropic\nopenai\n```\n\n----------------------------------------\n\nTITLE: Serving Marimo Notebook as Read-Only App\nDESCRIPTION: This command runs the Marimo notebook as a read-only application with sandboxing enabled, using the provided OpenAI API key. The 'uvx' command likely refers to a tool or environment manager used to run the Marimo application. The API key is passed as an environment variable.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/marimo_mcp_basic_agent/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<your-api-key> uvx marimo run --sandbox notebook.py\n```\n\n----------------------------------------\n\nTITLE: Azure AI Configuration (YAML)\nDESCRIPTION: This YAML snippet configures the Azure AI inference endpoint, including the default model, API key, and endpoint URL. This is an alternative configuration for Azure AI, distinct from Azure OpenAI.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_azure_agent/README.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n# Azure AI inference endpoint\nazure:\n    default_model: DeepSeek-V3\n    api_key: changethis\n    endpoint: https://<your-resource-name>.services.ai.azure.com/models\n```\n\n----------------------------------------\n\nTITLE: Defining Project Dependency\nDESCRIPTION: This snippet defines a project dependency by linking to a local mcp-agent project root. It utilizes the `file://` protocol to specify the local path to the mcp-agent project.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/workflow_orchestrator_worker/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmcp-agent @ file://../../  # Link to the local mcp-agent project root\n```\n\n----------------------------------------\n\nTITLE: Defining Additional Dependencies\nDESCRIPTION: This snippet defines additional dependencies for the project, which include 'anthropic' and 'openai'. These dependencies are likely external libraries or packages required for the project's functionality.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/workflow_orchestrator_worker/requirements.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nanthropic\nopenai\n```\n\n----------------------------------------\n\nTITLE: Run lint.py script\nDESCRIPTION: This command runs the lint.py script to check code style and apply automatic fixes. Dependencies: Python, uv.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuv run scripts/lint.py --fix\n```\n\n----------------------------------------\n\nTITLE: Declaring additional dependencies\nDESCRIPTION: This snippet declares additional dependencies required for the example. These include 'anthropic' and 'openai', which are likely libraries or packages used by the agent for specific functionalities such as interacting with the Anthropic and OpenAI APIs.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/workflow_router/requirements.txt#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nanthropic\nopenai\n```\n\n----------------------------------------\n\nTITLE: MCP Agent Dependency Declaration\nDESCRIPTION: Declares a dependency on the local mcp-agent project. This line links the current project to the mcp-agent project root, enabling the use of its functionalities. The path 'file://../../' specifies the relative location of the mcp-agent project.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_root_test/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nmcp-agent @ file://../../\n```\n\n----------------------------------------\n\nTITLE: Anthropic Dependency Declaration\nDESCRIPTION: Declares a dependency on the Anthropic library. This enables the project to utilize Anthropic's functionalities. It is assumed that a package manager like pip will handle the installation.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_root_test/requirements.txt#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nanthropic\n```\n\n----------------------------------------\n\nTITLE: MCP Agent Local Dependency\nDESCRIPTION: This line establishes a dependency on the mcp-agent framework, linking it to the local project root.  It allows the example to use the core functionalities of the mcp-agent without requiring it to be installed from a remote repository. The path 'file://../../' indicates that the mcp-agent project is located two directories up from the current file.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_sse_with_headers/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nmcp-agent @ file://../../\n```\n\n----------------------------------------\n\nTITLE: OpenAI Dependency\nDESCRIPTION: This line declares a dependency on the OpenAI library. This likely indicates that the example utilizes OpenAI's services or models for specific functionalities. The OpenAI library needs to be installed separately, typically via a package manager like pip.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_sse_with_headers/requirements.txt#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nopenai\n```\n\n----------------------------------------\n\nTITLE: Run example.py script to clean an example\nDESCRIPTION: This command executes the example.py script to clean up the virtual environment for a specific example. Dependencies: Python, uv.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv run scripts/example.py clean <example_name>\n```\n\n----------------------------------------\n\nTITLE: MCP Agent Core Framework Dependency\nDESCRIPTION: Defines the core framework dependency for the mcp-agent project. It links to the local mcp-agent project root using a file path.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_slack_agent/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmcp-agent @ file://../../  # Link to the local mcp-agent project root\n```\n\n----------------------------------------\n\nTITLE: MCP Agent Additional Dependencies\nDESCRIPTION: Specifies additional dependencies required for this example, including anthropic and openai. These are likely libraries or packages used within the mcp-agent project.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_slack_agent/requirements.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nanthropic\nopenai\n```\n\n----------------------------------------\n\nTITLE: MCP Agent Core Framework Dependency\nDESCRIPTION: This snippet defines the core framework dependency, mcp-agent, by linking to the local mcp-agent project root.  This allows the example application to use the mcp-agent framework's functionalities. It uses a file path to specify the dependency.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_hello_world/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmcp-agent @ file://../../  # Link to the local mcp-agent project root\n```\n\n----------------------------------------\n\nTITLE: Generate New Log File (Bash)\nDESCRIPTION: This bash script generates a new log file for testing. It first navigates to the examples directory for the basic agent, removes any existing log file, runs the agent with a sample query, and then copies the newly generated log file to the test fixture directory.\nSOURCE: https://github.com/lastmile-ai/mcp-agent/blob/main/tests/fixture/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/mcp_basic_agent\nrm -f mcp-agent.jsonl  # Start with a clean log file\nuv run python main.py \"What is the timestamp in different timezones?\"\ncp mcp-agent.jsonl ../../tests/fixture/mcp_basic_agent_20250131_205604.jsonl\n```"
  }
]
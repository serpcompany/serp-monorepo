[
  {
    "owner": "pytorch",
    "repo": "rl",
    "content": "TITLE: Asynchronous Data Collection Loop\nDESCRIPTION: Shows implementation of training loop using MultiaSyncDataCollector for parallel data collection across multiple devices. Includes policy updates and optimization steps.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nenv_make = lambda: GymEnv(\"Pendulum-v1\", from_pixels=True)\ncollector = MultiaSyncDataCollector(\n    [env_make, env_make],\n    policy=policy,\n    devices=[\"cuda:0\", \"cuda:0\"],\n    total_frames=10000,\n    frames_per_batch=50,\n    ...\n)\nfor i, tensordict_data in enumerate(collector):\n    loss = loss_module(tensordict_data)\n    loss.backward()\n    optim.step()\n    optim.zero_grad()\n    collector.update_policy_weights_()\n```\n\n----------------------------------------\n\nTITLE: Implementing PPO Training with TensorDict\nDESCRIPTION: Complete implementation of PPO (Proximal Policy Optimization) training using TensorDict and TorchRL components. Includes model definition, data collection, and training loop with less than 100 lines of code.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom tensordict.nn import TensorDictModule\nfrom tensordict.nn.distributions import NormalParamExtractor\nfrom torch import nn\n\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data.replay_buffers import TensorDictReplayBuffer, \\\n  LazyTensorStorage, SamplerWithoutReplacement\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.modules import ProbabilisticActor, ValueOperator, TanhNormal\nfrom torchrl.objectives import ClipPPOLoss\nfrom torchrl.objectives.value import GAE\n\nenv = GymEnv(\"Pendulum-v1\") \nmodel = TensorDictModule(\n  nn.Sequential(\n      nn.Linear(3, 128), nn.Tanh(),\n      nn.Linear(128, 128), nn.Tanh(),\n      nn.Linear(128, 128), nn.Tanh(),\n      nn.Linear(128, 2),\n      NormalParamExtractor()\n  ),\n  in_keys=[\"observation\"],\n  out_keys=[\"loc\", \"scale\"]\n)\ncritic = ValueOperator(\n  nn.Sequential(\n      nn.Linear(3, 128), nn.Tanh(),\n      nn.Linear(128, 128), nn.Tanh(),\n      nn.Linear(128, 128), nn.Tanh(),\n      nn.Linear(128, 1),\n  ),\n  in_keys=[\"observation\"],\n)\nactor = ProbabilisticActor(\n  model,\n  in_keys=[\"loc\", \"scale\"],\n  distribution_class=TanhNormal,\n  distribution_kwargs={\"low\": -1.0, \"high\": 1.0},\n  return_log_prob=True\n  )\nbuffer = TensorDictReplayBuffer(\n  storage=LazyTensorStorage(1000),\n  sampler=SamplerWithoutReplacement(),\n  batch_size=50,\n  )\ncollector = SyncDataCollector(\n  env,\n  actor,\n  frames_per_batch=1000,\n  total_frames=1_000_000,\n)\nloss_fn = ClipPPOLoss(actor, critic)\nadv_fn = GAE(value_network=critic, average_gae=True, gamma=0.99, lmbda=0.95)\noptim = torch.optim.Adam(loss_fn.parameters(), lr=2e-4)\n\nfor data in collector:  # collect data\n  for epoch in range(10):\n      adv_fn(data)  # compute advantage\n      buffer.extend(data)\n      for sample in buffer:  # consume data\n          loss_vals = loss_fn(sample)\n          loss_val = sum(\n              value for key, value in loss_vals.items() if\n              key.startswith(\"loss\")\n              )\n          loss_val.backward()\n          optim.step()\n          optim.zero_grad()\n  print(f\"avg reward: {data['next', 'reward'].mean().item(): 4.4f}\")\n```\n\n----------------------------------------\n\nTITLE: PPO Loss Calculation Implementation\nDESCRIPTION: Implements the core PPO (Proximal Policy Optimization) loss calculation. Handles log probability manipulation, advantage computation, and clipped objective calculation with epsilon bounds.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef ppo(tensordict):\n    prev_log_prob = tensordict.select(*log_prob_keys)\n    action = tensordict.select(*action_keys)\n    new_log_prob = dist.log_prob(action)\n    log_weight = new_log_prob - prev_log_prob\n    advantage = tensordict.get(\"advantage\") # computed by GAE earlier\n    # attempt to map shape\n    log_weight.batch_size = advantage.batch_size[:-1]\n    log_weight = sum(log_weight.sum(dim=\"feature\").values(True, True)) # get a single tensor of log_weights\n    return minimum(log_weight.exp() * advantage, log_weight.exp().clamp(1-eps, 1+eps) * advantage)\n```\n\n----------------------------------------\n\nTITLE: Basic TorchRL Environment Rollout Pattern\nDESCRIPTION: Demonstrates how to execute a non-stopping rollout with TorchRL environments by using the step_and_maybe_reset method. This pattern allows for efficient sequential execution of environment steps while automatically handling resets when episodes terminate.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndata_ = env.reset()\nresult = []\nfor i in range(N):\n    data, data_ = env.step_and_maybe_reset(data_)\n    result.append(data)\n\nresult = torch.stack(result)\n```\n\n----------------------------------------\n\nTITLE: Environment Transforms and Device Management\nDESCRIPTION: Demonstrates environment transformation pipeline including image processing and normalization, with device management and transform manipulation capabilities.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nenv_make = lambda: GymEnv(\"Pendulum-v1\", from_pixels=True)\nenv_base = ParallelEnv(4, env_make, device=\"cuda:0\")  # creates 4 envs in parallel\nenv = TransformedEnv(\n    env_base,\n    Compose(\n        ToTensorImage(),\n        ObservationNorm(loc=0.5, scale=1.0)),  # executes the transforms once and on device\n)\ntensordict = env.reset()\nassert tensordict.device == torch.device(\"cuda:0\")\n\nenv.insert_transform(0, NoopResetEnv())  # inserts the NoopResetEnv transform at the index 0\n\ntransform = env.transform[1]  # gathers the second transform of the list\nparent_env = transform.parent  # returns the base environment of the second transform, i.e. the base env + the first transform\n```\n\n----------------------------------------\n\nTITLE: Environment Rollout Comparison with TensorDict\nDESCRIPTION: Comparison between traditional environment rollout and TensorDict-based implementation, showing how TensorDict simplifies data handling and state management.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- obs, done = env.reset()\n+ tensordict = env.reset()\npolicy = SafeModule(\n    model,\n    in_keys=[\"observation_pixels\", \"observation_vector\"],\n    out_keys=[\"action\"],\n)\nout = []\nfor i in range(n_steps):\n-     action, log_prob = policy(obs)\n-     next_obs, reward, done, info = env.step(action)\n-     out.append((obs, next_obs, action, log_prob, reward, done))\n-     obs = next_obs\n+     tensordict = policy(tensordict)\n+     tensordict = env.step(tensordict)\n+     out.append(tensordict)\n+     tensordict = step_mdp(tensordict)  # renames next_observation_* keys to observation_*\n- obs, next_obs, action, log_prob, reward, done = [torch.stack(vals, 0) for vals in zip(*out)]\n+ out = torch.stack(out, 0)  # TensorDict supports multiple tensor operations\n```\n\n----------------------------------------\n\nTITLE: Creating a Probabilistic Actor with Normal Distribution in Python\nDESCRIPTION: Example showing how to build a probabilistic policy for algorithms like PPO using TensorDictModule and SafeProbabilisticModule. It creates a model that outputs distribution parameters and a constructor that samples from the distribution.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/modules.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tensordict.nn import NormalParamExtractor, TensorDictSequential, TensorDictModule\nfrom torchrl.modules import SafeProbabilisticModule\nfrom torchrl.envs import GymEnv\nfrom torch.distributions import Normal\nfrom torch import nn\n\nenv = GymEnv(\"Pendulum-v1\")\naction_spec = env.action_spec\nmodel = nn.Sequential(nn.LazyLinear(action_spec.shape[-1] * 2), NormalParamExtractor())\n# build the first module, which maps the observation on the mean and sd of the normal distribution\nmodel = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n# build the distribution constructor\nprob_module = SafeProbabilisticModule(\n    in_keys=[\"loc\", \"scale\"],\n    out_keys=[\"action\"],\n    distribution_class=Normal,\n    return_log_prob=True,\n    spec=action_spec,\n)\npolicy = TensorDictSequential(model, prob_module)\n# execute a rollout\nenv.rollout(3, policy)\n```\n\n----------------------------------------\n\nTITLE: Off-Policy Training Loop with TensorDict\nDESCRIPTION: Implementation of an off-policy training loop using TensorDict, demonstrating simplified data handling and algorithm-agnostic design.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- for i, (obs, next_obs, action, hidden_state, reward, done) in enumerate(collector):\n+ for i, tensordict in enumerate(collector):\n-     replay_buffer.add((obs, next_obs, action, log_prob, reward, done))\n+     replay_buffer.add(tensordict)\n    for j in range(num_optim_steps):\n-         obs, next_obs, action, hidden_state, reward, done = replay_buffer.sample(batch_size)\n-         loss = loss_fn(obs, next_obs, action, hidden_state, reward, done)\n+         tensordict = replay_buffer.sample(batch_size)\n+         loss = loss_fn(tensordict)\n        loss.backward()\n        optim.step()\n        optim.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Initializing SafeModule Actor with Action Spec Validation in Python\nDESCRIPTION: Example demonstrating how to create an Actor with safety checks using SafeModule. The safe flag ensures that model output always falls within the bounds of the action_spec domain, projecting outputs that violate these bounds into the desired domain.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/modules.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nenv = GymEnv(\"Pendulum-v1\")\naction_spec = env.action_spec\nmodel = nn.LazyLinear(action_spec.shape[-1])\npolicy = Actor(model, in_keys=[\"observation\"], spec=action_spec, safe=True)\n```\n\n----------------------------------------\n\nTITLE: VMAS Multi-Agent Environment Example in PyTorch\nDESCRIPTION: Example showing how to create and interact with a VMAS multi-agent environment with multiple agents and environments.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torchrl.envs.libs.vmas import VmasEnv\nenv = VmasEnv(\"balance\", num_envs=3, n_agents=5)\ntd = env.rand_step()\ntd\n```\n\n----------------------------------------\n\nTITLE: Setting Up Checkpointing in TorchRL Trainer\nDESCRIPTION: Shows how to configure and use checkpointing in a TorchRL Trainer. The example demonstrates creating a trainer with checkpointing enabled, registering hooks, and saving/loading trainer state to/from disk.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/trainers.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfilepath = \"path/to/dir/or/file\"\ntrainer = Trainer(\n    collector=collector,\n    total_frames=total_frames,\n    frame_skip=frame_skip,\n    loss_module=loss_module,\n    optimizer=optimizer,\n    save_trainer_file=filepath,\n)\nselect_keys = SelectKeys([\"action\", \"observation\"])\nselect_keys.register(trainer)\n# to save to a path\ntrainer.save_trainer(True)\n# to load from a path\ntrainer.load_from_file(filepath)\n```\n\n----------------------------------------\n\nTITLE: Basic Value Network Training Example\nDESCRIPTION: Demonstrates the basic pattern for training a value network using a value estimator to generate target values.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nvalue = value_network(states)\ntarget_value = value_estimator(rewards, done, value_network(next_state))\nvalue_net_loss = (value - target_value).pow(2).mean()\n```\n\n----------------------------------------\n\nTITLE: Simplified Probabilistic Actor Construction in Python\nDESCRIPTION: Example showing how to use the ProbabilisticActor class to simplify the creation of probabilistic policies. This approach combines the model and distribution constructor in a single class, eliminating the need for explicit TensorDictSequential construction.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/modules.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchrl.modules import ProbabilisticActor\npolicy = ProbabilisticActor(\n    model,\n    in_keys=[\"loc\", \"scale\"],\n    out_keys=[\"action\"],\n    distribution_class=Normal,\n    return_log_prob=True,\n    spec=action_spec,\n)\n```\n\n----------------------------------------\n\nTITLE: Transformer Integration with TensorDict\nDESCRIPTION: Examples of using TensorDict with transformer models, including both basic usage and modular implementation with encoder-decoder architecture.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\ntransformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n+ td_module = SafeModule(transformer_model, in_keys=[\"src\", \"tgt\"], out_keys=[\"out\"])\nsrc = torch.rand((10, 32, 512))\ntgt = torch.rand((20, 32, 512))\n+ tensordict = TensorDict({\"src\": src, \"tgt\": tgt}, batch_size=[20, 32])\n- out = transformer_model(src, tgt)\n+ td_module(tensordict)\n+ out = tensordict[\"out\"]\n```\n\nLANGUAGE: python\nCODE:\n```\nencoder_module = TransformerEncoder(...)\nencoder = TensorDictSequential(encoder_module, in_keys=[\"src\", \"src_mask\"], out_keys=[\"memory\"])\ndecoder_module = TransformerDecoder(...)\ndecoder = TensorDictModule(decoder_module, in_keys=[\"tgt\", \"memory\"], out_keys=[\"output\"])\ntransformer = TensorDictSequential(encoder, decoder)\nassert transformer.in_keys == [\"src\", \"src_mask\", \"tgt\"]\nassert transformer.out_keys == [\"memory\", \"output\"]\n\ntransformer.select_subsequence(out_keys=[\"memory\"])  # returns the encoder\ntransformer.select_subsequence(in_keys=[\"tgt\", \"memory\"])  # returns the decoder\n```\n\n----------------------------------------\n\nTITLE: Configuring ReplayBuffer with MultiSyncDataCollector and Parallel Environment in Python\nDESCRIPTION: This snippet demonstrates how to configure a ReplayBuffer with MultiSyncDataCollector and a parallel environment. It requires adjusting the ndim parameter accordingly.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/collectors.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nmemory = ReplayBuffer(\n    storage=LazyTensorStorage(N, ndim=3),\n    sampler=SliceSampler(num_slices=4, trajectory_key=(\"collector\", \"traj_ids\"))\n)\ncollector = MultiSyncDataCollector([ParallelEnv(2, make_env)] * 4,\n    policy,\n    frames_per_batch=N,\n    total_frames=-1,\n    cat_results=\"stack\")\nfor data in collector:\n    memory.extend(data)\n```\n\n----------------------------------------\n\nTITLE: Neural Network Architecture Configuration\nDESCRIPTION: Shows setup of complex neural network architectures including convolutional networks, MLPs, and actor-critic models with safe module wrapping and probabilistic components.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncommon_module = ConvNet(\n    bias_last_layer=True,\n    depth=None,\n    num_cells=[32, 64, 64],\n    kernel_sizes=[8, 4, 3],\n    strides=[4, 2, 1],\n)\ncommon_module = SafeModule(\n    common_module,\n    in_keys=[\"pixels\"],\n    out_keys=[\"hidden\"],\n)\npolicy_module = SafeModule(\n    NormalParamsWrapper(\n        MLP(num_cells=[64, 64], out_features=32, activation=nn.ELU)\n    ),\n    in_keys=[\"hidden\"],\n    out_keys=[\"loc\", \"scale\"],\n)\npolicy_module = SafeProbabilisticTensorDictSequential(\n    policy_module,\n    SafeProbabilisticModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=\"action\",\n        distribution_class=TanhNormal,\n    ),\n)\nvalue_module = MLP(\n    num_cells=[64, 64],\n    out_features=1,\n    activation=nn.ELU,\n)\nactor_value = ActorValueOperator(common_module, policy_module, value_module)\nstandalone_policy = actor_value.get_policy_operator()\n```\n\n----------------------------------------\n\nTITLE: Implementing Auto-resetting Gym Environment in PyTorch RL\nDESCRIPTION: Shows implementation of an auto-resetting environment that automatically handles reset states during rollouts. Includes example usage with CartPole environment.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchrl.envs import GymEnv\n>>> from torchrl.envs import set_gym_backend\n>>> import torch\n>>> torch.manual_seed(0)\n>>>\n>>> class AutoResettingGymEnv(GymEnv):\n...     def _step(self, tensordict):\n...         tensordict = super()._step(tensordict)\n...         if tensordict[\"done\"].any():\n...             td_reset = super().reset()\n...             tensordict.update(td_reset.exclude(*self.done_keys))\n...         return tensordict\n...     def _reset(self, tensordict=None):\n...         if tensordict is not None and \"_reset\" in tensordict:\n...             return tensordict.copy()\n...         return super()._reset(tensordict)\n```\n\n----------------------------------------\n\nTITLE: AsyncEnvPool Usage Example in PyTorch\nDESCRIPTION: Example demonstrating how to create and use an AsyncEnvPool with multiple environments using threading backend.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom torchrl.envs import AsyncEnvPool, GymEnv\nimport torch\n# Choose backend\nbackend = \"threading\"\nenv = AsyncEnvPool(\n    [partial(GymEnv, \"Pendulum-v1\"), partial(GymEnv, \"CartPole-v1\")],\n    stack=\"lazy\",\n    backend=backend\n)\n# Execute a synchronous reset\nreset = env.reset()\nprint(reset)\n# Execute a synchronous step\ns = env.rand_step(reset)\nprint(s)\n# Execute an asynchronous step in env 0\ns0 = s[0]\ns0[\"action\"] = torch.randn(1).clamp(-1, 1)\ns0[\"env_index\"] = 0\nenv.async_step_send(s0)\n# Receive data\ns0_result = env.async_step_recv()\nprint('result', s0_result)\n# Close env\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Parallel Environment Execution with TorchRL\nDESCRIPTION: Demonstrates creating and executing parallel environments using ParallelEnv class with Pendulum environment. Shows environment creation, parallel rollout, and spec validation.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nenv_make = lambda: GymEnv(\"Pendulum-v1\", from_pixels=True)\nenv_parallel = ParallelEnv(4, env_make)  # creates 4 envs in parallel\ntensordict = env_parallel.rollout(max_steps=20, policy=None)  # random rollout (no policy given)\nassert tensordict.shape == [4, 20]  # 4 envs, 20 steps rollout\nenv_parallel.action_spec.is_in(tensordict[\"action\"])  # spec check returns True\n```\n\n----------------------------------------\n\nTITLE: D4RL Dataset Integration\nDESCRIPTION: Shows how to use D4RL datasets for offline reinforcement learning using experience replay and custom sampling.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torchrl.data.replay_buffers import SamplerWithoutReplacement\nfrom torchrl.data.datasets.d4rl import D4RLExperienceReplay\ndata = D4RLExperienceReplay(\n    \"maze2d-open-v0\",\n    split_trajs=True,\n    batch_size=128,\n    sampler=SamplerWithoutReplacement(drop_last=True),\n)\nfor sample in data:  # or alternatively sample = data.sample()\n    fun(sample)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Specs Environment in PyTorch RL\nDESCRIPTION: Example of implementing an environment with dynamic specifications where tensor shapes can vary during execution. Shows how to define specs with variable dimensions.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchrl.envs import EnvBase\n>>> from torchrl.data import Unbounded, Composite, Bounded, Binary\n>>> import torch\n>>> from tensordict import TensorDict, TensorDictBase\n>>>\n>>> class EnvWithDynamicSpec(EnvBase):\n...     def __init__(self, max_count=5):\n...         super().__init__(batch_size=())\n...         self.observation_spec = Composite(\n...             observation=Unbounded(shape=(3, -1, 2)),\n...         )\n...         self.action_spec = Bounded(low=-1, high=1, shape=(2,))\n...         self.full_done_spec = Composite(\n...             done=Binary(1, shape=(1,), dtype=torch.bool),\n...             terminated=Binary(1, shape=(1,), dtype=torch.bool),\n...             truncated=Binary(1, shape=(1,), dtype=torch.bool),\n...         )\n...         self.reward_spec = Unbounded((1,), dtype=torch.float)\n...         self.count = 0\n...         self.max_count = max_count\n...\n```\n\n----------------------------------------\n\nTITLE: Running PPO Algorithm on MuJoCo Environments\nDESCRIPTION: Command to execute the PPO algorithm implementation on MuJoCo physics simulation environments. This runs the main script that contains the algorithm components and training loop for MuJoCo environments.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/ppo/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ppo_mujoco.py\n```\n\n----------------------------------------\n\nTITLE: Replay Buffer Configuration\nDESCRIPTION: Demonstrates setup of a prioritized replay buffer with memory-mapped storage for efficient data management. Includes storage configuration and buffer initialization.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstorage = LazyMemmapStorage(  # memory-mapped (physical) storage\n    cfg.buffer_size,\n    scratch_dir=\"/tmp/\"\n)\nbuffer = TensorDictPrioritizedReplayBuffer(\n    alpha=0.7,\n    beta=0.5,\n    collate_fn=lambda x: x,\n    pin_memory=device != torch.device(\"cpu\"),\n    prefetch=10,  # multi-threaded sampling\n    storage=storage\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring DQN Value Estimator\nDESCRIPTION: Example showing how to configure a value estimator for a DQN loss module using TD-lambda.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom torchrl.objectives import DQNLoss, ValueEstimators\nloss_module = DQNLoss(actor)\nkwargs = {\"gamma\": 0.9, \"lmbda\": 0.9}\nloss_module.make_value_estimator(ValueEstimators.TDLambda, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL Package\nDESCRIPTION: Commands for installing TorchRL stable and nightly builds using pip.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npip3 install torchrl\npip3 install torchrl-nightly\n```\n\n----------------------------------------\n\nTITLE: Implementing a Categorical Policy with Action Mask in Python\nDESCRIPTION: Shows how to create a custom policy that uses an action mask to filter out unavailable actions in environments with discrete actions.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom tensordict.nn import TensorDictModule, ProbabilisticTensorDictModule, TensorDictSequential\nimport torch.nn as nn\nfrom torchrl.modules import MaskedCategorical\nmodule = TensorDictModule(\n    nn.Linear(in_feats, out_feats),\n    in_keys=[\"observation\"],\n    out_keys=[\"logits\"],\n)\ndist = ProbabilisticTensorDictModule(\n    in_keys={\"logits\": \"logits\", \"mask\": \"action_mask\"},\n    out_keys=[\"action\"],\n    distribution_class=MaskedCategorical,\n)\nactor = TensorDictSequential(module, dist)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Transform in PyTorch RL\nDESCRIPTION: Example of creating a custom transform that adds 1 to the observation tensor. The transform subclasses the Transform base class and implements the _apply_transform method.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass AddOneToObs(Transform):\n    \"\"\"A transform that adds 1 to the observation tensor.\"\"\"\n\n    def __init__(self):\n        super().__init__(in_keys=[\"observation\"], out_keys=[\"observation\"])\n\n    def _apply_transform(self, obs: torch.Tensor) -> torch.Tensor:\n        return obs + 1\n```\n\n----------------------------------------\n\nTITLE: Composing Multiple Transforms in Python\nDESCRIPTION: Shows how to compose multiple transforms together using the Compose class and apply them to an environment.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntransform = Compose(AddOneToObs(), RewardSum())\nenv = TransformedEnv(GymEnv(\"Pendulum-v1\"), transform)\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor-Value Model in TorchRL\nDESCRIPTION: Code example showing how to initialize an ActorValueOperator with either shared or separate parameters between actor and value networks. Demonstrates the flexibility in model construction and policy extraction.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/modules.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nactor = make_actor()\nvalue = make_value()\nif shared_params:\n    common = make_common()\n    model = ActorValueOperator(common, actor, value)\nelse:\n    model = ActorValueOperator(actor, value)\npolicy = model.get_policy_operator()  # will work in both cases\n```\n\n----------------------------------------\n\nTITLE: Configuring ReplayBuffer with MultiSyncDataCollector and Regular Environment in Python\nDESCRIPTION: This code shows how to set up a ReplayBuffer with MultiSyncDataCollector and a regular environment. It behaves like a ParallelEnv when cat_results is set to \"stack\".\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/collectors.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmemory = ReplayBuffer(\n    storage=LazyTensorStorage(N, ndim=2),\n    sampler=SliceSampler(num_slices=4, trajectory_key=(\"collector\", \"traj_ids\"))\n)\ncollector = MultiSyncDataCollector([make_env] * 4,\n    policy,\n    frames_per_batch=N,\n    total_frames=-1,\n    cat_results=\"stack\")\nfor data in collector:\n    memory.extend(data)\n```\n\n----------------------------------------\n\nTITLE: Adding Inverse Transform to Environment in PyTorch RL\nDESCRIPTION: Example of adding a transform with inverse functionality to an environment. This transform converts the action from float32 to float64 before passing it to the base environment.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nenv.append_transform(DoubleToFloat(in_keys_inv=[\"action\"]))  # will map the action from float32 to float64 before calling the base_env.step\n```\n\n----------------------------------------\n\nTITLE: Configuring ReplayBuffer for Trajectory Slices with Batched Environments in Python\nDESCRIPTION: This snippet demonstrates how to configure a ReplayBuffer for collecting trajectory slices with batched environments. It uses a multi-dimensional buffer with ParallelEnv.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/collectors.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmemory = ReplayBuffer(\n    storage=LazyTensorStorage(N, ndim=2),\n    sampler=SliceSampler(num_slices=4, trajectory_key=(\"collector\", \"traj_ids\"))\n)\nenv = ParallelEnv(4, make_env)\ncollector = SyncDataCollector(env, policy, frames_per_batch=N, total_frames=-1)\nfor data in collector:\n    memory.extend(data)\n```\n\n----------------------------------------\n\nTITLE: Computing Total Loss in TorchRL\nDESCRIPTION: Example showing how to compute the total loss by summing individual loss components from a tensordict output.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nloss_val = sum(loss for key, loss in loss_vals.items() if key.startswith(\"loss_\"))\n```\n\n----------------------------------------\n\nTITLE: Using ActionMask Transform with Default Policy in Python\nDESCRIPTION: Demonstrates how to use the ActionMask transform to update the action mask in the action spec for use with default policies.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom tensordict.nn import TensorDictModule, ProbabilisticTensorDictModule, TensorDictSequential\nimport torch.nn as nn\nfrom torchrl.envs.transforms import TransformedEnv, ActionMask\nenv = TransformedEnv(\n    your_base_env\n    ActionMask(action_key=\"action\", mask_key=\"action_mask\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Transformed Environment with Image Processing in PyTorch RL\nDESCRIPTION: Example of creating a transformed environment that processes pixel-based observations from Pendulum-v1. It applies image transformations including conversion to tensor and resizing.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nbase_env = GymEnv(\"Pendulum-v1\", from_pixels=True, device=\"cuda:0\")\ntransform = Compose(ToTensorImage(in_keys=[\"pixels\"]), Resize(64, 64, in_keys=[\"pixels\"]))\nenv = TransformedEnv(base_env, transform)\n```\n\n----------------------------------------\n\nTITLE: Using PixelRenderTransform with Parallel Environments in TorchRL (Python)\nDESCRIPTION: This example demonstrates creating parallel environments with the PixelRenderTransform to collect rendered images and VideoRecorder to save them. It shows how to handle parallel environments, execute rollouts, and properly save videos while managing resources.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchrl.envs import GymEnv, check_env_specs, ParallelEnv, EnvCreator\n>>> from torchrl.record.loggers import CSVLogger\n>>> from torchrl.record.recorder import PixelRenderTransform, VideoRecorder\n>>>\n>>> def make_env():\n>>>     env = GymEnv(\"CartPole-v1\", render_mode=\"rgb_array\")\n>>>     # Uncomment this line to execute per-env\n>>>     # env = env.append_transform(PixelRenderTransform())\n>>>     return env\n>>>\n>>> if __name__ == \"__main__\":\n...     logger = CSVLogger(\"dummy\", video_format=\"mp4\")\n...\n...     env = ParallelEnv(16, EnvCreator(make_env))\n...     env.start()\n...     # Comment this line to execute per-env\n...     env = env.append_transform(PixelRenderTransform())\n...\n...     env = env.append_transform(VideoRecorder(logger=logger, tag=\"pixels_record\"))\n...     env.rollout(3)\n...\n...     check_env_specs(env)\n...\n...     r = env.rollout(30)\n...     env.transform.dump()\n...     env.close()\n```\n\n----------------------------------------\n\nTITLE: Managing Multiple Gym Backends with set_gym_backend in TorchRL (Python)\nDESCRIPTION: This example shows how to use the set_gym_backend decorator to control which gym implementation (gym or gymnasium) is used when creating environments. This allows for compatibility with different versions and implementations of the Gym API in the same codebase.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchrl.envs.libs.gym import GymEnv, set_gym_backend, gym_backend\n>>> import gymnasium, gym\n>>> with set_gym_backend(gymnasium):\n...     print(gym_backend())\n...     env1 = GymEnv(\"Pendulum-v1\")\n<module 'gymnasium' from '/path/to/venv/python3.9/site-packages/gymnasium/__init__.py'>\n>>> with set_gym_backend(gym):\n...     print(gym_backend())\n...     env2 = GymEnv(\"Pendulum-v1\")\n<module 'gym' from '/path/to/venv/python3.9/site-packages/gym/__init__.py'>\n>>> print(env1._env.env.env)\n<gymnasium.envs.classic_control.pendulum.PendulumEnv at 0x15147e190>\n>>> print(env2._env.env.env)\n<gym.envs.classic_control.pendulum.PendulumEnv at 0x1629916a0>\n```\n\n----------------------------------------\n\nTITLE: Exploration Policy Configuration\nDESCRIPTION: Demonstrates setup of exploration policies using epsilon-greedy strategy with context managers for switching between exploration and exploitation modes.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npolicy_explore = EGreedyWrapper(policy)\nwith set_exploration_type(ExplorationType.RANDOM):\n    tensordict = policy_explore(tensordict)  # will use eps-greedy\nwith set_exploration_type(ExplorationType.DETERMINISTIC):\n    tensordict = policy_explore(tensordict)  # will not use eps-greedy\n```\n\n----------------------------------------\n\nTITLE: Configuring ReplayBuffer for Trajectory Slices with Single Environment in Python\nDESCRIPTION: This code shows how to set up a ReplayBuffer for collecting trajectory slices with a single environment. It uses SliceSampler and SyncDataCollector.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/collectors.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmemory = ReplayBuffer(\n    storage=LazyTensorStorage(N),\n    sampler=SliceSampler(num_slices=4, trajectory_key=(\"collector\", \"traj_ids\"))\n)\ncollector = SyncDataCollector(env, policy, frames_per_batch=N, total_frames=-1)\nfor data in collector:\n    memory.extend(data)\n```\n\n----------------------------------------\n\nTITLE: Basic Video Recording in TorchRL\nDESCRIPTION: This snippet demonstrates the basic setup for recording a video in TorchRL using a CartPole environment. It uses CSVLogger and VideoRecorder to log the experiment and record the video.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VIDEO_CUSTOMISATION.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torchrl.envs import GymEnv, TransformedEnv\nfrom torchrl.record import CSVLogger, VideoRecorder\n\nlogger = CSVLogger(exp_name=\"my_exp\")\nenv = GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False)\n\nrecorder = VideoRecorder(logger, tag=\"my_video\")\nrecord_env = TransformedEnv(env, recorder)\nrollout = record_env.rollout(max_steps=3)\nrecorder.dump()\n```\n\n----------------------------------------\n\nTITLE: Creating Action Log Probability TensorDict Example 2\nDESCRIPTION: Creates a TensorDict structure containing action log probabilities for multiple agents with consistent dimensions. Shows the simpler case where all action probabilities have the same dimensionality.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\naction_td = TensorDict(\n    agents=TensorDict(\n        action0_log_prob=Tensor(batch, n_agents),\n        action1_log_prob=Tensor(batch, n_agents),\n        batch_size=torch.Size((batch, n_agents))\n    ),\n    batch_size=torch.Size((batch,))\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Multi-Agent Environment TensorDict\nDESCRIPTION: Example showing a complete tensordict structure for a multi-agent environment including shared states.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\naction_td = TensorDict(\n    agents=TensorDict(\n        action0=Tensor(batch, n_agents, f0),\n        action1=Tensor(batch, n_agents, f1, f2),\n        observation=Tensor(batch, n_agents, f3),\n        batch_size=torch.Size((batch, n_agents))\n    ),\n    done=Tensor(batch, 1),\n    batch_size=torch.Size((batch,))\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Logging Hook in TorchRL\nDESCRIPTION: Demonstrates how to implement a custom logging hook in TorchRL that inherits from TrainerHookBase. This example hook logs a value every 10 calls to the post_optim_log hook and includes state serialization for checkpointing.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/trainers.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass LoggingHook(TrainerHookBase):\n    def __init__(self):\n        self.counter = 0\n\n    def register(self, trainer, name):\n        trainer.register_module(self, \"logging_hook\")\n        trainer.register_op(\"post_optim_log\", self)\n\n    def save_dict(self):\n        return {\"counter\": self.counter}\n\n    def load_state_dict(self, state_dict):\n        self.counter = state_dict[\"counter\"]\n\n    def __call__(self, batch):\n        if self.counter % 10 == 0:\n            self.counter += 1\n            out = {\"some_value\": batch[\"some_value\"].item(), \"log_pbar\": False}\n        else:\n            out = None\n        self.counter += 1\n        return out\n```\n\n----------------------------------------\n\nTITLE: TensorDict Structure Example in PyTorch\nDESCRIPTION: Example showing the structure of a TensorDict object containing environment state information with tensor shapes and data types.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nTensorDict(\n    fields={\n        terminated: Tensor(torch.Size([4, 1]), dtype=torch.bool),\n        done: Tensor(torch.Size([4, 1]), dtype=torch.bool),\n        pixels: Tensor(torch.Size([4, 500, 500, 3]), dtype=torch.uint8),\n        truncated: Tensor(torch.Size([4, 1]), dtype=torch.bool),\n    batch_size=torch.Size([4]),\n    device=None,\n    is_shared=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Environment with Dynamic Specification in PyTorch RL\nDESCRIPTION: A code snippet showing the _step method of a custom environment class with dynamic observation specifications. This method increments a counter, generates observations based on the counter value, and returns observation, done, and reward signals.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _step(\n    self,\n    tensordict: TensorDictBase,\n) -> TensorDictBase:\n    self.count += 1\n    done = self.count >= self.max_count\n    observation = TensorDict(\n        {\n            \"observation\": torch.full(\n                (3, self.count + 1, 2),\n                self.count,\n                dtype=self.observation_spec[\"observation\"].dtype,\n            )\n        }\n    )\n    done = self.full_done_spec.zero() | done\n    reward = self.full_reward_spec.zero()\n    return observation.update(done).update(reward)\n```\n\n----------------------------------------\n\nTITLE: Multi-Agent Action Space Definition\nDESCRIPTION: Example showing how to structure a tensordict for multi-agent action spaces with different action dimensions.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\naction_td = TensorDict(\n    agents=TensorDict(\n        action0=Tensor(batch, n_agents, f0),\n        action1=Tensor(batch, n_agents, f1, f2),\n        batch_size=torch.Size((batch, n_agents))\n    ),\n    batch_size=torch.Size((batch,))\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Habitat-Sim with Bullet Physics in Headless Mode\nDESCRIPTION: Installs habitat-sim with bullet physics and in headless mode using conda, then installs habitat-lab using pip. Also sets environment variables to reduce verbosity.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/HABITAT.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install habitat-sim withbullet headless -c conda-forge -c aihabitat-nightly -y\npip install git+https://github.com/facebookresearch/habitat-lab.git#subdirectory=habitat-lab\n\n# This is to reduce verbosity\nexport MAGNUM_LOG=quiet && export HABITAT_SIM_LOG=quiet\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Environment with Dynamic Specification in PyTorch RL\nDESCRIPTION: Example of instantiating the custom environment class and performing a rollout of 5 steps. The output shows the structure of the resulting TensorDict with dynamic observation shapes.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nenv = EnvWithDynamicSpec()\nprint(env.rollout(5, return_contiguous=False))\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-agent Environment Specs in PyTorch RL\nDESCRIPTION: Demonstrates how to create specifications for a multi-agent environment where only the done flag is shared across agents. Shows creation of action, reward, and observation specs with proper nesting.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> action_specs = []\n>>> observation_specs = []\n>>> reward_specs = []\n>>> info_specs = []\n>>> for i in range(env.n_agents):\n...    action_specs.append(agent_i_action_spec)\n...    reward_specs.append(agent_i_reward_spec)\n...    observation_specs.append(agent_i_observation_spec)\n>>> env.action_spec = Composite(\n...    {\n...        \"agents\": Composite(\n...            {\"action\": torch.stack(action_specs)}, shape=(env.n_agents,)\n...        )\n...    }\n...)\n>>> env.reward_spec = Composite(\n...    {\n...        \"agents\": Composite(\n...            {\"reward\": torch.stack(reward_specs)}, shape=(env.n_agents,)\n...        )\n...    }\n...)\n>>> env.observation_spec = Composite(\n...    {\n...        \"agents\": Composite(\n...            {\"observation\": torch.stack(observation_specs)}, shape=(env.n_agents,)\n...        )\n...    }\n...)\n>>> env.done_spec = Categorical(\n...    n=2,\n...    shape=torch.Size((1,)),\n...    dtype=torch.bool,\n...)\n```\n\n----------------------------------------\n\nTITLE: Running DQN on CartPole Environment\nDESCRIPTION: Command to execute the DQN algorithm implementation for the CartPole environment.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/dqn/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython dqn_cartpole.py\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Node IMPALA Training\nDESCRIPTION: Commands to execute the multi-node IMPALA algorithm implementation on Atari environments using either Ray or Submitit for distribution.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/impala/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython impala_single_node_ray.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npython impala_single_node_submitit.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Parallel Environment in TorchRL\nDESCRIPTION: Creates a parallel environment with 4 instances of a Pendulum environment with custom gravity settings and CUDA support.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef make_env():\n    return GymEnv(\"Pendulum-v1\", from_pixels=True, g=9.81, device=\"cuda:0\")\ncheck_env_specs(env)  # this must pass for ParallelEnv to work\nenv = ParallelEnv(4, make_env)\nprint(env.batch_size)\n```\n\n----------------------------------------\n\nTITLE: Training the Transformer Model (Shell)\nDESCRIPTION: This command runs the train.py script to train the GPT model. It can be executed with default configurations or with command-line arguments to override specific options.\nSOURCE: https://github.com/pytorch/rl/blob/main/examples/rlhf/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Initializing ReplayBuffer with LazyTensorStorage and SliceSampler in Python\nDESCRIPTION: This snippet demonstrates how to initialize a ReplayBuffer with LazyTensorStorage and SliceSampler for collecting single transitions. It shows how to flatten the data before populating the storage.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/collectors.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nmemory = ReplayBuffer(\n    storage=LazyTensorStorage(N),\n    transform=lambda data: data.reshape(-1))\nfor data in collector:\n    memory.extend(data)\n```\n\n----------------------------------------\n\nTITLE: Running Single Node IMPALA Training\nDESCRIPTION: Command to execute the single node IMPALA algorithm implementation on Atari environments.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/impala/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython impala_single_node.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Trainer Loop Structure in TorchRL\nDESCRIPTION: Illustrates the basic structure of a TorchRL trainer with its nested loop pattern, showing how the outer loop handles data collection from a collector while the inner loop processes optimization steps.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/trainers.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfor batch in collector:\n    batch = self._process_batch_hook(batch)  # \"batch_process\"\n    self._pre_steps_log_hook(batch)  # \"pre_steps_log\"\n    self._pre_optim_hook()  # \"pre_optim_steps\"\n    for j in range(self.optim_steps_per_batch):\n        sub_batch = self._process_optim_batch_hook(batch)  # \"process_optim_batch\"\n        losses = self.loss_module(sub_batch)\n        self._post_loss_hook(sub_batch)  # \"post_loss\"\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n        self._post_optim_hook()  # \"post_optim\"\n        self._post_optim_log(sub_batch)  # \"post_optim_log\"\n    self._post_steps_hook()  # \"post_steps\"\n    self._post_steps_log_hook(batch)  #  \"post_steps_log\"\n```\n\n----------------------------------------\n\nTITLE: Training the Reward Model (Shell)\nDESCRIPTION: This command runs the train_reward.py script to train the reward model. It assumes that the supervised fine-tuning has been completed and the model checkpoint is available.\nSOURCE: https://github.com/pytorch/rl/blob/main/examples/rlhf/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython train_reward.py\n```\n\n----------------------------------------\n\nTITLE: Running PPO Algorithm on Atari Environments\nDESCRIPTION: Command to execute the PPO algorithm implementation on Atari environments. This runs the main script that contains the algorithm components and training loop for Atari games.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/ppo/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython ppo_atari.py\n```\n\n----------------------------------------\n\nTITLE: Loss Module and Advantage Computation\nDESCRIPTION: Shows implementation of DQN loss computation and vectorized advantage estimation using TD-lambda returns.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom torchrl.objectives import DQNLoss\nloss_module = DQNLoss(value_network=value_network, gamma=0.99)\ntensordict = replay_buffer.sample(batch_size)\nloss = loss_module(tensordict)\n\nfrom torchrl.objectives.value.functional import vec_td_lambda_return_estimate\nadvantage = vec_td_lambda_return_estimate(gamma, lmbda, next_state_value, reward, done, terminated)\n```\n\n----------------------------------------\n\nTITLE: Installing New MuJoCo Bindings\nDESCRIPTION: Setup instructions for installing modern MuJoCo bindings (â‰¥2.1.2) using conda and pip.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n mujoco_env python=3.9\nconda activate mujoco_env\npip install mujoco\n```\n\n----------------------------------------\n\nTITLE: Running DQN on Atari Environment\nDESCRIPTION: Command to execute the DQN algorithm implementation for Atari environments.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/dqn/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython dqn_atari.py\n```\n\n----------------------------------------\n\nTITLE: Implementing a QValueActor for Discrete Action Spaces in Python\nDESCRIPTION: Example demonstrating the QValueActor class that selects actions based on maximum state-action value. It takes a module and action specification, outputting the selected action and corresponding value for discrete action spaces.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/modules.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom tensordict import TensorDict\nfrom torch import nn\nfrom torchrl.data import OneHot\nfrom torchrl.modules.tensordict_module.actors import QValueActor\n# Create a tensor dict with an observation\ntd = TensorDict({'observation': torch.randn(5, 3)}, [5])\n# Define the action space\naction_spec = OneHot(4)\n# Create a linear module to output action values\nmodule = nn.Linear(3, 4)\n# Create a QValueActor instance\nqvalue_actor = QValueActor(module=module, spec=action_spec)\n# Run the actor on the tensor dict\nqvalue_actor(td)\nprint(td)\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL Nightly Builds\nDESCRIPTION: Commands to install the nightly development builds of TensorDict and TorchRL packages using pip.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install tensordict-nightly\n$ pip install torchrl-nightly\n```\n\n----------------------------------------\n\nTITLE: Executing A2C on MuJoCo Environments\nDESCRIPTION: Command to run the A2C algorithm implementation on MuJoCo environments with compilation and CUDA graph optimizations enabled.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/a2c/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython a2c_mujoco.py compile.compile=1 compile.cudagraphs=1\n```\n\n----------------------------------------\n\nTITLE: Creating a Distributional QValueActor in Python\nDESCRIPTION: Example showing how to implement distributional Q-learning using DistributionalQValueActor. This approach represents the Q-value as a probability distribution over possible values rather than a scalar, allowing the agent to learn about environment uncertainty.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/modules.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom tensordict import TensorDict\nfrom torch import nn\nfrom torchrl.data import OneHot\nfrom torchrl.modules import DistributionalQValueActor, MLP\n# Create a tensor dict with an observation\ntd = TensorDict({'observation': torch.randn(5, 4)}, [5])\n# Define the action space\naction_spec = OneHot(4)\n# Define the number of bins for the value distribution\nnbins = 3\n# Create an MLP module to output logits for the value distribution\nmodule = MLP(out_features=(nbins, 4), depth=2)\n# Create a DistributionalQValueActor instance\nqvalue_actor = DistributionalQValueActor(module=module, spec=action_spec, support=torch.arange(nbins))\n# Run the actor on the tensor dict\ntd = qvalue_actor(td)\nprint(td)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorDict Nightly Build\nDESCRIPTION: Command to install the nightly build version of TensorDict, which is recommended for TorchRL development\nSOURCE: https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install tensordict-nightly\n```\n\n----------------------------------------\n\nTITLE: Executing A2C on Atari Environments\nDESCRIPTION: Command to run the A2C algorithm implementation on Atari environments with compilation and CUDA graph optimizations enabled.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/a2c/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython a2c_atari.py compile.compile=1 compile.cudagraphs=1\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic RL Training Loop in PyTorch\nDESCRIPTION: Demonstrates a typical reinforcement learning training loop with data collection and optimization steps. Shows proper separation between non-differentiable environment interactions and differentiable loss computations using torch.no_grad() context manager.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/PRO-TIPS.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nobs = env.reset()\n\nfor _ in range(n_training_steps):\n    # STEP 1: data collection\n    # Get a new datapoint \"online\"\n    observations = []\n    actions = []\n    others = []\n    for _ in range(n_data_per_training):\n        with torch.no_grad():\n            action = policy(obs)\n        obs, *other = env.step(action)\n        observations.append(obs)\n        actions.append(action)\n        others.append(other)\n    replay_buffer.extend(observations, actions, others)\n\n    # STEP 2: loss and optimization\n    # => compute loss \"offline\"\n    loss = loss_fn(replay_buffer.sample(batch_size))\n    \n    loss.backward()\n    optim.step()\n    optim.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL on MacOS M1\nDESCRIPTION: Special installation command for MacOS M1 architecture to resolve compatibility issues\nSOURCE: https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nARCHFLAGS=\"-arch arm64\" python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Creating Action Log Probability TensorDict Example 1\nDESCRIPTION: Creates a TensorDict structure containing action log probabilities for multiple agents with varying feature dimensions. Shows how to structure multi-dimensional action probability tensors with batch and agent dimensions.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/objectives.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naction_td = TensorDict(\n    agents=TensorDict(\n        action0_log_prob=Tensor(batch, n_agents),\n        action1_log_prob=Tensor(batch, n_agents, f1),\n        batch_size=torch.Size((batch, n_agents))\n    ),\n    batch_size=torch.Size((batch,))\n)\n```\n\n----------------------------------------\n\nTITLE: Running MAPPO/IPPO Script\nDESCRIPTION: Example commands for running the MAPPO/IPPO script with default and custom configurations.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython mappo_ippo.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npython mappo_ippo.py --m env.scenario_name=navigation\n```\n\n----------------------------------------\n\nTITLE: Cloning Transforms in Python\nDESCRIPTION: Illustrates how to clone transforms to avoid issues with parent environments when reusing transforms.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nTransformedEnv(base_env, third_transform)  # raises an Exception as third_transform already has a parent\nTransformedEnv(base_env, third_transform.clone())  # works\n```\n\n----------------------------------------\n\nTITLE: Training with Custom Batch Size (Shell)\nDESCRIPTION: This command demonstrates how to run the training script with a custom batch size. It overrides the default configuration by specifying the batch_size parameter.\nSOURCE: https://github.com/pytorch/rl/blob/main/examples/rlhf/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython train.py --batch_size=128\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL with Extra Dependencies\nDESCRIPTION: Command to install TorchRL with additional features including Atari, DM Control, Gym, rendering, and testing dependencies.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip3 install \"torchrl[atari,dm_control,gym_continuous,rendering,tests,utils,marl,open_spiel,checkpointing]\"\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL from Source for Development\nDESCRIPTION: Step-by-step commands to clone the TensorDict and TorchRL repositories from GitHub and install them in development mode for contributing to the libraries.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/index.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cd path/to/root\n$ git clone https://github.com/pytorch/tensordict\n$ git clone https://github.com/pytorch/rl\n$ cd tensordict\n$ python setup.py develop\n$ cd ../rl\n$ python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Dependencies\nDESCRIPTION: Commands for installing various optional dependencies for TorchRL including rendering, testing, and environment libraries.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npip3 install tqdm tensorboard \"hydra-core>=1.1\" hydra-submitit-launcher\npip3 install \"moviepy<2.0.0\"\npip3 install dm_control\npip3 install \"gym[atari]\" \"gym[accept-rom-license]\" pygame\npip3 install pytest pyyaml pytest-instafail\npip3 install tensorboard\n```\n\n----------------------------------------\n\nTITLE: Accessing Gym Backend Modules in TorchRL (Python)\nDESCRIPTION: This code demonstrates how to use the gym_backend function to access specific modules from the currently selected gym implementation. This allows for fine-grained control when working with different components of gym/gymnasium.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n>>> import mo_gymnasium\n>>> with set_gym_backend(\"gym\"):\n...     wrappers = gym_backend('wrappers')\n...     print(wrappers)\n<module 'gym.wrappers' from '/path/to/venv/python3.9/site-packages/gym/wrappers/__init__.py'>\n>>> with set_gym_backend(\"gymnasium\"):\n...     wrappers = gym_backend('wrappers')\n...     print(wrappers)\n<module 'gymnasium.wrappers' from '/path/to/venv/python3.9/site-packages/gymnasium/wrappers/__init__.py'>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for TorchRL Performance Testing\nDESCRIPTION: Complete setup instructions including environment variable configuration, conda environment creation, and installation of required dependencies for running performance tests\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-check/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MUJOCO_GL=egl\n\nconda create -n rl-sota-bench python=3.10 -y \nconda install anaconda::libglu -y\npip3 install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu121\npip3 install \"gymnasium[atari,mujoco]\" vmas tqdm wandb pygame \"moviepy<2.0.0\" imageio submitit hydra-core transformers\n\ncd /path/to/tensordict\npython setup.py develop\ncd /path/to/torchrl\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Version-Specific Implementations with implement_for in TorchRL (Python)\nDESCRIPTION: This example shows how to use the implement_for decorator to create version-specific implementations of functions based on the installed library versions. This allows for maintaining backward compatibility and supporting multiple library versions with appropriate behavior.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchrl._utils import implement_for\n>>> @implement_for(\"gym\", None, \"0.26.0\")\n... def fun():\n...     return 0\n>>> @implement_for(\"gym\", \"0.26.0\", None)\n... def fun():\n...     return 1\n>>> fun()\n1\n```\n\n----------------------------------------\n\nTITLE: Resolving OpenGL Undefined Symbol Error\nDESCRIPTION: Bash commands to install necessary packages and set LD_PRELOAD to resolve the 'undefined symbol: _glapi_tls_Current' error with OpenGL.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/HABITAT.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda install -y -c conda-forge libglvnd-glx-cos7-x86_64 --force-reinstall\nconda install -y -c conda-forge xvfbwrapper --force-reinstall\nconda env config vars set LD_PRELOAD=$LD_PRELOAD:/path/to/conda/envs/env_name/x86_64-conda-linux-gnu/sysroot/usr/lib64/libGLdispatch.so.0\nconda deactivate && conda activate env_name\n```\n\n----------------------------------------\n\nTITLE: Setting Random Seed in PyTorch RL Environment\nDESCRIPTION: A method to set the random seed for the custom environment. It stores the provided seed as a class attribute and returns it.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _set_seed(self, seed: Optional[int]) -> None:\n    self.manual_seed = seed\n    return seed\n```\n\n----------------------------------------\n\nTITLE: Recording Environment Observations with VideoRecorder in TorchRL (Python)\nDESCRIPTION: This code snippet demonstrates how to set up a simple environment with a VideoRecorder transform to capture and save visual observations from a Gym environment. The recorder observes the 'pixels' key from the environment data and saves the video after rollout.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n>>> logger = CSVLogger(\"dummy-exp\", video_format=\"mp4\")\n>>> env = GymEnv(\"ALE/Pong-v5\")\n>>> env = env.append_transform(VideoRecorder(logger, tag=\"rendered\", in_keys=[\"pixels\"]))\n>>> env.rollout(10)\n>>> env.transform.dump()  # Save the video and clear cache\n```\n\n----------------------------------------\n\nTITLE: Customizing Video Quality in TorchRL\nDESCRIPTION: This snippet shows how to improve video quality by customizing FFmpeg settings in TorchRL. It sets the Constant Rate Factor (CRF) to 17 and the preset to 'slow' for better quality output.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VIDEO_CUSTOMISATION.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrecorder = VideoRecorder(logger, tag = \"my_video\", options = {\"crf\": \"17\", \"preset\": \"slow\"})\n```\n\n----------------------------------------\n\nTITLE: Accessing Parallel Environment Attributes\nDESCRIPTION: Demonstrates how to access attributes from contained environments in a parallel environment setup.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\na, b, c, d = env.g  # gets the g-force of the various envs, which we set to 9.81 before\nprint(a)\n```\n\n----------------------------------------\n\nTITLE: Reproducing TorchRL Import Error in Python\nDESCRIPTION: This code snippet demonstrates how to reproduce the undefined symbol error when importing TorchRL with an older version of PyTorch. It shows the typical error message encountered.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VERSIONING_ISSUES.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nImportError: /usr/local/lib/python3.7/dist-packages/torchrl/_torchrl.so: undefined symbol: _ZN8pybind116detail11type_casterIN2at6TensorEvE4loadENS_6handleEb\n```\n\n----------------------------------------\n\nTITLE: Installing mujoco-py from Source\nDESCRIPTION: Installation of mujoco-py by cloning the repository and building from source, recommended for GPU support.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nconda activate mujoco_env\ncd path/to/where/mujoco-py/must/be/cloned\ngit clone https://github.com/openai/mujoco-py\ncd mujoco-py\npython setup.py develop\nconda env config vars set LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\n```\n\n----------------------------------------\n\nTITLE: Accessing Parent Transform in PyTorch RL\nDESCRIPTION: Example of accessing a parent transform from a transformed environment. This returns a new TransformedEnv with all transforms up to a specific point.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresize_parent = env.transform[-1].parent  # returns the same as TransformedEnv(base_env, transform[:-1])\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Functorch Version\nDESCRIPTION: This bash command installs a specific version of functorch that is compatible with the user's PyTorch distribution. It's part of a workaround for using TorchRL with older PyTorch versions.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VERSIONING_ISSUES.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install functorch==0.2.0\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL from Source\nDESCRIPTION: This bash command installs TorchRL directly from the GitHub repository, allowing users to specify a particular version. It's used as part of a workaround for compatibility issues.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VERSIONING_ISSUES.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/pytorch/rl@<lib_version_here>\n```\n\n----------------------------------------\n\nTITLE: Implementing an Inverse Transform in Python\nDESCRIPTION: Demonstrates how to create a transform with an inverse operation, specifically adding 1 to the action tensor and subtracting 1 in the inverse.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass AddOneToAction(Transform):\n    \"\"\"A transform that adds 1 to the action tensor.\"\"\"\n    def __init__(self):\n        super().__init__(in_keys=[], out_keys=[], in_keys_inv=[\"action\"], out_keys_inv=[\"action\"])\n    def _inv_apply_transform(self, action: torch.Tensor) -> torch.Tensor:\n        return action + 1\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL in Python Environment\nDESCRIPTION: This bash command installs the TorchRL package using pip. It's part of the steps to reproduce the versioning issue in a Colab environment.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VERSIONING_ISSUES.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!pip install torchrl\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch and TensorDict Modules\nDESCRIPTION: Imports necessary modules from PyTorch and TensorDict for tree operations and tensor manipulation.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils._cxx_pytree import tree_map, tree_leaves, tree_flatten\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom tensordict import TensorDict, lazy_stack, is_tensor_collection\nimport torch\nfrom tensordict._pytree import *\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Template Structure\nDESCRIPTION: A reStructuredText template that sets up module context and generates automatic function documentation using Sphinx. Uses template variables for module name and function name.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/_templates/rl_template_fun.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autofunction:: {{ name }}\n```\n\n----------------------------------------\n\nTITLE: Creating Nested TensorDict Object\nDESCRIPTION: Constructs a deeply nested dictionary and converts it to a TensorDict object with a specified batch size.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nd_ = d = {}\nfor _ in range(100):\n    newd = {}\n    d_[\"a\"] = newd\n    d_[\"t\"] = torch.zeros((1,))\n    d_ = newd\ntd = TensorDict(d, batch_size=(1,))\ntd.depth\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Addition Operation on TensorDict\nDESCRIPTION: Measures the execution time of adding 1 to the entire TensorDict using the + operator.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntd + 1\n```\n\n----------------------------------------\n\nTITLE: TensorDict Operations Example\nDESCRIPTION: Demonstration of various tensor operations supported by TensorDict, including stacking, reshaping, indexing, and device management.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# stack and cat\ntensordict = torch.stack(list_of_tensordicts, 0)\ntensordict = torch.cat(list_of_tensordicts, 0)\n# reshape\ntensordict = tensordict.view(-1)\ntensordict = tensordict.permute(0, 2, 1)\ntensordict = tensordict.unsqueeze(-1)\ntensordict = tensordict.squeeze(-1)\n# indexing\ntensordict = tensordict[:2]\ntensordict[:, 2] = sub_tensordict\n# device and memory location\ntensordict.cuda()\ntensordict.to(\"cuda:1\")\ntensordict.share_memory_()\n```\n\n----------------------------------------\n\nTITLE: Creating Lazy-Stacked TensorDict\nDESCRIPTION: Constructs a lazy-stacked TensorDict by creating multiple clones of a nested TensorDict structure.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nd_ = d = {}\nfor _ in range(10):\n    newd = {}\n    d_[\"a\"] = newd\n    d_[\"t\"] = torch.zeros((1,))\n    d_ = newd\ntdls = TensorDict(d, batch_size=(1,))\ntdls.depth\n\ntdls = lazy_stack([tdls.clone() for _ in range(100)])\n```\n\n----------------------------------------\n\nTITLE: Local Installation of TorchRL\nDESCRIPTION: Commands for cloning and installing TorchRL locally from source code.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pytorch/rl\ngit checkout v0.4.0\ncd /path/to/torchrl/\npip3 install ninja -U\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Benchmarking apply Method on TensorDict\nDESCRIPTION: Measures the execution time of using the apply method to add 1 to each element in the TensorDict.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntd.apply(lambda x: x+1)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking values Method on TensorDict\nDESCRIPTION: Measures the execution time of retrieving all values from the TensorDict using the values method.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\nlist(td.values(True, True))\n```\n\n----------------------------------------\n\nTITLE: Upgrading PyTorch in Colab Environment\nDESCRIPTION: This bash command upgrades PyTorch to the latest version in a Colab environment, which is a solution to resolve the versioning issue before installing TorchRL.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VERSIONING_ISSUES.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n!pip3 install torch --extra-index-url https://download.pytorch.org/whl/cpu -U\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Addition on Lazy-Stacked TensorDict\nDESCRIPTION: Measures the execution time of adding 1 to the entire lazy-stacked TensorDict using the + operator.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntdls + 1\n```\n\n----------------------------------------\n\nTITLE: Asserting Correctness of tree_map Operation\nDESCRIPTION: Ensures that the tree_map operation correctly adds 1 to all elements in the TensorDict.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nassert (tree_map(lambda x: x+1, td) == 1).all()\n```\n\n----------------------------------------\n\nTITLE: Importing TorchRL in Python\nDESCRIPTION: This Python code imports the TorchRL module, which is the step that typically triggers the undefined symbol error when using incompatible versions.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/VERSIONING_ISSUES.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torchrl\n```\n\n----------------------------------------\n\nTITLE: Benchmarking tree_flatten Operation on TensorDict\nDESCRIPTION: Measures the execution time of flattening the TensorDict structure using tree_flatten function.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntree_flatten(td)\n```\n\n----------------------------------------\n\nTITLE: Checking Batch Size After tree_map Operation\nDESCRIPTION: Verifies that the batch size of the TensorDict is preserved after applying tree_map function.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntree_map(lambda x: x+1, td).batch_size\n```\n\n----------------------------------------\n\nTITLE: Installing MuJoCo Rendering Dependencies with Conda\nDESCRIPTION: Alternative installation of rendering dependencies using conda for environments without sudo access.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda activate mujoco_env\nconda install -c conda-forge glew\nconda install -c conda-forge mesalib\nconda install -c anaconda mesa-libgl-cos6-x86_64\nconda install -c menpo glfw3\n```\n\n----------------------------------------\n\nTITLE: Benchmarking tree_flatten on Lazy-Stacked TensorDict\nDESCRIPTION: Measures the execution time of flattening the lazy-stacked TensorDict structure using tree_flatten function.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntree_flatten(tdls)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking tree_map Operation on TensorDict\nDESCRIPTION: Measures the execution time of applying tree_map function to add 1 to each element in the TensorDict.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntree_map(lambda x: x+1, td)\n```\n\n----------------------------------------\n\nTITLE: Setting MuJoCo Rendering Backend Environment Variables\nDESCRIPTION: Configuration of environment variables to specify the EGL rendering backend for MuJoCo.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nconda env config vars set MUJOCO_GL=egl PYOPENGL_PLATFORM=egl\nconda deactivate && conda activate mujoco_env\n```\n\n----------------------------------------\n\nTITLE: Checking Batch Size of Lazy-Stacked TensorDict After tree_map\nDESCRIPTION: Verifies the batch size of the lazy-stacked TensorDict after applying the tree_map function.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntree_map(lambda x: x+1, tdls).batch_size\n```\n\n----------------------------------------\n\nTITLE: Performing Partial Reset in Parallel Environment\nDESCRIPTION: Shows how to perform a partial reset on specific environments within a parallel environment setup using a reset mask.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntensordict = TensorDict({\"_reset\": [[True], [False], [True], [True]]}, [4])\nenv.reset(tensordict)  # eliminates the \"_reset\" entry\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks\nDESCRIPTION: Command to run pre-commit checks on all files in the repository\nSOURCE: https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Setting Up Old MuJoCo Bindings\nDESCRIPTION: Detailed setup process for legacy MuJoCo bindings (â‰¤2.1.1) including environment configuration.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n mujoco_env python=3.9\nconda activate mujoco_env\nmkdir ~/.mujoco\ncd ~/.mujoco\nwget https://github.com/deepmind/mujoco/releases/download/2.1.0/mujoco210-linux-x86_64.tar.gz\ntar -xf mujoco210-linux-x86_64.tar.gz\nwget http://roboti.us/file/mjkey.txt\n```\n\n----------------------------------------\n\nTITLE: Environment Information Collection for MacOS Troubleshooting\nDESCRIPTION: Commands to download and run PyTorch's environment collection script that helps diagnose installation issues, particularly for MacOS users with Apple Silicon (M1) chips.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\npython collect_env.py\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL in Development Mode\nDESCRIPTION: Command to install TorchRL in development mode using setup.py\nSOURCE: https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Installing mujoco-py via pip\nDESCRIPTION: Simple installation of mujoco-py using pip package manager (not recommended for GPU rendering).\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nconda activate mujoco_env\npip install mujoco-py\n```\n\n----------------------------------------\n\nTITLE: Installing TensorDict from GitHub\nDESCRIPTION: Command to install TensorDict directly from the GitHub repository\nSOURCE: https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/pytorch/tensordict\n```\n\n----------------------------------------\n\nTITLE: Verifying MuJoCo Installation\nDESCRIPTION: Python commands to verify successful installation and GPU support in mujoco-py.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport mujoco_py\nprint(mujoco_py.cymj)\n```\n\n----------------------------------------\n\nTITLE: Referencing Gym Library Paths\nDESCRIPTION: Code references showing the path to GymWrapper class in TorchRL for wrapping Gym environments.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/GYM.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\ntorchrl.envs.libs.gym.GymWrapper\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for Habitat\nDESCRIPTION: Creates a new conda environment named 'habitat' with Python 3.7 and CMake 3.14.0, then activates it.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/HABITAT.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n habitat python=3.7 cmake=3.14.0 \nconda activate habitat\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies for PyTorch RL\nDESCRIPTION: Lists required Python packages and dependencies for a PyTorch Reinforcement Learning implementation. Includes both PyPI packages and direct GitHub repository installations.\nSOURCE: https://github.com/pytorch/rl/blob/main/examples/rlhf/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndatasets\nhydra-core\nmatplotlib\nnumpy\nPyYAML\nrequests\ntiktoken\ntqdm\ntransformers\ngit+https://github.com/pytorch/rl\ngit+https://github.com/pytorch-labs/tensordict\n```\n\n----------------------------------------\n\nTITLE: Listing Available Habitat Environments in Python\nDESCRIPTION: Python code to check if Habitat is installed and list available Habitat environments using TorchRL.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/HABITAT.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchrl.envs.libs.habitat import HabitatEnv, _has_habitat\nassert _has_habitat  # checks that habitat is installed\nprint([_env for _env in HabitatEnv.available_envs if _env.startswith(\"Habitat\")])\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX Entry for TorchRL\nDESCRIPTION: BibTeX citation format for referencing the TorchRL library in academic work.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_13\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{bou2023torchrl,\n      title={TorchRL: A data-driven decision-making library for PyTorch}, \n      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},\n      year={2023},\n      eprint={2306.00577},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing RL algorithm parameters with Hydra\nDESCRIPTION: Example command demonstrating how to modify hyperparameters when running an RL algorithm using Hydra's command line configuration.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython sac.py collector.frames_per_batch=63\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for PyTorch RL Project\nDESCRIPTION: A requirements list specifying pytest-benchmark for performance testing and tenacity for implementing retry logic. These packages would be installed using pip install -r requirements.txt.\nSOURCE: https://github.com/pytorch/rl/blob/main/benchmarks/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytest-benchmark\ntenacity\n```\n\n----------------------------------------\n\nTITLE: Resolving libllvmlite.so Not Found Error\nDESCRIPTION: Bash commands to install llvmlite and set LD_PRELOAD to resolve the 'Could not find/load shared object file: libllvmlite.so' error.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/HABITAT.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge llvmlite\nconda env config vars set LD_PRELOAD=$LD_PRELOAD:/path/to/conda/envs/env_name/lib/python3.8/site-packages/llvmlite/binding/libllvmlite.so\nconda deactivate && conda activate env_name\n```\n\n----------------------------------------\n\nTITLE: Installing Hydra dependency for PyTorch RL examples\nDESCRIPTION: Command to install the hydra-core package, which is required to run the RL algorithm examples.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install hydra-core\n```\n\n----------------------------------------\n\nTITLE: Installing Weights & Biases for TorchRL\nDESCRIPTION: Simple pip command to install the Weights & Biases (wandb) package, which is commonly used for experiment tracking in reinforcement learning projects.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npip3 install wandb\n```\n\n----------------------------------------\n\nTITLE: Testing TorchRL Installation from Within Repository\nDESCRIPTION: Code snippet to check if TorchRL is properly installed in development mode by trying to import a module from the package. This helps diagnose import errors when working from within the git repository.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ncd ~/path/to/rl/repo\npython -c 'from torchrl.envs.libs.gym import GymEnv'\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for RLHF Example (Shell)\nDESCRIPTION: This command installs the necessary dependencies for running the RLHF example. It uses pip to install the requirements listed in the requirements.txt file.\nSOURCE: https://github.com/pytorch/rl/blob/main/examples/rlhf/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying PyTorch RL Version Number\nDESCRIPTION: A simple version number (0.7.0) that indicates the current version of the PyTorch Reinforcement Learning library. This is used for versioning and dependency management.\nSOURCE: https://github.com/pytorch/rl/blob/main/version.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n0.7.0\n```\n\n----------------------------------------\n\nTITLE: Training the Final Model with RLHF (Shell)\nDESCRIPTION: This command runs the train_rlhf.py script to train the final model using Reinforcement Learning with Human Feedback. It requires the reward model checkpoint to be available.\nSOURCE: https://github.com/pytorch/rl/blob/main/examples/rlhf/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython train_rlhf.py\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit for Code Linting\nDESCRIPTION: Command to install pre-commit tool for code linting and formatting checks\nSOURCE: https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for TorchRL API Reference in reStructuredText\nDESCRIPTION: This snippet defines a table of contents for the TorchRL API reference documentation using reStructuredText syntax. It sets up a maxdepth of 3 and lists the main sections of the library.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    :maxdepth: 3\n\n    collectors\n    data\n    envs\n    modules\n    objectives\n    trainers\n    utils\n```\n\n----------------------------------------\n\nTITLE: Installing MuJoCo Rendering Dependencies with APT\nDESCRIPTION: System-level installation of required libraries for MuJoCo rendering capabilities using apt-get.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install libglfw3 libglew2.0 libgl1-mesa-glx libosmesa6\n```\n\n----------------------------------------\n\nTITLE: Using a Transform with TransformedEnv in Python\nDESCRIPTION: Demonstrates how to use a transform with an environment by passing it to the TransformedEnv constructor.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/envs.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nenv = TransformedEnv(GymEnv(\"Pendulum-v1\"), AddOneToObs())\n```\n\n----------------------------------------\n\nTITLE: Chess Moves - Pawn Promotions\nDESCRIPTION: List of all possible pawn promotion moves in chess notation, showing promotions to Queen (Q), Rook (R), Bishop (B) and Knight (N), including moves ending in both check (+) and checkmate (#)\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\naxb1=B# axb1=B+ axb1=N# axb1=N+ [...]\n```\n\n----------------------------------------\n\nTITLE: GLIBCXX Library Version Error\nDESCRIPTION: Runtime error when required GLIBCXX version is not found in the system libraries.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nImportError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /path/to/conda/envs/compile/bin/../lib/libOSMesa.so.8)\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport LD_PRELOAD=$LD_PRELOAD:/path/to/conda/envs/compile/lib/libstdc++.so.6\n```\n\n----------------------------------------\n\nTITLE: REDQ Example Documentation in Markdown\nDESCRIPTION: Documentation note explaining that this REDQ example is not included in v0.3 release benchmarks and will be validated in future releases.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/redq/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# REDQ example\n\n## Note:\nThis example is not included in the benchmarked results of the current release (v0.3). The intention is to include it in the\nbenchmarking of future releases, to ensure that it can be successfully run with the release code and that the\nresults are consistent. For now, be aware that this additional check has not been performed in the case of this\nspecific example.\n```\n\n----------------------------------------\n\nTITLE: Chess Piece Capture Moves\nDESCRIPTION: Standardized chess notation for capture moves involving bishops (B), knights (N), and queens (Q). Includes moves ending in check (#) and non-check (+) positions.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nbxc8=R\ncxb1=B\ncxb1=N\ncxb1=Q\ncxb1=R\ncxb8=B\ncxb8=N\ncxb8=Q\ncxb8=R\ncxd1=B\ncxd1=N\ncxd1=Q\ncxd1=R\ncxd8=B\ncxd8=N\ncxd8=Q\ncxd8=R\ndxc1=B\ndxc1=N\ndxc1=Q\ndxc1=R...\n```\n\n----------------------------------------\n\nTITLE: Chess Queen Move Notation - D-F Files\nDESCRIPTION: Standard chess notation showing all possible Queen moves from d, e and f files, including normal moves, captures (x), checks (+) and checkmates (#). The notation follows standard algebraic chess notation format.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_8\n\nLANGUAGE: chess\nCODE:\n```\nQd6h2# Qd6h2+ Qd6xa3 Qd6xb4 Qd6xb6 Qd6xb8 Qd6xc5 Qd6xc6 Qd6xc7 Qd6xd2 Qd6xd3 Qd6xd4 Qd6xd5 Qd6xd7 Qd6xe5 Qd6xe6 Qd6xe7 Qd6xf4 Qd6xf6 Qd6xf8 Qd6xg3 Qd6xg6 Qd6xh2 ...\n```\n\n----------------------------------------\n\nTITLE: Chess Notation - Rook Moves\nDESCRIPTION: Standard algebraic notation for Rook moves on a chess board, showing possible moves from ranks 1 and 2, including regular moves, captures, checks and checkmates.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_7\n\nLANGUAGE: chess\nCODE:\n```\nR1a2# R1a2+ R1a3# R1a3+ ... R2b7# R2b7+\n```\n\n----------------------------------------\n\nTITLE: Chess Algebraic Notation - Knight Moves\nDESCRIPTION: Standard chess notation for knight moves including regular moves, captures, checks and checkmates. Uses format N[from][to][x/+/#] where N represents knight, from/to are square coordinates, x indicates capture, + indicates check, and # indicates checkmate.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_2\n\nLANGUAGE: chess-notation\nCODE:\n```\nNec2+ Nec3# Nec3+ Nec4# ... Nf8g6 Nfd1# Nfd1+\n```\n\n----------------------------------------\n\nTITLE: Displaying Script Usage with Help Option\nDESCRIPTION: Command to show the usage instructions for the submitit-release-check.sh script\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-check/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./submitit-release-check.sh --help\n```\n\n----------------------------------------\n\nTITLE: Benchmarking values Method on Lazy-Stacked TensorDict\nDESCRIPTION: Measures the execution time of retrieving all values from the lazy-stacked TensorDict using the values method.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\nlist(tdls.values(True, True))\n```\n\n----------------------------------------\n\nTITLE: Mujoco Runtime Error Stack\nDESCRIPTION: OpenGL initialization error when running jobs using schedulers like slurm.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    File \"mjrendercontext.pyx\", line 46, in mujoco_py.cymj.MjRenderContext.__init__\n\n    File \"mjrendercontext.pyx\", line 114, in mujoco_py.cymj.    MjRenderContext._setup_opengl_context\n\n    File \"opengl_context.pyx\", line 130, in mujoco_py.cymj.OffscreenOpenGLContext.__init__\n\nRuntimeError: Failed to initialize OpenGL\n```\n\n----------------------------------------\n\nTITLE: Benchmarking tree_map on Lazy-Stacked TensorDict\nDESCRIPTION: Measures the execution time of applying tree_map function to add 1 to each element in the lazy-stacked TensorDict.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntree_map(lambda x: x+1, tdls)\n```\n\n----------------------------------------\n\nTITLE: GL/glew.h Header Error\nDESCRIPTION: Compilation error when GL/glew.h header file is missing from the system.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_8\n\nLANGUAGE: c\nCODE:\n```\n/path/to/mujoco-py/mujoco_py/gl/eglshim.c:4:10: fatal error: GL/glew.h: No such file or directory\n4 | #include <GL/glew.h>\n  |          ^~~~~~~~~~~\n```\n\n----------------------------------------\n\nTITLE: GL/glu.h Dependency Error\nDESCRIPTION: Compilation error when GL/gl.h header file is missing, typically resolved by installing mesalib.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_9\n\nLANGUAGE: c\nCODE:\n```\ninclude/GL/glu.h:38:10: fatal error: GL/gl.h: No such file or directory\n  #include <GL/gl.h>\n           ^~~~~~~~~\n```\n\n----------------------------------------\n\nTITLE: X11 Header Missing Error\nDESCRIPTION: Compilation error when X11/Xlib.h header file is missing from the system.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_12\n\nLANGUAGE: c\nCODE:\n```\nfatal error: X11/Xlib.h: No such file or directory\n   | #include <X11/Xlib.h>\n   |          ^~~~~~~~~~~~\n```\n\n----------------------------------------\n\nTITLE: Osmesa Header Missing Error\nDESCRIPTION: Compilation error when GL/osmesa.h header file is missing from the system.\nSOURCE: https://github.com/pytorch/rl/blob/main/knowledge_base/MUJOCO_INSTALLATION.md#2025-04-22_snippet_13\n\nLANGUAGE: c\nCODE:\n```\nfatal error: GL/osmesa.h: No such file or directory\n    1 | #include <GL/osmesa.h>\n      |          ^~~~~~~~~~~~~\ncompilation terminated.\n```\n\n----------------------------------------\n\nTITLE: Benchmarking apply Method on Lazy-Stacked TensorDict\nDESCRIPTION: Measures the execution time of using the apply method to add 1 to each element in the lazy-stacked TensorDict.\nSOURCE: https://github.com/pytorch/rl/blob/main/pytree.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntdls.apply(lambda x: x+1)\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Function Documentation Template in Jinja\nDESCRIPTION: A Jinja template that sets up the documentation structure for a Python function using Sphinx directives. The template includes module context setting and automatic function documentation generation.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/_templates/function.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autofunction:: {{ name }}\n```\n\n----------------------------------------\n\nTITLE: Setting up Knowledge Base Documentation in Sphinx with reStructuredText\nDESCRIPTION: This snippet configures the knowledge base section of the PyTorch RL documentation. It includes the main README file from the knowledge base directory and sets up a table of contents that will display all files in the generated/knowledge_base directory.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/reference/knowledge_base.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nKnowledge Base\n==============\n\n.. _ref_knowledge_base:\n\n.. include:: ../../../knowledge_base/README.md\n   :start-line: 1\n   :parser: myst_parser.sphinx_\n\n.. toctree::\n    :glob:\n    :maxdepth: 1\n    :caption: Contents:\n\n    generated/knowledge_base/*\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Class Documentation Template with Jinja2\nDESCRIPTION: A Jinja2 template that sets up the structure for documenting a Python class using Sphinx. It uses currentmodule directive to set the module context and autoclass directive to automatically document the class and its members.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/_templates/rl_template_noinherit.rst#2025-04-22_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n\n```\n\n----------------------------------------\n\nTITLE: Generating Class Documentation for PyTorch RL Module in reStructuredText\nDESCRIPTION: This snippet defines a template for documenting classes in PyTorch's reinforcement learning module. It sets the current module context, creates a heading for the class name, and uses autodoc to include all members and inherited members of the class.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/_templates/rl_template.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRL from PyPI\nDESCRIPTION: Command to install the TorchRL package using pip from the Python Package Index (PyPI).\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install torchrl\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for TorchRL\nDESCRIPTION: Commands to create and activate a new conda environment for TorchRL installation.\nSOURCE: https://github.com/pytorch/rl/blob/main/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name torch_rl python=3.9\nconda activate torch_rl\n```\n\n----------------------------------------\n\nTITLE: Chess Algebraic Notation - Castling\nDESCRIPTION: Standard chess notation for castling kingside (O-O) and queenside (O-O-O).\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_4\n\nLANGUAGE: chess-notation\nCODE:\n```\nO-O-O\n```\n\n----------------------------------------\n\nTITLE: Installing VMAS and Dependencies\nDESCRIPTION: Commands to install VMAS simulator and required dependencies including wandb, moviepy, and hydra-core.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/multiagent/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install vmas\npip install wandb \"moviepy<2.0.0\"\npip install hydra-core\n```\n\n----------------------------------------\n\nTITLE: Running basic RL algorithm examples with Hydra\nDESCRIPTION: Command showing how to run a RL algorithm example script from its directory.\nSOURCE: https://github.com/pytorch/rl/blob/main/sota-implementations/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython sac.py\n```\n\n----------------------------------------\n\nTITLE: Chess Algebraic Notation - Queen Moves\nDESCRIPTION: Standard chess notation for queen moves including regular moves, captures, checks and checkmates. Uses format Q[number][to][x/+/#] where Q represents queen, number indicates which queen if multiple queens, to is destination square, x indicates capture, + indicates check, and # indicates checkmate.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_3\n\nLANGUAGE: chess-notation\nCODE:\n```\nQ1a1# Q1a1+ Q1a2# ... Q7d2+ Q7d3#\n```\n\n----------------------------------------\n\nTITLE: Chess Queen Move Notation List\nDESCRIPTION: A systematic listing of possible Queen moves in chess using standard algebraic notation. Includes regular moves, captures (x), checks (+), and checkmates (#) from various board positions.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nQ7d4# Q7d4+ Q7d5# Q7d5+ Q7d6# Q7d6+ Q7d7# Q7d7+ Q7d8# Q7d8+ Q7e2# Q7e2+ Q7e3# Q7e3+ Q7e4# Q7e4+ Q7e5# Q7e5+ Q7e6# Q7e6+ Q7e7# Q7e7+ Q7e8# Q7e8+ Q7f2# Q7f2+ Q7f3# Q7f3+ Q7f4# Q7f4+ Q7f5# Q7f5+ Q7f6# Q7f6+ Q7f7# Q7f7+ Q7f8# Q7f8+ Q7g1# Q7g1+ Q7g2# Q7g2+ Q7g3# Q7g3+ Q7g4# Q7g4+ Q7g5# Q7g5+ Q7g6# Q7g6+ Q7g7# Q7g7+ Q7g8# Q7g8+ Q7h1# Q7h1+ Q7h2# Q7h2+ Q7h3# Q7h3+ Q7h4# Q7h4+ Q7h5# Q7h5+ Q7h6# Q7h6+ Q7h7# Q7h7+ Q7h8# Q7h8+\n```\n\n----------------------------------------\n\nTITLE: Chess Moves - Queen Captures\nDESCRIPTION: List of all possible queen capture moves in chess notation, including moves ending in both check (+) and checkmate (#)\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nQf3xc3# Qf3xc3+ Qf3xc6# Qf3xc6+ [...]\n```\n\n----------------------------------------\n\nTITLE: Chess Move Notation List\nDESCRIPTION: A structured list of chess moves in standard algebraic notation showing possible piece movements, captures, checks and checkmates. Organized by piece type (B, N, K) and destination squares.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nBf2d4\nBf2e3\nBf2g3\nBf3d5\nBf3e2\nBf3e4\n[...additional moves...]\n```\n\n----------------------------------------\n\nTITLE: Bishop Move Notation List\nDESCRIPTION: Collection of legal bishop moves in standard chess notation, including captures, checks, and checkmates. Organized by starting position and move type.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nRhb8\nRhc1\nRhc2\n[...truncated for length...]\n```\n\n----------------------------------------\n\nTITLE: Sphinx Class Documentation Template\nDESCRIPTION: A reStructuredText template that uses Jinja templating to generate class documentation. Sets the current module scope and generates class documentation with member listings.\nSOURCE: https://github.com/pytorch/rl/blob/main/docs/source/_templates/class.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Chess Notation - Queen Moves\nDESCRIPTION: Standard algebraic notation for Queen moves on a chess board, including regular moves (e.g. Qe4), captures (Qxe4), checks (Qe4+) and checkmates (Qe4#).\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_6\n\nLANGUAGE: chess\nCODE:\n```\nQe1a5 Qe1b1 Qe1b4 Qe1c1 Qe1c3 ... Qe8h5 Qea1# Qea1+\n```\n\n----------------------------------------\n\nTITLE: Queen Capture Moves in Chess Algebraic Notation\nDESCRIPTION: Standard chess notation showing queen captures from various positions on the board. Each move follows the pattern Q[starting position]x[ending position][checkmate/check symbol]. Includes both checkmate (#) and check (+) variations.\nSOURCE: https://github.com/pytorch/rl/blob/main/torchrl/envs/custom/san_moves.txt#2025-04-22_snippet_10\n\nLANGUAGE: chess\nCODE:\n```\nQb2xe2#\nQb2xe2+\nQb2xe5#\nQb2xe5+\nQb2xf2#\nQb2xf2+\nQb2xf6#\nQb2xf6+\nQb2xg2#\nQb2xg2+\nQb2xg7#\nQb2xg7+\nQb2xh8#\nQb2xh8+\n```"
  }
]
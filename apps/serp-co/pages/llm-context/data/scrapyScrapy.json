[
  {
    "owner": "scrapy",
    "repo": "scrapy",
    "content": "TITLE: Implementing a Basic Quote-Scraping Spider in Python with Scrapy\nDESCRIPTION: This code defines a Scrapy spider that scrapes quotes from the 'quotes.toscrape.com' website. It uses CSS and XPath selectors to extract quote text and author information, and implements pagination by following the 'next' link when available. The spider yields Python dictionaries that will be automatically converted to JSON output.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/overview.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        \"https://quotes.toscrape.com/tag/humor/\",\n    ]\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"author\": quote.xpath(\"span/small/text()\").get(),\n                \"text\": quote.css(\"span.text::text\").get(),\n            }\n\n        next_page = response.css('li.next a::attr(\"href\")').get()\n        if next_page is not None:\n            yield response.follow(next_page, self.parse)\n```\n\n----------------------------------------\n\nTITLE: Using ItemLoader to Populate a Product Item in a Scrapy Spider\nDESCRIPTION: This snippet demonstrates a typical ItemLoader usage in a Scrapy spider parse method. It shows how to create an ItemLoader instance, add data from different sources using add_xpath, add_css, and add_value methods, and finally return the populated item.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.loader import ItemLoader\nfrom myproject.items import Product\n\n\ndef parse(self, response):\n    l = ItemLoader(item=Product(), response=response)\n    l.add_xpath(\"name\", '//div[@class=\"product_name\"]')\n    l.add_xpath(\"name\", '//div[@class=\"product_title\"]')\n    l.add_xpath(\"price\", '//p[@id=\"price\"]')\n    l.add_css(\"stock\", \"p#stock\")\n    l.add_value(\"last_updated\", \"today\")  # you can also use literal values\n    return l.load_item()\n```\n\n----------------------------------------\n\nTITLE: Extracting Multiple Elements from a Web Page using Scrapy in Python\nDESCRIPTION: Demonstrates how to extract multiple elements (quotes, authors, and tags) from a web page using Scrapy selectors and create a dictionary for each set of data.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_6\n\nLANGUAGE: pycon\nCODE:\n```\n>>> for quote in response.css(\"div.quote\"):\n...     text = quote.css(\"span.text::text\").get()\n...     author = quote.css(\"small.author::text\").get()\n...     tags = quote.css(\"div.tags a.tag::text\").getall()\n...     print(dict(text=text, author=author, tags=tags))\n...\n{'text': '\"The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\"', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n{'text': '\"It is our choices, Harry, that show what we truly are, far more than our abilities.\"', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n...\n```\n\n----------------------------------------\n\nTITLE: Running a Single Spider with CrawlerProcess in Python\nDESCRIPTION: This snippet demonstrates how to run a single Scrapy spider using CrawlerProcess, which starts a Twisted reactor and configures logging automatically. It includes configuration for JSON output and blocks until crawling is finished.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/practices.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"FEEDS\": {\n            \"items.json\": {\"format\": \"json\"},\n        },\n    }\n)\n\nprocess.crawl(MySpider)\nprocess.start()  # the script will block here until the crawling is finished\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ItemLoader in Scrapy (Python)\nDESCRIPTION: Demonstrates how to create an ItemLoader instance, add values to fields, and load the resulting item. Shows the use of input and output processors for data cleaning.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_5\n\nLANGUAGE: pycon\nCODE:\n```\n>>> from scrapy.loader import ItemLoader\n>>> il = ItemLoader(item=Product())\n>>> il.add_value(\"name\", [\"Welcome to my\", \"<strong>website</strong>\"])\n>>> il.add_value(\"price\", [\"&euro;\", \"<span>1000</span>\"])\n>>> il.load_item()\n{'name': 'Welcome to my website', 'price': '1000'}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Scrapy Spider with Multiple Callbacks\nDESCRIPTION: Example of a basic Scrapy spider that parses multiple pages, extracts items, and follows links to item detail pages, passing partially populated items between callbacks using cb_kwargs.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/debug.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom myproject.items import MyItem\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n    start_urls = (\n        \"http://example.com/page1\",\n        \"http://example.com/page2\",\n    )\n\n    def parse(self, response):\n        # <processing code not shown>\n        # collect `item_urls`\n        for item_url in item_urls:\n            yield scrapy.Request(item_url, self.parse_item)\n\n    def parse_item(self, response):\n        # <processing code not shown>\n        item = MyItem()\n        # populate `item` fields\n        # and extract item_details_url\n        yield scrapy.Request(\n            item_details_url, self.parse_details, cb_kwargs={\"item\": item}\n        )\n\n    def parse_details(self, response, item):\n        # populate more `item` fields\n        return item\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Start Requests in Scrapy Spider\nDESCRIPTION: This snippet demonstrates how to override the start_requests() method in a Scrapy spider to perform custom initialization, such as logging in before scraping. It shows sending a POST request for authentication.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n\n    def start_requests(self):\n        return [\n            scrapy.FormRequest(\n                \"http://www.example.com/login\",\n                formdata={\"user\": \"john\", \"pass\": \"secret\"},\n                callback=self.logged_in,\n            )\n        ]\n\n    def logged_in(self, response):\n        # here you would extract links to follow and return Requests for\n        # each of them, with another callback\n        pass\n```\n\n----------------------------------------\n\nTITLE: Updating Spider Settings in Scrapy\nDESCRIPTION: This code shows how to use the update_settings() class method to dynamically modify spider settings. In this example, it updates the FEEDS setting with custom feed configuration.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n    custom_feed = {\n        \"/home/user/documents/items.json\": {\n            \"format\": \"json\",\n            \"indent\": 4,\n        }\n    }\n\n    @classmethod\n    def update_settings(cls, settings):\n        super().update_settings(settings)\n        settings.setdefault(\"FEEDS\", {}).update(cls.custom_feed)\n```\n\n----------------------------------------\n\nTITLE: Simplified Scrapy Spider with start_urls - Python\nDESCRIPTION: A simplified version of the spider that uses start_urls class attribute instead of start_requests method. This approach provides a shorter way to define initial URLs for crawling.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        \"https://quotes.toscrape.com/page/1/\",\n        \"https://quotes.toscrape.com/page/2/\",\n    ]\n\n    def parse(self, response):\n        page = response.url.split(\"/\")[-2]\n        filename = f\"quotes-{page}.html\"\n        Path(filename).write_bytes(response.body)\n```\n\n----------------------------------------\n\nTITLE: Spider with Command Line Arguments\nDESCRIPTION: Spider implementation demonstrating how to handle command line arguments to filter content based on tags.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n\n    def start_requests(self):\n        url = \"https://quotes.toscrape.com/\"\n        tag = getattr(self, \"tag\", None)\n        if tag is not None:\n            url = url + \"tag/\" + tag\n        yield scrapy.Request(url, self.parse)\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n            }\n\n        next_page = response.css(\"li.next a::attr(href)\").get()\n        if next_page is not None:\n            yield response.follow(next_page, self.parse)\n```\n\n----------------------------------------\n\nTITLE: Working with Attributes in Scrapy Selectors\nDESCRIPTION: Shows multiple ways to access element attributes using Scrapy's attrib property and selector methods, both for individual elements and lists of elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> [img.attrib[\"src\"] for img in response.css(\"img\")]\n['image1_thumb.jpg',\n'image2_thumb.jpg',\n'image3_thumb.jpg',\n'image4_thumb.jpg',\n'image5_thumb.jpg']\n\n>>> response.css(\"img\").attrib[\"src\"]\n'image1_thumb.jpg'\n\n>>> response.css(\"base\").attrib[\"href\"]\n'http://example.com/'\n```\n\n----------------------------------------\n\nTITLE: Returning Multiple Requests and Items from Scrapy Spider Callback\nDESCRIPTION: This example demonstrates how to yield multiple Requests and items from a single callback in a Scrapy spider. It extracts titles and follows links from the response.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"example.com\"\n    allowed_domains = [\"example.com\"]\n    start_urls = [\n        \"http://www.example.com/1.html\",\n        \"http://www.example.com/2.html\",\n        \"http://www.example.com/3.html\",\n    ]\n\n    def parse(self, response):\n        for h3 in response.xpath(\"//h3\").getall():\n            yield {\"title\": h3}\n\n        for href in response.xpath(\"//a/@href\").getall():\n            yield scrapy.Request(response.urljoin(href), self.parse)\n```\n\n----------------------------------------\n\nTITLE: Selecting Text with XPath in Scrapy\nDESCRIPTION: Shows how to use XPath expressions to select text content from HTML elements and extract both single and multiple results using get() and getall() methods.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> response.xpath(\"//title/text()\")\n[<Selector query='//title/text()' data='Example website'>]\n\n>>> response.xpath(\"//title/text()\").getall()\n['Example website']\n>>> response.xpath(\"//title/text()\").get()\n'Example website'\n```\n\n----------------------------------------\n\nTITLE: Defining Scrapy Items with ItemFields in Python\nDESCRIPTION: This code snippet demonstrates how to define Scrapy items using the new ItemField API. It includes examples of nested items and list fields, showcasing the flexibility of the proposed implementation.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-003.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.item.models import Item\nfrom scrapy.item.fields import ListField, ItemField, TextField, UrlField, DecimalField\n\n\nclass Supplier(Item):\n    name = TextField(default=\"anonymous supplier\")\n    url = UrlField()\n\n\nclass Variant(Item):\n    name = TextField(required=True)\n    url = UrlField()\n    price = DecimalField()\n\n\nclass Product(Variant):\n    supplier = ItemField(Supplier, default=Supplier(name=\"default supplier\"))\n    variants = ListField(ItemField(Variant))\n\n    # these ones are used for documenting default value examples\n    supplier2 = ItemField(Supplier)\n    variants2 = ListField(ItemField(Variant), default=[])\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Scrapy Spider in Python\nDESCRIPTION: This snippet shows how to implement a basic Scrapy spider with a parse method to handle responses from start URLs. It demonstrates logging response URLs and basic structure of a Spider class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"example.com\"\n    allowed_domains = [\"example.com\"]\n    start_urls = [\n        \"http://www.example.com/1.html\",\n        \"http://www.example.com/2.html\",\n        \"http://www.example.com/3.html\",\n    ]\n\n    def parse(self, response):\n        self.logger.info(\"A response from %s just arrived!\", response.url)\n```\n\n----------------------------------------\n\nTITLE: Implementing CrawlSpider with Rules in Python\nDESCRIPTION: Example of a CrawlSpider that crawls example.com using rules to follow category links and parse item pages. Uses LinkExtractor to define crawling patterns and implements methods for parsing item data with XPath.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\n\nclass MySpider(CrawlSpider):\n    name = \"example.com\"\n    allowed_domains = [\"example.com\"]\n    start_urls = [\"http://www.example.com\"]\n\n    rules = (\n        # Extract links matching 'category.php' (but not matching 'subsection.php')\n        # and follow links from them (since no callback means follow=True by default).\n        Rule(LinkExtractor(allow=(r\"category\\.php\",), deny=(r\"subsection\\.php\",))),\n        # Extract links matching 'item.php' and parse them with the spider's method parse_item\n        Rule(LinkExtractor(allow=(r\"item\\.php\",)), callback=\"parse_item\"),\n    )\n\n    def parse_item(self, response):\n        self.logger.info(\"Hi, this is an item page! %s\", response.url)\n        item = scrapy.Item()\n        item[\"id\"] = response.xpath('//td[@id=\"item_id\"]/text()').re(r\"ID: (\\d+)\")\n        item[\"name\"] = response.xpath('//td[@id=\"item_name\"]/text()').get()\n        item[\"description\"] = response.xpath(\n            '//td[@id=\"item_description\"]/text()'\n        ).get()\n        item[\"link_text\"] = response.meta[\"link_text\"]\n        url = response.xpath('//td[@id=\"additional_data\"]/@href').get()\n        return response.follow(\n            url, self.parse_additional_page, cb_kwargs=dict(item=item)\n        )\n\n    def parse_additional_page(self, response, item):\n        item[\"additional_data\"] = response.xpath(\n            '//p[@id=\"additional_data\"]/text()'\n        ).get()\n        return item\n```\n\n----------------------------------------\n\nTITLE: SitemapSpider with Multiple Callbacks in Python\nDESCRIPTION: This example demonstrates how to use SitemapSpider with different callbacks for different URL patterns. It processes product and category URLs separately.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import SitemapSpider\n\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = [\"http://www.example.com/sitemap.xml\"]\n    sitemap_rules = [\n        (\"/product/\", \"parse_product\"),\n        (\"/category/\", \"parse_category\"),\n    ]\n\n    def parse_product(self, response):\n        pass  # ... scrape product ...\n\n    def parse_category(self, response):\n        pass  # ... scrape category ...\n```\n\n----------------------------------------\n\nTITLE: Combining SitemapSpider with Other URL Sources in Python\nDESCRIPTION: This example demonstrates how to combine SitemapSpider with other sources of URLs. It processes both sitemap URLs and additional URLs specified in the other_urls list.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import SitemapSpider\n\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = [\"http://www.example.com/robots.txt\"]\n    sitemap_rules = [\n        (\"/shop/\", \"parse_shop\"),\n    ]\n\n    other_urls = [\"http://www.example.com/about\"]\n\n    def start_requests(self):\n        requests = list(super(MySpider, self).start_requests())\n        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]\n        return requests\n\n    def parse_shop(self, response):\n        pass  # ... scrape shop here ...\n\n    def parse_other(self, response):\n        pass  # ... scrape other here ...\n```\n\n----------------------------------------\n\nTITLE: Declaring a Custom ItemLoader with Processors in Scrapy\nDESCRIPTION: This example shows how to define a custom ItemLoader class with specified input and output processors for different fields. It demonstrates using MapCompose, Join, and TakeFirst processors from itemloaders.processors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom itemloaders.processors import TakeFirst, MapCompose, Join\nfrom scrapy.loader import ItemLoader\n\n\nclass ProductLoader(ItemLoader):\n    default_output_processor = TakeFirst()\n\n    name_in = MapCompose(str.title)\n    name_out = Join()\n\n    price_in = MapCompose(str.strip)\n\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Using start_requests and Item Objects in Scrapy Spider\nDESCRIPTION: This snippet shows how to use the start_requests method instead of start_urls, and how to use Item objects to give data more structure in a Scrapy spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom myproject.items import MyItem\n\n\nclass MySpider(scrapy.Spider):\n    name = \"example.com\"\n    allowed_domains = [\"example.com\"]\n\n    def start_requests(self):\n        yield scrapy.Request(\"http://www.example.com/1.html\", self.parse)\n        yield scrapy.Request(\"http://www.example.com/2.html\", self.parse)\n        yield scrapy.Request(\"http://www.example.com/3.html\", self.parse)\n\n    def parse(self, response):\n        for h3 in response.xpath(\"//h3\").getall():\n            yield MyItem(title=h3)\n\n        for href in response.xpath(\"//a/@href\").getall():\n            yield scrapy.Request(response.urljoin(href), self.parse)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Scrapy Spider - Python\nDESCRIPTION: Creates a basic Scrapy spider that crawls quotes.toscrape.com and saves the HTML content of each page. The spider uses start_requests method to define initial URLs and parse method to handle responses.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n\n    def start_requests(self):\n        urls = [\n            \"https://quotes.toscrape.com/page/1/\",\n            \"https://quotes.toscrape.com/page/2/\",\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n        page = response.url.split(\"/\")[-2]\n        filename = f\"quotes-{page}.html\"\n        Path(filename).write_bytes(response.body)\n        self.log(f\"Saved file {filename}\")\n```\n\n----------------------------------------\n\nTITLE: Modifying Spider Settings in from_crawler Method\nDESCRIPTION: Shows how to modify spider settings in the from_crawler method based on spider arguments or other logic.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        if \"some_argument\" in kwargs:\n            spider.settings.set(\n                \"SOME_SETTING\", kwargs[\"some_argument\"], priority=\"spider\"\n            )\n        return spider\n```\n\n----------------------------------------\n\nTITLE: Accessing Selectors from Response Objects in Scrapy\nDESCRIPTION: Demonstrates how to access the selector object from a Scrapy response and use XPath and CSS shortcuts to query HTML elements. These methods allow extracting data without manually constructing selector objects.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> response.selector.xpath(\"//span/text()\").get()\n'good'\n\n>>> response.xpath(\"//span/text()\").get()\n'good'\n>>> response.css(\"span::text\").get()\n'good'\n```\n\n----------------------------------------\n\nTITLE: Passing data to callback functions in Scrapy (Python)\nDESCRIPTION: This snippet demonstrates how to pass arguments to callback functions in Scrapy using the Request.cb_kwargs attribute. It shows how to create a request with callback arguments and access them in the callback function.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef parse(self, response):\n    request = scrapy.Request(\n        \"http://www.example.com/index.html\",\n        callback=self.parse_page2,\n        cb_kwargs=dict(main_url=response.url),\n    )\n    request.cb_kwargs[\"foo\"] = \"bar\"  # add more arguments for the callback\n    yield request\n\n\ndef parse_page2(self, response, main_url, foo):\n    yield dict(\n        main_url=main_url,\n        other_url=response.url,\n        foo=foo,\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Price Validation Pipeline in Python for Scrapy\nDESCRIPTION: A pipeline component that adjusts prices to include VAT when needed and drops items without prices. It uses ItemAdapter to access item fields and raises DropItem exception for items missing prices.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/item-pipeline.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom itemadapter import ItemAdapter\nfrom scrapy.exceptions import DropItem\n\n\nclass PricePipeline:\n    vat_factor = 1.15\n\n    def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        if adapter.get(\"price\"):\n            if adapter.get(\"price_excludes_vat\"):\n                adapter[\"price\"] = adapter[\"price\"] * self.vat_factor\n            return item\n        else:\n            raise DropItem(\"Missing price\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Scrapy Spider to Extract Quotes Data in Python\nDESCRIPTION: Shows how to implement a Scrapy spider that extracts quotes, authors, and tags from multiple pages of a website and yields the data as dictionaries.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        \"https://quotes.toscrape.com/page/1/\",\n        \"https://quotes.toscrape.com/page/2/\",\n    ]\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n            }\n```\n\n----------------------------------------\n\nTITLE: Implementing XMLFeedSpider in Python\nDESCRIPTION: Example of an XMLFeedSpider that parses XML feeds by iterating through item nodes. Demonstrates how to extract data from XML nodes and create items.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import XMLFeedSpider\nfrom myproject.items import TestItem\n\n\nclass MySpider(XMLFeedSpider):\n    name = \"example.com\"\n    allowed_domains = [\"example.com\"]\n    start_urls = [\"http://www.example.com/feed.xml\"]\n    iterator = \"iternodes\"  # This is actually unnecessary, since it's the default value\n    itertag = \"item\"\n\n    def parse_node(self, response, node):\n        self.logger.info(\n            \"Hi, this is a <%s> node!: %s\", self.itertag, \"\".join(node.getall())\n        )\n\n        item = TestItem()\n        item[\"id\"] = node.xpath(\"@id\").get()\n        item[\"name\"] = node.xpath(\"name\").get()\n        item[\"description\"] = node.xpath(\"description\").get()\n        return item\n```\n\n----------------------------------------\n\nTITLE: Extracting URLs with XPath and CSS in Scrapy\nDESCRIPTION: Demonstrates various techniques for extracting URLs and attributes from HTML documents using both XPath and CSS selectors, showing equivalent operations in both selector types.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> response.xpath(\"//base/@href\").get()\n'http://example.com/'\n\n>>> response.css(\"base::attr(href)\").get()\n'http://example.com/'\n\n>>> response.css(\"base\").attrib[\"href\"]\n'http://example.com/'\n\n>>> response.xpath('//a[contains(@href, \"image\")]/@href').getall()\n['image1.html',\n'image2.html',\n'image3.html',\n'image4.html',\n'image5.html']\n\n>>> response.css(\"a[href*=image]::attr(href)\").getall()\n['image1.html',\n'image2.html',\n'image3.html',\n'image4.html',\n'image5.html']\n\n>>> response.xpath('//a[contains(@href, \"image\")]/img/@src').getall()\n['image1_thumb.jpg',\n'image2_thumb.jpg',\n'image3_thumb.jpg',\n'image4_thumb.jpg',\n'image5_thumb.jpg']\n\n>>> response.css(\"a[href*=image] img::attr(src)\").getall()\n['image1_thumb.jpg',\n'image2_thumb.jpg',\n'image3_thumb.jpg',\n'image4_thumb.jpg',\n'image5_thumb.jpg']\n```\n\n----------------------------------------\n\nTITLE: Sending FormRequest POST in Scrapy Spider\nDESCRIPTION: Example of using FormRequest to simulate an HTML Form POST request in a Scrapy spider. It demonstrates how to send key-value fields via HTTP POST.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nreturn [\n    FormRequest(\n        url=\"http://www.example.com/post/action\",\n        formdata={\"name\": \"John Doe\", \"age\": \"27\"},\n        callback=self.after_post,\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Handling Spider Arguments in Scrapy\nDESCRIPTION: This example demonstrates how to handle spider arguments passed through the command line in a Scrapy spider's __init__ method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n\n    def __init__(self, category=None, *args, **kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.start_urls = [f\"http://www.example.com/categories/{category}\"]\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Using has-class XPath Extension with HTML Response in Scrapy\nDESCRIPTION: This example shows how to use the has-class XPath extension function to select elements with specific CSS classes. It demonstrates selecting paragraphs with different class combinations from an HTML document.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n>>> from scrapy.http import HtmlResponse\n>>> response = HtmlResponse(\n...     url=\"http://example.com\",\n...     body=\"\"\"\n... <html>\n...     <body>\n...         <p class=\"foo bar-baz\">First</p>\n...         <p class=\"foo\">Second</p>\n...         <p class=\"bar\">Third</p>\n...         <p>Fourth</p>\n...     </body>\n... </html>\n... \"\"\",\n...     encoding=\"utf-8\",\n... )\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON from Scrapy Response\nDESCRIPTION: Example showing how to extract JSON data from a Scrapy response and handle HTML/XML content embedded within JSON.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/dynamic-content.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndata = response.json()\nselector = Selector(data[\"html\"])\n```\n\n----------------------------------------\n\nTITLE: SitemapSpider with Robots.txt and Sitemap Filtering in Python\nDESCRIPTION: This example shows how to use SitemapSpider with robots.txt file and filter sitemaps based on URL patterns. It only follows sitemaps containing '/sitemap_shops' in their URL.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import SitemapSpider\n\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = [\"http://www.example.com/robots.txt\"]\n    sitemap_rules = [\n        (\"/shop/\", \"parse_shop\"),\n    ]\n    sitemap_follow = [\"/sitemap_shops\"]\n\n    def parse_shop(self, response):\n        pass  # ... scrape shop here ...\n```\n\n----------------------------------------\n\nTITLE: Returning Dictionaries in Spiders (Python)\nDESCRIPTION: Demonstrates the new ability to return explicit dictionaries from spider parse methods instead of Scrapy Items. This simplifies data collection in spiders.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\nclass MySpider(scrapy.Spider):\n    def parse(self, response):\n        return {'url': response.url}\n```\n\n----------------------------------------\n\nTITLE: Extracting Itemscope and Itemprop Data with XPath in Scrapy\nDESCRIPTION: This example demonstrates how to extract structured data from HTML with microdata (schema.org) using XPath. It iterates over itemscope elements and extracts their properties while excluding nested itemscope properties.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> sel = Selector(text=doc, type=\"html\")\n>>> for scope in sel.xpath(\"//div[@itemscope]\"):\n...     print(\"current scope:\", scope.xpath(\"@itemtype\").getall())\n...     props = scope.xpath(\n...         \"\"\"\n...                 set:difference(./descendant::*/@itemprop,\n...                                .//*[@itemscope]/*/@itemprop)\"\"\"\n...     )\n...     print(f\"    properties: {props.getall()}\")\n...     print(\"\")\n```\n\n----------------------------------------\n\nTITLE: Basic Recursive Spider Implementation\nDESCRIPTION: Implementation of a Scrapy spider that recursively follows pagination links while extracting quote data from each page.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        \"https://quotes.toscrape.com/page/1/\",\n    ]\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n            }\n\n        next_page = response.css(\"li.next a::attr(href)\").get()\n        if next_page is not None:\n            next_page = response.urljoin(next_page)\n            yield scrapy.Request(next_page, callback=self.parse)\n```\n\n----------------------------------------\n\nTITLE: Implementing MongoDB Storage Pipeline in Python for Scrapy\nDESCRIPTION: A pipeline that stores scraped items in MongoDB using pymongo. It gets MongoDB connection details from crawler settings, establishes connection when spider opens, and closes it when spider finishes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/item-pipeline.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pymongo\nfrom itemadapter import ItemAdapter\n\n\nclass MongoPipeline:\n    collection_name = \"scrapy_items\"\n\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get(\"MONGO_URI\"),\n            mongo_db=crawler.settings.get(\"MONGO_DATABASE\", \"items\"),\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider):\n        self.client.close()\n\n    def process_item(self, item, spider):\n        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())\n        return item\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Lines Writer Pipeline in Python for Scrapy\nDESCRIPTION: A pipeline that writes all scraped items to a single JSON Lines file (items.jsonl). It opens a file when the spider starts, writes each item as a JSON line, and closes the file when the spider finishes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/item-pipeline.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom itemadapter import ItemAdapter\n\n\nclass JsonWriterPipeline:\n    def open_spider(self, spider):\n        self.file = open(\"items.jsonl\", \"w\")\n\n    def close_spider(self, spider):\n        self.file.close()\n\n    def process_item(self, item, spider):\n        line = json.dumps(ItemAdapter(item).asdict()) + \"\\n\"\n        self.file.write(line)\n        return item\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Spiders Simultaneously with CrawlerProcess in Python\nDESCRIPTION: This snippet shows how to run multiple spiders in the same process using CrawlerProcess. It schedules multiple spiders to run simultaneously and blocks until all crawling jobs are complete.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/practices.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    ...\n\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    ...\n\n\nsettings = get_project_settings()\nprocess = CrawlerProcess(settings)\nprocess.crawl(MySpider1)\nprocess.crawl(MySpider2)\nprocess.start()  # the script will block here until all crawling jobs are finished\n```\n\n----------------------------------------\n\nTITLE: Configuring Feed Exports in Scrapy Python\nDESCRIPTION: Example of configuring feed exports using the FEEDS setting in Scrapy. Demonstrates setting export format, encoding, filters, and post-processing for different output files.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n    'items.json': {\n        'format': 'json',\n        'encoding': 'utf8',\n        'store_empty': False,\n        'item_classes': [MyItemClass1, 'myproject.items.MyItemClass2'],\n        'fields': None,\n        'indent': 4,\n        'item_export_kwargs': {\n           'export_empty_fields': True,\n        },\n    },\n    '/home/user/documents/items.xml': {\n        'format': 'xml',\n        'fields': ['name', 'price'],\n        'item_filter': MyCustomFilter1,\n        'encoding': 'latin1',\n        'indent': 8,\n    },\n    pathlib.Path('items.csv.gz'): {\n        'format': 'csv',\n        'fields': ['price', 'name'],\n        'item_filter': 'myproject.filters.MyCustomFilter2',\n        'postprocessing': [MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],\n        'gzip_compresslevel': 5,\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Spider Custom Settings via custom_settings Attribute\nDESCRIPTION: Shows how to define spider-specific settings using the custom_settings class attribute which will override project-level settings.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n\n    custom_settings = {\n        \"SOME_SETTING\": \"some value\",\n    }\n```\n\n----------------------------------------\n\nTITLE: Using has-class XPath Extension Function in Scrapy\nDESCRIPTION: This example demonstrates how to use the has-class XPath extension function to select elements with specific CSS classes. It shows different ways to select elements based on single or multiple class names.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n>>> response.xpath('//p[has-class(\"foo\")]')\n[<Selector query='//p[has-class(\"foo\")]' data='<p class=\"foo bar-baz\">First</p>'>,\n<Selector query='//p[has-class(\"foo\")]' data='<p class=\"foo\">Second</p>'>]\n>>> response.xpath('//p[has-class(\"foo\", \"bar-baz\")]')\n[<Selector query='//p[has-class(\"foo\", \"bar-baz\")]' data='<p class=\"foo bar-baz\">First</p>'>]\n>>> response.xpath('//p[has-class(\"foo\", \"bar\")]')\n[]\n```\n\n----------------------------------------\n\nTITLE: Implementing Fallback Component in Scrapy Add-on (Python)\nDESCRIPTION: Example of a Scrapy add-on that implements a custom download handler with fallback functionality. It demonstrates how to use a fallback setting and configure the handler to use either custom logic or the default handler based on request metadata.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/addons.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.core.downloader.handlers.http import HTTPDownloadHandler\nfrom scrapy.utils.misc import build_from_crawler\n\n\nFALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n\n\nclass MyHandler:\n    lazy = False\n\n    def __init__(self, settings, crawler):\n        dhcls = load_object(settings.get(FALLBACK_SETTING))\n        self._fallback_handler = build_from_crawler(dhcls, crawler)\n\n    def download_request(self, request, spider):\n        if request.meta.get(\"my_params\"):\n            # handle the request\n            ...\n        else:\n            return self._fallback_handler.download_request(request, spider)\n\n\nclass MyAddon:\n    def update_settings(self, settings):\n        if not settings.get(FALLBACK_SETTING):\n            settings.set(\n                FALLBACK_SETTING,\n                settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n                \"addon\",\n            )\n        settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler\n```\n\n----------------------------------------\n\nTITLE: Implementing Duplicates Filter Pipeline in Python for Scrapy\nDESCRIPTION: A pipeline that filters out duplicate items based on their ID. It maintains a set of seen IDs and drops items whose IDs have been processed before, preventing duplicate items from continuing through the pipeline.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/item-pipeline.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom itemadapter import ItemAdapter\nfrom scrapy.exceptions import DropItem\n\n\nclass DuplicatesPipeline:\n    def __init__(self):\n        self.ids_seen = set()\n\n    def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        if adapter[\"id\"] in self.ids_seen:\n            raise DropItem(f\"Item ID already seen: {adapter['id']}\")\n        else:\n            self.ids_seen.add(adapter[\"id\"])\n            return item\n```\n\n----------------------------------------\n\nTITLE: HTML Element Selection with XPath in Scrapy\nDESCRIPTION: This example demonstrates basic selector operations on HTML content. It shows how to select all <h1> elements from an HTML response body, returning a list of Selector objects.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nsel = Selector(html_response)\nsel.xpath(\"//h1\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Attributes with XPath in Python\nDESCRIPTION: Shows how to use XPath to extract attribute values from HTML elements, specifically the href attribute from anchor tags.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_13\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.xpath(\"//a/@href\").getall()\n['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']\n```\n\n----------------------------------------\n\nTITLE: Converting Deferred-based Item Pipeline to Async/Await in Scrapy\nDESCRIPTION: Comparison showing how to rewrite a Scrapy item pipeline from a Deferred-based approach to using async/await syntax. The example demonstrates fetching data from a database and updating an item field.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/coroutines.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom itemadapter import ItemAdapter\n\n\nclass DbPipeline:\n    def _update_item(self, data, item):\n        adapter = ItemAdapter(item)\n        adapter[\"field\"] = data\n        return item\n\n    def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        dfd = db.get_some_data(adapter[\"id\"])\n        dfd.addCallback(self._update_item, item)\n        return dfd\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom itemadapter import ItemAdapter\n\n\nclass DbPipeline:\n    async def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        adapter[\"field\"] = await db.get_some_data(adapter[\"id\"])\n        return item\n```\n\n----------------------------------------\n\nTITLE: Maintaining Cookie Sessions Across Requests in Scrapy\nDESCRIPTION: This example illustrates how to maintain the same cookie session across multiple requests by passing the cookiejar meta key in subsequent requests within a parse method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef parse_page(self, response):\n    # do some processing\n    return scrapy.Request(\n        \"http://www.example.com/otherpage\",\n        meta={\"cookiejar\": response.meta[\"cookiejar\"]},\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Parallel Requests in Scrapy Spider\nDESCRIPTION: Example of a Scrapy spider that makes multiple HTTP requests in parallel using DeferredList. The spider fetches price and color information from separate URLs and combines the results.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/coroutines.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy import Spider, Request\nfrom scrapy.utils.defer import maybe_deferred_to_future\nfrom twisted.internet.defer import DeferredList\n\n\nclass MultipleRequestsSpider(Spider):\n    name = \"multiple\"\n    start_urls = [\"https://example.com/product\"]\n\n    async def parse(self, response, **kwargs):\n        additional_requests = [\n            Request(\"https://example.com/price\"),\n            Request(\"https://example.com/color\"),\n        ]\n        deferreds = []\n        for r in additional_requests:\n            deferred = self.crawler.engine.download(r)\n            deferreds.append(deferred)\n        responses = await maybe_deferred_to_future(DeferredList(deferreds))\n        yield {\n            \"h1\": response.css(\"h1::text\").get(),\n            \"price\": responses[0][1].css(\".price::text\").get(),\n            \"price2\": responses[1][1].css(\".color::text\").get(),\n        }\n```\n\n----------------------------------------\n\nTITLE: Headless Browser Integration with Playwright\nDESCRIPTION: Demonstrates how to integrate a headless browser (Playwright) into a Scrapy spider for handling dynamic content.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/dynamic-content.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom playwright.async_api import async_playwright\n\nclass PlaywrightSpider(scrapy.Spider):\n    name = \"playwright\"\n    start_urls = [\"data:,\"]  # avoid using the default Scrapy downloader\n\n    async def parse(self, response):\n        async with async_playwright() as pw:\n            browser = await pw.chromium.launch()\n```\n\n----------------------------------------\n\nTITLE: Debugging Using inspect_response in Scrapy Shell\nDESCRIPTION: Demonstrates how to use the inspect_response function from scrapy.shell to debug a callback that sometimes receives no item, allowing inspection of the response in an interactive shell.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/debug.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.shell import inspect_response\n\n\ndef parse_details(self, response, item=None):\n    if item:\n        # populate more `item` fields\n        return item\n    else:\n        inspect_response(response, self)\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Signal Handling with Coroutines in Scrapy\nDESCRIPTION: This example shows how to implement asynchronous signal handling using coroutines, specifically for the item_scraped signal. The handler returns an awaitable object, allowing Scrapy to wait for the asynchronous operation to complete.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass SignalSpider(scrapy.Spider):\n    name = \"signals\"\n    start_urls = [\"https://quotes.toscrape.com/page/1/\"]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)\n        return spider\n\n    async def item_scraped(self, item):\n        # Send the scraped item to the server\n        response = await treq.post(\n            \"http://example.com/post\",\n            json.dumps(item).encode(\"ascii\"),\n            headers={b\"Content-Type\": [b\"application/json\"]},\n        )\n\n        return response\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n            }\n```\n\n----------------------------------------\n\nTITLE: Using LinkExtractor in Regular Spider Parse Method\nDESCRIPTION: Example showing how to use a link extractor in a regular spider callback method. The code demonstrates extracting links from a response and yielding Requests for each extracted link URL.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/link-extractors.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef parse(self, response):\n    for link in self.link_extractor.extract_links(response):\n        yield Request(link.url, callback=self.parse)\n```\n\n----------------------------------------\n\nTITLE: Stopping Response Download with StopDownload Exception in Python\nDESCRIPTION: A spider class demonstrating how to stop the download of a response by raising a StopDownload exception from a bytes_received signal handler.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass StopSpider(scrapy.Spider):\n    name = \"stop\"\n    start_urls = [\"https://docs.scrapy.org/en/latest/\"]\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        spider = super().from_crawler(crawler)\n        crawler.signals.connect(\n            spider.on_bytes_received, signal=scrapy.signals.bytes_received\n        )\n        return spider\n\n    def parse(self, response):\n        # 'last_chars' show that the full response was not downloaded\n        yield {\"len\": len(response.text), \"last_chars\": response.text[-40:]}\n\n    def on_bytes_received(self, data, request, spider):\n        raise scrapy.exceptions.StopDownload(fail=False)\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Loaders in Scrapy (Python)\nDESCRIPTION: Demonstrates how to use nested loaders to simplify xpath selections when parsing related values from a subsection of a document.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nloader = ItemLoader(item=Item())\n# load stuff not in the footer\nfooter_loader = loader.nested_xpath(\"//footer\")\nfooter_loader.add_xpath(\"social\", 'a[@class = \"social\"]/@href')\nfooter_loader.add_xpath(\"email\", 'a[@class = \"email\"]/@href')\n# no need to call footer_loader.load_item()\nloader.load_item()\n```\n\n----------------------------------------\n\nTITLE: Extracting First Match with Default Value in Scrapy\nDESCRIPTION: Demonstrates how to extract the first matching element and provide default values when elements are not found, improving error handling in scraping code.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> response.xpath('//div[@id=\"images\"]/a/text()').get()\n'Name: My image 1 '\n\n>>> response.xpath('//div[@id=\"not-exists\"]/text()').get() is None\nTrue\n\n>>> response.xpath('//div[@id=\"not-exists\"]/text()').get(default=\"not-found\")\n'not-found'\n```\n\n----------------------------------------\n\nTITLE: Chaining Selectors in Scrapy for Nested Data\nDESCRIPTION: Shows how to chain different selector methods to extract nested data from HTML documents, in this case getting image source attributes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> response.css(\"img\").xpath(\"@src\").getall()\n['image1_thumb.jpg',\n'image2_thumb.jpg',\n'image3_thumb.jpg',\n'image4_thumb.jpg',\n'image5_thumb.jpg']\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Inline Request in Scrapy Spider\nDESCRIPTION: Example of a Scrapy spider that makes an additional HTTP request within a callback and awaits its response. The spider downloads a product page and then fetches price information from another URL.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/coroutines.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy import Spider, Request\nfrom scrapy.utils.defer import maybe_deferred_to_future\n\n\nclass SingleRequestSpider(Spider):\n    name = \"single\"\n    start_urls = [\"https://example.org/product\"]\n\n    async def parse(self, response, **kwargs):\n        additional_request = Request(\"https://example.org/price\")\n        deferred = self.crawler.engine.download(additional_request)\n        additional_response = await maybe_deferred_to_future(deferred)\n        yield {\n            \"h1\": response.css(\"h1\").get(),\n            \"price\": additional_response.css(\"#price\").get(),\n        }\n```\n\n----------------------------------------\n\nTITLE: Setting up HTTP Authentication Domain in Scrapy\nDESCRIPTION: Important security fix related to HTTP authentication that requires setting http_auth_domain to prevent credential exposure. Shows how to restrict authentication credentials to specific domains.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Example usage\nclass MySpider(scrapy.Spider):\n    name = 'myspider'\n    # Only send credentials to example.com\n    http_auth_domain = 'example.com'\n    http_user = 'user'\n    http_pass = 'pass'\n\n    # Alternative: Allow credentials for any domain (not recommended)\n    http_auth_domain = None\n\n    # Alternative: Use w3lib for manual auth header\n    from w3lib.http import basic_auth_header\n    headers = {'Authorization': basic_auth_header('user', 'pass')}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Scrapy Spider for Paginated JSON API\nDESCRIPTION: This snippet shows a Scrapy spider that crawls a paginated JSON API. It parses the JSON response, extracts quotes, and follows pagination links to scrape all pages.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/developer-tools.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nimport json\n\n\nclass QuoteSpider(scrapy.Spider):\n    name = \"quote\"\n    allowed_domains = [\"quotes.toscrape.com\"]\n    page = 1\n    start_urls = [\"https://quotes.toscrape.com/api/quotes?page=1\"]\n\n    def parse(self, response):\n        data = json.loads(response.text)\n        for quote in data[\"quotes\"]:\n            yield {\"quote\": quote[\"text\"]}\n        if data[\"has_next\"]:\n            self.page += 1\n            url = f\"https://quotes.toscrape.com/api/quotes?page={self.page}\"\n            yield scrapy.Request(url=url, callback=self.parse)\n```\n\n----------------------------------------\n\nTITLE: Activating Spider Middleware Configuration in Python\nDESCRIPTION: Example of how to activate a spider middleware component by adding it to the SPIDER_MIDDLEWARES setting with an order value. The order determines processing sequence with lower numbers processed first.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spider-middleware.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSPIDER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomSpiderMiddleware\": 543,\n}\n```\n\n----------------------------------------\n\nTITLE: Using Response.follow_all Method in Python\nDESCRIPTION: Example of the new Response.follow_all method that can process multiple URLs in a single call and returns an iterable of requests, offering similar functionality to Response.follow but with multiple URL support.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nResponse.follow_all\n```\n\n----------------------------------------\n\nTITLE: Updating Spider Settings via update_settings Method\nDESCRIPTION: Demonstrates implementing the update_settings classmethod to modify spider settings with explicit priority.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n\n    @classmethod\n    def update_settings(cls, settings):\n        super().update_settings(settings)\n        settings.set(\"SOME_SETTING\", \"some value\", priority=\"spider\")\n```\n\n----------------------------------------\n\nTITLE: Extending ItemLoader for Site-Specific Parsing in Scrapy (Python)\nDESCRIPTION: Shows how to extend a ProductLoader to handle site-specific parsing rules, such as stripping dashes from product names.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom itemloaders.processors import MapCompose\nfrom myproject.ItemLoaders import ProductLoader\n\n\ndef strip_dashes(x):\n    return x.strip(\"-\")\n\n\nclass SiteSpecificLoader(ProductLoader):\n    name_in = MapCompose(strip_dashes, ProductLoader.name_in)\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Spiders Sequentially with CrawlerRunner in Python\nDESCRIPTION: This example shows how to run multiple spiders sequentially by chaining deferreds with inlineCallbacks. Each spider runs only after the previous one completes, providing controlled sequential execution.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/practices.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom twisted.internet import defer\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.project import get_project_settings\n\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    ...\n\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    ...\n\n\nsettings = get_project_settings()\nconfigure_logging(settings)\nrunner = CrawlerRunner(settings)\n\n\n@defer.inlineCallbacks\ndef crawl():\n    yield runner.crawl(MySpider1)\n    yield runner.crawl(MySpider2)\n    reactor.stop()\n\n\nfrom twisted.internet import reactor\n\ncrawl()\nreactor.run()  # the script will block here until the last crawl call is finished\n```\n\n----------------------------------------\n\nTITLE: Defining Spider Contracts in Python\nDESCRIPTION: This snippet demonstrates how to define contracts for a spider's parse method. It includes contracts for URL, expected returns, and scraped fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/contracts.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef parse(self, response):\n    \"\"\"\n    This function parses a sample response. Some contracts are mingled\n    with this docstring.\n\n    @url http://www.example.com/s?field-keywords=selfish+gene\n    @returns items 1 16\n    @returns requests 0 0\n    @scrapes Title Author Year Price\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Sending JSON POST Request with JsonRequest\nDESCRIPTION: Example of using JsonRequest to send a JSON POST request with a JSON payload. It demonstrates how to structure the data and use the JsonRequest class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndata = {\n    \"name1\": \"value1\",\n    \"name2\": \"value2\",\n}\nyield JsonRequest(url=\"http://www.example.com/post/action\", data=data)\n```\n\n----------------------------------------\n\nTITLE: Selecting Text with CSS in Scrapy\nDESCRIPTION: Demonstrates using CSS selectors with Scrapy's custom ::text pseudo-element to extract text content from HTML elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> response.css(\"title::text\").get()\n'Example website'\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Cookie Sessions in Scrapy Requests\nDESCRIPTION: This code snippet shows how to use multiple cookie sessions per spider by utilizing the cookiejar Request meta key. It demonstrates passing different identifiers to use separate cookie jars for each request.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor i, url in enumerate(urls):\n    yield scrapy.Request(url, meta={\"cookiejar\": i}, callback=self.parse_page)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Scrapy Extension in Python\nDESCRIPTION: This code snippet shows a complete implementation of a custom Scrapy extension called SpiderOpenCloseLogging. It logs messages when spiders are opened and closed, and when a specific number of items are scraped. The extension uses Scrapy signals and can be configured through settings.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/extensions.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\n\nlogger = logging.getLogger(__name__)\n\n\nclass SpiderOpenCloseLogging:\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool(\"MYEXT_ENABLED\"):\n            raise NotConfigured\n\n        # get the number of items from settings\n        item_count = crawler.settings.getint(\"MYEXT_ITEMCOUNT\", 1000)\n\n        # instantiate the extension object\n        ext = cls(item_count)\n\n        # connect the extension object to signals\n        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n\n        # return the extension object\n        return ext\n\n    def spider_opened(self, spider):\n        logger.info(\"opened spider %s\", spider.name)\n\n    def spider_closed(self, spider):\n        logger.info(\"closed spider %s\", spider.name)\n\n    def item_scraped(self, item, spider):\n        self.items_scraped += 1\n        if self.items_scraped % self.item_count == 0:\n            logger.info(\"scraped %d items\", self.items_scraped)\n```\n\n----------------------------------------\n\nTITLE: Using Relative XPaths for Nested Selectors in Python\nDESCRIPTION: Demonstrates the correct way to use relative XPaths when working with nested selectors to extract specific elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_17\n\nLANGUAGE: pycon\nCODE:\n```\n>>> divs = response.xpath(\"//div\")\n\n>>> for p in divs.xpath(\"//p\"):  # this is wrong - gets all <p> from the whole document\n...     print(p.get())\n...\n\n>>> for p in divs.xpath(\".//p\"):  # extracts all <p> inside\n...     print(p.get())\n...\n\n>>> for p in divs.xpath(\"p\"):  # extracts direct <p> children\n...     print(p.get())\n...\n```\n\n----------------------------------------\n\nTITLE: Setting HTTP Proxy for Scrapy Requests\nDESCRIPTION: Example of how to set an HTTP proxy for individual Scrapy requests by setting the 'proxy' meta value. This overrides environment variables and ignores the no_proxy setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nRequest(url, meta={'proxy': 'http://some_proxy_server:port'})\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Signal Handling in a Scrapy Spider\nDESCRIPTION: This snippet demonstrates how to catch and handle signals in a Scrapy spider by implementing the from_crawler class method and connecting a signal handler for the spider_closed signal.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy import signals\nfrom scrapy import Spider\n\n\nclass DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\",\n    ]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n        return spider\n\n    def spider_closed(self, spider):\n        spider.logger.info(\"Spider closed: %s\", spider.name)\n\n    def parse(self, response):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Passing Keyword Arguments to Callback Methods in Scrapy Requests\nDESCRIPTION: The new Request.cb_kwargs attribute provides a cleaner way to pass keyword arguments to callback methods in Scrapy requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\nRequest(url, callback=self.parse, cb_kwargs={'arg1': 'value1', 'arg2': 'value2'})\n```\n\n----------------------------------------\n\nTITLE: Custom request fingerprinter using header (Python)\nDESCRIPTION: This example demonstrates how to create a custom request fingerprinter that takes into account a specific header (X-ID) when generating the fingerprint. It uses the scrapy.utils.request.fingerprint function.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# my_project/settings.py\nREQUEST_FINGERPRINTER_CLASS = \"my_project.utils.RequestFingerprinter\"\n\n# my_project/utils.py\nfrom scrapy.utils.request import fingerprint\n\n\nclass RequestFingerprinter:\n    def fingerprint(self, request):\n        return fingerprint(request, include_headers=[\"X-ID\"])\n```\n\n----------------------------------------\n\nTITLE: Filtering Sitemap Entries by Date in Python\nDESCRIPTION: This example demonstrates how to create a custom SitemapSpider that filters sitemap entries based on their lastmod date. It only yields entries modified in 2005 or later.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom scrapy.spiders import SitemapSpider\n\n\nclass FilteredSitemapSpider(SitemapSpider):\n    name = \"filtered_sitemap_spider\"\n    allowed_domains = [\"example.com\"]\n    sitemap_urls = [\"http://example.com/sitemap.xml\"]\n\n    def sitemap_filter(self, entries):\n        for entry in entries:\n            date_time = datetime.strptime(entry[\"lastmod\"], \"%Y-%m-%d\")\n            if date_time.year >= 2005:\n                yield entry\n```\n\n----------------------------------------\n\nTITLE: Configuring Scrapy Extensions in Python\nDESCRIPTION: This snippet demonstrates how to enable extensions in Scrapy by adding them to the EXTENSIONS setting. It shows the configuration for enabling the CoreStats and TelnetConsole extensions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/extensions.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nEXTENSIONS = {\n    \"scrapy.extensions.corestats.CoreStats\": 500,\n    \"scrapy.extensions.telnet.TelnetConsole\": 500,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom XML Export Pipeline in Python\nDESCRIPTION: This code snippet demonstrates a Scrapy Item Pipeline that uses XmlItemExporter to export items to different XML files based on their 'year' field. It shows how to open files, create exporters, and manage the exporting process.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom itemadapter import ItemAdapter\nfrom scrapy.exporters import XmlItemExporter\n\n\nclass PerYearXmlExportPipeline:\n    \"\"\"Distribute items across multiple XML files according to their 'year' field\"\"\"\n\n    def open_spider(self, spider):\n        self.year_to_exporter = {}\n\n    def close_spider(self, spider):\n        for exporter, xml_file in self.year_to_exporter.values():\n            exporter.finish_exporting()\n            xml_file.close()\n\n    def _exporter_for_item(self, item):\n        adapter = ItemAdapter(item)\n        year = adapter[\"year\"]\n        if year not in self.year_to_exporter:\n            xml_file = open(f\"{year}.xml\", \"wb\")\n            exporter = XmlItemExporter(xml_file)\n            exporter.start_exporting()\n            self.year_to_exporter[year] = (exporter, xml_file)\n        return self.year_to_exporter[year][0]\n\n    def process_item(self, item, spider):\n        exporter = self._exporter_for_item(item)\n        exporter.export_item(item)\n        return item\n```\n\n----------------------------------------\n\nTITLE: Declaring Processors in Item Field Metadata\nDESCRIPTION: This code shows how to specify input and output processors directly in Item Field metadata when defining a Scrapy Item class. It demonstrates setting processors for name and price fields including a custom filter function.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom itemloaders.processors import Join, MapCompose, TakeFirst\nfrom w3lib.html import remove_tags\n\n\ndef filter_price(value):\n    if value.isdigit():\n        return value\n\n\nclass Product(scrapy.Item):\n    name = scrapy.Field(\n        input_processor=MapCompose(remove_tags),\n        output_processor=Join(),\n    )\n    price = scrapy.Field(\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Export Settings in Scrapy\nDESCRIPTION: This snippet demonstrates how to configure batch export settings in Scrapy by setting the FEED_EXPORT_BATCH_ITEM_COUNT and using placeholders in the feed URI for generating multiple output files.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nFEED_EXPORT_BATCH_ITEM_COUNT = 100\n```\n\n----------------------------------------\n\nTITLE: Creating Screenshot Pipeline with Async in Python for Scrapy\nDESCRIPTION: An asynchronous pipeline that uses Splash to render screenshots of item URLs. It requests a screenshot from a locally running Splash instance, saves the image to a file named with the URL hash, and adds the filename to the item.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/item-pipeline.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport hashlib\nfrom pathlib import Path\nfrom urllib.parse import quote\n\nimport scrapy\nfrom itemadapter import ItemAdapter\nfrom scrapy.http.request import NO_CALLBACK\nfrom scrapy.utils.defer import maybe_deferred_to_future\n\n\nclass ScreenshotPipeline:\n    \"\"\"Pipeline that uses Splash to render screenshot of\n    every Scrapy item.\"\"\"\n\n    SPLASH_URL = \"http://localhost:8050/render.png?url={}\"\n\n    async def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        encoded_item_url = quote(adapter[\"url\"])\n        screenshot_url = self.SPLASH_URL.format(encoded_item_url)\n        request = scrapy.Request(screenshot_url, callback=NO_CALLBACK)\n        response = await maybe_deferred_to_future(\n            spider.crawler.engine.download(request)\n        )\n\n        if response.status != 200:\n            # Error happened, return item.\n            return item\n\n        # Save screenshot to file, filename will be hash of url.\n        url = adapter[\"url\"]\n        url_hash = hashlib.md5(url.encode(\"utf8\")).hexdigest()\n        filename = f\"{url_hash}.png\"\n        Path(filename).write_bytes(response.body)\n\n        # Store filename in item.\n        adapter[\"screenshot_filename\"] = filename\n        return item\n```\n\n----------------------------------------\n\nTITLE: Example Scrapy Shell Session\nDESCRIPTION: An interactive example of a Scrapy shell session, demonstrating how to fetch URLs, extract data, and modify requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> response.xpath(\"//title/text()\").get()\n'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework'\n\n>>> fetch(\"https://old.reddit.com/\")\n\n>>> response.xpath(\"//title/text()\").get()\n'reddit: the front page of the internet'\n\n>>> request = request.replace(method=\"POST\")\n\n>>> fetch(request)\n\n>>> response.status\n404\n\n>>> from pprint import pprint\n\n>>> pprint(response.headers)\n{'Accept-Ranges': ['bytes'],\n'Cache-Control': ['max-age=0, must-revalidate'],\n'Content-Type': ['text/html; charset=UTF-8'],\n'Date': ['Thu, 08 Dec 2016 16:21:19 GMT'],\n'Server': ['snooserv'],\n'Set-Cookie': ['loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',\n                'loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',\n                'loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',\n                'loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure'],\n'Vary': ['accept-encoding'],\n'Via': ['1.1 varnish'],\n'X-Cache': ['MISS'],\n'X-Cache-Hits': ['0'],\n'X-Content-Type-Options': ['nosniff'],\n'X-Frame-Options': ['SAMEORIGIN'],\n'X-Moose': ['majestic'],\n'X-Served-By': ['cache-cdg8730-CDG'],\n'X-Timer': ['S1481214079.394283,VS0,VE159'],\n'X-Ua-Compatible': ['IE=edge'],\n'X-Xss-Protection': ['1; mode=block']}\n```\n\n----------------------------------------\n\nTITLE: Creating Scrapy Request from cURL Command\nDESCRIPTION: This snippet demonstrates how to create a Scrapy Request object from a cURL command using the from_curl() method. It's useful for replicating complex requests with headers and cookies.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/developer-tools.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy import Request\n\nrequest = Request.from_curl(\n    \"curl 'https://quotes.toscrape.com/api/quotes?page=1' -H 'User-Agent: Mozil\"\n    \"la/5.0 (X11; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0' -H 'Acce\"\n    \"pt: */*' -H 'Accept-Language: ca,en-US;q=0.7,en;q=0.3' --compressed -H 'X\"\n    \"-Requested-With: XMLHttpRequest' -H 'Proxy-Authorization: Basic QFRLLTAzM\"\n    \"zEwZTAxLTk5MWUtNDFiNC1iZWRmLTJjNGI4M2ZiNDBmNDpAVEstMDMzMTBlMDEtOTkxZS00MW\"\n    \"I0LWJlZGYtMmM0YjgzZmI0MGY0' -H 'Connection: keep-alive' -H 'Referer: http\"\n    \"://quotes.toscrape.com/scroll' -H 'Cache-Control: max-age=0'\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using Variables in XPath Expressions\nDESCRIPTION: Demonstrates how to use variables in XPath expressions, allowing for more flexible and reusable selectors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_25\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.xpath(\"//div[@id=$val]/a/text()\", val=\"images\").get()\n'Name: My image 1 '\n>>> response.xpath(\"//div[count(a)=$cnt]/@id\", cnt=5).get()\n'images'\n```\n\n----------------------------------------\n\nTITLE: Extracting JavaScript Object Data with Regular Expression\nDESCRIPTION: Demonstrates extracting and parsing JavaScript object data using regular expressions and JSON parsing.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/dynamic-content.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npattern = r\"\\bvar\\s+data\\s*=\\s*(\\{.*?\\})\\s*;\\s*\\n\"\njson_data = response.css(\"script::text\").re_first(pattern)\njson.loads(json_data)\n```\n\n----------------------------------------\n\nTITLE: TextResponse.json() Method Usage in Scrapy\nDESCRIPTION: New method added to TextResponse that allows deserializing JSON responses directly, making it easier to parse JSON API responses.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Instead of:\n# json.loads(response.text)\n\n# You can now use:\ndata = response.json()\n```\n\n----------------------------------------\n\nTITLE: Declaring Product Item with Fields\nDESCRIPTION: Example of declaring a Product item with multiple fields including name, price, stock, tags, and last_updated with a custom serializer.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/items.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\nclass Product(scrapy.Item):\n    name = scrapy.Field()\n    price = scrapy.Field()\n    stock = scrapy.Field()\n    tags = scrapy.Field()\n    last_updated = scrapy.Field(serializer=str)\n```\n\n----------------------------------------\n\nTITLE: Implementing errback handling in Scrapy spider (Python)\nDESCRIPTION: This example shows how to implement errback handling in a Scrapy spider. It demonstrates logging errors and catching specific exceptions like HttpError, DNSLookupError, and TimeoutError.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\nfrom scrapy.spidermiddlewares.httperror import HttpError\nfrom twisted.internet.error import DNSLookupError\nfrom twisted.internet.error import TimeoutError, TCPTimedOutError\n\n\nclass ErrbackSpider(scrapy.Spider):\n    name = \"errback_example\"\n    start_urls = [\n        \"http://www.httpbin.org/\",  # HTTP 200 expected\n        \"http://www.httpbin.org/status/404\",  # Not found error\n        \"http://www.httpbin.org/status/500\",  # server issue\n        \"http://www.httpbin.org:12345/\",  # non-responding host, timeout expected\n        \"https://example.invalid/\",  # DNS error expected\n    ]\n\n    def start_requests(self):\n        for u in self.start_urls:\n            yield scrapy.Request(\n                u,\n                callback=self.parse_httpbin,\n                errback=self.errback_httpbin,\n                dont_filter=True,\n            )\n\n    def parse_httpbin(self, response):\n        self.logger.info(\"Got successful response from {}\".format(response.url))\n        # do something useful here...\n\n    def errback_httpbin(self, failure):\n        # log all failures\n        self.logger.error(repr(failure))\n\n        # in case you want to do something special for some errors,\n        # you may need the failure's type:\n\n        if failure.check(HttpError):\n            # these exceptions come from HttpError spider middleware\n            # you can get the non-200 response\n            response = failure.value.response\n            self.logger.error(\"HttpError on %s\", response.url)\n\n        elif failure.check(DNSLookupError):\n            # this is the original request\n            request = failure.request\n            self.logger.error(\"DNSLookupError on %s\", request.url)\n\n        elif failure.check(TimeoutError, TCPTimedOutError):\n            request = failure.request\n            self.logger.error(\"TimeoutError on %s\", request.url)\n```\n\n----------------------------------------\n\nTITLE: Basic SitemapSpider Usage in Python\nDESCRIPTION: This example shows the simplest way to use SitemapSpider. It processes all URLs discovered through sitemaps using the parse callback.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import SitemapSpider\n\n\nclass MySpider(SitemapSpider):\n    sitemap_urls = [\"http://www.example.com/sitemap.xml\"]\n\n    def parse(self, response):\n        pass  # ... scrape item here ...\n```\n\n----------------------------------------\n\nTITLE: Implementing CSVFeedSpider in Python\nDESCRIPTION: Example of a CSVFeedSpider that processes CSV files by iterating through rows. Shows how to configure delimiter, quote character and headers for CSV parsing.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import CSVFeedSpider\nfrom myproject.items import TestItem\n\n\nclass MySpider(CSVFeedSpider):\n    name = \"example.com\"\n    allowed_domains = [\"example.com\"]\n    start_urls = [\"http://www.example.com/feed.csv\"]\n    delimiter = \";\"\n    quotechar = \"'\"\n    headers = [\"id\", \"name\", \"description\"]\n\n    def parse_row(self, response, row):\n        self.logger.info(\"Hi, this is a row!: %r\", row)\n\n        item = TestItem()\n        item[\"id\"] = row[\"id\"]\n        item[\"name\"] = row[\"name\"]\n        item[\"description\"] = row[\"description\"]\n        return item\n```\n\n----------------------------------------\n\nTITLE: Accessing and Modifying Nested Item Values in Scrapy\nDESCRIPTION: This snippet shows how to access and modify values in nested Scrapy items. It includes examples of accessing deeply nested fields, modifying values, and potential issues with type checking in list operations.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-003.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\np = Product(supplier=Supplier(name=\"some name\", url=\"http://example.com\"))\np[\"supplier\"][\"url\"]  # returns 'http://example.com'\np[\"supplier\"][\"url\"] = \"http://www.other.com\"  # works as expected\np[\"supplier\"][\"url\"] = 123  # fails: wrong type for supplier url\n\np[\"variants\"] = [v1, v2]\np[\"variants\"][0][\"name\"]  # returns v1 name\np[\"variants\"][1][\"name\"]  # returns v2 name\n\n# XXX: decide what to do about these cases:\np[\"variants\"].append(v3)  # works but doesn't check type of v3\np[\"variants\"].append(1)  # works but shouldn't?\n```\n\n----------------------------------------\n\nTITLE: Configuring Item Pipelines in Scrapy Settings\nDESCRIPTION: Example configuration for activating item pipeline components using the ITEM_PIPELINES setting. The integer values determine the order of execution from lower to higher values, typically defined in the 0-1000 range.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/item-pipeline.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nITEM_PIPELINES = {\n   \"myproject.pipelines.PricePipeline\": 300,\n   \"myproject.pipelines.JsonWriterPipeline\": 800,\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting XML Elements with XPath in Scrapy\nDESCRIPTION: This example shows how to select elements from an XML response. It demonstrates selecting all <product> elements from an XML document.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nsel = Selector(xml_response)\nsel.xpath(\"//product\")\n```\n\n----------------------------------------\n\nTITLE: Using EXSLT Extensions for Regular Expressions\nDESCRIPTION: Shows how to use EXSLT extensions for regular expressions in XPath, which can be more powerful than built-in XPath functions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_26\n\nLANGUAGE: pycon\nCODE:\n```\n>>> from scrapy import Selector\n>>> doc = \"\"\"\n... <div>\n...     <ul>\n...         <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n...         <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n...         <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n...         <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n...         <li class=\"item-0\"><a href=\"link5.html\">fifth item</a></li>\n...     </ul>\n... </div>\n... \"\"\"\n>>> sel = Selector(text=doc, type=\"html\")\n>>> sel.xpath(\"//li//@href\").getall()\n['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']\n>>> sel.xpath('//li[re:test(@class, \"item-\\d$\")]//@href').getall()\n['link1.html', 'link2.html', 'link4.html', 'link5.html']\n```\n\n----------------------------------------\n\nTITLE: Input and Output Processor Usage Example in ItemLoader\nDESCRIPTION: This snippet illustrates how input and output processors are called for a field when using various data extraction methods. It demonstrates the flow of data through the processors when using add_xpath, add_css, and add_value methods.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nl = ItemLoader(Product(), some_selector)\nl.add_xpath(\"name\", xpath1)  # (1)\nl.add_xpath(\"name\", xpath2)  # (2)\nl.add_css(\"name\", css)  # (3)\nl.add_value(\"name\", \"test\")  # (4)\nreturn l.load_item()  # (5)\n```\n\n----------------------------------------\n\nTITLE: Creating Dataclass-based Item\nDESCRIPTION: Example demonstrating how to create an item using Python's dataclass decorator with typed fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/items.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\n@dataclass\nclass CustomItem:\n    one_field: str\n    another_field: int\n```\n\n----------------------------------------\n\nTITLE: Implementing Callback Rules Dispatcher in Python\nDESCRIPTION: A LegSpider that dispatches responses to different callback methods based on URL patterns.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-016.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CallbackRules(LegSpider):\n    def __init__(self, *a, **kw):\n        super(CallbackRules, self).__init__(*a, **kw)\n        for regex, method_name in self.spider.callback_rules.items():\n            r = re.compile(regex)\n            m = getattr(self.spider, method_name, None)\n            if m:\n                self._rules[r] = m\n\n    def process_response(self, response):\n        for regex, method in self._rules.items():\n            m = regex.search(response.url)\n            if m:\n                return method(response)\n        return []\n\n\nclass MySpider(LegSpider):\n    legs = [CallbackRules()]\n    callback_rules = {\n        \"/product.php.*\": \"parse_product\",\n        \"/category.php.*\": \"parse_category\",\n    }\n\n    def parse_product(self, response):\n        # parse response and populate item\n        return item\n```\n\n----------------------------------------\n\nTITLE: Creating a Scrapy Request with Cookies (Python)\nDESCRIPTION: Demonstrates two ways to create a Scrapy Request object with custom cookies: using a dictionary and using a list of dictionaries. The latter allows for more detailed cookie configuration.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nrequest_with_cookies = Request(\n    url=\"http://www.example.com\",\n    cookies={\"currency\": \"USD\", \"country\": \"UY\"},\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nrequest_with_cookies = Request(\n    url=\"https://www.example.com\",\n    cookies=[\n        {\n            \"name\": \"currency\",\n            \"value\": \"USD\",\n            \"domain\": \"example.com\",\n            \"path\": \"/currency\",\n            \"secure\": True,\n        },\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Settings in Scrapy Add-on (Python)\nDESCRIPTION: Example of a Scrapy add-on that sets various settings, including enabling DNS caching, modifying a list setting, and adding an item pipeline. It demonstrates different methods for manipulating settings.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/addons.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom myproject.pipelines import MyPipeline\n\n\nclass MyAddon:\n    def update_settings(self, settings):\n        settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n        settings.remove_from_list(\"METAREFRESH_IGNORE_TAGS\", \"noscript\")\n        settings.setdefault_in_component_priority_dict(\n            \"ITEM_PIPELINES\", MyPipeline, 200\n        )\n```\n\n----------------------------------------\n\nTITLE: Constructing Scrapy Selectors Manually from Text\nDESCRIPTION: Shows how to manually create a Scrapy selector from an HTML string. This is useful when you need to parse HTML content that doesn't come from a response object.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from scrapy.selector import Selector\n>>> body = \"<html><body><span>good</span></body></html>\"\n>>> Selector(text=body).xpath(\"//span/text()\").get()\n'good'\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Custom Item Class\nDESCRIPTION: Example showing how to create a basic custom Item class with two fields using Scrapy's Item and Field classes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/items.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.item import Item, Field\n\nclass CustomItem(Item):\n    one_field = Field()\n    another_field = Field()\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Downloader Middlewares in Scrapy\nDESCRIPTION: This code snippet defines the default downloader middlewares and their order in Scrapy. It includes middlewares for various functionalities like robots.txt handling, authentication, timeouts, retries, redirects, compression, and caching.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"scrapy.downloadermiddlewares.offsite.OffsiteMiddleware\": 50,\n    \"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\": 100,\n    \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\": 300,\n    \"scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\": 350,\n    \"scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware\": 400,\n    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": 500,\n    \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 550,\n    \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware\": 560,\n    \"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\": 580,\n    \"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\": 590,\n    \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 600,\n    \"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\": 700,\n    \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\": 750,\n    \"scrapy.downloadermiddlewares.stats.DownloaderStats\": 850,\n    \"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\": 900,\n}\n```\n\n----------------------------------------\n\nTITLE: Using response.follow Shortcut in Scrapy (Python)\nDESCRIPTION: Example of using the new response.follow shortcut for creating requests in Scrapy 1.4. This simplifies the process of following links in a spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\nNew :ref:`response.follow <response-follow-example>` shortcut\nfor creating requests (:issue:`1940`)\n```\n\n----------------------------------------\n\nTITLE: Parsing JavaScript Objects with ChompJS\nDESCRIPTION: Shows how to use the chompjs library to parse JavaScript objects into Python dictionaries.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/dynamic-content.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport chompjs\njavascript = response.css(\"script::text\").get()\ndata = chompjs.parse_js_object(javascript)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Spider Contract in Python\nDESCRIPTION: This example demonstrates how to create a custom contract that checks for the presence of a specific header in the response. It inherits from the Contract class and implements the pre_process method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/contracts.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.contracts import Contract\nfrom scrapy.exceptions import ContractFail\n\n\nclass HasHeaderContract(Contract):\n    \"\"\"\n    Demo contract which checks the presence of a custom header\n    @has_header X-CustomHeader\n    \"\"\"\n\n    name = \"has_header\"\n\n    def pre_process(self, response):\n        for header in self.args:\n            if header not in response.headers:\n                raise ContractFail(\"X-CustomHeader not present\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Base LegSpider Class in Python\nDESCRIPTION: Core implementation of the LegSpider base class with core functionality.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-016.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.http import Request\nfrom scrapy.item import BaseItem\nfrom scrapy.spider import BaseSpider\nfrom scrapy.utils.spider import iterate_spider_output\n\n\nclass LegSpider(BaseSpider):\n    \"\"\"A spider made of legs\"\"\"\n\n    legs = []\n\n    def __init__(self, *args, **kwargs):\n        super(LegSpider, self).__init__(*args, **kwargs)\n        self._legs = [self] + self.legs[:]\n        for l in self._legs:\n            l.set_spider(self)\n\n    def parse(self, response):\n        res = self._process_response(response)\n        for r in res:\n            if isinstance(r, BaseItem):\n                yield self._process_item(r)\n            else:\n                yield self._process_request(r)\n\n    def process_response(self, response):\n        return []\n\n    def process_request(self, request):\n        return request\n\n    def process_item(self, item):\n        return item\n\n    def set_spider(self, spider):\n        self.spider = spider\n\n    def _process_response(self, response):\n        res = []\n        for l in self._legs:\n            res.extend(iterate_spider_output(l.process_response(response)))\n        return res\n\n    def _process_request(self, request):\n        for l in self._legs:\n            request = l.process_request(request)\n        return request\n\n    def _process_item(self, item):\n        for l in self._legs:\n            item = l.process_item(item)\n        return item\n```\n\n----------------------------------------\n\nTITLE: Modifying ItemLoader Context in Scrapy (Python)\nDESCRIPTION: Illustrates different ways to modify the ItemLoader context, including direct attribute modification, initialization parameters, and processor declaration.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nloader = ItemLoader(product)\nloader.context[\"unit\"] = \"cm\"\n\nloader = ItemLoader(product, unit=\"cm\")\n\nclass ProductLoader(ItemLoader):\n    length_out = MapCompose(parse_length, unit=\"cm\")\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Elements and Accessing Attributes in Scrapy\nDESCRIPTION: This example shows how to iterate over selected HTML elements and access their attributes. It selects all paragraph tags and prints their class attributes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfor node in sel.xpath(\"//p\"):\n    print(node.attrib[\"class\"])\n```\n\n----------------------------------------\n\nTITLE: Working with XML Namespaces in Scrapy\nDESCRIPTION: This example demonstrates how to register and use XML namespaces with Scrapy selectors. It shows extracting price elements from a Google Base XML feed after registering the appropriate namespace.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nsel.register_namespace(\"g\", \"http://base.google.com/ns/1.0\")\nsel.xpath(\"//g:price\").getall()\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Requests in Scrapy\nDESCRIPTION: A new JSONRequest class offers a more convenient way to build JSON requests in Scrapy.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\nfrom scrapy.http import JSONRequest\n\nJSONRequest(url, data={'key': 'value'})\n```\n\n----------------------------------------\n\nTITLE: Installing AsyncioSelectorReactor in Scrapy with CrawlerRunner\nDESCRIPTION: Demonstrates how to manually install the AsyncioSelectorReactor when using CrawlerRunner in Scrapy. This is necessary to enable asyncio support.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/asyncio.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n```\n\n----------------------------------------\n\nTITLE: Extending ItemLoader for XML Parsing in Scrapy (Python)\nDESCRIPTION: Demonstrates how to extend a ProductLoader to handle XML-specific parsing, such as removing CDATA occurrences.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom itemloaders.processors import MapCompose\nfrom myproject.ItemLoaders import ProductLoader\nfrom myproject.utils.xml import remove_cdata\n\n\nclass XmlProductLoader(ProductLoader):\n    name_in = MapCompose(remove_cdata, ProductLoader.name_in)\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Download Handlers in Scrapy\nDESCRIPTION: This code snippet defines the default download handlers for different URI schemes in Scrapy. It includes handlers for data URIs, file downloads, HTTP/HTTPS requests, S3, and FTP.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"data\": \"scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler\",\n    \"file\": \"scrapy.core.downloader.handlers.file.FileDownloadHandler\",\n    \"http\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"https\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n    \"s3\": \"scrapy.core.downloader.handlers.s3.S3DownloadHandler\",\n    \"ftp\": \"scrapy.core.downloader.handlers.ftp.FTPDownloadHandler\",\n}\n```\n\n----------------------------------------\n\nTITLE: Handling 404 Responses in Scrapy Spider\nDESCRIPTION: This code snippet demonstrates how to configure a Scrapy spider to handle 404 HTTP responses. It defines a custom spider class that inherits from CrawlSpider and sets the handle_httpstatus_list attribute to include 404 status codes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spider-middleware.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import CrawlSpider\n\n\nclass MySpider(CrawlSpider):\n    handle_httpstatus_list = [404]\n```\n\n----------------------------------------\n\nTITLE: Using Spider State for Persistent Item Count in Python\nDESCRIPTION: This code snippet demonstrates how to use the spider state to maintain a persistent item count between batches in a Scrapy spider's parse_item method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/jobs.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef parse_item(self, response):\n    # parse item here\n    self.state[\"items_count\"] = self.state.get(\"items_count\", 0) + 1\n```\n\n----------------------------------------\n\nTITLE: Defining Custom URI Parameters Function for Scrapy Feeds\nDESCRIPTION: This code defines a custom function to set parameters for feed URIs in Scrapy. It adds the spider name to the default parameters and is used with the FEED_URI_PARAMS setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# myproject/utils.py\ndef uri_params(params, spider):\n    return {**params, \"spider_name\": spider.name}\n```\n\n----------------------------------------\n\nTITLE: Populating Item Loader without Bulk Loader in Python\nDESCRIPTION: Demonstrates the manual approach to populating an Item Loader without using a Bulk Item Loader. It shows how to extract data from a definition list using XPath expressions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-020.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nxpath = '//div[@class=\"geeks\"]/dl/dt[contains(text(),\"%s\")]/following-sibling::dd[1]//text()'\ngl = XPathItemLoader(response=response, item=dict())\ngl.default_output_processor = Compose(TakeFirst(), lambda v: v.strip())\ngl.add_xpath(\"hacker\", xpath % \"hacker\")\ngl.add_xpath(\"nerd\", xpath % \"nerd\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Item Filter in Python for Scrapy Feed Exports\nDESCRIPTION: Demonstrates how to create a custom filtering class for Scrapy feed exports by implementing a class with an accepts method that evaluates items based on specific criteria.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyCustomFilter:\n    def __init__(self, feed_options):\n        self.feed_options = feed_options\n\n    def accepts(self, item):\n        if \"field1\" in item and item[\"field1\"] == \"expected_data\":\n            return True\n        return False\n```\n\n----------------------------------------\n\nTITLE: Incrementing Stat Value in Scrapy Stats Collector\nDESCRIPTION: This snippet demonstrates how to increment a stat value in the Scrapy Stats Collector. It increments the 'custom_count' stat.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/stats.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstats.inc_value(\"custom_count\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Subclass of Scrapy Item\nDESCRIPTION: Demonstrates how to extend an existing Product item class by creating a DiscountedProduct subclass with additional fields for discount information.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/items.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DiscountedProduct(Product):\n    discount_percent = scrapy.Field(serializer=str)\n    discount_expiration_date = scrapy.Field()\n```\n\n----------------------------------------\n\nTITLE: Using XPath Selectors in Scrapy with Python\nDESCRIPTION: Introduces XPath selectors as an alternative to CSS selectors in Scrapy, showing basic usage for extracting text from HTML elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_5\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.xpath(\"//title\")\n[<Selector query='//title' data='<title>Quotes to Scrape</title>'>]\n>>> response.xpath(\"//title/text()\").get()\n'Quotes to Scrape'\n```\n\n----------------------------------------\n\nTITLE: Creating an Item with ItemBuilder in Python\nDESCRIPTION: Demonstrates how to use the ItemBuilder API to instantiate a builder, add field values, append additional values to fields, and replace existing values. Shows how to obtain the populated item object.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#!python\nil = NewsBuilder(response)\nil.add_value(\"url\", response.url)\nil.add_value(\"headline\", x.x('//h1[@class=\"headline\"]'))\n\n# if we want to add another value to the same field\nil.add_value(\"headline\", x.x('//h1[@class=\"headline2\"]'))\n\n# if we want to replace the field value other value to the same field\nil.replace_value(\"headline\", x.x('//h1[@class=\"headline3\"]'))\n\nreturn il.get_item()\n```\n\n----------------------------------------\n\nTITLE: Simulating User Login with FormRequest.from_response()\nDESCRIPTION: A Scrapy spider example that uses FormRequest.from_response() to simulate a user login. It handles pre-populated form fields and overrides username and password.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\ndef authentication_failed(response):\n    # TODO: Check the contents of the response and return True if it failed\n    # or False if it succeeded.\n    pass\n\n\nclass LoginSpider(scrapy.Spider):\n    name = \"example.com\"\n    start_urls = [\"http://www.example.com/users/login.php\"]\n\n    def parse(self, response):\n        return scrapy.FormRequest.from_response(\n            response,\n            formdata={\"username\": \"john\", \"password\": \"secret\"},\n            callback=self.after_login,\n        )\n\n    def after_login(self, response):\n        if authentication_failed(response):\n            self.logger.error(\"Login failed\")\n            return\n\n        # continue scraping with authenticated session...\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Processor and External Callback in CrawlSpider\nDESCRIPTION: This example shows how to use external callbacks and custom request processors in a CrawlSpider. It includes a function to filter today's links and demonstrates the use of regex-based request extractors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef filter_today_links(requests):\n    # only crawl today links\n    today = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n    return [r for r in requests if today in r.url]\n\n\n# Callback defined out of spider\ndef my_external_callback(response):\n    # process item\n    pass\n\n\nclass SampleSpider(CrawlSpider):\n    rules = [\n        # The dispatcher uses first-match policy\n        Rule(UrlRegexMatch(r\"/news/(.+)/\"), my_external_callback),\n    ]\n\n    request_extractors = [\n        RegexRequestExtractor(r\"/sections/.+\"),\n        RegexRequestExtractor(r\"/news/.+\"),\n    ]\n\n    request_processors = [\n        # canonicalize all requests' urls\n        Canonicalize(),\n        filter_today_links,\n    ]\n```\n\n----------------------------------------\n\nTITLE: Activating Add-ons in Scrapy Settings (Python)\nDESCRIPTION: Example of enabling two add-ons in a Scrapy project's settings.py file. It demonstrates how to use both import paths and class references in the ADDONS setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/addons.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nADDONS = {\n    'path.to.someaddon': 0,\n    SomeAddonClass: 1,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Request with AutoThrottle Delay Adjustment Prevention\nDESCRIPTION: Example showing how to create a Scrapy request that prevents its response latency from affecting the AutoThrottle delay calculations for its download slot. This is done by setting the autothrottle_dont_adjust_delay metadata flag.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/autothrottle.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy import Request\n\nRequest(\"https://example.com\", meta={\"autothrottle_dont_adjust_delay\": True})\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content with CSS Selectors in Python\nDESCRIPTION: Demonstrates how to use CSS selectors to extract text content from HTML elements, with an option to provide a default value if the element doesn't exist.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_10\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.css(\"img::text\").get()\n>>> response.css(\"img::text\").get(default=\"\")\n''\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Serializer for Item Fields in Python\nDESCRIPTION: This snippet shows how to declare a custom serializer for a specific field in a Scrapy Item. It defines a 'serialize_price' function that formats the price value, and applies it to the 'price' field of a Product item.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\ndef serialize_price(value):\n    return f\"$ {str(value)}\"\n\n\nclass Product(scrapy.Item):\n    name = scrapy.Field()\n    price = scrapy.Field(serializer=serialize_price)\n```\n\n----------------------------------------\n\nTITLE: Implementing Logging in Scrapy Spider Callbacks\nDESCRIPTION: Shows how to add logging to spider callbacks to track execution flow and identify issues, such as when an expected item is not received in a callback.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/debug.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef parse_details(self, response, item=None):\n    if item:\n        # populate more `item` fields\n        return item\n    else:\n        self.logger.warning(\"No item received for %s\", response.url)\n```\n\n----------------------------------------\n\nTITLE: Combining CSS and XPath Selectors in Python\nDESCRIPTION: Shows how to combine CSS and XPath selectors for more efficient and readable selection of elements with specific classes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_18\n\nLANGUAGE: pycon\nCODE:\n```\n>>> from scrapy import Selector\n>>> sel = Selector(\n...     text='<div class=\"hero shout\"><time datetime=\"2014-07-23 19:00\">Special date</time></div>'\n... )\n>>> sel.css(\".shout\").xpath(\"./time/@datetime\").getall()\n['2014-07-23 19:00']\n```\n\n----------------------------------------\n\nTITLE: Generating Spiders using Templates\nDESCRIPTION: Examples of using the genspider command to create new spiders based on pre-defined templates within a project.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy genspider -l\nAvailable templates:\n  basic\n  crawl\n  csvfeed\n  xmlfeed\n\n$ scrapy genspider example example.com\nCreated spider 'example' using template 'basic'\n\n$ scrapy genspider -t crawl scrapyorg scrapy.org\nCreated spider 'scrapyorg' using template 'crawl'\n```\n\n----------------------------------------\n\nTITLE: Handling Errors in Selector Results in Python\nDESCRIPTION: Shows how to handle potential errors when accessing selector results, demonstrating the difference between indexing and using the get() method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_3\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.css(\"noelement\")[0].get()\nTraceback (most recent call last):\n...\nIndexError: list index out of range\n\n>>> response.css(\"noelement\").get()\n```\n\n----------------------------------------\n\nTITLE: Nesting Selectors for Complex Data Extraction in Python\nDESCRIPTION: Demonstrates how to nest selectors to extract complex data structures from HTML, including extracting href and src attributes from nested elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_12\n\nLANGUAGE: pycon\nCODE:\n```\n>>> links = response.xpath('//a[contains(@href, \"image\")]')\n>>> links.getall()\n['<a href=\"image1.html\">Name: My image 1 <br><img src=\"image1_thumb.jpg\" alt=\"image1\"></a>',\n'<a href=\"image2.html\">Name: My image 2 <br><img src=\"image2_thumb.jpg\" alt=\"image2\"></a>',\n'<a href=\"image3.html\">Name: My image 3 <br><img src=\"image3_thumb.jpg\" alt=\"image3\"></a>',\n'<a href=\"image4.html\">Name: My image 4 <br><img src=\"image4_thumb.jpg\" alt=\"image4\"></a>',\n'<a href=\"image5.html\">Name: My image 5 <br><img src=\"image5_thumb.jpg\" alt=\"image5\"></a>']\n\n>>> for index, link in enumerate(links):\n...     href_xpath = link.xpath(\"@href\").get()\n...     img_xpath = link.xpath(\"img/@src\").get()\n...     print(f\"Link number {index} points to url {href_xpath!r} and image {img_xpath!r}\")\n...\nLink number 0 points to url 'image1.html' and image 'image1_thumb.jpg'\nLink number 1 points to url 'image2.html' and image 'image2_thumb.jpg'\nLink number 2 points to url 'image3.html' and image 'image3_thumb.jpg'\nLink number 3 points to url 'image4.html' and image 'image4_thumb.jpg'\nLink number 4 points to url 'image5.html' and image 'image5_thumb.jpg'\n```\n\n----------------------------------------\n\nTITLE: Implementing ListField Class in Python for Scrapy\nDESCRIPTION: Core implementation of the ListField class that handles multi-valued fields. The class inherits from BaseField and provides functionality to validate and convert iterables to lists of the specified field type.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-002.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.item.fields import BaseField\n\nclass ListField(BaseField):\n    def __init__(self, field, default=None):\n        self._field = field\n        super(ListField, self).__init__(default)\n\n    def to_python(self, value):\n        if hasattr(value, \"__iter__\"):  # str/unicode not allowed\n            return [self._field.to_python(v) for v in value]\n        else:\n            raise TypeError(\"Expected iterable, got %s\" % type(value).__name__)\n\n    def get_default(self):\n        # must return a new copy to avoid unexpected behaviors with mutable defaults\n        return list(self._default)\n```\n\n----------------------------------------\n\nTITLE: Custom Download Delay Spider Implementation\nDESCRIPTION: Example of implementing download delay in a crawler spider to handle throttling and respect rate limits. Sets a 2-second delay between requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/faq.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import CrawlSpider\n\n\nclass MySpider(CrawlSpider):\n    name = \"myspider\"\n\n    download_delay = 2\n\n    # [ ... rest of the spider code ... ]\n```\n\n----------------------------------------\n\nTITLE: Using JMESPath Selectors with TextResponse in Python\nDESCRIPTION: Example of using the new JMESPath selector shortcut method on TextResponse objects, available since parsel 1.8.1.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nresponse.jmespath('foo.bar[*].baz')\n```\n\n----------------------------------------\n\nTITLE: Using bytes_received Signal to Monitor and Cancel Response Download\nDESCRIPTION: Example of using the new bytes_received signal to monitor download progress and potentially stop downloads based on specific conditions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Example of using bytes_received signal to cancel download\n# This would typically be set up in the spider's __init__ method\n\nfrom scrapy import signals\n\n@classmethod\ndef from_crawler(cls, crawler):\n    spider = super().from_crawler(crawler)\n    crawler.signals.connect(spider.on_bytes_received, \n                           signal=signals.bytes_received)\n    return spider\n    \ndef on_bytes_received(self, data, request, spider):\n    # Cancel download if response is too large\n    if len(data) > 1024 * 1024:  # 1MB\n        raise scrapy.exceptions.StopDownload(fail=False)\n    # Or if it's not the content type we're looking for\n    if b'Content-Type: text/html' not in data:\n        raise scrapy.exceptions.StopDownload(fail=False)\n```\n\n----------------------------------------\n\nTITLE: Text Extraction from HTML Elements with XPath in Scrapy\nDESCRIPTION: This example shows how to extract text content from HTML elements. It demonstrates two approaches: one that includes the tags and one that extracts only the text content.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nsel.xpath(\"//h1\").getall()  # this includes the h1 tag\nsel.xpath(\"//h1/text()\").getall()  # this excludes the h1 tag\n```\n\n----------------------------------------\n\nTITLE: Constructing Scrapy Selectors from Response Objects\nDESCRIPTION: Demonstrates creating a selector from a Scrapy HtmlResponse object. This approach is useful when you need more control over the parsing process.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from scrapy.selector import Selector\n>>> from scrapy.http import HtmlResponse\n>>> response = HtmlResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n>>> Selector(response=response).xpath(\"//span/text()\").get()\n'good'\n```\n\n----------------------------------------\n\nTITLE: Configuring Visual Studio Code for Debugging Scrapy Spiders\nDESCRIPTION: A launch.json configuration for Visual Studio Code that enables debugging of Scrapy spiders, allowing for breakpoints and inspection of variables during spider execution.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/debug.rst#2025-04-17_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"version\": \"0.1.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Launch Scrapy Spider\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"scrapy\",\n            \"args\": [\n                \"runspider\",\n                \"${file}\"\n            ],\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Opening Response in Browser from Scrapy Shell\nDESCRIPTION: Shows how to use the view() function to open the current response in a web browser for inspection\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> view(response)\nTrue\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Auth Spider in Scrapy\nDESCRIPTION: Example implementation of a crawler with HTTP Basic Authentication credentials. Sets user, password and authentication domain for protected intranet access.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spiders import CrawlSpider\n\nclass SomeIntranetSiteSpider(CrawlSpider):\n    http_user = \"someuser\"\n    http_pass = \"somepass\"\n    http_auth_domain = \"intranet.example.com\"\n    name = \"intranet.example.com\"\n    # .. rest of the spider code omitted ...\n```\n\n----------------------------------------\n\nTITLE: Defining a Dataclass Item with Optional Fields for Item Loaders\nDESCRIPTION: This code shows how to define a dataclass item with optional fields that can be populated incrementally by an ItemLoader. It uses dataclasses.field with default=None to make all fields optional.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n\n@dataclass\nclass InventoryItem:\n    name: Optional[str] = field(default=None)\n    price: Optional[float] = field(default=None)\n    stock: Optional[int] = field(default=None)\n```\n\n----------------------------------------\n\nTITLE: Initializing Extension with Stats Collector in Python\nDESCRIPTION: This snippet demonstrates how to create an extension class that accesses the Stats Collector through the Crawler object. It shows the initialization and class method for creating the extension from a crawler instance.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/stats.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ExtensionThatAccessStats:\n    def __init__(self, stats):\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.stats)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Duplicate Filter in Python\nDESCRIPTION: This code snippet demonstrates how to create a custom duplicate filter by subclassing RFPDupeFilter and implementing a custom request fingerprinter. It overrides the __init__ method to use a different request fingerprinting class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.dupefilters import RFPDupeFilter\nfrom scrapy.utils.request import fingerprint\n\n\nclass CustomRequestFingerprinter:\n    def fingerprint(self, request):\n        return fingerprint(request, include_headers=[\"X-ID\"])\n\n\nclass CustomDupeFilter(RFPDupeFilter):\n\n    def __init__(self, path=None, debug=False, *, fingerprinter=None):\n        super().__init__(\n            path=path, debug=debug, fingerprinter=CustomRequestFingerprinter()\n        )\n```\n\n----------------------------------------\n\nTITLE: Activating Downloader Middleware in Scrapy Settings\nDESCRIPTION: This snippet shows how to activate a custom downloader middleware component by adding it to the DOWNLOADER_MIDDLEWARES setting. The setting is a dict where keys are middleware class paths and values are ordering numbers.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDOWNLOADER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomDownloaderMiddleware\": 543,\n}\n```\n\n----------------------------------------\n\nTITLE: Basic trackref Usage in Telnet Console - Python\nDESCRIPTION: Example showing how to use the prefs() function in Scrapy's telnet console to inspect live object references.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/leaks.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> prefs()\nLive References\n\nExampleSpider                       1   oldest: 15s ago\nHtmlResponse                       10   oldest: 1s ago\nSelector                            2   oldest: 0s ago\nFormRequest                       878   oldest: 7s ago\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expressions with Selectors in Python\nDESCRIPTION: Shows how to use regular expressions with selectors to extract specific patterns from HTML content.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_15\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.xpath('//a[contains(@href, \"image\")]/text()').re(r\"Name:\\s*(.*)\")\n['My image 1 ',\n'My image 2 ',\n'My image 3 ',\n'My image 4 ',\n'My image 5 ']\n\n>>> response.xpath('//a[contains(@href, \"image\")]/text()').re_first(r\"Name:\\s*(.*)\")\n'My image 1 '\n```\n\n----------------------------------------\n\nTITLE: Handling Text Nodes in XPath Conditions\nDESCRIPTION: Demonstrates the correct way to use text content in XPath string functions, avoiding issues with node-set to string conversion.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_24\n\nLANGUAGE: pycon\nCODE:\n```\n>>> from scrapy import Selector\n>>> sel = Selector(\n...     text='<a href=\"#\">Click here to go to the <strong>Next Page</strong></a>'\n... )\n>>> sel.xpath(\"//a//text()\").getall()  # take a peek at the node-set\n['Click here to go to the ', 'Next Page']\n>>> sel.xpath(\"string(//a[1]//text())\").getall()  # convert it to string\n['Click here to go to the ']\n>>> sel.xpath(\"//a[1]\").getall()  # select the first node\n['<a href=\"#\">Click here to go to the <strong>Next Page</strong></a>']\n>>> sel.xpath(\"string(//a[1])\").getall()  # convert it to string\n['Click here to go to the Next Page']\n>>> sel.xpath(\"//a[contains(.//text(), 'Next Page')]\").getall()\n[]\n>>> sel.xpath(\"//a[contains(., 'Next Page')]\").getall()\n['<a href=\"#\">Click here to go to the <strong>Next Page</strong></a>']\n```\n\n----------------------------------------\n\nTITLE: Selecting First <li> Elements Under <ul> Parents\nDESCRIPTION: Demonstrates selecting all first <li> elements that are direct children of <ul> elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_22\n\nLANGUAGE: pycon\nCODE:\n```\n>>> xp(\"//ul/li[1]\")\n['<li>1</li>', '<li>4</li>']\n```\n\n----------------------------------------\n\nTITLE: Testing XPath Selector in Scrapy Shell\nDESCRIPTION: Demonstrates testing an XPath selector against the current response to extract elements with class 'fn' inside h1 tags\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> response.xpath('//h1[@class=\"fn\"]')\n[]\n```\n\n----------------------------------------\n\nTITLE: Converting Scrapy Request to cURL Command in Python\nDESCRIPTION: Using the new request_to_curl utility function to generate a cURL command from a Scrapy Request object.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom scrapy.utils.request import request_to_curl\n\nrequest = scrapy.Request(url='http://example.com')\ncurl_command = request_to_curl(request)\n```\n\n----------------------------------------\n\nTITLE: Implementing BeautifulSoup with Scrapy Spider\nDESCRIPTION: Example of using BeautifulSoup within a Scrapy spider to parse HTML content. Uses lxml parser for better performance and extracts URL and title information from web pages.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/faq.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom bs4 import BeautifulSoup\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = \"example\"\n    allowed_domains = [\"example.com\"]\n    start_urls = (\"http://www.example.com/\",)\n\n    def parse(self, response):\n        # use lxml to get decent HTML parsing speed\n        soup = BeautifulSoup(response.text, \"lxml\")\n        yield {\"url\": response.url, \"title\": soup.h1.string}\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Processors in Scrapy\nDESCRIPTION: This section defines various request processors including Canonicalize for URL canonicalization, Unique for filtering duplicate requests, FilterDomain for domain-based filtering, and FilterUrl for URL-based filtering.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Canonicalize(object):\n    \"\"\"Canonicalize Request Processor\"\"\"\n\n    def __call__(self, requests):\n        \"\"\"Canonicalize all requests' urls\"\"\"\n        for req in requests:\n            # replace in-place\n            req.url = canonicalize_url(req.url)\n            yield req\n\n\nclass Unique(object):\n    \"\"\"Filter duplicate Requests\"\"\"\n\n    def __init__(self, *attributes):\n        \"\"\"Initialize comparison attributes\"\"\"\n        self._attributes = attributes or [\"url\"]\n\n    def _requests_equal(self, req1, req2):\n        \"\"\"Attribute comparison helper\"\"\"\n        for attr in self._attributes:\n            if getattr(req1, attr) != getattr(req2, attr):\n                return False\n        # all attributes equal\n        return True\n\n    def _request_in(self, request, requests_seen):\n        \"\"\"Check if request is in given requests seen list\"\"\"\n        for seen in requests_seen:\n            if self._requests_equal(request, seen):\n                return True\n        # request not seen\n        return False\n\n    def __call__(self, requests):\n        \"\"\"Filter seen requests\"\"\"\n        # per-call duplicates filter\n        requests_seen = set()\n        for req in requests:\n            if not self._request_in(req, requests_seen):\n                yield req\n                # registry seen request\n                requests_seen.add(req)\n\n\nclass FilterDomain(object):\n    \"\"\"Filter request's domain\"\"\"\n\n    def __init__(self, allow=(), deny=()):\n        \"\"\"Initialize allow/deny attributes\"\"\"\n        self.allow = tuple(arg_to_iter(allow))\n        self.deny = tuple(arg_to_iter(deny))\n\n    def __call__(self, requests):\n        \"\"\"Filter domains\"\"\"\n        processed = (req for req in requests)\n\n        if self.allow:\n            processed = (\n                req for req in requests if url_is_from_any_domain(req.url, self.allow)\n            )\n        if self.deny:\n            processed = (\n                req\n                for req in requests\n                if not url_is_from_any_domain(req.url, self.deny)\n            )\n\n        return processed\n\n\nclass FilterUrl(object):\n    \"\"\"Filter request's url\"\"\"\n\n    def __init__(self, allow=(), deny=()):\n        \"\"\"Initialize allow/deny attributes\"\"\"\n        _re_type = type(re.compile(\"\", 0))\n\n        self.allow_res = [\n            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)\n        ]\n        self.deny_res = [\n            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)\n        ]\n\n    def __call__(self, requests):\n        \"\"\"Filter request's url based on allow/deny rules\"\"\"\n        # TODO: filter valid urls here?\n        processed = (req for req in requests)\n\n        if self.allow_res:\n            processed = (\n                req for req in requests if self._matches(req.url, self.allow_res)\n            )\n        if self.deny_res:\n            processed = (\n                req for req in requests if not self._matches(req.url, self.deny_res)\n            )\n\n        return processed\n\n    def _matches(self, url, regexs):\n        \"\"\"Returns True if url matches any regex in given list\"\"\"\n        return any(r.search(url) for r in regexs)\n```\n\n----------------------------------------\n\nTITLE: Accessing Settings in Spider Parse Method\nDESCRIPTION: Demonstrates how to access settings within a spider's parse method using self.settings.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n    start_urls = [\"http://example.com\"]\n\n    def parse(self, response):\n        print(f\"Existing settings: {self.settings.attributes.keys()}\")\n```\n\n----------------------------------------\n\nTITLE: Spider Error Signal Handler in Python\nDESCRIPTION: Signal handler for spider errors that is triggered when a spider callback generates an exception. Takes failure, response and spider objects as parameters.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef spider_error(failure, response, spider)\n```\n\n----------------------------------------\n\nTITLE: Using data: URI Download Handler in Scrapy (Python)\nDESCRIPTION: Example of using the new data: URI download handler added in Scrapy 1.4. This allows downloading from data URIs.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\nNew ``data:`` URI download handler (:issue:`2334`, fixes :issue:`2156`)\n```\n\n----------------------------------------\n\nTITLE: Implementing Request/Response Matchers in Python\nDESCRIPTION: This code defines base and specific matcher classes for evaluating request or response attributes. It includes implementations for URL matching and regex-based URL matching.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass BaseMatcher(object):\n    \"\"\"Base matcher. Returns True by default.\"\"\"\n\n    def matches_request(self, request):\n        \"\"\"Performs Request Matching\"\"\"\n        return True\n\n    def matches_response(self, response):\n        \"\"\"Performs Response Matching\"\"\"\n        return True\n\n\nclass UrlMatcher(BaseMatcher):\n    \"\"\"Matches URL attribute\"\"\"\n\n    def __init__(self, url):\n        \"\"\"Initialize url attribute\"\"\"\n        self._url = url\n\n    def matches_url(self, url):\n        \"\"\"Returns True if given url is equal to matcher's url\"\"\"\n        return self._url == url\n\n    def matches_request(self, request):\n        \"\"\"Returns True if Request's url matches initial url\"\"\"\n        return self.matches_url(request.url)\n\n    def matches_response(self, response):\n        \"\"\"REturns True if Response's url matches initial url\"\"\"\n        return self.matches_url(response.url)\n\n\nclass UrlRegexMatcher(UrlMatcher):\n    \"\"\"Matches URL using regular expression\"\"\"\n\n    def __init__(self, regex, flags=0):\n        \"\"\"Initialize regular expression\"\"\"\n        self._regex = re.compile(regex, flags)\n\n    def matches_url(self, url):\n        \"\"\"Returns True if url matches regular expression\"\"\"\n        return self._regex.search(url) is not None\n```\n\n----------------------------------------\n\nTITLE: Using Spider Arguments in start_requests Method\nDESCRIPTION: This snippet shows an alternative way to use spider arguments by accessing them as attributes in the start_requests method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n\n    def start_requests(self):\n        yield scrapy.Request(f\"http://www.example.com/categories/{self.category}\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Feed Export Field Names in Scrapy\nDESCRIPTION: Demonstrates how to customize the output names of item fields when exporting data feeds using the FEED_EXPORT_FIELDS setting as a dictionary.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nFEED_EXPORT_FIELDS = {\n    'name': 'Product Name',\n    'price': 'Retail Price'\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from HTML Elements using CSS Selectors in Python\nDESCRIPTION: Demonstrates how to use CSS selectors to extract text from HTML elements, specifically the title of a web page. Shows different methods of extraction and handling of results.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_2\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.css(\"title::text\").getall()\n['Quotes to Scrape']\n\n>>> response.css(\"title\").getall()\n['<title>Quotes to Scrape</title>']\n\n>>> response.css(\"title::text\").get()\n'Quotes to Scrape'\n\n>>> response.css(\"title::text\")[0].get()\n'Quotes to Scrape'\n```\n\n----------------------------------------\n\nTITLE: Decoding Response Body to String (Python)\nDESCRIPTION: This code snippet shows how to correctly convert the response body into a string using the encoding attribute. It warns against using str(response.body) as it produces an incorrect result.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> str(b\"body\")\n\"b'body'\"\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Spiders with CrawlerRunner in Python\nDESCRIPTION: Example of running multiple spiders simultaneously using CrawlerRunner. This approach uses the join() method to get a deferred that fires when all scheduled spiders have completed, then stops the reactor.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/practices.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.project import get_project_settings\n\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    ...\n\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    ...\n\n\nconfigure_logging()\nsettings = get_project_settings()\nrunner = CrawlerRunner(settings)\nrunner.crawl(MySpider1)\nrunner.crawl(MySpider2)\nd = runner.join()\n\nfrom twisted.internet import reactor\n\nd.addBoth(lambda _: reactor.stop())\n\nreactor.run()  # the script will block here until all crawling jobs are finished\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Scrapy Crawler Using the Proposed Library API in Python\nDESCRIPTION: This code demonstrates the proposed library-like API for Scrapy that allows creating crawlers with callback functions. It shows how to initialize a Crawler with start URLs, define parse functions for handling responses, and run the crawler to populate a list of scraped items.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-004.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python\nfrom scrapy.http import Request\nfrom scrapy import Crawler\n\n# a container to hold scraped items\nscraped_items = []\n\n\ndef parse_start_page(response):\n    # collect urls to follow into urls_to_follow list\n    requests = [Request(url, callback=parse_other_page) for url in urls_to_follow]\n    return requests\n\n\ndef parse_other_page(response):\n    # ... parse items from response content ...\n    scraped_items.extend(parsed_items)\n\n\nstart_urls = [\"http://www.example.com/start_page.html\"]\n\ncr = Crawler(start_urls, callback=parse_start_page)\ncr.run()  # blocking call - this populates scraped_items\n\nprint(\"%d items scraped\" % len(scraped_items))\n# ... do something more interesting with scraped_items ...\n```\n\n----------------------------------------\n\nTITLE: Launching Scrapy Shell with Local Files\nDESCRIPTION: Examples of how to launch the Scrapy shell with local HTML files using different syntaxes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# UNIX-style\nscrapy shell ./path/to/file.html\nscrapy shell ../other/path/to/file.html\nscrapy shell /absolute/path/to/file.html\n\n# File URI\nscrapy shell file:///absolute/path/to/file.html\n```\n\n----------------------------------------\n\nTITLE: Defining Per-Spider Settings (Python)\nDESCRIPTION: Shows how to define custom settings for individual spiders using the custom_settings class variable. This allows overriding project-level settings for specific spiders.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\nclass MySpider(scrapy.Spider):\n    custom_settings = {\n        \"DOWNLOAD_DELAY\": 5.0,\n        \"RETRY_ENABLED\": False,\n    }\n```\n\n----------------------------------------\n\nTITLE: Using Bulk Item Loader in Python\nDESCRIPTION: Shows how to use the proposed Bulk Item Loader to simplify data extraction from a definition list. It requires less code and automatically handles multiple entries.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-020.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbil = BulkItemLoader(response=response)\nbil.parse_dl('//div[@class=\"geeks\"]/dl')\n```\n\n----------------------------------------\n\nTITLE: Disabling Built-in Spider Middleware in Python\nDESCRIPTION: Example showing how to disable a built-in middleware by setting its value to None in SPIDER_MIDDLEWARES while enabling a custom middleware replacement.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spider-middleware.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSPIDER_MIDDLEWARES = {\n    \"scrapy.spidermiddlewares.referer.RefererMiddleware\": None,\n    \"myproject.middlewares.CustomRefererSpiderMiddleware\": 700,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Status Code Handling in Scrapy Spider\nDESCRIPTION: Example showing how to configure a CrawlSpider to handle specific HTTP status codes (301, 302) by setting the handle_httpstatus_list class attribute.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass MySpider(CrawlSpider):\n    handle_httpstatus_list = [301, 302]\n```\n\n----------------------------------------\n\nTITLE: Comparing Old and New Extraction Methods in Python\nDESCRIPTION: Compares the old extract() and extract_first() methods with the new get() and getall() methods for extracting data from selectors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_16\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.css(\"a::attr(href)\").get()\n'image1.html'\n>>> response.css(\"a::attr(href)\").extract_first()\n'image1.html'\n\n>>> response.css(\"a::attr(href)\").getall()\n['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']\n>>> response.css(\"a::attr(href)\").extract()\n['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']\n\n>>> response.css(\"a::attr(href)\")[0].get()\n'image1.html'\n>>> response.css(\"a::attr(href)\")[0].extract()\n'image1.html'\n\n>>> response.css(\"a::attr(href)\")[0].getall()\n['image1.html']\n```\n\n----------------------------------------\n\nTITLE: Request Scheduled Signal Handler in Python\nDESCRIPTION: Signal handler triggered when engine schedules a Request before it reaches the scheduler. Can raise IgnoreRequest to drop requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef request_scheduled(request, spider)\n```\n\n----------------------------------------\n\nTITLE: Accessing Redirect Reasons in Scrapy Requests\nDESCRIPTION: A new redirect_reasons request meta key exposes the reason (status code, meta refresh) behind every followed redirect.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\nrequest.meta['redirect_reasons']\n```\n\n----------------------------------------\n\nTITLE: Using CrawlerRunner with Manual Reactor Control in Python\nDESCRIPTION: Example of using CrawlerRunner, which provides more control over the crawling process by not interfering with existing reactors. This approach requires manually configuring logging and stopping the reactor after the spider finishes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/practices.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\n\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\n\nconfigure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\"})\nrunner = CrawlerRunner()\n\nd = runner.crawl(MySpider)\n\nfrom twisted.internet import reactor\n\nd.addBoth(lambda _: reactor.stop())\nreactor.run()  # the script will block here until the crawling is finished\n```\n\n----------------------------------------\n\nTITLE: Request Reached Downloader Signal Handler in Python\nDESCRIPTION: Signal handler executed when a Request reaches the downloader. Takes request and spider objects as parameters.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef request_reached_downloader(request, spider)\n```\n\n----------------------------------------\n\nTITLE: Creating Attrs-based Item\nDESCRIPTION: Example showing how to create an item using the attrs library for attribute management.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/items.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport attr\n\n@attr.s\nclass CustomItem:\n    one_field = attr.ib()\n    another_field = attr.ib()\n```\n\n----------------------------------------\n\nTITLE: Processing Link Values with Regular Expressions\nDESCRIPTION: Example of a process_value function that extracts links from JavaScript code using regular expressions. This function can be passed to the LxmlLinkExtractor to extract links from JavaScript code in href attributes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/link-extractors.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef process_value(value):\n    m = re.search(r\"javascript:goToPage\\('(.*?)'\\)\", value)\n    if m:\n        return m.group(1)\n```\n\n----------------------------------------\n\nTITLE: Detecting Spider Contract Check Runs in Python\nDESCRIPTION: This code snippet shows how to detect when spider contract checks are running using the SCRAPY_CHECK environment variable. It allows for adjustments to the spider's behavior during checks.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/contracts.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = \"example\"\n\n    def __init__(self):\n        if os.environ.get(\"SCRAPY_CHECK\"):\n            pass  # Do some scraper adjustments when a check is running\n```\n\n----------------------------------------\n\nTITLE: Defining Duplicate Filter Interface in Python\nDESCRIPTION: This code snippet outlines the interface that must be implemented by a class assigned to the DUPEFILTER_CLASS setting. It includes methods for initialization, request checking, opening and closing the filter, and logging filtered requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass MyDupeFilter:\n\n    @classmethod\n    def from_settings(cls, settings):\n        \"\"\"Returns an instance of this duplicate request filtering class\n        based on the current crawl settings.\"\"\"\n        return cls()\n\n    def request_seen(self, request):\n        \"\"\"Returns ``True`` if *request* is a duplicate of another request\n        seen in a previous call to :meth:`request_seen`, or ``False``\n        otherwise.\"\"\"\n        return False\n\n    def open(self):\n        \"\"\"Called before the spider opens. It may return a deferred.\"\"\"\n        pass\n\n    def close(self, reason):\n        \"\"\"Called before the spider closes. It may return a deferred.\"\"\"\n        pass\n\n    def log(self, request, spider):\n        \"\"\"Logs that a request has been filtered out.\n\n        It is called right after a call to :meth:`request_seen` that\n        returns ``True``.\n\n        If :meth:`request_seen` always returns ``False``, such as in the\n        case of :class:`~scrapy.dupefilters.BaseDupeFilter`, this method\n        may be omitted.\n        \"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Setting Spider User Agent in Scrapy\nDESCRIPTION: Example showing the required spider attribute structure for overriding the default user agent in Scrapy. The spider must define a user_agent attribute to customize the user agent string.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nuser_agent\n```\n\n----------------------------------------\n\nTITLE: Accessing Crawler Instance in Scrapy Add-on (Python)\nDESCRIPTION: Example of a Scrapy add-on that accesses the crawler instance. It demonstrates the use of __init__, from_crawler, and update_settings methods to properly initialize the add-on with the crawler object.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/addons.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyAddon:\n    def __init__(self, crawler) -> None:\n        super().__init__()\n        self.crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n    def update_settings(self, settings): ...\n```\n\n----------------------------------------\n\nTITLE: Using the attrib Property for Attribute Extraction in Python\nDESCRIPTION: Demonstrates how to use the attrib property of Selector and SelectorList objects to extract attribute values from HTML elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_14\n\nLANGUAGE: pycon\nCODE:\n```\n>>> [a.attrib[\"href\"] for a in response.css(\"a\")]\n['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']\n\n>>> response.css(\"base\").attrib\n{'href': 'http://example.com/'}\n>>> response.css(\"base\").attrib[\"href\"]\n'http://example.com/'\n\n>>> response.css(\"foo\").attrib\n{}\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry HTTP Codes in Scrapy Settings\nDESCRIPTION: The HTTP status code 429 is now included in the RETRY_HTTP_CODES setting by default. This change is backward incompatible and may require overriding the setting if 429 should not be retried.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nRETRY_HTTP_CODES = [...]  # Override to exclude 429 if needed\n```\n\n----------------------------------------\n\nTITLE: Raising CloseSpider Exception in Scrapy Spider\nDESCRIPTION: Example showing how to raise a CloseSpider exception from a spider callback to stop the spider when bandwidth limit is exceeded. The exception takes a reason parameter to specify why the spider is being closed.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exceptions.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef parse_page(self, response):\n    if \"Bandwidth exceeded\" in response.body:\n        raise CloseSpider(\"bandwidth_exceeded\")\n```\n\n----------------------------------------\n\nTITLE: Bytes Received Signal Handler in Python\nDESCRIPTION: Signal handler for when bytes are received by HTTP/S3 handlers for a request. Allows stopping downloads via StopDownload exception.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef bytes_received(data, request, spider)\n```\n\n----------------------------------------\n\nTITLE: Implementing CrawlSpider Class in Scrapy\nDESCRIPTION: This class extends InitSpider to implement a crawl spider. It initializes a rules manager and request generator, setting up the infrastructure for rule-based crawling.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass CrawlSpider(InitSpider):\n    \"\"\"CrawlSpider v2\"\"\"\n\n    request_extractors = []\n    request_processors = []\n    rules = []\n\n    def __init__(self):\n        \"\"\"Initialize dispatcher\"\"\"\n        super(CrawlSpider, self).__init__()\n\n        # wrap rules\n        self._rulesman = RulesManager(self.rules, spider=self)\n        # generates new requests with given callback\n        self._reqgen = RequestGenerator(\n            self.request_extractors, self.request_processors, self.parse\n        )\n\n    def parse(self, response):\n```\n\n----------------------------------------\n\nTITLE: Feed Exporter Closed Signal Handler in Python\nDESCRIPTION: Signal handler that executes when the feed exports extension is closed. Called during spider_closed signal handling after all exports are complete.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef feed_exporter_closed()\n```\n\n----------------------------------------\n\nTITLE: Disabling Built-in Middleware in Scrapy Settings\nDESCRIPTION: This example demonstrates how to disable a built-in middleware (UserAgentMiddleware in this case) by setting its value to None in the DOWNLOADER_MIDDLEWARES setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDOWNLOADER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomDownloaderMiddleware\": 543,\n    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": None,\n}\n```\n\n----------------------------------------\n\nTITLE: Running a Project Spider with CrawlerProcess in Python\nDESCRIPTION: This example shows how to run a spider from an existing Scrapy project using get_project_settings() to import project settings. It references a spider by name instead of class, suitable for working within an existing project.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/practices.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nprocess = CrawlerProcess(get_project_settings())\n\n# 'followall' is the name of one of the spiders of the project.\nprocess.crawl(\"followall\", domain=\"scrapy.org\")\nprocess.start()  # the script will block here until the crawling is finished\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Request Fingerprinter in Scrapy\nDESCRIPTION: Shows how to customize request fingerprinting in Scrapy using the new REQUEST_FINGERPRINTER_CLASS setting. This allows centralizing fingerprint configuration instead of changing it in every component.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nREQUEST_FINGERPRINTER_CLASS = 'myproject.fingerprinters.CustomFingerprinter'\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Spider Middleware in Scrapy\nDESCRIPTION: Shows how to define the process_spider_output method of a spider middleware as an asynchronous generator.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nasync def process_spider_output(self, response, result, spider):\n    async for item in result:\n        yield item\n```\n\n----------------------------------------\n\nTITLE: Response Received Signal Handler in Python\nDESCRIPTION: Signal handler called when engine receives a new Response from downloader. Takes response, request and spider objects as parameters.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef response_received(response, request, spider)\n```\n\n----------------------------------------\n\nTITLE: Custom Logging in Scrapy Spider\nDESCRIPTION: Demonstrates how to use a custom logger within a Scrapy spider instead of the default spider logger.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport scrapy\n\nlogger = logging.getLogger(\"mycustomlogger\")\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n    start_urls = [\"https://scrapy.org\"]\n\n    def parse(self, response):\n        logger.info(\"Parse function called on %s\", response.url)\n```\n\n----------------------------------------\n\nTITLE: Using JMESPath Query in TextResponse (Python)\nDESCRIPTION: This snippet demonstrates how to use the jmespath method of TextResponse to perform a JMESPath query on the response content.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresponse.jmespath('object.[*]')\n```\n\n----------------------------------------\n\nTITLE: Using Asyncio with Custom Loop in Scrapy\nDESCRIPTION: When using asyncio in Scrapy, it is now possible to set a custom asyncio loop for more control over asynchronous operations.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Example of setting a custom asyncio loop\nimport asyncio\nloop = asyncio.new_event_loop()\nasyncio.set_event_loop(loop)\n```\n\n----------------------------------------\n\nTITLE: Using keep_fragments Parameter with request_fingerprint\nDESCRIPTION: Example of the new keep_fragments parameter of request_fingerprint function that allows generating different fingerprints for requests with different fragments in their URL.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nscrapy.utils.request.request_fingerprint(keep_fragments=True)\n```\n\n----------------------------------------\n\nTITLE: Request Dropped Signal Handler in Python\nDESCRIPTION: Signal handler called when a scheduled Request is rejected by the scheduler. Takes request and spider objects as parameters.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef request_dropped(request, spider)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating ItemLoader Value Conversion in Python\nDESCRIPTION: Example showing how ItemLoader now automatically converts input item values into lists, which is needed to allow adding values to existing fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n>>> item = MyItem()\n>>> item[\"field\"] = \"value1\"\n>>> loader = ItemLoader(item=item)\n>>> item[\"field\"]\n['value1']\n```\n\n----------------------------------------\n\nTITLE: Implementing Item Multiplication Pipeline in Scrapy\nDESCRIPTION: A spider middleware implementation that demonstrates how to split a single item into multiple items during processing. The middleware checks if the result is a Request object and yields multiple copies of items based on a multiply_by field.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/faq.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import deepcopy\n\nfrom itemadapter import ItemAdapter\nfrom scrapy import Request\n\n\nclass MultiplyItemsMiddleware:\n    def process_spider_output(self, response, result, spider):\n        for item_or_request in result:\n            if isinstance(item_or_request, Request):\n                continue\n            adapter = ItemAdapter(item)\n            for _ in range(adapter[\"multiply_by\"]):\n                yield deepcopy(item)\n```\n\n----------------------------------------\n\nTITLE: Overriding serialize_field Method in Custom XML Exporter\nDESCRIPTION: This code demonstrates how to create a custom XML exporter by subclassing XmlItemExporter and overriding the serialize_field method. It applies custom serialization to the 'price' field while using default serialization for other fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.exporters import XmlItemExporter\n\n\nclass ProductXmlExporter(XmlItemExporter):\n    def serialize_field(self, field, name, value):\n        if name == \"price\":\n            return f\"$ {str(value)}\"\n        return super().serialize_field(field, name, value)\n```\n\n----------------------------------------\n\nTITLE: Sending Email with MailSender in Scrapy\nDESCRIPTION: Example of sending an email without attachments using the MailSender instance. Demonstrates setting recipients, subject, body and CC fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/email.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmailer.send(\n    to=[\"someone@example.com\"],\n    subject=\"Some subject\",\n    body=\"Some body\",\n    cc=[\"another@example.com\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Per-Domain Download Settings in Scrapy\nDESCRIPTION: This code snippet demonstrates how to configure concurrency and delay settings for specific domains using the DOWNLOAD_SLOTS setting in Scrapy.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nDOWNLOAD_SLOTS = {\n    \"quotes.toscrape.com\": {\"concurrency\": 1, \"delay\": 2, \"randomize_delay\": False},\n    \"books.toscrape.com\": {\"delay\": 3, \"randomize_delay\": False},\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Item List Fields in Scrapy\nDESCRIPTION: Example demonstrating how to use ListField with nested Item types, specifically showing a Product-Variant relationship where products can have multiple variants.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-002.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.item.models import Item\nfrom scrapy.item.fields import ListField, TextField\n\nclass Variant(Item):\n    name = TextField()\n\nclass Product(Variant):\n    variants = ListField(ItemField(Variant))\n```\n\n----------------------------------------\n\nTITLE: Performing CSS Query in TextResponse (Python)\nDESCRIPTION: This snippet illustrates how to use the css method of TextResponse to perform a CSS selector query on the response content.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nresponse.css('p')\n```\n\n----------------------------------------\n\nTITLE: Updating Selector API Usage in Python\nDESCRIPTION: The preferred methods for extracting data from selectors have changed from extract_first() and extract() to get() and getall(). This improves code readability and conciseness.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\n# Old API\nfirst_item = selector.extract_first()\nall_items = selector.extract()\n\n# New API \nfirst_item = selector.get()\nall_items = selector.getall()\n```\n\n----------------------------------------\n\nTITLE: Setting Custom URI Parameters Function in Scrapy Settings\nDESCRIPTION: This snippet shows how to set the FEED_URI_PARAMS setting to use a custom function for generating feed URI parameters in Scrapy's settings.py file.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# myproject/settings.py\nFEED_URI_PARAMS = \"myproject.utils.uri_params\"\n```\n\n----------------------------------------\n\nTITLE: JSON Lines Export Format Example\nDESCRIPTION: Example output format for JsonLinesItemExporter showing product data line by line\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"Color TV\", \"price\": \"1200\"}\n{\"name\": \"DVD player\", \"price\": \"200\"}\n```\n\n----------------------------------------\n\nTITLE: Extending NewsItemBuilder using static methods in Scrapy\nDESCRIPTION: This snippet demonstrates how to extend the NewsItemBuilder class using static methods, reusing the parent's published field and adding a date conversion.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SiteNewsItemBuilder(NewsItemBuilder):\n    published = reducers.Reducer(NewsItemBuilder.published, to_date(\"%d.%m.%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Invoking Scrapy Shell from Spider\nDESCRIPTION: Example of how to invoke the Scrapy shell from within a spider to inspect responses during crawling.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n    start_urls = [\n        \"http://example.com\",\n        \"http://example.org\",\n        \"http://example.net\",\n    ]\n\n    def parse(self, response):\n        # We want to inspect one specific response.\n        if \".org\" in response.url:\n            from scrapy.shell import inspect_response\n\n            inspect_response(response, self)\n\n        # Rest of parsing code.\n```\n\n----------------------------------------\n\nTITLE: Logging from Scrapy Spider\nDESCRIPTION: Shows how to use the built-in logger in a Scrapy spider to log messages during the parsing process.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = \"myspider\"\n    start_urls = [\"https://scrapy.org\"]\n\n    def parse(self, response):\n        self.logger.info(\"Parse function called on %s\", response.url)\n```\n\n----------------------------------------\n\nTITLE: Using NO_CALLBACK for Request callback (Python)\nDESCRIPTION: Shows how to use the new NO_CALLBACK constant to explicitly set no callback for a Request, distinguishing it from None which uses the default spider callback.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nRequest(url, callback=scrapy.http.request.NO_CALLBACK)\n```\n\n----------------------------------------\n\nTITLE: Accessing callback arguments in errback functions (Python)\nDESCRIPTION: This snippet shows how to access callback arguments in errback functions using Failure.request.cb_kwargs. It demonstrates passing arguments to both callback and errback functions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef parse(self, response):\n    request = scrapy.Request(\n        \"http://www.example.com/index.html\",\n        callback=self.parse_page2,\n        errback=self.errback_page2,\n        cb_kwargs=dict(main_url=response.url),\n    )\n    yield request\n\n\ndef parse_page2(self, response, main_url):\n    pass\n\n\ndef errback_page2(self, failure):\n    yield dict(\n        main_url=failure.request.cb_kwargs[\"main_url\"],\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrent Requests in Scrapy\nDESCRIPTION: Sets the global concurrency limit for parallel request processing. The recommended starting value is 100, but should be adjusted based on CPU and memory availability.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCONCURRENT_REQUESTS = 100\n```\n\n----------------------------------------\n\nTITLE: Extracting Attribute Values with CSS Selectors in Python\nDESCRIPTION: Shows how to use CSS selectors to extract attribute values from HTML elements, specifically the href attribute from anchor tags.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_11\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.css(\"a::attr(href)\").getall()\n['image1.html',\n'image2.html',\n'image3.html',\n'image4.html',\n'image5.html']\n```\n\n----------------------------------------\n\nTITLE: Accessing HTML Element Attributes in Python\nDESCRIPTION: New Selector.attrib and SelectorList.attrib properties have been introduced to make it easier to get attributes of HTML elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\n# Accessing attributes\nattribute_value = selector.attrib['attribute_name']\nattribute_values = selector_list.attrib['attribute_name']\n```\n\n----------------------------------------\n\nTITLE: Configuring Breadth-First Order Crawling in Scrapy\nDESCRIPTION: Settings configuration to make Scrapy crawl in breadth-first order (BFO) instead of the default depth-first order. Modifies queue settings and depth priority.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/faq.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDEPTH_PRIORITY = 1\nSCHEDULER_DISK_QUEUE = \"scrapy.squeues.PickleFifoDiskQueue\"\nSCHEDULER_MEMORY_QUEUE = \"scrapy.squeues.FifoMemoryQueue\"\n```\n\n----------------------------------------\n\nTITLE: Using Path Objects with FEED_URI Setting\nDESCRIPTION: Example showing how the FEED_URI setting now supports pathlib.Path values, allowing for more Pythonic file path handling.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nFEED_URI = pathlib.Path(...)\n```\n\n----------------------------------------\n\nTITLE: Initializing MailSender with Crawler Instance\nDESCRIPTION: Shows how to instantiate MailSender using a Scrapy Crawler instance, which will automatically respect the configured email settings.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/email.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmailer = MailSender.from_crawler(crawler)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Log Filter in Python\nDESCRIPTION: Demonstrates how to create a custom log filter to exclude certain log messages based on their content using regular expressions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport re\n\n\nclass ContentFilter(logging.Filter):\n    def filter(self, record):\n        match = re.search(r\"\\d{3} [Ee]rror, retrying\", record.message)\n        if match:\n            return False\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Item Pipeline Drop Handling in Python\nDESCRIPTION: Example showing how to customize log levels when dropping items in a Scrapy item pipeline by setting the log_level attribute on DropItem exceptions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.exceptions import DropItem\n\n\nclass MyPipeline:\n    def process_item(self, item, spider):\n        if not item.get(\"price\"):\n            raise DropItem(\"Missing price data\", log_level=\"INFO\")\n        return item\n```\n\n----------------------------------------\n\nTITLE: Configuring Periodic Log Extension in Scrapy\nDESCRIPTION: Example configuration for the PeriodicLog extension showing how to set up stats logging preferences including delta tracking, stats inclusion/exclusion, and timing data settings.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/extensions.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncustom_settings = {\n    \"LOG_LEVEL\": \"INFO\",\n    \"PERIODIC_LOG_STATS\": {\n        \"include\": [\"downloader/\", \"scheduler/\", \"log_count/\", \"item_scraped_count/\"],\n    },\n    \"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"]},\n    \"PERIODIC_LOG_TIMING_ENABLED\": True,\n    \"EXTENSIONS\": {\n        \"scrapy.extensions.periodic_log.PeriodicLog\": 0,\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Request Fingerprinting in Python\nDESCRIPTION: A simple RequestFingerprinter class that checks for a fingerprint in request.meta or generates one using the fingerprint function.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass RequestFingerprinter:\n    def fingerprint(self, request):\n        if \"fingerprint\" in request.meta:\n            return request.meta[\"fingerprint\"]\n        return fingerprint(request)\n```\n\n----------------------------------------\n\nTITLE: Controlling Scrapy Engine State\nDESCRIPTION: Examples showing how to pause, resume, and stop the Scrapy engine using telnet console commands.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/telnetconsole.rst#2025-04-17_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n```\n\nLANGUAGE: shell\nCODE:\n```\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n```\n\nLANGUAGE: shell\nCODE:\n```\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n```\n\n----------------------------------------\n\nTITLE: Installing Scrapy with Conda\nDESCRIPTION: Command to install Scrapy using the conda package manager from the conda-forge channel. This method is recommended for users of Anaconda or Miniconda and works on Linux, Windows, and macOS.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge scrapy\n```\n\n----------------------------------------\n\nTITLE: Using CSS Pseudo-elements for Text Selection in Scrapy\nDESCRIPTION: Demonstrates Scrapy's custom CSS pseudo-elements (::text and ::attr()) for selecting text nodes and attribute values, which are extensions beyond standard CSS selectors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> response.css(\"title::text\").get()\n'Example website'\n\n>>> response.css(\"#images *::text\").getall()\n['\\n   ',\n'Name: My image 1 ',\n'\\n   ',\n'Name: My image 2 ',\n'\\n   ',\n'Name: My image 3 ',\n'\\n   ',\n'Name: My image 4 ',\n'\\n   ',\n'Name: My image 5 ',\n'\\n  ']\n\n>>> response.css(\"img::text\").getall()\n[]\n```\n\n----------------------------------------\n\nTITLE: Enabling Asyncio in New Scrapy Projects\nDESCRIPTION: Demonstrates how to enable asyncio support by default in new Scrapy projects created with the startproject command.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nTWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n```\n\n----------------------------------------\n\nTITLE: Implementing sitemap_filter Method in SitemapSpider Subclass in Python\nDESCRIPTION: A new sitemap_filter method allows selecting sitemap entries based on their attributes in SitemapSpider subclasses.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\nclass MySitemapSpider(SitemapSpider):\n    def sitemap_filter(self, entries):\n        for entry in entries:\n            date_time = datetime.strptime(entry['lastmod'], '%Y-%m-%d')\n            if date_time.year >= 2005:\n                yield entry\n```\n\n----------------------------------------\n\nTITLE: Using DownloaderAwarePriorityQueue for Multi-Domain Crawls in Scrapy\nDESCRIPTION: A new scheduler priority queue, DownloaderAwarePriorityQueue, can be enabled for improved scheduling on multi-domain crawls, but it doesn't support CONCURRENT_REQUESTS_PER_IP.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nSCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n```\n\n----------------------------------------\n\nTITLE: Executing XPath Query in TextResponse (Python)\nDESCRIPTION: This code shows how to use the xpath method of TextResponse to execute an XPath query on the response content.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresponse.xpath('//p')\n```\n\n----------------------------------------\n\nTITLE: Custom request fingerprinter using URL only (Python)\nDESCRIPTION: This example shows how to implement a custom request fingerprinter that only considers the URL of the request when generating the fingerprint. It uses SHA1 hashing and WeakKeyDictionary for caching.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom hashlib import sha1\nfrom weakref import WeakKeyDictionary\n\nfrom scrapy.utils.python import to_bytes\n\n\nclass RequestFingerprinter:\n    cache = WeakKeyDictionary()\n\n    def fingerprint(self, request):\n        if request not in self.cache:\n            fp = sha1()\n            fp.update(to_bytes(request.url))\n            self.cache[request] = fp.digest()\n        return self.cache[request]\n```\n\n----------------------------------------\n\nTITLE: Default Request Headers Configuration in Python\nDESCRIPTION: Default HTTP headers configuration used by Scrapy for making requests, including Accept and Accept-Language headers.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\": \"en\",\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Built-in FTP Storage Backend in Scrapy Python\nDESCRIPTION: Example of how to disable the built-in FTP storage backend in Scrapy by setting its URI scheme to None in the FEED_STORAGES setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nFEED_STORAGES = {\n    \"ftp\": None,\n}\n```\n\n----------------------------------------\n\nTITLE: Spider Configuration with Multiple Middlewares in Python\nDESCRIPTION: Example of a spider class implementing multiple middleware components including link extraction, callback rules, URL canonicalization, and item ID setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MySpider(BaseSpider):\n    middlewares = [\n        RegexLinkExtractor(),\n        CallbackRules(),\n        CanonicalizeUrl(),\n        ItemIdSetter(),\n        OffsiteMiddleware(),\n    ]\n\n    allowed_domains = [\"example.com\", \"sub.example.com\"]\n\n    url_regexes_to_follow = [\"/product.php?.*\"]\n\n    callback_rules = {\n        \"/product.php.*\": \"parse_product\",\n        \"/category.php.*\": \"parse_category\",\n    }\n\n    canonicalization_rules = [\"sort-query-args\", \"normalize-percent-encoding\", ...]\n\n    id_field = \"guid\"\n    id_fields_to_hash = [\"supplier_name\", \"supplier_id\"]\n\n    def parse_product(self, item):\n        # extract item from response\n        return item\n\n    def parse_category(self, item):\n        # extract item from response\n        return item\n```\n\n----------------------------------------\n\nTITLE: Implementing Callback Rules Dispatcher in Scrapy Spider Middleware\nDESCRIPTION: This code defines a CallbackRules class that dispatches requests to different callback methods based on URL patterns. It uses spider signals to manage rules for each spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass CallbackRules(object):\n    def __init__(self):\n        self.rules = {}\n        dispatcher.connect(signals.spider_opened, self.spider_opened)\n        dispatcher.connect(signals.spider_closed, self.spider_closed)\n\n    def spider_opened(self, spider):\n        self.rules[spider] = {}\n        for regex, method_name in spider.callback_rules.items():\n            r = re.compile(regex)\n            m = getattr(self.spider, method_name, None)\n            if m:\n                self.rules[spider][r] = m\n\n    def spider_closed(self, spider):\n        del self.rules[spider]\n\n    def process_response(self, response, request, spider):\n        for regex, method in self.rules[spider].items():\n            m = regex.search(response.url)\n            if m:\n                return method(response)\n        return []\n\n\n# Example spider using this middleware\nclass MySpider(BaseSpider):\n    middlewares = [CallbackRules()]\n    callback_rules = {\n        \"/product.php.*\": \"parse_product\",\n        \"/category.php.*\": \"parse_category\",\n    }\n\n    def parse_product(self, response):\n        # parse response and populate item\n        return item\n```\n\n----------------------------------------\n\nTITLE: Running a Spider with Output Formatting\nDESCRIPTION: Examples of using the crawl command to run a spider with different output options, including appending to a file or overwriting a file with different formats.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy crawl myspider\n[ ... myspider starts crawling ... ]\n\n$ scrapy crawl -o myfile:csv myspider\n[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]\n\n$ scrapy crawl -O myfile:json myspider\n[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Usage Email Notification in Scrapy\nDESCRIPTION: Example of configuring email notification settings for memory usage monitoring in Scrapy. This setting specifies email addresses to notify when memory limits are reached.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n```\n\n----------------------------------------\n\nTITLE: Processing RSS Responses in Scrapy Spider Middleware\nDESCRIPTION: This snippet demonstrates a process_response method that handles RSS XML responses. It extracts item links from the RSS feed and returns new requests for those URLs.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef process_response(self, response, request, spider):\n    if response.headers.get(\"Content-type\") == \"application/rss+xml\":\n        xs = XmlXPathSelector(response)\n        urls = xs.select(\"//item/link/text()\").extract()\n        return [Request(x) for x in urls]\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon S3 ACL for Scrapy Feed Exports\nDESCRIPTION: The new FEED_STORAGE_S3_ACL setting allows defining a custom ACL for feeds exported to Amazon S3.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\nFEED_STORAGE_S3_ACL = 'public-read'\n```\n\n----------------------------------------\n\nTITLE: Passing Run-time Arguments to Adaptors with ItemBuilder in Python\nDESCRIPTION: Shows two approaches for passing arguments to adaptors with the ItemBuilder API: either at method call time or at instantiation time. Both methods achieve the same result.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#!python\nil.add_value(\"width\", x.x('//p[@class=\"width\"]'), default_unit=\"cm\")\n\n# an alternative approach (more efficient)\nil = NewsBuilder(response, default_unit=\"cm\")\nil.add_value(\"width\", x.x('//p[@class=\"width\"]'))\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expressions with Scrapy Selectors in Python\nDESCRIPTION: Demonstrates the use of regular expressions with Scrapy selectors to extract and manipulate text data from HTML elements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_4\n\nLANGUAGE: pycon\nCODE:\n```\n>>> response.css(\"title::text\").re(r\"Quotes.*\")\n['Quotes to Scrape']\n>>> response.css(\"title::text\").re(r\"Q\\w+\")\n['Quotes']\n>>> response.css(\"title::text\").re(r\"(\\w+) to (\\w+)\")\n['Quotes', 'Scrape']\n```\n\n----------------------------------------\n\nTITLE: Checking Dependencies in Scrapy Add-on (Python)\nDESCRIPTION: Example of a Scrapy add-on that checks for a required external library (boto) before configuring itself. It raises a NotConfigured exception if the dependency is not met.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/addons.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyAddon:\n    def update_settings(self, settings):\n        try:\n            import boto\n        except ImportError:\n            raise NotConfigured(\"MyAddon requires the boto library\")\n        ...\n```\n\n----------------------------------------\n\nTITLE: Using Response.cb_kwargs Attribute as a Shortcut\nDESCRIPTION: Example of the new Response.cb_kwargs attribute that serves as a shortcut for accessing Response.request.cb_kwargs, simplifying access to callback keyword arguments.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nResponse.cb_kwargs\n```\n\n----------------------------------------\n\nTITLE: Defining Rule and CompiledRule Classes for Scrapy\nDESCRIPTION: These classes define the structure for crawler rules and their compiled versions. They store attributes such as matchers, callbacks, and follow flags for use in the crawling process.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Rule(object):\n    \"\"\"Crawler Rule\"\"\"\n\n    def __init__(\n        self, matcher, callback=None, cb_args=None, cb_kwargs=None, follow=True\n    ):\n        \"\"\"Store attributes\"\"\"\n        self.matcher = matcher\n        self.callback = callback\n        self.cb_args = cb_args if cb_args else ()\n        self.cb_kwargs = cb_kwargs if cb_kwargs else {}\n        self.follow = follow\n\n\nclass CompiledRule(object):\n    \"\"\"Compiled version of Rule\"\"\"\n\n    def __init__(self, matcher, callback=None, follow=False):\n        \"\"\"Initialize attributes checking type\"\"\"\n        assert isinstance(matcher, BaseMatcher)\n        assert callback is None or callable(callback)\n        assert isinstance(follow, bool)\n\n        self.matcher = matcher\n        self.callback = callback\n        self.follow = follow\n```\n\n----------------------------------------\n\nTITLE: Setting Twisted Thread Pool Size in Scrapy\nDESCRIPTION: Increases the maximum thread pool size for DNS resolution to improve crawling speed at higher concurrency levels.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nREACTOR_THREADPOOL_MAXSIZE = 20\n```\n\n----------------------------------------\n\nTITLE: Disabling Cookies in Scrapy\nDESCRIPTION: Disables cookie handling to improve performance and reduce memory usage when cookies aren't needed.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCOOKIES_ENABLED = False\n```\n\n----------------------------------------\n\nTITLE: Using pathlib.Path for Files and Images Storage in Python\nDESCRIPTION: Example of using pathlib.Path objects to specify storage locations for FILES_STORE and IMAGES_STORE settings.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom pathlib import Path\n\nFILES_STORE = Path('/path/to/files')\nIMAGES_STORE = Path('/path/to/images')\n```\n\n----------------------------------------\n\nTITLE: Headers Received Signal Handler in Python\nDESCRIPTION: Signal handler triggered when response headers are available before downloading content. Allows stopping downloads via StopDownload exception.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef headers_received(headers, body_length, request, spider)\n```\n\n----------------------------------------\n\nTITLE: Using FTP Active Mode for Scrapy Feed Exports\nDESCRIPTION: The new FEED_STORAGE_FTP_ACTIVE setting allows using FTP's active connection mode for feeds exported to FTP servers.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\nFEED_STORAGE_FTP_ACTIVE = True\n```\n\n----------------------------------------\n\nTITLE: Spider Using Response.follow Shortcut\nDESCRIPTION: Modified version of the quotes spider using response.follow helper method for simplified link following.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        \"https://quotes.toscrape.com/page/1/\",\n    ]\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"span small::text\").get(),\n                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n            }\n\n        next_page = response.css(\"li.next a::attr(href)\").get()\n        if next_page is not None:\n            yield response.follow(next_page, callback=self.parse)\n```\n\n----------------------------------------\n\nTITLE: Filtering Duplicate Requests in Scrapy Spider Middleware\nDESCRIPTION: The FilterDuplicates middleware prevents crawling the same URL multiple times. It uses a configurable dupefilter class to keep track of seen requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass FilterDuplicates(object):\n    def __init__(self):\n        clspath = settings.get(\"DUPEFILTER_CLASS\")\n        self.dupefilter = load_object(clspath)()\n        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n\n    def enqueue_request(self, spider, request):\n        seen = self.dupefilter.request_seen(spider, request)\n        if not seen or request.dont_filter:\n            return request\n\n    def spider_opened(self, spider):\n        self.dupefilter.open_spider(spider)\n\n    def spider_closed(self, spider):\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Stats in Scrapy Stats Collector\nDESCRIPTION: This snippet shows how to retrieve all collected stats from the Scrapy Stats Collector. It returns a dictionary containing all stat key-value pairs.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/stats.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> stats.get_stats()\n{'custom_count': 1, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}\n```\n\n----------------------------------------\n\nTITLE: Using Google Cloud Storage for Feed Exports in Scrapy\nDESCRIPTION: Example reference to Google Cloud Storage feed export functionality in Scrapy, mentioned as a new feature. This requires setting up the appropriate storage settings for GCS.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Feed exports support Google Cloud Storage\n# Example usage would be in settings.py:\n# FEEDS = {\n#     'gs://mybucket/items.json': {\n#         'format': 'json',\n#     },\n# }\n```\n\n----------------------------------------\n\nTITLE: Declaring Item Parsers in Scrapy\nDESCRIPTION: Example of how to declare a custom ProductParser by extending XPathItemParser and defining input and output parsers for specific fields. This demonstrates the inheritance-based approach to configuring parsers.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-008.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.contrib.itemparser import XPathItemParser, parsers\n\n\nclass ProductParser(XPathItemParser):\n    name_in = parsers.MapConcat(removetags, filterx)\n    price_in = parsers.MapConcat(...)\n\n    price_out = parsers.TakeFirst()\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Crawling with CrawlSpider in Python\nDESCRIPTION: This snippet demonstrates a basic implementation of a CrawlSpider subclass with rules, request extractors, and request processors. It defines methods for parsing items and pages based on URL patterns.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass SampleSpider(CrawlSpider):\n    rules = [\n        # The dispatcher uses first-match policy\n        Rule(UrlRegexMatch(r\"product\\.html\\?id=\\d+\"), \"parse_item\", follow=False),\n        # by default, if the first param is string is wrapped into UrlRegexMatch\n        Rule(r\".+\", \"parse_page\"),\n    ]\n\n    request_extractors = [\n        # crawl all links looking for products and images\n        SgmlRequestExtractor(),\n    ]\n\n    request_processors = [\n        # canonicalize all requests' urls\n        Canonicalize(),\n    ]\n\n    def parse_item(self, response):\n        # parse and extract items from response\n        pass\n\n    def parse_page(self, response):\n        # extract images on all pages\n        pass\n```\n\n----------------------------------------\n\nTITLE: Setting DOWNLOAD_SLOTS for Per-Domain Download Settings in Python\nDESCRIPTION: Example of using the new DOWNLOAD_SLOTS setting to configure per-domain download settings for delay, concurrency, and randomization.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nDOWNLOAD_SLOTS = {\n    'example.com': {\n        'delay': 1.0,\n        'concurrency': 2,\n        'randomize_delay': False\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing from_crawler Method for Feed Exporters in Python\nDESCRIPTION: Support for from_crawler method has been added to feed exporters and storages, allowing access to Scrapy settings and other crawler components.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\nclass MyFeedExporter:\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        # Use settings or crawler here\n        return cls()\n```\n\n----------------------------------------\n\nTITLE: Implementing Spider Contract for Product Scraping in Python\nDESCRIPTION: This snippet demonstrates how to define a spider contract for a product scraping callback. It specifies the URL to test and the fields expected to be scraped.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-017.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ProductSpider(BaseSpider):\n    def parse_product(self, response):\n        \"\"\"\n        @url http://www.example.com/store/product.php?id=123\n        @scrapes name, price, description\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Ignored Tags for Meta Refresh in Scrapy\nDESCRIPTION: The new METAREFRESH_IGNORE_TAGS setting allows overriding which HTML tags are ignored when searching a response for HTML meta tags that trigger a redirect.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\nMETAREFRESH_IGNORE_TAGS = ['script', 'noscript']\n```\n\n----------------------------------------\n\nTITLE: Creating a New Scrapy Project\nDESCRIPTION: Command example demonstrating how to create a new Scrapy project using the startproject command.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy startproject myproject\n```\n\n----------------------------------------\n\nTITLE: Disabling FTP Download Handler in Scrapy\nDESCRIPTION: This code snippet demonstrates how to disable the built-in FTP download handler in Scrapy by setting its value to None in the DOWNLOAD_HANDLERS setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nDOWNLOAD_HANDLERS = {\n    \"ftp\": None,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Scrapy Project Settings in scrapy.cfg\nDESCRIPTION: Example of a basic scrapy.cfg configuration file that defines the project settings module. This file is typically placed in the project root directory.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[settings]\ndefault = myproject.settings\n```\n\n----------------------------------------\n\nTITLE: Using Different Adaptors per Spider/Site with ItemForm in Python\nDESCRIPTION: Shows how to create a site-specific news form by extending the base NewsForm class and overriding the published adaptor to use a specific date format.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#!python\nclass SiteNewsFrom(NewsForm):\n    published = adaptor(HtmlNewsForm.published, to_date(\"%d.%m.%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Retrieving Stat Value in Scrapy Stats Collector\nDESCRIPTION: This code demonstrates how to retrieve a specific stat value from the Scrapy Stats Collector. It retrieves the value of 'custom_count'.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/stats.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> stats.get_value(\"custom_count\")\n1\n```\n\n----------------------------------------\n\nTITLE: Python Abstract Class Definition: MediaPipeline\nDESCRIPTION: MediaPipeline class marked as abstract with abstract methods that must be overridden in subclasses\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MediaPipeline(abc.ABC)\n```\n\n----------------------------------------\n\nTITLE: Parsing XML with Namespaces using xmliter in Python\nDESCRIPTION: The scrapy.utils.iterators.xmliter function now supports parsing XML with namespaced node names, allowing for more flexible XML processing.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.utils.iterators import xmliter\n\n# Example usage with namespaces\nfor elem in xmliter(response, 'ns:nodename'):\n    # Process elem\n```\n\n----------------------------------------\n\nTITLE: Logging in Spider Classes (Python)\nDESCRIPTION: Shows how to use the new logger attribute in spider classes for logging events specific to that spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_68\n\nLANGUAGE: Python\nCODE:\n```\nclass MySpider(scrapy.Spider):\n    def parse(self, response):\n        self.logger.info('Response received')\n```\n\n----------------------------------------\n\nTITLE: Response Downloaded Signal Handler in Python\nDESCRIPTION: Signal handler triggered immediately after a HTTPResponse is downloaded. Takes response, request and spider objects as parameters.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef response_downloaded(response, request, spider)\n```\n\n----------------------------------------\n\nTITLE: Defining Default Spider Contracts in Python\nDESCRIPTION: This snippet shows the default configuration for Scrapy spider contracts. It defines a dictionary with contract classes and their associated priority values.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"scrapy.contracts.default.UrlContract\": 1,\n    \"scrapy.contracts.default.ReturnsContract\": 2,\n    \"scrapy.contracts.default.ScrapesContract\": 3,\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Scrapy Shell with URL\nDESCRIPTION: Command to launch the Scrapy shell with a specific URL to scrape.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nscrapy shell <url>\n```\n\n----------------------------------------\n\nTITLE: Implementing RSS2 Link Extractor in Python\nDESCRIPTION: A LegSpider that extracts links from RSS2 feeds using XPath selectors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-016.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Rss2LinkExtractor(LegSpider):\n    def process_response(self, response):\n        if response.headers.get(\"Content-type\") == \"application/rss+xml\":\n            xs = XmlXPathSelector(response)\n            urls = xs.select(\"//item/link/text()\").extract()\n            return [Request(x) for x in urls]\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Spider Middlewares in Scrapy\nDESCRIPTION: This snippet shows the default configuration for Scrapy spider middlewares. It defines a dictionary with middleware classes and their associated priority values.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n    \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n    \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n    \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 900,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Blank Request in Scrapy\nDESCRIPTION: Example showing how to create a blank request using a data URI scheme. This approach allows making requests without specific content using the data URL protocol.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/faq.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy import Request\n\n\nblank_request = Request(\"data:,\")\n```\n\n----------------------------------------\n\nTITLE: Defining Adaptors with ItemForm in Python\nDESCRIPTION: Shows how to create a NewsForm class that inherits from ItemForm and defines adaptors for url and headline fields. The adaptors apply a series of processing functions to the extracted data.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!python\nclass NewsForm(ItemForm):\n    item_class = NewsItem\n\n    url = adaptor(extract, remove_tags(), unquote(), strip)\n    headline = adaptor(extract, remove_tags(), unquote(), strip)\n```\n\n----------------------------------------\n\nTITLE: Implementing Scrapy 2.6 Compatible Request Fingerprinting in Python\nDESCRIPTION: A RequestFingerprinter class that reproduces the fingerprinting algorithm used in Scrapy 2.6, using SHA1 hashing and request caching.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom hashlib import sha1\nfrom weakref import WeakKeyDictionary\n\nfrom scrapy.utils.python import to_bytes\nfrom w3lib.url import canonicalize_url\n\n\nclass RequestFingerprinter:\n    cache = WeakKeyDictionary()\n\n    def fingerprint(self, request):\n        if request not in self.cache:\n            fp = sha1()\n            fp.update(to_bytes(request.method))\n            fp.update(to_bytes(canonicalize_url(request.url)))\n            fp.update(request.body or b\"\")\n            self.cache[request] = fp.digest()\n        return self.cache[request]\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Stat Value in Scrapy Stats Collector\nDESCRIPTION: This code shows how to set a stat value only if it's greater than the previous value. It's used for tracking maximum values like 'max_items_scraped'.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/stats.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstats.max_value(\"max_items_scraped\", value)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Spider Contracts in Scrapy Settings\nDESCRIPTION: This code shows how to add custom spider contracts to the Scrapy project settings. It specifies the contract classes and their priority.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/contracts.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSPIDER_CONTRACTS = {\n    \"myproject.contracts.ResponseCheck\": 10,\n    \"myproject.contracts.ItemValidate\": 10,\n}\n```\n\n----------------------------------------\n\nTITLE: Scrapy Shell Command Examples\nDESCRIPTION: Examples of using Scrapy shell command with different options including redirect handling\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy shell http://www.example.com/some/page.html\n$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'\n$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n```\n\n----------------------------------------\n\nTITLE: Disabling Redirects in Scrapy\nDESCRIPTION: Disables follow-up of redirects to maintain consistent request numbers per crawl batch and prevent redirect loops.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nREDIRECT_ENABLED = False\n```\n\n----------------------------------------\n\nTITLE: Using CrawlerRunner with Custom Reactor in Python\nDESCRIPTION: This example demonstrates using CrawlerRunner with a non-default reactor (AsyncioSelectorReactor). It shows how to explicitly install a reactor when not using CrawlerProcess, which would otherwise handle this automatically.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/practices.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\n\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\n\nconfigure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\"})\n\nfrom scrapy.utils.reactor import install_reactor\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\nrunner = CrawlerRunner()\nd = runner.crawl(MySpider)\n\nfrom twisted.internet import reactor\n\nd.addBoth(lambda _: reactor.stop())\nreactor.run()  # the script will block here until the crawling is finished\n```\n\n----------------------------------------\n\nTITLE: Implementing update_settings Method in Scrapy Add-on (Python)\nDESCRIPTION: Example of an update_settings method in a Scrapy add-on class. It shows how to set a setting with the 'addon' priority, allowing users to override it in project or spider configuration.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/addons.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyAddon:\n    def update_settings(self, settings):\n        settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n```\n\n----------------------------------------\n\nTITLE: Accessing SSL Certificate with Response.certificate Attribute\nDESCRIPTION: Using the new Response.certificate attribute to access the SSL certificate of the server as a twisted.internet.ssl.Certificate object for HTTPS responses.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nResponse.certificate\n```\n\n----------------------------------------\n\nTITLE: Customizing Logger Level in Scrapy Spider\nDESCRIPTION: Shows how to customize the logging level for a specific logger within a Scrapy spider's __init__ method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    # ...\n    def __init__(self, *args, **kwargs):\n        logger = logging.getLogger(\"scrapy.spidermiddlewares.httperror\")\n        logger.setLevel(logging.WARNING)\n        super().__init__(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Implementing BulkItemLoader Class in Python\nDESCRIPTION: Defines the BulkItemLoader class that extends XPathItemLoader. It includes methods for parsing definition lists and extracting key-value pairs automatically.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-020.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.contrib.loader import XPathItemLoader\nfrom scrapy.contrib.loader.processor import MapCompose\n\n\nclass BulkItemLoader(XPathItemLoader):\n    \"\"\"Item loader based on specified pattern recognition\"\"\"\n\n    default_item_class = dict\n    base_xpath = \"//body\"\n    ignore = ()\n\n    def _get_label(self, entity):\n        \"\"\"Pull the text label out of selected markup\n\n        :param entity: Found markup\n        :type entity: Selector\n        \"\"\"\n        label = \" \".join(entity.xpath(\".//text()\").extract())\n        label = label.encode(\"ascii\", \"xmlcharrefreplace\") if label else \"\"\n        label = label.strip(\"&#160;\") if \"&#160;\" in label else label\n        label = label.strip(\":\") if \":\" in label else label\n        label = label.strip()\n        return label\n\n    def _get_entities(self, xpath):\n        \"\"\"Retrieve the list of selectors for a given sub-pattern\n\n        :param xpath: The xpath to select\n        :type xpath: String\n        :return: The list of selectors\n        :rtype: list\n        \"\"\"\n        return self.selector.xpath(self.base_xpath + xpath)\n\n    def parse_dl(self, xpath=\"//dl\"):\n        \"\"\"Look for the specified definition list pattern and store all found\n        values for the enclosed terms and descriptions.\n\n        :param xpath: The xpath to select\n        :type xpath: String\n        \"\"\"\n        for term in self._get_entities(xpath + \"/dt\"):\n            label = self._get_label(term)\n            if label and label not in self.ignore:\n                value = term.xpath(\"following-sibling::dd[1]//text()\")\n                if value:\n                    self.add_value(\n                        label, value.extract(), MapCompose(lambda v: v.strip())\n                    )\n```\n\n----------------------------------------\n\nTITLE: Defining List Fields in Scrapy Items\nDESCRIPTION: Example showing how to define items with different types of list fields including TextField, DateField, and IntegerField. Demonstrates basic list field definition patterns.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-002.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.item.models import Item\nfrom scrapy.item.fields import ListField, TextField, DateField, IntegerField\n\nclass Article(Item):\n    categories = ListField(TextField)\n    dates = ListField(DateField, default=[])\n    numbers = ListField(IntegerField, [])\n```\n\n----------------------------------------\n\nTITLE: Request Priority Depth Adjustment in Python\nDESCRIPTION: Code example showing how request priority is adjusted based on crawl depth using the DEPTH_PRIORITY setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrequest.priority = request.priority - (depth * DEPTH_PRIORITY)\n```\n\n----------------------------------------\n\nTITLE: Enforcing asyncio Requirement in Scrapy Components\nDESCRIPTION: Demonstrates how to enforce asyncio as a requirement in a Scrapy component by checking if the asyncio reactor is installed and raising an error if it's not.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/asyncio.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed\n\n\nclass MyComponent:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise ValueError(\n                f\"{MyComponent.__qualname__} requires the asyncio Twisted \"\n                f\"reactor. Make sure you have it configured in the \"\n                f\"TWISTED_REACTOR setting. See the asyncio documentation \"\n                f\"of Scrapy for more information.\"\n            )\n```\n\n----------------------------------------\n\nTITLE: Resuming a Paused Scrapy Spider\nDESCRIPTION: This command shows how to resume a previously paused Scrapy spider by using the same job directory.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/jobs.rst#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nscrapy crawl somespider -s JOBDIR=crawls/somespider-1\n```\n\n----------------------------------------\n\nTITLE: Initializing MailSender Example\nDESCRIPTION: Example showing new MailSender feature that accepts single strings as 'to' and 'cc' arguments\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nmail_sender.send(to='user@example.com', cc='other@example.com')\n```\n\n----------------------------------------\n\nTITLE: Running Spiders with New Crawler API (Python)\nDESCRIPTION: Demonstrates how to run a spider manually using the new Crawler API introduced in Scrapy 1.0. This example sets up a CrawlerProcess with custom settings and runs a spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_69\n\nLANGUAGE: Python\nCODE:\n```\nfrom scrapy.crawler import CrawlerProcess\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\nprocess.crawl(MySpider)\nprocess.start()\n```\n\n----------------------------------------\n\nTITLE: Debugging Memory Leaks with trackref Utilities\nDESCRIPTION: Example showing how to use trackref utilities to inspect object references and find memory leaks.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/leaks.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from scrapy.utils.trackref import get_oldest\n>>> r = get_oldest(\"HtmlResponse\")\n>>> r.url\n'http://www.somenastyspider.com/product.php?pid=123'\n\n>>> from scrapy.utils.trackref import iter_all\n>>> [r.url for r in iter_all(\"HtmlResponse\")]\n['http://www.somenastyspider.com/product.php?pid=123',\n'http://www.somenastyspider.com/product.php?pid=584',\n...]\n```\n\n----------------------------------------\n\nTITLE: Initializing Scrapy Selector with HTML Content\nDESCRIPTION: Creates a Scrapy Selector object with sample HTML content containing nested lists. This setup is used for subsequent XPath examples.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_19\n\nLANGUAGE: pycon\nCODE:\n```\n>>> from scrapy import Selector\n>>> sel = Selector(\n...     text=\"\"\"\n...     <ul class=\"list\">\n...         <li>1</li>\n...         <li>2</li>\n...         <li>3</li>\n...     </ul>\n...     <ul class=\"list\">\n...         <li>4</li>\n...         <li>5</li>\n...         <li>6</li>\n...     </ul>\"\"\"\n... )\n>>> xp = lambda x: sel.xpath(x).getall()\n```\n\n----------------------------------------\n\nTITLE: Logging with Specific Level in Python\nDESCRIPTION: Shows how to log a message using a specific log level by calling the log() function with the desired level as an argument.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.log(logging.WARNING, \"This is a warning\")\n```\n\n----------------------------------------\n\nTITLE: Default Retry Exception Configuration in Scrapy\nDESCRIPTION: List of default exception types that trigger retry behavior in Scrapy's RetryMiddleware.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n[\n    'twisted.internet.defer.TimeoutError',\n    'twisted.internet.error.TimeoutError',\n    'twisted.internet.error.DNSLookupError',\n    'twisted.internet.error.ConnectionRefusedError',\n    'twisted.internet.error.ConnectionDone',\n    'twisted.internet.error.ConnectError',\n    'twisted.internet.error.ConnectionLost',\n    'twisted.internet.error.TCPTimedOutError',\n    'twisted.web.client.ResponseFailed',\n    IOError,\n    'scrapy.core.downloader.handlers.http11.TunnelError',\n]\n```\n\n----------------------------------------\n\nTITLE: Declaring Parsers in Item Fields\nDESCRIPTION: Example showing how to define parser configurations directly in Item Fields. This approach allows setting input and output parsers at the field level within the Item class definition.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-008.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Product(Item):\n    name = Field(output_parser=parsers.Join(), ...)\n    price = Field(output_parser=parsers.TakeFirst(), ...)\n\n    description = Field(input_parser=parsers.MapConcat(removetags))\n```\n\n----------------------------------------\n\nTITLE: Initializing SGML Request Extractor in Scrapy\nDESCRIPTION: This class initializes an SGML Request Extractor with custom tag and attribute function checkers. It sets default values for tags and attributes if not provided.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SgmlRequestExtractor(BaseSgmlRequestExtractor):\n    \"\"\"SGML Request Extractor\"\"\"\n\n    def __init__(self, tags=None, attrs=None):\n        \"\"\"Initialize with custom tag & attribute function checkers\"\"\"\n        # defaults\n        tags = tuple(tags) if tags else (\"a\", \"area\")\n        attrs = tuple(attrs) if attrs else (\"href\",)\n\n        tag_func = lambda x: x in tags\n        attr_func = lambda x: x in attrs\n        BaseSgmlRequestExtractor.__init__(self, tag=tag_func, attr=attr_func)\n```\n\n----------------------------------------\n\nTITLE: Controlling Crawling Depth in Scrapy Spider Middleware\nDESCRIPTION: The SetLimitDepth middleware sets and limits the crawling depth. It tracks the depth of each request and can be configured to stop crawling beyond a certain depth.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass SetLimitDepth(object):\n    def __init__(self, maxdepth=0):\n        self.maxdepth = maxdepth or settings.getint(\"DEPTH_LIMIT\")\n\n    def process_request(self, request, response, spider):\n        depth = response.request.meta[\"depth\"] + 1\n        request.meta[\"depth\"] = depth\n        if not self.maxdepth or depth <= self.maxdepth:\n            return request\n        spider.log(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request))\n\n    def process_start_request(self, request, spider):\n        request.meta[\"depth\"] = 0\n        return request\n```\n\n----------------------------------------\n\nTITLE: Implementing XPath-based Request Extraction in Scrapy\nDESCRIPTION: This class extends the SGML Request Extractor to include XPath restrictions. It initializes XPath restrictions and overrides the extract_requests method to apply these restrictions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass XPathRequestExtractor(SgmlRequestExtractor):\n    \"\"\"SGML Request Extractor with XPath restriction\"\"\"\n\n    def __init__(self, restrict_xpaths, tags=None, attrs=None):\n        \"\"\"Initialize XPath restrictions\"\"\"\n        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n        SgmlRequestExtractor.__init__(self, tags, attrs)\n\n    def extract_requests(self, response):\n        \"\"\"Restrict to XPath regions\"\"\"\n        hxs = HtmlXPathSelector(response)\n        fragments = (\n            \"\".join(html_frag for html_frag in hxs.select(xpath).extract())\n            for xpath in self.restrict_xpaths\n        )\n        html_slice = \"\".join(html_frag for html_frag in fragments)\n        return self._extract_requests(html_slice, response.url, response.encoding)\n```\n\n----------------------------------------\n\nTITLE: S3 ACL Setting Example - Python\nDESCRIPTION: Example of configuring S3 ACL permissions for file uploads\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nFILES_STORE_S3_ACL = 'private' # Configure S3 ACL policy\n```\n\n----------------------------------------\n\nTITLE: Checking Extracted Value with ItemBuilder in Python\nDESCRIPTION: Demonstrates how to check if a value was successfully extracted using the ItemBuilder API, with a fallback extraction pattern if the first attempt fails.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#!python\nil = NewsBuilder(response)\nil.add_value(\"headline\", x.x('//h1[@class=\"headline\"]'))\nif not nf.get_value(\"headline\"):\n    il.add_value(\"headline\", x.x('//h1[@class=\"title\"]'))\n```\n\n----------------------------------------\n\nTITLE: Scrapy Cookie Debug Log Example\nDESCRIPTION: Sample log output when COOKIES_DEBUG setting is enabled, showing cookie headers being sent and received during crawling.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened\n2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>\n        Cookie: clientlanguage_nl=en_EN\n2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>\n        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/\n        Set-Cookie: ip_isocode=US\n        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/\n2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)\n[...]\n```\n\n----------------------------------------\n\nTITLE: Defining a TestItem in Scrapy\nDESCRIPTION: This code defines a TestItem class which is a Scrapy Item used to structure scraped data with specific fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/spiders.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass TestItem(scrapy.Item):\n    id = scrapy.Field()\n    name = scrapy.Field()\n    description = scrapy.Field()\n```\n\n----------------------------------------\n\nTITLE: Enforcing Minimum Scrapy Version Requirement\nDESCRIPTION: Example demonstrating how to enforce a minimum Scrapy version for a component. The component checks the Scrapy version during initialization and raises an exception if the version requirement is not met.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/components.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom packaging.version import parse as parse_version\n\nimport scrapy\n\n\nclass MyComponent:\n    def __init__(self):\n        if parse_version(scrapy.__version__) < parse_version(\"2.7\"):\n            raise RuntimeError(\n                f\"{MyComponent.__qualname__} requires Scrapy 2.7 or \"\n                f\"later, which allow defining the process_spider_output \"\n                f\"method of spider middlewares as an asynchronous \"\n                f\"generator.\"\n            )\n```\n\n----------------------------------------\n\nTITLE: Starting a Scrapy Spider with Persistence Support\nDESCRIPTION: This command demonstrates how to start a Scrapy spider with persistence support enabled by specifying a job directory.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/jobs.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nscrapy crawl somespider -s JOBDIR=crawls/somespider-1\n```\n\n----------------------------------------\n\nTITLE: URL Length Limiting in Scrapy Spider Middleware\nDESCRIPTION: This LimitUrlLength middleware filters out requests with URLs exceeding a specified maximum length. It uses a setting to determine the maximum allowed URL length.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass LimitUrlLength(object):\n    def __init__(self):\n        self.maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n\n    def process_request(self, request, response, spider):\n        return self.process_start_request(self, request)\n\n    def process_start_request(self, request, spider):\n        if len(request.url) <= self.maxlength:\n            return request\n        spider.log(\n            \"Ignoring request (url length > %d): %s \" % (self.maxlength, request.url)\n        )\n```\n\n----------------------------------------\n\nTITLE: Viewing Scrapy Engine Status\nDESCRIPTION: Example demonstrating how to use the est() method to view the Scrapy engine's current state through the telnet console.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/telnetconsole.rst#2025-04-17_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle()                         : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n```\n\n----------------------------------------\n\nTITLE: Using Python Logging in Scrapy (Python)\nDESCRIPTION: Demonstrates the switch from Twisted logging to Python's built-in logging module in Scrapy 1.0. Shows both the old and new ways of logging messages.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\n# Old version\nfrom scrapy import log\nlog.msg('MESSAGE', log.INFO)\n\n# New version\nimport logging\nlogging.info('MESSAGE')\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseSgmlRequestExtractor in Python for Scrapy\nDESCRIPTION: This code defines a base SGML request extractor class for Scrapy. It parses HTML/XML responses, extracts links, and generates Request objects with absolute URLs and proper encoding.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BaseSgmlRequestExtractor(FixedSGMLParser):\n    \"\"\"Base SGML Request Extractor\"\"\"\n\n    def __init__(self, tag=\"a\", attr=\"href\"):\n        \"\"\"Initialize attributes\"\"\"\n        FixedSGMLParser.__init__(self)\n\n        self.scan_tag = tag if callable(tag) else lambda t: t = tag\n        self.scan_attr = attr if callable(attr) else lambda a: a = attr\n        self.current_request = None\n\n    def extract_requests(self, response):\n        \"\"\"Returns list of requests extracted from response\"\"\"\n        return self._extract_requests(response.body, response.url, response.encoding)\n\n    def _extract_requests(self, response_text, response_url, response_encoding):\n        \"\"\"Extract requests with absolute urls\"\"\"\n        self.reset()\n        self.feed(response_text)\n        self.close()\n\n        base_url = self.base_url if self.base_url else response_url\n        self._make_absolute_urls(base_url, response_encoding)\n        self._fix_link_text_encoding(response_encoding)\n\n        return self.requests\n\n    def _make_absolute_urls(self, base_url, encoding):\n        \"\"\"Makes all request's urls absolute\"\"\"\n        for req in self.requests:\n            url = req.url\n            # make absolute url\n            url = urljoin_rfc(base_url, url, encoding)\n            url = safe_url_string(url, encoding)\n            # replace in-place request's url\n            req.url = url\n\n    def _fix_link_text_encoding(self, encoding):\n        \"\"\"Convert link_text to unicode for each request\"\"\"\n        for req in self.requests:\n            req.meta.setdefault(\"link_text\", \"\")\n            req.meta[\"link_text\"] = str_to_unicode(req.meta[\"link_text\"], encoding)\n\n    def reset(self):\n        \"\"\"Reset state\"\"\"\n        FixedSGMLParser.reset(self)\n        self.requests = []\n        self.base_url = None\n\n    def unknown_starttag(self, tag, attrs):\n        \"\"\"Process unknown start tag\"\"\"\n        if \"base\" == tag:\n            self.base_url = dict(attrs).get(\"href\")\n\n        if self.scan_tag(tag):\n            for attr, value in attrs:\n                if self.scan_attr(attr):\n                    if value is not None:\n                        req = Request(url=value)\n                        self.requests.append(req)\n                        self.current_request = req\n\n    def unknown_endtag(self, tag):\n        \"\"\"Process unknown end tag\"\"\"\n        self.current_request = None\n```\n\n----------------------------------------\n\nTITLE: Editing a Spider with Default Editor\nDESCRIPTION: Example of using the edit command to open a spider in the editor defined by the EDITOR environment variable or setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy edit spider1\n```\n\n----------------------------------------\n\nTITLE: Navigating Web Page and Extracting Title with Playwright\nDESCRIPTION: Demonstrates how to create a new browser page, navigate to a URL, and extract the page title using Playwright. The code creates a new page instance, navigates to 'example.org', retrieves the page title, and returns it in a dictionary.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/dynamic-content.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npage = await browser.new_page()\nawait page.goto(\"https://example.org\")\ntitle = await page.title()\nreturn {\"title\": title}\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from XPathSelectors in Python\nDESCRIPTION: The extract function attempts to extract data from given locations. It processes XPathSelectors and includes other data as-is in the result. This functionality is now included in XpathLoader and is considered obsolete.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-007.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# extract function (obsolete)\n# No code provided in the original text\n```\n\n----------------------------------------\n\nTITLE: Specifying Spider Modules in Scrapy Configuration\nDESCRIPTION: This example shows how to configure the SPIDER_MODULES setting in Scrapy, which specifies where Scrapy should look for spider definitions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nSPIDER_MODULES = [\"mybot.spiders_prod\", \"mybot.spiders_dev\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Scheduler Priority Queue in Scrapy\nDESCRIPTION: Configures the scheduler priority queue for optimal performance in multi-domain crawling scenarios.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSCHEDULER_PRIORITY_QUEUE = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n```\n\n----------------------------------------\n\nTITLE: Passing Same-Named Arguments to Different Adaptors with ItemForm in Python\nDESCRIPTION: Shows how to handle cases where different fields need different values for the same argument name using the ItemForm API, either by subclassing or by passing parameters at instantiation time.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#!python\nclass MySiteForm(ItemForm):\n    width = adaptor(ItemForm.width, default_unit=\"cm\")\n    volume = adaptor(ItemForm.width, default_unit=\"lt\")\n\n\nia[\"width\"] = x.x('//p[@class=\"width\"]')\nia[\"volume\"] = x.x('//p[@class=\"volume\"]')\n\n# another example passing parameters on instance\nia = NewsForm(response, encoding=\"utf-8\")\nia[\"name\"] = x.x('//p[@class=\"name\"]')\n```\n\n----------------------------------------\n\nTITLE: Accessing Settings in a Scrapy Component\nDESCRIPTION: Example showing how to create a Scrapy extension that accesses settings from the crawler object. The component reads a boolean setting (LOG_ENABLED) during initialization and performs an action based on its value.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/components.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyExtension:\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        return cls(settings.getbool(\"LOG_ENABLED\"))\n\n    def __init__(self, log_is_enabled=False):\n        if log_is_enabled:\n            print(\"log is enabled!\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Scrapy Telnet Console\nDESCRIPTION: Example showing how to connect to Scrapy's telnet console on the default port 6023 with authentication.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/telnetconsole.rst#2025-04-17_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntelnet localhost 6023\nTrying localhost...\nConnected to localhost.\nEscape character is '^]'.\nUsername:\nPassword:\n>>>\n```\n\n----------------------------------------\n\nTITLE: Implementing URL Canonicalizer in Python\nDESCRIPTION: A LegSpider that canonicalizes URLs based on configurable rules.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-016.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CanonicalizeUrl(LegSpider):\n    def process_request(self, request):\n        curl = canonicalize_url(request.url, rules=self.spider.canonicalization_rules)\n        return request.replace(url=curl)\n\n\nclass MySpider(LegSpider):\n    legs = [CanonicalizeUrl()]\n    canonicalization_rules = [\"sort-query-args\", \"normalize-percent-encoding\", ...]\n\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Processing HTML Data in Scrapy Requests\nDESCRIPTION: This method handles processing of data within a Scrapy request. It adds the stripped data as 'link_text' to the current request's metadata if it doesn't already exist.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef handle_data(self, data):\n    \"\"\"Process data\"\"\"\n    current = self.current_request\n    if current and not \"link_text\" in current.meta:\n        current.meta[\"link_text\"] = data.strip()\n```\n\n----------------------------------------\n\nTITLE: Scheduler Queue Configuration - Python\nDESCRIPTION: Example of customizing the scheduler priority queue implementation\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nSCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n```\n\n----------------------------------------\n\nTITLE: Enabling HTTP/2 Support in Scrapy\nDESCRIPTION: This code snippet shows how to enable HTTP/2 support in Scrapy by updating the DOWNLOAD_HANDLERS setting to use the H2DownloadHandler for HTTPS requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nDOWNLOAD_HANDLERS = {\n    \"https\": \"scrapy.core.downloader.handlers.http2.H2DownloadHandler\",\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring New Spider Module Location in Scrapy\nDESCRIPTION: Example showing how to specify the module location where new spiders will be created when using the genspider command.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n```\n\n----------------------------------------\n\nTITLE: Sample Spider Callback with Yield\nDESCRIPTION: Updated spider callback code pattern that uses yield statements\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndef parse(self, response):\n    yield item\n```\n\n----------------------------------------\n\nTITLE: Disabling Retries in Scrapy\nDESCRIPTION: Turns off request retrying to prevent capacity wastage on slow or failing domains.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nRETRY_ENABLED = False\n```\n\n----------------------------------------\n\nTITLE: Implementing Regex HTML Link Extractor in Python\nDESCRIPTION: A LegSpider implementation that extracts links from HTML responses based on regex patterns.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-016.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass RegexHtmlLinkExtractor(LegSpider):\n    def process_response(self, response):\n        if isinstance(response, HtmlResponse):\n            allowed_regexes = self.spider.url_regexes_to_follow\n            # extract urls to follow using allowed_regexes\n            return [Request(x) for x in urls_to_follow]\n\n\nclass MySpider(LegSpider):\n    legs = [RegexHtmlLinkExtractor()]\n    url_regexes_to_follow = [\"/product.php?.*\"]\n\n    def parse_response(self, response):\n        # parse response and extract items\n        return items\n```\n\n----------------------------------------\n\nTITLE: Implementing Universal Spider Middleware in Python\nDESCRIPTION: This code snippet demonstrates how to create a universal spider middleware that supports both synchronous and asynchronous output processing in Scrapy. It defines two methods: a synchronous 'process_spider_output' and an asynchronous 'process_spider_output_async'.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/coroutines.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass UniversalSpiderMiddleware:\n    def process_spider_output(self, response, result, spider):\n        for r in result:\n            # ... do something with r\n            yield r\n\n    async def process_spider_output_async(self, response, result, spider):\n        async for r in result:\n            # ... do something with r\n            yield r\n```\n\n----------------------------------------\n\nTITLE: Implementing Combined LegSpider in Python\nDESCRIPTION: Example showing how to combine multiple LegSpider components into a single spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-016.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MySpider(LegSpider):\n    legs = [RegexLinkExtractor(), ParseRules(), CanonicalizeUrl(), ItemIdSetter()]\n\n    url_regexes_to_follow = [\"/product.php?.*\"]\n\n    parse_rules = {\n        \"/product.php.*\": \"parse_product\",\n        \"/category.php.*\": \"parse_category\",\n    }\n\n    canonicalization_rules = [\"sort-query-args\", \"normalize-percent-encoding\", ...]\n\n    id_field = \"guid\"\n    id_fields_to_hash = [\"supplier_name\", \"supplier_id\"]\n\n    def process_product(self, item):\n        # extract item from response\n        return item\n\n    def process_category(self, item):\n        # extract item from response\n        return item\n```\n\n----------------------------------------\n\nTITLE: RegexHtmlLinkExtractor Implementation in Python\nDESCRIPTION: Example of a spider middleware that extracts links from HTML responses based on regex patterns defined in the spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass RegexHtmlLinkExtractor(object):\n    def process_response(self, response, request, spider):\n        if isinstance(response, HtmlResponse):\n            allowed_regexes = spider.url_regexes_to_follow\n            # extract urls to follow using allowed_regexes\n            return [Request(x) for x in urls_to_follow]\n\n\n# Example spider using this middleware\nclass MySpider(BaseSpider):\n    middlewares = [RegexHtmlLinkExtractor()]\n    url_regexes_to_follow = [\"/product.php?.*\"]\n\n    # parsing callbacks below\n```\n\n----------------------------------------\n\nTITLE: Implementing Rules Manager for Scrapy Crawling\nDESCRIPTION: This class manages the compilation and resolution of crawling rules. It initializes rules using a spider and default matcher, and provides a method to get the first matching rule for a given response.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass RulesManager(object):\n    \"\"\"Rules Manager\"\"\"\n\n    def __init__(self, rules, spider, default_matcher=UrlRegexMatcher):\n        \"\"\"Initialize rules using spider and default matcher\"\"\"\n        self._rules = tuple()\n\n        # compile absolute/relative-to-spider callbacks\"\"\"\n        for rule in rules:\n            # prepare matcher\n            if isinstance(rule.matcher, BaseMatcher):\n                matcher = rule.matcher\n            else:\n                # matcher not BaseMatcher, check for string\n                if isinstance(rule.matcher, basestring):\n                    # instance default matcher\n                    matcher = default_matcher(rule.matcher)\n                else:\n                    raise ValueError(\n                        \"Not valid matcher given %r in %r\" % (rule.matcher, rule)\n                    )\n\n            # prepare callback\n            if callable(rule.callback):\n                callback = rule.callback\n            elif not rule.callback is None:\n                # callback from spider\n                callback = getattr(spider, rule.callback)\n\n                if not callable(callback):\n                    raise AttributeError(\n                        \"Invalid callback %r can not be resolved\" % callback\n                    )\n            else:\n                callback = None\n\n            if rule.cb_args or rule.cb_kwargs:\n                # build partial callback\n                callback = partial(callback, *rule.cb_args, **rule.cb_kwargs)\n\n            # append compiled rule to rules list\n            crule = CompiledRule(matcher, callback, follow=rule.follow)\n            self._rules += (crule,)\n\n    def get_rule(self, response):\n        \"\"\"Returns first rule that matches response\"\"\"\n        for rule in self._rules:\n            if rule.matcher.matches_response(response):\n                return rule\n```\n\n----------------------------------------\n\nTITLE: Listing Available Spiders\nDESCRIPTION: Example of using the list command to display all available spiders in a project.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy list\nspider1\nspider2\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Length Parser with Loader Context (Python)\nDESCRIPTION: Shows how to create a custom parsing function that accepts a loader context, allowing for flexible unit conversion based on context.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/loaders.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef parse_length(text, loader_context):\n    unit = loader_context.get(\"unit\", \"m\")\n    # ... length parsing code goes here ...\n    return parsed_length\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Commands via setup.py\nDESCRIPTION: Example of registering custom Scrapy commands through setup.py entry points configuration\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"scrapy-mymodule\",\n    entry_points={\n        \"scrapy.commands\": [\n            \"my_command=my_scrapy_module.commands:MyCommand\",\n        ],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting reducers in NewsItemBuilder for Scrapy\nDESCRIPTION: This example shows how to set reducers in a NewsItemBuilder class, using TakeFirst for headline and a custom Reducer for published date.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass NewsItemBuilder(ItemBuilder):\n    item_class = NewsItem\n\n    headline = reducers.TakeFirst(extract, remove_tags(), unquote(), strip)\n    published = reducers.Reducer(extract, remove_tags(), unquote(), strip)\n```\n\n----------------------------------------\n\nTITLE: Setting Download Timeout in Scrapy\nDESCRIPTION: Reduces the download timeout to quickly discard stuck requests and free up crawler capacity.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nDOWNLOAD_TIMEOUT = 15\n```\n\n----------------------------------------\n\nTITLE: Implementing Parsley Extractor Middleware in Python for Scrapy\nDESCRIPTION: This code snippet defines a ParsleyExtractor class for scraping data using Parsley in Scrapy. It creates a dynamic Item class based on the Parsley JSON code and processes responses to extract data.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#!python\nfrom pyparsley import PyParsley\n\n\nclass ParsleyExtractor(object):\n    def __init__(self, parsley_json_code):\n        parsley = json.loads(parselet_json_code)\n\n        class ParsleyItem(Item):\n            def __init__(self, *a, **kw):\n                for name in parsley.keys():\n                    self.fields[name] = Field()\n\n            super(ParsleyItem, self).__init__(*a, **kw)\n\n        self.item_class = ParsleyItem\n        self.parsley = PyParsley(parsley, output=\"python\")\n\n    def process_response(self, response, request, spider):\n        return self.item_class(self.parsley.parse(string=response.body))\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Filter to Specific Logger in Scrapy\nDESCRIPTION: Demonstrates how to apply a custom log filter to a specific logger in a Scrapy spider without affecting other loggers.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    # ...\n    def __init__(self, *args, **kwargs):\n        logger = logging.getLogger(\"my_logger\")\n        logger.addFilter(ContentFilter())\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Retry HTTP Codes in Scrapy\nDESCRIPTION: Default configuration showing the HTTP response codes that trigger automatic retries in Scrapy's RetryMiddleware.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n[500, 502, 503, 504, 522, 524, 408, 429]\n```\n\n----------------------------------------\n\nTITLE: Adding flags Parameter to Response.follow Method\nDESCRIPTION: Using the new flags parameter added to Response.follow method for consistency with the Request class, allowing to set flags when following links.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nResponse.follow(flags=...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Generator for Scrapy\nDESCRIPTION: This class generates new requests from a response using extractors and processors. It applies the specified callback to each generated request.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass RequestGenerator(object):\n    def __init__(self, req_extractors, req_processors, callback):\n        self._request_extractors = req_extractors\n        self._request_processors = req_processors\n        self.callback = callback\n\n    def generate_requests(self, response):\n        \"\"\"\n        Extract and process new requests from response\n        \"\"\"\n        requests = []\n        for ext in self._request_extractors:\n            requests.extend(ext.extract_requests(response))\n\n        for proc in self._request_processors:\n            requests = proc(requests)\n\n        for request in requests:\n            yield request.replace(callback=self.callback)\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy Benchmark Command\nDESCRIPTION: Command to execute Scrapy's built-in benchmarking suite. This spawns a local HTTP server and crawls it at maximum speed to measure performance.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/benchmarking.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nscrapy bench\n```\n\n----------------------------------------\n\nTITLE: Accessing Response in Rule Process Request Callback\nDESCRIPTION: The process_request callback passed to the Rule __init__ method now receives the Response object that originated the request as its second argument.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\ndef process_request(request, response):\n    # Handle request based on response\n    pass\n\nRule(callback=parse_item, process_request=process_request)\n```\n\n----------------------------------------\n\nTITLE: Creating an Item with ItemForm in Python\nDESCRIPTION: Demonstrates how to use the ItemForm API to instantiate a form, set field values, add additional values to fields, and replace existing values. Shows how to obtain the populated item object.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#!python\nia = NewsForm(response)\nia[\"url\"] = response.url\nia[\"headline\"] = x.x('//h1[@class=\"headline\"]')\n\n# if we want to add another value to the same field\nia[\"headline\"] += x.x('//h1[@class=\"headline2\"]')\n\n# if we want to replace the field value other value to the same field\nia[\"headline\"] = x.x('//h1[@class=\"headline3\"]')\n\nreturn ia.get_item()\n```\n\n----------------------------------------\n\nTITLE: Handling Default Values in Scrapy ItemFields\nDESCRIPTION: This code snippet demonstrates how default values are handled in Scrapy ItemFields. It shows examples of accessing fields with and without default values, and how KeyError is raised for undefined fields without defaults.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-003.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\np = Product()\n\np[\"supplier\"]  # returns: Supplier(name='default supplier')\np[\"supplier2\"]  # raises KeyError\np[\"supplier2\"] = Supplier()\np[\"supplier2\"]  # returns: Supplier(name='anonymous supplier')\n\np[\"variants\"]  # raises KeyError\np[\"variants2\"]  # returns []\n\np[\"categories\"]  # raises KeyError\np.get(\"categories\")  # returns None\n\np[\"numbers\"]  # returns []\n```\n\n----------------------------------------\n\nTITLE: Configuring Scrapy Shell in scrapy.cfg\nDESCRIPTION: Example of how to configure the Scrapy shell to use a specific Python interpreter (bpython in this case) in the scrapy.cfg file.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[settings]\nshell = bpython\n```\n\n----------------------------------------\n\nTITLE: Setting Stat Value in Scrapy Stats Collector\nDESCRIPTION: This code snippet shows how to set a stat value in the Scrapy Stats Collector. It sets the 'hostname' stat to the current machine's hostname.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/stats.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstats.set_value(\"hostname\", socket.gethostname())\n```\n\n----------------------------------------\n\nTITLE: Module-specific Logging in Python\nDESCRIPTION: Demonstrates how to create a logger specific to the current module using the __name__ variable, which is a common practice for organizing logs.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.warning(\"This is a warning\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Links by Text in Scrapy LinkExtractor\nDESCRIPTION: A new restrict_text parameter for the LinkExtractor __init__ method allows filtering links by linking text.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\nLinkExtractor(restrict_text='some text')\n```\n\n----------------------------------------\n\nTITLE: Request Left Downloader Signal Handler in Python\nDESCRIPTION: Signal handler triggered when a Request leaves the downloader, including failure cases. Takes request and spider objects as parameters.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef request_left_downloader(request, spider)\n```\n\n----------------------------------------\n\nTITLE: Processing Spider Response Rules and Generating Requests in Python\nDESCRIPTION: Implements response handling logic that checks for matching rules, executes callbacks if defined, and generates follow-up requests when rule.follow is enabled. Uses Scrapy's iterate_spider_output to process callback results and yield request or item objects.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-014.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Dispatch callback and generate requests\"\"\"\n# get rule for response\nrule = self._rulesman.get_rule(response)\nif rule:\n    # dispatch callback if set\n    if rule.callback:\n        output = iterate_spider_output(rule.callback(response))\n        for req_or_item in output:\n            yield req_or_item\n\n    if rule.follow:\n        for req in self._reqgen.generate_requests(response):\n            yield req\n```\n\n----------------------------------------\n\nTITLE: Defining SpiderManager API in Python\nDESCRIPTION: This snippet outlines the proposed API for the SpiderManager class, including methods for getting a Spider instance, finding spiders by request, and listing all spider names.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-015.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nget(spider_name) -> Spider instance\nfind_by_request(request) -> list of spider names\nlist() -> list of spider names\n```\n\n----------------------------------------\n\nTITLE: Setting expanders/reducers with BuilderField in NewsItemBuilder for Scrapy\nDESCRIPTION: This snippet illustrates a new way of setting expanders and reducers using BuilderField and a nested Reducer class in NewsItemBuilder.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass NewsItemBuilder(ItemBuilder):\n    item_class = NewsItem\n\n    headline = BuilderField(extract, remove_tags(), unquote(), strip)\n    content = BuilderField(extract, remove_tags(), unquote(), strip)\n\n    class Reducer:\n        headline = TakeFirst\n```\n\n----------------------------------------\n\nTITLE: Defining Adaptors with ItemBuilder in Python\nDESCRIPTION: Shows how to create a NewsBuilder class that inherits from ItemBuilder and defines adaptors for url and headline fields. Similar to the ItemForm example, it applies processing functions to extracted data.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#!python\nclass NewsBuilder(ItemBuilder):\n    item_class = NewsItem\n\n    url = adaptor(extract, remove_tags(), unquote(), strip)\n    headline = adaptor(extract, remove_tags(), unquote(), strip)\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Filter to Root Logger in Scrapy\nDESCRIPTION: Shows how to apply a custom log filter to the root logger in a Scrapy spider, affecting all loggers in the project.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport scrapy\n\n\nclass MySpider(scrapy.Spider):\n    # ...\n    def __init__(self, *args, **kwargs):\n        for handler in logging.root.handlers:\n            handler.addFilter(ContentFilter())\n```\n\n----------------------------------------\n\nTITLE: Fixing Twisted Reactor Import Issues in Scrapy\nDESCRIPTION: Shows how to resolve issues with pre-installed Twisted reactors by moving imports inside function definitions instead of at the module level.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/asyncio.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom twisted.internet import reactor\n\n\ndef my_function():\n    reactor.callLater(...)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef my_function():\n    from twisted.internet import reactor\n\n    reactor.callLater(...)\n```\n\n----------------------------------------\n\nTITLE: Converting JavaScript to XML with js2xml\nDESCRIPTION: Example of converting JavaScript code to XML format for easier parsing using selectors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/dynamic-content.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport js2xml\nimport lxml.etree\nfrom parsel import Selector\njavascript = response.css(\"script::text\").get()\nxml = lxml.etree.tostring(js2xml.parse(javascript), encoding=\"unicode\")\nselector = Selector(text=xml)\nselector.css('var[name=\"data\"]').get()\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Logger in Python\nDESCRIPTION: Shows how to create and use a custom named logger for more specific logging purposes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger(\"mycustomlogger\")\nlogger.warning(\"This is a warning\")\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy tests with pytest-xdist for parallel execution\nDESCRIPTION: Command to run Scrapy tests using pytest-xdist plugin for parallel execution on all CPU cores.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntox -e py310 -- scrapy tests -n auto\n```\n\n----------------------------------------\n\nTITLE: Checking Response URL in Scrapy Shell\nDESCRIPTION: Shows how to access the current response URL in the Scrapy shell\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> response.url\n'http://example.org'\n```\n\n----------------------------------------\n\nTITLE: URL Canonicalization in Scrapy Spider Middleware\nDESCRIPTION: This snippet shows a CanonicalizeUrl middleware that applies URL canonicalization rules to requests. It uses a spider's canonicalization_rules to modify request URLs.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CanonicalizeUrl(object):\n    def process_request(self, request, response, spider):\n        curl = canonicalize_url(request.url, rules=spider.canonicalization_rules)\n        return request.replace(url=curl)\n\n\n# Example spider using this middleware\nclass MySpider(BaseSpider):\n    middlewares = [CanonicalizeUrl()]\n    canonicalization_rules = [\"sort-query-args\", \"normalize-percent-encoding\", ...]\n\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Adding a Value to a List Field with ItemBuilder in Python\nDESCRIPTION: Shows how to append a value to an existing list field using the ItemBuilder API's add_value method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#!python\nil.add_value(\"headline\", x.x('//h1[@class=\"headline\"]'))\n```\n\n----------------------------------------\n\nTITLE: Basic Scrapy Command Usage\nDESCRIPTION: Examples of basic Scrapy command usage including fetch and headers display\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy fetch --nolog http://www.example.com/some/page.html\n$ scrapy fetch --nolog --headers http://www.example.com/\n```\n\n----------------------------------------\n\nTITLE: Offsite Filtering in Scrapy Spider Middleware\nDESCRIPTION: This OffsiteMiddleware class filters out requests to URLs outside the allowed domains. It uses regular expressions to match hostnames and keeps track of seen hosts.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass SpiderInfo(object):\n    def __init__(self, host_regex):\n        self.host_regex = host_regex\n        self.hosts_seen = set()\n\n\nclass OffsiteMiddleware(object):\n    def __init__(self):\n        self.spiders = {}\n        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n\n    def process_request(self, request, response, spider):\n        return self.process_start_request(self, request)\n\n    def process_start_request(self, request, spider):\n        if self.should_follow(request, spider):\n            return request\n        else:\n            info = self.spiders[spider]\n            host = urlparse_cached(x).hostname\n            if host and host not in info.hosts_seen:\n                spider.log(\"Filtered offsite request to %r: %s\" % (host, request))\n                info.hosts_seen.add(host)\n\n    def should_follow(self, request, spider):\n        info = self.spiders[spider]\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or \"\"\n        return bool(info.regex.search(host))\n\n    def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        domains = [d.replace(\".\", r\"\\.\") for d in spider.allowed_domains]\n        regex = r\"^(.*\\.)?(%s)$\" % \"|\".join(domains)\n        return re.compile(regex)\n\n    def spider_opened(self, spider):\n        info = SpiderInfo(self.get_host_regex(spider))\n        self.spiders[spider] = info\n\n    def spider_closed(self, spider):\n        del self.spiders[spider]\n```\n\n----------------------------------------\n\nTITLE: Passing Run-time Arguments to Adaptors with ItemForm in Python\nDESCRIPTION: Demonstrates how to pass arguments to adaptors at instantiation time with the ItemForm API, which allows setting default values that will be used for all fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#!python\n# Only approach is passing arguments when instantiating the form\nia = NewsForm(response, default_unit=\"cm\")\nia[\"width\"] = x.x('//p[@class=\"width\"]')\n```\n\n----------------------------------------\n\nTITLE: Setting Per-Request Retry Times in Scrapy (Python)\nDESCRIPTION: Shows how to set per-request retry times using the new max_retry_times meta key in Scrapy 1.4. This allows customizing retry behavior for individual requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\nPer-request retry times with the new :reqmeta:`max_retry_times` meta key\n(:issue:`2642`)\n```\n\n----------------------------------------\n\nTITLE: List Field Assignment Operations in Scrapy\nDESCRIPTION: Examples of valid and invalid list field assignments, including type checking and conversion behaviors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-002.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ni = Article()\n\ni[\"categories\"] = []\ni[\"categories\"] = [\"politics\", \"sport\"]\ni[\"categories\"] = [\"test\", 1]  # -> raises TypeError\ni[\"categories\"] = asd  # -> raises TypeError\n\ni[\"dates\"] = []\ni[\"dates\"] = [\"2009-01-01\"]  # raises TypeError? (depends on TextField)\n\ni[\"numbers\"] = [\"1\", 2, \"3\"]\ni[\"numbers\"]  # returns [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Logging a Warning Message in Python\nDESCRIPTION: Demonstrates how to log a warning message using Python's built-in logging module. This is a basic example of logging at the WARNING level.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.warning(\"This is a warning\")\n```\n\n----------------------------------------\n\nTITLE: Running specific Scrapy test file with tox\nDESCRIPTION: Command to run a specific test file (tests/test_loader.py) using tox for Scrapy development.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntox -- tests/test_loader.py\n```\n\n----------------------------------------\n\nTITLE: Feed Slot Closed Signal Handler in Python\nDESCRIPTION: Signal handler that executes when a feed exports slot is closed. Takes a FeedSlot object as parameter and supports returning deferreds.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/signals.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef feed_slot_closed(slot)\n```\n\n----------------------------------------\n\nTITLE: Robots.txt Exclusion in Scrapy Spider Middleware\nDESCRIPTION: This complex middleware, RobotsTxtMiddleware, implements robots.txt exclusion. It fetches and parses robots.txt files, caches the results, and filters requests based on the rules.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass SpiderInfo(object):\n    def __init__(self, useragent):\n        self.useragent = useragent\n        self.parsers = {}\n        self.pending = defaultdict(list)\n\n\nclass AllowAllParser(object):\n    def can_fetch(useragent, url):\n        return True\n\n\nclass RobotsTxtMiddleware(object):\n    REQUEST_PRIORITY = 1000\n\n    def __init__(self):\n        self.spiders = {}\n        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n\n    def process_request(self, request, response, spider):\n        return self.process_start_request(request)\n\n    def process_start_request(self, request, spider):\n        info = self.spiders[spider]\n        url = urlparse_cached(request)\n        netloc = url.netloc\n        if netloc in info.parsers:\n            rp = info.parsers[netloc]\n            if rp.can_fetch(info.useragent, request.url):\n                res = request\n            else:\n                spider.log(\"Forbidden by robots.txt: %s\" % request)\n                res = None\n        else:\n            if netloc in info.pending:\n                res = None\n            else:\n                robotsurl = \"%s://%s/robots.txt\" % (url.scheme, netloc)\n                meta = {\"spider\": spider, \"handle_httpstatus_list\": [403, 404, 500]}\n                res = Request(\n                    robotsurl,\n                    callback=self.parse_robots,\n                    meta=meta,\n                    priority=self.REQUEST_PRIORITY,\n                )\n            info.pending[netloc].append(request)\n        return res\n\n    def parse_robots(self, response):\n        spider = response.request.meta[\"spider\"]\n        netloc = urlparse_cached(response).netloc\n        info = self.spiders[spider]\n        if response.status == 200:\n            rp = robotparser.RobotFileParser(response.url)\n            rp.parse(response.body.splitlines())\n            info.parsers[netloc] = rp\n        else:\n            info.parsers[netloc] = AllowAllParser()\n        return info.pending[netloc]\n\n    def spider_opened(self, spider):\n        ua = getattr(spider, \"user_agent\", None) or settings[\"USER_AGENT\"]\n        self.spiders[spider] = SpiderInfo(ua)\n\n    def spider_closed(self, spider):\n        del self.spiders[spider]\n```\n\n----------------------------------------\n\nTITLE: Module Path Changes in Scrapy Framework\nDESCRIPTION: Documentation showing the mapping between old and new module paths in Scrapy after reorganization. This includes changes to contrib_exp and contrib dissolvement, module pluralization, and class renames.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_70\n\nLANGUAGE: plaintext\nCODE:\n```\nscrapy.contrib_exp.downloadermiddleware.decompression -> scrapy.downloadermiddlewares.decompression\nscrapy.contrib_exp.iterators -> scrapy.utils.iterators\nscrapy.contrib.downloadermiddleware -> scrapy.downloadermiddlewares\nscrapy.contrib.exporter -> scrapy.exporters\nscrapy.contrib.linkextractors -> scrapy.linkextractors\nscrapy.contrib.loader -> scrapy.loader\nscrapy.contrib.loader.processor -> scrapy.loader.processors\nscrapy.contrib.pipeline -> scrapy.pipelines\nscrapy.contrib.spidermiddleware -> scrapy.spidermiddlewares\nscrapy.contrib.spiders -> scrapy.spiders\n```\n\n----------------------------------------\n\nTITLE: Initializing Middleware with Crawler Object in Python\nDESCRIPTION: This code snippet shows the proposed method of initializing a middleware in Scrapy using a Crawler object. It demonstrates how settings would be accessed through the crawler instance.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-009.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#!python\nfrom scrapy.core.exceptions import NotConfigured\n\n\nclass SomeMiddleware(object):\n    def __init__(self, crawler):\n        if not crawler.settings.getbool(\"SOMEMIDDLEWARE_ENABLED\"):\n            raise NotConfigured\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Stat Value in Scrapy Stats Collector\nDESCRIPTION: This snippet illustrates how to set a stat value only if it's lower than the previous value. It's used for tracking minimum values like 'min_free_memory_percent'.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/stats.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstats.min_value(\"min_free_memory_percent\", value)\n```\n\n----------------------------------------\n\nTITLE: Adding a Value to a List Field with ItemForm in Python\nDESCRIPTION: Shows how to append a value to an existing list field using the ItemForm API's += operator syntax.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#!python\nia[\"headline\"] += x.x('//h1[@class=\"headline\"]')\n```\n\n----------------------------------------\n\nTITLE: Deep Copying Scrapy Items\nDESCRIPTION: A new Item.deepcopy() method makes it easier to deep-copy items in Scrapy.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\ncopied_item = item.deepcopy()\n```\n\n----------------------------------------\n\nTITLE: Defining a NewsItem class in Python for Scrapy\nDESCRIPTION: This snippet defines a NewsItem class that inherits from Item and specifies fields for url, headline, content, and published date.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass NewsItem(Item):\n    url = fields.TextField()\n    headline = fields.TextField()\n    content = fields.TextField()\n    published = fields.DateField()\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Logging to File in Python for Scrapy\nDESCRIPTION: This code snippet demonstrates how to set up basic logging configuration in Python to redirect INFO or higher level messages to a file named 'log.txt'. It uses the logging.basicConfig function to specify the log file, format, and logging level.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.basicConfig(\n    filename=\"log.txt\", format=\"%(levelname)s: %(message)s\", level=logging.INFO\n)\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy tests with specific Python version using tox\nDESCRIPTION: Command to run Scrapy tests using a specific Python version (3.10) with tox environment.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntox -e py310\n```\n\n----------------------------------------\n\nTITLE: Generating Spider with New Name Attribute in Python\nDESCRIPTION: Example of generating a new spider using the proposed 'name' attribute and 'allowed_domains' list. This demonstrates how the spider class would be structured after implementing the proposed changes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-012.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass GooglecomSpider(BaseSpider):\n    name = \"google\"\n    allowed_domains = [\"google.com\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Referer in Scrapy Spider Middleware\nDESCRIPTION: This simple SetReferer middleware sets the Referer header for outgoing requests. It uses the URL of the response that generated the request as the referer.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass SetReferer(object):\n    def process_request(self, request, response, spider):\n        request.headers.setdefault(\"Referer\", response.url)\n        return request\n```\n\n----------------------------------------\n\nTITLE: Debugging Scheduler Serialization Error Log Example\nDESCRIPTION: Example log output when scheduler debugging is enabled and requests cannot be serialized to disk.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n1956-01-31 00:00:00+0800 [scrapy.core.scheduler] ERROR: Unable to serialize request:\\n<GET http://example.com> - reason: cannot serialize <Request at 0x9a7c7ec>\\n(type Request)> - no more unserializable requests will be logged\\n(see 'scheduler/unserializable' stats counter)\n```\n\n----------------------------------------\n\nTITLE: Constructing Absolute URL in Response Class (Python)\nDESCRIPTION: This snippet demonstrates how to use the urljoin method of the Response class to construct an absolute URL by combining the response's URL with a relative URL. It's a wrapper over urllib.parse.urljoin.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/request-response.rst#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nurllib.parse.urljoin(response.url, url)\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Level in Scrapy\nDESCRIPTION: Sets the logging level to INFO to reduce CPU usage and storage requirements during production crawls.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/broad-crawls.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nLOG_LEVEL = \"INFO\"\n```\n\n----------------------------------------\n\nTITLE: Using Root Logger in Python\nDESCRIPTION: Demonstrates how to explicitly get and use the root logger for logging messages. This is equivalent to using the logging shortcuts.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/logging.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger()\nlogger.warning(\"This is a warning\")\n```\n\n----------------------------------------\n\nTITLE: Initializing CsvItemExporter with Error Handling in Python\nDESCRIPTION: The __init__ method of CsvItemExporter now supports an 'errors' parameter to indicate how to handle encoding errors when exporting items to CSV.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nCsvItemExporter.__init__(..., errors='strict')\n```\n\n----------------------------------------\n\nTITLE: Setting expanders in NewsItemBuilder for Scrapy\nDESCRIPTION: This snippet demonstrates how to set expanders in a NewsItemBuilder class, overriding the Reducer class for BuilderFields based on their Item Field class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass NewsItemBuilder(ItemBuilder):\n    item_class = NewsItem\n\n    headline = reducers.Reducer(extract, remove_tags(), unquote(), strip)\n```\n\n----------------------------------------\n\nTITLE: Extending default ItemBuilder for site-specific news in Scrapy\nDESCRIPTION: This example shows how to extend the default ItemBuilder class to create a site-specific builder with a custom reducer for the published field.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass SiteNewsItemBuilder(NewsItemBuilder):\n    published = reducers.Reducer(\n        extract, remove_tags(), unquote(), strip, to_date(\"%d.%m.%Y\")\n    )\n```\n\n----------------------------------------\n\nTITLE: Scheduler Middleware API Definition - Python\nDESCRIPTION: Details the current and proposed new API for Scheduler middleware, including new dequeue methods and exception handling.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-013.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Current API\nenqueue_request(spider, request)\nopen_spider(spider)\nclose_spider(spider)\n\n# New API\n# EnqueueRequest deferred\nenqueue_request(request, spider)\nenqueue_request_exception(request, exception, spider)\n\n# DequeueRequest deferred\ndequeue_request(request, spider)\ndequeue_request_exception(exception, spider)\n```\n\n----------------------------------------\n\nTITLE: Implementing Chained Callback Contracts for User Profile Scraping in Python\nDESCRIPTION: This example shows how to implement contracts for chained callbacks. It includes a login callback that returns a request and a profile page callback that scrapes user information.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-017.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass UserProfileSpider(BaseSpider):\n    def parse_login_page(self, response):\n        \"\"\"\n        @url http://www.example.com/login.php\n        @returns_request\n        \"\"\"\n        # returns Request with callback=self.parse_profile_page\n\n    def parse_profile_page(self, response):\n        \"\"\"\n        @after parse_login_page\n        @scrapes user, name, email\n        \"\"\"\n        # ...\n```\n\n----------------------------------------\n\nTITLE: HTML Pagination Markup Example\nDESCRIPTION: HTML markup structure showing the pagination link element used for navigating between pages.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_8\n\nLANGUAGE: html\nCODE:\n```\n<ul class=\"pager\">\n    <li class=\"next\">\n        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n    </li>\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Pretty Print Export Format Example\nDESCRIPTION: Example output format for PprintItemExporter showing product entries\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n{'name': 'Color TV', 'price': '1200'}\n{'name': 'DVD player', 'price': '200'}\n```\n\n----------------------------------------\n\nTITLE: Defining Settings Priorities in Scrapy Python Framework\nDESCRIPTION: Dictionary that defines the default settings priorities used in Scrapy. Each entry has a code name and integer priority level. Greater priority values take precedence when setting and retrieving values in the Settings class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/api.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSETTINGS_PRIORITIES = {\n    \"default\": 0,\n    \"command\": 10,\n    \"addon\": 15,\n    \"project\": 20,\n    \"spider\": 30,\n    \"cmdline\": 40,\n}\n```\n\n----------------------------------------\n\nTITLE: Extending default ItemBuilder using static methods in Scrapy\nDESCRIPTION: This snippet demonstrates how to extend the default ItemBuilder class using static methods, reusing the parent's default_builder and adding a date conversion for the published field.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass SiteNewsItemBuilder(NewsItemBuilder):\n    published = reducers.Reducer(NewsItemBuilder.default_builder, to_date(\"%d.%m.%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Cache Storage in Scrapy\nDESCRIPTION: Example of how to configure a custom HTTP cache storage backend in Scrapy by setting the HTTPCACHE_STORAGE setting to the Python import path of the custom storage class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/downloader-middleware.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nHTTPCACHE_STORAGE = 'path.to.your.CustomStorageClass'\n```\n\n----------------------------------------\n\nTITLE: Memory Leak Example - Request with Response Reference\nDESCRIPTION: Example of code that can cause memory leaks by passing response references in requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/leaks.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nreturn Request(f\"http://www.somenastyspider.com/product.php?pid={product_id}\",\n               callback=self.parse, cb_kwargs={'referer': response})\n```\n\n----------------------------------------\n\nTITLE: Using default_builder in ItemBuilder for Scrapy\nDESCRIPTION: This example shows how to use a default_builder in an ItemBuilder subclass, which will be used for every field in the item class if not overridden.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass DefaultedNewsItemBuilder(ItemBuilder):\n    item_class = NewsItem\n\n    default_builder = reducers.Reducer(extract, remove_tags(), unquote(), strip)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Projects with Shared Root Directory\nDESCRIPTION: Example of a scrapy.cfg file that defines multiple project settings modules with aliases. This allows sharing a single root directory between multiple Scrapy projects.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[settings]\ndefault = myproject1.settings\nproject1 = myproject1.settings\nproject2 = myproject2.settings\n```\n\n----------------------------------------\n\nTITLE: Initializing Middleware with Singleton Pattern in Python\nDESCRIPTION: This code snippet demonstrates the current method of initializing a middleware in Scrapy using the singleton pattern. It imports settings and checks a boolean configuration value.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-009.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!python\nfrom scrapy.core.exceptions import NotConfigured\nfrom scrapy.conf import settings\n\n\nclass SomeMiddleware(object):\n    def __init__(self):\n        if not settings.getbool(\"SOMEMIDDLEWARE_ENABLED\"):\n            raise NotConfigured\n```\n\n----------------------------------------\n\nTITLE: Implementing Item ID Setter in Python\nDESCRIPTION: A LegSpider that sets unique identifiers for items based on specified fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-016.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ItemIdSetter(LegSpider):\n    def process_item(self, item):\n        id_field = self.spider.id_field\n        id_fields_to_hash = self.spider.id_fields_to_hash\n        item[id_field] = make_hash_based_on_fields(item, id_fields_to_hash)\n        return item\n\n\nclass MySpider(LegSpider):\n    legs = [ItemIdSetter()]\n    id_field = \"guid\"\n    id_fields_to_hash = [\"supplier_name\", \"supplier_id\"]\n\n    def process_response(self, item):\n        # extract item from response\n        return item\n```\n\n----------------------------------------\n\nTITLE: RSS2 Link Extractor Class Definition in Python\nDESCRIPTION: Partial implementation showing the class definition for an RSS2 link extractor middleware.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Rss2LinkExtractor(object):\n```\n\n----------------------------------------\n\nTITLE: Extending Field Metadata in Scrapy Item\nDESCRIPTION: Shows how to extend or modify field metadata in a subclass by referencing the parent class field metadata while adding or replacing specific attributes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/items.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SpecificProduct(Product):\n    name = scrapy.Field(Product.fields[\"name\"], serializer=my_serializer)\n```\n\n----------------------------------------\n\nTITLE: Initializing Scrapy Scheduler with Crawler Parameter\nDESCRIPTION: The __init__ method of the Scheduler class now requires an additional 'crawler' parameter. Custom scheduler subclasses may need to be updated to accept this new parameter.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nclass CustomScheduler(Scheduler):\n    def __init__(self, crawler, *args, **kwargs):\n        super().__init__(crawler, *args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: HTTP Compression Stats Counters\nDESCRIPTION: Example showing the stats counters for HTTP compression responses and bytes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nhttpcompression/response_bytes\nhttpcompression/response_count\n```\n\n----------------------------------------\n\nTITLE: Extending NewsItemBuilder for site-specific news in Scrapy\nDESCRIPTION: This example shows how to extend the NewsItemBuilder class to create a site-specific builder with a custom reducer for the published field.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SiteNewsItemBuilder(NewsItemBuilder):\n    published = reducers.Reducer(\n        extract, remove_tags(), unquote(), strip, to_date(\"%d.%m.%Y\")\n    )\n```\n\n----------------------------------------\n\nTITLE: Using Different Adaptors per Spider/Site with ItemBuilder in Python\nDESCRIPTION: Shows how to create a site-specific news builder by extending the base NewsBuilder class and overriding the published adaptor to use a specific date format.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#!python\nclass SiteNewsBuilder(NewsBuilder):\n    published = adaptor(HtmlNewsBuilder.published, to_date(\"%d.%m.%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Reinstalling Twisted with TLS Support\nDESCRIPTION: Command to reinstall Twisted with TLS support to resolve pyOpenSSL compatibility issues. This is a troubleshooting step for AttributeError related to TLS version constants.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install twisted[tls]\n```\n\n----------------------------------------\n\nTITLE: Implementing Command Logic in Python\nDESCRIPTION: This snippet shows the proposed logic for handling commands in Scrapy, determining whether to crawl a URL or a spider name based on the input argument.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-015.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif is_url(arg):\n   - calls ScrapyManager.crawl_url(arg)\nelse:\n   - calls ScrapyManager.crawl_spider_name(arg)\n```\n\n----------------------------------------\n\nTITLE: Getting Running Spiders via HTTP GET\nDESCRIPTION: API endpoint to retrieve a list of all currently running spiders. Returns a list of dictionaries containing spider id and domain_name for each running spider.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nGET /spiders/opened\n```\n\n----------------------------------------\n\nTITLE: Closing a Spider via HTTP POST\nDESCRIPTION: API endpoint to close a running spider. Requires the spider ID in the URL path.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nPOST /spider/1238/close\n```\n\n----------------------------------------\n\nTITLE: Defining ScrapyManager API in Python\nDESCRIPTION: This snippet details the proposed API for the ScrapyManager class, including methods for crawling requests, spiders, spider names, and URLs. It also specifies the logic for handling spider selection and request generation.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-015.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncrawl_request(request, spider=None)\n   - calls SpiderManager.find_by_request(request) if spider is None\n   - fails if len(spiders returned) != 1\ncrawl_spider(spider)\n   - calls spider.start_requests()\ncrawl_spider_name(spider_name)\n   - calls SpiderManager.get(spider_name)\n   - calls spider.start_requests()\ncrawl_url(url)\n   - calls spider.make_requests_from_url()\n```\n\n----------------------------------------\n\nTITLE: Resetting default_builder for a specific field in Scrapy ItemBuilder\nDESCRIPTION: This snippet demonstrates how to reset the default_builder for a specific field (url) in an ItemBuilder subclass.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-005.rst#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass DefaultedNewsItemBuilder(ItemBuilder):\n    item_class = NewsItem\n\n    default_builder = reducers.Reducer(extract, remove_tags(), unquote(), strip)\n    url = BuilderField()\n```\n\n----------------------------------------\n\nTITLE: Installing Scrapy with Pip\nDESCRIPTION: Command to install Scrapy using pip, the Python package manager. This method requires Python 3.9+ and will install Scrapy and its dependencies from PyPI.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install Scrapy\n```\n\n----------------------------------------\n\nTITLE: Spider Middleware API Definition - Python\nDESCRIPTION: Defines the current and proposed new API for Spider middleware, including renamed exception handling methods and new input/output deferreds.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-013.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Current API\nprocess_spider_input(response, spider)\nprocess_spider_output(response, result, spider)\nprocess_spider_exception(response, exception, spider=spider)\n\n# New API\n# SpiderInput deferred\nprocess_spider_input(response, spider)\nprocess_spider_input_exception(response, exception, spider=spider)\n\n# SpiderOutput deferred\nprocess_spider_output(response, result, spider)\nprocess_spider_output_exception(response, exception, spider=spider)\n```\n\n----------------------------------------\n\nTITLE: Running Contract Checks for Spiders\nDESCRIPTION: Examples of using the check command to run contract checks on spiders, listing available contracts and showing error reporting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy check -l\nfirst_spider\n  * parse\n  * parse_item\nsecond_spider\n  * parse\n  * parse_item\n\n$ scrapy check\n[FAILED] first_spider:parse_item\n>>> 'RetailPricex' field is missing\n\n[FAILED] first_spider:parse\n>>> Returned 92 requests, expected 0..4\n```\n\n----------------------------------------\n\nTITLE: Getting Available Spiders via HTTP GET\nDESCRIPTION: API endpoint to retrieve a list of all available spiders in the Scrapy server.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nGET /spiders/all\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy tests in parallel with multiple Python versions\nDESCRIPTION: Command to run Scrapy tests in parallel using multiple Python versions (3.9 and 3.10) with tox.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntox -e py39,py310 -p auto\n```\n\n----------------------------------------\n\nTITLE: Getting Spider-Specific Statistics via HTTP GET\nDESCRIPTION: API endpoint to retrieve statistics for a specific spider. Requires the spider ID in the URL path.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_7\n\nLANGUAGE: http\nCODE:\n```\nGET /spider/1238/stats/\n```\n\n----------------------------------------\n\nTITLE: Retrieving Bot Name with Project Environment Variable\nDESCRIPTION: Example of using the SCRAPY_PROJECT environment variable to specify which project configuration to use when running Scrapy commands.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/commands.rst#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ scrapy settings --get BOT_NAME\nProject 1 Bot\n$ export SCRAPY_PROJECT=project2\n$ scrapy settings --get BOT_NAME\nProject 2 Bot\n```\n\n----------------------------------------\n\nTITLE: Defining Default Feed Storage Backends in Scrapy Python\nDESCRIPTION: Default configuration for FEED_STORAGES_BASE setting in Scrapy, defining built-in feed storage backends for different URI schemes.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n    \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\",\n    \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n    \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n}\n```\n\n----------------------------------------\n\nTITLE: Downloader Middleware API Definition - Python\nDESCRIPTION: Specifies the current and proposed new API for Downloader middleware, including renamed exception handling methods and new request/response deferreds.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-013.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Current API\nprocess_request(request, spider)\nprocess_response(request, response, spider)\nprocess_exception(request, exception, spider)\n\n# New API\n# ProcessRequest deferred\nprocess_request(request, spider)\nprocess_request_exception(request, exception, response)\n\n# ProcessResponse deferred\nprocess_response(request, spider, response)\nprocess_response_exception(request, exception, response)\n```\n\n----------------------------------------\n\nTITLE: Using Muppy for Memory Analysis\nDESCRIPTION: Example demonstrating how to use muppy to analyze memory usage and object distribution in Python heap.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/leaks.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from pympler import muppy\n>>> all_objects = muppy.get_objects()\n>>> len(all_objects)\n28667\n>>> from pympler import summary\n>>> suml = summary.summarize(all_objects)\n>>> summary.print_(suml)\n```\n\n----------------------------------------\n\nTITLE: Installing Scrapy via pip\nDESCRIPTION: A simple command to install Scrapy using pip package manager. This is the quickest way to get Scrapy installed on your system.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/README.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install scrapy\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy tests with custom pytest options\nDESCRIPTION: Command to run Scrapy tests with custom pytest options, such as stopping after the first failure.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntox -- scrapy tests -x  # stop after first failure\n```\n\n----------------------------------------\n\nTITLE: Importing LinkExtractor in Scrapy\nDESCRIPTION: Code snippet showing how to import the LinkExtractor class from Scrapy's linkextractors module, which is an alias for the LxmlLinkExtractor class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/link-extractors.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.linkextractors import LinkExtractor\n```\n\n----------------------------------------\n\nTITLE: Defining Scrapy Documentation Structure in reStructuredText\nDESCRIPTION: This code snippet defines the structure of the Scrapy documentation using reStructuredText directives. It includes sections for first steps, basic concepts, built-in services, solving specific problems, and extending Scrapy. Each section contains links to relevant topics and brief descriptions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/index.rst#2025-04-17_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :caption: First steps\n   :hidden:\n\n   intro/overview\n   intro/install\n   intro/tutorial\n   intro/examples\n\n:doc:`intro/overview`\n    Understand what Scrapy is and how it can help you.\n\n:doc:`intro/install`\n    Get Scrapy installed on your computer.\n\n:doc:`intro/tutorial`\n    Write your first Scrapy project.\n\n:doc:`intro/examples`\n    Learn more by playing with a pre-made Scrapy project.\n```\n\n----------------------------------------\n\nTITLE: JSON Stats Output Example\nDESCRIPTION: Example of the JSON stats output format produced by the PeriodicLog extension, showing delta changes, current stats, and timing information.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/extensions.rst#2025-04-17_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n2023-08-04 02:30:57 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 162 pages/min), scraped 925 items (at 161 items/min)\n2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {\n    \"delta\": {\n        \"downloader/request_bytes\": 55582,\n        \"downloader/request_count\": 162,\n        \"downloader/request_method_count/GET\": 162,\n        \"downloader/response_bytes\": 618133,\n        \"downloader/response_count\": 162,\n        \"downloader/response_status_count/200\": 162,\n        \"item_scraped_count\": 161\n    },\n    \"stats\": {\n        \"downloader/request_bytes\": 338243,\n        \"downloader/request_count\": 992,\n        \"downloader/request_method_count/GET\": 992,\n        \"downloader/response_bytes\": 3836736,\n        \"downloader/response_count\": 976,\n        \"downloader/response_status_count/200\": 976,\n        \"item_scraped_count\": 925,\n        \"log_count/INFO\": 21,\n        \"log_count/WARNING\": 1,\n        \"scheduler/dequeued\": 992,\n        \"scheduler/dequeued/memory\": 992,\n        \"scheduler/enqueued\": 1050,\n        \"scheduler/enqueued/memory\": 1050\n    },\n    \"time\": {\n        \"elapsed\": 360.008903,\n        \"log_interval\": 60.0,\n        \"log_interval_real\": 60.006694,\n        \"start_time\": \"2023-08-03 23:24:57\",\n        \"utcnow\": \"2023-08-03 23:30:57\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing ItemField Class in Python for Scrapy\nDESCRIPTION: This snippet shows the proposed implementation of the ItemField class, which extends BaseField. It includes methods for initializing the field, converting values to the correct item type, and handling default values.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-003.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.item.fields import BaseField\n\n\nclass ItemField(BaseField):\n    def __init__(self, item_type, default=None):\n        self._item_type = item_type\n        super(ItemField, self).__init__(default)\n\n    def to_python(self, value):\n        return (\n            self._item_type(value) if not isinstance(value, self._item_type) else value\n        )\n\n    def get_default(self):\n        # WARNING: returns default item instead of a copy - this must be\n        # well documented, as Items are mutable objects and may lead to\n        # unexpected behaviors # always returning a copy may not be desirable\n        # either (see Supplier item, for example). this method can be\n        # overridden to change this behaviour\n        return self._default\n```\n\n----------------------------------------\n\nTITLE: List Field Value Appending in Scrapy\nDESCRIPTION: Shows how to append values to list fields and raises questions about type validation during append operations.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-002.rst#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ni = Article()\n\ni[\"categories\"] = [\"one\", \"two\"]\ni[\"categories\"].append(3)  # XXX: should this fail?\n```\n\n----------------------------------------\n\nTITLE: Using python -m scrapy Command in Scrapy (Python)\nDESCRIPTION: Example of using 'python -m scrapy' as an alternative to the 'scrapy' command in Scrapy 1.4. This provides a more explicit way to run Scrapy commands.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\n``python -m scrapy`` as a more explicit alternative to ``scrapy`` command\n(:issue:`2740`)\n```\n\n----------------------------------------\n\nTITLE: Setting Configuration Example - Python\nDESCRIPTION: Example of Scrapy settings usage showing retry status codes configuration\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nRETRY_HTTP_CODES = [400] # Add 400 to retry these response codes\n```\n\n----------------------------------------\n\nTITLE: Getting Closed Spiders via HTTP GET\nDESCRIPTION: API endpoint to retrieve a list of all closed spiders in the Scrapy server.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nGET /spiders/closed\n```\n\n----------------------------------------\n\nTITLE: Python Class Definition: JsonResponse\nDESCRIPTION: New Response subclass added for handling responses with JSON MIME type.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass JsonResponse(Response):\n```\n\n----------------------------------------\n\nTITLE: Passing Same-Named Arguments to Different Adaptors with ItemBuilder in Python\nDESCRIPTION: Demonstrates how to pass different values for the same argument name to different fields using the ItemBuilder API by specifying the arguments when calling add_value().\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#!python\nil.add_value(\"width\", x.x('//p[@class=\"width\"]'), default_unit=\"cm\")\nil.add_value(\"volume\", x.x('//p[@class=\"volume\"]'), default_unit=\"lt\")\n```\n\n----------------------------------------\n\nTITLE: Exiting Scrapy Shell\nDESCRIPTION: Demonstrates exiting the Scrapy shell using Ctrl-D (Unix) or Ctrl-Z (Windows) to resume crawling\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/shell.rst#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> ^D\n2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.net> (referer: None)\n...\n```\n\n----------------------------------------\n\nTITLE: Pulling Existing Pull Request to Local Branch\nDESCRIPTION: This Git command fetches a specific pull request from the upstream repository and creates a local branch from it, allowing contributors to work on stalled pull requests.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch upstream pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE\n```\n\n----------------------------------------\n\nTITLE: Selecting First <li> Elements Under Any Parent\nDESCRIPTION: Demonstrates using XPath to select all first <li> elements under their respective parents.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_20\n\nLANGUAGE: pycon\nCODE:\n```\n>>> xp(\"//li[1]\")\n['<li>1</li>', '<li>4</li>']\n```\n\n----------------------------------------\n\nTITLE: Setting FEED_EXPORT_ENCODING in settings.py (Python)\nDESCRIPTION: Sets the FEED_EXPORT_ENCODING setting to 'utf-8' in the settings.py file generated by the startproject command. This allows JSON exports to use non-ASCII characters without escape sequences.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nFEED_EXPORT_ENCODING = \"utf-8\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Built-in CSV Exporter in Scrapy Settings\nDESCRIPTION: This snippet shows how to disable the built-in CSV exporter in Scrapy by setting its value to None in the FEED_EXPORTERS dictionary within the settings.py file.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/feed-exports.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nFEED_EXPORTERS = {\n    \"csv\": None,\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing ItemLoader with Pre-Populated Item in Python\nDESCRIPTION: When an item loader is initialized with an item, calling ItemLoader.load_item() resets the item, causing subsequent calls to get_output_value() or load_item() to return empty data.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nItemLoader.load_item()\n```\n\nLANGUAGE: Python\nCODE:\n```\nItemLoader.get_output_value()\n```\n\n----------------------------------------\n\nTITLE: Enabling Anonymous FTP in Scrapy (Python)\nDESCRIPTION: Code snippet showing how to enable anonymous FTP support in Scrapy. This was added as a new feature in Scrapy 1.4.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\nSupport Anonymous FTP (:issue:`2342`)\n```\n\n----------------------------------------\n\nTITLE: Setting Proxy Credentials in Scrapy Request Meta (Python)\nDESCRIPTION: Shows how to set proxy credentials in the proxy request meta key in Scrapy 1.4. This allows passing authentication details for proxies.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\nAccept proxy credentials in :reqmeta:`proxy` request meta key (:issue:`2526`)\n```\n\n----------------------------------------\n\nTITLE: Updating IO Imports for Python 3 in Scrapy\nDESCRIPTION: Uses six.BytesIO and six.moves.cStringIO for better IO handling across Python versions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_76\n\nLANGUAGE: Python\nCODE:\n```\nfrom six import BytesIO\nfrom six.moves import cStringIO\n```\n\n----------------------------------------\n\nTITLE: Running all Scrapy tests with tox\nDESCRIPTION: Command to run all Scrapy tests using tox, which manages virtual environments and test execution.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntox\n```\n\n----------------------------------------\n\nTITLE: Updating Dictionary and ConfigParser Usage for Python 3 in Scrapy\nDESCRIPTION: Uses six.iterkeys, six.iteritems, and six.moves.configparser for better Python 3 compatibility in dictionary and config parsing operations.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_79\n\nLANGUAGE: Python\nCODE:\n```\nfrom six import iterkeys, iteritems\nfrom six.moves import configparser\n```\n\n----------------------------------------\n\nTITLE: Setting Item Identifiers in Scrapy Spider Middleware\nDESCRIPTION: This middleware, ItemIdSetter, generates a unique identifier for items based on specified fields. It uses a hash function to create the identifier from the specified fields.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-018.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass ItemIdSetter(object):\n    def process_item(self, item, response, spider):\n        id_field = spider.id_field\n        id_fields_to_hash = spider.id_fields_to_hash\n        item[id_field] = make_hash_based_on_fields(item, id_fields_to_hash)\n        return item\n\n\n# Example spider using this middleware\nclass MySpider(BaseSpider):\n    middlewares = [ItemIdSetter()]\n    id_field = \"guid\"\n    id_fields_to_hash = [\"supplier_name\", \"supplier_id\"]\n\n    def parse(self, response):\n        # extract item from response\n        return item\n```\n\n----------------------------------------\n\nTITLE: CSV Export Format Example\nDESCRIPTION: Example output format for CSV item exporter showing product data\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_4\n\nLANGUAGE: csv\nCODE:\n```\nproduct,price\nColor TV,1200\nDVD player,200\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit hook for Scrapy development\nDESCRIPTION: Instructions for installing the pre-commit hook to automatically check code before commits in Scrapy development.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Replacing has_key and Updating ConfigParser Usage in Scrapy\nDESCRIPTION: Removes usage of deprecated has_key method and updates ConfigParser import for Python 3 compatibility.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_80\n\nLANGUAGE: Python\nCODE:\n```\n# Old code\nif dict.has_key(key):\n    # do something\n\n# New code\nif key in dict:\n    # do something\n\nfrom six.moves import configparser\n```\n\n----------------------------------------\n\nTITLE: Selecting First <li> Element Under <ul> in Entire Document\nDESCRIPTION: Shows how to select the first <li> element that is a child of a <ul> element in the whole document.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_23\n\nLANGUAGE: pycon\nCODE:\n```\n>>> xp(\"(//ul/li)[1]\")\n['<li>1</li>']\n```\n\n----------------------------------------\n\nTITLE: Replacing chain(*iterable) with chain.from_iterable(iterable) in Python\nDESCRIPTION: Code improvement that replaces the use of chain(*iterable) with chain.from_iterable(iterable), which is more efficient as it avoids unpacking the iterable.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nchain.from_iterable(iterable)\n```\n\n----------------------------------------\n\nTITLE: Stats Counter Example for URL Length Middleware\nDESCRIPTION: Example showing the stats counter for ignored URLs in the UrlLengthMiddleware.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nurllength/request_ignored_count\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Tox\nDESCRIPTION: Alternative method to compile documentation using tox. Generated output will be placed in the .tox/docs/tmp/html directory.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/README.rst#2025-04-17_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntox -e docs\n```\n\n----------------------------------------\n\nTITLE: Displaying Memory Usage Class in Python\nDESCRIPTION: This snippet shows a partial output of memory usage analysis, likely from a memory profiling tool. It displays the memory usage for an integer class.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/leaks.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n<class 'int |        1425 |     43.20 KB\n```\n\n----------------------------------------\n\nTITLE: Queue Class Initialization Example\nDESCRIPTION: Example of the new queue class initialization signature in Scrapy's scheduler showing the updated parameter structure and requirements.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n__init__(self, crawler, key)\n```\n\n----------------------------------------\n\nTITLE: Defining Minimum Package Dependencies for Scrapy\nDESCRIPTION: Specifies minimum version requirements for Scrapy's core dependencies. Includes packages for HTTP handling, XML processing, testing, and core functionality. Higher version requirements help prevent pip from spending excessive time resolving dependencies.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/tests/upper-constraints.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nattrs>=20.2.0\nAutomat>=0.8.0\nbotocore>=1.20.30\nitemadapter>=0.1.1\nitemloaders>=1.0.3\nlxml>=4.6.1\nparsel>=1.5.2\nPillow>=8.0.1\npyOpenSSL>=17.5  # mitmproxy 4.0.4\npytest>=6.2.1\npytest-twisted>=1.13.1\nservice_identity>=17.0.0\nsix>=1.14.0\nsybil>=2.0.0\nTwisted>=19.10.0\n```\n\n----------------------------------------\n\nTITLE: Converting Date String to YYYY-MM-DD Format in Python\nDESCRIPTION: The to_date function converts a date string to a YYYY-MM-DD format suitable for DateField. However, this function is now considered obsolete as DateField no longer exists.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-007.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# to_date function (obsolete)\n# No code provided in the original text\n```\n\n----------------------------------------\n\nTITLE: Converting String to Unicode in Python\nDESCRIPTION: The to_unicode function converts a string to unicode using a specified encoding (default is utf-8). It returns a new unicode object.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-007.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>> to_unicode('it costs 20\\xe2\\x82\\xac, or 30\\xc2\\xa3')\n[u'it costs 20\\u20ac, or 30\\xa3']\n```\n\n----------------------------------------\n\nTITLE: JSON Export Format Example\nDESCRIPTION: Example output format for JsonItemExporter showing product data as array\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[{\"name\": \"Color TV\", \"price\": \"1200\"},\n{\"name\": \"DVD player\", \"price\": \"200\"}]\n```\n\n----------------------------------------\n\nTITLE: Getting Engine Status via HTTP GET\nDESCRIPTION: API endpoint to retrieve the current status of the Scrapy engine.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_8\n\nLANGUAGE: http\nCODE:\n```\nGET /engine/status\n```\n\n----------------------------------------\n\nTITLE: Python Class Definition: MetadataContract\nDESCRIPTION: New contract class for setting request meta data in spider contracts\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MetadataContract(Contract)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Scrapy on Ubuntu\nDESCRIPTION: Command to install the required system dependencies for Scrapy on Ubuntu-based systems. These packages provide the necessary development libraries for lxml, cryptography, and other Scrapy dependencies.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install python3 python3-dev python3-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev\n```\n\n----------------------------------------\n\nTITLE: Using six for robotparser and urlparse in Scrapy\nDESCRIPTION: Updates import statements to use six.moves for robotparser and urlparse, ensuring compatibility across Python versions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_78\n\nLANGUAGE: Python\nCODE:\n```\nfrom six.moves import robotparser\nfrom six.moves.urllib.parse import urlparse\n```\n\n----------------------------------------\n\nTITLE: Initializing MailSender in Python using Scrapy\nDESCRIPTION: Demonstrates basic instantiation of the MailSender class using the standard constructor method. This is one of two ways to create a mailer instance.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/email.rst#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.mail import MailSender\n\nmailer = MailSender()\n```\n\n----------------------------------------\n\nTITLE: Updating Deprecated Python 3 Assertions in Scrapy Tests\nDESCRIPTION: Updates the test suite to use assertCountEqual instead of the deprecated assertItemsEqual for Python 3 compatibility.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_72\n\nLANGUAGE: Python\nCODE:\n```\n# Old code\nself.assertItemsEqual(expected, actual)\n\n# New code \nself.assertCountEqual(expected, actual)\n```\n\n----------------------------------------\n\nTITLE: Removing None Values from Python Iterables\nDESCRIPTION: The drop_empty function removes any index that evaluates to None from the provided iterable. This functionality is now included in reducers and is considered obsolete.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-007.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>> drop_empty([0, 'this', None, 'is', False, 'an example'])\n['this', 'is', 'an example']\n```\n\n----------------------------------------\n\nTITLE: Assigning Values to ItemFields in Scrapy\nDESCRIPTION: This snippet illustrates various ways of assigning values to ItemFields in Scrapy, including direct assignment, dictionary-based assignment, and list assignments for nested items. It also shows examples of invalid assignments that would raise errors.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-003.rst#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsupplier = Supplier(name=\"Supplier 1\", url=\"http://example.com\")\n\np = Product()\n\n# standard assignment\np[\"supplier\"] = supplier\n# this also works as it tries to instantiate a Supplier with the given dict\np[\"supplier\"] = {\"name\": \"Supplier 1\", url: \"http://example.com\"}\n# this fails because it can't instantiate a Supplier\np[\"supplier\"] = \"Supplier 1\"\n# this fails because url doesn't have the valid type\np[\"supplier\"] = {\"name\": \"Supplier 1\", url: 123}\n\nv1 = Variant()\nv1[\"name\"] = \"lala\"\nv1[\"price\"] = Decimal(\"100\")\n\nv2 = Variant()\nv2[\"name\"] = \"lolo\"\nv2[\"price\"] = Decimal(\"150\")\n\n# standard assignment\np[\"variants\"] = [v1, v2]  # OK\n# can also instantiate at assignment time\np[\"variants\"] = [v1, Variant(name=\"lolo\", price=Decimal(\"150\"))]\n# this also works as it tries to instantiate a Variant with the given dict\np[\"variants\"] = [v1, {\"name\": \"lolo\", \"price\": Decimal(\"150\")}]\n# this fails because it can't instantiate a Variant\np[\"variants\"] = [v1, \"test\"]\n# this fails because 'coco' is not a valid value for price\np[\"variants\"] = [v1, {\"name\": \"lolo\", \"price\": \"coco\"}]\n```\n\n----------------------------------------\n\nTITLE: Stats Storage Example for Feed Exports in Python\nDESCRIPTION: Example showing the stats storage format for feed export successes and failures with different storage backend types.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfeedexport/success_count/<storage type>\nfeedexport/failed_count/<storage type>\n```\n\n----------------------------------------\n\nTITLE: Installing Python with Homebrew on macOS\nDESCRIPTION: Command to install Python using the homebrew package manager on macOS. This provides a version of Python that doesn't conflict with the system Python installation.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbrew install python\n```\n\n----------------------------------------\n\nTITLE: Checking Extracted Value with ItemForm in Python\nDESCRIPTION: Demonstrates how to check if a value was successfully extracted using the ItemForm API, with a fallback extraction pattern if the first attempt fails.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-001.rst#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#!python\nia = NewsForm(response)\nia[\"headline\"] = x.x('//h1[@class=\"headline\"]')\nif not ia[\"headline\"]:\n    ia[\"headline\"] = x.x('//h1[@class=\"title\"]')\n```\n\n----------------------------------------\n\nTITLE: Installing Xcode Command-line Tools on macOS\nDESCRIPTION: Command to install Apple's Xcode command-line tools on macOS, which are required for building Scrapy's dependencies. This provides the necessary C compiler and development headers.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Importing unittest.mock in Scrapy\nDESCRIPTION: Adds support for importing unittest.mock if available, improving testing capabilities.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_73\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    import unittest.mock as mock\nexcept ImportError:\n    import mock\n```\n\n----------------------------------------\n\nTITLE: Default Value Handling in List Fields\nDESCRIPTION: Demonstrates how default values are handled in list fields, including KeyError raising and None returns.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-002.rst#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ni = Article()\n\ni[\"categories\"]  # raises KeyError\ni.get(\"categories\")  # returns None\n\ni[\"numbers\"]  # returns []\n```\n\n----------------------------------------\n\nTITLE: Using MutableMapping for Python 3 Compatibility in Scrapy\nDESCRIPTION: Updates code to use MutableMapping for better Python 3 compatibility in dictionary-like objects.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_75\n\nLANGUAGE: Python\nCODE:\n```\nfrom collections import MutableMapping\n\nclass MyDict(MutableMapping):\n    # Implementation\n```\n\n----------------------------------------\n\nTITLE: Linking Pull Request to Issue using GitHub Keywords\nDESCRIPTION: This code snippet demonstrates how to link a pull request to an issue using GitHub's keywords, which automatically closes the referenced issue when the pull request is merged.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nResolves #123\n```\n\n----------------------------------------\n\nTITLE: Defining External Links in ReStructuredText for Scrapy Documentation\nDESCRIPTION: This snippet defines two external links using ReStructuredText syntax. These links are likely referenced elsewhere in Scrapy's documentation to provide resources for Microsoft C++ Build Tools and conda-forge.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_8\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _Microsoft C++ Build Tools: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n.. _conda-forge: https://conda-forge.org/\n```\n\n----------------------------------------\n\nTITLE: Python Method Addition: get_slot_key\nDESCRIPTION: New method replacing deprecated _get_slot_key() in scrapy.core.downloader.Downloader\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nscrapy.core.downloader.Downloader.get_slot_key\n```\n\n----------------------------------------\n\nTITLE: Reloading Bash Configuration on macOS\nDESCRIPTION: Command to reload the bash configuration file after updating the PATH variable on macOS. This ensures the changes take effect in the current terminal session.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsource ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Memory Usage Interval Setting - Python\nDESCRIPTION: Example of configuring the memory usage check interval\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nMEMUSAGE_CHECK_INTERVAL_SECONDS = 60 # Set check interval in seconds\n```\n\n----------------------------------------\n\nTITLE: Using six.moves.cPickle in Scrapy\nDESCRIPTION: Updates pickle import to use six.moves.cPickle for better performance and compatibility across Python versions.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_81\n\nLANGUAGE: Python\nCODE:\n```\nfrom six.moves import cPickle as pickle\n```\n\n----------------------------------------\n\nTITLE: Example Scrapy Version Number Format in reStructuredText\nDESCRIPTION: Shows the format of Scrapy version numbers using the example of version 1.1.1, which is described as the first bugfix release of the 1.1 series and is considered safe for production use.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/versioning.rst#2025-04-17_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n* *1.1.1* is the first bugfix release of the *1.1* series (safe to use in\n  production)\n```\n\n----------------------------------------\n\nTITLE: Disabling a Spider Contract in Scrapy Settings\nDESCRIPTION: This example demonstrates how to disable a specific spider contract (ScrapesContract) by setting its value to None in the SPIDER_CONTRACTS setting.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/settings.rst#2025-04-17_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nSPIDER_CONTRACTS = {\n    \"scrapy.contracts.default.ScrapesContract\": None,\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation Coverage Report with Tox\nDESCRIPTION: This command runs the documentation coverage check using tox to ensure that private APIs are properly excluded from documentation coverage.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntox -e docs-coverage\n```\n\n----------------------------------------\n\nTITLE: Feed Storage Directory Config - Python\nDESCRIPTION: Example of setting temporary directory for feed storage\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nFEED_TEMPDIR = '/custom/temp/dir' # Set custom temp directory\n```\n\n----------------------------------------\n\nTITLE: Example Spider Using BulkItemLoader in Python\nDESCRIPTION: Demonstrates how to use the BulkItemLoader in a Scrapy spider to extract data from a W3C HTML specification page. It parses a definition list and prints the extracted items.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-020.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom scrapy.spider import BaseSpider\nfrom scrapy.contrib.loader.bulk import BulkItemLoader\n\n\nclass W3cSpider(BaseSpider):\n    name = \"w3c\"\n    allowed_domains = [\"w3.org\"]\n    start_urls = (\"http://www.w3.org/TR/html401/struct/lists.html\",)\n\n    def parse(self, response):\n        el = BulkItemLoader(response=response)\n        el.parse_dl(\"//dl[2]\")\n        item = el.load_item()\n\n        from pprint import pprint\n\n        pprint(item)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Documentation Build Files\nDESCRIPTION: Removes all generated documentation files while preserving source files, allowing for a clean rebuild.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/README.rst#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake clean\n```\n\n----------------------------------------\n\nTITLE: Updating PATH for Homebrew Packages on macOS\nDESCRIPTION: Command to update the PATH environment variable to prioritize homebrew packages over system packages on macOS. This helps avoid conflicts when installing Python packages.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/install.rst#2025-04-17_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\necho \"export PATH=/usr/local/bin:/usr/local/sbin:$PATH\" >> ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Auto-updating Documentation on Changes\nDESCRIPTION: Uses watchdog to automatically rebuild documentation when changes are detected. Requires installing watchdog via pip first.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/README.rst#2025-04-17_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake watch\n```\n\n----------------------------------------\n\nTITLE: Selecting First <li> Element in Entire Document\nDESCRIPTION: Shows how to select the first <li> element in the whole document using XPath.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/selectors.rst#2025-04-17_snippet_21\n\nLANGUAGE: pycon\nCODE:\n```\n>>> xp(\"(//li)[1]\")\n['<li>1</li>']\n```\n\n----------------------------------------\n\nTITLE: Generating coverage report for Scrapy tests\nDESCRIPTION: Command to generate a coverage report after running Scrapy tests using the coverage tool.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/contributing.rst#2025-04-17_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncoverage report\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies with pip\nDESCRIPTION: Installs Sphinx and all required dependencies for compiling Scrapy documentation using pip and a requirements file.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/README.rst#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Launching Scrapy in Server Mode\nDESCRIPTION: Command to start Scrapy in server mode, which enables the REST API functionality.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nscrapy-ctl.py start\n```\n\n----------------------------------------\n\nTITLE: Getting Global Statistics via HTTP GET\nDESCRIPTION: API endpoint to retrieve global statistics from the Scrapy server. Does not include spider-specific statistics.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_6\n\nLANGUAGE: http\nCODE:\n```\nGET /stats\n```\n\n----------------------------------------\n\nTITLE: Cleaning Multiple Spaces in Python Strings\nDESCRIPTION: The clean_spaces function converts multiple spaces into single spaces for a given string.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-007.rst#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>> clean_spaces(u'Hello   sir')\nu'Hello sir'\n```\n\n----------------------------------------\n\nTITLE: Scheduling a Spider via HTTP POST\nDESCRIPTION: API endpoint to schedule a spider for execution. Requires a 'schedule' parameter with the domain name to crawl.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nPOST /spiders\n```\n\n----------------------------------------\n\nTITLE: XML Data Structure Example\nDESCRIPTION: Example showing XML structure for name and age data\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/topics/exporters.rst#2025-04-17_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<name>\n  <value>John</value>\n  <value>Doe</value>\n</name>\n<age>23</age>\n```\n\n----------------------------------------\n\nTITLE: Getting Scheduled Spiders via HTTP GET\nDESCRIPTION: API endpoint to retrieve a list of all scheduled spiders in the Scrapy server. This includes closed spiders.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/sep/sep-010.rst#2025-04-17_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nGET /spiders/scheduled\n```\n\n----------------------------------------\n\nTITLE: Settings and Class Name Changes in Scrapy\nDESCRIPTION: Documentation of renamed settings variables and class names in Scrapy to maintain consistency and clarity in the framework.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_71\n\nLANGUAGE: plaintext\nCODE:\n```\nSPIDER_MANAGER_CLASS -> SPIDER_LOADER_CLASS\nscrapy.spidermanager.SpiderManager -> scrapy.spiderloader.SpiderLoader\n```\n\n----------------------------------------\n\nTITLE: Supported Versions Table in Markdown\nDESCRIPTION: Markdown table showing which Scrapy versions receive security support. Version 2.12.x is supported while older versions are not.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/SECURITY.md#2025-04-17_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version | Supported          |\n| ------- | ------------------ |\n| 2.12.x     | :white_check_mark: |\n| < 2.12.x   | :x:                |\n```\n\n----------------------------------------\n\nTITLE: Detecting Scrapy Contract Check Runs\nDESCRIPTION: The SCRAPY_CHECK variable is set to 'true' during runs of the check command, allowing detection of contract check runs from code.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nif os.environ.get('SCRAPY_CHECK', 'false') == 'true':\n    # This is a contract check run\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Documentation Dependencies\nDESCRIPTION: This requirements file specifies the exact versions of Sphinx and related packages needed for generating project documentation. It includes the core Sphinx framework, the ReadTheDocs theme with dark mode support, and extension packages for enhanced functionality like hover references and 404 page handling.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nsphinx==8.1.3\nsphinx-hoverxref==1.4.2\nsphinx-notfound-page==1.0.4\nsphinx-rtd-theme==3.0.2\nsphinx-rtd-dark-mode==1.3.0\n```\n\n----------------------------------------\n\nTITLE: Fixing xmlrpclib and email Imports for Python 3 in Scrapy\nDESCRIPTION: Updates import statements for xmlrpclib and email modules to be compatible with Python 3.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_77\n\nLANGUAGE: Python\nCODE:\n```\n# Python 2\nimport xmlrpclib\nimport email\n\n# Python 3\nfrom six.moves import xmlrpc_client as xmlrpclib\nimport email\n```\n\n----------------------------------------\n\nTITLE: Author Information Spider\nDESCRIPTION: Spider implementation for scraping author information using multiple callbacks and follow_all method.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/intro/tutorial.rst#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport scrapy\n\n\nclass AuthorSpider(scrapy.Spider):\n    name = \"author\"\n\n    start_urls = [\"https://quotes.toscrape.com/\"]\n\n    def parse(self, response):\n        author_page_links = response.css(\".author + a\")\n        yield from response.follow_all(author_page_links, self.parse_author)\n\n        pagination_links = response.css(\"li.next a\")\n        yield from response.follow_all(pagination_links, self.parse)\n\n    def parse_author(self, response):\n        def extract_with_css(query):\n            return response.css(query).get(default=\"\").strip()\n\n        yield {\n            \"name\": extract_with_css(\"h3.author-title::text\"),\n            \"birthdate\": extract_with_css(\".author-born-date::text\"),\n            \"bio\": extract_with_css(\".author-description::text\"),\n        }\n```\n\n----------------------------------------\n\nTITLE: Updating Deprecated cgi.parse_qsl Usage in Scrapy\nDESCRIPTION: Replaces the deprecated cgi.parse_qsl function with six's parse_qsl for better compatibility.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_74\n\nLANGUAGE: Python\nCODE:\n```\n# Old code\nfrom cgi import parse_qsl\n\n# New code\nfrom six.moves.urllib.parse import parse_qsl\n```\n\n----------------------------------------\n\nTITLE: Compiling Documentation to HTML\nDESCRIPTION: Builds the Scrapy documentation in HTML format using make. Generated output will be placed in the build/html directory.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/README.rst#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Viewing Compiled Documentation\nDESCRIPTION: Opens the previously generated HTML documentation in the default web browser for easy viewing.\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/README.rst#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake htmlview\n```\n\n----------------------------------------\n\nTITLE: File Path Usage Example - Shell\nDESCRIPTION: Example showing proper local file path usage in Scrapy shell\nSOURCE: https://github.com/scrapy/scrapy/blob/master/docs/news.rst#2025-04-17_snippet_60\n\nLANGUAGE: shell\nCODE:\n```\nscrapy shell ./index.html # Use ./ for local files\n```"
  }
]
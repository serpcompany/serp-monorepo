[
  {
    "owner": "dagster-io",
    "repo": "dagster",
    "content": "TITLE: Basic Asset Path Selection in Shell\nDESCRIPTION: Selects all assets on the path between raw_data_b and summary_stats_2 assets\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkey:\"raw_data_b\"+ and +key:\"summary_stats_2\"\n```\n\n----------------------------------------\n\nTITLE: Creating a New Dagster Project with Shell Commands\nDESCRIPTION: This snippet demonstrates how to initialize a new Dagster project using the 'dg init' command. It creates project files and offers the option to set up a virtual environment using 'uv sync'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/1-scaffolding-project.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg init my-project\n\nCreating a Dagster project at /.../my-project.\nScaffolded files for Dagster project at /.../my-project.\nRun uv sync? This will create the virtual environment you need to activate in order to work on this project. (y/n) [y]: Running `uv sync`...\n...\n```\n\n----------------------------------------\n\nTITLE: Passing Data Between Assets Using I/O Managers in Python\nDESCRIPTION: This example shows how to use Dagster's I/O managers to implicitly handle data passing between assets. It uses a DuckDBPandasIOManager to manage reading and writing data to a local DuckDB file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/passing-data-between-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Content of passing-data-io-manager.py file\n\n# Code example not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Project from Example\nDESCRIPTION: Commands to create a new Dagster project using an official example template.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/creating-a-new-project.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster\ndagster project from-example \\\n  --name my-dagster-project \\\n  --example quickstart_etl\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Database Schema in PostgreSQL\nDESCRIPTION: Complete PostgreSQL database schema definition for the Dagster workflow orchestration system. It includes table definitions for storing pipeline runs, events, snapshots, assets, jobs, and other core Dagster concepts along with their sequences and relationships.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n--\n-- PostgreSQL database dump\n--\n\n-- Dumped from database version 13.1\n-- Dumped by pg_dump version 13.3\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\nSET default_tablespace = '';\n\nSET default_table_access_method = heap;\n\n--\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\n\nALTER TABLE public.alembic_version OWNER TO test;\n\n--\n-- Name: asset_keys; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.asset_keys OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n\n\n--\n-- Name: bulk_actions; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.bulk_actions (\n    id integer NOT NULL,\n    key character varying(32) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\n\nALTER TABLE public.bulk_actions OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.bulk_actions_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.bulk_actions_id_seq OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.bulk_actions_id_seq OWNED BY public.bulk_actions.id;\n\n\n--\n-- Name: daemon_heartbeats; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\n\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n\n--\n-- Name: event_logs; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\n\n\nALTER TABLE public.event_logs OWNER TO test;\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\n\n\n--\n-- Name: job_ticks; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.job_ticks OWNER TO test;\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.job_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.job_ticks_id_seq OWNER TO test;\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.job_ticks_id_seq OWNED BY public.job_ticks.id;\n\n\n--\n-- Name: jobs; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.jobs OWNER TO test;\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.jobs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.jobs_id_seq OWNER TO test;\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.jobs_id_seq OWNED BY public.jobs.id;\n\n\n--\n-- Name: run_tags; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\n\n\nALTER TABLE public.run_tags OWNER TO test;\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\n\n\n--\n-- Name: runs; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.runs OWNER TO test;\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\n\n\n--\n-- Name: secondary_indexes; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying(512),\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n\n\nALTER TABLE public.secondary_indexes OWNER TO test;\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.secondary_indexes_id_seq OWNER TO test;\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.secondary_indexes_id_seq OWNED BY public.secondary_indexes.id;\n\n\n--\n-- Name: snapshots; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n\n\nALTER TABLE public.snapshots OWNER TO test;\n\n--\n-- Name: snapshots_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.snapshots_id_seq OWNER TO test;\n\n--\n-- Name: snapshots_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\n\n\n--\n-- Name: asset_keys id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n\n\n--\n-- Name: bulk_actions id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.bulk_actions ALTER COLUMN id SET DEFAULT nextval('public.bulk_actions_id_seq'::regclass);\n\n\n--\n-- Name: event_logs id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\n\n--\n-- Name: job_ticks id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n\n\n--\n-- Name: jobs id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n\n\n--\n-- Name: run_tags id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\n\n--\n-- Name: runs id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\n\n--\n-- Name: secondary_indexes id; Type: DEFAULT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\n\n--\n-- Name: snapshots id; Type: DEFAULT; Schema: public; Owner: test\n--\n```\n\n----------------------------------------\n\nTITLE: Complete Dagster-Snowflake Integration Example\nDESCRIPTION: Full Python code example showcasing the integration of Snowflake with Dagster, including configuration, asset definitions, and data processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster-io-managers.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import Definitions, asset\nfrom dagster_snowflake_pandas import SnowflakePandasIOManager\nfrom sklearn.datasets import load_iris\n\n@asset\ndef iris_dataset() -> pd.DataFrame:\n    iris = load_iris()\n    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    df.columns = [col.replace(\" (cm)\", \"\").replace(\" \", \"_\") for col in df.columns]\n    return df\n\n@asset\ndef iris_cleaned(iris_dataset: pd.DataFrame) -> pd.DataFrame:\n    return iris_dataset[iris_dataset[\"sepal_length\"] > 5.0]\n\ndefs = Definitions(\n    assets=[iris_dataset, iris_cleaned],\n    resources={\n        \"io_manager\": SnowflakePandasIOManager(\n            account=\"your-account\",\n            user=\"${env:SNOWFLAKE_USER}\",\n            password=\"${env:SNOWFLAKE_PASSWORD}\",\n            database=\"PLANTS\",\n            warehouse=\"PLANTS\",\n            schema=\"FLOWERS\",\n            role=\"writer\",\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Op Jobs with Config Mapping in Python\nDESCRIPTION: Demonstrates how to use ConfigMapping to expose a narrower config interface for an op job, translating it into config for all the job's ops and resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/op-jobs.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, Config, ConfigMapping\n\n@op(config_schema={\"x\": int})\ndef my_op(context):\n    return context.op_config[\"x\"]\n\ndef my_config_fn(outer_context):\n    return {\"ops\": {\"my_op\": {\"config\": {\"x\": outer_context[\"y\"] + 1}}}}\n\n@job(\n    config=ConfigMapping(\n        config_schema={\"y\": int},\n        config_fn=my_config_fn,\n    )\n)\ndef my_job():\n    my_op()\n```\n\n----------------------------------------\n\nTITLE: Installing and Scaffolding Dagster Project\nDESCRIPTION: Commands to install Dagster and create a new project using the default project skeleton.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/creating-a-new-project.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster\ndagster project scaffold --name my-dagster-project\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster in Editable Mode with Pip\nDESCRIPTION: This command installs Dagster in editable mode, which is useful for developers working on the Dagster codebase. It allows changes to the source code to be immediately reflected without reinstallation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/2-c-pip-venv.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install --editable .\n```\n\n----------------------------------------\n\nTITLE: Implementing Kubernetes Pod Execution in Dagster\nDESCRIPTION: Example code demonstrating how to use the PipesK8sClient resource to launch a Kubernetes pod and execute external code from a Dagster asset. It shows the setup of the resource, definition of an asset, and configuration of the Kubernetes job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Definitions, OpExecutionContext\nfrom dagster_k8s import PipesK8sClient, k8s_job_config\n\n@asset\ndef my_k8s_asset(context: OpExecutionContext, k8s_client: PipesK8sClient):\n    job_name = \"my-job\"\n    namespace = \"default\"\n    image = \"my-docker-image:latest\"\n    command = [\"python\", \"-c\", \"print('Hello from Kubernetes!')\"]  # example command\n\n    k8s_job = k8s_client.launch_k8s_job(\n        job_name=job_name,\n        namespace=namespace,\n        config=k8s_job_config(\n            image=image,\n            command=command,\n        ),\n    )\n\n    # Wait for the job to complete and process events\n    for event in k8s_job.stream_events():\n        if event.is_log_message:\n            context.log.info(event.message)\n        elif event.is_asset_materialization:\n            context.log_asset_materialization(event.asset_materialization)\n\n    # Optionally, you can get the job's exit code\n    exit_code = k8s_job.get_exit_code()\n    context.log.info(f\"Job completed with exit code: {exit_code}\")\n\ndefs = Definitions(\n    assets=[my_k8s_asset],\n    resources={\n        \"k8s_client\": PipesK8sClient()\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Definitions for Kubernetes Pipes Integration\nDESCRIPTION: This Python code creates Dagster Definitions, including the Kubernetes Pipes asset and the PipesK8sClient resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/kubernetes-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# dagster_k8s_pipes.py\n\ndefs = Definitions(\n  assets=[k8s_pipes_asset],\n  resources={\n    \"k8s_pipes_client\": PipesK8sClient(),\n  },\n)\n```\n\n----------------------------------------\n\nTITLE: Dagster CLI Plugin List Output\nDESCRIPTION: Terminal output from 'dg list plugins' command showing available Dagster plugins and their components. The output is formatted in a table structure showing plugins, their symbols, summaries and available features.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/27-dg-list-plugins.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nwarnings.warn(message)\nUsing /.../jaffle-platform/.venv/bin/dagster-components\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Plugin           ┃ Objects                                                                                           ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ dagster          │ [Table showing core dagster components and features]                                              │\n│ dagster_dbt      │ [Table showing DBT integration components]                                                        │\n│ dagster_evidence │ [Table showing Evidence.dev integration components]                                               │\n│ dagster_sling    │ [Table showing Sling replication components]                                                      │\n└──────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: GitHub Issues Embeddings Generation Asset\nDESCRIPTION: Dagster asset that converts GitHub issues Documents into embeddings using OpenAI's text-embedding-3-small model and uploads them to Pinecone. Includes index creation and batch processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/embeddings.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    partitions_def=weekly_partitions,\n    compute_kind=\"openai\",\n    deps=[github_issues_raw],\n)\ndef github_issues_embeddings(\n    context, github_issues_raw, openai: OpenAIResource, pinecone: PineconeResource\n):\n    if \"github-issues\" not in pinecone.list_indexes():\n        pinecone.create_index(\n            \"github-issues\",\n            dimension=1536,\n            metric=\"cosine\",\n        )\n    index = pinecone.Index(\"github-issues\")\n\n    batch_size = 100\n    for i in range(0, len(github_issues_raw), batch_size):\n        # Get batch of documents\n        batch = github_issues_raw[i : i + batch_size]\n\n        # Create the embeddings\n        texts = [doc.page_content for doc in batch]\n        embeddings = openai.get_embeddings(texts)\n\n        # Create vectors to upload to Pinecone\n        to_upsert = []\n        for doc, embedding in zip(batch, embeddings):\n            to_upsert.append(\n                (\n                    doc.metadata[\"source\"],\n                    embedding,\n                    {\"text\": doc.page_content, **doc.metadata},\n                )\n            )\n\n        # Upload to Pinecone\n        index.upsert(vectors=to_upsert)\n```\n\n----------------------------------------\n\nTITLE: Defining Subsettable Graph-backed Asset in Python using Dagster\nDESCRIPTION: This example demonstrates how to define a subsettable graph-backed asset using the @graph_multi_asset decorator. It creates three assets: 'foo_asset', 'bar_asset', and 'baz_asset', with the ability to selectively materialize them.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/graph-backed-assets.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@graph_multi_asset(\n    outs={\"foo_asset\": AssetOut(), \"baz_asset\": AssetOut()},\n    can_subset=True,\n)\ndef my_graph():\n    foo_1, foo_2 = foo()\n    bar_result = bar(foo_2)\n    baz_result = baz(bar_result)\n    return {\"foo_asset\": foo_1, \"baz_asset\": baz_result}\n\n@op(out={\"foo_1\": Out(is_required=False), \"foo_2\": Out()})\ndef foo(context):\n    if \"foo_1\" in context.selected_output_names:\n        yield Output(1, output_name=\"foo_1\")\n    yield Output(2, output_name=\"foo_2\")\n\n@op\ndef bar(x):\n    return x + 1\n\n@op\ndef baz(x):\n    return x + 1\n```\n\n----------------------------------------\n\nTITLE: Attaching Row Count Metadata to Dagster Assets in Python\nDESCRIPTION: This example shows how to attach row count metadata to a Dagster asset as runtime metadata. It uses the MaterializeResult object to return the row count in the metadata parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/table-metadata.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MaterializeResult\n\n@asset\ndef my_asset():\n    df = get_dataframe()\n    return MaterializeResult(\n        metadata={\n            \"dagster/row_count\": len(df)\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Daily Partitioned Config in Python\nDESCRIPTION: Example demonstrating how to create a daily partitioned configuration using the daily_partitioned_config decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-ops.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@daily_partitioned_config(start_date=datetime.datetime(2020, 1, 1))\ndef my_partitioned_config(start: datetime.datetime, _end: datetime.datetime):\n    return {\n        \"ops\": {\"process_data_for_date\": {\"config\": {\"date\": start.strftime(\"%Y-%m-%d\")}}}\n    }\n```\n\n----------------------------------------\n\nTITLE: Monitoring Multiple Assets with Multi-Asset Sensor in Python\nDESCRIPTION: This example demonstrates the use of a multi-asset sensor to monitor two assets and trigger a job when both have been materialized. It shows how to set up the sensor and define the evaluation function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/asset-sensors.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, RunRequest, multi_asset_sensor\n\n@multi_asset_sensor(\n    monitored_assets=[AssetKey(\"asset_1\"), AssetKey(\"asset_2\")],\n    job=my_job,\n)\ndef my_multi_asset_sensor(context):\n    if context.all_assets_materialized():\n        return RunRequest(run_key=None)\n\n    return None\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Webserver\nDESCRIPTION: This command starts the Dagster webserver on port 3333, allowing users to access the Dagster UI through a web browser. It is essential for running Dagster's functionality, including job monitoring and execution. Ensure that the Dagster environment is set up correctly before executing this command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster-webserver/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndagster-webserver -p 3333\n```\n\n----------------------------------------\n\nTITLE: Adding Table Schema Metadata to Dagster Assets in Python\nDESCRIPTION: This example demonstrates how to attach table and column schema metadata to a Dagster asset both at definition time and runtime. It uses the TableSchema and TableColumn classes to define the schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MaterializeResult, MetadataValue\nfrom dagster.core.definitions.metadata import TableSchema, TableColumn\n\n@asset(\n    metadata={\n        \"dagster/table_name\": \"my_table\",\n        \"dagster/column_schema\": TableSchema(\n            columns=[TableColumn(name=\"id\", type=\"int\"), TableColumn(name=\"name\", type=\"string\")]\n        )\n    }\n)\ndef my_asset():\n    df = get_dataframe()\n    return MaterializeResult(\n        metadata={\n            \"dagster/column_schema\": TableSchema(\n                columns=[\n                    TableColumn(name=\"id\", type=\"int\"),\n                    TableColumn(name=\"name\", type=\"string\"),\n                    TableColumn(name=\"age\", type=\"int\")\n                ]\n            )\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Uploading Files to OpenAI using Dagster Asset\nDESCRIPTION: This snippet demonstrates how to create a Dagster asset that uploads a file to OpenAI's storage endpoint. It uses the OpenAI resource to get a client and returns the file ID provided by OpenAI after successful upload.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/open-ai-job.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(group_name=\"openai\", deps=[file_path])\ndef upload_file(context, openai):\n    file = openai.files.create(file=open(file_path.path, \"rb\"), purpose=\"fine-tune\")\n    return file.id\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using AWS Lambda with Dagster\nDESCRIPTION: This Python code demonstrates how to set up and use AWS Lambda functions within a Dagster pipeline. It includes resource configuration, function invocation, and handling of results.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/lambda.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom dagster import job, op, Failure, Output, ResourceDefinition\nfrom dagster_aws.lambda_ import PipesLambdaClient, pipes_lambda_client\n\n@op(required_resource_keys={\"lambda_client\"})\ndef invoke_lambda(context):\n    lambda_client = context.resources.lambda_client\n    try:\n        result = lambda_client.invoke(\n            \"my-lambda-function\",\n            payload=json.dumps({\"key\": \"value\"}),\n        )\n        return Output(result[\"Payload\"])\n    except Exception as e:\n        raise Failure(description=str(e))\n\n@job(resource_defs={\"lambda_client\": pipes_lambda_client})\ndef my_lambda_job():\n    invoke_lambda()\n\ndef define_lambda_job():\n    return my_lambda_job.to_job(\n        resource_defs={\n            \"lambda_client\": ResourceDefinition.hardcoded_resource(\n                PipesLambdaClient(\n                    region=\"us-east-1\",\n                    client_config={\n                        \"aws_access_key_id\": \"your-access-key\",\n                        \"aws_secret_access_key\": \"your-secret-key\",\n                    },\n                )\n            )\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining a Pythonic I/O Manager in Python\nDESCRIPTION: This snippet shows how to create a Pythonic I/O manager by subclassing ConfigurableIOManager. It implements handle_output and load_input methods with configuration fields as attributes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/defining-a-custom-io-manager.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass MyIOManager(ConfigurableIOManager):\n    base_path: str\n\n    def handle_output(self, context, obj):\n        path = self._get_path(context)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, \"w\") as f:\n            json.dump(obj, f)\n\n    def load_input(self, context):\n        path = self._get_path(context.upstream_output)\n        with open(path, \"r\") as f:\n            return json.load(f)\n\n    def _get_path(self, context):\n        return os.path.join(self.base_path, context.asset_key.path[-1] + \".json\")\n```\n\n----------------------------------------\n\nTITLE: REST API Materialization for Dagster OSS\nDESCRIPTION: Example of using REST API to report asset materialization in Dagster OSS without authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/external-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl \\\n  -X POST \\\n  -H 'Content-Type: application/json' \\\n  'http://localhost:3000/report_asset_materialization/' \\\n  -d '\n{\n  \"asset_key\": \"raw_transactions\",\n  \"metadata\": {\n    \"file_last_modified_at_ms\": 1724614700266\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Definitions from Airflow DAGs\nDESCRIPTION: Python code to create a Dagster Definitions object representing Airflow DAGs using the build_defs_from_airflow_instance function from dagster-airlift.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/peer.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_airlift.core import build_defs_from_airflow_instance\n\ndefs = Definitions(\n    assets=build_defs_from_airflow_instance(\n        instance_uri=\"http://localhost:8080\",\n        username=\"admin\",\n        password=\"admin\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dagster Job with Dask Executor in Python\nDESCRIPTION: This snippet demonstrates how to create a Dagster job using the dask_executor for local execution. It defines a simple job with two ops and configures it to use the Dask executor.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/dask.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@job(executor_def=dask_executor)\ndef hello_dask_job():\n    hello_world()\n    goodbye_world()\n```\n\n----------------------------------------\n\nTITLE: Complete DuckDB Integration Example\nDESCRIPTION: Full implementation showing asset configuration, external table integration, and downstream processing in a single file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetSpec\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n@asset\ndef iris_dataset() -> pd.DataFrame:\n    \"\"\"The iris dataset, renamed to have more readable column names.\"\"\"\n    iris = load_iris()\n    return pd.DataFrame(\n        {\n            \"sepal_length\": iris.data[:, 0],\n            \"sepal_width\": iris.data[:, 1],\n            \"petal_length\": iris.data[:, 2],\n            \"petal_width\": iris.data[:, 3],\n            \"species\": iris.target,\n        }\n    )\n\niris_harvest_data = AssetSpec(\"iris_harvest_data\")\n\n@asset\ndef iris_setosa(iris_dataset: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Only the examples of the iris dataset that are of the species setosa.\"\"\"\n    return iris_dataset[iris_dataset[\"species\"] == 0]\n```\n\n----------------------------------------\n\nTITLE: Defining ConfigurableResource for Assets in Python\nDESCRIPTION: Demonstrates defining a ConfigurableResource subclass representing an external service connection. The resource is configured in the Definitions call and used in asset definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/defining-resources.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster import ConfigurableResource, asset, Definitions\n\nclass MyExternalService(ConfigurableResource):\n    conn_str: str\n\n    def query_service(self, query: str) -> str:\n        # Use self.conn_str to connect to the service\n        ...\n\n@asset\ndef my_asset(external_service: MyExternalService):\n    result = external_service.query_service(\"SELECT * FROM my_table\")\n    return result\n\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"external_service\": MyExternalService(conn_str=\"my_connection_string\")\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Asset Relationships in Dagster\nDESCRIPTION: This mermaid diagram shows the relationships between Assets and other Dagster concepts. It illustrates how Assets interact with concepts like AssetChecks, Config, Partitions, and how they are used in Jobs, Schedules, and Sensors.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Config fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Partition fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    style Job fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Schedule fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Sensor fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style IOManager fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Resource fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style AssetCheck fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    AssetCheck -.-> Asset\n    Config -.-> Asset\n    Partition -.-> Asset\n    Resource -.-> Asset\n    IOManager -.-> Asset\n\n    Asset(Asset)\n\n    Asset -.-> Job\n    Asset -.-> Schedule\n    Asset -.-> Sensor\n    Asset ==> Definitions\n```\n\n----------------------------------------\n\nTITLE: Custom Snowflake I/O Manager for Pandas and PySpark DataFrames\nDESCRIPTION: This example demonstrates how to create a custom Snowflake I/O manager that handles both Pandas and PySpark DataFrames. It inherits from the base SnowflakeIOManager and implements type-specific handlers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake import SnowflakeIOManager\nfrom dagster_snowflake.pandas import SnowflakePandasTypeHandler\nfrom dagster_snowflake_pyspark import SnowflakePySparkTypeHandler\n\nclass CustomSnowflakeIOManager(SnowflakeIOManager):\n    def type_handlers(self):\n        return {\n            \"pandas\": SnowflakePandasTypeHandler(),\n            \"pyspark\": SnowflakePySparkTypeHandler(),\n        }\n\n    def default_load_type(self):\n        return \"pandas\"\n```\n\n----------------------------------------\n\nTITLE: Defining Assets with Dagster in Python\nDESCRIPTION: This code snippet demonstrates how to define data assets using Dagster's `@asset` decorator in Python. It showcases the creation of three assets: `country_populations`, `continent_change_model`, and `continent_stats`. Each asset is defined as a Python function, with dependencies specified through function parameters. The assets define data transformations and models, and showcases using Pandas and scikit-learn within Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n```python\nfrom dagster import asset\nfrom pandas import DataFrame, read_html, get_dummies\nfrom sklearn.linear_model import LinearRegression\n\n@asset\ndef country_populations() -> DataFrame:\n    df = read_html(\"https://tinyurl.com/mry64ebh\")[0]\n    df.columns = [\"country\", \"pop2022\", \"pop2023\", \"change\", \"continent\", \"region\"]\n    df[\"change\"] = df[\"change\"].str.rstrip(\"%\").str.replace(\"−\", \"-\").astype(\"float\")\n    return df\n\n@asset\ndef continent_change_model(country_populations: DataFrame) -> LinearRegression:\n    data = country_populations.dropna(subset=[\"change\"])\n    return LinearRegression().fit(get_dummies(data[[\"continent\"]]), data[\"change\"])\n\n@asset\ndef continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -> DataFrame:\n    result = country_populations.groupby(\"continent\").sum()\n    result[\"pop_change_factor\"] = continent_change_model.coef_\n    return result\n```\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Op in Python\nDESCRIPTION: Demonstrates how to define a simple op using the @op decorator. The op takes no inputs and returns a string.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op():\n    return \"Hello, World!\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster with Environment Variables in YAML\nDESCRIPTION: Examples of how to specify environment variables in YAML configuration files or config dictionaries for Dagster. Uses a special syntax with the 'env' key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"PARAMETER_NAME\": {\"env\": \"ENVIRONMENT_VARIABLE_NAME\"}\n```\n\nLANGUAGE: python\nCODE:\n```\n\"access_token\": {\"env\": \"GITHUB_ACCESS_TOKEN\"}\n```\n\n----------------------------------------\n\nTITLE: Importing Shared Python Function in Airflow\nDESCRIPTION: Example showing how to import a shared Python function from a common package in Airflow to promote code reuse during migration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/python-operator.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom shared_package.shared_module import write_to_db\n\nwrite_to_db_task = PythonOperator(\n    task_id=\"write_to_db\",\n    python_callable=write_to_db,\n    op_kwargs={\"directory\": Path(\"/path/to/files\")},\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Single Asset with @asset Decorator in Python\nDESCRIPTION: Demonstrates how to create a basic data asset using the @asset decorator to define a weekly sales report asset that logs its output.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset\ndef weekly_sales_report():\n    # Asset implementation here\n    result = \"Sales data...\"\n    print(f\"Generated report: {result}\")\n    return result\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Logic in Dagster Ops\nDESCRIPTION: This snippet demonstrates how to use the RetryRequested exception in a Dagster op to handle recoverable failures. It shows how to catch an exception and request a retry with a specified maximum number of attempts.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@op(max_retries=3)\ndef retry_op():\n    try:\n        do_something_flaky()\n    except FlakySomethingException:\n        raise RetryRequested(\n            max_retries=3,\n            message=\"Flaky operation failed, retrying\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing Static Partitioning in Dagster\nDESCRIPTION: This snippet shows how to implement static partitioning for processing data based on predefined categories. It defines static partitions for different regions and creates assets for regional sales data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, StaticPartitionsDefinition, asset\n\nregion_partitions = StaticPartitionsDefinition([\"us\", \"ca\", \"uk\", \"de\", \"fr\", \"jp\"])\n\n@asset(partitions_def=region_partitions)\ndef regional_sales_data(context: AssetExecutionContext):\n    region = context.partition_key\n    # Fetch or generate sales data for the given region\n    return {\"region\": region, \"sales\": 1000}  # placeholder\n\n@asset(partitions_def=region_partitions)\ndef regional_sales_summary(context: AssetExecutionContext, regional_sales_data):\n    # Process the regional sales data into a summary\n    return {\"region\": regional_sales_data[\"region\"], \"total_sales\": regional_sales_data[\"sales\"]}\n```\n\n----------------------------------------\n\nTITLE: Implementing Two-Dimensional Partitioning in Dagster\nDESCRIPTION: This snippet demonstrates how to implement two-dimensional partitioning for processing data along two different axes simultaneously. It defines partitions for both date and region, and creates assets and a schedule for daily regional sales data processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, DailyPartitionsDefinition, MultiPartitionsDefinition, StaticPartitionsDefinition, asset, define_asset_job, ScheduleDefinition, MultiPartitionKey\nfrom datetime import datetime, timedelta\n\ndate_partitions = DailyPartitionsDefinition(start_date=\"2024-01-01\")\nregion_partitions = StaticPartitionsDefinition([\"us\", \"ca\", \"uk\"])\n\ntwo_dimensional_partitions = MultiPartitionsDefinition({\n    \"date\": date_partitions,\n    \"region\": region_partitions,\n})\n\n@asset(partitions_def=two_dimensional_partitions)\ndef daily_regional_sales_data(context: AssetExecutionContext):\n    date_str = context.partition_key[\"date\"]\n    region = context.partition_key[\"region\"]\n    date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n    # Fetch or generate sales data for the given date and region\n    return {\"date\": date, \"region\": region, \"sales\": 1000}  # placeholder\n\n@asset(partitions_def=two_dimensional_partitions)\ndef daily_regional_sales_summary(context: AssetExecutionContext, daily_regional_sales_data):\n    # Process the daily regional sales data into a summary\n    return {\n        \"date\": daily_regional_sales_data[\"date\"],\n        \"region\": daily_regional_sales_data[\"region\"],\n        \"total_sales\": daily_regional_sales_data[\"sales\"]\n    }\n\ndaily_regional_sales_job = define_asset_job(\n    \"daily_regional_sales_job\",\n    selection=[daily_regional_sales_data, daily_regional_sales_summary]\n)\n\ndef get_partition_key_for_current_date():\n    yesterday = datetime.now().date() - timedelta(days=1)\n    return MultiPartitionKey({\"date\": yesterday.strftime(\"%Y-%m-%d\"), \"region\": \"all\"})\n\ndaily_regional_sales_schedule = ScheduleDefinition(\n    job=daily_regional_sales_job,\n    cron_schedule=\"0 1 * * *\",  # Run every day at 1:00 AM\n    default_status=\"running\",\n    execution_timezone=\"America/New_York\",\n    partition_key=get_partition_key_for_current_date,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-part Asset Keys in Python\nDESCRIPTION: Shows how to create an asset with a multi-part key using the key_prefix argument for hierarchical namespacing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstart_marker\n@asset(key_prefix=[\"my_database\", \"some_schema\"])\ndef my_table():\n    \"\"\"This asset will have the key ['my_database', 'some_schema', 'my_table']\"\"\"\n    ...\nend_marker\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Asset Check in Python with Dagster\nDESCRIPTION: Demonstrates how to define a single asset check that verifies the order_id column contains no null values. Uses the @asset_check decorator to define the check that runs after asset materialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/asset-checks.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, asset_check\n\n@asset\ndef orders():\n    return pd.DataFrame({\"order_id\": [1, 2, 3], \"item_id\": [\"a\", \"b\", \"c\"]})\n\n@asset_check(asset=orders)\ndef orders_id_has_no_nulls(_):\n    df = orders()\n    if df[\"order_id\"].isnull().any():\n        return AssetCheckResult(success=False, metadata={\"null_count\": df[\"order_id\"].isnull().sum()})\n```\n\n----------------------------------------\n\nTITLE: Adding Production Code References in Dagster+\nDESCRIPTION: Illustrates how to configure Dagster+ to automatically annotate assets with code references linked to source control systems like GitHub or GitLab.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, load_assets_from_modules\nfrom . import assets\n\ndefs = Definitions(\n    assets=load_assets_from_modules([assets]),\n    link_code_references_to_git=True  # Enable git code references\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Asset in Python\nDESCRIPTION: Creates a basic Dagster asset that returns a hardcoded number. This example demonstrates the default behavior of code and data versioning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset\ndef a_number():\n    return 1\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 for Compute Logs\nDESCRIPTION: Configuration for S3ComputeLogManager which writes logs to AWS S3.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster_aws.s3\n  class: S3ComputeLogManager\n  config:\n    bucket: \"my-bucket\"\n```\n\n----------------------------------------\n\nTITLE: Implementing API Client Resource with State Management in Python\nDESCRIPTION: Example showing how to set up an API client resource that manages an API token state using setup_for_execution and teardown_after_execution hooks. The resource uses private attributes to store the token and implements lifecycle methods for initialization and cleanup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/managing-resource-state.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ApiClient(ConfigurableResource):\n    username: str\n    password: str\n\n    _token: str = PrivateAttr()\n\n    def setup_for_execution(self, context: InitResourceContext) -> None:\n        # make API call to get token using username and password\n        self._token = \"my_token\"\n\n    def teardown_after_execution(self, context: InitResourceContext) -> None:\n        # clean up token\n        self._token = \"\"\n\n    def make_request(self):\n        # use token to make request\n        return f\"made request with {self._token}\"\n\n@asset(required_resource_keys={\"api_client\"})\ndef my_asset(api_client: ApiClient):\n    return api_client.make_request()\n```\n\n----------------------------------------\n\nTITLE: Implementing Column Lineage in Dagster Asset\nDESCRIPTION: Example demonstrating how to define column-level lineage in a Dagster asset using MaterializeResult and TableColumnLineage. The code shows how to specify dependencies between columns across different assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/column-level-lineage.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef my_asset():\n    # Asset computation logic here\n    return MaterializeResult(\n        metadata={\n            \"dagster/column_lineage\": TableColumnLineage(\n                deps_by_column={\n                    \"new_column_foo\": [\n                        TableColumnDep(\n                            asset_key=[\"source_bar\"],\n                            column_name=\"column_bar\",\n                        ),\n                        TableColumnDep(\n                            asset_key=[\"source_baz\"],\n                            column_name=\"column_baz\",\n                        ),\n                    ],\n                    \"new_column_qux\": [\n                        TableColumnDep(\n                            asset_key=[\"source_bar\"],\n                            column_name=\"column_quuz\",\n                        ),\n                    ],\n                }\n            )\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Assets for ETL Pipeline\nDESCRIPTION: Python code defining the Dagster assets for the ETL pipeline, including data extraction, transformation, and loading.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/quickstart.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset\n\n@asset\ndef process_data():\n    # Extract: Read data from CSV\n    df = pd.read_csv('data/sample_data.csv')\n    \n    # Transform: Add age_group column\n    df['age_group'] = pd.cut(df['age'], bins=[0, 30, 40, 100], labels=['Young', 'Middle', 'Senior'])\n    \n    # Load: Save processed data to CSV\n    df.to_csv('data/processed_data.csv', index=False)\n    \n    return df\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Multiple Outputs in Python\nDESCRIPTION: Shows how to define an op with multiple named outputs using the 'out' parameter of the @op decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op(out={\"x\": Out(), \"y\": Out()})\ndef my_op():\n    yield Output(1, output_name=\"x\")\n    yield Output(2, output_name=\"y\")\n```\n\n----------------------------------------\n\nTITLE: Attaching Definition-Time Metadata to Dagster Assets in Python\nDESCRIPTION: This code demonstrates how to add rich metadata to a Dagster asset at definition time. It includes examples of attaching a Markdown description, a path, and a JSON object as metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MetadataValue\n\n@asset(\n    metadata={\n        \"description\": MetadataValue.md(\"# My Asset\\n\\nThis asset does some important stuff.\"),\n        \"path\": MetadataValue.path(\"/path/to/file\"),\n        \"row_count\": 1000,\n        \"complex_metadata\": MetadataValue.json({\"key\": \"value\"})\n    }\n)\ndef my_asset():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Monitoring for ML Model Performance in Python\nDESCRIPTION: This snippet demonstrates how to implement conditional monitoring for a machine learning model in Dagster. It evaluates the new model against the previous model's accuracy and returns the model only if the accuracy has improved.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/managing-ml.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef ml_model(context, data):\n    new_model = train_model(data)\n    new_accuracy = evaluate_model(new_model, data)\n    \n    try:\n        previous_accuracy = context.instance.get_asset_materialization(\n            AssetKey(\"ml_model\")\n        ).metadata[\"accuracy\"]\n        if new_accuracy > previous_accuracy:\n            context.log.info(\"New model performs better. Updating.\")\n            return new_model\n        else:\n            context.log.info(\"New model does not perform better. Keeping old model.\")\n            return None\n    except:\n        context.log.info(\"No previous model found. Using new model.\")\n        return new_model\n```\n\n----------------------------------------\n\nTITLE: Full Example of Dynamic Job Processing in Python with Dagster\nDESCRIPTION: This comprehensive example demonstrates a full implementation of a dynamic job in Dagster. It simulates processing a large dataset in chunks, showcasing dynamic outputs, mapping, and collection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DynamicOut, DynamicOutput, job, op\n\n@op(out=DynamicOut())\ndef split_chunks():\n    yield DynamicOutput(value=[1, 2, 3], mapping_key=\"chunk_1\")\n    yield DynamicOutput(value=[4, 5, 6], mapping_key=\"chunk_2\")\n    yield DynamicOutput(value=[7, 8, 9], mapping_key=\"chunk_3\")\n\n@op\ndef process_chunk(chunk):\n    return [x * 2 for x in chunk]\n\n@op\ndef combine_processed_chunks(processed_chunks):\n    return sum([sum(chunk) for chunk in processed_chunks])\n\n@job\ndef dynamic_job():\n    chunks = split_chunks()\n    processed = chunks.map(process_chunk)\n    combine_processed_chunks(processed.collect())\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Branching in a Graph\nDESCRIPTION: Demonstrates how to create a graph with conditional execution using branching logic and yielding Output objects.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef branching_op(branch: str):\n    if branch == \"a\":\n        yield Output(1, \"branch_1\")\n    else:\n        yield Output(2, \"branch_2\")\n\n@op\ndef echo(x):\n    return x\n\n@graph\ndef branching():\n    x = branching_op()\n    echo(x.branch_1)\n    echo(x.branch_2)\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Asset With Resources in Python\nDESCRIPTION: Example of testing a Dagster asset that uses resources by creating and using mock resources to avoid external service dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/unit-testing-assets-and-ops.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef test_asset_with_resource():\n    class MyResource:\n        def get_data(self):\n            return [1, 2, 3]\n\n    @asset\n    def my_asset(my_resource: MyResource):\n        return my_resource.get_data()\n\n    mock_resource = MyResource()\n    assert my_asset(mock_resource) == [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Applying Hooks to All Ops in a Dagster Job\nDESCRIPTION: This example shows how to apply a failure hook to every op in a Dagster job using the @job decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-hooks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op\n\n@op\ndef my_op():\n    pass\n\n@job(hooks={slack_message_on_failure})\ndef my_job():\n    my_op()\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Schemas in Dagster Config\nDESCRIPTION: Demonstrates how to nest config schemas within each other or within basic Python data structures. This enables complex hierarchical configuration patterns with strongly typed objects.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass UserData(Config):\n    age: int\n    email: str\n\nclass MyAssetConfig(Config):\n    users: Dict[str, UserData]\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    config = MyAssetConfig(\n        users={\n            \"Alice\": UserData(age=30, email=\"alice@example.com\"),\n            \"Bob\": UserData(age=40, email=\"bob@example.com\"),\n        }\n    )\n    assert config.users[\"Alice\"].age == 30\n    assert config.users[\"Bob\"].email == \"bob@example.com\"\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Input in Python\nDESCRIPTION: Shows how to define an op that takes an input argument. The input is automatically inferred from the function signature.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op(x):\n    return x * 2\n```\n\n----------------------------------------\n\nTITLE: Basic Sensor Implementation in Python\nDESCRIPTION: Demonstrates a basic sensor implementation that checks for new files and triggers a job run when files are found. Uses the @sensor decorator and includes run key implementation for deduplication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/automation/simple-sensor-example.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Asset Sensor in Python\nDESCRIPTION: This snippet demonstrates how to create a basic asset sensor that triggers a job when an asset is materialized. It monitors the 'daily_sales_data' asset and triggers the 'process_sales_data' job upon materialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/asset-sensors.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSelection, RunRequest, define_asset_job, job, op, asset_sensor\n\n@asset_sensor(asset_key=[\"daily_sales_data\"], job=process_sales_data_job)\ndef daily_sales_data_sensor(context, asset_event):\n    return RunRequest(\n        run_key=context.cursor,\n        run_config={\"ops\": {\"process_sales_data\": {\"config\": {\"date\": asset_event.dagster_event.partition}}}},\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing a Static Job in Python with Dagster\nDESCRIPTION: This snippet demonstrates a basic static job in Dagster with a single expensive operation. It illustrates the limitations of this approach when dealing with large datasets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef expensive_processing():\n    for i in range(1000000):\n        do_some_processing(i)\n\n@job\ndef non_dynamic_job():\n    expensive_processing()\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Asset With Config in Python\nDESCRIPTION: Shows how to test a Dagster asset that requires configuration by constructing and passing a config object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/unit-testing-assets-and-ops.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef test_asset_with_config():\n    class MyConfig(Config):\n        x: int\n\n    @asset\n    def my_asset(config: MyConfig):\n        return config.x * 2\n\n    assert my_asset(MyConfig(x=3)) == 6\n```\n\n----------------------------------------\n\nTITLE: Ingesting Hacker News Data in Python using Dagster\nDESCRIPTION: Creates an asset that retrieves and processes recent Hacker News records, filtering for valid stories and returning them as a pandas DataFrame.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/ml-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nrecent_stories = asset()\ndef get_recent_stories() -> pd.DataFrame:\n    stories = pd.DataFrame()\n    for story_id in range(max_story_id - num_stories, max_story_id):\n        record = request_story(story_id)\n        if record and record.get(\"title\") and \"Ask HN\" not in record.get(\"title\"):\n            stories = pd.concat([stories, pd.DataFrame.from_records([record])])\n    return stories\n```\n\n----------------------------------------\n\nTITLE: Defining Freshness Check for Materializable Assets in Python\nDESCRIPTION: This snippet demonstrates how to define a freshness check for a materializable asset in Dagster. It creates a check that fails if the asset's latest materialization occurred more than one hour before the current time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/data-freshness-testing.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSelection, Definitions, asset\nfrom dagster.core.asset_checks import build_last_update_freshness_checks\nfrom datetime import timedelta\n\n@asset\ndef my_asset():\n    ...\n\nfreshness_check = build_last_update_freshness_checks(\n    AssetSelection.assets(my_asset),\n    maximum_lag=timedelta(hours=1),\n)\n\ndefs = Definitions(\n    assets=[my_asset],\n    asset_checks=[freshness_check],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple Assets with @multi_asset Decorator in Python\nDESCRIPTION: Shows how to generate multiple assets from a single operation using the @multi_asset decorator, producing two related assets from one function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dg.multi_asset(outs={\"asset_one\": AssetOut(), \"asset_two\": AssetOut()})\ndef my_multi_asset():\n    # Implementation that produces two assets\n    return {\"asset_one\": \"first result\", \"asset_two\": \"second result\"}\n```\n\n----------------------------------------\n\nTITLE: Text Vectorization for ML Features using TfidfVectorizer\nDESCRIPTION: Transforms text data into numerical features using TF-IDF vectorization, preparing the data for model training.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/ml-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(deps=[training_test_data])\ndef vectorized_data(training_test_data: TrainingTestData) -> VectorizedData:\n    vectorizer = TfidfVectorizer()\n    X_train = vectorizer.fit_transform(training_test_data.X_train)\n    X_test = vectorizer.transform(training_test_data.X_test)\n    return VectorizedData(\n        X_train=X_train.toarray(),\n        X_test=X_test.toarray(),\n        y_train=training_test_data.y_train.values,\n        y_test=training_test_data.y_test.values,\n        vectorizer=vectorizer,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Cron-based Materialization for ML Model Asset in Python\nDESCRIPTION: This code shows how to set up cron-based materialization for a machine learning model asset in Dagster. It uses the on_cron condition to update the asset on a schedule, but only after upstream dependencies have been updated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/managing-ml.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    deps=[data_asset],\n    auto_materialize_policy=AutoMaterializePolicy.on_cron(\n        \"0 0 * * *\"  # every day at midnight\n    )\n)\ndef ml_model(data_asset):\n    # model training code\n    return model\n```\n\n----------------------------------------\n\nTITLE: Creating NREL API Resource in Dagster\nDESCRIPTION: Defines a custom resource class that interfaces with the NREL API to retrieve alternative fuel station data. The resource includes a method to fetch nearby stations based on latitude, longitude, and fuel type parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/custom-resource.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample\\n  path=\"docs_projects/project_prompt_eng/project_prompt_eng/resources.py\"\\n  language=\"python\"\\n  startAfter=\"start_resource\"\\n  endBefore=\"end_resource\"\\n/>\n```\n\n----------------------------------------\n\nTITLE: Implementing GitHub API Client as Dagster Resource\nDESCRIPTION: Demonstrates how to set up a bare Python object (GitHub API client) as a Dagster resource and use it in an asset definition. The example shows the usage of ResourceParam annotation to properly identify the resource parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/using-bare-python-objects-as-resources.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom github import Github\nfrom typing import Annotated\n\n# Create a Github client\ngithub_client = Github(\"access_token\")\n\n@asset\ndef github_stars(github: Annotated[Github, ResourceParam()]):\n    \"\"\"Get the number of stars on the dagster repo\"\"\"\n    repo = github.get_repo(\"dagster-io/dagster\")\n    return {\"stars\": repo.stargazers_count}\n```\n\n----------------------------------------\n\nTITLE: Defining a Downstream Asset with Dependencies in Dagster\nDESCRIPTION: Creates a downstream asset called 'joined_data' that depends on three upstream assets: sales_data, sales_reps, and products. The asset uses SQL to join these tables together in DuckDB and selects relevant columns for analysis.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-a-downstream-asset.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    group=\"transformed\",\n    deps=[AssetKey(\"sales_data\"), AssetKey(\"sales_reps\"), AssetKey(\"products\")],\n)\ndef joined_data(duckdb, sales_data, sales_reps, products):\n    \"\"\"Table with sales data joined with sales rep and product information.\"\"\"\n    # Establish our database connection\n    con = duckdb.sql(\"CREATE OR REPLACE VIEW joined_data as \"\n               \"SELECT \"\n               \"s.id as sale_id, \"\n               \"s.price, \"\n               \"s.quantity, \"\n               \"s.timestamp as sale_date, \"\n               \"sr.name as sales_rep_name, \"\n               \"sr.region as region, \"\n               \"p.name as product_name, \"\n               \"p.category as product_category \"\n               \"FROM sales_data s \"\n               \"LEFT JOIN sales_reps sr ON s.sales_rep_id = sr.id \"\n               \"LEFT JOIN products p ON s.product_id = p.id;\")\n\n    # Validating our view was created\n    result = duckdb.sql(\"SELECT * FROM joined_data LIMIT 5\").fetchall()\n    \n    # Compute and log some simple metrics - we'll use this later\n    total_sales = duckdb.sql(\n        \"SELECT COUNT(*) as total_sales, \"\n        \"SUM(price * quantity) as total_revenue \"\n        \"FROM joined_data\"\n    ).fetchone()\n    \n    logger = get_dagster_logger()\n    logger.info(f\"Total sales: {total_sales[0]}\")\n    logger.info(f\"Total revenue: ${total_sales[1]:.2f}\")\n    \n    return result\n```\n\n----------------------------------------\n\nTITLE: Creating Sales Reps Asset in Dagster\nDESCRIPTION: Defines a sales_reps asset that loads data from a CSV file into a DuckDB table. Similar to the products asset, it includes metadata for the Dagster UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(group_name=\"bronze\", compute_kind=\"duckdb\")\ndef sales_reps(duckdb: DuckDBResource):\n    \"\"\"Table that holds the sales reps that work for the company\"\"\"\n\n    # First, create the table from the CSV data.\n    duckdb.execute_query(\n        '''\n        CREATE OR REPLACE TABLE sales_reps AS\n        SELECT *\n        FROM read_csv_auto('data/sales_reps.csv', header=True)\n        '''\n    )\n\n    # Next, let's return a preview of the table to show in the UI\n    sales_reps_df = duckdb.execute_query(\"SELECT * FROM sales_reps\").fetch_df()\n    num_records = len(sales_reps_df)\n\n    return dg.MaterializeResult(\n        metadata={\n            \"num_records\": num_records,\n            \"preview\": dg.MetadataValue.md(sales_reps_df.head().to_markdown()),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Dependencies\nDESCRIPTION: Command for installing Dagster and required packages including the webserver, pandas for data manipulation, and the DuckDB integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-webserver pandas dagster-duckdb\n```\n\n----------------------------------------\n\nTITLE: Updating Dagster Definitions with Filtered Asset in Python\nDESCRIPTION: Updating the Dagster Definitions object to include only the filtered 'load_customers' asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[load_customers_asset],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Complex Asset with @graph_asset Decorator in Python\nDESCRIPTION: Illustrates how to create a complex asset from multiple operations using the @graph_asset decorator, combining step_one and step_two into a single asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dg.graph_asset\ndef complex_asset():\n    step_two(step_one())\n```\n\n----------------------------------------\n\nTITLE: Creating a Project Directory for Dagster ETL Tutorial\nDESCRIPTION: Commands for creating a new directory for the Dagster ETL project and navigating into it. This is the first step in setting up the environment for the Dagster ETL tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir dagster-etl-tutorial\ncd dagster-etl-tutorial\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Context Loader for Dagster Pipes\nDESCRIPTION: Example of creating a custom context loader that reads from a cloud service key/value store.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/dagster-pipes-details-and-customization.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom cloud_service import read_from_kv_store\nfrom dagster_pipes import PipesContextLoader\n\nclass CloudServiceContextLoader(PipesContextLoader):\n    def load_context(self, bootstrap_params: dict) -> str:\n        return read_from_kv_store(bootstrap_params[\"context_path\"])\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting Local Dagster Cloud Environment\nDESCRIPTION: Commands to install development dependencies and start a local Dagster Cloud instance. The installation uses pip to install dependencies in editable mode with development extras.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/external_assets/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables and Extras in External Process\nDESCRIPTION: Demonstrates how to access environment variables and extra parameters in external code via PipesContext\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Received extras: {context.extras['my_param']}\")\nprint(f\"Received env var: {context.env['MY_ENV_VAR']}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Executable Multi-Asset for Airflow DAG in Python\nDESCRIPTION: Creates a multi-asset function that executes an Airflow DAG using an AirflowInstance and materializes the result in Dagster. It triggers the 'customer_metrics' DAG and handles success or failure outcomes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/federate-execution.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@multi_asset()\ndef run_customer_metrics(context, airflow_instance):\n    run = airflow_instance.trigger_dag(\n        \"customer_metrics\", conf={}, execution_date=context.get_current_time()\n    )\n    if run.state == \"success\":\n        yield AssetMaterialization(asset_key=\"customer_metrics\")\n    else:\n        raise Exception(f\"DAG run failed with state {run.state}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Asset Checks for OpenAI Training Files in Python\nDESCRIPTION: This code defines asset checks for both training and validation files to ensure they meet OpenAI's format requirements. The checks are connected to their respective file assets and validate the JSONL format before submission to the OpenAI API.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/file-creation.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset_check(asset=training_file)\ndef check_training_file():\n    return validate_file(\"training_data.jsonl\")\n\n\n@asset_check(asset=validation_file)\ndef check_validation_file():\n    return validate_file(\"validation_data.jsonl\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Fixed Fan-in Graph in Python\nDESCRIPTION: Shows how to create a graph with a fixed set of ops that all return the same output type, collected into a list and passed to a single downstream op.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef return_one():\n    return 1\n\n@op\ndef sum_fan_in(nums):\n    return sum(nums)\n\n@graph\ndef fan_in_graph():\n    ones = [return_one() for _ in range(10)]\n    return sum_fan_in(ones)\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Materialization to Dagster+ using Python Requests\nDESCRIPTION: Python example using the requests library to report an asset materialization to Dagster+. Includes authentication, JSON payload preparation, and response handling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = \"https://{ORGANIZATION}.dagster.cloud/{DEPLOYMENT_NAME}/report_asset_materialization/\"\n\npayload = {\n    \"asset_key\": \"ASSET_KEY\",\n    \"metadata\": {\"rows\": 10},\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Dagster-Cloud-Api-Token\": \"TOKEN\"\n}\n\nresponse = requests.request(\"POST\", url, json=payload, headers=headers)\nresponse.raise_for_status()\n```\n\n----------------------------------------\n\nTITLE: Testing Run Status Sensor with Context in Python using Dagster\nDESCRIPTION: This snippet shows how to execute a test job, retrieve the success event, and build a context for testing a run status sensor. It uses build_run_status_sensor_context to create the appropriate context object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-run-status-sensors.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import build_run_status_sensor_context\n\nresult = my_job.execute_in_process()\njob_success_event = result.get_job_success_event()\n\ncontext = build_run_status_sensor_context(\n    sensor_name=\"my_run_success_sensor\",\n    dagster_instance=result.instance,\n    run=result.dagster_run,\n    job_state=job_success_event.job_state,\n    run_status=DagsterRunStatus.SUCCESS,\n)\n\nmy_run_success_sensor(context)\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Asset to Load CSV to Snowflake\nDESCRIPTION: This Python code defines a Dagster asset that reads a CSV file and loads its contents into a Snowflake table using a COPY INTO query. It utilizes the Snowflake resource to execute SQL commands and logs the completion of the data loading process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/copy_csv_to_snowflake.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"from dagster import AssetExecutionContext, Definitions, EnvVar, asset\nfrom dagster_snowflake import SnowflakeResource\n\n\n@asset\ndef load_csv_to_snowflake(context: AssetExecutionContext, snowflake: SnowflakeResource):\n\n    file_name = \\\"example.csv\\\"\n    file_path = Path(__file__).parent / file_name\n    table_name = \\\"example_table\\\"\n\n    create_format = \\\"\\\"\\\"\n    create or replace file format csv_format\n        type = 'CSV',\n        field_optionally_enclosed_by = '\\\"'\n    \\\"\\\"\\\"\n\n    create_stage = \\\"\\\"\\\"\n    create or replace stage temporary_stage\n        file_format = csv_format\n    \\\"\\\"\\\"\n\n    put_file = f\\\"\\\"\\\"\n    put 'file://{file_path}' @temporary_stage\n        auto_compress=TRUE\n    \\\"\\\"\\\"\n\n    create_table = f\\\"\\\"\\\"\n    create table if not exists {table_name} (\n        user_id INT,\n        first_name VARCHAR,\n        last_name VARCHAR,\n        occupation VARCHAR\n    )\n    \\\"\\\"\\\"\n\n    copy_into = f\\\"\\\"\\\"\n    copy into {table_name}\n    from @temporary_stage/{file_name}.gz\n    file_format = csv_format;\n    \\\"\\\"\\\"\n\n    with snowflake.get_connection() as conn:\n        with conn.cursor() as curs:\n            curs.execute(create_format)\n            curs.execute(create_stage)\n            curs.execute(put_file)\n            curs.execute(create_table)\n            curs.execute(copy_into)\n\n    context.log.info(f\\\"Loaded data from {file_path} into {table_name}\\\")\n\n\ndefs = Definitions(assets=[load_csv_to_snowflake], resources={\\\"snowflake\\\": snowflake})\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Airflow DAG for Dagster proxying (Python)\nDESCRIPTION: This code snippet shows how to configure an Airflow DAG to proxy execution to Dagster. It includes a flag to enable or disable proxying.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/migrate.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.utils.dates import days_ago\nfrom dagster_airflow.proxying_to_dagster import execute_job_proxy\n\nproxied = True\n\nwith DAG(\n    dag_id=\"rebuild_customers_list\",\n    start_date=days_ago(1),\n    schedule_interval=\"@daily\",\n) as dag:\n    if proxied:\n        PythonOperator(\n            task_id=\"rebuild_customers_list\",\n            python_callable=execute_job_proxy,\n            op_kwargs={\n                \"yaml_path\": \"path/to/rebuild_customers_list.yaml\",\n            },\n        )\n    else:\n        # Original Airflow tasks\n```\n\n----------------------------------------\n\nTITLE: Handling Partitioned Assets in Custom I/O Manager\nDESCRIPTION: This example demonstrates how to handle partitioned assets in a custom I/O manager. It uses the asset_partition_key property to determine which partition is being stored or loaded.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/defining-a-custom-io-manager.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass PartitionedIOManager(IOManager):\n    def handle_output(self, context, obj):\n        partition_key = context.asset_partition_key\n        # handle storage logic for the specific partition\n\n    def load_input(self, context):\n        partition_key = context.asset_partition_key\n        # handle loading logic for the specific partition\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Asset Dependencies in Python with Dagster\nDESCRIPTION: Example showing how to create dependent assets using the @asset decorator. Creates a sugary_cereals asset that depends on cereals table, and a shopping_list asset that depends on sugary_cereals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets-with-asset-dependencies.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef cereals():\n    return pd.read_csv(\"cereals.csv\")\n\n@asset(deps=[cereals])\ndef sugary_cereals(cereals):\n    return cereals[cereals[\"sugars\"] > 8]\n\n@asset(deps=[sugary_cereals])\ndef shopping_list(sugary_cereals):\n    sugary_cereals_with_store = sugary_cereals.assign(\n        store=[\"Walmart\" if x % 2 == 0 else \"Target\" for x in range(len(sugary_cereals))]\n    )\n    return sugary_cereals_with_store\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Dagster Data Passing Examples\nDESCRIPTION: This bash command installs the necessary Python packages to run the code examples in this guide. It includes Dagster and the DuckDB-Pandas integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/passing-data-between-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-duckdb-pandas\n```\n\n----------------------------------------\n\nTITLE: Configuring Run Retries in YAML Configuration\nDESCRIPTION: YAML configuration example for enabling run retries in Dagster Open Source via dagster.yaml, setting a maximum of 3 retries for all runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-retries.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrun_retries:\n  enabled: true # Omit this key if using Dagster+, since run retries are enabled by default\n  max_retries: 3\n```\n\n----------------------------------------\n\nTITLE: Implementing Assets Without I/O Managers in Python\nDESCRIPTION: Example showing asset implementation with manual data reading and writing to DuckDB without using I/O managers. The code demonstrates connecting to DuckDB, reading data, and writing transformed data directly in the asset functions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nimport pandas as pd\nfrom dagster import asset\n\n@asset\ndef make_duck_connection():\n    return duckdb.connect(\"/path/to/my/db\")\n\n@asset\ndef raw_sales_data(make_duck_connection):\n    df = pd.read_csv(\"path/to/sales.csv\")\n    make_duck_connection.execute(\n        \"create table if not exists raw_sales_data as select * from df\"\n    )\n    return df\n\n@asset\ndef clean_sales_data(make_duck_connection, raw_sales_data):\n    # get data from upstream table\n    df = make_duck_connection.execute(\n        \"select * from raw_sales_data\"\n    ).df()\n    \n    # transform data\n    cleaned = df.dropna()\n    \n    # write to new table\n    make_duck_connection.execute(\n        \"create table if not exists clean_sales_data as select * from cleaned\"\n    )\n    return cleaned\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Partitioning in Dagster\nDESCRIPTION: This snippet illustrates how to implement dynamic partitioning for creating partitions based on runtime information. It defines dynamic partitions for regions and creates assets and a sensor for processing sales data for dynamically added regions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, DynamicPartitionsDefinition, DynamicPartitionKey, asset, define_asset_job, DagsterRunStatus, RunRequest, SensorResult, sensor\n\nregion_partitions = DynamicPartitionsDefinition(name=\"region\")\n\n@asset(partitions_def=region_partitions)\ndef regional_sales_data(context: AssetExecutionContext):\n    region = context.partition_key\n    # Fetch or generate sales data for the given region\n    return {\"region\": region, \"sales\": 1000}  # placeholder\n\n@asset(partitions_def=region_partitions)\ndef regional_sales_summary(context: AssetExecutionContext, regional_sales_data):\n    # Process the regional sales data into a summary\n    return {\"region\": regional_sales_data[\"region\"], \"total_sales\": regional_sales_data[\"sales\"]}\n\nregional_sales_job = define_asset_job(\n    \"regional_sales_job\",\n    selection=[regional_sales_data, regional_sales_summary]\n)\n\n@sensor(job=regional_sales_job)\ndef all_regions_sensor(context):\n    all_regions = [\"us\", \"ca\", \"uk\", \"de\", \"fr\", \"jp\"]  # This could be fetched from an external system\n\n    known_partitions = context.instance.get_dynamic_partitions(\"region\")\n    new_partitions = set(all_regions) - set(known_partitions)\n\n    if new_partitions:\n        context.instance.add_dynamic_partitions(\"region\", list(new_partitions))\n\n    run_requests = []\n    for region in all_regions:\n        if context.instance.has_serialized_partition_key(region):\n            continue\n        run_requests.append(RunRequest(partition_key=DynamicPartitionKey(region)))\n\n    return SensorResult(run_requests=run_requests)\n```\n\n----------------------------------------\n\nTITLE: Defining Managed-loading Dependencies for Graph-backed Assets in Python\nDESCRIPTION: This example shows how to define managed-loading dependencies for graph-backed assets. It creates three assets: 'upstream_asset', 'middle_asset' (which depends on 'upstream_asset'), and 'downstream_asset' (which depends on 'middle_asset').\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/graph-backed-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef upstream_asset():\n    return 5\n\n@graph_asset\ndef middle_asset(upstream_asset):\n    return do_stuff(upstream_asset)\n\n@op\ndef do_stuff(x):\n    return x + 1\n\n@asset\ndef downstream_asset(middle_asset):\n    return middle_asset + 1\n```\n\n----------------------------------------\n\nTITLE: Displaying Default Dagster Project Structure\nDESCRIPTION: This snippet shows the directory structure of a default Dagster project created using the 'dagster project scaffold' command. It includes the main project directory, tests directory, and configuration files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/dagster-project-file-reference.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n.\n├── README.md\n├── my_dagster_project\n│   ├── __init__.py\n│   ├──  assets.py\n│   └──  definitions.py\n├── my_dagster_project_tests\n├── pyproject.toml\n├── setup.cfg\n├── setup.py\n└── tox.ini\n```\n\n----------------------------------------\n\nTITLE: Creating Permissive Schemas in Dagster Config\nDESCRIPTION: Shows how to use the PermissiveConfig base class to allow arbitrary fields in config objects. This is useful when you want to allow users to specify fields that are not explicitly defined in the schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MyAssetConfig(PermissiveConfig):\n    greeting_phrase: str\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    # Can specify arbitrary fields\n    config = MyAssetConfig(greeting_phrase=\"hello\", arbitrary_field=\"value\")\n    assert config.greeting_phrase == \"hello\"\n    assert config.arbitrary_field == \"value\"\n```\n\n----------------------------------------\n\nTITLE: Recording Asset Materializations in Dagster Ops\nDESCRIPTION: This snippet demonstrates how to record asset materializations in Dagster ops by logging an AssetMaterialization event after persisting data to an external system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, AssetMaterialization, AssetKey\n\n@op\ndef my_op():\n    # ... persist some data to a file at a known path\n    path = \"/path/to/file.csv\"\n    file_size = 1337\n    log.info(f\"Persisted data to: {path}\")\n    context.log_event(\n        AssetMaterialization(\n            asset_key=AssetKey(\"my_dataset\"),\n            description=\"Persisted data to a file.\",\n            metadata={\"file_path\": path, \"size (bytes)\": file_size},\n        )\n    )\n    return path\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Dagster Asset in Python\nDESCRIPTION: Example implementation of a Dagster asset that creates and returns a DataFrame containing stock price information for different companies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/dagster-definitions.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nimport pandas as pd\n\n\n@asset\ndef stock_prices():\n    \"\"\"Stock prices for a number of companies.\"\"\"\n    return pd.DataFrame({\n        \"company\": [\"AAPL\", \"GOOGL\", \"META\"],\n        \"price\": [176.65, 142.56, 324.77],\n    })\n```\n\n----------------------------------------\n\nTITLE: Defining Data Assets in Python with Dagster\nDESCRIPTION: This snippet demonstrates how to define three interconnected data assets using Dagster's asset decorator. It includes functions for fetching country population data, creating a linear regression model, and generating continent statistics.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom pandas import DataFrame, read_html, get_dummies\nfrom sklearn.linear_model import LinearRegression\n\n@asset\ndef country_populations() -> DataFrame:\n    df = read_html(\"https://tinyurl.com/mry64ebh\")[0]\n    df.columns = [\"country\", \"pop2022\", \"pop2023\", \"change\", \"continent\", \"region\"]\n    df[\"change\"] = df[\"change\"].str.rstrip(\"%\").str.replace(\"−\", \"-\").astype(\"float\")\n    return df\n\n@asset\ndef continent_change_model(country_populations: DataFrame) -> LinearRegression:\n    data = country_populations.dropna(subset=[\"change\"])\n    return LinearRegression().fit(get_dummies(data[[\"continent\"]]), data[\"change\"])\n\n@asset\ndef continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -> DataFrame:\n    result = country_populations.groupby(\"continent\").sum()\n    result[\"pop_change_factor\"] = continent_change_model.coef_\n    return result\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Asset With Upstream Dependencies in Python\nDESCRIPTION: Demonstrates how to test a Dagster asset that depends on upstream assets by passing dependency values directly during testing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/unit-testing-assets-and-ops.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef test_asset_with_deps():\n    @asset\n    def upstream_asset():\n        return [1, 2, 3]\n\n    @asset\n    def downstream_asset(upstream_asset):\n        return [x * 2 for x in upstream_asset]\n\n    assert downstream_asset([1, 2, 3]) == [2, 4, 6]\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster with AWS CloudWatch Logger\nDESCRIPTION: This Python code snippet demonstrates how to configure Dagster to use AWS CloudWatch for logging. It includes setting up the CloudWatchLogger, defining a job, and executing it with CloudWatch logging enabled.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/cloudwatch.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\nfrom dagster import job, op, DagsterInstance\nfrom dagster_aws.cloudwatch.logger import CloudWatchLogger\n\n@op\ndef hello_cloudwatch():\n    return \"Hello, CloudWatch!\"\n\n@job\ndef hello_cloudwatch_job():\n    hello_cloudwatch()\n\nif __name__ == \"__main__\":\n    # Configure the CloudWatch logger\n    cloudwatch_logger = CloudWatchLogger(\n        log_group_name=\"/dagster/logs\",\n        log_stream_name=\"my-dagster-stream\",\n        aws_region=\"us-west-2\",\n        # Optional: provide AWS credentials if not using IAM roles\n        # aws_access_key_id=\"YOUR_ACCESS_KEY\",\n        # aws_secret_access_key=\"YOUR_SECRET_KEY\",\n    )\n\n    # Create a DagsterInstance with the CloudWatch logger\n    instance = DagsterInstance.ephemeral(event_storage=cloudwatch_logger)\n\n    # Execute the job\n    result = hello_cloudwatch_job.execute_in_process(instance=instance)\n\n    assert result.success\n```\n\n----------------------------------------\n\nTITLE: Defining and Accessing Op Configuration in Python\nDESCRIPTION: This snippet shows how to define a Config subclass for an op and access it through the config parameter in the op function body. The Config class defines a person_name field that will be provided at runtime.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/run-configuration.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass OpConfig(Config):\n    person_name: str\n\n@op\ndef op_using_config(config: OpConfig):\n    # Access the config value through the config parameter\n    return f\"Hello, {config.person_name}!\"\n\n@job\ndef config_job():\n    op_using_config()\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Kinds to a Dagster Asset\nDESCRIPTION: This snippet demonstrates how to add multiple kinds to a Dagster asset using the 'kinds' argument in the asset decorator. It shows an asset built with Python and stored in Snowflake, tagged with both 'python' and 'snowflake' kinds.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/kind-tags.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(kinds=[\"python\", \"snowflake\"])\ndef my_asset():\n    # Asset definition\n```\n\n----------------------------------------\n\nTITLE: Using Asset Context in Python\nDESCRIPTION: Shows how to use the context parameter in an asset definition to access system information like loggers and run IDs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, asset\n\n@asset\ndef context_asset(context: AssetExecutionContext):\n    context.log.info(f\"My run ID is {context.run.run_id}\")\n    ...\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Asset for Kubernetes Pipes Integration\nDESCRIPTION: This Python code defines a Dagster asset that uses PipesK8sClient to run a Kubernetes container with Dagster Pipes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/kubernetes-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# dagster_k8s_pipes.py\n\nfrom dagster import AssetExecutionContext, Definitions, asset\nfrom dagster_k8s import PipesK8sClient\n\n\n@asset\ndef k8s_pipes_asset(context: AssetExecutionContext, k8s_pipes_client: PipesK8sClient):\n  return k8s_pipes_client.run(\n      context=context,\n      image=\"pipes-example:v1\",\n      extras={\n            \"some_parameter\": 1\n      }\n  ).get_materialize_result()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using BigQueryResource in Dagster\nDESCRIPTION: Python code example demonstrating how to set up and use the BigQueryResource in a Dagster project. It includes resource configuration, job definition, and a simple operation to execute a BigQuery query.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, Definitions\nfrom dagster_gcp import BigQueryResource\n\n@op(required_resource_keys={\"bigquery\"})\ndef hello_bigquery(context):\n    client = context.resources.bigquery\n    result = client.query(\"SELECT 1\").result()\n    for row in result:\n        print(row)\n\n@job(resource_defs={\"bigquery\": BigQueryResource()})\ndef hello_bigquery_job():\n    hello_bigquery()\n\ndefs = Definitions(\n    jobs=[hello_bigquery_job],\n    resources={\n        \"bigquery\": BigQueryResource(\n            project=\"my-project\"\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI Format Validation for JSONL Files in Python\nDESCRIPTION: This function validates JSONL files against OpenAI's format requirements for fine-tuning. It checks that the file exists, can be parsed, contains messages in the correct format, has appropriate role fields, and meets content length requirements.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/file-creation.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef validate_file(file_path: str) -> AssetCheckResult:\n    # Format validation inspired by the OpenAI cookbook:\n    # https://cookbook.openai.com/examples/chat_finetuning_data_prep#format-validation\n    \n    try:\n        # Check if the file exists\n        if not os.path.exists(file_path):\n            return AssetCheckResult(\n                passed=False,\n                metadata={\"error\": f\"File {file_path} does not exist\"}\n            )\n        \n        # Check if the file can be parsed\n        dataset = []\n        try:\n            with open(file_path, \"r\") as f:\n                for line in f:\n                    dataset.append(json.loads(line))\n        except json.JSONDecodeError as e:\n            return AssetCheckResult(\n                passed=False,\n                metadata={\"error\": f\"File {file_path} contains invalid JSON: {e}\"}\n            )\n            \n        # Check if the file is empty\n        if len(dataset) == 0:\n            return AssetCheckResult(\n                passed=False,\n                metadata={\"error\": f\"File {file_path} is empty\"}\n            )\n\n        # Validate the format of each record\n        format_errors = []\n        for i, example in enumerate(dataset):\n            if not isinstance(example, dict):\n                format_errors.append(f\"Example {i} is not a dictionary\")\n                continue\n                \n            if \"messages\" not in example:\n                format_errors.append(f\"Example {i} does not contain a 'messages' field\")\n                continue\n                \n            if not isinstance(example[\"messages\"], list):\n                format_errors.append(f\"Example {i}'s 'messages' field is not a list\")\n                continue\n            \n            # Check if there are at least two messages\n            if len(example[\"messages\"]) < 2:\n                format_errors.append(f\"Example {i} has fewer than 2 messages\")\n                continue\n                \n            # Validate each message has the correct format\n            for j, message in enumerate(example[\"messages\"]):\n                if not isinstance(message, dict):\n                    format_errors.append(f\"Example {i}, message {j} is not a dictionary\")\n                    continue\n                \n                if \"role\" not in message or \"content\" not in message:\n                    format_errors.append(f\"Example {i}, message {j} does not contain 'role' or 'content' fields\")\n                    continue\n                    \n                # Validate role\n                if message[\"role\"] not in [\"system\", \"user\", \"assistant\"]:\n                    format_errors.append(f\"Example {i}, message {j} has an invalid role: {message['role']}\")\n                    continue\n\n                # Check if content is too long\n                if len(message[\"content\"]) > 32768:\n                    format_errors.append(f\"Example {i}, message {j} has content longer than 32,768 characters\")\n                    continue\n        \n        if format_errors:\n            return AssetCheckResult(\n                passed=False,\n                metadata={\"errors\": format_errors}\n            )\n            \n        return AssetCheckResult(\n            passed=True,\n            metadata={\"num_examples\": len(dataset)}\n        )\n    except Exception as e:\n        return AssetCheckResult(\n            passed=False,\n            metadata={\"error\": str(e)}\n        )\n```\n\n----------------------------------------\n\nTITLE: Scheduling dbt Models in Dagster\nDESCRIPTION: This Python code demonstrates how to schedule dbt models using Dagster. It uses the 'build_schedule_from_dbt_selection' function to create a schedule that runs all dbt models daily at 6 AM.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, ScheduleDefinition\nfrom dagster_dbt import DbtCliResource, dbt_assets\nfrom dagster_dbt.dagster_dbt_translator import DbtSelection, build_schedule_from_dbt_selection\n\ndbt_resource = DbtCliResource(project_dir=\"./basic-dbt-project\")\n\n@dbt_assets(manifest=dbt_resource.get_manifest())\ndef my_dbt_assets():\n    pass\n\ndbt_selection = DbtSelection(include=\"fqn:*\")\n\ndbt_schedule = build_schedule_from_dbt_selection(\n    [my_dbt_assets],\n    dbt_selection=dbt_selection,\n    job_name=\"run_all_dbt_models\",\n    cron_schedule=\"0 6 * * *\",\n    description=\"Run all dbt models every day at 6 AM\",\n)\n\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\"dbt\": dbt_resource},\n    schedules=[dbt_schedule]\n)\n```\n\n----------------------------------------\n\nTITLE: Returning Dynamic Outputs in Python with Dagster\nDESCRIPTION: This snippet shows how to return DynamicOutput objects as part of a list instead of yielding them. It also demonstrates the use of DynamicOutput as a generic type annotation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\n@op(out=DynamicOut())\ndef return_dynamic_outputs() -> List[DynamicOutput[int]]:\n    return [\n        DynamicOutput(value=i, mapping_key=str(i))\n        for i in range(10)\n    ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Column Validation Constraint\nDESCRIPTION: Shows how to create a custom column constraint for validating that payment amounts are divisible by five.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pandas.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DivisibleByFiveConstraint(ColumnConstraint):\n    def __init__(self):\n        message = \"Value must be divisible by 5\"\n        super().__init__(error_description=message, markdown_description=message)\n\n    def validate(self, dataframe, column_name):\n        rows_with_remainder = dataframe[dataframe[column_name] % 5 != 0]\n        if not rows_with_remainder.empty:\n            raise ColumnConstraintViolationException(\n                constraint_name=self.name,\n                constraint_description=self.error_description,\n                column_name=column_name,\n                offending_rows=rows_with_remainder,\n            )\n```\n\n----------------------------------------\n\nTITLE: Defining AI Prompt for Location and Vehicle Information in Python\nDESCRIPTION: This snippet defines a prompt template for an AI model to extract location coordinates and vehicle fuel type from user input. It structures the output as JSON for easy parsing in the data pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/prompts.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nLOCATION_PROMPT = \"\"\"Given a user's location and vehicle information, extract the latitude, longitude, and fuel type. Respond only with a JSON object containing the following fields: latitude (float), longitude (float), and fuel_type (string). For fuel type, use 'ELEC' for electric vehicles, 'GAS' for gasoline, 'DSL' for diesel, or 'HY' for hydrogen.\n\nExample input: \"I'm at the Empire State Building with my Tesla Model 3\"\nExample output: {\"latitude\": 40.7484, \"longitude\": -73.9857, \"fuel_type\": \"ELEC\"}\n\nUser input: {user_input}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Monthly Partition Definition in Dagster\nDESCRIPTION: Defines a monthly partition configuration for time-based asset partitioning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-partitioned-asset.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmonthly_partition = dg.TimeWindowPartitionsDefinition(cron_schedule=\"0 0 1 * *\", fmt=\"%Y-%m\")\n```\n\n----------------------------------------\n\nTITLE: Attaching Metadata to Config Fields in Python with Dagster\nDESCRIPTION: Demonstrates how to annotate config fields with metadata like descriptions and value ranges using the Dagster Field class. These annotations provide additional information and validation for the config fields.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyAssetConfig(Config):\n    greeting_phrase: str = Field(description=\"The phrase to greet the person with\")\n    count: int = Field(gt=0, lt=10, description=\"The number of times to greet the person\")\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    for _ in range(context.op_config.count):\n        context.log.info(\n            f\"{context.op_config.greeting_phrase}, {context.op_config.count} times\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions for Multiple Environments\nDESCRIPTION: Python code updating Dagster Definitions to handle different configurations for local, staging, and production environments based on an environment variable.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[items, comments, stories],\n    resources={\n        \"io_manager\": SnowflakePandasIOManager(\n            account=\"abc123\",\n            user=\"dagster_dev\",\n            password=\"my_password\",\n            database={\n                \"local\": \"DEVELOPMENT\",\n                \"staging\": \"STAGING\",\n                \"production\": \"PRODUCTION\",\n            }[os.getenv(\"DAGSTER_DEPLOYMENT\", \"local\")],\n            warehouse=\"USERS\",\n            schema=\"johndoe\",\n            role=\"DEVELOPER\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Delta Lake Integration Example\nDESCRIPTION: Full example showing Delta Lake configuration, asset definition, and data processing pipeline\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/using-deltalake-with-dagster.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import Definitions, asset\nfrom dagster_deltalake_pandas import DeltaLakePandasIOManager\n\n@asset\ndef iris_dataset() -> pd.DataFrame:\n    \"\"\"The iris dataset cleaned for use.\"\"\"\n    raw_data = pd.read_csv(\n        \"https://raw.githubusercontent.com/plotly/datasets/master/iris-data.csv\"\n    )\n    # Rename the first four columns\n    raw_data.columns = [\n        \"sepal_length\",\n        \"sepal_width\",\n        \"petal_length\",\n        \"petal_width\",\n        \"species\",\n    ]\n    return raw_data\n\n@asset\ndef iris_cleaned(iris_dataset: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean null values from the iris dataset.\"\"\"\n    return iris_dataset.dropna()\n\ndefs = Definitions(\n    assets=[iris_dataset, iris_cleaned],\n    resources={\n        \"io_manager\": DeltaLakePandasIOManager(\n            root_path=\"path/to/deltalake\",\n            schema=\"iris\"\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Partition-Based Asset Schedule in Python\nDESCRIPTION: Demonstrates creating a schedule from a partitioned asset job using build_schedule_from_partitioned_job. Shows the complete setup including asset definition and job configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2023-01-01\"))\ndef my_partitioned_asset():\n    \"\"\"Asset logic here\"\"\"\n\nmy_job = define_asset_job(\"my_job\", selection=[my_partitioned_asset])\n\nmy_schedule = build_schedule_from_partitioned_job(my_job)\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Asset With Multiple Parameters in Python\nDESCRIPTION: Shows how to test a Dagster asset that has multiple parameters using keyword arguments for clarity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/unit-testing-assets-and-ops.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef test_asset_with_multiple_params():\n    class MyConfig(Config):\n        x: int\n\n    @asset\n    def my_asset(upstream_asset, config: MyConfig, my_resource):\n        return upstream_asset + [config.x] + my_resource.get_data()\n\n    result = my_asset(\n        upstream_asset=[1, 2],\n        config=MyConfig(x=3),\n        my_resource=MyResource(),\n    )\n    assert result == [1, 2, 3, 1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Cross-Location Asset Dependencies in Dagster\nDESCRIPTION: Demonstrates how to define assets that depend on assets from different code locations. Shows both the producer asset in code_location_1 and the consumer asset in code_location_2.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets-with-asset-dependencies.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# code_location_1\n@repository\ndef my_repository():\n    return []\n\n@asset\ndef code_location_1_asset():\n    with open(\"my_file.json\", \"r\") as f:\n        return json.loads(f.read())\n\n# code_location_2\nfrom dagster import AssetKey\n\n@asset(deps=[AssetKey(\"code_location_1_asset\")])\ndef code_location_2_asset(code_location_1_asset):\n    return code_location_1_asset[\"some_key\"]\n```\n\n----------------------------------------\n\nTITLE: Attaching Labels to AutomationCondition in Python\nDESCRIPTION: This snippet demonstrates how to use the with_label() method to attach a descriptive label to a complex AutomationCondition in Dagster. It creates a condition that checks if any dependencies are in progress or have failed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/describing-conditions-with-labels.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AutomationCondition\n\nin_progress_or_failed_deps = AutomationCondition.any_deps_match(\n    AutomationCondition.in_progress() | AutomationCondition.failed()\n).with_label(\"Any deps in progress or failed\")\n```\n\n----------------------------------------\n\nTITLE: Passing Data Between Assets Using External Storage in Python\nDESCRIPTION: This example demonstrates how to pass data between assets using a SQLite database as external storage. It shows two assets: one that writes data to the database and another that reads from it.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/passing-data-between-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Content of passing-data-explicit.py file\n\n# Code example not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Modeling External Asset Dependencies\nDESCRIPTION: Shows how to define multiple external assets with dependencies to model a complete external data pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/external-assets.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSpec\n\nraw_transactions = AssetSpec(\n    \"raw_transactions\",\n    description=\"Raw transaction data from Stripe\",\n    metadata={\"source\": \"stripe\"},\n)\n\ntransactions = AssetSpec(\n    \"transactions\",\n    description=\"Cleaned transaction data\",\n    metadata={\"source\": \"airflow\"},\n    deps=[raw_transactions],\n)\n\ntransaction_stats = AssetSpec(\n    \"transaction_stats\",\n    description=\"Aggregate statistics about transactions\",\n    metadata={\"source\": \"airflow\"},\n    deps=[transactions],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dagster Asset with PipesSubprocessClient\nDESCRIPTION: Example of migrating a BashOperator to a Dagster asset using PipesSubprocessClient. This shows how to create an asset-decorated function that runs a bash command through a subprocess and handles the output.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/bash-operator-general.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster._utils.merger import merge_dicts\nfrom dagster._pipes import PipesSubprocessClient\n\n@asset\ndef print_date():\n    result = PipesSubprocessClient().execute_command(\n        command_args=[\"bash\", \"-c\", \"echo \\\"$(date)\\\" > /tmp/execution_date.txt\"],\n        env=merge_dicts(os.environ, {\"SOME_VAR\": \"value\"}),\n    )\n    return result.output\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Validation to Config Fields in Dagster\nDESCRIPTION: Demonstrates how to implement custom validation logic for config fields using Pydantic validators. These validators are triggered when the config class is instantiated and can enforce complex validation rules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import validator\n\nclass UserConfig(Config):\n    name: str\n    username: str\n\n    @validator(\"name\")\n    def name_must_be_bob(cls, v):\n        if v.lower() != \"bob\":\n            raise ValueError(\"name must be bob\")\n        return v.title()\n\n    @validator(\"username\")\n    def username_must_contain_underscore(cls, v):\n        if \"_\" not in v:\n            raise ValueError(\"username must contain an underscore\")\n        return v\n\n\n@asset(config_schema=UserConfig)\ndef validated_config_user(context):\n    # Here, config is guaranteed to have a name of \"Bob\" (or some capitalization of it)\n    # And username is guaranteed to have an underscore\n    context.log.info(f\"Name: {context.op_config.name}, Username: {context.op_config.username}\")\n    return context.op_config.username\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with Snowflake Resources\nDESCRIPTION: Final configuration of Dagster Definitions object with Snowflake resources and assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[iris_dataset, iris_setosa],\n    resources={\n        \"snowflake\": snowflake_resource\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Dagster Asset in Python\nDESCRIPTION: This code snippet defines a Dagster asset named 'my_asset' using the @dg.asset decorator. The function takes a context parameter of type AssetExecutionContext and returns a MaterializeResult.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/dagster-definitions/3-cat.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dagster as dg\n\n@dg.asset\ndef my_asset(context: dg.AssetExecutionContext) -> dg.MaterializeResult: ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Union Types with Discriminated Unions in Dagster Config\nDESCRIPTION: Demonstrates how to use discriminated unions to support config fields that can be one of multiple types. Each union type must be a Config subclass, and a discriminator field determines which type to use.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Cat(Config):\n    meows: int\n\nclass Dog(Config):\n    barks: int\n\nclass MyAssetConfig(Config):\n    pet: Union[Cat, Dog] = Field(discriminator=\"pet_type\")\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    # Can specify a cat\n    config1 = MyAssetConfig(pet=Cat(meows=10))\n    assert isinstance(config1.pet, Cat)\n    assert config1.pet.meows == 10\n\n    # Or a dog\n    config2 = MyAssetConfig(pet=Dog(barks=5))\n    assert isinstance(config2.pet, Dog)\n    assert config2.pet.barks == 5\n```\n\n----------------------------------------\n\nTITLE: Using API Resource in Dagster Asset\nDESCRIPTION: Demonstrates how to use the SunResource in a Dagster asset definition and include it in Definitions object\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-apis.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Definitions\n\n@asset\ndef sfo_sunrise(sun_resource: SunResource):\n    return sun_resource.get_sunrise()\n\ndefs = Definitions(\n    assets=[sfo_sunrise],\n    resources={\n        \"sun_resource\": SunResource(),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Tag Concurrency Configuration\nDESCRIPTION: Python code demonstrating how to set tag-based concurrency limits for job execution\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/job-execution.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrun_config = {\n    \"execution\": {\n        \"config\": {\n            \"multiprocess\": {\n                \"max_concurrent\": 4,\n                \"tag_concurrency_limits\": [\n                    {\"key\": \"database\", \"value\": \"redshift\", \"limit\": 2}\n                ]\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Instance in YAML\nDESCRIPTION: The dagster.yaml file is used to specify configuration for instance-level components in a Dagster deployment. This snippet shows a placeholder for the file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/oss-deployment-architecture.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndagster.yaml\n```\n\n----------------------------------------\n\nTITLE: Implementing External Python Script for Dagster Pipes\nDESCRIPTION: Example of a standalone Python script that performs external computation without direct Dagster dependencies. This script represents code that will be invoked through Dagster Pipes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    print(\"Running external computation...\")\n    # Perform computation\n    result = 42\n    print(f\"Computation complete. Result: {result}\")\n    return result\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Scaffolding New Dagster Project - CLI Command\nDESCRIPTION: Command to create a new Dagster project using the CLI scaffolding tool, which generates the initial project structure with basic files and directories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/structuring-your-dagster-project.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ dagster project scaffold --name example-dagster-project\n```\n\n----------------------------------------\n\nTITLE: Defining Asset Configuration Schema in Python with Dagster\nDESCRIPTION: Example demonstrating how to create a configurable asset by defining a run configuration schema that inherits from Dagster's Config class. The code shows how to set up a lookback window parameter for asset computation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/configuring-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Config\nfrom datetime import timedelta\n\nclass TradeConfig(Config):\n    lookback_days: int = 30\n\n@asset(config_schema=TradeConfig)\ndef trades(context):\n    lookback = timedelta(days=context.op_config.lookback_days)\n    # Use lookback to determine time window for trade computation\n```\n\n----------------------------------------\n\nTITLE: Reusing Op Definitions in a Graph\nDESCRIPTION: Shows how to use the same op definition multiple times in a single graph, with automatic and manual aliasing of op names.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@graph\ndef multiple_usage_graph():\n    value = return_one()\n    value = add_one(value)\n    return add_one(value)\n```\n\nLANGUAGE: python\nCODE:\n```\n@graph\ndef alias_graph():\n    value = return_one()\n    value = add_one(value)\n    return add_one.alias(\"add_one_again\")(value)\n```\n\n----------------------------------------\n\nTITLE: Defining Software-Defined Assets in Dagster\nDESCRIPTION: Demonstrates how to create assets using the @asset decorator in Dagster, with support for labeling, grouping, and adding descriptions\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_snowflake/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    compute_kind=\"HackerNews API\",\n    group_name=\"hackernews\",\n    description=\"Fetch top HackerNews story IDs\"\n)\ndef hackernews_topstory_ids():\n    # Asset implementation details\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Replication Assets Decorator in Python\nDESCRIPTION: A function that returns a custom multi_asset decorator for defining replication assets based on the configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef custom_replication_assets(\n    *,\n    replication_project: ReplicationProject,\n    name: Optional[str] = None,\n    group_name: Optional[str] = None,\n) -> Callable[[Callable[..., Any]], AssetsDefinition]:\n    project = replication_project.load()\n\n    return multi_asset(\n        name=name,\n        group_name=group_name,\n        specs=[\n            AssetSpec(\n                key=table.get(\"name\"),\n            )\n            for table in project.get(\"tables\")\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Time-Based Partitioning in Dagster\nDESCRIPTION: This snippet demonstrates how to implement time-based partitioning for processing data in specific time intervals. It defines daily partitions and creates assets and a schedule for daily sales data processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, DailyPartitionsDefinition, asset, define_asset_job, ScheduleDefinition\nfrom datetime import datetime, timedelta\n\ndaily_partitions = DailyPartitionsDefinition(start_date=\"2024-01-01\")\n\n@asset(partitions_def=daily_partitions)\ndef daily_sales_data(context: AssetExecutionContext):\n    partition_date_str = context.partition_key\n    date = datetime.strptime(partition_date_str, \"%Y-%m-%d\").date()\n    # Fetch or generate sales data for the given date\n    return {\"date\": date, \"sales\": 1000}  # placeholder\n\n@asset(partitions_def=daily_partitions)\ndef daily_sales_summary(context: AssetExecutionContext, daily_sales_data):\n    # Process the daily sales data into a summary\n    return {\"date\": daily_sales_data[\"date\"], \"total_sales\": daily_sales_data[\"sales\"]}\n\ndaily_sales_job = define_asset_job(\"daily_sales_job\", selection=[daily_sales_data, daily_sales_summary])\n\ndaily_sales_schedule = ScheduleDefinition(\n    job=daily_sales_job,\n    cron_schedule=\"0 1 * * *\",  # Run every day at 1:00 AM\n    default_status=\"running\",\n    execution_timezone=\"America/New_York\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Definitions Object\nDESCRIPTION: Creates the initial Definitions object for a Dagster project. This object will contain all assets and resources for the ETL workflow.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\n\nfrom dagster_duckdb import DuckDBResource\n\nimport dagster as dg\n\ndefs = dg.Definitions(\n  assets=[],\n  resources={},\n)\n```\n\n----------------------------------------\n\nTITLE: Asset Path Selection Job in Python\nDESCRIPTION: Defines a job that selects assets between raw_data_b and summary_stats_2\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nraw_data_b_summary_stats_2_job = define_asset_job(\n    name=\"raw_data_b_summary_stats_2_job\", selection='key:\"raw_data_b\"+ and +key:\"summary_stats_2\"'\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Assets With I/O Managers in Python\nDESCRIPTION: Example showing asset implementation using DuckDBPandasIOManager to handle data reading and writing. The code demonstrates simplified asset functions that focus only on data transformation logic.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset\nfrom dagster_duckdb_pandas import DuckDBPandasIOManager\n\n@asset\ndef raw_sales_data():\n    return pd.read_csv(\"path/to/sales.csv\")\n\n@asset\ndef clean_sales_data(raw_sales_data):\n    return raw_sales_data.dropna()\n```\n\n----------------------------------------\n\nTITLE: Creating Evaluation Metric Plots for ML Model in Python\nDESCRIPTION: This function generates plots of evaluation metrics for a machine learning model during the training process. It uses Matplotlib to create the plots and returns them as Markdown for display in the Dagster UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/managing-ml.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef plot_eval_metric(metric_name, values):\n    plt.figure(figsize=(10, 5))\n    plt.plot(values)\n    plt.title(f'{metric_name} Over Training Iterations')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric_name)\n    img = io.BytesIO()\n    plt.savefig(img, format='png')\n    img.seek(0)\n    data = base64.b64encode(img.getvalue()).decode()\n    return f\"![{metric_name}](data:image/png;base64,{data})\"\n```\n\n----------------------------------------\n\nTITLE: Defining an Event-Driven Asset in Dagster\nDESCRIPTION: This code defines an event-driven asset that generates a pivot table report for sales results. It accepts request data through the materialization context and processes it into a report when triggered.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-a-sensor-asset.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef adhoc_request(product_metrics):\n    \"\"\"Generate a pivot table report based on an ad-hoc request.\"\"\"\n    context = get_dagster_logger()\n\n    # Read in our data from the product metrics asset\n    metrics_df = product_metrics.copy()\n\n    # Get the request from context\n    request_context = get_materialization_context()\n    metadata = request_context.asset_materialization_metadata\n    request_str = metadata[\"request\"]\n    request = json.loads(request_str)\n\n    # Get the values from the request\n    department = request.get(\"department\")\n    if department:\n        metrics_df = metrics_df[metrics_df[\"product_department\"] == department]\n\n    # Create a pivot table with the data\n    pivot = metrics_df.pivot_table(\n        index=\"product_name\",\n        values=\"total_sales\",\n        aggfunc=\"sum\",\n    )\n\n    # Save and return our results\n    result_path = \"data/adhoc_results/report.csv\"\n    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n    pivot.to_csv(result_path)\n\n    return pivot\n```\n\n----------------------------------------\n\nTITLE: Launching Run on Successful Status in Dagster\nDESCRIPTION: This snippet demonstrates a run status sensor that launches a run of 'status_reporting_job' when another run is successful. It includes logic to prevent infinite loops of triggered runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/run-status-sensors.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@run_status_sensor(run_status=DagsterRunStatus.SUCCESS)\ndef report_status_sensor(context):\n    if context.dagster_run.job_name != \"status_reporting_job\":\n        return RunRequest(\n            run_key=None,\n            run_config={},\n            job_name=\"status_reporting_job\",\n        )\n\n\n@job\ndef request_job():\n    report_status_sensor()\n```\n\n----------------------------------------\n\nTITLE: Testing Partitioned Job Execution in Python\nDESCRIPTION: Shows how to execute a partitioned job in-process for a specific partition using execute_in_process method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/testing-partitioned-config-and-jobs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef test_my_partitioned_job():\n    result = my_job.execute_in_process(partition_key=\"2023-01-01\")\n    assert result.success\n```\n\n----------------------------------------\n\nTITLE: Defining a Graph-backed Asset in Python using Dagster\nDESCRIPTION: This snippet demonstrates how to define a graph-backed asset using the @graph_asset decorator. It creates an asset named 'slack_files_table' that combines two ops: 'fetch_files_from_slack' and 'store_files'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/graph-backed-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@graph_asset\ndef slack_files_table():\n    store_files(fetch_files_from_slack())\n\n@op\ndef fetch_files_from_slack():\n    ...\n\n@op\ndef store_files(files):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Customizing ECS Task Configuration for Dagster Runs\nDESCRIPTION: This Python code demonstrates how to use job tags to customize the CPU, memory, and ephemeral storage settings for every run of a particular Dagster job in ECS.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op\n\n@op()\ndef my_op(context):\n  context.log.info('running')\n\n@job(\n  tags = {\n    \"ecs/cpu\": \"256\",\n    \"ecs/memory\": \"512\",\n    \"ecs/ephemeral_storage\": \"40\",\n  }\n)\ndef my_job():\n  my_op()\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Templating Values with get_additional_scope\nDESCRIPTION: Implementation of a custom scope for templating in component YAML files, providing pre-defined partitions definitions accessible in templates.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DailyPartitionsDefinition\nfrom datetime import datetime\n\nclass ShellCommandComponent(Resolvable, Component):\n    # ... other code ...\n    \n    @classmethod\n    def get_additional_scope(cls):\n        # This will be available to templates with {{ daily_partitions }}\n        return {\n            \"daily_partitions\": DailyPartitionsDefinition(start_date=datetime(2023, 1, 1))\n        }\n```\n\n----------------------------------------\n\nTITLE: Validating Polars DataFrame with Patito Model in Dagster Asset\nDESCRIPTION: Demonstrates how to create a Patito model for data validation and use it in a Dagster asset. The example shows user data validation with field constraints and descriptions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/patito.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagster as dg\nimport patito as pt\nimport polars as pl\n\nclass User(pt.Model):\n    uid: str = pt.Field(unique=True, description=\"User ID\")\n    age: int | None = pt.Field(gt=18, description=\"User age\")\n\n\n@dg.asset(io_manager_key=\"polars_parquet_io_manager\")\ndef my_asset() -> User.DataFrame:\n    my_data = ...\n    return User.DataFrame(my_data)\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment (Windows)\nDESCRIPTION: Commands to create and activate a virtual environment on Windows using uv.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example dagster_example\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Loading String Input from Config in Dagster Job\nDESCRIPTION: Demonstrates how to create a basic Dagster job with an unconnected string input that can be provided via run config.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/unconnected-inputs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, In\n\n@op(ins={\"input_string\": In(str)})\ndef my_op(input_string):\n    # do something\n    return input_string\n\n@job\ndef my_job():\n    my_op()\n\nresult = my_job.execute_in_process(\n    run_config={\"ops\": {\"my_op\": {\"inputs\": {\"input_string\": {\"value\": \"Hello, Dagster!\"}}}}})\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with Snowflake I/O Manager\nDESCRIPTION: Python code setting up Dagster Definitions with assets and Snowflake I/O manager configuration for local development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[items, comments, stories],\n    resources={\n        \"io_manager\": SnowflakePandasIOManager(\n            account=\"abc123\",\n            user=\"dagster_dev\",\n            password=\"my_password\",\n            database=\"DEVELOPMENT\",\n            warehouse=\"USERS\",\n            schema=\"johndoe\",\n            role=\"DEVELOPER\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Using Custom DataFrame Type in Dagster Ops\nDESCRIPTION: Shows how to use the custom DataFrame type as type declarations for inputs and outputs in Dagster ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pandas.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef trip_etl(@op):\n    return trip_dataframe.DataFrame(...)\n\n\n@op(ins={\"trips\": In(trip_dataframe)})\ndef process_trips(trips):\n    trips.describe()\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Validation Asset Check in Python for OpenAI Fine-Tuned Models\nDESCRIPTION: This code snippet defines an asset check function that compares the accuracy of a fine-tuned OpenAI model against the base model. It uses a sample of 100 records from the enriched_graphic_novels asset, queries both models, and compares their performance in correctly identifying genres.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/model-validation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset_check(asset=fine_tuned_model, name=\"model_validation\")\ndef model_validation(\n    context: AssetCheckContext,\n    openai_resource: OpenAIResource,\n    enriched_graphic_novels: pd.DataFrame,\n):\n    # Sample 100 records\n    sampled_data = enriched_graphic_novels.sample(n=100, random_state=42)\n\n    base_model_correct = 0\n    fine_tuned_model_correct = 0\n\n    for _, row in sampled_data.iterrows():\n        title = row[\"title\"]\n        description = row[\"description\"]\n        actual_genre = row[\"genre\"]\n\n        prompt = f\"Title: {title}\\nDescription: {description}\\n\\nBased on the title and description, what is the most likely genre for this book? Please respond with only the genre name.\"\n\n        # Query base model\n        base_response = openai_resource.chat.completions.create(\n            model=\"gpt-4-0125-preview\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=10,\n        )\n        base_genre = base_response.choices[0].message.content.strip().lower()\n\n        # Query fine-tuned model\n        fine_tuned_response = openai_resource.chat.completions.create(\n            model=context.asset_key[\"fine_tuned_model\"].path,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=10,\n        )\n        fine_tuned_genre = fine_tuned_response.choices[0].message.content.strip().lower()\n\n        if base_genre == actual_genre.lower():\n            base_model_correct += 1\n        if fine_tuned_genre == actual_genre.lower():\n            fine_tuned_model_correct += 1\n\n    base_accuracy = base_model_correct / len(sampled_data)\n    fine_tuned_accuracy = fine_tuned_model_correct / len(sampled_data)\n\n    context.add_metadata(\n        {\n            \"base_model_accuracy\": base_accuracy,\n            \"fine_tuned_model_accuracy\": fine_tuned_accuracy,\n        }\n    )\n\n    if fine_tuned_accuracy > base_accuracy:\n        return AssetCheckResult(passed=True)\n    else:\n        return AssetCheckResult(\n            passed=False,\n            metadata={\n                \"reason\": \"Fine-tuned model did not outperform base model\"\n            },\n        )\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to ML Model Asset in Python\nDESCRIPTION: This snippet demonstrates how to add metadata to a machine learning model asset in Dagster. It includes evaluation metrics and plots of the training process, which can be viewed in the Dagster UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/managing-ml.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef xgboost_comments_model(context, comments, labels):\n    X_train, X_test, y_train, y_test = train_test_split(comments, labels, test_size=0.2)\n    model = xgb.XGBRegressor()\n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='mae')\n    \n    score = mean_absolute_error(y_test, model.predict(X_test))\n    \n    metadata = {\n        \"score (mean_absolute_error)\": score,\n        \"training_history_plot\": MetadataValue.md(plot_eval_metric(\"MAE\", model.evals_result()[\"validation_0\"][\"mae\"]))\n    }\n    \n    context.add_output_metadata(metadata=metadata)\n    \n    return model\n```\n\n----------------------------------------\n\nTITLE: Fetching Column Metadata for dbt Models in Dagster\nDESCRIPTION: This snippet demonstrates how to fetch column-level metadata for dbt models and emit it as materialization metadata in Dagster. It uses the fetch_column_metadata() method on the DbtEventIterator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@dbt_assets(manifest=manifest)\ndef my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):\n    yield from dbt.cli(\n        [\n            \"build\",\n            \"--select\",\n            \"my_model\",\n            \"--profiles-dir\",\n            \".\",\n            \"--project-dir\",\n            \".\",\n        ],\n        context=context,\n    ).stream().fetch_column_metadata()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with PipesECSClient\nDESCRIPTION: Set up Dagster Definitions to include the PipesECSClient resource. This allows Dagster to launch ECS tasks and receive logs and events from them.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-ecs-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_aws.pipes import PipesECSClient\n\ndefs = Definitions(\n    assets=[ecs_pipes_asset],\n    resources={\n        \"pipes_ecs\": PipesECSClient(\n            message_reader=PipesCloudwatchLogReader(\n                log_group=\"/ecs/my-task-definition\"\n            )\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sitemap Parsing for Web Scraping in Python\nDESCRIPTION: This code snippet defines a method in the SitemapScraper class to parse a sitemap XML and extract individual page URLs. It uses the requests library to fetch the sitemap and xml.etree.ElementTree for XML parsing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/sources.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport xml.etree.ElementTree as ET\n\nclass SitemapScraper:\n    def parse_sitemap(self, sitemap_url: str) -> List[str]:\n        response = requests.get(sitemap_url)\n        root = ET.fromstring(response.content)\n\n        urls = []\n        for url in root.findall(\"{http://www.sitemaps.org/schemas/sitemap/0.9}url\"):\n            loc = url.find(\"{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")\n            if loc is not None:\n                urls.append(loc.text)\n\n        return urls\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment with venv\nDESCRIPTION: Creates a new Python virtual environment in a directory named .venv using the venv module. This isolates project dependencies from the system Python installation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-b-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Limiting Total Concurrent Runs in Dagster YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to set a maximum limit on the number of concurrent runs for a Dagster instance. It's added to the dagster.yaml file or deployment settings in Dagster+.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  runs:\n    max_concurrent_runs: 15\n```\n\n----------------------------------------\n\nTITLE: Creating a Sensor for Ad-hoc Requests in Dagster\nDESCRIPTION: This sensor monitors a directory for JSON files containing ad-hoc report requests. When a new request file is detected, it triggers the materialization of the adhoc_request asset with the appropriate context data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-a-sensor-asset.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@sensor(job=materialize([adhoc_request]))\ndef adhoc_request_sensor():\n    \"\"\"Sensor that detects file-based adhoc requests and triggers materialization of the request\"\"\"\n    # Get all the files in the request directory\n    REQUEST_PATH = \"data/requests/\"\n    context = get_dagster_logger()\n\n    os.makedirs(REQUEST_PATH, exist_ok=True)\n    files = os.listdir(REQUEST_PATH)\n    json_files = [f for f in files if f.endswith(\".json\")]\n\n    # If we have files, read them and create a run request for each\n    for file in json_files:\n        file_path = os.path.join(REQUEST_PATH, file)\n        context.info(f\"Found request file: {file_path}\")\n\n        # Read the request file\n        with open(file_path, \"r\") as f:\n            request = json.load(f)\n\n        # Move the file to processed\n        processed_path = os.path.join(\"data/processed_requests/\", file)\n        os.makedirs(os.path.dirname(processed_path), exist_ok=True)\n        shutil.move(file_path, processed_path)\n\n        # Create a run request with the file contents\n        yield RunRequest(\n            run_key=f\"adhoc_request_{file}\",\n            asset_selection=[AssetKey(\"adhoc_request\")],\n            tags={\"request\": file},\n            asset_materializations_meta={\"adhoc_request\": {\"request\": json.dumps(request)}},\n        )\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to ShellCommandComponent\nDESCRIPTION: Enhanced ShellCommandComponent that includes metadata like owners and tags through the get_component_type_metadata method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, Definitions, asset\nfrom dagster._core.definitions.asset_spec import AssetSpec\nfrom dagster_components import Component, ResolvedAssetSpec, Resolvable, ComponentTypeMetadata\nfrom pydantic import model_validator\nfrom typing_extensions import Annotated, override\n\nfrom typing import List\n\n\nclass ShellCommandComponent(Resolvable, Component):\n    \"\"\"A component that executes a shell command.\"\"\"\n\n    script_path: str\n    \"\"\"The path to the shell script to execute.\"\"\"\n\n    asset_specs: List[ResolvedAssetSpec]\n    \"\"\"The assets that this script produces.\"\"\"\n\n    @override\n    def build_defs(self) -> Definitions:\n        \"\"\"Build the definitions for this component.\"\"\"\n        return Definitions()\n\n    @classmethod\n    @override\n    def get_component_type_metadata(cls) -> ComponentTypeMetadata:\n        return ComponentTypeMetadata(\n            owners=[\"data-platform-team\"],\n            tags={\"tier\": \"production\"},\n        )\n```\n\n----------------------------------------\n\nTITLE: Creating Factory Method Based Assets in Dagster\nDESCRIPTION: Demonstrates how to use the factory pattern to create multiple similar assets based on parameter specifications. The example shows creating assets for processing different database tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/integration-approaches.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\n\nparameters = [\n    {\"name\": \"asset1\", \"table\": \"users\"},\n    {\"name\": \"asset2\", \"table\": \"orders\"},\n]\n\n\ndef process_table(table_name: str) -> None:\n    pass\n\n\ndef build_asset(params):\n    @asset(name=params[\"name\"])\n    def _asset():\n        process_table(params[\"table\"])\n\n    return _asset\n\n\nassets = [build_asset(params) for params in parameters]\n\ndefs = Definitions(assets=assets)\n```\n\n----------------------------------------\n\nTITLE: Asset Observation with Metadata\nDESCRIPTION: Example demonstrating how to attach metadata to an asset observation using MetadataValue class.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/asset-observations.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef observe_with_metadata(context):\n    context.log_event(\n        AssetObservation(\n            asset_key=AssetKey(\"my_table\"),\n            metadata={\n                \"num_rows\": MetadataValue.int(100),\n                \"row_count_increased\": MetadataValue.bool(True),\n                \"run_time\": MetadataValue.float(0.5)\n            }\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Scaffolding an Evidence Project with Dagster CLI\nDESCRIPTION: Command to generate a new Evidence project structure named 'jaffle_dashboard' using the Dagster CLI scaffold command. The command creates the initial files and directory structure for an Evidence project that integrates with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/27-scaffold-jaffle-dashboard.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndg scaffold dagster_evidence.EvidenceProject jaffle_dashboard\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Window Partition Mapping in Python\nDESCRIPTION: Shows how to use TimeWindowPartitionMapping to specify that each partition of a daily-partitioned asset should depend on the prior day's partition in an upstream asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/defining-dependencies-between-partitioned-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    partitions_def=DailyPartitionsDefinition(start_date=\"2024-01-01\"),\n    load_from=SourceData(\n        \"my_source\",\n        partition_mapping=TimeWindowPartitionMapping(\n            start_offset=-datetime.timedelta(days=1)\n        )\n    )\n)\ndef my_asset(context):\n    # Load and process data\n    pass\n```\n\n----------------------------------------\n\nTITLE: Rich Metadata Examples with Dagster Pipes\nDESCRIPTION: Collection of examples showing how to pass various types of rich metadata back to Dagster\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmetadata = {\n    \"type\": \"url\",\n    \"raw_value\": \"https://www.example.com\"\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nmetadata = {\n    \"type\": \"path\",\n    \"raw_value\": \"/path/to/file.txt\"\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nmetadata = {\n    \"type\": \"notebook\",\n    \"raw_value\": {\"path\": \"path/to/notebook.ipynb\"}\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nmetadata = {\n    \"type\": \"json\",\n    \"raw_value\": {\"key\": \"value\"}\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nmetadata = {\n    \"type\": \"markdown\",\n    \"raw_value\": \"# Hello World\"\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nmetadata = {\n    \"type\": \"table\",\n    \"raw_value\": [[\"header1\", \"header2\"], [\"value1\", \"value2\"]]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running AWS EMR Job in Dagster\nDESCRIPTION: This Python code demonstrates how to configure and run an AWS EMR job within a Dagster pipeline. It includes setting up EMR resources, defining job flow parameters, and creating an op to execute the EMR job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/emr.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, job, resource\nfrom dagster_aws.emr import emr_resource, EmrJobConfig\n\n@resource\ndef emr_config():\n    return EmrJobConfig(\n        cluster={\n            \"Name\": \"my_emr_cluster\",\n            \"ReleaseLabel\": \"emr-6.2.0\",\n            \"Applications\": [{\"Name\": \"Spark\"}],\n            \"Instances\": {\n                \"InstanceGroups\": [\n                    {\n                        \"Name\": \"Master nodes\",\n                        \"Market\": \"ON_DEMAND\",\n                        \"InstanceRole\": \"MASTER\",\n                        \"InstanceType\": \"m5.xlarge\",\n                        \"InstanceCount\": 1,\n                    }\n                ],\n                \"KeepJobFlowAliveWhenNoSteps\": False,\n                \"TerminationProtected\": False,\n            },\n            \"VisibleToAllUsers\": True,\n            \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n            \"ServiceRole\": \"EMR_DefaultRole\",\n        },\n        job_config={\n            \"Name\": \"my_emr_job\",\n            \"ReleaseLabel\": \"emr-6.2.0\",\n            \"Instances\": {\n                \"InstanceGroups\": [\n                    {\n                        \"Name\": \"Master nodes\",\n                        \"Market\": \"ON_DEMAND\",\n                        \"InstanceRole\": \"MASTER\",\n                        \"InstanceType\": \"m5.xlarge\",\n                        \"InstanceCount\": 1,\n                    }\n                ],\n                \"KeepJobFlowAliveWhenNoSteps\": False,\n                \"TerminationProtected\": False,\n            },\n            \"Steps\": [\n                {\n                    \"Name\": \"Spark Application\",\n                    \"ActionOnFailure\": \"CONTINUE\",\n                    \"HadoopJarStep\": {\n                        \"Jar\": \"command-runner.jar\",\n                        \"Args\": [\n                            \"spark-submit\",\n                            \"--deploy-mode\",\n                            \"cluster\",\n                            \"s3://my-bucket/my-spark-script.py\",\n                        ],\n                    },\n                }\n            ],\n            \"VisibleToAllUsers\": True,\n            \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n            \"ServiceRole\": \"EMR_DefaultRole\",\n        },\n    )\n\n@op(required_resource_keys={\"emr\"})\ndef run_emr_job(context):\n    job_flow_id = context.resources.emr.create_job_flow()\n    context.log.info(f\"Created EMR job flow {job_flow_id}\")\n\n@job(resource_defs={\"emr\": emr_resource, \"emr_config\": emr_config})\ndef my_emr_job():\n    run_emr_job()\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Asset for CSV Replication to Snowflake using Sling in Python\nDESCRIPTION: This snippet defines a Dagster asset that uses the Sling resource and replication configuration to perform the data transfer from S3 to Snowflake. It includes the asset definition, Sling replication function, and Dagster definitions setup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/sling_replicate_csv_files_to_snowflake.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_sling import SlingConnectionResource, SlingResource, sling_assets\n\n# ...\n\n@sling_assets(replication_config=replication_config)\ndef replicate_csv_to_snowflake(context, sling: SlingResource):\n    yield from sling.replicate(context=context)\n\n\ndefs = Definitions(assets=[replicate_csv_to_snowflake], resources={\"sling\": sling_resource})\n```\n\n----------------------------------------\n\nTITLE: Creating a Distributed Dagster Job with S3 IO Manager in Python\nDESCRIPTION: This snippet shows how to create a Dagster job for distributed execution using the Dask executor and an S3-based IO manager. It defines resources for S3 access and configures the job with these components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/dask.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@job(\n    executor_def=dask_executor,\n    resource_defs={\n        \"io_manager\": s3_pickle_io_manager,\n        \"s3\": s3_resource,\n    },\n)\ndef hello_dask_distributed():\n    hello_world()\n    goodbye_world()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Company Holiday Automation Condition\nDESCRIPTION: Example implementation of a custom AutomationCondition that checks if the current time is during a company holiday.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/arbitrary-python-automation-conditions.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dagster as dg\n\nclass IsCompanyHoliday(dg.AutomationCondition):\n  def evaluate(self, context: dg.AutomationContext) -> dg.AutomationResult:\n    if is_company_holiday(context.evaluation_time):\n      true_subset = context.candidate_subset\n    else:\n      true_subset = context.get_empty_subset()\n    return dg.AutomationResult(true_subset, context=context)\n```\n\n----------------------------------------\n\nTITLE: Using BigQuery Resource for Custom SQL Queries in Dagster\nDESCRIPTION: Shows how to use the BigQuery resource in Dagster to execute custom SQL queries. This example demonstrates querying a table and returning the results as a Pandas DataFrame.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Definitions\nfrom dagster_gcp import bigquery_resource\n\n@asset\ndef small_petals(bigquery):\n    with bigquery.get_client() as client:\n        query = \"\"\"\n        SELECT *\n        FROM `my-project.my_dataset.IRIS_DATA`\n        WHERE petal_length < 1.5\n        \"\"\"\n        return client.query(query).to_dataframe()\n\ndefs = Definitions(\n    assets=[small_petals],\n    resources={\n        \"bigquery\": bigquery_resource.configured(\n            {\"project\": \"my-project\"}\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Runtime Metadata to Dagster Assets in Python\nDESCRIPTION: This snippet shows how to attach metadata to a Dagster asset at runtime by returning a MaterializeResult object. It includes examples of adding various types of metadata, including numerical values that will be treated as time series.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MaterializeResult, MetadataValue\n\n@asset\ndef my_asset():\n    df = do_some_data_processing()\n    return MaterializeResult(\n        metadata={\n            \"row_count\": len(df),\n            \"preview\": MetadataValue.md(df.head().to_markdown()),\n            \"dataset_url\": MetadataValue.url(\"https://my-data-warehouse.com/my-table\"),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from AWS S3 to Teradata Vantage Using dagster-teradata\nDESCRIPTION: A Dagster job that demonstrates how to transfer data from AWS S3 to Teradata Vantage. It configures S3 and Teradata resources, drops existing tables, and uses the s3_to_teradata method to load data from S3 into Teradata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom dagster import job, op, Definitions, EnvVar, DagsterError\nfrom dagster_aws.s3 import S3Resource, s3_resource\nfrom dagster_teradata import TeradataResource, teradata_resource\n\ns3_resource = S3Resource(\n    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.getenv(\"AWS_SESSION_TOKEN\"),\n)\n\ntd_resource = TeradataResource(\n    host=os.getenv(\"TERADATA_HOST\"),\n    user=os.getenv(\"TERADATA_USER\"),\n    password=os.getenv(\"TERADATA_PASSWORD\"),\n    database=os.getenv(\"TERADATA_DATABASE\"),\n)\n\n@op(required_resource_keys={\"teradata\"})\ndef drop_existing_table(context):\n     context.resources.teradata.drop_table(\"people\")\n     return \"Tables Dropped\"\n\n@op(required_resource_keys={\"teradata\", \"s3\"})\ndef ingest_s3_to_teradata(context, status):\n    if status == \"Tables Dropped\":\n        context.resources.teradata.s3_to_teradata(s3_resource, os.getenv(\"AWS_S3_LOCATION\"), \"people\")\n    else:\n        raise DagsterError(\"Tables not dropped\")\n\n@job(resource_defs={\"teradata\": td_resource, \"s3\": s3_resource})\ndef example_job():\n     ingest_s3_to_teradata(drop_existing_table())\n\ndefs = Definitions(\n    jobs=[example_job]\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Dagster Definitions with Automation Sensor in Python\nDESCRIPTION: Adds the automation_sensor to the Dagster Definitions object, enabling the Declarative Automation framework to trigger asset executions based on defined conditions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/federate-execution.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndefs = Definitions(\n    assets=[load_customers_dag_asset, run_customer_metrics],\n    resources={\n        \"customers_airflow_instance\": customers_airflow_instance,\n        \"metrics_airflow_instance\": metrics_airflow_instance,\n    },\n    sensors=[automation_sensor],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset with Code Version in Main Deployment - Python\nDESCRIPTION: Example of an asset definition with a code_version parameter set to 'v1' in the main deployment. The code_version parameter is used for asset versioning and caching.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(code_version=\"v1\")\ndef customers(): ...\n```\n\n----------------------------------------\n\nTITLE: Unit Testing with Stub HN Resource\nDESCRIPTION: Demonstrates how to use the stubbed Hacker News resource in unit tests to verify asset behavior and data transformations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nstart\n```\n\n----------------------------------------\n\nTITLE: Implementing Time-Based Partition Dependencies in Python\nDESCRIPTION: Demonstrates how to create dependencies between daily and weekly partitioned assets, including automation conditions for scheduling. Shows daily data aggregation into weekly summaries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/defining-dependencies-between-partitioned-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    auto_materialize_policy=AutoMaterializePolicy.eager(),\n    partitions_def=DailyPartitionsDefinition(start_date=\"2023-01-01\"),\n    automation_condition=AutomationCondition.cron(cron_schedule=\"0 1 * * *\")\n)\ndef daily_sales_data(context):\n    start_date = pd.Timestamp(context.partition_key)\n    end_date = start_date + pd.Timedelta(days=1)\n    df = get_sales_data(start_date, end_date)\n    return df\n\n@asset(\n    auto_materialize_policy=AutoMaterializePolicy.eager(),\n    partitions_def=WeeklyPartitionsDefinition(start_date=\"2023-01-01\"),\n    automation_condition=AutomationCondition.eager()\n)\ndef weekly_sales_summary(context, daily_sales_data) -> pd.DataFrame:\n    # Partition ranges are inclusive on both sides\n    partition_range = context.asset_partition_key_range_for_input(\"daily_sales_data\")\n    partition_keys = partition_range.get_partition_keys()\n\n    dfs = []\n    for partition_key in partition_keys:\n        dfs.append(context.asset_value_for_partition_key(\"daily_sales_data\", partition_key))\n    df = pd.concat(dfs)\n    return pd.DataFrame({\"total_sales\": [df[\"amount\"].sum()]})\n```\n\n----------------------------------------\n\nTITLE: Providing Configuration in Graph Definition (Python)\nDESCRIPTION: This example shows how to provide configuration for ops inside a graph directly in the graph definition using the 'config' argument.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@graph(config={\"add_n\": {\"n\": 3}, \"multiply_by_m\": {\"m\": 2}})\ndef add_and_multiply_with_config(num):\n    return multiply_by_m(add_n(num))\n```\n\n----------------------------------------\n\nTITLE: Observable Source Asset Definition\nDESCRIPTION: Example demonstrating how to define a source asset with an observation function that tracks file hash changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/asset-observations.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@observable_source_asset\ndef my_source_file():\n    with open(\"my_source_file.txt\", \"rb\") as f:\n        return hashlib.md5(f.read()).hexdigest()\n```\n\n----------------------------------------\n\nTITLE: Integrating Meltano with Dagster for ETL Pipeline\nDESCRIPTION: Python code demonstrating how to use the dagster-meltano library to create a Dagster job that runs a Meltano ELT pipeline. It includes setting up a Meltano project, defining a Dagster asset, and configuring the Meltano command to be executed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/meltano.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom dagster import AssetExecutionContext, asset\nfrom dagster_meltano import meltano_resource\n\n@asset(required_resource_keys={\"meltano\"})\ndef my_meltano_asset(context: AssetExecutionContext):\n    result = context.resources.meltano.run(\n        command=[\"elt\", \"tap-gitlab\", \"target-postgres\", \"--job_id=gitlab-to-postgres\"],\n        project_dir=os.getenv(\"MELTANO_PROJECT_ROOT\"),\n    )\n    return {\"exit_code\": result.return_code, \"output\": result.output()}\n\nresources = {\n    \"meltano\": meltano_resource,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dagster DataFrame Type for E-bike Trips\nDESCRIPTION: Demonstrates how to create a custom DataFrame type using create_dagster_pandas_dataframe_type with PandasColumn objects to specify schema and constraints for e-bike trip data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pandas.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrip_dataframe = create_dagster_pandas_dataframe_type(\n    name=\"TripDataFrame\",\n    columns=[\n        PandasColumn.integer_column(\"bike_id\", min_value=0),\n        PandasColumn.integer_column(\"payment_id\", min_value=0),\n        PandasColumn.string_column(\"payment_type\"),\n        PandasColumn.float_column(\"amount\", min_value=0),\n        PandasColumn.datetime_column(\"started_at\"),\n        PandasColumn.datetime_column(\"ended_at\"),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with Assets and Schedules\nDESCRIPTION: Creates a Dagster Definitions object that combines all assets, asset checks, schedules, and resources needed for the pipeline, making them available for execution in the Dagster environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/automate-your-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndefs = dg.Definitions(\n    assets=[products,\n        sales_reps,\n        sales_data,\n        joined_data,\n        monthly_sales_performance,\n        product_performance,\n    ],\n    asset_checks=[missing_dimension_check],\n    schedules=[weekly_update_schedule],\n    resources={\"duckdb\": DuckDBResource(database=\"data/mydb.duckdb\")},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Graph for Op Job Creation in Python\nDESCRIPTION: Shows how to define a graph using the @graph decorator, which can be later used to create op jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/op-jobs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@graph\ndef my_graph():\n    add_one(return_five())\n```\n\n----------------------------------------\n\nTITLE: Using Config Mapping in Dagster Graphs (Python)\nDESCRIPTION: This snippet demonstrates how to use config mapping in Dagster graphs, allowing dynamic configuration based on input provided to the graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import graph, op, Field, String\n\n@op(config_schema={\"celsius\": Field(float)})\ndef convert_celsius_to_fahrenheit(context):\n    celsius = context.op_config[\"celsius\"]\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\n@op(config_schema={\"fahrenheit\": Field(float)})\ndef convert_fahrenheit_to_celsius(context):\n    fahrenheit = context.op_config[\"fahrenheit\"]\n    celsius = (fahrenheit - 32) * 5 / 9\n    return celsius\n\n@graph(config_schema={\"from_unit\": Field(String)})\ndef to_fahrenheit(temp):\n    def _config_fn(config):\n        from_unit = config[\"from_unit\"]\n        if from_unit == \"F\":\n            return {\"convert_fahrenheit_to_celsius\": {\"fahrenheit\": temp}}\n        else:\n            return {\"convert_celsius_to_fahrenheit\": {\"celsius\": temp}}\n\n    return convert_celsius_to_fahrenheit.alias(\"to_fahrenheit\")(_config_fn)\n```\n\n----------------------------------------\n\nTITLE: Removing Legacy dbt Seed File\nDESCRIPTION: Shell commands to navigate to the correct directory and remove the deprecated raw_customers seed file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/upstream-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd ..\nrm seeds/raw_customers.csv\n```\n\n----------------------------------------\n\nTITLE: Implementing Anomaly Detection for Freshness Checks in Python (Dagster+ Pro)\nDESCRIPTION: This snippet demonstrates how to use the anomaly detection feature in Dagster+ Pro for freshness checks. It applies a time series anomaly detection model to determine if data arrives later than expected.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/data-freshness-testing.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSelection, Definitions, asset\nfrom dagster_plus.asset_checks import build_anomaly_detection_freshness_checks\n\n@asset\ndef my_asset():\n    ...\n\nfreshness_check = build_anomaly_detection_freshness_checks(\n    AssetSelection.assets(my_asset),\n)\n\ndefs = Definitions(\n    assets=[my_asset],\n    asset_checks=[freshness_check],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Per-Input I/O Manager in Python\nDESCRIPTION: This example shows how to create a per-input I/O manager that loads data as a NumPy array instead of a Pandas DataFrame. It subclasses an existing I/O manager and overrides the load_input method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/defining-a-custom-io-manager.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass NumpyInputManager(PandasIOManager):\n    def load_input(self, context):\n        path = self._get_path(context.upstream_output)\n        return np.genfromtxt(path, delimiter=',')\n```\n\n----------------------------------------\n\nTITLE: Storing Static Partitioned Assets in BigQuery with Dagster\nDESCRIPTION: Example of defining static partitioned assets in Dagster for storage in BigQuery, using partition_expr metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nimport pandas as pd\n\n@asset(partitions_def=StaticPartitionsDefinition([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]),\n       metadata={\"partition_expr\": \"SPECIES\"})\ndef iris_data() -> pd.DataFrame:\n    ...\n\n@asset\ndef downstream_asset(iris_data: pd.DataFrame):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Defining External Asset for Existing Snowflake Table\nDESCRIPTION: Python code showing how to define an external asset in Dagster for an existing Snowflake table, making it available to other Dagster assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster-io-managers.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, AssetSpec\n\niris_harvest_data = AssetSpec(\n    key=AssetKey(\"iris_harvest_data\"),\n    io_manager_key=\"io_manager\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Not Diamond LLM Router with Dagster Assets\nDESCRIPTION: Complete example showing how to use Not Diamond for LLM model selection and OpenAI integration in Dagster. Implements two assets: one for handling book review data and another for generating summaries using the optimal LLM model selected by Not Diamond.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/notdiamond.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport dagster as dg\nimport dagster_notdiamond as nd\nimport dagster_openai as oai\n\n\n@dg.asset(kinds={\"python\"})\ndef book_review_data(context: dg.AssetExecutionContext) -> dict:\n    data = {\n        \"title\": \"Cat's Cradle\",\n        \"author\": \"Kurt Vonnegut\",\n        \"genre\": \"Science Fiction\",\n        \"publicationYear\": 1963,\n        \"reviews\": [\n            {\n                \"reviewer\": \"John Doe\",\n                \"rating\": 4.5,\n                \"content\": \"A thought-provoking satire on science and religion. Vonnegut's wit shines through.\",\n            },\n            {\n                \"reviewer\": \"Jane Smith\",\n                \"rating\": 5,\n                \"content\": \"An imaginative and darkly humorous exploration of humanity's follies. A must-read!\",\n            },\n            {\n                \"reviewer\": \"Alice Johnson\",\n                \"rating\": 3.5,\n                \"content\": \"Intriguing premise but felt a bit disjointed at times. Still enjoyable.\",\n            },\n        ],\n    }\n    context.add_output_metadata(metadata={\"num_reviews\": len(data.get(\"reviews\", []))})\n    return data\n\n\n@dg.asset(\n    kinds={\"openai\", \"notdiamond\"}, automation_condition=dg.AutomationCondition.eager()\n)\ndef book_reviews_summary(\n    context: dg.AssetExecutionContext,\n    notdiamond: nd.NotDiamondResource,\n    openai: oai.OpenAIResource,\n    book_review_data: dict,\n) -> dg.MaterializeResult:\n    prompt = f\"\"\"\n    Given the book reviews for {book_review_data[\"title\"]}, provide a detailed summary:\n\n    {'|'.join([r['content'] for r in book_review_data[\"reviews\"]])}\n    \"\"\"\n\n    with notdiamond.get_client(context) as client:\n        start = time.time()\n        session_id, best_llm = client.model_select(\n            model=[\"openai/gpt-4o\", \"openai/gpt-4o-mini\"],\n            tradeoff=\"cost\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert in literature\"},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        )\n        duration = time.time() - start\n\n    with openai.get_client(context) as client:\n        chat_completion = client.chat.completions.create(\n            model=best_llm.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n\n    summary = chat_completion.choices[0].message.content or \"\"\n\n    return dg.MaterializeResult(\n        metadata={\n            \"nd_session_id\": session_id,\n            \"nd_best_llm_model\": best_llm.model,\n            \"nd_best_llm_provider\": best_llm.provider,\n            \"nd_routing_latency\": duration,\n            \"summary\": dg.MetadataValue.md(summary),\n        }\n    )\n\n\ndefs = dg.Definitions(\n    assets=[book_review_data, book_reviews_summary],\n    resources={\n        \"notdiamond\": nd.NotDiamondResource(api_key=dg.EnvVar(\"NOTDIAMOND_API_KEY\")),\n        \"openai\": oai.OpenAIResource(api_key=dg.EnvVar(\"OPENAI_API_KEY\")),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Sales Data Asset in Dagster\nDESCRIPTION: Defines a sales_data asset that loads data from a CSV file into a DuckDB table. This asset contains sales transaction data and includes metadata for the Dagster UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-assets.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(group_name=\"bronze\", compute_kind=\"duckdb\")\ndef sales_data(duckdb: DuckDBResource):\n    \"\"\"Table that holds the sales transactions for the company\"\"\"\n\n    # First, create the table from the CSV data.\n    duckdb.execute_query(\n        '''\n        CREATE OR REPLACE TABLE sales_data AS\n        SELECT *\n        FROM read_csv_auto('data/sales_data.csv', header=True)\n        '''\n    )\n\n    # Next, let's return a preview of the table to show in the UI\n    sales_data_df = duckdb.execute_query(\"SELECT * FROM sales_data\").fetch_df()\n    num_records = len(sales_data_df)\n\n    return dg.MaterializeResult(\n        metadata={\n            \"num_records\": num_records,\n            \"preview\": dg.MetadataValue.md(sales_data_df.head().to_markdown()),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Job with Nested Resources\nDESCRIPTION: Shows how to define a Dagster job using resources with nested dependencies. The example creates Definitions with resources that require launch time configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/configuring-resources.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\n\n@asset\ndef my_asset(database: DatabaseResource, filestore: FilestoreResource):\n    pass\n\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"credentials\": credentials,\n        \"database\": database,\n        \"filestore\": filestore,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Single-Run Backfill Asset in Python\nDESCRIPTION: Example of configuring a partitioned asset with a single-run backfill policy. The code demonstrates how to define an asset that processes multiple partitions in a single run using partition_key_range context property.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/backfilling-data.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2022-01-01\"),\n       backfill_policy=BackfillPolicy.single_run())\ndef my_partitioned_asset(context):\n    # Use partition_key_range instead of partition_key\n    start_key, end_key = context.partition_key_range\n    \n    # Query data for a range of partitions in a single query\n    results = query_data_for_range(start_key, end_key)\n    \n    # Process the results\n    return process_results(results)\n```\n\n----------------------------------------\n\nTITLE: Defining Graph-backed Multi-assets in Python using Dagster\nDESCRIPTION: This snippet demonstrates how to create a combined definition of multiple assets using the @graph_multi_asset decorator. It defines 'two_assets' which accepts 'upstream_asset' and outputs two assets: 'first_asset' and 'second_asset'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/graph-backed-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef upstream_asset():\n    return 5\n\n@graph_multi_asset(outs={\"first_asset\": AssetOut(), \"second_asset\": AssetOut()})\ndef two_assets(upstream_asset):\n    first, second = multi_output_op(upstream_asset)\n    return {\"first_asset\": first, \"second_asset\": second}\n\n@op(out={\"first\": Out(), \"second\": Out()})\ndef multi_output_op(x):\n    return x + 1, x + 2\n```\n\n----------------------------------------\n\nTITLE: Unexpected Materialization in Subsettable Graph-backed Asset Op\nDESCRIPTION: This snippet shows an implementation of the 'foo' op that could lead to unexpected materializations when subsetting graph-backed assets. It always yields both outputs, potentially causing unintended asset materializations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/graph-backed-assets.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@op(out={\"foo_1\": Out(), \"foo_2\": Out()})\ndef foo():\n    yield Output(1, output_name=\"foo_1\")\n    yield Output(2, output_name=\"foo_2\")\n```\n\n----------------------------------------\n\nTITLE: REST API Materialization for Dagster+\nDESCRIPTION: Example of using REST API to report asset materialization in Dagster+ with required authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/external-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl \\\n  -X POST \\\n  -H 'Content-Type: application/json' \\\n  -H 'Dagster-Cloud-Api-Token: [YOUR API TOKEN]' \\\n  'https://[YOUR ORG NAME].dagster.cloud/[YOUR DEPLOYMENT NAME]/report_asset_materialization/' \\\n  -d '\n{\n  \"asset_key\": \"raw_transactions\",\n  \"metadata\": {\n    \"file_last_modified_at_ms\": 1724614700266\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI Web Server\nDESCRIPTION: Command to start the Dagster UI web server for local development. This launches the Dagster interface accessible at http://localhost:3000.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_etl/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Asset for Loading Graphic Novels Data into DuckDB\nDESCRIPTION: This code snippet defines a Dagster asset that loads data from a 'goodreads_books_comics_graphic.json.gz' file into a DuckDB table named 'graphic_novels'. It uses the Dagster DuckDBResource to interact with DuckDB and execute SQL queries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/ingestion.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(name=\"graphic_novels\")\ndef graphic_novels(duckdb: DuckDBResource):\n    duckdb.execute(\n        \"\"\"CREATE TABLE graphic_novels AS\n        SELECT *\n        FROM read_json_auto('goodreads_books_comics_graphic.json.gz')\n        \"\"\"\n    )\n    return duckdb.execute(\"SELECT COUNT(*) FROM graphic_novels\").fetchone()[0]\n```\n\n----------------------------------------\n\nTITLE: Adding Tableau Data Quality Warning Sensor\nDESCRIPTION: Implementation of a sensor that adds data quality warnings to Tableau data sources when upstream dependencies fail.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/tableau.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Sequence\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Timezone for Schedule Definition in Python\nDESCRIPTION: This snippet demonstrates how to set a custom timezone (US Pacific time) for a schedule definition using the execution_timezone parameter. The schedule is set to execute every day at 9:00 AM in the specified timezone.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/customizing-execution-timezone.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@schedule(\n    cron_schedule=\"0 9 * * *\",\n    job=my_job,\n    execution_timezone=\"America/Los_Angeles\",\n)\ndef my_schedule(context):\n    return {}\n```\n\n----------------------------------------\n\nTITLE: Implementing Nearest Fuel Station Asset\nDESCRIPTION: Creates a Dagster asset that utilizes the NREL API resource to query and process fuel station data. The asset handles the API response and extracts operational hours information from the access_days_time field.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/custom-resource.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample\\n  path=\"docs_projects/project_prompt_eng/project_prompt_eng/assets.py\"\\n  language=\"python\"\\n  startAfter=\"start_nearest_fuel_stations\"\\n  endBefore=\"end_nearest_fuel_stations\"\\n/>\n```\n\n----------------------------------------\n\nTITLE: Complete Dagster YAML Configuration\nDESCRIPTION: Comprehensive example of a dagster.yaml file showing all available configuration options including storage, compute logs, run coordination, scheduling, and auto-materialization settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlocal_artifact_storage:\n  module: dagster.core.storage.root\n  class: LocalArtifactStorage\n  config:\n    base_dir: '/path/to/dir'\n\ncompute_logs:\n  module: dagster.core.storage.local_compute_log_manager\n  class: LocalComputeLogManager\n  config:\n    base_dir: /path/to/compute/logs\n\nstorage:\n  sqlite:\n    base_dir: /path/to/sqlite/storage\n\nrun_queue:\n  max_concurrent_runs: 15\n  tag_concurrency_limits:\n    - key: 'database'\n      value: 'redshift'\n      limit: 4\n    - key: 'dagster/backfill'\n      limit: 10\n\nrun_storage:\n  module: dagster.core.storage.runs\n  class: SqliteRunStorage\n  config:\n    base_dir: /path/to/dagster/home/history\n\nevent_log_storage:\n  module: dagster.core.storage.event_log\n  class: SqliteEventLogStorage\n  config:\n    base_dir: /path/to/dagster/home/history\n\nschedule_storage:\n  module: dagster.core.storage.schedules\n  class: SqliteScheduleStorage\n  config:\n    base_dir: /path/to/dagster/home/schedules\n\nscheduler:\n  module: dagster.core.scheduler\n  class: DagsterDaemonScheduler\n\nrun_coordinator:\n  module: dagster.core.run_coordinator\n  class: DefaultRunCoordinator\n\nrun_launcher:\n  module: dagster.core.launcher\n  class: DefaultRunLauncher\n\ntelemetry:\n  enabled: true\n\nrun_monitoring:\n  enabled: true\n  poll_interval_seconds: 60\n\nrun_retries:\n  enabled: true\n  max_retries: 3\n  retry_on_asset_or_op_failure: true\n\ncode_servers:\n  local_startup_timeout: 360\n\nsecrets:\n  my_secret:\n    env: MY_SECRET_ENV_VAR\n\nretention:\n  schedule:\n    purge_after_days: 90\n  sensor:\n    purge_after_days:\n      skipped: 7\n      failure: 30\n      success: -1\n\nsensors:\n  use_threads: true\n  num_workers: 8\n\nschedules:\n  use_threads: true\n  num_workers: 8\n\nauto_materialize:\n  enabled: true\n  minimum_interval_seconds: 3600\n  run_tags:\n    key: 'value'\n  respect_materialization_data_versions: true\n  max_tick_retries: 3\n  use_sensors: false\n  use_threads: false\n  num_workers: 4\n```\n\n----------------------------------------\n\nTITLE: Setting up a new project with pip installation of dg\nDESCRIPTION: Commands to create a new project directory, set up a Python virtual environment, activate it, and install the dagster-dg package using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my_project && cd my_project\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv && source .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-dg\n```\n\n----------------------------------------\n\nTITLE: Defining Assets for ETL Pipeline in Python\nDESCRIPTION: Python code defining assets for reading CSV, dropping table, creating table, inserting rows, and reading table in Teradata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset\n\n@asset(required_resource_keys={\"teradata\"})\ndef read_csv_file(context):\n    df = pd.read_csv(\"dagster_quickstart/data/sample_data.csv\")\n    context.log.info(df)\n    return df\n\n@asset(required_resource_keys={\"teradata\"})\ndef drop_table(context):\n    result = context.resources.teradata.drop_table([\"tmp_table\"])\n    context.log.info(result)\n\n@asset(required_resource_keys={\"teradata\"})\ndef create_table(context, drop_table):\n    result = context.resources.teradata.execute_query('''CREATE TABLE tmp_table (\n                                                            id INTEGER,\n                                                            name VARCHAR(50),\n                                                            age INTEGER,\n                                                            city VARCHAR(50));''')\n    context.log.info(result)\n\n@asset(required_resource_keys={\"teradata\"}, deps=[read_csv_file])\ndef insert_rows(context, create_table, read_csv_file):\n    data_tuples = [tuple(row) for row in read_csv_file.to_numpy()]\n    for row in data_tuples:\n        result = context.resources.teradata.execute_query(\n            f\"INSERT INTO tmp_table (id, name, age, city) VALUES ({row[0]}, '{row[1]}', {row[2]}, '{row[3]}');\"\n        )\n        context.log.info(result)\n\n@asset(required_resource_keys={\"teradata\"})\ndef read_table(context, insert_rows):\n    result = context.resources.teradata.execute_query(\"select * from tmp_table;\", True)\n    context.log.info(result)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Helm Chart with Celery Support\nDESCRIPTION: Command to install or upgrade a Dagster Helm release with Celery configuration. Sets the run launcher type to CeleryK8sRunLauncher and enables RabbitMQ.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/kubernetes-and-celery.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --install dagster dagster/dagster -f /path/to/values.yaml \\\n  --set runLauncher.type=CeleryK8sRunLauncher \\\n  --set rabbitmq.enabled=true\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Asset with PipesECSClient\nDESCRIPTION: Define a Dagster asset that uses PipesECSClient to launch an ECS task. This asset specifies the task definition, cluster, and other parameters for the ECS task.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-ecs-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef ecs_pipes_asset(pipes_ecs: PipesECSClient):\n    result = pipes_ecs.launch_task(\n        cluster=\"my-cluster\",\n        task_definition=\"my-task-definition\",\n        launch_type=\"FARGATE\",\n        network_configuration={\n            \"awsvpcConfiguration\": {\n                \"subnets\": [\"subnet-12345678\"],\n                \"securityGroups\": [\"sg-12345678\"],\n                \"assignPublicIp\": \"ENABLED\",\n            }\n        },\n    )\n    return result\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Schedule Definition in Python\nDESCRIPTION: Demonstrates how to define a basic schedule using ScheduleDefinition with a job and cron schedule. The schedule is configured to run a job at 9am every day.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndaily_refresh_job = define_asset_job(\"daily_refresh_job\", selection=\"*\")\n\ndaily_schedule = ScheduleDefinition(\n    job=daily_refresh_job,\n    cron_schedule=\"0 9 * * *\",  # At 9am every day\n)\n```\n\n----------------------------------------\n\nTITLE: Blocking Downstream Assets with Asset Checks in Python using Dagster\nDESCRIPTION: Shows how to prevent downstream asset materialization when an asset check fails by setting blocking=True.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/asset-checks.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, asset_check\n\n@asset\ndef orders():\n    return pd.DataFrame({\"order_id\": [1, 2, 3], \"item_id\": [\"a\", \"b\", \"c\"]})\n\n@asset_check(asset=orders, blocking=True)\ndef orders_id_has_no_nulls(_):\n    df = orders()\n    if df[\"order_id\"].isnull().any():\n        return AssetCheckResult(success=False)\n\n@asset\ndef augmented_orders(orders):\n    return orders.assign(order_value=100)\n```\n\n----------------------------------------\n\nTITLE: Integrating Hightouch with Dagster\nDESCRIPTION: Example code demonstrating how to integrate Hightouch with Dagster. It includes setting up resources, defining an asset, and creating a job to trigger and monitor Hightouch syncs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/hightouch.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, asset, define_asset_job, EnvVar, job, op\nfrom dagster_hightouch import HighTouchResource, hightouch_sync_op\n\n@asset\ndef upstream_data():\n    return [1, 2, 3, 4, 5]\n\n@asset(deps=[upstream_data])\ndef hightouch_sync(context: AssetExecutionContext, upstream_data):\n    return hightouch_sync_op(\n        sync_id=\"1234\",\n        poll_interval=10,\n        poll_timeout=600,\n    )(context)\n\nhightouch_job = define_asset_job(\n    \"hightouch_job\",\n    selection=[\"upstream_data\", \"hightouch_sync\"],\n).resolve(\n    resources={\n        \"hightouch\": HighTouchResource(\n            api_key=EnvVar(\"HIGHTOUCH_API_KEY\"),\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: EMR Job Script with Dagster Pipes Integration\nDESCRIPTION: Python script implementing EMR job logic with Dagster Pipes integration for message passing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-containers-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import open_dagster_pipes\nfrom dagster_aws.pipes import PipesS3MessageWriter\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\nwith open_dagster_pipes(PipesS3MessageWriter()) as pipes:\n    # Define job code\n    pipes.log(\"Executing EMR job...\")\n    # ... rest of job code here\n    pipes.log(\"EMR job completed successfully\")\n```\n\n----------------------------------------\n\nTITLE: Creating Prompt Records for LLM Fine-Tuning in Python\nDESCRIPTION: This function creates prompt records formatted as conversational messages for LLM fine-tuning. It formats graphic novel data into a system prompt and user-assistant message pairs that mimic how a chatbot would respond to queries about graphic novels.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/file-creation.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef create_prompt_record(record: dict) -> dict:\n    # Create messages for the novel\n    system_prompt = (\n        \"As a comics expert, you specialize in recommending graphic novels to readers. \"\n        \"Provide information about graphic novels.\"\n    )\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Tell me about the graphic novel '{record['title']}'\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": f\"'{record['title']}' is a graphic novel by {record['author']} published in {record['publishing_year']}. It falls into the {record['genre']} genre and has a rating of {record['avg_rating']} out of 5 stars based on user reviews. {record['description']}\"\n        }\n    ]\n    return {'messages': messages}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom CSV I/O Manager in Python\nDESCRIPTION: This snippet defines a custom I/O manager that reads and writes CSV files to the filesystem. It extends the ConfigurableIOManager class and implements handle_output and load_input methods.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/defining-a-custom-io-manager.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass CSVIOManager(ConfigurableIOManager):\n    prefix: str = Field(default=\"\", description=\"Prefix path to store files\")\n\n    def handle_output(self, context, obj):\n        file_path = self._get_path(context)\n        obj.to_csv(file_path, index=False)\n\n    def load_input(self, context):\n        file_path = self._get_path(context.upstream_output)\n        return pd.read_csv(file_path)\n\n    def _get_path(self, context):\n        return os.path.join(self.prefix, context.asset_key.path[-1] + \".csv\")\n```\n\n----------------------------------------\n\nTITLE: Creating a GitHub Client Resource with ConfigurableResource\nDESCRIPTION: Implementation of a GitHub client resource using Dagster's ConfigurableResource. This resource requires an access token and provides a method to get a GitHub client instance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n## resources.py\n\nfrom dagster import StringSource, resource\nfrom github import Github\n\nclass GithubClientResource(ConfigurableResource):\n  access_token: str\n\n  def get_client(self) -> Github:\n    return Github(self.access_token)\n```\n\n----------------------------------------\n\nTITLE: Implementing GitHub Resource for Querying Issues and Discussions in Python\nDESCRIPTION: This code snippet defines a GithubResource class with methods to retrieve issues and discussions from GitHub using GraphQL queries. It also includes functions to convert the retrieved data into LangChain Documents for easier processing in the RAG system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/sources.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass GithubResource:\n    def __init__(self, github_token: str):\n        self._github_token = github_token\n\n    def get_issues(self, owner: str, repo: str, start_date: str, end_date: str) -> List[Document]:\n        query = f\"\"\"\n        query ($owner: String!, $repo: String!, $startDate: DateTime!, $endDate: DateTime!, $after: String) {{\n            repository(owner: $owner, name: $repo) {{\n                issues(first: 100, after: $after, filterBy: {{since: $startDate}}, orderBy: {{field: CREATED_AT, direction: DESC}}) {{\n                    pageInfo {{\n                        hasNextPage\n                        endCursor\n                    }}\n                    nodes {{\n                        number\n                        title\n                        body\n                        createdAt\n                        updatedAt\n                        url\n                        labels(first: 10) {{\n                            nodes {{\n                                name\n                            }}\n                        }}\n                        author {{\n                            login\n                        }}\n                        comments(first: 100) {{\n                            nodes {{\n                                body\n                                createdAt\n                                author {{\n                                    login\n                                }}\n                            }}\n                        }}\n                    }}\n                }}\n            }}\n        }}\n        \"\"\"\n        # Implementation details omitted for brevity\n\n    def get_discussions(self, owner: str, repo: str, start_date: str, end_date: str) -> List[Document]:\n        query = f\"\"\"\n        query ($owner: String!, $repo: String!, $startDate: DateTime!, $endDate: DateTime!, $after: String) {{\n            repository(owner: $owner, name: $repo) {{\n                discussions(first: 100, after: $after, orderBy: {{field: CREATED_AT, direction: DESC}}) {{\n                    pageInfo {{\n                        hasNextPage\n                        endCursor\n                    }}\n                    nodes {{\n                        number\n                        title\n                        body\n                        createdAt\n                        updatedAt\n                        url\n                        category {{\n                            name\n                        }}\n                        author {{\n                            login\n                        }}\n                        comments(first: 100) {{\n                            nodes {{\n                                body\n                                createdAt\n                                author {{\n                                    login\n                                }}\n                            }}\n                        }}\n                    }}\n                }}\n            }}\n        }}\n        \"\"\"\n        # Implementation details omitted for brevity\n\n    def convert_discussions_to_documents(self, discussions: List[Dict]) -> List[Document]:\n        # Implementation details omitted for brevity\n\n    def convert_issues_to_documents(self, issues: List[Dict]) -> List[Document]:\n        # Implementation details omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Core and Webserver\nDESCRIPTION: Command to install the main Dagster library and webserver components using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/installation.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Complete Dagster Code for Subprocess Execution in Python\nDESCRIPTION: This is the complete dagster_code.py file, including imports, asset definition, and Definitions object creation for subprocess execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/create-subprocess-asset.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport shutil\n\nfrom dagster import (\n    AssetExecutionContext,\n    Definitions,\n    asset,\n)\nfrom dagster_pipes import PipesSubprocessClient\n\n@asset(required_resource_keys={\"pipes_subprocess_client\"})\ndef subprocess_asset(context: AssetExecutionContext):\n    cmd = [\n        shutil.which(\"python\"),\n        os.path.join(os.path.dirname(__file__), \"external_code.py\"),\n    ]\n\n    with context.resources.pipes_subprocess_client.run(cmd) as invocation:\n        result = invocation.get_materialize_result()\n        return result\n\ndefs = Definitions(\n    assets=[subprocess_asset],\n    resources={\n        \"pipes_subprocess_client\": PipesSubprocessClient(),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dagster Pipes in ECS Task Script\nDESCRIPTION: Modify the ECS task script to use open_dagster_pipes for creating a context to send messages to Dagster. This example demonstrates logging, asset materialization, and metadata handling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-ecs-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom dagster_pipes import open_dagster_pipes\n\nwith open_dagster_pipes() as ctx:\n    ctx.log.info(\"Starting task\")\n    time.sleep(5)\n    ctx.log.info(\"Task complete\")\n\n    ctx.materialize_asset(\n        asset_key=[\"my_asset\"],\n        metadata={\n            \"rows\": {\"raw_value\": 10, \"type\": \"int\"},\n            \"columns\": {\"raw_value\": 5, \"type\": \"int\"},\n            \"dtypes\": {\"raw_value\": {\"a\": \"int\", \"b\": \"str\"}, \"type\": \"dict\"},\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Syncing and Materializing Airbyte Cloud Assets\nDESCRIPTION: Use Dagster to sync Airbyte Cloud connections and materialize connection tables using the build_airbyte_assets_definitions factory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_airbyte import AirbyteCloudWorkspace, build_airbyte_assets_definitions\n\nairbyte_cloud_resource = AirbyteCloudWorkspace(\n    workspace_id=\"your_workspace_id\",\n    client_id=\"your_client_id\",\n    client_secret=\"your_client_secret\",\n)\n\nairbyte_cloud_assets = build_airbyte_assets_definitions(airbyte_cloud_resource)\n\ndefs = Definitions(\n    assets=airbyte_cloud_assets,\n    resources={\"airbyte_cloud\": airbyte_cloud_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Linear Op Graph in Python\nDESCRIPTION: Demonstrates how to create a simple linear op graph using the @graph decorator in Dagster. The graph passes data through single inputs and outputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef return_one():\n    return 1\n\n@op\ndef add_one(num):\n    return num + 1\n\n@graph\ndef linear_graph():\n    value = return_one()\n    return add_one(value)\n```\n\n----------------------------------------\n\nTITLE: Complex Resource State Management with Context Manager in Python\nDESCRIPTION: Example demonstrating complex resource state management using a custom yield_for_execution context manager. Shows how to handle database connections that need to remain open during execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/managing-resource-state.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass DatabaseConnection(ConfigurableResource):\n    connection_url: str\n\n    _db: Any = PrivateAttr()\n\n    @contextmanager\n    def yield_for_execution(self, context: InitResourceContext):\n        self._db = create_connection(self.connection_url)\n        try:\n            yield self\n        finally:\n            self._db.close()\n\n    def query(self, query_string: str):\n        return self._db.execute(query_string)\n\n@asset(required_resource_keys={\"db\"})\ndef my_asset(db: DatabaseConnection):\n    return db.query(\"SELECT * FROM my_table\")\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Asset Without Arguments in Python\nDESCRIPTION: Example of testing a simple Dagster asset that has no input arguments. Shows how to directly invoke asset definitions and verify their output.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/unit-testing-assets-and-ops.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef test_simple_asset():\n    @asset\n    def my_asset():\n        return [1, 2, 3]\n\n    assert my_asset() == [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom CSV-Formatted Logger in Dagster\nDESCRIPTION: Example of a custom logger implementation that produces comma-separated values from selected fields in the log record. This demonstrates how to access both standard Python LogRecord fields and Dagster-specific metadata in a custom logger.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/custom-logging.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport csv\nimport io\nimport logging\nimport sys\n\nfrom dagster import Field, IntSource, StringSource, logger\nfrom typing import Any, Dict, Optional\n\n\nclass CsvConsoleHandler(logging.Handler):\n    \"\"\"\n    A custom logging handler that formats log messages as comma-separated values (CSV).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._stream = sys.stdout\n\n    def emit(self, record: logging.LogRecord):\n        \"\"\"\n        Format the record as CSV and write to the output stream.\n        \"\"\"\n        try:\n            # Create a string buffer to hold the CSV output\n            output = io.StringIO()\n            writer = csv.writer(output)\n\n            # Get the Dagster-specific metadata from the record\n            dagster_meta = getattr(record, \"dagster_meta\", {})\n\n            # Create a row with the fields we want to include\n            row = [\n                dagster_meta.get(\"log_timestamp\", \"\"),  # Timestamp\n                record.name,                            # Logger name\n                record.levelname,                       # Log level\n                dagster_meta.get(\"run_id\", \"\"),         # Run ID\n                dagster_meta.get(\"job_name\", \"\"),       # Job name\n                dagster_meta.get(\"op_name\", \"\"),        # Op name\n                record.getMessage()                      # Log message\n            ]\n\n            # Write the row to the CSV writer\n            writer.writerow(row)\n\n            # Get the CSV output and write to the stream\n            csv_line = output.getvalue().strip()\n            self._stream.write(csv_line + \"\\n\")\n            self._stream.flush()\n        except Exception as e:\n            self.handleError(record)\n\n\n@logger(\n    {\n        \"name\": Field(StringSource, is_required=False, default_value=\"dagster\"),\n        \"log_level\": Field(IntSource, is_required=False, default_value=logging.INFO),\n    },\n    description=\"A logger that outputs logs in CSV format\",\n)\ndef csv_console_logger(init_context):\n    \"\"\"\n    Initialize a logger that formats messages as CSV.\n\n    Args:\n        init_context: The logger initialization context.\n\n    Returns:\n        A configured logger instance.\n    \"\"\"\n    level = init_context.logger_config[\"log_level\"]\n    name = init_context.logger_config[\"name\"]\n\n    klass = logging.getLoggerClass()\n    logger_instance = klass(name, level=level)\n\n    handler = CsvConsoleHandler()\n    handler.setLevel(level)\n    logger_instance.addHandler(handler)\n\n    return logger_instance\n```\n\n----------------------------------------\n\nTITLE: Custom I/O Manager for Pandas and PySpark DataFrames with BigQuery in Dagster\nDESCRIPTION: Demonstrates how to create a custom I/O manager that handles both Pandas and PySpark DataFrames for BigQuery in Dagster. This allows for flexible handling of different DataFrame types.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import IOManager, OutputContext, InputContext, AssetKey\nfrom dagster_gcp import BigQueryIOManager\nfrom dagster_gcp_pandas import BigQueryPandasTypeHandler\nfrom dagster_gcp_pyspark import BigQueryPySparkTypeHandler\n\nclass PandasAndPySparkBigQueryIOManager(BigQueryIOManager):\n    def __init__(self, project, **kwargs):\n        super().__init__(project, **kwargs)\n\n    def type_handlers(self):\n        return {\n            \"pandas\": BigQueryPandasTypeHandler(),\n            \"pyspark\": BigQueryPySparkTypeHandler(),\n        }\n\n    def default_load_type(self) -> str:\n        return \"pandas\"\n\n    def get_output_asset_key(self, context: OutputContext) -> AssetKey:\n        return context.asset_key\n\n    def get_input_asset_key(self, context: InputContext) -> AssetKey:\n        return context.asset_key\n```\n\n----------------------------------------\n\nTITLE: Setting Per-job Kubernetes Configuration in Python\nDESCRIPTION: This example shows how to use the 'dagster-k8s/config' tag on a Dagster job to specify custom Kubernetes configuration for that job. It includes settings for container resources, volumes, and metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@job(\n    tags={\n        \"dagster-k8s/config\": {\n            \"container_config\": {\n                \"resources\": {\n                    \"requests\": {\"cpu\": \"250m\", \"memory\": \"64Mi\"},\n                    \"limits\": {\"cpu\": \"500m\", \"memory\": \"2560Mi\"},\n                }\n            },\n            \"pod_spec_config\": {\n                \"volumes\": [\n                    {\"name\": \"my-volume\", \"emptyDir\": {}}\n                ]\n            },\n            \"pod_template_spec_metadata\": {\n                \"annotations\": {\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\"}\n            },\n        }\n    }\n)\ndef my_job():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Resources with Environment Variables in Dagster\nDESCRIPTION: Python code showing how to use environment variables to configure Snowflake resources in Dagster. It demonstrates setting up both local and production configurations using the EnvVar class.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-databases.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\n\ndefs = Definitions(\n    assets=[iris_dataset, iris_db],\n    resources={\n        \"iris_db\": {\n            \"local\": SnowflakeResource(\n                account=EnvVar(\"DEV_SNOWFLAKE_ACCOUNT\"),\n                user=EnvVar(\"DEV_SNOWFLAKE_USER\"),\n                password=EnvVar(\"DEV_SNOWFLAKE_PASSWORD\"),\n                database=EnvVar(\"DEV_SNOWFLAKE_DATABASE\"),\n                schema=EnvVar(\"DEV_SNOWFLAKE_SCHEMA\"),\n                warehouse=EnvVar(\"DEV_SNOWFLAKE_WAREHOUSE\"),\n            ),\n            \"production\": SnowflakeResource(\n                account=EnvVar(\"PROD_SNOWFLAKE_ACCOUNT\"),\n                user=EnvVar(\"PROD_SNOWFLAKE_USER\"),\n                password=EnvVar(\"PROD_SNOWFLAKE_PASSWORD\"),\n                database=EnvVar(\"PROD_SNOWFLAKE_DATABASE\"),\n                schema=EnvVar(\"PROD_SNOWFLAKE_SCHEMA\"),\n                warehouse=EnvVar(\"PROD_SNOWFLAKE_WAREHOUSE\"),\n            ),\n        }[EnvVar(\"deployment_name\")]\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple I/O Managers with Dagster Assets\nDESCRIPTION: Demonstrates how to use different I/O managers for different assets in Dagster. This example shows storing one asset in BigQuery and another in Amazon S3.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@asset(io_manager_key=\"warehouse_io_manager\")\ndef iris_data():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n\n@asset(io_manager_key=\"blob_io_manager\")\ndef iris_plots(iris_data):\n    return px.scatter(iris_data, x=\"sepal length\", y=\"sepal width\", color=\"species\")\n\ndefs = Definitions(\n    assets=[iris_data, iris_plots],\n    resources={\n        \"warehouse_io_manager\": BigQueryIOManager(\n            project=\"my-project\", dataset=\"my_dataset\"\n        ),\n        \"blob_io_manager\": S3IOManager(s3_bucket=\"my-s3-bucket\"),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Standalone Dagster Assets with Scheduling After Airflow Migration\nDESCRIPTION: This code demonstrates the final state of Dagster assets after migration from Airflow. It defines weather-related assets that retrieve, process, and analyze weather data with appropriate scheduling using ScheduleDefinition. The code includes asset dependencies and a proper schedule configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/decommission.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, AssetKey, ScheduleDefinition, asset\n\nfrom tutorial_example.dagster_defs.resources import (\n    DevResourceFactory,\n    ProdResourceFactory,\n    make_resource_defs,\n)\n\n\n@asset(group_name=\"weather\", compute_kind=\"python\")\ndef download_weather_report(context: AssetExecutionContext) -> bytes:\n    \"\"\"Downloads the weather report\"\"\"\n    url = context.resources.config[\"weather_dataset_url\"]\n    response_content = context.resources.http.get(url).content\n    context.log.info(f\"Downloaded {len(response_content)} bytes\")\n    return response_content\n\n\n@asset(group_name=\"weather\", compute_kind=\"dbt\")\ndef analyzed_weather_data(context: AssetExecutionContext, download_weather_report: bytes):\n    \"\"\"Imports and analyzes the weather data\"\"\"\n    # download_weather_report is passed in as a dependency\n    wmo_ids = context.resources.weather_processor.process_weather_data(download_weather_report)\n    context.log.info(f\"Analyzed data for {len(wmo_ids)} weather stations\")\n\n\n@asset(group_name=\"weather\", compute_kind=\"python\")\ndef weather_report_predictions(context: AssetExecutionContext, analyzed_weather_data):\n    \"\"\"Make predictions based on weather data\"\"\"\n    # analyzed_weather_data is passed in as a dependency\n    context.resources.weather_predictor.predict(AssetKey(\"analyzed_weather_data\"))\n    context.log.info(\"Generated predictions\")\n\n\nweather_pipeline_assets = [\n    download_weather_report,\n    analyzed_weather_data,\n    weather_report_predictions,\n]\n\nstandalone_defs = [\n    *weather_pipeline_assets,\n    ScheduleDefinition(\n        job_name=\"weather_pipeline_job\",\n        cron_schedule=\"0 0 * * *\",  # daily at midnight\n        asset_selection=weather_pipeline_assets,\n    ),\n    make_resource_defs,\n    DevResourceFactory,\n    ProdResourceFactory,\n]\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster Development Server\nDESCRIPTION: Command to start both the Dagster webserver and daemon for local development. This provides a full local deployment of Dagster and typically runs on port 3000.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/webserver.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Test Sets using Dagster\nDESCRIPTION: Asset that splits the story data into training and test sets for model evaluation, using scikit-learn's train_test_split function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/ml-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(deps=[recent_stories])\ndef training_test_data(recent_stories: pd.DataFrame) -> TrainingTestData:\n    X = recent_stories[\"title\"]\n    y = recent_stories[\"descendants\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    return TrainingTestData(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n```\n\n----------------------------------------\n\nTITLE: Custom Power BI Semantic Model Materialization\nDESCRIPTION: Advanced example showing how to create custom asset definitions for materializing Power BI semantic models with additional functionality\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/powerbi.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(name=\"my_semantic_model\")\ndef my_semantic_model(power_bi):\n    # Custom pre-refresh logic\n    print(\"Running pre-refresh tasks...\")\n    \n    # Trigger the refresh\n    power_bi.refresh_semantic_model(\"your-semantic-model-id\")\n    \n    # Custom post-refresh logic\n    print(\"Running post-refresh tasks...\")\n```\n\n----------------------------------------\n\nTITLE: Constructing GraphDefinition Programmatically in Python\nDESCRIPTION: This snippet demonstrates how to programmatically construct a GraphDefinition object in Dagster, defining ops, their dependencies, and connecting inputs and outputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import GraphDefinition, In, Out, DependencyDefinition, op\n\n@op\ndef return_one(context):\n    return 1\n\n@op\ndef add_one(context, num):\n    return num + 1\n\n@op(ins={\"num1\": In(), \"num2\": In()})\ndef adder(context, num1, num2):\n    return num1 + num2\n\nmy_graph = GraphDefinition(\n    name=\"my_graph\",\n    node_defs=[return_one, add_one, adder],\n    dependencies={\n        \"add_one\": {\n            \"num\": DependencyDefinition(\"return_one\"),\n        },\n        \"adder\": {\n            \"num1\": DependencyDefinition(\"return_one\"),\n            \"num2\": DependencyDefinition(\"add_one\"),\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring ETL Tables Using YAML\nDESCRIPTION: YAML configuration that defines table specifications including names, dependencies, and SQL queries for data transformation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-dependencies-with-asset-factories.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntables:\n  - name: \"daily_metrics\"\n    dependencies: []\n    query: \"SELECT * FROM raw_metrics WHERE date = current_date\"\n  - name: \"weekly_metrics\"\n    dependencies: [\"daily_metrics\"]\n    query: \"SELECT * FROM daily_metrics GROUP BY week\"\n  - name: \"monthly_metrics\"\n    dependencies: [\"weekly_metrics\"]\n    query: \"SELECT * FROM weekly_metrics GROUP BY month\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Weekly Partitions in Dagster Asset\nDESCRIPTION: Creates a weekly partition definition to manage GitHub data extraction and avoid rate limiting. The partition is configured to update every Monday using AutomationCondition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/embeddings.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nweekly_partitions = WeeklyPartitionsDefinition(start_date=\"2023-01-01\")\n```\n\n----------------------------------------\n\nTITLE: AWS Lambda Function Implementation with Dagster Pipes\nDESCRIPTION: Python code for the Lambda function handler that uses Dagster Pipes to communicate with Dagster. Demonstrates logging and asset materialization reporting.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-lambda-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import PipesMappingParamsLoader, open_dagster_pipes\n\ndef handler(event, context):\n    with open_dagster_pipes(\n        PipesMappingParamsLoader(event[\"pipes_info\"])\n    ) as pipes:\n        pipes.log(message=\"Executing lambda function\")\n        pipes.report_asset_materialization(\n            \"my_asset\",\n            metadata={\n                \"sample_float\": {\"raw_value\": 42.0, \"type\": \"float\"},\n                \"sample_string\": {\"raw_value\": \"hello\", \"type\": \"string\"},\n            },\n        )\n```\n\n----------------------------------------\n\nTITLE: Creating a Stateful I/O Manager Factory in Python\nDESCRIPTION: This example demonstrates how to implement a stateful I/O manager using ConfigurableIOManagerFactory. It creates a factory function that returns an I/O manager with a cache.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/defining-a-custom-io-manager.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass CachedIOManager(IOManager):\n    def __init__(self, cache):\n        self.cache = cache\n\n    def handle_output(self, context, obj):\n        self.cache[context.asset_key] = obj\n\n    def load_input(self, context):\n        return self.cache[context.asset_key]\n\nclass CachedIOManagerFactory(ConfigurableIOManagerFactory):\n    cache_size: int\n\n    def create_io_manager(self, context) -> CachedIOManager:\n        return CachedIOManager(cache=LRUCache(self.cache_size))\n```\n\n----------------------------------------\n\nTITLE: Storing Time-Partitioned Assets in DuckDB\nDESCRIPTION: This example demonstrates storing time-partitioned assets in DuckDB using the DuckDB I/O manager. It uses the 'partition_expr' metadata to specify the time column for partitioning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"partition_expr\": \"TO_TIMESTAMP(TIME)\"})\ndef iris_by_time():\n    return pd.DataFrame({\n        \"TIME\": [1672531200, 1672617600, 1672704000],  # 2023-01-01, 2023-01-02, 2023-01-03\n        \"SPECIES\": [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"],\n        \"sepal_length_cm\": [5.1, 7.0, 6.3],\n        \"sepal_width_cm\": [3.5, 3.2, 3.3],\n        \"petal_length_cm\": [1.4, 4.7, 6.0],\n        \"petal_width_cm\": [0.2, 1.4, 2.5],\n    })\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Commands to install the necessary Python packages (Pandas, DuckDB, and PyArrow) for data fetching and storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/upstream-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install pandas duckdb pyarrow\n```\n\n----------------------------------------\n\nTITLE: Initializing PowerBI Workspace Resource in Dagster\nDESCRIPTION: Creates a PowerBIWorkspace resource to establish communication between Dagster and Power BI. This resource allows Dagster to interact with Power BI dashboards and reports.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/dashboard.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npowerbi_workspace = PowerBIWorkspace(\n    service_principal={\n        \"tenant_id\": {\"env\": \"POWER_BI_TENANT_ID\"},\n        \"client_id\": {\"env\": \"POWER_BI_CLIENT_ID\"},\n        \"client_secret\": {\"env\": \"POWER_BI_CLIENT_SECRET\"},\n    },\n    workspace_id={\"env\": \"POWER_BI_WORKSPACE_ID\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Dagster Op Without Retry\nDESCRIPTION: Example of a basic Dagster op that fails without retry mechanism, requiring the entire job to be rerun on failure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-retries.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op():\n    if random() > 0.5:\n        raise Exception(\"Random failure!\")\n    return \"success\"\n```\n\n----------------------------------------\n\nTITLE: Preventing Concurrent Runs Using a Schedule in Python\nDESCRIPTION: This Python code shows how to use a schedule to prevent a new run from starting if another run is already in progress. It uses Dagster's metadata to check for currently running jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@schedule(\n    job=my_job,\n    cron_schedule=\"0 0 * * *\",\n)\ndef my_schedule(context):\n    # Only start a run if there are no currently running jobs\n    instance = context.instance\n    running_job_count = instance.get_runs_count(\n        filters=RunsFilter(statuses=[DagsterRunStatus.STARTED])\n    )\n    if running_job_count == 0:\n        return RunRequest(run_key=None, run_config={})\n    else:\n        return SkipReason(\"A job is already running\")\n```\n\n----------------------------------------\n\nTITLE: Adding Staging Configuration to Dagster Definitions\nDESCRIPTION: Python code snippet showing how to add a staging configuration to the Dagster Definitions for a self-hosted staging deployment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"staging\": {\n    \"io_manager\": SnowflakePandasIOManager(\n        account=EnvVar(\"SNOWFLAKE_ACCOUNT\"),\n        user=EnvVar(\"SNOWFLAKE_USER\"),\n        password=EnvVar(\"SNOWFLAKE_PASSWORD\"),\n        database=\"STAGING\",\n        warehouse=EnvVar(\"SNOWFLAKE_WAREHOUSE\"),\n        schema=EnvVar(\"SNOWFLAKE_SCHEMA\"),\n        role=EnvVar(\"SNOWFLAKE_ROLE\"),\n    )\n},\n```\n\n----------------------------------------\n\nTITLE: Defining Product Category Partition in Dagster\nDESCRIPTION: Creates a static partition definition for product categories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-partitioned-asset.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nproduct_category_partition = dg.StaticPartitionsDefinition(\n    [\"electronics\", \"clothing\", \"books\", \"accessories\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Defining ReplicationProject Class in Python\nDESCRIPTION: A class to encapsulate the logic for loading and processing the replication configuration YAML file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\nfrom pathlib import Path\n\n\nclass ReplicationProject():\n    def __init__(self, replication_configuration_yaml: str):\n        self.replication_configuration_yaml = replication_configuration_yaml\n\n    def load(self):\n        return yaml.safe_load(Path(self.replication_configuration_yaml).read_text())\n```\n\n----------------------------------------\n\nTITLE: Creating a PowerBI Translator for dbt Models\nDESCRIPTION: Defines a translator that maps Power BI assets to their corresponding dbt models. This ensures that Power BI reports are properly linked to their data sources and updated when those sources change.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/dashboard.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npowerbi_translator = PowerBITranslator(\n    powerbi_workspace=powerbi_workspace,\n    upstream_asset_key_by_powerbi_table_name={\n        \"fact_engagement\": AssetKey([\"dbt\", \"fact_engagement\"]),\n        \"dim_urls\": AssetKey([\"dbt\", \"dim_urls\"]),\n        \"dim_users\": AssetKey([\"dbt\", \"dim_users\"]),\n        \"fact_post\": AssetKey([\"dbt\", \"fact_post\"]),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing PineconeResource in Dagster Configuration\nDESCRIPTION: This snippet demonstrates how to initialize the PineconeResource in a Dagster project configuration. It uses the @resource decorator and configures the resource with an API key loaded from environment variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/vector-database.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@resource(config_schema={\"api_key\": str})\ndef pinecone_resource(context):\n    return PineconeResource(api_key=context.resource_config[\"api_key\"])\n```\n\n----------------------------------------\n\nTITLE: Using Snowflake Resource in Dagster Assets\nDESCRIPTION: Python code demonstrating how to use the SnowflakeResource in Dagster assets. It defines two assets that interact with the Snowflake database and includes them in a Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-databases.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\nimport pandas as pd\n\n@asset\ndef iris_dataset():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n\n@asset\ndef iris_db(iris_dataset, iris_db: SnowflakeResource):\n    iris_db.insert_dataframe(iris_dataset, \"iris_table\")\n    return iris_db.execute_query(\"SELECT * FROM iris_table LIMIT 5\")\n\ndefs = Definitions(\n    assets=[iris_dataset, iris_db],\n    resources={\n        \"iris_db\": SnowflakeResource(\n            account=\"my_account\",\n            user=\"my_user\",\n            password=\"my_password\",\n            database=\"my_database\",\n            schema=\"my_schema\",\n            warehouse=\"my_warehouse\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Dagster Components in a Central Definitions File\nDESCRIPTION: This file imports and organizes all the separate components (assets, partitions, schedules, sensors) into a single Definitions object that Dagster uses to understand the project structure. It shows how to load assets from modules and create a cohesive project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/refactor-your-project.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Code placeholder for definitions.py\n# The actual implementation would be shown in the CodeExample component\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Virtual Environment for Windows\nDESCRIPTION: Commands for creating and activating a Python virtual environment on Windows. This isolates the dependencies for the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv dagster_tutorial dagster_tutorial\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Custom Document I/O Manager Implementation\nDESCRIPTION: Implementation of a custom I/O manager for handling LangChain Documents serialization and deserialization. Manages storage and retrieval of Document objects as JSON files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/embeddings.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@io_manager(config_schema={\"base_dir\": StringSource})\ndef document_io_manager(init_context):\n    return DocumentIOManager(base_dir=init_context.resource_config[\"base_dir\"])\n\n\nclass DocumentIOManager(IOManager):\n    def __init__(self, base_dir):\n        self._base_dir = base_dir\n\n    def _get_path(self, context):\n        \"\"\"Defines the path at which the asset will be stored\"\"\"\n        key = context.asset_key.path[-1]\n\n        if context.has_asset_partitions:\n            partition_key = context.asset_partition_key_for_output()\n            return os.path.join(self._base_dir, key, f\"{partition_key}.json\")\n\n        return os.path.join(self._base_dir, f\"{key}.json\")\n\n    def handle_output(self, context, obj):\n        \"\"\"This saves the list of Document objects to a json file\"\"\"\n        docs_dict = [{\"page_content\": d.page_content, \"metadata\": d.metadata} for d in obj]\n\n        path = self._get_path(context)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n\n        with open(path, \"w\") as f:\n            json.dump(docs_dict, f)\n\n    def load_input(self, context):\n        \"\"\"This loads the list of Document objects from a json file\"\"\"\n        path = self._get_path(context.upstream_output)\n\n        with open(path, \"r\") as f:\n            docs_dict = json.load(f)\n\n        return [Document(page_content=d[\"page_content\"], metadata=d[\"metadata\"]) for d in docs_dict]\n```\n\n----------------------------------------\n\nTITLE: Creating a Weekly Update Schedule in Dagster\nDESCRIPTION: Defines a weekly update schedule that runs on Sundays at 1am for a set of assets, simulating a scenario where updated CSVs are uploaded at a specific time every week.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/automate-your-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nweekly_update_schedule = dg.ScheduleDefinition(\n    name=\"weekly_update_schedule\",\n    cron_schedule=\"0 1 * * 0\",  # At 1am on Sunday\n    job=dg.define_asset_job(name=\"analysis_update_job\", selection=[\"*\"])\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dependencies Between Factory and Regular Assets\nDESCRIPTION: Example showing how to define a regular Dagster asset that depends on assets created by a factory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-dependencies-with-asset-factories.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(deps=[\"daily_metrics\", \"weekly_metrics\"])\ndef metrics_summary():\n    # Process the metrics\n    pass\n```\n\n----------------------------------------\n\nTITLE: Rewriting Assets to Avoid Data Passing in Python\nDESCRIPTION: This example shows a refactored version of the previous example, where multiple operations (download, unzip, load) are combined into a single asset to avoid passing data between assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/passing-data-between-assets.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Content of passing-data-rewrite-assets.py file\n\n# Code example not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Testing Direct Resource Construction in Python\nDESCRIPTION: Demonstrates how to test a ConfigurableResource by constructing it manually without additional context or nested resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/testing-configurable-resources.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef test_my_resource():\n    resource = MyResource(foo=\"bar\")\n    assert resource.foo == \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Integrating Asset Jobs with Dagster Definitions\nDESCRIPTION: This example demonstrates how to include asset jobs in a Definitions object, making them available to Dagster tools such as the UI, GraphQL, and command line.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/asset-jobs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[asset1, asset2, asset3],\n    jobs=[job1, job2],\n    schedules=[schedule1, schedule2],\n    sensors=[sensor1, sensor2],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Sensor to Detect New S3 Files in Dagster\nDESCRIPTION: This snippet defines a sensor in Dagster that detects new files in a source S3 bucket. It uses the get_s3_keys function to retrieve unprocessed object keys and yields RunRequests for each new file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/monitor_files_in_aws_s3.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import (\n    Config,\n    Definitions,\n    RunConfig,\n    RunRequest,\n    SkipReason,\n    sensor,\n)\nfrom dagster_aws.s3 import S3Resource\nfrom dagster_aws.s3.sensor import get_s3_keys\n\nAWS_S3_BUCKET = \"example-source-bucket\"\nAWS_S3_OBJECT_PREFIX = \"example-source-prefix\"\n\nclass ObjectConfig(Config):\n    key: str\n\n@sensor(target=s3_file_backup)\ndef s3_backup_sensor(context):\n    latest_key = context.cursor or None\n    unprocessed_object_keys = get_s3_keys(\n        bucket=AWS_S3_BUCKET, prefix=AWS_S3_OBJECT_PREFIX, since_key=latest_key\n    )\n\n    for key in unprocessed_object_keys:\n        yield RunRequest(\n            run_key=key, run_config=RunConfig(ops={\"s3_file_backup\": ObjectConfig(key=key)})\n        )\n\n    if not unprocessed_object_keys:\n        return SkipReason(\"No new s3 files found for bucket source-bucket.\")\n\n    last_key = unprocessed_object_keys[-1]\n    context.update_cursor(last_key)\n\ndefs = Definitions(\n    assets=[s3_file_backup],\n    resources={\n        \"s3\": s3_resource,\n    },\n    sensors=[s3_backup_sensor],\n)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Resource with Dagster Ops\nDESCRIPTION: Demonstration of using the OpenAI resource within Dagster ops to interact with the OpenAI API.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/openai.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, job\nfrom dagster_openai import OpenAIResource\n\n@op\ndef openai_op(openai):\n    completion = openai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n    )\n    return completion.choices[0].message.content\n\n@job(resource_defs={\"openai\": OpenAIResource})\ndef my_job():\n    openai_op()\n```\n\n----------------------------------------\n\nTITLE: Submitting a Job Run with Dagster GraphQL Client\nDESCRIPTION: This example demonstrates how to submit a job run using the client, specifying the repository location, repository name, and job name for execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/graphql-client.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Submit a run\nrun_id = client.submit_job_execution(\n    repository_location_name=\"my_repository_location\",\n    repository_name=\"my_repository\",\n    job_name=\"my_job\",\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Webserver\nDESCRIPTION: The code starts the Dagster web server, allowing you to interact with your Dagster project through a web interface. This assumes the environment and dependencies have been set up correctly.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_wandb/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Asset With Context in Python\nDESCRIPTION: Demonstrates testing a Dagster asset that requires context by using build_asset_context() to construct a context object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/unit-testing-assets-and-ops.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef test_asset_with_context():\n    @asset\n    def my_asset(context):\n        return context.run_id\n\n    context = build_asset_context()\n    assert my_asset(context) is not None\n```\n\n----------------------------------------\n\nTITLE: Extracting and Aggregating Popular Shelves in SQL\nDESCRIPTION: Advanced SQL query that unpacks the popular_shelves field, aggregates the data, and ranks shelf categories by popularity to identify the most common book categorizations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/feature-engineering.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect\n\tshelf.name as category,\n\tsum(cast(shelf.count as integer)) as category_count\nfrom (\n    select\n        unnest(popular_shelves) as shelf\n    from graphic_novels\n)\ngroup by 1\norder by 2 desc\nlimit 15;\n```\n\n----------------------------------------\n\nTITLE: Customizing ECS Run Resources for Dagster\nDESCRIPTION: This YAML configuration customizes the default resources for Dagster runs in ECS. It specifies CPU, memory, and ephemeral storage allocations for the EcsRunLauncher.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: 'dagster_aws.ecs'\n  class: 'EcsRunLauncher'\n  config:\n    run_resources:\n      cpu: '256'\n      memory: '512' # In MiB\n      ephemeral_storage: 128 # In GiB\n```\n\n----------------------------------------\n\nTITLE: Customizing Power BI Asset Dependencies\nDESCRIPTION: Example showing how to customize upstream dependencies for Power BI assets using a custom DagsterPowerBITranslator\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/powerbi.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_semantic_model_asset_spec(self, semantic_model):\n    asset_spec = super().get_semantic_model_asset_spec(semantic_model)\n    if semantic_model.name == \"my_powerbi_semantic_model\":\n        asset_spec.dependencies.append(\"my_upstream_asset\")\n    return asset_spec\n```\n\n----------------------------------------\n\nTITLE: Storing Dagster Asset as BigQuery Table\nDESCRIPTION: Demonstrates creating a Dagster asset that fetches the Iris dataset as a Pandas DataFrame and stores it in BigQuery. The asset renames columns and uses type hints to indicate DataFrame handling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef iris_data() -> pd.DataFrame:\n    \"\"\"Raw iris data pulled from scikit-learn\"\"\"\n    iris = datasets.load_iris()\n    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n    # rename columns\n    df.columns = [col.replace(\" (cm)\", \"\").replace(\" \", \"_\") for col in df.columns]\n    return df\n```\n\n----------------------------------------\n\nTITLE: Defining Materialized Since Cron Scheduling Condition in Dagster\nDESCRIPTION: This snippet shows the materialized since cron scheduling condition, which allows for scheduling based on whether an asset has been materialized since a specific cron job ran, aiding in time-based executions of assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.materialized_since_cron()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Message Reader for Dagster Pipes\nDESCRIPTION: Example of creating a custom message reader that reads from a cloud service blob store with message chunking support.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/dagster-pipes-details-and-customization.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom cloud_service import read_from_blob_store\nfrom dagster import PipesBlobStoreMessageReader\n\nclass CloudServiceMessageReader(PipesBlobStoreMessageReader):\n    def __init__(self, path):\n        self._path = path\n        super().__init__()\n\n    def _read_chunks(self, start_chunk: int, end_chunk: int) -> list[str]:\n        all_chunks = read_from_blob_store(self._path)\n        return all_chunks[start_chunk:end_chunk]\n```\n\n----------------------------------------\n\nTITLE: Managing VantageCloud Lake Compute Clusters with dagster-teradata\nDESCRIPTION: A Dagster job that demonstrates compute cluster management for Teradata VantageCloud Lake. It creates a compute cluster, runs a dbt job, and then drops the compute cluster, showing the full lifecycle of compute resource management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, DagsterError, op, materialize, job\nfrom dagster_dbt import DbtCliResource\nfrom dagster_teradata import teradata_resource, TeradataResource\n\nfrom .assets import jaffle_shop_dbt_assets\nfrom .project import jaffle_shop_project\nfrom .schedules import schedules\n\n@op(required_resource_keys={\"teradata\"})\ndef create_compute_cluster(context):\n    context.resources.teradata.create_teradata_compute_cluster(\n        \"ShippingCG01\",\n        \"Shipping\",\n        \"STANDARD\",\n        \"TD_COMPUTE_MEDIUM\",\n        \"MIN_COMPUTE_COUNT(1) MAX_COMPUTE_COUNT(1) INITIALLY_SUSPENDED('FALSE')\",\n    )\n    return \"Compute Cluster Created\"\n\n@op(required_resource_keys={\"teradata\", \"dbt\"})\ndef run_dbt(context, status):\n    if status == \"Compute Cluster Created\":\n        materialize(\n            [jaffle_shop_dbt_assets],\n            resources={\n                \"dbt\": DbtCliResource(project_dir=jaffle_shop_project)\n            }\n        )\n        return \"DBT Run Completed\"\n    else:\n        raise DagsterError(\"DBT Run Failed\")\n\n@op(required_resource_keys={\"teradata\"})\ndef drop_compute_cluster(context, status):\n    if status == \"DBT Run Completed\":\n        context.resources.teradata.drop_teradata_compute_cluster(\"ShippingCG01\", \"Shipping\", True)\n    else:\n        raise DagsterError(\"DBT Run Failed\")\n\n@job(resource_defs={\"teradata\": teradata_resource, \"dbt\": DbtCliResource})\ndef example_job():\n    drop_compute_cluster(run_dbt(create_compute_cluster()))\n\ndefs = Definitions(\n    assets=[jaffle_shop_dbt_assets],\n    jobs=[example_job],\n    schedules=schedules,\n    resources={\n        \"dbt\": DbtCliResource(project_dir=jaffle_shop_project),\n        \"teradata\": TeradataResource(),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Graph with Multiple Inputs in Python\nDESCRIPTION: Illustrates how to create a graph where a single output is passed to multiple inputs on downstream ops, and then combined in a final op.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef return_one():\n    return 1\n\n@op\ndef add_one(num):\n    return num + 1\n\n@op\ndef mult_two(num):\n    return num * 2\n\n@op\ndef sum_two(num1, num2):\n    return num1 + num2\n\n@graph\ndef multiple_io_graph():\n    value = return_one()\n    added = add_one(value)\n    multed = mult_two(value)\n    return sum_two(added, multed)\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster Webserver\nDESCRIPTION: Start the Dagster webserver to ensure correct installation and setup of the environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/index.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Creating Asset Components in Dagster ETL Project\nDESCRIPTION: This file defines all the data assets for the ETL pipeline, including dbt models, transformations, and related operations. It appears to be the largest component of the project containing asset definitions and their relationships.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/refactor-your-project.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Code placeholder for assets.py\n# The actual implementation would be shown in the CodeExample component\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Source for Raw Customer Data\nDESCRIPTION: This YAML configuration defines a dbt source that points to the upstream 'raw_customers' asset created in Dagster. It establishes the connection between the Dagster asset and the dbt model.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: example\n    tables:\n      - name: raw_customers\n```\n\n----------------------------------------\n\nTITLE: Using Output Objects with Type Annotations in Dagster Ops\nDESCRIPTION: This example shows how to use Output objects with return annotations in Dagster ops, allowing for additional functionality while maintaining coherent type annotations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef string_output() -> Output[str]:\n    return Output(\"foo\")\n```\n\n----------------------------------------\n\nTITLE: Creating Downstream Assets with Snowflake Dependencies\nDESCRIPTION: Python code showing how to create downstream assets that depend on Snowflake table data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef iris_setosa(snowflake, iris_dataset):\n    snowflake.execute_query(\n        \"\"\"CREATE OR REPLACE TABLE IRIS_SETOSA AS\n        SELECT * FROM IRIS_DATASET WHERE SPECIES = 'setosa'\n        \"\"\",\n        quote_identifiers=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Executor for Local Execution in YAML\nDESCRIPTION: This YAML configuration block sets up the Dask executor for local execution. It specifies the cluster configuration and execution parameters for a Dagster job running on a local Dask cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/dask.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexecution:\n  config:\n    cluster:\n      n_workers: 2\n      threads_per_worker: 1\n    job_executor:\n      start_method: spawn\n```\n\n----------------------------------------\n\nTITLE: Adding Downstream Dependencies to Dagster Definitions\nDESCRIPTION: This Python code adds a downstream asset that depends on the output of the dbt 'customers' model. It creates a histogram of customer first names, demonstrating how to use dbt model outputs in Dagster assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport plotly.express as px\nfrom dagster import asset, Definitions, AssetExecutionContext\nfrom dagster_dbt import DbtCliResource, dbt_assets, get_asset_key_for_model\n\n@asset\ndef raw_customers():\n    return pd.DataFrame(\n        {\n            \"customer_id\": [\"1\", \"2\", \"3\"],\n            \"first_name\": [\"Rachel\", \"Monica\", \"Ross\"],\n            \"last_name\": [\"Green\", \"Geller\", \"Geller\"],\n        }\n    )\n\ndbt_resource = DbtCliResource(project_dir=\"./basic-dbt-project\")\n\n@dbt_assets(manifest=dbt_resource.get_manifest())\ndef my_dbt_assets():\n    pass\n\n@asset\ndef customers_first_name_histogram(context: AssetExecutionContext, dbt: DbtCliResource):\n    dbt_customers_table = dbt.duckdb_query(\n        \"SELECT * FROM customers\",\n        \"dbt_data.duckdb\",\n    )\n    customers_df = pd.DataFrame(dbt_customers_table)\n    fig = px.histogram(customers_df, x=\"first_name\")\n    fig.write_image(\"customers_first_name_histogram.png\")\n\ndefs = Definitions(\n    assets=[raw_customers, my_dbt_assets, customers_first_name_histogram],\n    resources={\"dbt\": dbt_resource}\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dataproc Job Script\nDESCRIPTION: Python script showing how to use dagster-pipes in a Dataproc job to send messages and metadata back to Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/gcp-dataproc-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/dagster/dagster_pipes/gcp/dataproc_job/script.py\" />\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model for Comment Prediction\nDESCRIPTION: Trains an XGBoost regression model to predict comment counts based on story titles, using the vectorized training data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/ml-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset(deps=[vectorized_data])\ndef regressor_model(vectorized_data: VectorizedData) -> RegressionModel:\n    regressor = XGBRegressor(random_state=0)\n    regressor.fit(vectorized_data.X_train, vectorized_data.y_train)\n    score = regressor.score(vectorized_data.X_test, vectorized_data.y_test)\n    return RegressionModel(regressor=regressor, score=score)\n```\n\n----------------------------------------\n\nTITLE: Defining Upstream Dependencies for Airbyte Cloud Assets\nDESCRIPTION: Set upstream dependencies for Airbyte Cloud assets by creating a custom DagsterAirbyteTranslator and overriding the get_upstream_assets method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass CustomAirbyteTranslator(DagsterAirbyteTranslator):\n    def get_upstream_assets(self, connection):\n        upstream_assets = super().get_upstream_assets(connection)\n        upstream_assets.append(AssetKey([\"custom\", \"upstream\", \"asset\"]))\n        return upstream_assets\n```\n\n----------------------------------------\n\nTITLE: Setting Asset Code Version in Python\nDESCRIPTION: Demonstrates how to assign a code version to an asset for tracking changes and avoiding redundant computation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-assets.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset(code_version=\"1\")\ndef asset_with_version():\n    with open(\"data/asset_with_version.json\", \"w\") as f:\n        json.dump(100, f)\n```\n\n----------------------------------------\n\nTITLE: Scheduling and Monitoring Asset Checks in Python with Dagster\nDESCRIPTION: Demonstrates how to schedule asset materialization and checks independently, and implement monitoring with email alerts on check failures.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/asset-checks.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, ScheduleDefinition, SensorDefinition, RunRequest, DefaultSensorStatus\n\n@asset\ndef orders():\n    return pd.DataFrame({\"order_id\": [1, 2, 3], \"item_id\": [\"a\", \"b\", \"c\"]})\n\n@asset_check(asset=orders)\ndef orders_id_has_no_nulls(_):\n    df = orders()\n    if df[\"order_id\"].isnull().any():\n        return AssetCheckResult(success=False)\n\nasset_job = define_asset_job(\"asset_job\", selection=[orders])\ncheck_job = define_asset_job(\"check_job\", selection=[orders_id_has_no_nulls])\n\nasset_schedule = ScheduleDefinition(\n    job=asset_job,\n    cron_schedule=\"0 0 * * *\",\n)\n\ncheck_schedule = ScheduleDefinition(\n    job=check_job,\n    cron_schedule=\"0 12 * * *\",\n)\n\ndef send_email_alert():\n    # Implementation of email alert\n    pass\n\n@sensor(job=check_job)\ndef check_alert_sensor(context):\n    if context.latest_run and context.latest_run.status == DagsterRunStatus.FAILURE:\n        send_email_alert()\n\ndefs = Definitions(\n    assets=[orders],\n    asset_checks=[orders_id_has_no_nulls],\n    schedules=[asset_schedule, check_schedule],\n    sensors=[check_alert_sensor],\n)\n```\n\n----------------------------------------\n\nTITLE: External DuckDB Table Integration\nDESCRIPTION: Shows how to make existing DuckDB tables available in Dagster using external assets. This allows tracking data lineage for pre-existing tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSpec\n\niris_harvest_data = AssetSpec(\"iris_harvest_data\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Resource-Dependent Schedule in Dagster\nDESCRIPTION: Example showing how to define a schedule that depends on a resource in Dagster. The code demonstrates a ConfigurableResource implementation and its usage within a scheduled job execution pattern.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/using-resources-in-schedules.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@resource\nclass MyNewResource(ConfigurableResource):\n    def __init__(self, connection_url: str):\n        self._connection_url = connection_url\n\n    def get_data(self):\n        return {\"url\": self._connection_url}\n\n@job\ndef my_job():\n    ...\n\n@schedule(\n    cron_schedule=\"0 0 * * *\",\n    job=my_job,\n)\ndef my_schedule(resource: MyNewResource):\n    data = resource.get_data()\n    return RunRequest(\n        run_key=None,\n        run_config=RunConfig(\n            ops={\"my_op\": {\"config\": {\"data\": data}}},\n        ),\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating GraphDefinition from YAML in Python\nDESCRIPTION: This Python snippet demonstrates how to programmatically generate a GraphDefinition from a YAML file using Dagster's GraphDSL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import graph_from_config\nfrom dagster.utils import file_relative_path\n\nwith open(file_relative_path(__file__, \"my_graph.yaml\"), \"r\") as fd:\n    graph_config = yaml.safe_load(fd.read())\n\nmy_graph = graph_from_config(graph_config, {\"return_one\": return_one, \"add_one\": add_one, \"adder\": adder})\n```\n\n----------------------------------------\n\nTITLE: Overriding Partition Dependencies with AssetDep in Python\nDESCRIPTION: Demonstrates how to override default partition dependency rules using AssetDep for basic asset dependencies. Shows configuration for daily-partitioned assets depending on specific partitions of upstream assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/defining-dependencies-between-partitioned-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2024-01-01\"))\ndef downstream_asset(context, upstream):\n    # Some computation using the upstream partition\n    pass\n\n@asset(deps=[\n    AssetDep(\n        \"upstream_asset\",\n        partition_mapping=TimeWindowPartitionMapping(\n            start_offset=-datetime.timedelta(days=1)\n        )\n    )\n])\ndef downstream_asset_with_mapping(context, upstream_asset):\n    # Some computation using the upstream partition\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining Pipelines, Schedules, and Repositories in Python (0.8.0+)\nDESCRIPTION: Updated example showing how to define pipelines, schedules, and repositories in Dagster 0.8.0 and later, using the new @repository decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n@pipeline\ndef test_pipeline():\n    ...\n\n@daily_schedule(\n    pipeline_name='test',\n    start_date=datetime.datetime(2020, 1, 1),\n)\ndef daily_test_schedule(_):\n    return {}\n\ntest_partition_set = PartitionSetDefinition(\n    name=\"test\",\n    pipeline_name=\"test\",\n    partition_fn=lambda: [\"test\"],\n    run_config_fn_for_partition=lambda _: {},\n)\n\n@repository\ndef test_repository():\n    return [test_pipeline, daily_test_schedule, test_partition_set]\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster Development Server\nDESCRIPTION: Command to start the Dagster development server. This launches the Dagster UI for monitoring and interacting with the data pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/index.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Creating Environment-Specific Jobs with Hooks in Dagster\nDESCRIPTION: This snippet demonstrates how to create different jobs for development and production environments, with environment-specific hooks and resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-hooks.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import graph, op, ResourceDefinition, job\n\n@op\ndef my_op():\n    pass\n\n@graph\ndef my_graph():\n    my_op()\n\ndev_job = my_graph.to_job(\n    name=\"dev\",\n    resource_defs={\n        \"slack\": ResourceDefinition.hardcoded_resource(MagicMock())\n    },\n)\n\nprod_job = my_graph.to_job(\n    name=\"prod\",\n    hooks={slack_message_on_failure},\n    resource_defs={\"slack\": slack_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Polars assets in Dagster\nDESCRIPTION: Example of defining Dagster assets using Polars DataFrames and LazyFrames. It demonstrates how to use the PolarsParquetIOManager and type annotations for eager and lazy DataFrames.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/polars.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\nfrom dagster import asset, Definitions\nfrom dagster_polars import PolarsParquetIOManager\n\n@asset(io_manager_key=\"polars_parquet_io_manager\")\ndef upstream():\n    return DataFrame({\"foo\": [1, 2, 3]})\n\n@asset(io_manager_key=\"polars_parquet_io_manager\")\ndef downstream(upstream) -> pl.LazyFrame:\n    assert isinstance(upstream, pl.DataFrame)\n    return upstream.lazy()  # LazyFrame will be sinked\n\n\ndefinitions = Definitions(assets=[upstrea, downstream], resources={\"polars_parquet_io_manager\": PolarsParquetIOManager(...)})\n```\n\n----------------------------------------\n\nTITLE: Setting PDB Breakpoint in Dagster Asset\nDESCRIPTION: Demonstrates how to add a PDB breakpoint in a Dagster asset definition. The example shows adding the context parameter and using context.pdb.set_trace() to set a breakpoint for debugging.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/debugging/debugging-pdb.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset\ndef pdb_asset(context):\n    x = 10\n    context.pdb.set_trace()\n    x += 5\n    x += 20\n    return x\n```\n\n----------------------------------------\n\nTITLE: Configuring ATProto Resource Class for Bluesky Integration\nDESCRIPTION: Implementation of a ConfigurableResource class that handles Bluesky API authentication and client initialization. Includes caching to handle rate limits efficiently.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/ingestion.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstart_resource\nclass ATProtoResource(ConfigurableResource):\n    username: str\n    password: str\n\n    _client = None\n\n    def _login(self):\n        if not self._client:\n            self._client = AtprotoClient()\n            self._client.login(self.username, self.password)\n        return self._client\n\n    def get_client(self):\n        return self._login()\nend_resource\n```\n\n----------------------------------------\n\nTITLE: Creating Book Category Asset in Dagster\nDESCRIPTION: Dagster asset definition that processes the book data to extract the most common genre for each book. This asset will be materialized as a table in DuckDB.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/feature-engineering.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample\n  path=\"docs_projects/project_llm_fine_tune/project_llm_fine_tune/assets.py\"\n  language=\"python\"\n  startAfter=\"start_book_category\"\n  endBefore=\"end_book_category\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Composing a Dynamic Job in Python with Dagster\nDESCRIPTION: This example demonstrates how to compose a dynamic job in Dagster using the map and collect functions. It shows how to handle dynamic outputs and downstream operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef downstream_op(i):\n    return i * 2\n\n@op\ndef collect_op(nums):\n    return sum(nums)\n\n@job\ndef dynamic_job():\n    dynamic_outs = dynamic_outputs()\n    results = dynamic_outs.map(downstream_op)\n    collect_op(results.collect())\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Asset for Dataproc Job\nDESCRIPTION: Python code demonstrating how to create a Dagster asset that launches and monitors a Dataproc job using PipesDataprocJobClient.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/gcp-dataproc-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/dagster/dagster_pipes/gcp/dataproc_job/dagster_code.py\" startAfter=\"start_asset_marker\" endBefore=\"end_asset_marker\" />\n```\n\n----------------------------------------\n\nTITLE: Specifying Connection Information for DbtProject in Python\nDESCRIPTION: This snippet demonstrates how to specify the profiles directory, profile, and target when creating a DbtProject object in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndbt_project = DbtProject(\n    project_dir=\"path/to/dbt/project\",\n    profiles_dir=\"path/to/profiles/dir\",\n    profile=\"my_profile\",\n    target=\"my_target\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Eager Automation Condition to Asset in Python\nDESCRIPTION: Adds an AutomationCondition.eager() to the customer_metrics asset, instructing Dagster to run the asset whenever its dependency (load_customers) is materialized.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/federate-execution.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n@multi_asset(\n    ins={\"load_customers\": AssetIn()},\n    auto_materialize_policy=AutoMaterializePolicy.eager(),\n    auto_materialize_condition=AutomationCondition.eager(),\n)\ndef run_customer_metrics(context, airflow_instance):\n    # ... (previous code)\n```\n\n----------------------------------------\n\nTITLE: Dagster Asset Implementation for EMR Integration\nDESCRIPTION: Python code defining a Dagster asset that launches and monitors EMR jobs using PipesEMRContainersClient.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-containers-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster_aws.emr import EmrContainersJobConfig\n\n@asset\ndef emr_containers_asset(pipes_emr_containers):\n    job = pipes_emr_containers.run(\n        job_config=EmrContainersJobConfig(\n            name=\"test-job\",\n            virtual_cluster_id=\"your-virtual-cluster-id\",\n            execution_role_arn=\"your-execution-role-arn\",\n            release_label=\"emr-6.9.0-latest\",\n            job_driver={\n                \"sparkSubmit\": {\n                    \"entryPoint\": \"/opt/script.py\",\n                }\n            },\n            configuration_overrides={\n                \"monitoringConfiguration\": {\n                    \"s3MonitoringConfiguration\": {\n                        \"logUri\": \"s3://your-bucket/logs/\"\n                    }\n                }\n            }\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Subsetting checks in @multi_asset_checks using Python\nDESCRIPTION: This snippet demonstrates how to execute a subset of checks in a single op using the multi_asset_check decorator. It uses the specs and can_subset arguments to control which checks are executed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/running-a-subset-of-asset-checks.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@multi_asset_check(\n    name=\"my_multi_asset_check\",\n    asset_checks=[\n        AssetCheckSpec(asset_key=[\"my_asset_1\"], name=\"check_1\"),\n        AssetCheckSpec(asset_key=[\"my_asset_2\"], name=\"check_2\"),\n    ],\n    can_subset=True,\n)\ndef my_multi_asset_check(context: AssetCheckExecutionContext):\n    for check_key in context.selected_asset_check_keys:\n        if check_key.name == \"check_1\":\n            yield AssetCheckResult(\n                check_name=\"check_1\",\n                passed=True,\n                severity=AssetCheckSeverity.ERROR,\n            )\n        elif check_key.name == \"check_2\":\n            yield AssetCheckResult(\n                check_name=\"check_2\",\n                passed=True,\n                severity=AssetCheckSeverity.ERROR,\n            )\n```\n\n----------------------------------------\n\nTITLE: Fetching Dagster Asset in Jupyter Notebook with Python\nDESCRIPTION: Demonstrates how to load a Dagster asset value (iris_dataset) within a Jupyter notebook for standalone analysis.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom tutorial_template import template_tutorial\n\niris = template_tutorial.load_asset_value(\"iris_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Resource with Dagster Assets\nDESCRIPTION: Example of using the OpenAI resource within Dagster assets to interact with the OpenAI API and log usage metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/openai.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom dagster import Definitions, asset\nfrom dagster_openai import OpenAIResource\n\n@asset\ndef openai_asset(openai):\n    completion = openai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n    )\n    return completion.choices[0].message.content\n\ndefs = Definitions(\n    assets=[openai_asset],\n    resources={\n        \"openai\": OpenAIResource(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Attaching Column Schema Metadata to Dagster Assets in Python\nDESCRIPTION: This snippet demonstrates how to attach column schema metadata to a Dagster asset, both as definition metadata and as runtime metadata. It uses the TableSchema and TableColumn objects to define the schema structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/table-metadata.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MaterializeResult\nfrom dagster.experimental import TableSchema, TableColumn\n\n@asset(\n    metadata={\n        \"dagster/column_schema\": TableSchema(\n            columns=[\n                TableColumn(name=\"id\", type=\"int\"),\n                TableColumn(name=\"name\", type=\"string\"),\n                TableColumn(name=\"age\", type=\"int\"),\n            ]\n        )\n    }\n)\ndef my_asset():\n    ...\n\n@asset\ndef my_other_asset():\n    df = get_dataframe()\n    return MaterializeResult(\n        metadata={\n            \"dagster/column_schema\": TableSchema(\n                columns=[\n                    TableColumn(name=col, type=str(df[col].dtype))\n                    for col in df.columns\n                ]\n            )\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Asset Checks in Python with Dagster\nDESCRIPTION: Shows how to define multiple asset checks using @multi_asset_check decorator. Includes checks for null values in both order_id and item_id columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/asset-checks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, multi_asset_check\n\n@asset\ndef orders():\n    return pd.DataFrame({\"order_id\": [1, 2, 3], \"item_id\": [\"a\", \"b\", \"c\"]})\n\n@multi_asset_check(assets=[orders])\ndef check_orders(context):\n    df = orders()\n    order_id_null_count = df[\"order_id\"].isnull().sum()\n    if order_id_null_count > 0:\n        yield AssetCheckResult(\n            success=False,\n            message=\"order_id contains null values\",\n            metadata={\"null_count\": order_id_null_count},\n        )\n\n    item_id_null_count = df[\"item_id\"].isnull().sum()\n    if item_id_null_count > 0:\n        yield AssetCheckResult(\n            success=False,\n            message=\"item_id contains null values\",\n            metadata={\"null_count\": item_id_null_count},\n        )\n```\n\n----------------------------------------\n\nTITLE: Attaching Metadata to Asset Materializations in Dagster\nDESCRIPTION: This snippet demonstrates how to attach metadata to Asset Materializations in Dagster, allowing tracking of specific aspects of an asset over time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, AssetMaterialization, AssetKey, MetadataValue\n\n@op\ndef materialize_table():\n    # ... do stuff\n    context.log.info(f\"Persisted data to: {path}\")\n    context.log_event(\n        AssetMaterialization(\n            asset_key=AssetKey(\"my_table\"),\n            metadata={\n                \"table_name\": \"my_table\",\n                \"date\": \"2021-05-05\",\n                \"row_count\": 1000,\n                \"schema\": MetadataValue.json({\"columns\": [{\"name\": \"a\", \"type\": \"int\"}]}),\n            },\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Triggering Job with Custom Configuration using Asset Sensor in Python\nDESCRIPTION: This snippet illustrates how to use an asset sensor to trigger a job with custom configuration. It passes metadata about the asset materialization to the job configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/asset-sensors.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, RunRequest, asset_sensor\n\n@asset_sensor(asset_key=AssetKey(\"my_table\"))\ndef my_table_sensor(context, asset_event):\n    return RunRequest(\n        run_key=None,\n        run_config={\n            \"ops\": {\n                \"process_table\": {\n                    \"config\": {\n                        \"table_name\": \"my_table\",\n                        \"timestamp\": asset_event.timestamp.isoformat(),\n                        \"materialization_id\": asset_event.event_log_entry.dagster_event.event_specific_data.materialization.metadata[\"materialization_id\"].value,\n                    }\n                }\n            }\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Kubernetes Configuration for Individual Assets in Python\nDESCRIPTION: This example shows how to use the 'dagster-k8s/config' tag on a Dagster asset to specify custom Kubernetes configuration for that specific asset. It includes settings for container resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    tags={\n        \"dagster-k8s/config\": {\n            \"container_config\": {\n                \"resources\": {\n                    \"requests\": {\"cpu\": \"250m\", \"memory\": \"64Mi\"},\n                    \"limits\": {\"cpu\": \"500m\", \"memory\": \"2560Mi\"},\n                }\n            }\n        }\n    }\n)\ndef my_asset():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Web Server in Bash\nDESCRIPTION: Command to launch the Dagster web server, which allows viewing the project example in Dagster's user interface.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Configuring SnowflakePySparkIOManager in Dagster\nDESCRIPTION: This code shows how to configure the SnowflakePySparkIOManager in Dagster's Definitions object. It includes the necessary configuration for connecting to Snowflake.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake_pyspark import SnowflakePySparkIOManager\n\ndefs = Definitions(\n    assets=[iris_dataset],\n    resources={\n        \"snowflake_pyspark_io_manager\": SnowflakePySparkIOManager(\n            account=\"YOUR_ACCOUNT\",\n            user=\"YOUR_USER\",\n            password=\"YOUR_PASSWORD\",\n            warehouse=\"YOUR_WAREHOUSE\",\n            database=\"YOUR_DATABASE\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Limits for Jobs Using Tags in Python\nDESCRIPTION: Example of using job tags in Python to customize CPU and memory allocation for specific jobs in Amazon ECS. These tags will override any defaults set at the code location or deployment level.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/amazon-ecs/configuration-reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op\n\n@op()\ndef my_op(context):\n  context.log.info('running')\n\n@job(\n  tags = {\n    \"ecs/cpu\": \"256\",\n    \"ecs/memory\": \"512\",\n  }\n)\ndef my_job():\n  my_op()\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment - Windows\nDESCRIPTION: Commands to create and activate a Python virtual environment using uv on Windows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example dagster_example\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS EMR Serverless CloudWatch Logging\nDESCRIPTION: This JSON snippet shows the configuration required to enable CloudWatch logging for an AWS EMR Serverless job. This is necessary for Dagster to receive logs from the job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-serverless-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"monitoringConfiguration\": {\n    \"cloudWatchLoggingConfiguration\": { \"enabled\": true }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Source Assets in Dagster\nDESCRIPTION: Demonstrates how to define AssetSpec objects with metadata for upstream DLT assets. This example shows how to iterate over Thinkific assets and set group names.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSpec\nfrom dagster_dlt import build_dlt_asset_specs\n\nthinkific_assets = build_dlt_asset_specs()  # your dlt assets\n\nsource_assets = [\n    AssetSpec(\n        key=asset_key,\n        group_name=\"thinkific_raw\",\n    )\n    for asset_key in thinkific_assets\n]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Executables in dagster_cloud.yaml\nDESCRIPTION: Example of defining different Python executables for multiple code locations in dagster_cloud.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/dagster-cloud-yaml.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: data-eng-pipeline\n    code_source:\n      package_name: example_etl\n    executable_path: venvs/path/to/dataengineering_spark_team/bin/python\n  - location_name: machine_learning\n    code_source:\n      python_file: ml_model.py\n    executable_path: venvs/path/to/ml_tensorflow/bin/python\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Dagster Pipes Kubernetes Container\nDESCRIPTION: This shell command builds the Docker image for the Dagster Pipes Kubernetes container.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/kubernetes-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker build -t pipes-example:v1 .\n```\n\n----------------------------------------\n\nTITLE: Storing Multi-Partitioned Assets in BigQuery with Dagster\nDESCRIPTION: Shows how to define multi-partitioned assets in Dagster for storage in BigQuery, using a dictionary of partition_expr metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MultiPartitionsDefinition, DailyPartitionsDefinition, StaticPartitionsDefinition\nimport pandas as pd\n\n@asset(\n    partitions_def=MultiPartitionsDefinition({\n        \"date\": DailyPartitionsDefinition(start_date=\"2023-01-01\"),\n        \"species\": StaticPartitionsDefinition([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n    }),\n    metadata={\n        \"partition_expr\": {\n            \"date\": \"TIMESTAMP_SECONDS(TIME)\",\n            \"species\": \"SPECIES\"\n        }\n    }\n)\ndef iris_data() -> pd.DataFrame:\n    ...\n\n@asset\ndef downstream_asset(iris_data: pd.DataFrame):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Tag Concurrency Limits in YAML\nDESCRIPTION: YAML configuration example demonstrating how to set tag-based concurrency limits in Dagster instance settings, limiting runs with specific team tags to execute one at a time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/customizing-run-queue-priority.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  runs:\n    tag_concurrency_limits:\n      - key: 'team'\n        limit: 1\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Dagster Resources\nDESCRIPTION: Illustrates how to create resources that depend on other resources. The example defines a CredentialsResource and two resources (DatabaseResource and FilestoreResource) that depend on it.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/configuring-resources.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ConfigurableResource\n\nclass CredentialsResource(ConfigurableResource):\n    username: str\n    password: str\n\nclass DatabaseResource(ConfigurableResource):\n    credentials: CredentialsResource\n    host: str\n\nclass FilestoreResource(ConfigurableResource):\n    credentials: CredentialsResource\n    bucket: str\n\ncredentials = CredentialsResource.configure_at_launch()\ndatabase = DatabaseResource(credentials=credentials, host=\"myhost\")\nfilestore = FilestoreResource(credentials=credentials, bucket=\"mybucket\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Helm Chart from Remote\nDESCRIPTION: Installs the Dagster Helm chart from the remote repository. This command creates a new namespace named `dagster` and deploys a Dagster release named `my-release` into it.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-release dagster/dagster \\\n    --namespace dagster \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Complete BigQuery Integration Example\nDESCRIPTION: Full implementation showing BigQuery I/O manager configuration, asset definitions, and data processing pipeline using Dagster and BigQuery.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\nfrom dagster_gcp_pandas import BigQueryPandasIOManager\nfrom sklearn import datasets\nimport pandas as pd\n\n@asset\ndef iris_data() -> pd.DataFrame:\n    \"\"\"Raw iris data pulled from scikit-learn\"\"\"\n    iris = datasets.load_iris()\n    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n    # rename columns\n    df.columns = [col.replace(\" (cm)\", \"\").replace(\" \", \"_\") for col in df.columns]\n    return df\n\n@asset\ndef iris_setosa(iris_data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Data for only Iris-Setosa species\"\"\"\n    return iris_data[iris_data[\"species\"] == \"setosa\"]\n\ndefs = Definitions(\n    assets=[iris_data, iris_setosa],\n    resources={\n        \"io_manager\": BigQueryPandasIOManager(\n            project=\"my-gcp-project\",\n            dataset=\"IRIS\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with PipesEMRServerlessClient\nDESCRIPTION: This Python code sets up Dagster definitions, including the PipesEMRServerlessClient resource. It configures the EMR Serverless client with AWS region and credentials, and includes the previously defined asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-serverless-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[emr_serverless_asset],\n    resources={\n        \"emr_serverless_client\": PipesEMRServerlessClient(\n            region=\"us-east-1\",\n            aws_access_key_id={\"env\": \"AWS_ACCESS_KEY_ID\"},\n            aws_secret_access_key={\"env\": \"AWS_SECRET_ACCESS_KEY\"},\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up Dagster Project with pip\nDESCRIPTION: Series of commands to set up a Dagster project using pip, including directory creation, virtual environment setup, and package installation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/scaffolding-a-project.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir my-project && cd my-project\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-dg\n```\n\nLANGUAGE: bash\nCODE:\n```\ndg init .\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Implementing Dagster Asset for AI-Powered User Input Processing in Python\nDESCRIPTION: This snippet defines a Dagster asset that uses the Anthropic API to process user input with the defined prompt. It includes Pydantic schema validation for the output and utilizes Dagster's run configuration for flexible inputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/prompts.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass UserInputSchema(BaseModel):\n    latitude: float\n    longitude: float\n    fuel_type: str\n\n@asset(required_resource_keys={\"anthropic\"})\ndef user_input_prompt(context, config):\n    user_input = config[\"user_input\"]\n    prompt = LOCATION_PROMPT.format(user_input=user_input)\n    response = context.resources.anthropic.completion(\n        model=\"claude-2\",\n        max_tokens_to_sample=300,\n        temperature=0,\n        prompt=prompt,\n    )\n    result = json.loads(response.completion)\n    return UserInputSchema(**result)\n```\n\n----------------------------------------\n\nTITLE: Configuring GCS IO Manager for Dagster in Python\nDESCRIPTION: This Python code defines a Dagster job that uses the GCS Pickle IO Manager. It sets up a resource definition for the IO manager and creates a job with this resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/gcp.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job\nfrom dagster_gcp import gcs_pickle_io_manager\n\n@job\ndef my_gcs_job():\n    # job implementation here\n    pass\n\nmy_gcs_job_configured = my_gcs_job.to_job(\n    resource_defs={\n        \"io_manager\": gcs_pickle_io_manager.configured(\n            {\"gcs_bucket\": \"my-cool-bucket\"}\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Captured Python Logger in Dagster\nDESCRIPTION: Example of using the get_dagster_logger utility function to create a logger that's captured by Dagster without modifying dagster.yaml. Useful for logging from nested functions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/python-logging.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, get_dagster_logger\n\n@asset\ndef my_asset():\n    logger = get_dagster_logger()\n    logger.info(\"This log will be captured by Dagster\")\n    return 5\n```\n\n----------------------------------------\n\nTITLE: Selecting Root Assets Feeding into a Group in Dagster\nDESCRIPTION: Demonstrates how to select root assets that feed into the 'public_data' group but do not belong to that group. This syntax uses the 'roots' function with a '+' operator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nroots(+group:\"public_data\")\n```\n\nLANGUAGE: python\nCODE:\n```\nroots_feed_to_public_data_job = define_asset_job(\n    name=\"roots_feed_to_public_data_job\", selection='roots(+group:\"public_data\")'\n)\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster asset list --select 'roots(+group:\"public_data\")'\ndagster asset materialize --select 'roots(+group:\"public_data\")'\n```\n\n----------------------------------------\n\nTITLE: Creating Definitions from Airflow DAGs\nDESCRIPTION: Python code that uses build_defs_from_airflow_instance function to create Dagster asset definitions representing Airflow DAGs. Connects to an Airflow instance and creates external assets that are marked as materialized when DAG runs complete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/peer.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport urllib.parse\n\nfrom dagster import Definitions\nfrom dagster_airlift.core import build_defs_from_airflow_instance\n\nAIRFLOW_URL = os.getenv(\"AIRFLOW__API__URL\", \"http://localhost:8080/api/\")\n\ncore_defs = build_defs_from_airflow_instance(\n    url=AIRFLOW_URL,\n    # Credentials aren't required if the Airflow instance doesn't require auth\n    # username=urllib.parse.quote(os.environ.get(\"AIRFLOW_USERNAME\", \"airflow\")),\n    # password=urllib.parse.quote(os.environ.get(\"AIRFLOW_PASSWORD\", \"airflow\")),\n)\n\ndefs = Definitions(\n    assets=core_defs.assets,\n    sensors=core_defs.sensors,\n    # If you want to use the UI, but don't want the sensor to monitor your Airflow instance,\n    # you can instead include an explicit empty list of sensors: sensors=[]\n)\n```\n\n----------------------------------------\n\nTITLE: Using ReplicationResource with Custom Assets in Python\nDESCRIPTION: Refactored example of using the custom_replication_assets decorator with the ReplicationResource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@custom_replication_assets(\n    replication_project=replication_project,\n    name=\"my_custom_replication_assets\",\n    group_name=\"replication\",\n)\ndef my_assets(replication_resource: ReplicationProject):\n    replication_resource.run(replication_project)\n```\n\n----------------------------------------\n\nTITLE: Representing Power BI Assets in Dagster\nDESCRIPTION: Example showing how to load Power BI assets into the Dagster asset graph using PowerBIWorkspace resource and credentials configuration\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/powerbi.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_powerbi import PowerBIWorkspace, load_powerbi_asset_specs\n\npower_bi_workspace = PowerBIWorkspace(\n    workspace_id=\"your-workspace-id\",\n    tenant_id=\"your-tenant-id\",\n    client_id=\"your-client-id\",\n    client_secret=EnvVar(\"POWERBI_CLIENT_SECRET\"),\n)\n\ndefs = Definitions(\n    assets=load_powerbi_asset_specs(power_bi_workspace),\n    resources={\"power_bi\": power_bi_workspace},\n)\n```\n\n----------------------------------------\n\nTITLE: Dagster Op with Dask Resource Tags\nDESCRIPTION: Python code demonstrating how to define a Dagster op with Dask resource requirements using tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/dask.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op(\n    ...\n    tags={'dagster-dask/resource_requirements': {\"GPU\": 1}},\n)\ndef my_op(...):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Context Injector for Dagster Pipes\nDESCRIPTION: Example of creating a custom context injector that writes to a cloud service key/value store.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/dagster-pipes-details-and-customization.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom cloud_service import write_to_kv_store\nfrom dagster import PipesContextInjector\n\nclass CloudServiceContextInjector(PipesContextInjector):\n    def inject_context(self, context_json: str) -> dict:\n        context_path = write_to_kv_store(context_json)\n        return {\"context_path\": context_path}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: This snippet calculates the score of the fitted linear regression model using the `score` method and logs the score to the Dagster context. The score represents the coefficient of determination (R^2) of the model.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_LR.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nscore = fit.score(X, y)\ncontext.log.info(\"Linear regression model has score {}\".format(score))\n```\n\n----------------------------------------\n\nTITLE: Defining ConfigurableResource for Jobs in Python\nDESCRIPTION: Shows how to define a ConfigurableResource subclass for use with jobs. The resource is configured in the Definitions call and methods are defined that depend on config values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/defining-resources.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster import ConfigurableResource, op, job, Definitions\n\nclass MyExternalService(ConfigurableResource):\n    conn_str: str\n\n    def query_service(self, query: str) -> str:\n        # Use self.conn_str to connect to the service\n        ...\n\n@op\ndef my_op(external_service: MyExternalService):\n    result = external_service.query_service(\"SELECT * FROM my_table\")\n    return result\n\n@job\ndef my_job():\n    my_op()\n\ndefs = Definitions(\n    jobs=[my_job],\n    resources={\n        \"external_service\": MyExternalService(conn_str=\"my_connection_string\")\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Answer with OpenAI in Dagster RAG System\nDESCRIPTION: Combines retrieved context from Pinecone with the original question to create a prompt, then uses OpenAI's GPT-4 model to generate an answer. The result is materialized in the Dagster Catalog.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/retrieval.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprompt = f\"\"\"Answer the following question using only the context provided. If you cannot answer the question based on the context, say \"I don't know\".\n\nContext: {context}\n\nQuestion: {config.question}\n\nAnswer:\"\"\"\n\nresponse = openai.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\n\nreturn MaterializeResult(\n    metadata={\"answer\": response.choices[0].message.content}\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Manual Data Versions in Dagster Asset\nDESCRIPTION: This snippet shows how to implement manual data versions for a Dagster asset by returning an Output object with a custom data version.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@asset(code_version=\"v4\")\ndef versioned_number():\n    result = 20\n    return Output(value=result, metadata={\"data_version\": DataVersion(str(result))})\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Single Tuple Output in Python\nDESCRIPTION: Shows how to specify a single tuple output for an op using type annotations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op() -> Tuple[int, str]:\n    return (1, \"foo\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster op with Datadog metrics publishing\nDESCRIPTION: Example of setting up a Dagster op that publishes metrics to Datadog. It demonstrates importing necessary modules, creating a Datadog resource, defining an op that publishes metrics, and creating a job with the Datadog resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/datadog.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, resource\nfrom dagster_datadog import datadog_resource\n\n@op(required_resource_keys={\"datadog\"})\ndef datadog_op(context):\n    context.resources.datadog.gauge(\"dagster.test\", 1.0, tags=[\"foo:bar\"])\n\n@job(resource_defs={\"datadog\": datadog_resource})\ndef datadog_job():\n    datadog_op()\n\nif __name__ == \"__main__\":\n    datadog_job.execute_in_process(\n        run_config={\n            \"resources\": {\n                \"datadog\": {\n                    \"config\": {\n                        \"api_key\": \"YOUR-API-KEY\",\n                        \"app_key\": \"YOUR-APP-KEY\",\n                    }\n                }\n            }\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Partition-Based Op Schedule in Python\nDESCRIPTION: Shows how to create a schedule from a partitioned op-based job. Includes the complete setup with job definition and partition configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@job(partitions_def=DailyPartitionsDefinition(start_date=\"2023-01-01\"))\ndef my_partitioned_job():\n    \"\"\"Job logic here\"\"\"\n\nmy_schedule = build_schedule_from_partitioned_job(my_partitioned_job)\n```\n\n----------------------------------------\n\nTITLE: Creating Enriched Graphic Novels Asset in Dagster\nDESCRIPTION: Dagster asset definition that combines book category data with author and graphic novel data to create the final dataset for modeling. It creates a table in DuckDB and returns a DataFrame for further processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/feature-engineering.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample\n  path=\"docs_projects/project_llm_fine_tune/project_llm_fine_tune/assets.py\"\n  language=\"python\"\n  startAfter=\"start_enriched_graphic_novels\"\n  endBefore=\"end_enriched_graphic_novels\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Creating Schedule for Static Partitioned Job in Python\nDESCRIPTION: This snippet shows how to create a schedule that targets all partitions of a statically partitioned job using the @schedule decorator in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/constructing-schedules-for-partitioned-assets-and-jobs.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@schedule(cron_schedule=\"0 0 * * *\", job=continent_job)\ndef continent_schedule():\n    return RunRequest(\n        run_key=None,\n        partition_key=continent_job.partitions_def.get_last_partition_key(),\n    )\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Logger to Asset Job in Dagster\nDESCRIPTION: Example showing how to add a JSON console logger to your Dagster asset job definition. This overrides the default colored console logger to produce logs in JSON format.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/custom-logging.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset, AssetExecutionContext, define_asset_job, json_console_logger\n\n@asset\ndef get_hackernews_topstory_ids(context: AssetExecutionContext):\n    import requests\n    \n    newstories_url = \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n    top_story_ids = requests.get(newstories_url).json()\n    context.log.info(f\"Got {len(top_story_ids)} top stories.\")\n    return top_story_ids\n\n# Define an asset job that uses the get_hackernews_topstory_ids asset\ntopstory_ids_job = define_asset_job(name=\"hackernews_topstory_ids_job\", selection=\"get_hackernews_topstory_ids\")\n\n# Add the custom logger when defining Dagster definitions\ndefs = Definitions(\n    assets=[get_hackernews_topstory_ids],\n    jobs=[topstory_ids_job],\n    loggers={\"console\": json_console_logger}\n)\n```\n\n----------------------------------------\n\nTITLE: Referencing an Upstream Dagster Asset in a dbt Model\nDESCRIPTION: This SQL snippet shows how to reference an upstream Dagster asset in a dbt model. It selects data from the 'upstream' source, which corresponds to the previously defined Dagster asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nselect *\n  from {{ source(\"dagster\", \"upstream\") }}\n where foo=1\n```\n\n----------------------------------------\n\nTITLE: Storing Multi-Partitioned Assets in DuckDB\nDESCRIPTION: This snippet shows how to store assets partitioned on multiple dimensions in DuckDB using the DuckDB I/O manager. It uses a dictionary of 'partition_expr' metadata to specify columns for each partition dimension.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    metadata={\n        \"partition_expr\": {\n            \"time\": \"TO_TIMESTAMP(TIME)\",\n            \"species\": \"SPECIES\",\n        }\n    }\n)\ndef iris_by_time_and_species():\n    return pd.DataFrame({\n        \"TIME\": [1672531200, 1672617600, 1672704000],  # 2023-01-01, 2023-01-02, 2023-01-03\n        \"SPECIES\": [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"],\n        \"sepal_length_cm\": [5.1, 7.0, 6.3],\n        \"sepal_width_cm\": [3.5, 3.2, 3.3],\n        \"petal_length_cm\": [1.4, 4.7, 6.0],\n        \"petal_width_cm\": [0.2, 1.4, 2.5],\n    })\n```\n\n----------------------------------------\n\nTITLE: Integrating PagerDuty with Dagster\nDESCRIPTION: Example code demonstrating how to integrate PagerDuty with Dagster. It includes importing necessary modules, setting up a PagerDuty resource, defining an op that triggers a PagerDuty alert, and creating a job that uses this op.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pagerduty.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, ResourceDefinition\nfrom dagster_pagerduty import pagerduty_resource\n\n@op(required_resource_keys={\"pagerduty\"})\ndef pagerduty_op(context):\n    context.resources.pagerduty.EventV2.create(\n        summary=\"Example alert from Dagster\",\n        source=\"Example Dagster Job\",\n        severity=\"error\",\n    )\n\n@job(resource_defs={\"pagerduty\": pagerduty_resource})\ndef pagerduty_job():\n    pagerduty_op()\n\nif __name__ == \"__main__\":\n    result = pagerduty_job.execute_in_process(\n        run_config={\n            \"resources\": {\n                \"pagerduty\": {\"config\": {\"routing_key\": \"<YOUR_ROUTING_KEY>\"}}\n            }\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Assigning Assets and Ops to Concurrency Pools in Python\nDESCRIPTION: This Python code shows how to assign assets and ops to concurrency pools using the 'pool' keyword argument. This allows for limiting the number of in-progress executions across all runs for specific assets or ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(pool=\"database\")\ndef my_database_asset():\n    ...\n\n@op(pool=\"database\")\ndef my_database_op():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Subsetting checks in @multi_assets using Python\nDESCRIPTION: This example shows how to execute specific checks when certain assets within a multi-asset are materialized. It uses the AssetSpec, can_subset parameter, and context objects to control check execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/running-a-subset-of-asset-checks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@multi_asset(\n    outs={\n        \"multi_asset_piece_1\": AssetOut(),\n        \"multi_asset_piece_2\": AssetOut(),\n    },\n    can_subset=True,\n    asset_checks=[\n        AssetCheckSpec(\n            asset_key=[\"multi_asset_piece_1\"],\n            name=\"my_check\",\n        )\n    ],\n)\ndef multi_asset_1_and_2(context: AssetExecutionContext):\n    if AssetKey([\"multi_asset_piece_1\"]) in context.selected_asset_keys:\n        yield MaterializeResult(\n            asset_key=AssetKey([\"multi_asset_piece_1\"]),\n            metadata={\"data\": \"some_data\"},\n        )\n\n    if AssetKey([\"multi_asset_piece_2\"]) in context.selected_asset_keys:\n        yield MaterializeResult(\n            asset_key=AssetKey([\"multi_asset_piece_2\"]),\n            metadata={\"data\": \"some_other_data\"},\n        )\n\n    if AssetCheckKey([\"multi_asset_piece_1\"], \"my_check\") in context.selected_asset_check_keys:\n        yield AssetCheckResult(\n            check_name=\"my_check\",\n            passed=True,\n            severity=AssetCheckSeverity.ERROR,\n        )\n```\n\n----------------------------------------\n\nTITLE: Transferring Data from Azure Blob Storage to Teradata Vantage Using dagster-teradata\nDESCRIPTION: A Dagster job that demonstrates how to transfer data from Azure Blob Storage to Teradata Vantage. It configures Azure ADLS2 and Teradata resources, drops existing tables, and uses the azure_blob_to_teradata method to load data from Azure Blob into Teradata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom dagster import job, op, Definitions, EnvVar, DagsterError\nfrom dagster_azure.adls2 import ADLS2Resource, ADLS2SASToken\nfrom dagster_teradata import TeradataResource, teradata_resource\n\nazure_resource = ADLS2Resource(\n    storage_account=\"\",\n    credential=ADLS2SASToken(token=\"\"),\n)\n\ntd_resource = TeradataResource(\n    host=os.getenv(\"TERADATA_HOST\"),\n    user=os.getenv(\"TERADATA_USER\"),\n    password=os.getenv(\"TERADATA_PASSWORD\"),\n    database=os.getenv(\"TERADATA_DATABASE\"),\n)\n\n@op(required_resource_keys={\"teradata\"})\ndef drop_existing_table(context):\n     context.resources.teradata.drop_table(\"people\")\n     return \"Tables Dropped\"\n\n@op(required_resource_keys={\"teradata\", \"azure\"})\ndef ingest_azure_to_teradata(context, status):\n    if status == \"Tables Dropped\":\n        context.resources.teradata.azure_blob_to_teradata(azure_resource, \"/az/akiaxox5jikeotfww4ul.blob.core.windows.net/td-usgs/CSVDATA/09380000/2018/06/\", \"people\", True)\n    else:\n        raise DagsterError(\"Tables not dropped\")\n\n@job(resource_defs={\"teradata\": td_resource, \"azure\": azure_resource})\ndef example_job():\n     ingest_azure_to_teradata(drop_existing_table())\n\ndefs = Definitions(\n    jobs=[example_job]\n)\n```\n\n----------------------------------------\n\nTITLE: Original Step Launcher Implementation\nDESCRIPTION: Example of the original implementation using Spark step launchers with Dagster assets and resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/migrating-from-step-launchers-to-pipes.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, multi_asset, AssetOut\nfrom pyspark.sql import DataFrame\n\n@asset\ndef upstream(context) -> DataFrame:\n    return context.resources.spark.spark_session.createDataFrame(\n        [{\"a\": 1}, {\"a\": 2}]\n    ).write.parquet(\"/path/to/tmp\")\n\n@multi_asset(outs={\"output_1\": AssetOut(), \"output_2\": AssetOut()})\ndef downstream(upstream: DataFrame):\n    df1 = upstream.where(\"a = 1\")\n    df2 = upstream.where(\"a = 2\")\n    return df1, df2\n```\n\n----------------------------------------\n\nTITLE: Updating Asset Code Version in Python\nDESCRIPTION: Modifies the asset to use a new code version, signaling to Dagster that the implementation has changed and may need re-materialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(code_version=\"v2\")\ndef versioned_number():\n    return 1\n```\n\n----------------------------------------\n\nTITLE: Running Tests with pytest for Dagster GCP Quickstart\nDESCRIPTION: Command to run tests using pytest for the Dagster GCP quickstart project. The tests are located in the 'quickstart_gcp_tests' directory and are executed using the pytest command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_gcp/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest quickstart_gcp_tests\n```\n\n----------------------------------------\n\nTITLE: Defining a Configurable Resource in Python\nDESCRIPTION: This example demonstrates creating a configurable resource by subclassing ConfigurableResource. The resource defines a connection_url attribute that can be provided at runtime and accessed within the resource's methods.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/run-configuration.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass DatabaseConnection(ConfigurableResource):\n    connection_url: str\n\n    def query(self, query_text):\n        # Use the connection_url parameter to connect to the database\n        print(f\"Connecting to {self.connection_url}\")\n        print(f\"Running query: {query_text}\")\n        # In a real implementation, this would actually connect and execute the query\n        return [\"result1\", \"result2\"]\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project Structure via CLI\nDESCRIPTION: Command line output from 'dg list defs' showing defined assets (customers_table, my_analytics_asset, orders_table, products_table), jobs (regenerate_analytics_job, sync_tables_job), and schedules with their cron timings (hourly and daily runs).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-definitions/8-list-defs-after-all.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndg list defs\n\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Section   ┃ Definitions                                                   ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Assets    │ ┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┓ │\n│           │ ┃ Key                ┃ Group   ┃ Deps ┃ Kinds ┃ Description ┃ │\n│           │ ┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━┩ │\n│           │ │ customers_table    │ default │      │       │             │ │\n│           │ ├────────────────────┼─────────┼──────┼───────┼─────────────┤ │\n│           │ │ my_analytics_asset │ default │      │       │             │ │\n│           │ ├────────────────────┼─────────┼──────┼───────┼─────────────┤ │\n│           │ │ orders_table       │ default │      │       │             │ │\n│           │ ├────────────────────┼─────────┼──────┼───────┼─────────────┤ │\n│           │ │ products_table     │ default │      │       │             │ │\n│           │ └────────────────────┴─────────┴──────┴───────┴─────────────┘ │\n│ Jobs      │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓                                  │\n│           │ ┃ Name                     ┃                                  │\n│           │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩                                  │\n│           │ │ regenerate_analytics_job │                                  │\n│           │ ├──────────────────────────┤                                  │\n│           │ │ sync_tables_job          │                                  │\n│           │ └──────────────────────────┘                                  │\n│ Schedules │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓         │\n│           │ ┃ Name                              ┃ Cron schedule ┃         │\n│           │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩         │\n│           │ │ regenerate_analytics_job_schedule │ 0 * * * *     │         │\n│           │ ├───────────────────────────────────┼───────────────┤         │\n│           │ │ sync_tables_job_schedule          │ 0 0 * * *     │         │\n│           │ └───────────────────────────────────┴───────────────┘         │\n└───────────┴───────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Updating ReplicationResource Class in Python\nDESCRIPTION: Modifies the ReplicationResource class to use the translator and project provided in the metadata. It ensures type consistency and uses the translator when yielding asset materializations, maintaining consistency between asset declarations and materializations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass ReplicationResource(ConfigurableResource):\n    @public\n    def run(self, context: AssetExecutionContext) -> Iterator[AssetMaterialization]:\n        metadata_by_key = context.assets_def.metadata_by_key\n        first_asset_metadata = next(iter(metadata_by_key.values()))\n\n        project = check.inst(\n            first_asset_metadata.get(\"replication_project\"),\n            ReplicationProject,\n        )\n\n        translator = check.inst(\n            first_asset_metadata.get(\"replication_translator\"),\n            ReplicationTranslator,\n        )\n\n        results = replicate(Path(project.replication_configuration_yaml))\n        for table in results:\n            if table.get(\"status\") == \"SUCCESS\":\n                yield AssetMaterialization(\n                    asset_key=translator.get_asset_key(table), metadata=table\n                )\n```\n\n----------------------------------------\n\nTITLE: Implementing Observable Source Asset in Dagster\nDESCRIPTION: This snippet shows how to create an observable source asset in Dagster, which computes a hash of file contents as a data version.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@observable_source_asset\ndef input_number():\n    with open(\"input_number.txt\", \"r\") as f:\n        contents = f.read()\n    return DataVersion(hashlib.md5(contents.encode()).hexdigest())\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Dagster Definitions\nDESCRIPTION: Example showing how to merge multiple Definitions objects from different modules into a single root definition using the Definitions.merge method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/structuring-your-dagster-project.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dbt.definitions import dbt_definitions\nfrom dlt.definitions import dlt_definitions\n\n\ndefs = Definitions.merge(\n    dbt_definitions,\n    dlt_definitions,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Downstream Dependencies for DLT Assets\nDESCRIPTION: Illustrates how to create assets that depend on DLT resources by defining downstream dependencies using asset keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster_dlt import DagsterDltTranslator\n\ntranslator = DagsterDltTranslator()\nasset_key = translator.get_asset_spec().key\n\n@asset(deps=[asset_key])\ndef example_downstream_asset(context):\n    # Load and process data from the upstream dlt asset\n    pass\n```\n\n----------------------------------------\n\nTITLE: Run Key Implementation for Deduplication\nDESCRIPTION: Example of using run keys to prevent duplicate runs by uniquely identifying RunRequests.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nyield dg.RunRequest(run_key=filename)\n```\n\n----------------------------------------\n\nTITLE: Downloading RSS Feed Asset in Python\nDESCRIPTION: Downloads RSS feed content and uploads it to an R2 bucket. Uses the feedparser library to process RSS content and stores metadata in Dagster's catalog.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/rss-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstart_podcast_audio\n```\n\n----------------------------------------\n\nTITLE: Testing Resource with Initialization Context in Python\nDESCRIPTION: Illustrates testing a ConfigurableResource that uses the resource initialization context, utilizing build_init_resource_context and with_init_resource_context utilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/testing-configurable-resources.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster import build_init_resource_context\n\ndef test_my_context_resource():\n    context = build_init_resource_context(config={\"foo\": \"bar\"})\n    with MyResource.with_init_resource_context(context) as resource:\n        assert resource.foo == \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Dagster Project\nDESCRIPTION: This command installs the Python dependencies required for the project, including development dependencies. It uses pip to install the packages in editable mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/feature_graph_backed_assets/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Authenticating REST API Requests with User Token in Dagster+\nDESCRIPTION: Example of how to authenticate API requests to Dagster+ using a user token in the Dagster-Cloud-Api-Token header. This shows how to report an asset materialization with metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n    --url https://{ORGANIZATION}.dagster.cloud/{deployment_name}/report_asset_materialization/ \\\n    --header 'Content-Type: application/json' \\\n    --header 'Dagster-Cloud-Api-Token: {TOKEN}' \\\n    --data '{\n        \"asset_key\": \"{ASSET_KEY}\",\n        \"metadata\": {\n            \"rows\": 10\n        },\n    }'\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Assets and Resources for AWS Lambda\nDESCRIPTION: This code snippet creates Dagster definitions including an asset and AWS Lambda resource. It makes the resource available to other Dagster definitions in the project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-lambda-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[lambda_asset],\n    resources={\n        \"lambda_client\": lambda_client,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project Dependencies\nDESCRIPTION: This command installs the project and its development dependencies using pip. The -e flag installs the project in editable mode, which is useful for development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_openai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Defining Pipeline and Resources in Python\nDESCRIPTION: Python code defining the Dagster pipeline, including assets and Teradata resource configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import EnvVar, Definitions\nfrom dagster_teradata import TeradataResource\n\nfrom .assets import read_csv_file, read_table, create_table, drop_table, insert_rows\n\n# Define the pipeline and resources\ndefs = Definitions(\n    assets=[read_csv_file, read_table, create_table, drop_table, insert_rows],\n    resources={\n        \"teradata\": TeradataResource(\n            host=EnvVar(\"TERADATA_HOST\"),\n            user=EnvVar(\"TERADATA_USER\"),\n            password=EnvVar(\"TERADATA_PASSWORD\"),\n            database=EnvVar(\"TERADATA_DATABASE\"),\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Asset Selection through Middle Node\nDESCRIPTION: Defines a job selecting assets that pass through a specific middle asset\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmiddle_asset_job = define_asset_job(\n    name=\"middle_asset_job\", selection='key:\"raw_data_c\"+ and +key:\"combo_a_b_c_data\"+ and +key:\"summary_stats_1\"'\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Retention Policies\nDESCRIPTION: Configuration for setting retention periods for different types of schedule and sensor ticks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nretention:\n  schedule_tick:\n    purge_after_days:\n      success: 30\n      failure: 7\n      skipped: 1\n  sensor_tick:\n    purge_after_days:\n      success: 30\n      failure: 7\n      skipped: 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Partitioned Job with Date Config in Python\nDESCRIPTION: Example showing how to define a basic non-partitioned job that processes data for a specific date using Dagster ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-ops.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ConfigurableResource, op, job\n\n@op\ndef process_data_for_date(context, date: str):\n    context.log.info(f\"Processing data for {date}\")\n\n@job\ndef date_job():\n    process_data_for_date()\n```\n\n----------------------------------------\n\nTITLE: Defining a Configurable Job Schedule in Python\nDESCRIPTION: This snippet demonstrates how to create a schedule for a configurable job using the @schedule decorator. It sets up a daily schedule and provides run configuration based on the current date.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/testing-schedules.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@schedule(\n    job=configurable_job,\n    cron_schedule=\"0 0 * * *\",\n)\ndef configurable_job_schedule(context):\n    date = context.scheduled_execution_time.strftime(\"%Y-%m-%d\")\n    return RunConfig(\n        ops={\"configurable_op\": {\"config\": {\"date\": date}}}\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring an Op with API Endpoint in Python\nDESCRIPTION: This snippet demonstrates how to define an op with a configurable API endpoint using a Config class. The op function takes a config parameter and uses it to make an API request.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, Config\nfrom pydantic import Field\nimport requests\n\nclass ApiConfig(Config):\n    api_endpoint: str = Field(description=\"The API endpoint to query\")\n\n@op(config_schema=ApiConfig)\ndef configurable_api_op(context, config: ApiConfig):\n    response = requests.get(config.api_endpoint)\n    return response.json()\n```\n\n----------------------------------------\n\nTITLE: Configuring Assets with GitHub Resource Using Environment Variables\nDESCRIPTION: An example of how to configure a GitHub resource with an access token from an environment variable and provide it to assets in a Dagster Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=a_bunch_of_github_assets,\n    resources={\n        \"github\": GithubClientResource(\n            access_token=EnvVar(\"GITHUB_ACCESS_TOKEN\"),\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Resources at Launch Time\nDESCRIPTION: Shows how to use configure_at_launch() to defer resource configuration until runtime. The example defines a ConfigurableResource with a target_table parameter that can be set at launch time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/configuring-resources.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ConfigurableResource\n\nclass ConfigurableResource(ConfigurableResource):\n    target_table: str\n\n    def query_data(self):\n        print(f\"Querying {self.target_table}\")\n\nconfigurable_resource = ConfigurableResource.configure_at_launch()\n```\n\n----------------------------------------\n\nTITLE: Configuring pip Project for dg in dg.toml\nDESCRIPTION: TOML configuration in dg.toml that sets up a Dagster project to work with dg, specifying the project type, root module, and location of the top-level Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\ndirectory_type = \"project\"\nproject.root_module = \"my_existing_project\"\nproject.code_location_target_module = \"my_existing_project.definitions\"\n\n```\n\n----------------------------------------\n\nTITLE: Creating Column Schema Change Checks for Dagster Assets in Python\nDESCRIPTION: This snippet demonstrates how to use the build_column_schema_change_checks function to create asset checks that detect schema changes between materializations, including added columns, removed columns, and changed column types.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/table-metadata.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MaterializeResult\nfrom dagster.experimental import TableSchema, TableColumn\nfrom dagster.experimental.asset_checks import build_column_schema_change_checks\n\n@asset(\n    checks=build_column_schema_change_checks(\n        # Assert that no columns are added or removed\n        allow_column_additions=False,\n        allow_column_removals=False,\n        # Assert that the types of existing columns don't change\n        allow_column_type_changes=False,\n    )\n)\ndef my_other_asset():\n    df = get_dataframe()\n    return MaterializeResult(\n        metadata={\n            \"dagster/column_schema\": TableSchema(\n                columns=[\n                    TableColumn(name=col, type=str(df[col].dtype))\n                    for col in df.columns\n                ]\n            )\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Using Custom Replication Assets Decorator in Python\nDESCRIPTION: Example of using the custom_replication_assets decorator to define assets and perform replication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext\n\n\nreplication_project_path = \"replication.yaml\"\nreplication_project = ReplicationProject(replication_project_path)\n\n\n@custom_replication_assets(\n    replication_project=replication_project,\n    name=\"my_custom_replication_assets\",\n    group_name=\"replication\",\n)\ndef my_assets(context: AssetExecutionContext):\n    results = replicate(Path(replication_project_path))\n    for table in results:\n        if table.get(\"status\") == \"SUCCESS\":\n            yield AssetMaterialization(asset_key=str(table.get(\"name\")), metadata=table)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project in Editable Mode\nDESCRIPTION: Installs the Dagster code location as a Python package in editable mode, allowing local code changes to automatically apply during development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_dagster_university_start/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Constructing Schedule for Time-Partitioned Asset Job in Python\nDESCRIPTION: This snippet demonstrates how to create a schedule for a time-partitioned asset job using the build_schedule_from_partitioned_job function in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/constructing-schedules-for-partitioned-assets-and-jobs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nasset_partitioned_schedule = build_schedule_from_partitioned_job(\n    job=partitioned_job,\n    name=\"asset_partitioned_schedule\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring ADLS2Resource in Dagster\nDESCRIPTION: Example showing how to use ADLS2Resource for Azure Data Lake Storage Gen 2 integration. The code demonstrates instantiation and configuration of ADLS2Resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/azure-adls2.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/integrations/azure-adls2.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Defining Configuration Schema for ShellCommandComponent\nDESCRIPTION: Implementation of the ShellCommandComponent class with configuration schema, defining script_path and asset_specs as required parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, Definitions, asset\nfrom dagster._core.definitions.asset_spec import AssetSpec\nfrom dagster_components import Component, ResolvedAssetSpec, Resolvable\nfrom pydantic import model_validator\nfrom typing_extensions import Annotated, override\n\nfrom typing import List\n\n\nclass ShellCommandComponent(Resolvable, Component):\n    \"\"\"A component that executes a shell command.\"\"\"\n\n    script_path: str\n    \"\"\"The path to the shell script to execute.\"\"\"\n\n    asset_specs: List[ResolvedAssetSpec]\n    \"\"\"The assets that this script produces.\"\"\"\n\n    @override\n    def build_defs(self) -> Definitions:\n        \"\"\"Build the definitions for this component.\"\"\"\n        return Definitions()\n```\n\n----------------------------------------\n\nTITLE: Loading Dagster Code Location from Shell\nDESCRIPTION: Shell commands demonstrating how to load Dagster code locations from files and modules using the dagster dev command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/managing-code-locations-with-definitions.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -f my_file.py\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -f my_file.py -f my_second_file.py\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -m your_module_name.definitions\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -m your_module_name.definitions -m your_second_module.definitions\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Kubernetes Agent\nDESCRIPTION: Example configuration in dagster_cloud.yaml for setting environment variables and secrets using Kubernetes agent. Shows how to specify environment variables and integrate with Kubernetes secrets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/environment-variables/agent-config.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      k8s:\n        env_vars:\n          - database_name\n          - database_username=hooli_testing\n        env_secrets:\n          - database_password\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Schedule with @schedule Decorator in Dagster\nDESCRIPTION: This example shows how to define a schedule using the @schedule decorator, which provides more flexibility than ScheduleDefinition. It allows for configuring job behavior based on scheduled run time or emitting log messages.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/defining-schedules.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@schedule(job=my_job, cron_schedule=\"0 0 * * *\")\ndef basic_schedule(): ...\n  # things the schedule does, like returning a RunRequest or SkipReason\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Job in Python\nDESCRIPTION: Example showing how to create a job using the partitioned configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-ops.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@job(config=my_partitioned_config)\ndef timezone_partitioned_job():\n    process_data_for_date()\n```\n\n----------------------------------------\n\nTITLE: Implementing Static Partitioned Job by Continent\nDESCRIPTION: Example demonstrating how to create a statically partitioned job using continents as partition keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-ops.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import static_partitioned_config, op, job\n\n@op\ndef process_data_for_continent(continent: str):\n    print(f\"Processing data for {continent}\")\n\n@static_partitioned_config(partition_keys=[\"AFRICA\", \"ANTARCTICA\", \"ASIA\", \"EUROPE\", \"NORTH_AMERICA\", \"OCEANIA\", \"SOUTH_AMERICA\"])\ndef my_continent_config(partition_key: str):\n    return {\"ops\": {\"process_data_for_continent\": {\"config\": {\"continent\": partition_key}}}}\n\n@job(config=my_continent_config)\ndef continent_partitioned_job():\n    process_data_for_continent()\n```\n\n----------------------------------------\n\nTITLE: Storing Statically-Partitioned Assets in Snowflake\nDESCRIPTION: Demonstrates how to store statically-partitioned assets in Snowflake using partition_expr metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(metadata={\"partition_expr\": \"SPECIES\"})\ndef iris_dataset():\n    return get_iris_data()\n\n@asset\ndef species_data(iris_dataset):\n    return iris_dataset.groupby(\"SPECIES\").mean()\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profile for DuckDB in YAML\nDESCRIPTION: Creates a profiles.yml file with configuration for using DuckDB as the data warehouse for the dbt project. Specifies the target, output type, database path, and number of threads.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/set-up-dbt-project.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\njaffle_shop:\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      path: tutorial.duckdb\n      threads: 24\n```\n\n----------------------------------------\n\nTITLE: Using EnvVar.get_value() to Access Environment Variables\nDESCRIPTION: A Python code snippet demonstrating an alternative way to access environment variables in Dagster using the EnvVar class's get_value() method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import EnvVar\n\ndatabase_name = EnvVar('DATABASE_NAME').get_value()\n```\n\n----------------------------------------\n\nTITLE: Modal Transcription Integration in Python\nDESCRIPTION: Integrates Modal application with Dagster using Dagster Pipes for audio transcription. Uses ModalClient to handle the transcription process and manage environment variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/rss-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstart_transcription\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project via CLI\nDESCRIPTION: This command initializes a new Dagster project named 'my-project' using the 'dg init' command. It sets up the basic project structure and necessary files for a Dagster workflow.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-a-uv-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg init my-project\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Python Type Annotations in Dagster\nDESCRIPTION: This snippet demonstrates how to define a Dagster op using Python's native type annotations without additional modification.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op(x: int) -> str:\n    return str(x)\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Assets in Dagster\nDESCRIPTION: Python code that defines Dagster assets from dbt models using the dbt manifest file and configures how they should be materialized.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/load-dbt-models.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dbt_assets(manifest=jaffle_shop_project.manifest_path)\ndef jaffle_shop_dbt_assets(context: AssetExecutionContext, dbt: DbtCliInvocation):\n    yield from dbt.cli(\"build\", select=context.selected_assets_to_dbt_selection()).stream()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Dagster and dbt Integration\nDESCRIPTION: This command installs the necessary Python packages for integrating Dagster with dbt and DuckDB. It includes Dagster, DuckDB, Plotly, Pandas, and dbt-related packages.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster duckdb plotly pandas dagster-dbt dbt-duckdb\n```\n\n----------------------------------------\n\nTITLE: Implementing Assets With Snowflake I/O Manager in Python\nDESCRIPTION: Example showing how to swap storage backends by using SnowflakePandasIOManager instead of DuckDB, demonstrating the flexibility of I/O managers in changing data stores without modifying asset logic.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset\nfrom dagster_snowflake_pandas import SnowflakePandasIOManager\n\n@asset\ndef raw_sales_data():\n    return pd.read_csv(\"path/to/sales.csv\")\n\n@asset\ndef clean_sales_data(raw_sales_data):\n    return raw_sales_data.dropna()\n```\n\n----------------------------------------\n\nTITLE: Avoiding Data Passing by Combining Assets in Python\nDESCRIPTION: This example demonstrates how to avoid passing data between assets by combining multiple operations into a single asset. It shows a pipeline that downloads, unzips, and loads data from a zip file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/passing-data-between-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Content of passing-data-avoid.py file\n\n# Code example not provided in the input text\n```\n\n----------------------------------------\n\nTITLE: Creating Asset Definitions for Custom Airflow Operators\nDESCRIPTION: Python code that defines factory functions to create Dagster software-defined assets corresponding to custom Airflow operators. These functions handle both the load_raw_customers and export_customers tasks from the original DAG.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/migrate.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom dagster import AssetExecutionContext, FreshnessPolicy, MonthlyPartitionsDefinition, asset\n\nfrom ..resources.duckdb_io_manager import ParquetDuckDBIOManager\n\n\ndef load_csv_to_duckdb_defs(duckdb_io_manager: ParquetDuckDBIOManager):\n    @asset(\n        key=\"raw_customer_data\",\n        io_manager_key=\"duckdb_io_manager\",\n        # This tells Dagster that this asset is associated with the `load_raw_customers` task in Airflow.\n        dagster_airlift_metadata={\"dag_id\": \"rebuild_customers_list\", \"task_id\": \"load_raw_customers\"},\n        freshness_policy=FreshnessPolicy(maximum_lag_minutes=60 * 24),\n        partitions_def=MonthlyPartitionsDefinition(start_date=\"2023-01-01\"),\n    )\n    def raw_customer_data(context: AssetExecutionContext) -> None:\n        dt = datetime.strptime(context.partition_key, \"%Y-%m-%d\")\n        year = dt.strftime(\"%Y\")\n        month = dt.strftime(\"%m\")\n\n        # Parse, transform, and store data for this year/month\n        path = f\"./raw_data/customers_{year}_{month}.csv\"\n        # The io manager will store this dataframe in a partitioned location\n        return duckdb_io_manager.read_csv_to_df(path)\n\n    return [raw_customer_data]\n\n\ndef export_duckdb_to_csv_defs(\n    duckdb_io_manager: ParquetDuckDBIOManager, exported_columns: Optional[List[str]] = None\n):\n    @asset(\n        key=\"customers_export\",\n        # This tells Dagster that this asset is associated with the `export_customers` task in Airflow.\n        dagster_airlift_metadata={\"dag_id\": \"rebuild_customers_list\", \"task_id\": \"export_customers\"},\n        freshness_policy=FreshnessPolicy(maximum_lag_minutes=60 * 24),\n        partitions_def=MonthlyPartitionsDefinition(start_date=\"2023-01-01\"),\n    )\n    def customers_export(context: AssetExecutionContext, customers: object) -> None:\n        # Here is where we could implement filters to only export certain columns\n        dt = datetime.strptime(context.partition_key, \"%Y-%m-%d\")\n        year = dt.strftime(\"%Y\")\n        month = dt.strftime(\"%m\")\n\n        # Filter if columns specified\n        if exported_columns is not None:\n            customers = customers[exported_columns]\n\n        # Export to CSV in the expected location\n        customers.to_csv(f\"./exports/customers_{year}_{month}.csv\", index=False)\n\n    return [customers_export]\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Notifications for Failed ML Job in Python\nDESCRIPTION: This code sets up a sensor in Dagster to send a Slack message when the ml_job fails. It demonstrates how to implement custom alerts for monitoring ML pipeline failures.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/managing-ml.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@sensor(job=ml_job)\ndef ml_job_failure_sensor(context):\n    if context.instance.get_run_records(limit=1)[0].status == DagsterRunStatus.FAILURE:\n        slack_client.chat_postMessage(\n            channel=\"#ml-alerts\",\n            text=f\"ML job failed at {datetime.now()}\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing Product Performance Asset in Dagster\nDESCRIPTION: Creates a category-partitioned asset that calculates performance metrics for each product category using DuckDB, including total sales and units sold.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-partitioned-asset.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(\n    deps=[joined_data],\n    partitions_def=product_category_partition,\n    group_name=\"analysis\",\n    compute_kind=\"duckdb\",\n)\ndef product_performance(context: dg.AssetExecutionContext, duckdb: DuckDBResource):\n    product_category_str = context.partition_key\n\n    with duckdb.get_connection() as conn:\n        conn.execute(\n            f\"\"\"\n            create table if not exists product_performance (\n                product_category varchar,\n                product_name varchar,\n                total_dollar_amount double,\n                total_units_sold double\n            );\n\n            delete from product_performance where product_category = '{product_category_str}';\n\n            insert into product_performance\n            select\n                '{product_category_str}' as product_category,\n                product_name,\n                sum(dollar_amount) as total_dollar_amount,\n                sum(quantity) as total_units_sold\n            from joined_data\n            where category = '{product_category_str}'\n            group by '{product_category_str}', product_name;\n            \"\"\"\n        )\n        preview_query = f\"select * from product_performance where product_category = '{product_category_str}';\"\n        preview_df = conn.execute(preview_query).fetchdf()\n        row_count = conn.execute(\n            f\"\"\"\n            SELECT COUNT(*)\n            FROM product_performance\n            WHERE product_category = '{product_category_str}';\n            \"\"\"\n        ).fetchone()\n        count = row_count[0] if row_count else 0\n\n    return dg.MaterializeResult(\n        metadata={\n            \"row_count\": dg.MetadataValue.int(count),\n            \"preview\": dg.MetadataValue.md(preview_df.to_markdown(index=False)),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Manual Retry Control with RetryRequested\nDESCRIPTION: Implementation of manual retry control using RetryRequested exception for more nuanced retry scenarios.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-retries.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op():\n    try:\n        if random() > 0.5:\n            raise Exception(\"Random failure!\")\n    except Exception as e:\n        raise RetryRequested(\n            max_retries=3,\n            seconds_to_wait=1.0\n        ) from e\n    return \"success\"\n```\n\n----------------------------------------\n\nTITLE: Creating Iris Dataset Asset in Python\nDESCRIPTION: Defines a Dagster asset that fetches the Iris dataset from a URL and returns it as a Pandas DataFrame. The asset is grouped under 'template_tutorial'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    group_name=\"template_tutorial\"\n)\ndef iris_dataset():\n    return pd.read_csv(\n        \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n        names=[\n            \"Sepal length (cm)\",\n            \"Sepal width (cm)\",\n            \"Petal length (cm)\",\n            \"Petal width (cm)\",\n            \"Species\",\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Table dbt Source in YAML\nDESCRIPTION: Example of defining a dbt source with multiple tables in a sources.yml file. This defines a source named 'clients_data' with two tables: 'names' and 'history'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: clients_data\n    tables:\n      - name: names\n      - name: history\n```\n\n----------------------------------------\n\nTITLE: Configuring AutomationCondition.eager() for Missing Upstream Data\nDESCRIPTION: Example showing how to modify AutomationCondition.eager() to allow execution even with missing upstream data by removing the any_deps_missing() condition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/example-customizations.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(auto_materialize_policy=AutoMaterializePolicy(AutomationCondition.eager() & ~AutomationCondition.any_deps_missing()))\n```\n\n----------------------------------------\n\nTITLE: Working with Partitioned Assets in Polars\nDESCRIPTION: Example showing how to use Polars with partitioned Dagster assets, where partitions are loaded as a dictionary of DataFrames.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-polars.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef downstream(partitioned_upstream: Dict[str, pl.LazyFrame]):\n    assert isinstance(partitioned_upstream, dict)\n    assert isinstance(partitioned_upstream[\"my_partition\"], pl.LazyFrame)\n```\n\n----------------------------------------\n\nTITLE: Defining Sling Connections for S3 and Snowflake in Python\nDESCRIPTION: This code snippet defines connections to S3 and Snowflake using SlingConnectionResource. It uses environment variables to securely manage sensitive information such as access keys and passwords.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/snowflake_to_s3_sling.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import EnvVar\nfrom dagster_sling import SlingConnectionResource, SlingResource\n\ns3_connection = SlingConnectionResource(\n    name=\"MY_S3\",\n    type=\"s3\",\n    bucket=\"your-s3-bucket\",\n    access_key_id=EnvVar(\"AWS_ACCESS_KEY_ID\"),\n    secret_access_key=EnvVar(\"AWS_SECRET_ACCESS_KEY\"),\n)\n\nsnowflake_connection = SlingConnectionResource(\n    name=\"MY_SNOWFLAKE\",\n    type=\"snowflake\",\n    host=\"your-snowflake-host\",\n    user=\"your-snowflake-user\",\n    database=\"your-snowflake-database\",\n    password=EnvVar(\"SNOWFLAKE_PASSWORD\"),\n    role=\"your-snowflake-role\",\n)\n\nsling_resource = SlingResource(connections=[s3_connection, snowflake_connection])\n```\n\n----------------------------------------\n\nTITLE: Limiting Concurrent Op Execution for a Single Run in Python\nDESCRIPTION: This Python code demonstrates how to limit the number of ops executing within a single run by configuring the run executor. It uses the 'max_concurrent' parameter in the run config.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@job\ndef my_job():\n    ...\n\nmy_job.execute_in_process(\n    run_config={\n        \"execution\": {\"config\": {\"multiprocess\": {\"max_concurrent\": 4}}}\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset Check in Dagster\nDESCRIPTION: Creates an asset check named 'missing_dimension_check' that validates if any rows in the joined_data asset are missing values for rep_name or product_name columns. The check uses SQL to query the asset and returns a CheckResult with success status and validation information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/ensure-data-quality-with-asset-checks.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset_check(asset=joined_data)\ndef missing_dimension_check(context, joined_data: pd.DataFrame) -> dg.CheckResult:\n    \n    # Find rows that have missing values for either rep_name or product_name\n    missing_values = joined_data[(joined_data[\"rep_name\"].isna()) | (joined_data[\"product_name\"].isna())]\n    \n    # Count how many rows have missing values\n    missing_count = len(missing_values)\n    \n    # Return a check result with metadata\n    return dg.CheckResult(\n        passed=missing_count == 0,\n        metadata={\n            \"missing_count\": missing_count,\n            \"missing_values\": missing_values.to_dict(\"records\") if missing_count > 0 else None,\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Asset Jobs in Python for Dagster\nDESCRIPTION: This snippet shows how to create asset jobs using the define_asset_job method. It creates two jobs: one targeting all assets and another targeting only the sugary_cereals asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/asset-jobs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nall_assets_job = define_asset_job(name=\"all_assets_job\", selection=\"*\")\nsugary_cereals_job = define_asset_job(name=\"sugary_cereals_job\", selection=\"sugary_cereals\")\n```\n\n----------------------------------------\n\nTITLE: Defining Snowflake Resource in Dagster\nDESCRIPTION: This Python code defines the Snowflake resource within Dagster using environment variables for configuration.  It leverages the `SnowflakeResource` class from the `dagster_snowflake` library to establish a connection to Snowflake.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/copy_csv_to_snowflake.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"from dagster import EnvVar\nfrom dagster_snowflake import SnowflakeResource\n\n\nsnowflake = SnowflakeResource(\n    account=EnvVar(\"SNOWFLAKE_ACCOUNT\"),\n    user=EnvVar(\"SNOWFLAKE_USER\"),\n    password=EnvVar(\"SNOWFLAKE_PASSWORD\"),\n    warehouse=EnvVar(\"SNOWFLAKE_WAREHOUSE\"),\n    database=EnvVar(\"SNOWFLAKE_DATABASE\"),\n    schema=EnvVar(\"SNOWFLAKE_SCHEMA\"),\n    role=EnvVar(\"SNOWFLAKE_ROLE\"),\n)\"\n```\n\n----------------------------------------\n\nTITLE: Adding Partitions to Dagster Assets\nDESCRIPTION: Python code demonstrating how to add DailyPartitionsDefinition to assets that correspond to time-partitioned Airflow tasks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/observe.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Code example omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: CLI Job Execution\nDESCRIPTION: Command line interface command to execute a Dagster job directly\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/job-execution.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster job execute -f my_job.py\n```\n\n----------------------------------------\n\nTITLE: Scheduling dbt-only Jobs in Python\nDESCRIPTION: This example demonstrates how to create and schedule a job that only contains dbt assets, selecting models with a specific tag.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[dbt_project_assets],\n    schedules=[\n        build_schedule_from_dbt_selection(\n            [dbt_project_assets],\n            job_name=\"daily_dbt_models\",\n            cron_schedule=\"0 0 * * *\",\n            dbt_select=\"tag:daily\",\n        )\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Message Passing in External Code\nDESCRIPTION: Shows how to send custom JSON-serializable messages from external code back to Dagster\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncontext.report_custom_message({\"foo\": \"bar\"})\n```\n\n----------------------------------------\n\nTITLE: Defining Databricks Asset Client\nDESCRIPTION: Python code that defines a Dagster asset that launches and manages a Databricks job with Pipes integration\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/databricks-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, MaterializeResult, asset\nfrom dagster_databricks import PipesDatabricksClient\nfrom databricks.sdk.service.jobs import ClusterLogConf, DockerImage, Library, NewCluster, SparkPythonTask, SubmitTask\n\n@asset(required_resource_keys={\"databricks_client\"})\ndef databricks_asset(context: AssetExecutionContext) -> MaterializeResult:\n    task = SubmitTask(\n        task_key=\"test_task\",\n        new_cluster=NewCluster(\n            node_type_id=\"Standard_DS3_v2\",\n            num_workers=1,\n            spark_version=\"11.3.x-scala2.12\",\n            custom_tags={\"ResourceClass\": \"SingleNode\"},\n            spark_conf={\"spark.master\": \"local[*, 4]\"},\n            cluster_log_conf=ClusterLogConf(dbfs={\"destination\": \"dbfs:/logs\"}, enable_cloudwatch=False),\n        ),\n        libraries=[Library(pypi={\"package\": \"dagster-pipes\"})],\n        spark_python_task=SparkPythonTask(\n            python_file=\"dbfs:/scripts/databricks_job.py\",\n        ),\n    )\n\n    extras = {\"some_parameter\": \"some_value\"}\n\n    pipes_invocation = context.resources.databricks_client.run(task, context, extras)\n    return pipes_invocation.get_materialize_result()\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with Environment Variables\nDESCRIPTION: Python code updating Dagster Definitions to use environment variables for sensitive configuration values, improving security and flexibility across environments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[items, comments, stories],\n    resources={\n        \"io_manager\": SnowflakePandasIOManager(\n            account=EnvVar(\"SNOWFLAKE_ACCOUNT\"),\n            user=EnvVar(\"SNOWFLAKE_USER\"),\n            password=EnvVar(\"SNOWFLAKE_PASSWORD\"),\n            database={\n                \"local\": \"DEVELOPMENT\",\n                \"staging\": \"STAGING\",\n                \"production\": \"PRODUCTION\",\n            }[os.getenv(\"DAGSTER_DEPLOYMENT\", \"local\")],\n            warehouse=EnvVar(\"SNOWFLAKE_WAREHOUSE\"),\n            schema=EnvVar(\"SNOWFLAKE_SCHEMA\"),\n            role=EnvVar(\"SNOWFLAKE_ROLE\"),\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset with Explicit Code Version in Python\nDESCRIPTION: Updates the previous asset to include an explicit code version, which helps Dagster track changes to the asset's implementation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(code_version=\"v1\")\ndef versioned_number():\n    return 1\n```\n\n----------------------------------------\n\nTITLE: Listing Available Component Types\nDESCRIPTION: Command-line example showing how to list all available component types after registering a new one.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ dg list --kind component_types\nType                   Module                                  Description\n--------------------  --------------------------------------  --------------------------------\nshell_command          my_component_library.lib.shell_command  A component that executes a shell command.\n```\n\n----------------------------------------\n\nTITLE: Pipeline Factory Implementation in Python\nDESCRIPTION: Defines a factory function to create reusable pipeline components for multiple RSS feeds. Includes configuration for assets, sensors, and job definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/rss-assets.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstart_factory\n```\n\n----------------------------------------\n\nTITLE: Updated Dagster Implementation with Pipes\nDESCRIPTION: Complete implementation showing the migration from step launchers to Dagster Pipes, including asset definitions and resource configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/migrating-from-step-launchers-to-pipes.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset, multi_asset, AssetOut, EventLogEntry\nfrom dagster_aws.pipes import PipesEMRClient\n\n@asset\ndef upstream(emr: PipesEMRClient):\n    emr.run_job(\n        get_emr_config(\"s3://my-bucket/upstream_asset_script.py\")\n    )\n    path = context.instance.get_event_records(\n        EventRecordsFilter(event_type=\"path\", asset_key=AssetKey(\"upstream\"))\n    )[0].event_log_entry.logged_by.step_key\n    return {\"path\": path}\n\n@multi_asset(outs={\"output_1\": AssetOut(), \"output_2\": AssetOut()})\ndef downstream(context, emr: PipesEMRClient, upstream):\n    result = emr.run_job(\n        get_emr_config(\n            \"s3://my-bucket/downstream_asset_script.py\",\n            extras={\"path\": upstream[\"path\"]},\n        )\n    )\n    return (\n        result[\"output_1\"],\n        result[\"output_2\"],\n    )\n\ndefs = Definitions(\n    assets=[upstream, downstream],\n    resources={\n        \"emr\": PipesEMRClient(\n            aws_region=\"us-east-1\",\n            poll_interval_seconds=10,\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Instance-level Kubernetes Settings in YAML\nDESCRIPTION: This snippet demonstrates how to set instance-level Kubernetes configuration using the k8sRunLauncher.runK8sConfig dictionary in the Helm chart. It includes settings for container, pod spec, metadata, and job configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nk8sRunLauncher:\n  runK8sConfig:\n    containerConfig:\n      resources:\n        limits:\n          cpu: 250m\n          memory: 512Mi\n        requests:\n          cpu: 100m\n          memory: 128Mi\n    podSpecConfig:\n      volumes:\n        - name: my-volume\n          emptyDir: {}\n    podTemplateSpecMetadata:\n      annotations:\n        \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\"\n    jobSpecConfig:\n      ttlSecondsAfterFinished: 600\n    jobMetadata:\n      annotations:\n        \"example.com/department\": \"finance\"\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI web server - Bash\nDESCRIPTION: This command starts the Dagster UI web server, allowing users to interact with their Dagster projects through a web interface at the specified port.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_gcp/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Migrating from non_argument_deps to deps in Asset Definitions\nDESCRIPTION: Examples showing how to update asset definitions to use the new deps parameter instead of the deprecated non_argument_deps parameter. This includes changing from sets to lists and optionally using Python symbols instead of asset keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef my_asset():\n   ...\n\n@asset(\n   non_argument_deps={\"my_asset\"}\n)\ndef a_downstream_asset():\n   ...\n\n# becomes\n\n@asset\ndef my_asset():\n   ...\n\n@asset(\n   deps=[\"my_asset\"]\n)\ndef a_downstream_asset():\n   ...\n\n# or\n\n@asset\ndef my_asset():\n   ...\n\n@asset(\n   deps=[my_asset]\n)\ndef a_downstream_asset():\n   ...\n```\n\n----------------------------------------\n\nTITLE: Creating Pipeline Factory Definition in Dagster\nDESCRIPTION: Defines a pipeline factory function that returns a Definitions object containing assets, jobs, and a sensor for podcast feed processing. Shows the basic structure of a reusable pipeline factory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/factory-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef rss_pipeline_factory(podcast_feed_url, podcast_name):\n    # factory returns definitions containing assets, job, sensor\n    return Definitions(\n        assets=[podcast_feed, latest_episode_ts, waveform_data, episode_spectrogram],\n        jobs=[define_asset_job(f\"{podcast_name}_job\")],\n        sensors=[make_podcast_sensor(podcast_feed_url, podcast_name)],\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Evidence Dashboard Component in YAML\nDESCRIPTION: YAML configuration for the EvidenceProject component, specifying the project directory and upstream assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/components-etl-pipeline-tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nevidence:\n  project_dir: ../../../../jaffle_dashboard\n  upstream_assets:\n    - target/main/customers\n    - target/main/orders\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Component Subclass in Python\nDESCRIPTION: Demonstrates how to create a basic subclass of a Dagster component in a component.py file to customize behavior.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/customizing-components.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/components/custom-subclass/basic-subclass.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Operands and Grouping in Markdown\nDESCRIPTION: This markdown table illustrates the use of operands (and, or, not) and grouping in Dagster asset selection queries. It shows how to combine multiple filters and control the order of evaluation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Operand | Syntax | Description |\n|---------|--------|-------------|\n| **`and`** | `owner:\"alice\" and kind:\"dbt\"` | Selects assets owned by `alice` of kind `dbt`. |\n| **`or`** | `owner:\"billing\" or owner:\"sales\"` | Selects assets owned by either `billing` or `sales`. |\n| **`not`** | `not tag:\"obsolete\"` | Excludes assets tagged with `obsolete`. |\n| **Grouping `()`** | `(owner:\"alice\" or group:\"analytics\") and  kind:\"table\"` | Symbols `(` and `)` used to group expressions and control the order of evaluation in queries. This example selects assets that are both owned by `alice` and of kind `table`, or that belong to the `analytics` group. |\n```\n\n----------------------------------------\n\nTITLE: Adding Asset Checks to Dagster Definitions\nDESCRIPTION: Python code demonstrating how to add an asset check to validate the quality of the 'customers' CSV output in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/peer.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom dagster import Definitions, asset_check\nfrom dagster_airlift.core import build_defs_from_airflow_instance\nimport pandas as pd\n\n@asset_check(asset_key=\"rebuild_customers_list\")\ndef check_customers_output():\n    customers_path = os.path.join(\n        os.environ[\"TUTORIAL_EXAMPLE_DIR\"],\n        \"tutorial_example/shared/output/customers.csv\"\n    )\n    assert os.path.exists(customers_path), f\"Output file {customers_path} does not exist\"\n    df = pd.read_csv(customers_path)\n    assert len(df) > A0, f\"Output file {customers_path} is empty\"\n\ndefs = Definitions(\n    assets=build_defs_from_airflow_instance(\n        instance_uri=\"http://localhost:8080\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    asset_checks=[check_customers_output],\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling Asset Checks for dbt Tests in Dagster\nDESCRIPTION: This snippet demonstrates how to disable modeling dbt tests as asset checks in Dagster. It uses a custom DagsterDbtTranslator with DagsterDbtTranslatorSettings to disable asset checks when using the dbt_assets decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass CustomDbtTranslator(DagsterDbtTranslator):\n    def __init__(self):\n        super().__init__(\n            settings=DagsterDbtTranslatorSettings(\n                enable_asset_checks=False,\n            )\n        )\n\n@dbt_assets(\n    manifest=manifest,\n    dagster_dbt_translator=CustomDbtTranslator(),\n)\ndef my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):\n    yield from dbt.cli([\n        \"build\",\n        \"--select\",\n        \"my_model\",\n        \"--profiles-dir\",\n        \".\",\n        \"--project-dir\",\n        \".\",\n    ], context=context).stream()\n```\n\n----------------------------------------\n\nTITLE: Creating Dynamic Outputs in Python with Dagster\nDESCRIPTION: This code snippet shows how to define an op that uses dynamic outputs in Dagster. It demonstrates the use of DynamicOut for declaration and yielding DynamicOutput objects with unique mapping keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@op(out=DynamicOut())\ndef dynamic_outputs():\n    for i in range(10):\n        yield DynamicOutput(\n            value=i,\n            mapping_key=str(i),\n        )\n```\n\n----------------------------------------\n\nTITLE: Setting Global Log Level in Dagster YAML\nDESCRIPTION: YAML configuration to set a global log level for all loggers managed by Dagster. This example sets the log level to INFO, filtering out DEBUG level logs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/python-logging.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npython_logs:\n  python_log_level: INFO\n```\n\n----------------------------------------\n\nTITLE: Implementing Resources in Dagster Sensor\nDESCRIPTION: Example showing how to define a sensor that uses a resource to access an external API. The resource is specified as a parameter to the sensor function and provided through the Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/using-resources-in-sensors.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@sensor(job=do_stuff)\ndef my_sensor(context, my_api_client):\n    should_run = my_api_client.should_run()\n    if should_run:\n        return RunRequest(\n            run_key=None,\n            run_config={},\n        )\n    return SkipReason(\"Condition not met\")\n\ndefs = Definitions(\n    sensors=[my_sensor],\n    resources={\n        \"my_api_client\": MyAPIClient(),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sensor Evaluation in Dagster YAML\nDESCRIPTION: This snippet shows how to configure sensor evaluation in the Dagster instance YAML file. It demonstrates setting up parallel evaluation using threads and specifying the number of workers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nsensors:\n  use_threads: true\n  num_workers: 4\n```\n\n----------------------------------------\n\nTITLE: Representing Looker Assets in Dagster\nDESCRIPTION: Python code to set up a LookerResource and load Looker asset specs into the Dagster asset graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/looker.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_looker import LookerResource, load_looker_asset_specs\n\nlooker_resource = LookerResource(\n    base_url=EnvVar(\"LOOKER_BASE_URL\"),\n    client_id=EnvVar(\"LOOKER_CLIENT_ID\"),\n    client_secret=EnvVar(\"LOOKER_CLIENT_SECRET\"),\n)\n\nlooker_asset_specs = load_looker_asset_specs(looker_resource)\n\ndefs = Definitions(\n    assets=looker_asset_specs,\n    resources={\"looker\": looker_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Computing Bollinger Bands for Multiple Stocks in Python\nDESCRIPTION: This snippet calculates Bollinger Bands for multiple stocks using the previously loaded price data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/notebooks/bollinger.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nBOLL = bol.compute_bollinger_bands_multi(PRICES)\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project with CLI\nDESCRIPTION: This command uses the Dagster CLI to create a new project named 'jaffle-platform'. It sets up the initial project structure and necessary files for a Dagster workflow.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-a-uv-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg init jaffle-platform\n```\n\n----------------------------------------\n\nTITLE: Configuring Logger in Dagster UI with YAML\nDESCRIPTION: Example of YAML configuration to change logger settings in the Dagster UI. This allows modifying the logger configuration without changing code and is useful for manual asset materialization or adjusting log verbosity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/custom-logging.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nloggers:\n  console:\n    config:\n      log_level: DEBUG\n```\n\n----------------------------------------\n\nTITLE: Configuring Default and Optional Fields in Dagster Config\nDESCRIPTION: Shows how to specify default values for config fields and make fields optional. Fields with defaults are not required when constructing the config object, and fields marked as Optional have an implicit default of None.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyAssetConfig(Config):\n    greeting_phrase: str = \"hello\"\n    person_name: Optional[str] = None\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    # Can construct with defaults\n    config1 = MyAssetConfig()\n    assert config1.greeting_phrase == \"hello\"\n    assert config1.person_name is None\n\n    # Or can override defaults\n    config2 = MyAssetConfig(greeting_phrase=\"hi\", person_name=\"Alice\")\n    assert config2.greeting_phrase == \"hi\"\n    assert config2.person_name == \"Alice\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Resource in Python\nDESCRIPTION: Python code showing how to configure the Snowflake resource with account details, authentication, and storage settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsnowflake_resource = SnowflakeResource(\n    account=\"your-account\",\n    user=os.getenv(\"SNOWFLAKE_USER\"),\n    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n    warehouse=\"PLANTS\",\n    database=\"FLOWERS\",\n    schema=\"IRIS\",\n    role=\"WRITER\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Asset with PipesEMRServerlessClient\nDESCRIPTION: This Python code defines a Dagster asset that uses the PipesEMRServerlessClient to launch an EMR Serverless job. It specifies the application ID, job name, execution role, and other parameters needed for the job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-serverless-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef emr_serverless_asset(emr_serverless_client: PipesEMRServerlessClient):\n    emr_serverless_client.run(\n        application_id=\"app-12345\",\n        execution_role_arn=\"arn:aws:iam::123456789012:role/EMRServerlessS3RuntimeRole\",\n        job_role_arn=\"arn:aws:iam::123456789012:role/EMRServerlessJobRole\",\n        name=\"emr-serverless-job\",\n        spark_submit_entry_point=\"s3://my-bucket/emr-serverless-job.py\",\n        wait_for_completion=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Providing Dagster Resource Configuration at Launch Time\nDESCRIPTION: Demonstrates how to provide configuration for a resource at launch time using a RunRequest. The example shows a sensor that triggers a run with a specific target_table configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/configuring-resources.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import RunRequest, sensor\n\n@sensor(job=my_job)\ndef my_sensor():\n    yield RunRequest(\n        run_key=None,\n        run_config={\n            \"resources\": {\n                \"configurable_resource\": {\"config\": {\"target_table\": \"my_table\"}}\n            }\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns in Downstream Snowflake Asset\nDESCRIPTION: Shows how to select specific columns when loading data from a Snowflake table into a downstream asset using metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetIn\n\n@asset(\n    ins={\n        \"iris_dataset\": AssetIn(\n            metadata={\"columns\": [\"sepal_length_cm\", \"sepal_width_cm\"]}\n        )\n    }\n)\ndef sepal_data(iris_dataset):\n    return iris_dataset[[\"sepal_length_cm\", \"sepal_width_cm\"]]\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster with Environment Variables in Python Code\nDESCRIPTION: Examples of how to use the EnvVar class to incorporate environment variables into Dagster configuration. Includes examples for string and integer values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"PARAMETER_NAME\": EnvVar(\"ENVIRONMENT_VARIABLE_NAME\")\n```\n\nLANGUAGE: python\nCODE:\n```\n\"access_token\": EnvVar(\"GITHUB_ACCESS_TOKEN\")\n```\n\nLANGUAGE: python\nCODE:\n```\n\"database_port\": EnvVar.int(\"DATABASE_PORT\")\n```\n\n----------------------------------------\n\nTITLE: Enhancing Branch Deployment Creation with Additional Parameters\nDESCRIPTION: This snippet shows how to include optional parameters when creating or updating a branch deployment. These parameters provide additional information for the Branch Deployments UI in Dagster+, such as code review URL, pull request status, commit message, and author details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nBRANCH_DEPLOYMENT_NAME=$(\n    dagster-cloud branch-deployment create-or-update \\\n        --organization $ORGANIZATION_NAME \\\n        --api-token $DAGSTER_CLOUD_API_TOKEN \\\n        --git-repo-name $REPOSITORY_NAME \\\n        --branch-name $BRANCH_NAME \\\n        --commit-hash $COMMIT_SHA \\\n        --timestamp $TIMESTAMP\n        --code-review-url $PR_URL \\ # URL to review the given changes, e.g.\n            # Pull Request or Merge Request\n        --code-review-id $INPUT_PR \\ # Alphanumeric ID for the given set of changes\n        --pull-request-status $PR_STATUS \\ # A status, one of `OPEN`, `CLOSED`,\n            # or `MERGED`, that describes the set of changes\n        --commit-message $MESSAGE \\ # The message associated with the latest commit\n        --author-name $NAME \\ # A display name for the latest commit's author\n        --author-email $EMAIL \\ # An email for the latest commit's author\n        --author-avatar-url $AVATAR_URL # An avatar URL for the latest commit's author\n        --base-deployment-name $BASE_DEPLOYMENT_NAME # The main deployment that will be compared against. Default is 'prod'\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Deployment Settings in YAML\nDESCRIPTION: Example YAML configuration for setting up concurrency, run monitoring, and run retries in a Dagster+ deployment. This configuration file can be used with the dagster-cloud CLI to update deployment settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/managing-deployments.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# my-settings.yaml\n\nconcurrency:\n  pools:\n    granularity: 'run'\n    default_limit: 1\n  runs:\n    max_concurrent_runs: 10\n    tag_concurrency_limits:\n      - key: 'database'\n        value: 'redshift'\n        limit: 5\n\nrun_monitoring:\n  start_timeout_seconds: 1200\n  cancel_timeout_seconds: 1200\n\nrun_retries:\n  max_retries: 0\n```\n\n----------------------------------------\n\nTITLE: Batch Processing for Documentation Embeddings\nDESCRIPTION: Implementation of batch processing for documentation content, including text chunking and embedding generation. Handles large documentation pages by splitting them into manageable chunks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/embeddings.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef split_text(text, chunk_size=1000, chunk_overlap=100):\n    \"\"\"Split text into chunks of a specified size with overlap\"\"\"\n    words = text.split()\n    chunks = []\n    for i in range(0, len(words), chunk_size - chunk_overlap):\n        chunk = \" \".join(words[i:i + chunk_size])\n        chunks.append(chunk)\n    return chunks\n\n\nchunks = []\nfor doc in docs_content_raw:\n    split_chunks = split_text(doc.page_content)\n    url_hash = str(hash(doc.metadata[\"source\"]))[:5]\n    for i, chunk in enumerate(split_chunks):\n        chunks.append(\n            Document(\n                page_content=chunk,\n                metadata={\n                    **doc.metadata,\n                    \"chunk_id\": f\"{url_hash}_{i}\",\n                },\n            )\n        )\n\nbatch_size = 100\nfor i in range(0, len(chunks), batch_size):\n    batch = chunks[i : i + batch_size]\n    texts = [doc.page_content for doc in batch]\n    embeds = openai.get_embeddings(texts)\n\n    to_upsert = []\n    for doc, embedding in zip(batch, embeds):\n        to_upsert.append(\n            (\n                f\"{doc.metadata['source']}_{doc.metadata['chunk_id']}\",\n                embedding,\n                {\"text\": doc.page_content, **doc.metadata},\n            )\n        )\n\n    index.upsert(vectors=to_upsert)\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Tables with Dagster\nDESCRIPTION: Python code demonstrating how to create BigQuery tables using the BigQuery resource in Dagster\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\n@asset\ndef iris_data(context):\n    iris_raw = load_iris()\n    df = pd.DataFrame(iris_raw[\"data\"], columns=iris_raw[\"feature_names\"])\n    df.columns = [col.replace(\" (cm)\", \"\").replace(\" \", \"_\") for col in df.columns]\n    context.resources.bigquery.load_table_from_dataframe(\n        dataframe=df,\n        table_ref=\"iris.iris_data\",\n        project=\"my-gcp-project\"\n    )\n    return df\n```\n\n----------------------------------------\n\nTITLE: Attaching Metadata to Expectation Results in Dagster\nDESCRIPTION: This snippet demonstrates how to attach different types of metadata to an expectation result in a Dagster op. It uses the MetadataValue class to associate various data types with the expectation event.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef metadata_expectation_op():\n    yield ExpectationResult(\n        success=True,\n        description=\"Expectation result with metadata\",\n        metadata={\n            \"text\": MetadataValue.text(\"Text-based metadata for humans to read\"),\n            \"url\": MetadataValue.url(\"http://mycoolsite.com/url_for_clicking.html\"),\n            \"json\": MetadataValue.json({\"key\": \"value\", \"nested_key\": {\"key\": \"value\"}}),\n            \"python\": MetadataValue.python_artifact(List),\n            \"path\": MetadataValue.path(\"/path/to/file\"),\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: GitHub Issues Raw Data Extraction Asset\nDESCRIPTION: Dagster asset that extracts raw GitHub issues data using GithubResource and converts them into LangChain Documents. Uses weekly partitioning and automation conditions for scheduled updates.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/embeddings.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    partitions_def=weekly_partitions,\n    compute_kind=\"github\",\n    auto_materialize_policy=AutomationCondition.ON_MONDAYS,\n)\ndef github_issues_raw(context, github: GithubResource):\n    issues = []\n    partition_date_str = context.asset_partition_key_for_output()\n    start_date = pd.to_datetime(partition_date_str)\n    end_date = start_date + pd.Timedelta(days=7)\n\n    for repo in context.resources.github.repos:\n        for issue in github.get_issues(repo, start_date, end_date):\n            issues.append(\n                Document(\n                    page_content=f\"Title: {issue.title}\\n\\nBody: {issue.body}\",\n                    metadata={\n                        \"source\": issue.html_url,\n                        \"type\": \"issue\",\n                        \"created_at\": issue.created_at.isoformat(),\n                    },\n                )\n            )\n    return issues\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-dbt Package\nDESCRIPTION: Command to install the dagster-dbt Python package via pip package manager\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/dbt-cloud.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-dbt\n```\n\n----------------------------------------\n\nTITLE: Configurable API Resource Implementation\nDESCRIPTION: Enhanced version of the SunResource that supports configurable location parameters\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-apis.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Definitions, Config\n\nclass SunResourceConfig(Config):\n    latitude: str\n    longitude: str\n    timezone: str\n\nclass SunResource(ConfigurableResource):\n    config: SunResourceConfig\n\n    def get_sunrise(self):\n        response = requests.get(\n            \"https://api.sunrise-sunset.org/json\",\n            params={\n                \"lat\": self.config.latitude,\n                \"lng\": self.config.longitude,\n                \"formatted\": 0,\n            },\n        )\n        response.raise_for_status()\n        return response.json()[\"results\"][\"sunrise\"]\n\ndefs = Definitions(\n    assets=[sfo_sunrise],\n    resources={\n        \"sun_resource\": SunResource(config=SunResourceConfig(latitude=\"37.6213\", longitude=\"-122.3790\", timezone=\"US/Pacific\")),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Automation Condition Sensor for Declarative Automation in Python\nDESCRIPTION: Defines an AutomationConditionSensorDefinition to set up Declarative Automation. This sensor will trigger the execution of assets based on their automation conditions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/federate-execution.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nautomation_sensor = AutomationConditionSensorDefinition(\n    name=\"automation_sensor\",\n    asset_selection=AssetSelection.all(),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Single Output in Python\nDESCRIPTION: Demonstrates how to define an op with a single output. The output is automatically named 'result'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op():\n    return 1\n```\n\n----------------------------------------\n\nTITLE: DBFS File Upload Command\nDESCRIPTION: Shell command to upload a Python script to Databricks File System (DBFS).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/databricks-pipeline.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndbfs cp my_python_script.py dbfs:/my_python_script.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Helm Values for Dagster Deployment\nDESCRIPTION: YAML configuration for Helm chart deployment specifying the user deployment settings including image name, tag, and gRPC server arguments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/deploying-to-kubernetes.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndagster-user-deployments:\n  enabled: true\n  deployments:\n    - name: iris-analysis\n      image:\n        name: iris_analysis\n        tag: 1\n        pullPolicy: IfNotPresent\n      dagsterApiGrpcArgs:\n        - --python-file\n        - /iris_analysis/definitions.py\n```\n\n----------------------------------------\n\nTITLE: Creating Training Files for LLM Fine-Tuning in Python\nDESCRIPTION: This asset function generates a training file by sampling data from the enriched_graphic_novels dataset and converting it to the JSONL format required by OpenAI. It samples a subset of records, creates prompt records for each, and writes them to a local JSONL file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/file-creation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(deps=[enriched_graphic_novels])\ndef training_file(\n    context: AssetExecutionContext, enriched_graphic_novels: pd.DataFrame\n) -> None:\n    # Create a JSONL file with the data for fine-tuning\n    sample_records = enriched_graphic_novels.sample(n=10, random_state=42)\n    prompt_records = [create_prompt_record(record) for record in sample_records.to_dict(orient=\"records\")]\n    \n    filename = \"training_data.jsonl\"\n    context.log.info(f\"Creating {filename} with {len(prompt_records)} records\")\n    \n    # Write the records to a JSONL file\n    with open(filename, \"w\") as f:\n        for record in prompt_records:\n            f.write(json.dumps(record) + \"\\n\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Automation Based on Run Tags\nDESCRIPTION: Shows how to configure AutomationCondition.eager() to only respond to upstream events from runs with specific tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/example-customizations.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@asset(auto_materialize_policy=AutoMaterializePolicy(AutomationCondition.eager() & AutomationCondition.executed_with_tags({\"dagster/automation\": \"true\"})))\n```\n\n----------------------------------------\n\nTITLE: Implementing dbt Execution with Dagster dbt Assets\nDESCRIPTION: Equivalent implementation in Dagster using the dagster-dbt library. The code demonstrates how to define dbt assets using the @dbt_assets decorator with a DbtCliResource to execute dbt commands.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/bash-operator-dbt.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, Definitions\nfrom dagster_dbt import DbtCliResource, dbt_assets\n\n@dbt_assets(manifest_path=\"/path/to/dbt/project/target/manifest.json\")\ndef my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):\n    yield from dbt.cli([\"run\"], context=context).stream()\n\n\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\n        \"dbt\": DbtCliResource(\n            project_dir=\"/path/to/dbt/project\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Multi-DataFrame DuckDB I/O Manager\nDESCRIPTION: Implementation of a custom I/O manager that handles multiple DataFrame types in DuckDB.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass MultiDataFrameIOManager(DuckDBIOManager):\n    @property\n    def type_handlers(self) -> Mapping[Type, DuckDBTypeHandler]:\n        return {\n            pd.DataFrame: PandasTypeHandler(),\n            pl.DataFrame: PolarsTypeHandler(),\n            pyspark.sql.DataFrame: PySparkTypeHandler(),\n        }\n\n    def default_load_type(self, upstream_output: Optional[AssetMaterialization]) -> Type:\n        return pd.DataFrame\n```\n\n----------------------------------------\n\nTITLE: Authenticating Snowflake Resource with Private Key\nDESCRIPTION: Demonstrates how to authenticate a Snowflake resource using a private key, either directly or via a file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake import SnowflakeResource\n\nsnowflake_resource = SnowflakeResource(\n    account=\"account\",\n    user=\"user\",\n    private_key=\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\nMIIFHDBOBgkqhkiG9w0BBQ0wQTApBgkqhkiG9w0BBQwwHAQIxwnF5G/9/eQCAggA\\n...\n-----END ENCRYPTED PRIVATE KEY-----\",\n    private_key_password=\"password\",\n    warehouse=\"warehouse\",\n    database=\"database\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake import SnowflakeResource\n\nsnowflake_resource = SnowflakeResource(\n    account=\"account\",\n    user=\"user\",\n    private_key_path=\"/path/to/private/key/file.p8\",\n    private_key_password=\"password\",\n    warehouse=\"warehouse\",\n    database=\"database\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster DLT Assets and Resource\nDESCRIPTION: Complete example showing how to define DLT assets and resources in Dagster, including GitHub integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, Definitions\nfrom dagster_dlt import DagsterDltResource, dlt_assets\nfrom dlt import pipeline\nfrom dlt_sources.github import github_reactions\n\n\n@dlt_assets(\n    dlt_source=github_reactions(\n        \"dagster-io\", \"dagster\", max_items=250\n    ),\n    dlt_pipeline=pipeline(\n        pipeline_name=\"github_issues\",\n        dataset_name=\"github\",\n        destination=\"snowflake\",\n        progress=\"log\",\n    ),\n    name=\"github\",\n    group_name=\"github\",\n)\ndef dagster_github_assets(context: AssetExecutionContext, dlt: DagsterDltResource):\n    yield from dlt.run(context=context)\n```\n\n----------------------------------------\n\nTITLE: Setting Automation Conditions on Dagster Assets\nDESCRIPTION: Demonstrates how to set automation conditions on assets using either the @asset decorator or AssetSpec object. Shows configuration for eager execution and cron-based scheduling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagster as dg\n\n@dg.asset(automation_condition=dg.AutomationCondition.eager())\ndef my_eager_asset(): ...\n\nAssetSpec(\"my_cron_asset\", automation_condition=AutomationCondition.on_cron(\"@daily\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Asset for SFTP File Upload\nDESCRIPTION: This code defines a Dagster asset that uses the SFTP resource to upload a file. It specifies the local and remote file paths, performs the upload, and logs the operation. The asset is then included in the Dagster Definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/sftp_file_upload.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetExecutionContext, asset\nfrom dagster_ssh import SSHResource\n\n\n@asset\ndef sftp_file(context: AssetExecutionContext, sftp: SSHResource):\n    local_file_path = \"hello.txt\"\n    remote_file_path = \"/path/to/destination/hello.txt\"\n    sftp.sftp_put(local_file_path, remote_file_path)\n    context.log.info(f\"Uploaded {local_file_path} to {remote_file_path} on SFTP server\")\n\n\ndefs = Definitions(\n    assets=[sftp_file],\n    resources={\n        \"sftp\": sftp_resource,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental dbt Model in SQL\nDESCRIPTION: Example of configuring an incremental dbt model in SQL. This model is set up to use the delete+insert strategy and filters rows based on order_date using Dagster partition variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\n-- Configure the model as incremental, use a unique_key and the delete+insert strategy to ensure the pipeline is idempotent.\n{{ config(materialized='incremental', unique_key='order_date', incremental_strategy=\"delete+insert\") }}\n\nselect * from {{ ref('my_model') }}\n\n-- Use the Dagster partition variables to filter rows on an incremental run\n{% if is_incremental() %}\nwhere order_date >= '{{ var('min_date') }}' and order_date <= '{{ var('max_date') }}'\n{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Implementing InputManager Interface for Unconnected Inputs\nDESCRIPTION: Shows how to define a class that implements the InputManager interface to load unconnected inputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/unconnected-inputs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import InputManager, In, op, job\n\nclass Table1InputManager(InputManager):\n    def load_input(self, context):\n        # do something to load data\n        return [1, 2, 3]\n\n@op(ins={\"table1\": In(input_manager_key=\"table1\")})\ndef process_table1(table1):\n    return table1\n\n@job(resource_defs={\"table1\": Table1InputManager()})\ndef my_job():\n    process_table1()\n```\n\n----------------------------------------\n\nTITLE: Creating Resource Definitions in Dagster Hierarchy\nDESCRIPTION: Shows how to create a Definitions object to add resources at any level in the Dagster defs hierarchy. Demonstrates resource binding flexibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/using-resources.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/dg/using-resources/3-resource-defs-at-project-root.py\" />\n```\n\n----------------------------------------\n\nTITLE: Modifying an Asset's Metadata in Branch Deployment - Python\nDESCRIPTION: Example of the same asset with updated metadata that includes an additional 'backstock' column in the branch deployment. Change Tracking will detect metadata changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"expected_columns\": [\"sku\", \"price\", \"supplier\", \"backstock\"]})\ndef products(): ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving Bluesky Starter Pack Members\nDESCRIPTION: Utility function to fetch members from a Bluesky starter pack using the atproto client.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/ingestion.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstart_starter_pack\ndef get_starter_pack_members(client: AtprotoClient, pack_name: str) -> list[dict]:\n    pack_list = client.get_actor_pack_list(pack_name)\n    members = []\n    for member in pack_list.data.items:\n        members.append({\"member\": member.actor, \"name\": member.name})\n    return members\nend_starter_pack\n```\n\n----------------------------------------\n\nTITLE: Limiting Concurrent Runs by Run Tag in YAML Configuration\nDESCRIPTION: This YAML configuration demonstrates how to limit the number of in-progress runs by run tag. It includes examples for limiting runs by sensor name and backfill tag.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  runs:\n    tag_concurrency_limits:\n      - key: 'dagster/sensor_name'\n        value: 'my_cool_sensor'\n        limit: 5\n      - key: 'dagster/backfill'\n        limit: 10\n```\n\n----------------------------------------\n\nTITLE: Defining DAG-mapped assets in Dagster (Python)\nDESCRIPTION: This code snippet shows how to define fully migrated DAG-mapped assets in Dagster. It includes asset definitions with partitioning and dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/migrate.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetIn, DailyPartitionsDefinition\nfrom dagster_airbyte import airbyte_resource, load_assets_from_airbyte_instance\nfrom dagster_dbt import load_assets_from_dbt_project\n\npartition_def = DailyPartitionsDefinition(start_date=\"2023-01-01\")\n\n@asset(partitions_def=partition_def)\ndef customers_raw():\n    ...\n\n@asset(partitions_def=partition_def)\ndef customers_clean():\n    ...\n\n@asset(\n    partitions_def=partition_def,\n    ins={\"customers_clean\": AssetIn(partition_mapping={\"start\": \"start\", \"end\": \"end\"})},\n)\ndef rebuild_customers_list(context, customers_clean):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Fetching Row Counts for dbt Models in Dagster\nDESCRIPTION: This example shows how to automatically fetch row counts for dbt-generated tables and emit them as materialization metadata in Dagster. It uses the fetch_row_counts() method on the DbtEventIterator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@dbt_assets(manifest=manifest)\ndef my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):\n    yield from dbt.cli(\n        [\n            \"build\",\n            \"--select\",\n            \"my_model\",\n            \"--profiles-dir\",\n            \".\",\n            \"--project-dir\",\n            \".\",\n        ],\n        context=context,\n    ).stream().fetch_row_counts()\n```\n\n----------------------------------------\n\nTITLE: Building dbt Project in Shell\nDESCRIPTION: Executes the dbt build command to run all models, seeds, and snapshots in the project, storing the resulting tables in the DuckDB database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/set-up-dbt-project.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndbt build\n```\n\n----------------------------------------\n\nTITLE: Customizing Airbyte Cloud Asset Materialization\nDESCRIPTION: Customize the sync of Airbyte Cloud connections using the airbyte_assets decorator to execute custom code before and after the sync.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_airbyte import AirbyteCloudWorkspace, airbyte_assets\n\nairbyte_cloud_resource = AirbyteCloudWorkspace(\n    workspace_id=\"your_workspace_id\",\n    client_id=\"your_client_id\",\n    client_secret=\"your_client_secret\",\n)\n\n@airbyte_assets(airbyte_cloud_resource)\ndef my_airbyte_assets(context):\n    # Custom code before sync\n    context.log.info(\"Starting Airbyte Cloud sync\")\n    \n    yield\n    \n    # Custom code after sync\n    context.log.info(\"Airbyte Cloud sync completed\")\n\ndefs = Definitions(\n    assets=[my_airbyte_assets],\n    resources={\"airbyte_cloud\": airbyte_cloud_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Pipes Session in Orchestration Process (Python)\nDESCRIPTION: Demonstrates how to start a Pipes session using the open_pipes_session context manager within an asset execution context. It shows the setup of context injector and message reader components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/dagster-pipes-details-and-customization.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetExecutionContext\nfrom dagster.core.execution.context.compute import AssetExecutionContext\nfrom dagster.pipes import (\n    open_pipes_session,\n    PipesTempFileContextInjector,\n    PipesTempFileMessageReader,\n)\n\n@asset\ndef my_asset(context: AssetExecutionContext):\n    with open_pipes_session(\n        context=context,\n        extras={\"my_key\": \"my_value\"},\n        context_injector=PipesTempFileContextInjector(),\n        message_reader=PipesTempFileMessageReader(),\n    ) as pipes_session:\n        # Launch external process\n        # Poll for completion\n        # Terminate external process\n        yield from pipes_session.get_results()\n```\n\n----------------------------------------\n\nTITLE: Customizing ECS Task Configuration with Job Tags\nDESCRIPTION: This Python code shows how to use job tags to customize the ECS task configuration for every run of a particular Dagster job, specifically setting it to use Fargate Spot instances.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op\n\n@op()\ndef my_op(context):\n  context.log.info('running')\n\n@job(\n  tags = {\n    \"ecs/run_task_kwargs\": {\n      \"capacityProviderStrategy\": [\n        {\n          \"capacityProvider\": \"FARGATE_SPOT\",\n        },\n      ],\n    },\n  }\n)\ndef my_job():\n  my_op()\n```\n\n----------------------------------------\n\nTITLE: Setting Timezone for Partitioned Job in Python\nDESCRIPTION: This example shows how to set a timezone (US Pacific time) for a partitioned job. The partition definition uses the timezone parameter to specify the America/Los_Angeles timezone.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/customizing-execution-timezone.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DailyPartitionsDefinition\nfrom datetime import datetime\n\nmy_partitions = DailyPartitionsDefinition(\n    start_date=datetime(2023, 1, 1),\n    timezone=\"America/Los_Angeles\",\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Documentation Snippets with Tox\nDESCRIPTION: Shows how to use tox with UV to verify that all code snippets load correctly into Python. This command should be used to test documentation examples before submitting them.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install tox-uv\ntox\n```\n\n----------------------------------------\n\nTITLE: Defining a Dagster asset to run CLI command using PipesSubprocessClient\nDESCRIPTION: This Python code defines a Dagster asset that uses PipesSubprocessClient to run the external Bash script. It demonstrates how to set up the command, pass environment variables, and use the subprocess client within a Dagster asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/pipes_cli_command.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport shutil\n\nfrom dagster import AssetExecutionContext, Definitions, PipesSubprocessClient, asset\n\n@asset\ndef cli_command_asset(\n    context: AssetExecutionContext, pipes_subprocess_client: PipesSubprocessClient\n):\n    cmd = [shutil.which(\"bash\"), \"external_script.sh\"]\n    return pipes_subprocess_client.run(\n        command=cmd,\n        context=context,\n        env={\"MY_ENV_VAR\": \"example_value\"},\n    ).get_materialize_result()\n\ndefs = Definitions(\n    assets=[cli_command_asset],\n    resources={\"pipes_subprocess_client\": PipesSubprocessClient()},\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Blocking Asset Checks\nDESCRIPTION: Demonstrates how to combine blocking checks with other automation conditions to ensure asset execution only occurs after upstream checks pass.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/example-customizations.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@asset(auto_materialize_policy=AutoMaterializePolicy(AutomationCondition.on_cron(\"0 0 * * *\") & AutomationCondition.all_deps_blocking_checks_passed()))\n```\n\n----------------------------------------\n\nTITLE: Updating Dagster Definitions with Executable Asset in Python\nDESCRIPTION: Replaces the static asset representation with the executable 'run_customer_metrics' function in the Dagster Definitions object. This allows the asset to be materialized through Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/federate-execution.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndefs = Definitions(\n    assets=[load_customers_dag_asset, run_customer_metrics],\n    resources={\n        \"customers_airflow_instance\": customers_airflow_instance,\n        \"metrics_airflow_instance\": metrics_airflow_instance,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Owners to Dagster Assets in Python\nDESCRIPTION: This snippet demonstrates how to add ownership information to a Dagster asset using the 'owners' parameter. Owners can be specified as email addresses or team names prefixed with 'team:'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(owners=[\"data-science@company.com\", \"team:data-eng\"])\ndef my_asset():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating Job with Static Partitions in Python\nDESCRIPTION: This snippet demonstrates how to create a job with static partitions (by continent) using define_asset_job in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/constructing-schedules-for-partitioned-assets-and-jobs.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncontinent_job = define_asset_job(\n    \"continent_job\",\n    selection=\"*\",\n    partitions_def=StaticPartitionsDefinition(\n        [\"Africa\", \"Antarctica\", \"Asia\", \"Europe\", \"North America\", \"Oceania\", \"South America\"]\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Database to Database Sling Configuration\nDESCRIPTION: Complete example showing how to configure Sling for database-to-database replication using Postgres and Snowflake\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sling.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_sling import sling_assets, SlingResource, SlingConnectionResource\n\nconfig = {\n    \"source\": \"MY_POSTGRES\",\n    \"target\": \"MY_SNOWFLAKE\",\n    \"streams\": {\n        \"public.accounts\": {},\n        \"public.users\": {},\n    }\n}\n\nsling = SlingResource(\n    connections=[\n        SlingConnectionResource(\n            name=\"MY_POSTGRES\",\n            type=\"postgres\",\n            host=\"localhost\",\n            port=5432,\n            database=\"my_database\",\n            username=\"my_user\",\n            password=\"my_password\",\n        ),\n        SlingConnectionResource(\n            name=\"MY_SNOWFLAKE\",\n            type=\"snowflake\",\n            account=\"my_account\",\n            warehouse=\"my_warehouse\",\n            database=\"my_database\",\n            schema=\"my_schema\",\n            username=\"my_user\",\n            password=\"my_password\",\n        ),\n    ]\n)\n\n@sling_assets(config=config)\ndef my_sling_assets(sling=sling):\n    yield from sling.replicate()\n\ndefs = Definitions(\n    assets=[my_sling_assets],\n    resources={\n        \"sling\": sling\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Peering to Multiple Airflow Instances with Dagster-Airlift\nDESCRIPTION: Code showing how to connect to multiple Airflow instances by calling build_defs_from_airflow_instance multiple times and combining them with Definitions.merge. Each instance is configured with its own authentication credentials.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/migration-reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\n\nfrom dagster_airlift.core import AirflowInstance, build_defs_from_airflow_instance\n\ndefs = Definitions.merge(\n    build_defs_from_airflow_instance(\n        airflow_instance=AirflowInstance(\n            auth_backend=BasicAuthBackend(\n                webserver_url=\"http://yourcompany.com/instance_one\",\n                username=\"admin\",\n                password=\"admin\",\n            ),\n            name=\"airflow_instance_one\",\n        )\n    ),\n    build_defs_from_airflow_instance(\n        airflow_instance=AirflowInstance(\n            auth_backend=BasicAuthBackend(\n                webserver_url=\"http://yourcompany.com/instance_two\",\n                username=\"admin\",\n                password=\"admin\",\n            ),\n            name=\"airflow_instance_two\",\n        )\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalComputeLogManager in YAML\nDESCRIPTION: Configuration for LocalComputeLogManager which writes stdout and stderr logs to disk.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster._core.storage.local_compute_log_manager\n  class: LocalComputeLogManager\n  config:\n    base_dir: \"/path/to/logs\"\n```\n\n----------------------------------------\n\nTITLE: Implementing IOManager for Unconnected Inputs\nDESCRIPTION: Demonstrates how to implement an IOManager to handle both inputs and outputs, including unconnected inputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/unconnected-inputs.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import IOManager, In, op, job\n\nclass Table1IOManager(IOManager):\n    def handle_output(self, context, obj):\n        # do something to store data\n        pass\n\n    def load_input(self, context):\n        # do something to load data\n        return [1, 2, 3]\n\n@op(ins={\"table1\": In(input_manager_key=\"io_manager\")})\ndef process_table1(table1):\n    return table1\n\n@job(resource_defs={\"io_manager\": Table1IOManager()})\ndef my_job():\n    process_table1()\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Check Evaluation in Dagster\nDESCRIPTION: Example demonstrating how to report an asset check evaluation using DagsterInstance API. Uses AssetCheckEvaluation to record the check result with an asset key and check name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/external-assets-instance-api.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DagsterInstance, AssetCheckEvaluation, AssetCheckKey\n\ninstance = DagsterInstance.get()\ninstance.report_runless_asset_event(\n  AssetCheckEvaluation(\n    asset_key=AssetKey(\"example_asset\"),\n    check_name=\"example_check\",\n    passed=True\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Job Behavior Based on Scheduled Run Time\nDESCRIPTION: This code defines a Dagster job that uses a schedule to vary its behavior based on the scheduled run time. It includes an op that prints the current time and a schedule that generates run requests with different configurations depending on the time of day.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/configuring-job-behavior.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@op\ndef hello_op(context: OpExecutionContext):\n    now = pendulum.now(\"America/New_York\")\n    context.log.info(f\"Hello from New York! It's {now}\")\n\n\n@job\ndef hello_job():\n    hello_op()\n\n\n@schedule(\n    job=hello_job,\n    cron_schedule=\"0 * * * *\",  # every hour\n)\ndef hello_schedule(context: ScheduleEvaluationContext):\n    scheduled_time = pendulum.from_timestamp(context.scheduled_execution_time, tz=\"America/New_York\")\n    if scheduled_time.hour < 12:\n        message = \"Good morning!\"\n    elif 12 <= scheduled_time.hour < 18:\n        message = \"Good afternoon!\"\n    else:\n        message = \"Good evening!\"\n\n    return RunRequest(\n        run_key=None,\n        run_config={\"ops\": {\"hello_op\": {\"config\": {\"message\": message}}}},\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating an Autoloaded Dagster Asset\nDESCRIPTION: Python code defining a new Dagster asset in the defs directory that will be automatically discovered and loaded by dg through the load_defs function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n\n@asset\ndef autoloaded_asset():\n    return 2\n\n```\n\n----------------------------------------\n\nTITLE: Applying Non-Isolated Run Tag in Dagster Asset Job Definition (Python)\nDESCRIPTION: This Python code snippet shows how to programmatically indicate that a run should be non-isolated by applying a specific run tag when defining an asset job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/run-isolation.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefine_asset_job(tags={\"dagster/isolation\": \"disabled\"}, ...)\n```\n\n----------------------------------------\n\nTITLE: Scaffolding a Dagster Asset with dg CLI\nDESCRIPTION: Command to scaffold a new Dagster asset named 'my_asset.py' in the 'defs/assets' directory using the dg CLI tool.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/dagster-definitions.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndg scaffold asset my_asset.py --output defs/assets\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagstermill Asset with Parameters\nDESCRIPTION: Shows how to add configuration options to a dagstermill asset, specifically adding a config field for the number of clusters in k-means clustering.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\niris_kmeans_notebook = define_dagstermill_asset(\n    name=\"iris_kmeans\",\n    notebook_path=\"iris_kmeans.ipynb\",\n    config_schema={\"k\": int},\n)\n```\n\n----------------------------------------\n\nTITLE: Installing required packages for Dagster and Snowflake\nDESCRIPTION: Bash command to install the necessary Python packages for running the example code, including Dagster, Dagster-Snowflake, and pandas.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-databases.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-snowflake pandas\n```\n\n----------------------------------------\n\nTITLE: Configuring a Job Using YAML\nDESCRIPTION: This YAML file example shows how to configure a job using a YAML file that can be passed to the Dagster CLI. The YAML structure specifies configuration for a specific op within the job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/run-configuration.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nops:\n  op_using_config:\n    config:\n      person_name: Alice\n```\n\n----------------------------------------\n\nTITLE: Customizing Code References in Dagster Assets\nDESCRIPTION: Shows how to manually add custom code references to asset definitions using the dagster/code_references metadata key, useful for domain-specific language implementations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(\n    metadata={\n        \"dagster/code_references\": {\n            \"file_path\": \"my_custom_path.py\",\n            \"start_line\": 1,\n            \"end_line\": 10,\n        }\n    }\n)\ndef my_asset():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Materialization to Local Dagster Webserver\nDESCRIPTION: Example of reporting an asset materialization to a locally running Dagster webserver using cURL. This is the simplest form of the API call using just the asset key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST localhost:3000/report_asset_materialization/{ASSET_KEY}\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Base Image for Webserver and Daemon\nDESCRIPTION: Dockerfile configuration for setting up the base Dagster image that will run the webserver and daemon processes. Installs required Dagster packages and sets up the environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/docker.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.10-slim\n\nRUN pip install \\\n    dagster \\\n    dagster-graphql \\\n    dagster-webserver \\\n    dagster-postgres \\\n    dagster-docker\n\n# Set $DAGSTER_HOME and copy dagster.yaml and workspace.yaml there\nENV DAGSTER_HOME=/opt/dagster/dagster_home/\n\nRUN mkdir -p $DAGSTER_HOME\n\nCOPY dagster.yaml workspace.yaml $DAGSTER_HOME\n\nWORKDIR $DAGSTER_HOME\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Table from Dagster Asset\nDESCRIPTION: Python code defining a Dagster asset that fetches the Iris dataset and stores it as a table in Snowflake using the configured I/O manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster-io-managers.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset\nfrom sklearn.datasets import load_iris\n\n@asset\ndef iris_dataset() -> pd.DataFrame:\n    iris = load_iris()\n    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    df.columns = [col.replace(\" (cm)\", \"\").replace(\" \", \"_\") for col in df.columns]\n    return df\n```\n\n----------------------------------------\n\nTITLE: Defining Assets with Dependencies in Python\nDESCRIPTION: Creates two assets with a dependency relationship, demonstrating how Dagster handles versioning across dependent assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(code_version=\"v2\")\ndef versioned_number():\n    return 1\n\n@asset\ndef multiplied_number(versioned_number):\n    return versioned_number * 5\n```\n\n----------------------------------------\n\nTITLE: Limiting Concurrent Runs by Unique Tag Value in YAML\nDESCRIPTION: This YAML snippet shows how to apply separate limits to each unique value of a run tag using the 'applyLimitPerUniqueValue' option. It's useful for limiting runs for each backfill independently.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  runs:\n    tag_concurrency_limits:\n      - key: 'dagster/backfill'\n        value:\n          applyLimitPerUniqueValue: true\n        limit: 10\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Dagster Asset with JavaScript Integration\nDESCRIPTION: Python code showing how to create a Dagster asset that runs a JavaScript script using PipesSubprocessClient.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/javascript-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster_pipes import PipesSubprocessClient\n\n@asset(required_resource_keys={\"pipes\"}, compute_kind=\"javascript\")\ndef tensorflow_model(context):\n    context.resources.pipes.run_python(\"node tensorflow/main.js\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Dagster Webserver with pip\nDESCRIPTION: This bash command installs the core Dagster package and the Dagster webserver using pip. It includes both the core programming model and the server that hosts Dagster's web UI for development and operation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Defining an Upstream Dagster Asset for a dbt Model\nDESCRIPTION: This snippet demonstrates how to define an existing Dagster asset as an upstream dependency of a dbt model. It creates an asset with the key 'upstream' that can be referenced in a dbt model.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n@asset(key=\"upstream\")\ndef upstream_asset():\n    # ... asset logic here\n    return pd.DataFrame({\"foo\": [1, 2, 3]})\n```\n\n----------------------------------------\n\nTITLE: Attaching Metadata to Outputs in Dagster Ops\nDESCRIPTION: This example shows how to attach metadata to op outputs using the Output object and the EventMetadata class for customizing the display of structured metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, Output, MetadataValue\n\n@op\ndef output_metadata():\n    return Output(\n        value=\"foo\",\n        metadata={\n            \"row_count\": 100,\n            \"full_data\": MetadataValue.json({\"a\": [1, 2, 3]}),\n            \"column_summary\": MetadataValue.md(\"# Column Summary\\n- a: int\\n- b: string\")\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Schedule Evaluation in Dagster YAML\nDESCRIPTION: This snippet illustrates the configuration for schedule evaluation in the Dagster instance YAML file. It shows how to enable parallel evaluation using threads and set the number of workers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nschedules:\n  use_threads: true\n  num_workers: 4\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Job Creation After Materializing a Dagster Asset\nDESCRIPTION: This command lists all Kubernetes jobs in the current namespace, allowing you to verify that a Dagster materialization job was created and completed successfully. It displays the job name, completion status, duration, and age.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/deploying-to-kubernetes.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get jobs\nNAME                                               COMPLETIONS   DURATION   AGE\ndagster-run-5ee8a0b3-7ca5-44e6-97a6-8f4bd86ee630   1/1           4s         11s\n```\n\n----------------------------------------\n\nTITLE: DBT with Snowflake Integration (Before Insights)\nDESCRIPTION: Basic configuration for using DBT with Snowflake in Dagster without Insights tracking. This code sets up DBT CLI resources and defines assets based on DBT models.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/snowflake.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_dbt import DbtCliResource, dbt_assets\n\n@dbt_assets\ndef my_dbt_assets():\n    return ...\n\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\n        \"dbt\": DbtCliResource(\n            project_dir=\"path/to/dbt/project\",\n            profiles_dir=\"path/to/profiles/dir\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Using S3Resource in Dagster Job\nDESCRIPTION: This example demonstrates how to use the S3Resource in a Dagster job to interact with AWS S3. It includes defining resources, ops, and a job that reads from and writes to S3 buckets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/s3.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, resource\nfrom dagster_aws.s3 import S3Resource\n\n@resource\ndef s3_resource():\n    return S3Resource()\n\n@op\ndef example_op(context):\n    s3 = context.resources.s3\n    # Read from S3\n    obj = s3.get_object(Bucket=\"my-bucket\", Key=\"my-key\")\n    file_content = obj[\"Body\"].read().decode(\"utf-8\")\n    context.log.info(f\"Content: {file_content}\")\n\n    # Write to S3\n    s3.put_object(Bucket=\"my-bucket\", Key=\"my-output-key\", Body=\"Hello, S3!\")\n\n@job(resource_defs={\"s3\": s3_resource})\ndef my_s3_job():\n    example_op()\n```\n\n----------------------------------------\n\nTITLE: Migrating from load_assets_from_dbt_manifest to dbt_assets in Python\nDESCRIPTION: Demonstrates how to replace the deprecated load_assets_from_dbt_manifest function with the new @dbt_assets decorator, DbtCliResource, and DagsterDbtTranslator. The new approach uses a decorator with a yield statement for streaming dbt CLI commands.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Before, using `load_assets_from_dbt_manifest`\nfrom dagster_dbt import load_assets_from_dbt_manifest\n\nmy_dbt_assets = load_assets_from_dbt_manifest(\n    manifest=manifest,\n    use_build_command=True,\n)\n\n# After, using `@dbt_assets`, `DbtCliResource`, and `DagsterDbtTranslator\nfrom dagster import AssetExecutionContext\nfrom dagster_dbt import dbt_assets, DbtCliResource\n\n@dbt_assets(manifest=manifest)\ndef my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):\n    yield from dbt.cli([\"build\"], context=context).stream()\n```\n\n----------------------------------------\n\nTITLE: Setting Per-Job Run Timeout with Tags\nDESCRIPTION: Python code demonstrating how to set a specific timeout of 10 seconds for a Dagster job using the dagster/max_runtime tag.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-monitoring.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef sleepy():\n    time.sleep(20)\n\n\n@job(tags={\"dagster/max_runtime\": \"10\"})\ndef sleepy_job():\n    sleepy()\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset with Resource Dependency in Dagster\nDESCRIPTION: Example showing how to define a Dagster asset that requires a resource dependency. The asset uses the '@asset' decorator and specifies required resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/using-resources.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/dg/using-resources/1-asset-one.py\" />\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Cloud CLI using pip\nDESCRIPTION: Installs the Dagster+ Agent library from PyPi using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-cloud-cli/installing-and-configuring.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-cloud\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery I/O Manager\nDESCRIPTION: Python code to set up and configure the BigQuery I/O manager for handling DataFrames\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_gcp_pandas import BigQueryPandasIOManager\n\ndefs = Definitions(\n    resources={\n        \"io_manager\": BigQueryPandasIOManager(\n            project=\"my-gcp-project\",\n            location=\"us-east5\",\n            dataset=\"IRIS\"\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Constructing Schedule for Time-Partitioned Op Job in Python\nDESCRIPTION: This snippet shows how to create a schedule for a time-partitioned op job using the build_schedule_from_partitioned_job function in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/constructing-schedules-for-partitioned-assets-and-jobs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@job(partitions_def=hourly_partitions)\ndef partitioned_op_job():\n    my_op()\n\npartitioned_op_schedule = build_schedule_from_partitioned_job(\n    job=partitioned_op_job,\n    name=\"partitioned_op_schedule\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sensors Configuration in YAML\nDESCRIPTION: Configuration for sensor evaluation settings in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsensors:\n  use_threads: true\n  num_workers: 8\n```\n\n----------------------------------------\n\nTITLE: Creating Asset Checks Using Factory Pattern in Python with Dagster\nDESCRIPTION: Demonstrates how to generate asset checks programmatically using a factory pattern with the @multi_asset_check decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/asset-checks.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, multi_asset_check\n\n@asset\ndef orders():\n    return pd.DataFrame({\"order_id\": [1, 2, 3], \"item_id\": [\"a\", \"b\", \"c\"]})\n\ndef create_null_check(column):\n    def _check_null(df):\n        null_count = df[column].isnull().sum()\n        if null_count > 0:\n            return AssetCheckResult(\n                success=False,\n                message=f\"{column} contains null values\",\n                metadata={\"null_count\": null_count},\n            )\n\n    return _check_null\n\n@multi_asset_check(assets=[orders])\ndef check_orders(context):\n    df = orders()\n    checks = [create_null_check(\"order_id\"), create_null_check(\"item_id\")]\n    for check in checks:\n        result = check(df)\n        if result:\n            yield result\n```\n\n----------------------------------------\n\nTITLE: Authenticating Snowflake I/O Manager with Private Key\nDESCRIPTION: Shows how to authenticate a Snowflake I/O manager using a private key, either directly or via a file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake import build_snowflake_io_manager\n\nsnowflake_io_manager = build_snowflake_io_manager(\n    account=\"account\",\n    user=\"user\",\n    private_key=\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\nMIIFHDBOBgkqhkiG9w0BBQ0wQTApBgkqhkiG9w0BBQwwHAQIxwnF5G/9/eQCAggA\\n...\n-----END ENCRYPTED PRIVATE KEY-----\",\n    private_key_password=\"password\",\n    warehouse=\"warehouse\",\n    database=\"database\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake import build_snowflake_io_manager\n\nsnowflake_io_manager = build_snowflake_io_manager(\n    account=\"account\",\n    user=\"user\",\n    private_key_path=\"/path/to/private/key/file.p8\",\n    private_key_password=\"password\",\n    warehouse=\"warehouse\",\n    database=\"database\",\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Definitions for dbt Integration\nDESCRIPTION: This Python code sets up the initial Dagster definitions for integrating with dbt. It creates a dbt resource and defines assets based on the dbt models.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_dbt import DbtCliResource, dbt_assets\n\ndbt_resource = DbtCliResource(project_dir=\"./basic-dbt-project\")\n\n@dbt_assets(manifest=dbt_resource.get_manifest())\ndef my_dbt_assets():\n    pass\n\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\"dbt\": dbt_resource}\n)\n```\n\n----------------------------------------\n\nTITLE: Composing Custom Automation Conditions\nDESCRIPTION: Example of combining custom automation conditions with built-in conditions using logical operators.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/arbitrary-python-automation-conditions.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dagster as dg\n\ncondition = AutomationCondition.eager() & ~IsCompanyHoliday()\n```\n\n----------------------------------------\n\nTITLE: Using Enum Types in Dagster Config\nDESCRIPTION: Shows how to use Python enum types in config schemas. This allows for config fields with a predefined set of valid values, providing type safety and validation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass UserRole(str, Enum):\n    ADMIN = \"ADMIN\"\n    GUEST = \"GUEST\"\n\nclass MyAssetConfig(Config):\n    users_list: Dict[str, UserRole]\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    config = MyAssetConfig(users_list={\"Bob\": UserRole.GUEST, \"Alice\": UserRole.ADMIN})\n    assert config.users_list[\"Bob\"] == UserRole.GUEST\n    assert config.users_list[\"Alice\"] == UserRole.ADMIN\n```\n\n----------------------------------------\n\nTITLE: Defining Modal App with Dependencies for Audio Transcription\nDESCRIPTION: Sets up a Modal application with necessary dependencies (OpenAI's Whisper, FFmpeg, and other Python packages) for audio processing. This creates the environment that Modal will use for running the transcription functions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/modal-application.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\napp = modal.App(\"transcribe\")\n\napp_image = modal.Image.debian_slim(python_version=\"3.10\").pip_install(\n    \"openai-whisper==20230314\",\n    \"boto3\",\n    \"pydub\",\n).apt_install(\"ffmpeg\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Run Failure Sensor in Python with Dagster\nDESCRIPTION: This example shows how to create a run failure sensor that triggers on failed job runs. It uses the @run_failure_sensor decorator to define the sensor function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-run-status-sensors.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@run_failure_sensor\ndef my_run_failure_sensor(context):\n    email_alert(f\"Job {context.dagster_run.job_name} failed\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Code Locations with Different Sources\nDESCRIPTION: Example showing how to define multiple code locations with different source types in dagster_cloud.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/dagster-cloud-yaml.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: data-eng-pipeline\n    code_source:\n      package_name: example_etl\n  - location_name: machine_learning\n    code_source:\n      python_file: ml/ml_model.py\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Job Executor in Python\nDESCRIPTION: Example of configuring a Dagster job to run each op in its own Kubernetes pod using k8s_job_executor.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/configuration.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job\nfrom dagster_k8s import k8s_job_executor\n\n@job(executor_def=k8s_job_executor)\ndef k8s_job():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Syncing and Materializing Fivetran Assets\nDESCRIPTION: Example showing how to sync Fivetran connectors and materialize connector tables using build_fivetran_assets_definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npath=\"docs_snippets/docs_snippets/integrations/fivetran/sync_and_materialize_fivetran_assets.py\"\n```\n\n----------------------------------------\n\nTITLE: Multiple I/O Managers Configuration in Dagster\nDESCRIPTION: Example of configuring multiple I/O managers for different storage backends (DuckDB and S3).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(io_manager_key=\"warehouse_io_manager\")\ndef iris_dataset() -> pd.DataFrame:\n    return pd.read_csv(\"iris.csv\")\n\n@asset(io_manager_key=\"blob_io_manager\")\ndef iris_plots(iris_dataset) -> None:\n    plt.scatter(iris_dataset[\"sepal_length\"], iris_dataset[\"sepal_width\"])\n\ndefs = Definitions(\n    assets=[iris_dataset, iris_plots],\n    resources={\n        \"warehouse_io_manager\": DuckDBIOManager(database=\"example.duckdb\"),\n        \"blob_io_manager\": S3IOManager(s3_bucket=\"my-bucket\"),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sensors in Dagster ETL Project\nDESCRIPTION: This file contains the job and sensor definitions for the adhoc_request asset, which likely monitors for new data requests and triggers processing when detected.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/refactor-your-project.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Code placeholder for sensors.py\n# The actual implementation would be shown in the CodeExample component\n```\n\n----------------------------------------\n\nTITLE: Storing Time-Partitioned Assets in Snowflake\nDESCRIPTION: Shows how to store time-partitioned assets in Snowflake using partition_expr metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, DailyPartitionsDefinition\n\npartitions = DailyPartitionsDefinition(start_date=\"2023-01-01\")\n\n@asset(\n    partitions_def=partitions,\n    metadata={\"partition_expr\": \"TO_TIMESTAMP(TIME::INT)\"}\n)\ndef iris_dataset():\n    return get_iris_data()\n\n@asset(partitions_def=partitions)\ndef daily_averages(iris_dataset):\n    return iris_dataset.mean()\n```\n\n----------------------------------------\n\nTITLE: Creating Products Asset in Dagster\nDESCRIPTION: Defines a products asset that loads data from a CSV file into a DuckDB table. The asset includes metadata for categorization and preview in the Dagster UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(group_name=\"bronze\", compute_kind=\"duckdb\")\ndef products(duckdb: DuckDBResource):\n    \"\"\"Table that holds the products sold by the company\"\"\"\n\n    # First, create the table from the CSV data.\n    duckdb.execute_query(\n        '''\n        CREATE OR REPLACE TABLE products AS\n        SELECT *\n        FROM read_csv_auto('data/products.csv', header=True)\n        '''\n    )\n\n    # Next, let's return a preview of the table to show in the UI\n    product_df = duckdb.execute_query(\"SELECT * FROM products\").fetch_df()\n    num_records = len(product_df)\n\n    return dg.MaterializeResult(\n        metadata={\n            \"num_records\": num_records,\n            \"preview\": dg.MetadataValue.md(product_df.head().to_markdown()),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Processing Complete Audio Episodes into Segments\nDESCRIPTION: Creates a Modal function that splits audio files into smaller segments for parallel processing, applies the transcribe_segment function to each piece, and writes the resulting transcriptions to JSON files in the R2 bucket.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/modal-application.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@app.function(\n    image=app_image,\n    mounts=[r2_bucket],\n    cpu=4,\n)\ndef transcribe_episode(episode_info):\n    from pydub import AudioSegment\n    import json\n\n    audio_path = episode_info[\"file_path\"]\n    print(f\"Transcribing {audio_path}\")\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_path)\n\n    # Split into 10-minute segments\n    segment_length = 10 * 60 * 1000  # 10 minutes in milliseconds\n    segments = [audio[i : i + segment_length] for i in range(0, len(audio), segment_length)]\n\n    # Save segments to temporary files\n    segment_paths = []\n    for i, segment in enumerate(segments):\n        segment_path = f\"/tmp/segment_{i}.mp3\"\n        segment.export(segment_path, format=\"mp3\")\n        segment_paths.append(segment_path)\n\n    # Transcribe each segment in parallel\n    transcribe_outputs = []\n    for segment_path in segment_paths:\n        output = transcribe_segment.remote(segment_path)\n        transcribe_outputs.append(output)\n\n    # Combine transcriptions and write to file\n    full_transcript = \" \".join(transcribe_outputs)\n\n    # Save transcript to the R2 bucket\n    transcript_dir = os.path.join(\n        os.path.dirname(audio_path), \"transcripts\", os.path.basename(audio_path).split(\".\")[0]\n    )\n    os.makedirs(transcript_dir, exist_ok=True)\n\n    transcript_file = os.path.join(transcript_dir, \"transcript.json\")\n    with open(transcript_file, \"w\") as f:\n        json.dump({\"transcript\": full_transcript}, f)\n\n    return {\"transcript\": full_transcript, \"transcript_file\": transcript_file}\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Logger to Ops-Based Job in Dagster\nDESCRIPTION: Example showing how to add a JSON console logger to an ops-based job definition in Dagster. This demonstrates the configuration approach for ops jobs, which differs slightly from asset job configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/custom-logging.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, job, op, OpExecutionContext, json_console_logger\n\n@op\ndef get_hackernews_topstory_ids(context: OpExecutionContext):\n    import requests\n    \n    newstories_url = \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n    top_story_ids = requests.get(newstories_url).json()\n    context.log.info(f\"Got {len(top_story_ids)} top stories.\")\n    return top_story_ids\n\n@job\ndef hackernews_topstory_ids_job():\n    get_hackernews_topstory_ids()\n\n# Add the custom logger when defining Dagster definitions\ndefs = Definitions(\n    jobs=[hackernews_topstory_ids_job],\n    loggers={\"console\": json_console_logger}\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring EcsRunLauncher for EC2 Launch Type\nDESCRIPTION: This YAML configuration customizes the EcsRunLauncher to launch new Dagster runs in EC2 from a task running in Fargate. It uses the run_task_kwargs field to specify the launch type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: 'dagster_aws.ecs'\n  class: 'EcsRunLauncher'\n  config:\n    run_task_kwargs:\n      launchType: 'EC2'\n```\n\n----------------------------------------\n\nTITLE: Implementing S3-based I/O Manager for Dagster\nDESCRIPTION: This Python code defines a resource that uses the s3_pickle_io_manager from dagster_aws to manage I/O operations using an S3 bucket. It's used to enable parallel computation and persistent storage between ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_aws.s3 import s3_pickle_io_manager\nfrom dagster import job, op, resource\n\n@resource(config_schema={\"bucket\": str})\ndef s3_io_manager(init_context):\n    return s3_pickle_io_manager.configured({\"s3_bucket\": init_context.resource_config[\"bucket\"]})\n\n@op\ndef my_op():\n    return 1\n\n@job(resource_defs={\"io_manager\": s3_io_manager})\ndef my_s3_job():\n    my_op()\n```\n\n----------------------------------------\n\nTITLE: Syncing Alert Policies using Dagster+ CLI\nDESCRIPTION: This command syncs an alert policy configuration file to a Dagster+ deployment using the dagster-cloud CLI. It requires a path to a YAML configuration file that defines the alert policies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/alerts/creating-alerts.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud deployment alert-policies sync -a /path/to/alert_policies.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using GCS Resources in Dagster\nDESCRIPTION: Example of defining GCS resources, I/O managers, and assets in a Dagster project. Demonstrates reading from and writing to GCS buckets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/gcs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\nfrom dagster_gcp import GCSResource, ConfigurableGCSIOManager\n\n@asset\ndef my_gcs_asset(gcs: GCSResource):\n    # Use the GCS resource to interact with GCS\n    bucket = gcs.get_bucket(\"my-bucket\")\n    blob = bucket.blob(\"my-object\")\n    blob.upload_from_string(\"Hello, GCS!\")\n\n@asset\ndef read_from_gcs(gcs_io_manager):\n    # This asset will be automatically stored in GCS\n    return \"Data to be stored in GCS\"\n\ndefs = Definitions(\n    assets=[my_gcs_asset, read_from_gcs],\n    resources={\n        \"gcs\": GCSResource(project=\"my-project\"),\n        \"gcs_io_manager\": ConfigurableGCSIOManager(\n            bucket=\"my-bucket\",\n            prefix=\"my-prefix\"\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring QueuedRunCoordinator in YAML\nDESCRIPTION: Configuration for QueuedRunCoordinator which allows setting limits on concurrent run execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nrun_coordinator:\n  module: dagster._core.run_coordinator\n  class: QueuedRunCoordinator\n  config:\n    max_concurrent_runs: 25\n    tag_concurrency_limits:\n      - key: \"slack\"\n        value: \"*\"\n        limit: 1\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Server in Workspace YAML\nDESCRIPTION: YAML configuration to load a code location from a custom gRPC server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# workspace.yaml\n\nload_from:\n  - grpc_server:\n      host: localhost\n      port: 4266\n      location_name: 'my_grpc_server'\n```\n\n----------------------------------------\n\nTITLE: Accessing Op Context for Logging in Python\nDESCRIPTION: This example shows how to access the op context to use the logger for logging an info message within an op function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef hello_logs(context):\n    context.log.info(\"Hello, world!\")\n    return 1\n```\n\n----------------------------------------\n\nTITLE: Defining Dagstermill Asset from Jupyter Notebook\nDESCRIPTION: Creates a Dagster asset from a Jupyter notebook using define_dagstermill_asset. Specifies the asset name and notebook path to execute the notebook as part of a data pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagstermill import define_dagstermill_asset\n\niris_kmeans_notebook = define_dagstermill_asset(\n    name=\"iris_kmeans\",\n    notebook_path=\"iris_kmeans.ipynb\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining DLT Source Example in Python\nDESCRIPTION: Example of defining a dlt source with two resources (courses and users) using decorators and API requests.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dlt.source\ndef example(api_key: str = dlt.secrets.value):\n    @dlt.resource(primary_key=\"id\", write_disposition=\"merge\")\n    def courses():\n        response = requests.get(url=BASE_URL + \"courses\")\n        response.raise_for_status()\n        yield response.json().get(\"items\")\n\n    @dlt.resource(primary_key=\"id\", write_disposition=\"merge\")\n    def users():\n        for page in _paginate(BASE_URL + \"users\"):\n            yield page\n\n    return courses, users\n```\n\n----------------------------------------\n\nTITLE: Adding Column Lineage Metadata to Dagster Assets in Python\nDESCRIPTION: This snippet shows how to add column lineage metadata to a Dagster asset. It uses the TableColumnLineage class to define the lineage between input and output columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, TableColumnLineage\n\n@asset(\n    metadata={\n        \"dagster/column_lineage\": TableColumnLineage(\n            input_columns=[\"users.id\", \"users.name\", \"orders.id\", \"orders.user_id\", \"orders.total\"],\n            output_columns=[\n                {\"name\": \"user_id\", \"inputs\": [\"users.id\"]},\n                {\"name\": \"user_name\", \"inputs\": [\"users.name\"]},\n                {\"name\": \"total_spent\", \"inputs\": [\"orders.total\"]},\n            ]\n        )\n    }\n)\ndef user_order_summary():\n    ...\n\n```\n\n----------------------------------------\n\nTITLE: Dagster Asset Definition for Lambda Integration\nDESCRIPTION: Python code defining a Dagster asset that invokes the AWS Lambda function using PipesLambdaClient.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-lambda-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef lambda_pipes_asset(context: AssetExecutionContext):\n    result = PipesLambdaClient(\n        context=context,\n        function_name=\"dagster_pipes_function\",\n        event={\"input_data\": \"sample_input\"},\n    ).run()\n    return result.get_materialize_result()\n```\n\n----------------------------------------\n\nTITLE: Defining PowerBI Dashboard Assets in Dagster\nDESCRIPTION: Creates the definition for PowerBI dashboard assets and includes the PowerBI resource. This connects the dashboard layer to the rest of the data pipeline and ensures updates propagate through the entire system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/dashboard.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndashboard_def = Definitions(\n    assets=load_assets_from_powerbi_workspace(\n        powerbi_workspace=powerbi_workspace,\n        translator=powerbi_translator,\n    ),\n    resources={\n        \"powerbi\": powerbi_workspace,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Webserver with pip\nDESCRIPTION: This bash command installs Dagster and the Dagster webserver using pip, the Python package installer.  `dagster` provides the core programming model, while `dagster-webserver` hosts the web UI for development and operation. This installation command requires Python and pip to be installed and configured.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n```bash\npip install dagster dagster-webserver\n```\n```\n\n----------------------------------------\n\nTITLE: Loading Definitions from Module\nDESCRIPTION: Function to load Dagster definitions from a specified module.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/definitions.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nload_definitions_from_module\n```\n\n----------------------------------------\n\nTITLE: Creating Dagstermill Op from Jupyter Notebook\nDESCRIPTION: Demonstrates creating a Dagster op from a Jupyter notebook using define_dagstermill_op, including output notebook configuration and job definition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nk_means_iris = define_dagstermill_op(\n    \"k_means_iris\",\n    notebook_path=\"iris_kmeans.ipynb\",\n    output_notebook_name=\"iris_kmeans_output\",\n)\n\n@job(\n    resource_defs={\n        \"output_notebook_io_manager\": ConfigurableLocalOutputNotebookIOManager(),\n    }\n)\ndef iris_classify():\n    k_means_iris()\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Delta Lake Asset\nDESCRIPTION: Example of defining a Dagster asset that stores data in Delta Lake using the Iris dataset\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/using-deltalake-with-dagster.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef iris_dataset() -> pd.DataFrame:\n    \"\"\"The iris dataset cleaned for use.\"\"\"\n    raw_data = pd.read_csv(\n        \"https://raw.githubusercontent.com/plotly/datasets/master/iris-data.csv\"\n    )\n    # Rename the first four columns\n    raw_data.columns = [\n        \"sepal_length\",\n        \"sepal_width\",\n        \"petal_length\",\n        \"petal_width\",\n        \"species\",\n    ]\n    return raw_data\n```\n\n----------------------------------------\n\nTITLE: Using dagster-cloud CLI to Set Deployment Settings\nDESCRIPTION: Shell command demonstrating how to use the dagster-cloud CLI to upload and apply deployment settings from a YAML file. This command replaces all configured settings with those specified in the file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/managing-deployments.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud deployment settings set-from-file my-settings.yaml\n```\n\n----------------------------------------\n\nTITLE: Exposing Component Types in Package __init__.py\nDESCRIPTION: This Python code shows how to expose a custom component type in the package's __init__.py file, making it visible to the 'dg' plugin system. It imports and re-exports the EmptyComponent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-dg-plugin.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Import and re-export EmptyComponent to expose it via the entry point\nfrom my_library.empty_component import EmptyComponent\n```\n\n----------------------------------------\n\nTITLE: Group-based Asset Selection in Python\nDESCRIPTION: Creates a job selecting assets between sensitive_data and public_data groups\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsensitive_to_public_asset_job = define_asset_job(\n    name=\"sensitive_to_public_asset_job\", selection='group:\"sensitive_data\"+ and +group:\"public_data\"'\n)\n```\n\n----------------------------------------\n\nTITLE: Using Input Managers with Subselection\nDESCRIPTION: Demonstrates how to use input managers to control input loading when executing a subset of ops in a job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/unconnected-inputs.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import IOManager, In, op, job\n\nclass MyIOManager(IOManager):\n    def handle_output(self, context, obj):\n        # Store output\n        pass\n\n    def load_input(self, context):\n        if context.upstream_output is None:\n            # Load table_1 when there's no upstream output\n            return [1, 2, 3]\n        else:\n            # Load from upstream output\n            return context.upstream_output\n\n@op\ndef op1():\n    return [4, 5, 6]\n\n@op(ins={\"table_1\": In(input_manager_key=\"io_manager\")})\ndef op2(table_1):\n    return table_1\n\n@job(resource_defs={\"io_manager\": MyIOManager()})\ndef my_job():\n    op2(op1())\n\n# Execute full job\nresult = my_job.execute_in_process()\n\n# Execute subset (only op2)\nresult = my_job.execute_in_process(op_selection=[\"op2\"])\n```\n\n----------------------------------------\n\nTITLE: Scaffolding Dagster Project from dbt Project using CLI\nDESCRIPTION: Command to create a new Dagster project that wraps an existing dbt project using the dagster-dbt CLI tool. This generates a directory with files defining a Dagster project that loads the specified dbt project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster-dbt project scaffold --project-name project_dagster --dbt-project-dir path/to/dbt/project\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom DBT Project Component in Python for Dagster\nDESCRIPTION: This code snippet demonstrates how to create a custom DBT project component in Python for a Dagster project. It uses the @component decorator and returns an instance of DbtProjectComponent with a custom translator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/adding-components-python.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import component\nfrom dagster_dbt import DbtProjectComponent\n\n@component\ndef my_dbt_project():\n    return DbtProjectComponent(\n        project_dir=\"path/to/dbt/project\",\n        manifest_path=\"path/to/manifest.json\",\n        translator={\n            \"my_model\": \"my_materialized_model\",\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing OSS Code References with Git Integration\nDESCRIPTION: Demonstrates manual implementation of code references for open source Dagster installations, supporting GitHub and GitLab repositories with customizable file path mapping.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, load_assets_from_modules\nfrom . import assets\n\ndefs = Definitions(\n    assets=load_assets_from_modules([assets]),\n    link_code_references_to_git={\n        \"repo_url\": \"https://github.com/my-org/my-repo\",\n        \"branch\": \"main\",\n        \"repo_root\": \"/path/to/repo/root\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Slack Alert on Job Success in Dagster\nDESCRIPTION: This example shows a sensor that reports job success by sending a Slack message. It uses the run_status_sensor decorator to trigger on successful runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/run-status-sensors.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@run_status_sensor(run_status=DagsterRunStatus.SUCCESS)\ndef job_success_sensor(context):\n    message = f\"Job {context.dagster_run.job_name} completed successfully\"\n    slack_client.chat_postMessage(channel=\"#dagster\", text=message)\n```\n\n----------------------------------------\n\nTITLE: Customizing Asset Sensor Evaluation Function in Python\nDESCRIPTION: This example shows how to create an asset sensor with a custom evaluation function. It checks for specific metadata in the asset materialization event and triggers a run or skips based on the evaluation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/asset-sensors.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, RunRequest, SkipReason, asset_sensor\n\n@asset_sensor(asset_key=AssetKey(\"my_table\"))\ndef my_table_sensor(context, asset_event):\n    table_metadata = asset_event.metadata.get(\"table_metadata\")\n    if table_metadata and table_metadata.get(\"row_count\", 0) > 100:\n        return RunRequest(\n            run_key=None,\n            run_config={\"ops\": {\"process_table\": {\"config\": {\"table_name\": \"my_table\"}}}},\n        )\n    return SkipReason(f\"The table only has {table_metadata.get('row_count', 0)} rows\")\n```\n\n----------------------------------------\n\nTITLE: Telemetry Configuration in YAML\nDESCRIPTION: Configuration for disabling telemetry collection in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ntelemetry:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring AutomationConditionSensorDefinition with User Code Server\nDESCRIPTION: Setup code to enable execution of arbitrary Python code in automation conditions by configuring the sensor to run on the user code server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/arbitrary-python-automation-conditions.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagster as dg\n\ndefs = dg.Definitions(\n  sensors=[dg.AutomationConditionSensorDefinition(\"automation_condition_sensor\", target=dg.AssetSelection.all(), use_user_code_server=True)]\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Census Sync with Dagster Pipeline in Python\nDESCRIPTION: This code snippet demonstrates how to use the CensusResource to execute a Census sync within a Dagster pipeline. It includes setting up the resource, defining an op that uses the resource to trigger a sync, and creating a job that utilizes this op.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/census.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, resource\nfrom dagster_census import census_resource\n\n@resource(config_schema={\"api_key\": str})\ndef my_census_resource(context):\n    return census_resource(api_key=context.resource_config[\"api_key\"])\n\n@op(required_resource_keys={\"census\"})\ndef census_sync(context):\n    sync_id = \"YOUR_SYNC_ID\"\n    context.resources.census.sync_and_poll(sync_id)\n\n@job\ndef my_census_job():\n    census_sync()\n```\n\n----------------------------------------\n\nTITLE: Creating Basic API Resource in Dagster\nDESCRIPTION: Defines a basic Dagster resource that connects to a sunrise API with hardcoded location for San Francisco International Airport\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-apis.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ConfigurableResource\nimport requests\n\nclass SunResource(ConfigurableResource):\n    def get_sunrise(self):\n        response = requests.get(\n            \"https://api.sunrise-sunset.org/json\",\n            params={\n                \"lat\": \"37.6213\",\n                \"lng\": \"-122.3790\",\n                \"formatted\": 0,\n            },\n        )\n        response.raise_for_status()\n        return response.json()[\"results\"][\"sunrise\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Storage in Dagster YAML\nDESCRIPTION: Example configuration for using MySQL storage in the dagster.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nstorage:\n  mysql:\n    mysql_db:\n      username: dagster\n      password: \\{env:\\{env_var: MYSQL_PASSWORD\\}\\}\n      hostname: mysql\n      db_name: dagster\n      port: 3306\n```\n\n----------------------------------------\n\nTITLE: Downstream DuckDB Asset Processing\nDESCRIPTION: Example of creating a downstream asset that loads data from a DuckDB table, processes it, and stores the result in a new table. Shows how to handle dependencies between assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef iris_setosa(iris_dataset: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Only the examples of the iris dataset that are of the species setosa.\"\"\"\n    return iris_dataset[iris_dataset[\"species\"] == 0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Dagster Pipes in AWS Glue job script\nDESCRIPTION: Add dagster-pipes to the Glue job script by calling open_dagster_pipes and using the context to send messages to Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-glue-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import open_dagster_pipes\n\ndef my_glue_job():\n    with open_dagster_pipes() as context:\n        # Your existing job logic here\n        context.log.info(\"Starting job\")\n        context.log_asset_materialization(\n            \"my_asset\",\n            description=\"Asset created by Glue job\",\n            metadata={\n                \"row_count\": {\"raw_value\": 1000, \"type\": \"python.int\"},\n                \"columns\": {\"raw_value\": [\"col1\", \"col2\"], \"type\": \"python.list\"},\n            },\n        )\n        context.log.info(\"Job complete\")\n\nif __name__ == \"__main__\":\n    my_glue_job()\n```\n\n----------------------------------------\n\nTITLE: Implementing Logging in Dagster Sensor\nDESCRIPTION: Example showing how to emit log messages during sensor evaluation. The sensor checks a predetermined directory path and logs status messages about the files found.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/logging-in-sensors.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@sensor(job=my_job)\ndef my_sensor(context):\n    directory = \"/path/to/directory\"\n    files = os.listdir(directory)\n    \n    if not files:\n        context.log.info(f\"No files found in {directory}\")\n        return SkipReason(f\"No files found in {directory}\")\n    \n    context.log.info(f\"Found {len(files)} files in {directory}\")\n    return RunRequest(\n        run_key=str(len(files)),\n        run_config={\"ops\": {\"process_files\": {\"config\": {\"files\": files}}}},\n    )\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Materialization in Dagster\nDESCRIPTION: Example showing how to report an external asset materialization using DagsterInstance API. Uses AssetMaterialization and AssetKey classes to record the materialization event.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/external-assets-instance-api.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DagsterInstance, AssetMaterialization, AssetKey\n\ninstance = DagsterInstance.get()\ninstance.report_runless_asset_event(AssetMaterialization(AssetKey(\"example_asset\")))\n```\n\n----------------------------------------\n\nTITLE: Adding Private Package to setup.py (Python)\nDESCRIPTION: This snippet shows how to add a private package name to the install_requires section in a Dagster project's setup.py file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ninstall_requires=[\n    \"dagster\",\n    \"dagster-cloud\",\n    \"private-package\",   # add this line - must match your private Python package name\n```\n\n----------------------------------------\n\nTITLE: Initial Dagster Definitions File\nDESCRIPTION: Original Python code that defines a single Dagster asset and exposes it in a top-level Definitions object, representing the starting point before migrating to dg.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\n\n\n@asset\ndef my_asset():\n    return 1\n\n\ndefs = Definitions(assets=[my_asset])\n\n```\n\n----------------------------------------\n\nTITLE: Configuring EcsRunLauncher for Fargate Spot Instances\nDESCRIPTION: This YAML configuration sets up the EcsRunLauncher to use Fargate Spot instances for Dagster runs. It specifies the capacity provider strategy in the run_task_kwargs field.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: 'dagster_aws.ecs'\n  class: 'EcsRunLauncher'\n  config:\n    run_task_kwargs:\n      capacityProviderStrategy:\n        - capacityProvider: 'FARGATE_SPOT'\n```\n\n----------------------------------------\n\nTITLE: Downstream Asset PySpark Script\nDESCRIPTION: External PySpark script for the downstream asset implementation using Dagster Pipes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/migrating-from-step-launchers-to-pipes.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport sys\nfrom dagster_pipes import create_client\nfrom pyspark.sql import SparkSession\n\nwith create_client() as client:\n    spark = SparkSession.builder.getOrCreate()\n    extras = json.loads(sys.argv[sys.argv.index(\"--extras\") + 1])\n    df = spark.read.parquet(extras[\"path\"])\n    df1 = df.where(\"a = 1\")\n    df2 = df.where(\"a = 2\")\n    result = {\"output_1\": df1.count(), \"output_2\": df2.count()}\n    client.log_event(\"result\", result)\n```\n\n----------------------------------------\n\nTITLE: Using BigQueryPySparkIOManager with Custom SparkSession in Dagster\nDESCRIPTION: Shows how to use the BigQueryPySparkIOManager with a custom SparkSession in Dagster. This approach allows for more control over the SparkSession configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Definitions\nfrom dagster_gcp_pyspark import BigQueryPySparkIOManager\nfrom pyspark.sql import SparkSession\n\n@asset\ndef my_asset():\n    spark = (\n        SparkSession.builder.appName(\"MyApp\")\n        .config(\n            \"spark.jars\",\n            \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\",\n        )\n        .config(\n            \"spark.hadoop.google.cloud.auth.service.account.enable\",\n            \"true\",\n        )\n        .config(\n            \"spark.hadoop.google.cloud.auth.service.account.json.keyfile\",\n            \"/path/to/keyfile.json\",\n        )\n        .getOrCreate()\n    )\n    # Your asset logic here\n    ...\n\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"io_manager\": BigQueryPySparkIOManager(\n            project=\"my-project\", dataset=\"my_dataset\"\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Using SecretsManagerResource in Dagster Jobs with AWS Secrets Manager\nDESCRIPTION: This example demonstrates how to configure and use the SecretsManagerResource in Dagster jobs to retrieve secrets from AWS Secrets Manager. It includes setting up the resource, defining a job that uses the secret, and executing the job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/secretsmanager.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op\nfrom dagster_aws.secretsmanager import SecretsManagerResource\n\n@op\ndef secret_op(context):\n    secret = context.resources.secrets_manager.get_secret_value(\"my-secret-name\")\n    context.log.info(f\"The secret is: {secret}\")\n\n@job\ndef my_job():\n    secret_op()\n\nmy_job.execute_in_process(\n    resources={\n        \"secrets_manager\": SecretsManagerResource(\n            region_name=\"us-east-1\",\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Failures in Dagster Ops\nDESCRIPTION: This snippet shows how to use the Failure exception in a Dagster op. The Failure exception includes structured metadata and can be used to control retry behavior.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef failure_op():\n    raise Failure(\n        description=\"This op failed\",\n        metadata={\n            \"error_message\": MetadataValue.text(\"The op failed because of X\"),\n            \"error_stack\": MetadataValue.text(\"The full error stack\"),\n            \"url\": MetadataValue.url(\"http://mycoolsite.com/url_for_clicking.html\"),\n            \"json\": MetadataValue.json({\"key\": \"value\", \"nested_key\": {\"key\": \"value\"}}),\n        },\n        allow_retries=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring K8sRunLauncher in YAML\nDESCRIPTION: Configuration for K8sRunLauncher which allocates a Kubernetes job per run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: dagster_k8s\n  class: K8sRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Docker Compose for Dagster-PySpark-MinIO Setup\nDESCRIPTION: This Docker Compose file sets up a local environment with Dagster, PySpark, and MinIO for simulating S3 storage. It configures networking and volumes for the services.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/pyspark-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3'\nservices:\n  dagster:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - ./workspace.yaml:/opt/dagster/app/workspace.yaml\n    command: dagster dev -h 0.0.0.0 -p 3000\n    depends_on:\n      - minio\n  minio:\n    image: minio/minio\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    volumes:\n      - minio_storage:/data\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin\n    command: server /data --console-address \":9001\"\n\nvolumes:\n  minio_storage: {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nested Graphs in Dagster (Python)\nDESCRIPTION: This snippet demonstrates how to configure ops inside a nested graph and create a job that uses the nested graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Field, Int, job, op, graph\n\n@op(config_schema={\"n\": Field(Int)})\ndef add_n(context, number):\n    return number + context.op_config[\"n\"]\n\n@op(config_schema={\"m\": Field(Int)})\ndef multiply_by_m(context, number):\n    return number * context.op_config[\"m\"]\n\n@graph\ndef add_and_multiply(num):\n    return multiply_by_m(add_n(num))\n\n@job\ndef do_math():\n    add_and_multiply()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dask Executor for Distributed Execution in YAML\nDESCRIPTION: This YAML configuration block sets up the Dask executor for distributed execution. It specifies the Dask scheduler address and port, along with S3 configuration for the IO manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/dask.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexecution:\n  config:\n    cluster:\n      existing:\n        address: 172.17.0.2:8786\nresources:\n  io_manager:\n    config:\n      s3_bucket: my-s3-bucket\n  s3:\n    config:\n      region_name: us-east-1\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Assets for Different Environments\nDESCRIPTION: Sets up a Dagster repository with different I/O managers for production and branch deployment environments, using environment variables to determine the current context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, load_assets_from_modules\nfrom dagster_snowflake import SnowflakeIOManager\nimport os\n\nfrom .assets import assets\n\nPRODUCTION_SNOWFLAKE_CONFIG = {\n    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n    \"database\": \"PRODUCTION\",\n    \"schema\": \"HACKER_NEWS\",\n}\n\nBRANCH_SNOWFLAKE_CONFIG = {\n    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n    \"database\": f\"PRODUCTION_CLONE_{os.getenv('DAGSTER_CLOUD_PULL_REQUEST_ID')}\",\n    \"schema\": \"HACKER_NEWS\",\n}\n\ndefs = Definitions(\n    assets=load_assets_from_modules([assets]),\n    resources={\n        \"snowflake_io_manager\": SnowflakeIOManager(\n            BRANCH_SNOWFLAKE_CONFIG if os.getenv(\"DAGSTER_CLOUD_IS_BRANCH_DEPLOYMENT\") else PRODUCTION_SNOWFLAKE_CONFIG\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Kinds for Multi-Assets using AssetSpec\nDESCRIPTION: This example shows how to specify kinds for multi-assets using the AssetSpec class in Dagster. It demonstrates setting kinds for multiple assets within a single definition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/kind-tags.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@multi_asset(\n    outs={\n        \"asset_1\": AssetSpec(kinds=[\"python\", \"snowflake\"]),\n        \"asset_2\": AssetSpec(kinds=[\"python\", \"postgres\"]),\n    }\n)\ndef my_multi_asset():\n    # Multi-asset definition\n```\n\n----------------------------------------\n\nTITLE: Representing Fivetran Assets\nDESCRIPTION: Code to load Fivetran connector tables as asset specs in the Dagster asset graph using the FivetranWorkspace resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npath=\"docs_snippets/docs_snippets/integrations/fivetran/representing_fivetran_assets.py\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Partitioned Assets with Delta Lake I/O Manager in Python\nDESCRIPTION: Illustrates how to set up a multi-partitioned asset for storage in Delta Lake by specifying multiple partition expressions in metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    metadata={\n        \"partition_expr\": {\n            \"time\": \"time\",\n            \"species\": \"species\"\n        }\n    }\n)\ndef iris_by_time_and_species(context: OpExecutionContext):\n    # Asset implementation\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Python Code References in Dagster\nDESCRIPTION: Demonstrates how to configure Dagster to automatically attach code references to assets during local development using the enable_local_code_references function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, load_assets_from_modules\nfrom . import assets\n\ndefs = Definitions(\n    assets=load_assets_from_modules([assets]),\n    enable_local_code_references=True  # Enable automatic code references\n)\n```\n\n----------------------------------------\n\nTITLE: OpenAI Summary Generation in Python\nDESCRIPTION: Creates summaries of transcribed text using OpenAI and stores results in Dagster's catalog. Includes materialization of summary text and R2 storage keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/rss-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstart_summary\n```\n\n----------------------------------------\n\nTITLE: Yielding Output Objects in Dagster Ops\nDESCRIPTION: This snippet demonstrates how to yield Output objects in Dagster ops when type annotations cannot be used, specifying type information using the 'out' argument of the op decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@op(out=Out(str))\ndef yield_str():\n    yield Output(\"foo\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Freshness Check for External Assets in Python\nDESCRIPTION: This example shows how to define a freshness check for external assets in Dagster. It includes setting up a schedule to run the check periodically and demonstrates how to emit update timestamps for external assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/data-freshness-testing.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSelection, Definitions, asset, materialize, define_asset_job, build_schedule_from_partitioned_job\nfrom dagster.core.asset_checks import build_last_update_freshness_checks\nfrom dagster_plus.asset_checks import build_sensor_for_freshness_checks\nfrom datetime import timedelta\n\n@asset(group_name=\"external\")\ndef external_asset():\n    ...\n\nfreshness_check = build_last_update_freshness_checks(\n    AssetSelection.groups(\"external\"),\n    maximum_lag=timedelta(hours=1),\n)\n\ncheck_job = define_asset_job(\"freshness_check_job\", selection=AssetSelection.groups(\"external\"))\ncheck_schedule = build_schedule_from_partitioned_job(check_job, hour_of_day=12)\n\ndefs = Definitions(\n    assets=[external_asset],\n    asset_checks=[freshness_check],\n    schedules=[check_schedule],\n)\n\n# In the job that updates the external asset:\nwith materialize(external_asset) as mat:\n    mat.add_metadata_entry(\n        key=\"dagster/last_updated_timestamp\",\n        value=datetime.now().timestamp(),\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining a Single Code Location in dagster_cloud.yaml\nDESCRIPTION: Example of a simple dagster_cloud.yaml configuration with a single code location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/dagster-cloud-yaml.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: data-eng-pipeline\n    code_source:\n      package_name: example_etl\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud SQL for Dagster in YAML\nDESCRIPTION: This YAML configuration sets up Dagster to use Cloud SQL for run and event log storage. It specifies the storage type as PostgreSQL and provides connection details for the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/gcp.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrun_storage:\n  module: dagster_postgres.run_storage\n  class: PostgresRunStorage\n  config:\n    postgres_db:\n      hostname: REDACTED\n      username: REDACTED\n      password: REDACTED\n      db_name: REDACTED\n      port: 5432\n\nevent_log_storage:\n  module: dagster_postgres.event_log\n  class: PostgresEventLogStorage\n  config:\n    postgres_db:\n      hostname: REDACTED\n      username: REDACTED\n      password: REDACTED\n      db_name: REDACTED\n      port: 5432\n\nschedule_storage:\n  module: dagster_postgres.schedule_storage\n  class: PostgresScheduleStorage\n  config:\n    postgres_db:\n      hostname: REDACTED\n      username: REDACTED\n      password: REDACTED\n      db_name: REDACTED\n      port: 5432\n```\n\n----------------------------------------\n\nTITLE: Executing a Job with Configuration in Python\nDESCRIPTION: This snippet shows how to execute a job with configuration values using the RunConfig object. It demonstrates providing configuration values for a specific op within the job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/run-configuration.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult = config_job.execute_in_process(\n    run_config=RunConfig(\n        ops={\n            \"op_using_config\": OpConfig(person_name=\"Alice\")\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Main Entry Point for Dagster Integration\nDESCRIPTION: Defines the main function that serves as the entry point for Dagster to interact with Modal. It processes episode information and returns the transcription results by invoking the Modal application.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/modal-application.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef main(episode_info):\n    with app.run():\n        return transcribe_episode.remote(episode_info)\n```\n\n----------------------------------------\n\nTITLE: Defining Required Config Fields in Dagster\nDESCRIPTION: Demonstrates how to make config fields required by using an ellipsis (...) as a sentinel value. This ensures a value must be provided when the config object is constructed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyAssetConfig(Config):\n    greeting_phrase: str = ...\n    person_name: str = \"Alice\"\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    # This would raise an error\n    # config1 = MyAssetConfig()\n\n    # Must specify greeting_phrase\n    config2 = MyAssetConfig(greeting_phrase=\"hi\")\n    assert config2.greeting_phrase == \"hi\"\n    assert config2.person_name == \"Alice\"\n```\n\n----------------------------------------\n\nTITLE: Concurrency Configuration Example in YAML for Dagster+\nDESCRIPTION: An example of Dagster+ concurrency settings in YAML format. This shows how to configure pool granularity, default limits, maximum concurrent runs, and tag-based concurrency limits.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/deployment-settings-reference.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  pools:\n    granularity: 'run'\n    default_limit: 1\n  runs:\n    max_concurrent_runs: 10\n    tag_concurrency_limits:\n      - key: 'database'\n        value: 'redshift'\n        limit: 5\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration\nDESCRIPTION: This snippet shows how to install the dagster-aws package using pip. This package is required to use the AWS S3 integration with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/s3.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Loading Snowflake Table in Downstream Dagster Asset\nDESCRIPTION: Python code demonstrating how to load data from a Snowflake table into a downstream Dagster asset for further processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster-io-managers.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset\n\n@asset\ndef iris_cleaned(iris_dataset: pd.DataFrame) -> pd.DataFrame:\n    return iris_dataset[iris_dataset[\"sepal_length\"] > 5.0]\n```\n\n----------------------------------------\n\nTITLE: Defining SFTP Resource in Dagster\nDESCRIPTION: This code snippet defines an SFTP resource using the SSHResource from dagster-ssh. It specifies the connection details for the SFTP server and includes it in the Dagster Definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/sftp_file_upload.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_ssh import SSHResource\n\n\nsftp_resource = SSHResource(\n    remote_host=\"your.hostname\",\n    remote_port=22,\n    username=\"example\",\n    key_file=\"/Users/example/.ssh/id_ed25519\",\n)\n\ndefs = Definitions(\n    resources={\n        'sftp': sftp_resource,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Docker Compose Services\nDESCRIPTION: Docker Compose configuration that orchestrates all Dagster services including PostgreSQL database, user code server, webserver, and daemon processes. Includes networking and volume configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/docker.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3.7'\n\nservices:\n  docker_example_postgresql:\n    image: postgres:11\n    container_name: docker_example_postgresql\n    environment:\n      POSTGRES_USER: 'postgres_user'\n      POSTGRES_PASSWORD: 'postgres_password'\n      POSTGRES_DB: 'postgres_db'\n    networks:\n      - docker_example_network\n    healthcheck:\n      test: ['CMD-SHELL', 'pg_isready -U postgres_user -d postgres_db']\n      interval: 10s\n      timeout: 8s\n      retries: 5\n\n  docker_example_user_code:\n    build:\n      context: .\n      dockerfile: ./Dockerfile_user_code\n    container_name: docker_example_user_code\n    image: docker_example_user_code_image\n    restart: always\n    environment:\n      DAGSTER_POSTGRES_USER: 'postgres_user'\n      DAGSTER_POSTGRES_PASSWORD: 'postgres_password'\n      DAGSTER_POSTGRES_DB: 'postgres_db'\n      DAGSTER_CURRENT_IMAGE: 'docker_example_user_code_image'\n    networks:\n      - docker_example_network\n\n  docker_example_webserver:\n    build:\n      context: .\n      dockerfile: ./Dockerfile_dagster\n    entrypoint:\n      - dagster-webserver\n      - -h\n      - '0.0.0.0'\n      - -p\n      - '3000'\n      - -w\n      - workspace.yaml\n    container_name: docker_example_webserver\n    expose:\n      - '3000'\n    ports:\n      - '3000:3000'\n    environment:\n      DAGSTER_POSTGRES_USER: 'postgres_user'\n      DAGSTER_POSTGRES_PASSWORD: 'postgres_password'\n      DAGSTER_POSTGRES_DB: 'postgres_db'\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /tmp/io_manager_storage:/tmp/io_manager_storage\n    networks:\n      - docker_example_network\n    depends_on:\n      docker_example_postgresql:\n        condition: service_healthy\n      docker_example_user_code:\n        condition: service_started\n\n  docker_example_daemon:\n    build:\n      context: .\n      dockerfile: ./Dockerfile_dagster\n    entrypoint:\n      - dagster-daemon\n      - run\n    container_name: docker_example_daemon\n    restart: on-failure\n    environment:\n      DAGSTER_POSTGRES_USER: 'postgres_user'\n      DAGSTER_POSTGRES_PASSWORD: 'postgres_password'\n      DAGSTER_POSTGRES_DB: 'postgres_db'\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /tmp/io_manager_storage:/tmp/io_manager_storage\n    networks:\n      - docker_example_network\n    depends_on:\n      docker_example_postgresql:\n        condition: service_healthy\n      docker_example_user_code:\n        condition: service_started\n\nnetworks:\n  docker_example_network:\n    driver: bridge\n    name: docker_example_network\n```\n\n----------------------------------------\n\nTITLE: Testing a Schedule with Resources in Python\nDESCRIPTION: This snippet demonstrates how to test a schedule that uses resources. It creates a mock resource, builds a context, and invokes the schedule function with the resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/testing-schedules.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef test_process_data_schedule():\n    date_formatter = MockDateFormatter()\n    context = build_schedule_context(\n        scheduled_execution_time=pendulum.datetime(2019, 2, 27, tz=\"US/Central\"),\n        resources={\"date_formatter\": date_formatter},\n    )\n    run_config = process_data_schedule(context)\n    assert run_config.to_dict() == {\n        \"ops\": {\"process_data\": {\"config\": {\"date\": \"27-02-2019\"}}}\n    }\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Definitions with Airflow Assets in Python\nDESCRIPTION: Adding the created Airflow assets to a Dagster Definitions object for use in the Dagster environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\n\ndefs = Definitions(\n    assets=all_warehouse_assets,\n)\n```\n\n----------------------------------------\n\nTITLE: Tag-based Asset Selection in Python\nDESCRIPTION: Defines a job selecting assets between those tagged as private and public\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprivate_public_assets_job = define_asset_job(\n    name=\"private_public_assets_job\", selection='tag:\"private\"+ and +tag:\"public\"'\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring PipesK8sClient for EKS Cluster Access\nDESCRIPTION: Example of configuring the PipesK8sClient to access an Amazon EKS Kubernetes cluster. This shows how to set up proper authentication and connection parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/kubernetes-pod-operator.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example configuration for connecting to an EKS cluster\nfrom dagster_k8s import PipesK8sClient\n\n# Configure the client\nclient = PipesK8sClient(\n    kubeconfig=\"path/to/kubeconfig\",  # Path to kubeconfig file\n    kubecontext=\"my-context\",  # Kubernetes context to use\n    env={\"AWS_PROFILE\": \"my-profile\"},  # Environment variables for authentication\n)\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in JSX\nDESCRIPTION: Imports and renders the DocCardList component from the theme directory to display a list of documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Configuring Time-Partitioned Assets with Delta Lake I/O Manager in Python\nDESCRIPTION: Demonstrates the configuration of a time-partitioned asset for storage in Delta Lake using partition expression metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"partition_expr\": \"time\"})\ndef time_series_data(context: OpExecutionContext):\n    # Asset implementation\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Amazon ECS Agent\nDESCRIPTION: Example configuration in dagster_cloud.yaml for setting environment variables and secrets using Amazon ECS agent. Shows how to specify direct environment variables and AWS Secrets Manager integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/environment-variables/agent-config.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      ecs:\n        env_vars:\n          - DATABASE_NAME=testing\n          - DATABASE_PASSWORD\n        secrets:\n          - name: 'MY_API_TOKEN'\n            valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:token::'\n          - name: 'MY_PASSWORD'\n            valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:password::'\n        secrets_tags:\n          - 'my_tag_name'\n```\n\n----------------------------------------\n\nTITLE: Yielding Results in Dagstermill using Python\nDESCRIPTION: This example demonstrates how to yield a result from within a Dagstermill notebook operation. It extracts the \"greeting\" value from the configuration and yields it back to the Dagster pipeline, enabling data flow management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_config.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndagstermill.yield_result(context.op_config[\"greeting\"])\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dagster Concept Relationships with Mermaid\nDESCRIPTION: This mermaid diagram illustrates the relationships between various Dagster concepts, including Assets, Jobs, Schedules, Sensors, and more. It shows how these concepts interact and depend on each other within the Dagster ecosystem.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph TD\n\n    AssetCheck(AssetCheck)\n    %% click AssetCheck href \"concepts#asset-check\"\n\n    Asset(Asset)\n    %%click Asset href \"concepts#asset\"\n\n    Config(Config)\n    %%click Config href \"concepts#config\"\n\n    CodeLocation(Code Location)\n    %%click CodeLocation href \"concepts#code-location\"\n\n    Definitions(Definitions)\n    %%click Definitions href \"concepts#definitions\"\n\n    Graph(Graph)\n    %%click Graph href \"concepts#graph\"\n\n    IoManager(IO Manager)\n    %%click IoManager href \"concepts#io-manager\"\n\n    Job(Job)\n    %%click Job href \"concepts#job\"\n\n    Op(Op)\n    %%click Op href \"concepts#op\"\n\n    Partition(Partition)\n    %%click Partition href \"concepts#partition\"\n\n    Resource(Resource)\n    %%click Resource href \"concepts#resource\"\n\n    Schedule(Schedule)\n    %%click Schedule href \"concepts#schedule\"\n\n    Sensor(Sensor)\n    %%click Sensor href \"concepts#sensor\"\n\n    Type(Type)\n    %%click Type href \"concepts#type\"\n\n    Type ==> Op\n    Op ==> Graph\n    Graph ==> Job\n\n    Job ==> Schedule\n    Job ==> Sensor\n    Job ==> Component\n\n    Partition ==> Asset\n    IoManager ==> Asset\n\n    Resource ==> Asset\n    Resource ==> Schedule\n    Resource ==> Sensor\n\n    AssetCheck ==> Component\n    AssetCheck ==> Asset\n\n    Config ==> Schedule\n    Config ==> Sensor\n    Config ==> Job\n    Config ==> Asset\n\n    Asset ==> Job\n    Asset ==> Schedule\n    Asset ==> Sensor\n\n    subgraph Component\n    Definitions\n    end\n\n    Asset ==> Component\n    Schedule ==> Component\n    Sensor ==> Component\n    IoManager ==> Component\n    Resource ==> Component\n\n    Component ==> CodeLocation\n```\n\n----------------------------------------\n\nTITLE: Using Multiple I/O Managers with Dagster Assets\nDESCRIPTION: This snippet demonstrates how to use different I/O managers for different assets in Dagster. It shows storing one asset in Snowflake and another in Amazon S3.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@asset(io_manager_key=\"warehouse_io_manager\")\ndef iris_dataset():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n\n@asset(io_manager_key=\"blob_io_manager\")\ndef iris_plots(iris_dataset):\n    return sns.pairplot(iris_dataset)\n```\n\n----------------------------------------\n\nTITLE: Distributed Dask Configuration in YAML\nDESCRIPTION: YAML configuration for executing a Dagster job on a remote Dask cluster, specifying the scheduler address and port.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/dask.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexecution:\n  config:\n    cluster:\n      address: scheduler-address\n      port: 8786\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and OpenAI Libraries\nDESCRIPTION: Command to install the required Python packages dagster and dagster-openai using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/openai.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-openai\n```\n\n----------------------------------------\n\nTITLE: Uploading SAML metadata to Dagster+ using CLI\nDESCRIPTION: This command uses the dagster-cloud CLI to upload the SAML metadata file from OneLogin to Dagster+. It requires a user token with appropriate permissions and the Dagster+ organization URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/onelogin-sso.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud organization settings saml upload-identity-provider-metadata <path/to/metadata> \\\n  --api-token=<user_token> \\\n  --url https://<organization_name>.dagster.cloud\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset to Process S3 Files in Dagster\nDESCRIPTION: This snippet defines a Dagster asset that processes files from the source S3 bucket. It includes the sensor definition, S3 resource configuration, and the main asset function for file processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/monitor_files_in_aws_s3.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import (\n    AssetExecutionContext,\n    Config,\n    Definitions,\n    EnvVar,\n    RunConfig,\n    RunRequest,\n    SkipReason,\n    asset,\n    sensor,\n)\nfrom dagster_aws.s3 import S3Resource\nfrom dagster_aws.s3.sensor import get_s3_keys\n\nAWS_S3_BUCKET = \"example-source-bucket\"\nAWS_S3_OBJECT_PREFIX = \"example-source-prefix\"\n\ns3_resource = S3Resource(\n    aws_access_key_id=EnvVar(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=EnvVar(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=EnvVar(\"AWS_SESSION_TOKEN\"),\n    region_name=\"us-west-2\",\n)\n\nclass ObjectConfig(Config):\n    key: str\n\n@asset()\ndef s3_file_backup(context: AssetExecutionContext, s3: S3Resource, config: ObjectConfig):\n    s3 = context.resources.s3\n    context.log.info(f\"Reading {config.key}\")\n    _ = s3.get_object(Bucket=AWS_S3_BUCKET, Key=config.key)  # process object here\n\n@sensor(target=s3_file_backup)\ndef s3_backup_sensor(context):\n    latest_key = context.cursor or None\n    unprocessed_object_keys = get_s3_keys(\n        bucket=AWS_S3_BUCKET, prefix=AWS_S3_OBJECT_PREFIX, since_key=latest_key\n    )\n\n    for key in unprocessed_object_keys:\n        yield RunRequest(\n            run_key=key, run_config=RunConfig(ops={\"s3_file_backup\": ObjectConfig(key=key)})\n        )\n\n    if not unprocessed_object_keys:\n        return SkipReason(\"No new s3 files found for bucket source-bucket.\")\n\n    last_key = unprocessed_object_keys[-1]\n    context.update_cursor(last_key)\n\ndefs = Definitions(\n    assets=[s3_file_backup],\n    resources={\n        \"s3\": s3_resource,\n    },\n    sensors=[s3_backup_sensor],\n)\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Observation to Local Webserver using cURL (Bash)\nDESCRIPTION: This snippet demonstrates how to report an asset observation with a data version to a locally running Dagster webserver using cURL. It uses a POST request with the asset key in the URL path and the data version as a query parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST localhost:3000/report_asset_observation/{ASSET_KEY}?data_version={VERSION}\n```\n\n----------------------------------------\n\nTITLE: Specifying BigQuery Dataset in Asset Key with Dagster\nDESCRIPTION: Shows how to specify the BigQuery dataset as part of the asset's key in Dagster. This approach allows storing assets in different datasets based on their key prefixes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(key_prefix=[\"IRIS\"])\ndef iris_data():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n\n@asset(key_prefix=[\"DAFFODIL\"])\ndef daffodil_data():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n```\n\n----------------------------------------\n\nTITLE: Updating Definitions Object to Include Sensor in Dagster\nDESCRIPTION: This code updates the Dagster Definitions object to include the adhoc_request asset and the adhoc_request_sensor. It organizes assets into groups and registers all necessary components for the ETL pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-a-sensor-asset.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[\n        # Extract assets\n        orders,\n        reviews,\n        # Transform assets\n        sales,\n        shipped_orders,\n        product_metrics,\n        dept_product_metrics,\n        product_performance,\n        # Special assets\n        adhoc_request,\n    ],\n    asset_checks=[orders_check, reviews_check],\n    sensors=[adhoc_request_sensor],\n    jobs=[\n        # Optional jobs\n        extract_job,\n    ],\n    schedules=[daily_etl_schedule],\n)\n```\n\n----------------------------------------\n\nTITLE: Scheduling Source Asset Observations\nDESCRIPTION: Example showing how to set up a scheduled job for running source asset observations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/asset-observations.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nobserve_job = define_asset_job(\n    \"observe_my_sources\",\n    selection=[my_source_file],\n)\n\n@schedule(\n    job=observe_job,\n    cron_schedule=\"0 * * * *\"\n)\ndef hourly_observations():\n    return RunRequest()\n```\n\n----------------------------------------\n\nTITLE: Setting Celery Priority for Dagster Op and Job\nDESCRIPTION: This Python code shows how to set Celery priorities for both a Dagster op and job using tags. The priorities are summed when both are set.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/kubernetes-and-celery.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@op(\n  tags = {\n    'dagster-celery/priority': 2,\n  }\n)\ndef my_op(context):\n  context.log.info('running')\n\n@job(\n  tags = {\n    'dagster-celery/run_priority': 3,\n  }\n)\ndef my_job():\n  my_op()\n```\n\n----------------------------------------\n\nTITLE: Basic Data Exploration\nDESCRIPTION: Performs basic exploratory data analysis using pandas methods to view data head, unique species, and statistical descriptions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/tutorial_template/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\niris.head()\n\niris[\"Species\"].unique()\n\niris.describe()\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Job Execution in Dagster\nDESCRIPTION: This snippet demonstrates how to configure and execute a Spark job from within a Dagster asset. It includes setting up CLI arguments and running the spark-submit command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/pyspark-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n        cli_args = session.get_bootstrap_cli_arguments()\n        spark_submit_args = [\n            \"spark-submit\",\n            \"--master\", \"local[*]\",\n            \"--conf\", \"spark.hadoop.fs.s3a.endpoint=http://minio:9000\",\n            \"--conf\", \"spark.hadoop.fs.s3a.access.key=minioadmin\",\n            \"--conf\", \"spark.hadoop.fs.s3a.secret.key=minioadmin\",\n            \"--conf\", \"spark.hadoop.fs.s3a.path.style.access=true\",\n            \"--conf\", \"spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\",\n            \"script.py\",\n            *cli_args,\n        ]\n        result = subprocess.run(spark_submit_args, capture_output=True, text=True)\n        print(result.stdout)\n        print(result.stderr, file=sys.stderr)\n        if result.returncode != 0:\n            raise Exception(f\"Spark job failed with return code {result.returncode}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Success and Failure Hooks in Python\nDESCRIPTION: This snippet demonstrates how to define success and failure hooks using the @success_hook and @failure_hook decorators. The hooks send Slack messages on op success or failure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-hooks.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import success_hook, failure_hook, HookContext\n\n@success_hook(required_resource_keys={\"slack\"})\ndef slack_message_on_success(context: HookContext):\n    message = f\"Op {context.op.name} succeeded\"\n    context.resources.slack.send_message(message)\n\n@failure_hook(required_resource_keys={\"slack\"})\ndef slack_message_on_failure(context: HookContext):\n    message = f\"Op {context.op.name} failed\"\n    context.resources.slack.send_message(message)\n```\n\n----------------------------------------\n\nTITLE: Dagster Project Definitions Configuration\nDESCRIPTION: Configuration of Dagster project definitions including assets, resources, and the custom I/O manager setup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/embeddings.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=load_assets_from_modules([assets]),\n    resources={\n        \"github\": github_resource.configured(\n            {\n                \"repos\": [\n                    \"dagster-io/dagster\",\n                ],\n                \"github_token\": EnvVar(\"GITHUB_TOKEN\"),\n            }\n        ),\n        \"openai\": openai_resource.configured(\n            {\"openai_api_key\": EnvVar(\"OPENAI_API_KEY\")}\n        ),\n        \"pinecone\": pinecone_resource.configured(\n            {\n                \"pinecone_api_key\": EnvVar(\"PINECONE_API_KEY\"),\n                \"pinecone_env\": EnvVar(\"PINECONE_ENV\"),\n            }\n        ),\n        \"document_io_manager\": document_io_manager.configured(\n            {\"base_dir\": \"/tmp/docs_project\"}\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Schedule Timezone in Python\nDESCRIPTION: Shows how to set a custom timezone for a schedule using the timezone parameter. The example sets the timezone to America/Los_Angeles.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndaily_schedule = ScheduleDefinition(\n    job=daily_refresh_job,\n    cron_schedule=\"0 0 * * *\",\n    timezone=\"America/Los_Angeles\",\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project Directory Tree in Plaintext\nDESCRIPTION: Shows the directory structure of a Dagster project, including the main project folder 'my_existing_project' with its Python files, and configuration files at the root level. This structure is typical for a Dagster project setup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/1-uv-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── my_existing_project\n│   ├── __init__.py\n│   ├── assets.py\n│   ├── definitions.py\n│   └── py.typed\n├── pyproject.toml\n└── uv.lock\n\n2 directories, 6 files\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxied State with DBT Task Proxied\nDESCRIPTION: Updated YAML configuration where the build_dbt_models task has been set to proxied: True while other tasks remain unproxied. This enables the DBT task to be executed in Dagster while other tasks run in Airflow.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/migrate.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndag_id: rebuild_customers_list\ntasks:\n  load_raw_customers:\n    proxied: false\n  build_dbt_models:\n    proxied: true\n  export_customers:\n    proxied: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Logger Level in Dagster YAML\nDESCRIPTION: Example configuration for filtering log messages in Dagster, showing how to set the console logger to only display ERROR level logs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/index.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"docs_snippets/docs_snippets/concepts/logging/config.yaml\"\n```\n\n----------------------------------------\n\nTITLE: GitLab CI/CD Configuration for Dagster+ Hybrid\nDESCRIPTION: Example GitLab CI/CD configuration for building and deploying a Dagster+ Hybrid code location, including Docker image building and deployment steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/index.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nvariables:\nDAGSTER_CLOUD_ORGANIZATION: <organinization-name>\nDAGSTER_PROJECT_DIR: .\nIMAGE_REGISTRY: <account-id>.dkr.ecr.us-west-2.amazonaws.com/<image-name>\nIMAGE_TAG: $CI_COMMIT_SHORT_SHA-$CI_PIPELINE_ID\n\nstages:\n- build\n- deploy\n\nbuild:\nstage: build\nimage: docker:latest\nservices:\n    - docker:dind\nbefore_script:\n    # # For Gitlab Container Registry\n    # - echo $CI_JOB_TOKEN | docker login --username $CI_REGISTRY_USER --password-stdin $REGISTRY_URL\n    # # For DockerHub\n    # - echo $DOCKERHUB_TOKEN | docker login --username $DOCKERHUB_USERNAME --password-stdin $REGISTRY_URL\n    # # For AWS Elastic Container Registry (ECR)\n    # - apk add --no-cache curl jq python3 py3-pip\n    # - pip install awscli\n    # - echo $AWS_ECR_PASSWORD | docker login --username AWS --password-stdin $IMAGE_REGISTRY\n    # # For Google Container Registry (GCR)\n    # - echo $GCR_JSON_KEY | docker login --username _json_key --password-stdin $REGISTRY_URL\nscript:\n    - docker build . -t $IMAGE_REGISTRY:$IMAGE_TAG\n    - docker push $IMAGE_REGISTRY:$IMAGE_TAG\n\ndeploy:\nstage: deploy\ndependencies:\n    - build\nimage: ghcr.io/dagster-io/dagster-cloud-action:0.1.43\nscript:\n    - dagster-cloud deployment add-location --deployment prod --image\n    $IMAGE_REGISTRY:$IMAGE_TAG --location-name quickstart --package-name quickstart\n```\n\n----------------------------------------\n\nTITLE: Setting Snowflake Credentials as Environment Variables\nDESCRIPTION: Shell commands to set Snowflake username and password as environment variables for authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster-io-managers.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport SNOWFLAKE_USER=<your username>\nexport SNOWFLAKE_PASSWORD=<your password>\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Settings for Individual Ops in Python\nDESCRIPTION: This snippet demonstrates how to use the 'dagster-k8s/config' tag on a Dagster op to set custom Kubernetes configuration for that specific op. It includes settings for container resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op(\n    tags={\n        \"dagster-k8s/config\": {\n            \"container_config\": {\n                \"resources\": {\n                    \"requests\": {\"cpu\": \"250m\", \"memory\": \"64Mi\"},\n                    \"limits\": {\"cpu\": \"500m\", \"memory\": \"2560Mi\"},\n                }\n            }\n        }\n    }\n)\ndef my_op():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Using LazyFrames in Dagster assets\nDESCRIPTION: Example of using Polars LazyFrames in Dagster assets. It shows how to annotate inputs and outputs as LazyFrames for scanning and sinking operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/polars.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(io_manager_key=\"polars_parquet_io_manager\")\ndef downstream(upstream: pl.LazyFrame) -> pl.LazyFrame:\n    assert isinstance(upstream, pl.LazyFrame)\n    return upstream\n```\n\n----------------------------------------\n\nTITLE: Importing and Creating Assets in Dagster Python\nDESCRIPTION: Demonstrates the preferred code style for Dagster examples, using short import aliases and proper type annotations. Shows how to create an asset with context and return a MaterializeResult with metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagster as dg\n\n@dg.asset\ndef my_cool_asset(context: dg.AssetExecutionContext) -> dg.MaterializeResult:\n    return dg.MaterializeResult(\n        metadata={\n            \"foo\": \"bar\",\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Run Monitoring Configuration Example in YAML for Dagster+\nDESCRIPTION: An example of Dagster+ run monitoring settings in YAML format. This demonstrates how to set timeouts for run starting, cancellation, and maximum runtime.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/deployment-settings-reference.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nrun_monitoring:\n  start_timeout_seconds: 1200\n  cancel_timeout_seconds: 1200\n  max_runtime_seconds: 7200\n```\n\n----------------------------------------\n\nTITLE: Chaining Dynamic Outputs in Python with Dagster\nDESCRIPTION: These examples demonstrate two equivalent ways to establish a sequence of ops that occur for each dynamic output in Dagster, showcasing different syntax for the same functionality.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@job\ndef chained_dynamic_job():\n    dynamic_outs = dynamic_outputs()\n    step_1 = dynamic_outs.map(downstream_op_1)\n    step_2 = step_1.map(downstream_op_2)\n    step_3 = step_2.map(downstream_op_3)\n\n@job\ndef alt_chained_dynamic_job():\n    dynamic_outs = dynamic_outputs()\n    step_3 = dynamic_outs.map(downstream_op_1).map(downstream_op_2).map(downstream_op_3)\n```\n\n----------------------------------------\n\nTITLE: Generating Dagster Type from Pandera Schema for Stock Price Data\nDESCRIPTION: This snippet demonstrates creating a Pandera schema for stock prices, converting it to a Dagster Type, and using it in a Dagster asset. It includes a toy job that generates corrupted stock price data for testing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pandera.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport pandera as pa\nfrom dagster import asset, job\nfrom dagster_pandera import pandera_schema_to_dagster_type\n\n# Define a Pandera schema\nclass StockPrices(pa.DataFrameModel):\n    date: pa.typing.Series[pa.typing.Date]\n    open: pa.typing.Series[float] = pa.Field(gt=0)\n    close: pa.typing.Series[float] = pa.Field(gt=0)\n\n# Convert the Pandera schema to a Dagster Type\nStockPricesType = pandera_schema_to_dagster_type(StockPrices)\n\n@asset(dagster_type=StockPricesType)\ndef apple_stock_prices_dirty():\n    # Generate some toy data\n    df = pd.DataFrame(\n        {\n            \"date\": pd.date_range(start=\"2021-01-01\", periods=7),\n            \"open\": [143.0, 145.3, 144.2, 143.8, 142.7, 143.2, 143.5],\n            \"close\": [144.5, 144.9, 143.7, 142.5, 143.1, 143.7, 144.1],\n        }\n    )\n    # Corrupt the data with some nulls\n    df.loc[2, \"open\"] = None\n    df.loc[5, \"close\"] = None\n    return df\n\n@job\ndef stocks_job():\n    apple_stock_prices_dirty()\n```\n\n----------------------------------------\n\nTITLE: Configuring GCSComputeLogManager in YAML for Dagster\nDESCRIPTION: This snippet shows how to configure the GCSComputeLogManager, including authentication with a service account and using environment variables for credentials.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/obstore.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# there are multiples ways to configure the GCSComputeLogManager\n# Authenticate with service account\ncompute_logs:\n  module:  dagster_obstore.gcs.compute_log_manager\n  class: GCSComputeLogManager\n  config:\n    bucket: \"dagster-logs\"\n    service_account: \"access-key-id\"\n    service_account_key: \"my-key\"\n    local_dir: \"/tmp/dagster-logs\"\n# Don't set secrets through config, but let obstore pick it up from ENV VARS\ncompute_logs:\n  module:  dagster_obstore.gcs.compute_log_manager\n  class: GCSComputeLogManager\n  config:\n    bucket: \"dagster-logs\"\n    local_dir: \"/tmp/dagster-logs\"\n```\n\n----------------------------------------\n\nTITLE: Configuring EcsRunLauncher for Custom Secrets Tag\nDESCRIPTION: This YAML configuration sets up the EcsRunLauncher to use a custom tag for identifying AWS Secrets Manager secrets to be included as environment variables in Dagster runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: 'dagster_aws.ecs'\n  class: 'EcsRunLauncher'\n  config:\n    secrets_tag: 'my-tag-name'\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Job Cleanup Commands\nDESCRIPTION: Shell commands for cleaning up old Dagster jobs and pods in Kubernetes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get job | grep -e dagster-run -e dagster-step | awk 'match($4,/[0-9]+d/) {print $1}' | xargs kubectl delete job\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pod | grep -e dagster-run -e dagster-step | awk 'match($3,/Completed/) {print $0}' | awk 'match($5,/[0-9]+d/) {print $1}' | xargs kubectl delete pod\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Profiles for R2 and DuckDB Connection\nDESCRIPTION: Defines dbt profiles.yml configuration for connecting to R2 bucket and DuckDB database with separate dev and production environments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/modeling.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\natproto_dashboard:\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      path: ':memory:'\n      schema: profiles_dev\n      external_root: 'r2://dagster-demo'\n      extensions:\n        - httpfs\n    prod:\n      type: duckdb\n      path: ':memory:'\n      schema: profiles_prod\n      external_root: 'r2://dagster-demo'\n      extensions:\n        - httpfs\n```\n\n----------------------------------------\n\nTITLE: Creating Schedule for Single Partition of Static Partitioned Job in Python\nDESCRIPTION: This snippet demonstrates how to create a schedule that targets a single partition (Antarctica) of a statically partitioned job using the @schedule decorator in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/constructing-schedules-for-partitioned-assets-and-jobs.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@schedule(cron_schedule=\"0 0 * * *\", job=continent_job)\ndef antarctica_schedule():\n    return RunRequest(\n        run_key=None,\n        partition_key=\"Antarctica\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQueryPySparkIOManager in Dagster\nDESCRIPTION: Shows how to configure the BigQueryPySparkIOManager in Dagster definitions. This I/O manager supports storing and loading PySpark DataFrames in BigQuery.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_gcp_pyspark import BigQueryPySparkIOManager\n\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"io_manager\": BigQueryPySparkIOManager(\n            project=\"my-project\",\n            dataset=\"my_dataset\",\n            temporary_gcs_bucket=\"my-temporary-bucket\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: External PostgreSQL Database Configuration\nDESCRIPTION: YAML configuration for setting up an external PostgreSQL database in Dagster, including host, credentials, and database details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npostgresql:\n  enabled: false\n  postgresqlHost: 'postgresqlHost'\n  postgresqlUsername: 'postgresqlUsername'\n  postgresqlPassword: 'postgresqlPassword'\n  postgresqlDatabase: 'postgresqlDatabase'\n  service:\n    port: 5432\n```\n\n----------------------------------------\n\nTITLE: Updating custom_replication_assets Function in Python\nDESCRIPTION: Modifies the custom_replication_assets function to incorporate the ReplicationTranslator. It uses the translator to define asset keys and includes the replication project and translator in the AssetSpec metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef custom_replication_assets(\n    *,\n    replication_project: ReplicationProject,\n    name: Optional[str] = None,\n    group_name: Optional[str] = None,\n    translator: Optional[ReplicationTranslator] = None,\n) -> Callable[[Callable[..., Any]], AssetsDefinition]:\n    project = replication_project.load()\n\n    translator = (\n        check.opt_inst_param(translator, \"translator\", ReplicationTranslator)\n        or ReplicationTranslator()\n    )\n\n    return multi_asset(\n        name=name,\n        group_name=group_name,\n        specs=[\n            AssetSpec(\n                key=translator.get_asset_key(table),\n                metadata={\n                    \"replication_project\": project,\n                    \"replication_translator\": translator,\n                },\n            )\n            for table in project.get(\"tables\")\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Testing Cursor-based Sensor with Context\nDESCRIPTION: Example showing how to test a sensor that requires context, specifically for cursor-based directory monitoring.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-sensors.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontext = build_sensor_context(cursor=\"foo\")\nresult = my_directory_sensor_cursor(context)\nrun_requests = list(result.run_requests)\n```\n\n----------------------------------------\n\nTITLE: Report Asset Materialization\nDESCRIPTION: Shows how to report asset materialization metadata back to Dagster using PipesContext. Includes passing structured metadata about total orders.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/modify-external-code.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import open_dagster_pipes, PipesContext\n\nwith open_dagster_pipes():\n    orders = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n    context = PipesContext.get()\n    total = sum(quantity for _, quantity in orders)\n    context.log.info(f\"Total orders: {total}\")\n    context.report_asset_materialization(metadata={\"total_orders\": total})\n```\n\n----------------------------------------\n\nTITLE: Setting DAGSTER_HOME Environment Variable\nDESCRIPTION: Commands to set the DAGSTER_HOME environment variable which specifies where Dagster stores its persistent data and logs. Includes both Linux/macOS and Windows versions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/deploying-dagster-as-a-service.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport DAGSTER_HOME=\"/home/yourusername/dagster_home\"\n```\n\nLANGUAGE: powershell\nCODE:\n```\n$env:DAGSTER_HOME = \"C:\\Users\\YourUsername\\dagster_home\"\n```\n\n----------------------------------------\n\nTITLE: Testing Partition Keys Generation in Python\nDESCRIPTION: Demonstrates how to test partition key generation and configuration retrieval using get_partition_keys and get_run_config_for_partition_key functions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/testing-partitioned-config-and-jobs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef test_my_partitioned_config_keys():\n    keys = my_partitioned_config.get_partition_keys()\n    assert len(keys) > 0\n    config = my_partitioned_config.get_run_config_for_partition_key(keys[0])\n    assert config is not None\n```\n\n----------------------------------------\n\nTITLE: Implementing dbt Execution with Airflow BashOperator\nDESCRIPTION: Example of an Airflow DAG using BashOperator to run a dbt command. The code shows how to define a DAG with a BashOperator that executes 'dbt run' in a specific project directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/bash-operator-dbt.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\ndag = DAG(\n    \"dbt_run\",\n    start_date=datetime(2023, 1, 1),\n    schedule_interval=\"@daily\",\n)\n\ndbt_run = BashOperator(\n    task_id=\"dbt_run\",\n    bash_command=\"cd /path/to/dbt/project && dbt run\",\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery Resource\nDESCRIPTION: Python code to configure the BigQuery resource in Dagster with project and location settings\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_gcp import bigquery_resource\n\ndefs = Definitions(\n    resources={\n        \"bigquery\": bigquery_resource.configured({\n            \"project\": \"my-gcp-project\",\n            \"location\": \"us-east5\"\n        })\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Dagster Type from Config\nDESCRIPTION: Shows how to define a custom Dagster type with a DagsterTypeLoader to load input via run config.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/unconnected-inputs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DagsterType, dagster_type_loader, In, job, op\n\nclass MyClass:\n    def __init__(self, val):\n        self.val = val\n\n@dagster_type_loader(str)\ndef my_loader(_, cfg):\n    return MyClass(cfg)\n\nMyDagsterType = DagsterType(\n    name=\"MyDagsterType\",\n    type_check_fn=lambda _, value: isinstance(value, MyClass),\n    loader=my_loader,\n)\n\n@op(ins={\"my_input\": In(MyDagsterType)})\ndef my_op(my_input):\n    return my_input.val\n\n@job\ndef my_job():\n    my_op()\n\nresult = my_job.execute_in_process(\n    run_config={\"ops\": {\"my_op\": {\"inputs\": {\"my_input\": {\"value\": \"test\"}}}}}\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing Debug Logs for Dagster Asset Job Execution\nDESCRIPTION: Example showing debug-level logs generated during the execution of a Dagster asset job, demonstrating various execution stages including run start, worker initialization, resource handling, and step completion.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n12:40:05 - DEBUG - RUN_START - Started execution of run for \"__ASSET_JOB_0\".\n12:40:05 - DEBUG - ENGINE_EVENT - Executing steps using multiprocess executor: parent process (pid: 86387)\n12:40:05 - DEBUG - taxi_zones_file - STEP_WORKER_STARTING - Launching subprocess for \"taxi_zones_file\".\n12:40:07 - DEBUG - STEP_WORKER_STARTED - Executing step \"taxi_zones_file\" in subprocess.\n12:40:07 - DEBUG - taxi_zones_file - RESOURCE_INIT_STARTED - Starting initialization of resources [io_manager].\n12:40:07 - DEBUG - taxi_zones_file - RESOURCE_INIT_SUCCESS - Finished initialization of resources [io_manager].\n12:40:07 - DEBUG - LOGS_CAPTURED - Started capturing logs in process (pid: 86390).\n12:40:07 - DEBUG - taxi_zones_file - STEP_START - Started execution of step \"taxi_zones_file\".\n12:40:09 - DEBUG - taxi_zones_file - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n12:40:09 - DEBUG - __ASSET_JOB_0 - taxi_zones_file - Writing file at: /Users/erincochran/Desktop/dagster-examples/project-dagster-university/tmpfxsoltsc/storage/taxi_zones_file using PickledObjectFilesystemIOManager...\n12:40:09 - DEBUG - taxi_zones_file - ASSET_MATERIALIZATION - Materialized value taxi_zones_file.\n12:40:09 - DEBUG - taxi_zones_file - HANDLED_OUTPUT - Handled output \"result\" using IO manager \"io_manager\"\n12:40:09 - DEBUG - taxi_zones_file - STEP_SUCCESS - Finished execution of step \"taxi_zones_file\" in 1.17s.\n12:40:09 - DEBUG - ENGINE_EVENT - Multiprocess executor: parent process exiting after 4.38s (pid: 86387)\n12:40:09 - DEBUG - RUN_SUCCESS - Finished execution of run for \"__ASSET_JOB_0\".\n```\n\n----------------------------------------\n\nTITLE: Configuring StringSource in Dagster Python\nDESCRIPTION: Demonstrates how to use StringSource for configuring an operation with either a string literal or an environment variable selector.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/config.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, StringSource\n\n@op(config_schema=StringSource)\ndef secret_op(context) -> str:\n    return context.op_config\n\n@job\ndef secret_job():\n    secret_op()\n\nsecret_job.execute_in_process(\n    run_config={\n        'ops': {'secret_op': {'config': 'test_value'}}\n    }\n)\n\nsecret_job.execute_in_process(\n    run_config={\n        'ops': {'secret_op': {'config': {'env': 'VERY_SECRET_ENV_VARIABLE'}}}\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Code Location in TOML\nDESCRIPTION: TOML configuration examples showing how to specify module names and code location names in pyproject.toml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/managing-code-locations-with-definitions.md#2025-04-22_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[tool.dagster]\nmodule_name = \"your_module_name.definitions\"  ## name of project's Python module and where to find the definitions\ncode_location_name = \"your_code_location_name\"  ## optional, name of code location to display in the Dagster UI\n```\n\nLANGUAGE: toml\nCODE:\n```\n[tool.dagster]\nmodules = [{ type = \"module\", name = \"foo\" }, { type = \"module\", name = \"bar\" }]\n```\n\n----------------------------------------\n\nTITLE: Accessing Failure Information in Dagster Hooks\nDESCRIPTION: This example shows how to access detailed failure information within a failure hook using the HookContext.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-hooks.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@failure_hook(required_resource_keys={\"slack\"})\ndef slack_message_on_failure(context):\n    exception = context.op_exception\n    message = (\n        f\"Op {context.op.name} failed with exception:\\n\"\n        f\"{''.join(traceback.format_tb(exception.__traceback__))}\"\n    )\n    context.resources.slack.send_message(message)\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Python Loggers in Dagster YAML\nDESCRIPTION: YAML configuration to capture logs from specific Python loggers in Dagster. This allows treating normal Python log calls the same as context.log calls.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/python-logging.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npython_logs:\n  managed_python_loggers:\n    - my_logger\n    - other_logger\n```\n\n----------------------------------------\n\nTITLE: Configuring Date-Based Job with YAML\nDESCRIPTION: YAML configuration example for specifying the date parameter in a Dagster job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/partitioning-ops.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nops:\n  process_data_for_date:\n    config:\n      date: \"2020-05-05\"\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio Segments with Whisper Model\nDESCRIPTION: Defines a Modal function for transcribing individual audio segments using the OpenAI Whisper model. This function is decorated with Modal settings including the required image, R2 bucket mount, and CPU resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/modal-application.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@app.function(\n    image=app_image,\n    mounts=[r2_bucket],\n    cpu=4,\n)\ndef transcribe_segment(segment_path):\n    import whisper\n\n    model = whisper.load_model(\"small\")\n    result = model.transcribe(segment_path)\n    return result[\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Snowflake Libraries\nDESCRIPTION: Command to install the required Dagster Snowflake libraries using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster-io-managers.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-snowflake dagster-snowflake-pandas\n```\n\n----------------------------------------\n\nTITLE: Specifying Schema in Asset Key for Delta Lake I/O Manager in Python\nDESCRIPTION: Shows how to specify a Delta Lake schema for an asset by including it in the asset key definition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset(key=[\"IRIS\", \"iris_dataset\"])\ndef iris_dataset():\n    # Asset implementation\n\n@asset(key=[\"DAFFODIL\", \"daffodil_dataset\"])\ndef daffodil_dataset():\n    # Asset implementation\n```\n\n----------------------------------------\n\nTITLE: Defining S3 Resources in Dagster\nDESCRIPTION: This snippet shows how to define S3 resources in Dagster for interacting with S3 buckets. It uses environment variables for AWS credentials and specifies the region.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/monitor_files_in_aws_s3.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_aws.s3 import S3Resource\n\ns3_resource = S3Resource(\n    aws_access_key_id=EnvVar(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=EnvVar(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=EnvVar(\"AWS_SESSION_TOKEN\"),\n    region_name=\"us-west-2\",\n)\n\ndefs = Definitions(\n    resources={\n        \"s3\": s3_resource,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Client and Definitions\nDESCRIPTION: Python code for setting up the Databricks client resource and creating Dagster definitions\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/databricks-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\n\ndefinitions = Definitions(\n    assets=[databricks_asset],\n    resources={\n        \"databricks_client\": PipesDatabricksClient(),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Instance-Level Concurrency Limits in Dagster YAML\nDESCRIPTION: This YAML configuration sets up concurrency limits at the Dagster instance level. It establishes a tag concurrency limit of 1 for the 'actor_feed_snapshot' tag, ensuring only one execution runs at a time to prevent hitting API rate limits.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/rate-limiting.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  tag_concurrency_limits:\n    - key: \"actor_feed_snapshot\"\n      value: 1\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dagster Helm Chart\nDESCRIPTION: This command upgrades or installs the Dagster Helm chart with specified values. It modifies the existing 'dagster' release if it exists, or creates a new one if it doesn't.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/migrating-while-upgrading.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhelm upgrade --install dagster dagster/dagster -f /path/to/values.yaml\n```\n\n----------------------------------------\n\nTITLE: Establishing Cross-DAG Lineage in Python\nDESCRIPTION: Using the replace_attributes function to add a dependency from the 'load_customers' asset to the 'customer_metrics' asset, establishing cross-DAG lineage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_airlift import replace_attributes\n\ncustomer_metrics_dag_asset = replace_attributes(\n    customer_metrics_dag_asset,\n    deps=[load_customers_asset],\n)\n```\n\n----------------------------------------\n\nTITLE: Executing OpenAI Fine-Tuning Job with Dagster Asset\nDESCRIPTION: This code creates a Dagster asset that initiates an OpenAI fine-tuning job and waits for its completion. It uses the OpenAI resource to submit the job, polls for status updates, and returns the fine-tuned model's name. The asset also records the model name as metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/open-ai-job.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(group_name=\"openai\", deps=[training_file_id, validation_file_id])\ndef fine_tuned_model(context, openai, training_file_id, validation_file_id):\n    job = openai.fine_tunes.create(\n        training_file=training_file_id,\n        validation_file=validation_file_id,\n        model=\"gpt-3.5-turbo\",\n    )\n\n    while job.status != \"succeeded\":\n        time.sleep(60)\n        job = openai.fine_tunes.retrieve(id=job.id)\n        logger.info(f\"Fine-tuning status: {job.status}\")\n\n    context.add_output_metadata({\"model_name\": job.fine_tuned_model})\n    return job.fine_tuned_model\n```\n\n----------------------------------------\n\nTITLE: Customizing DAG-Level Proxy Operator in Dagster-Airlift\nDESCRIPTION: Example demonstrating how to customize the operator constructed on a per-DAG basis using the build_from_dag_fn argument of proxying_to_dagster. This example shows how to provide an authorization header for authentication with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/migration-reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"airlift-migration-tutorial/tutorial_example/snippets/custom_operator_examples/custom_dag_level_proxy.py\" />\n```\n\n----------------------------------------\n\nTITLE: Multiprocess Execution Configuration\nDESCRIPTION: Python code showing how to configure multiprocess execution parameters\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/job-execution.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\njob.execute_in_process(\n    run_config={\n        \"execution\": {\n            \"config\": {\n                \"multiprocess\": {\n                    \"max_concurrent\": 4,\n                    \"start_method\": \"forkserver\"\n                }\n            }\n        }\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dummy Replication Function in Python\nDESCRIPTION: A mock replication function that reads the YAML configuration and yields replication status for each table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\n\nfrom pathlib import Path\nfrom typing import Mapping, Iterator, Any\n\n\ndef replicate(replication_configuration_yaml: Path) -> Iterator[Mapping[str, Any]]:\n    data = yaml.safe_load(replication_configuration_yaml.read_text())\n    for table in data.get(\"tables\"):\n        # < perform replication here, and get status >\n        yield {\"table\": table.get(\"name\"), \"status\": \"success\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Chroma Integration\nDESCRIPTION: Command to install the Dagster and Chroma integration packages via pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/chroma.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-chroma\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Python Environments in Dagster Workspace\nDESCRIPTION: This YAML snippet shows how to set up multiple code locations with different Python environments in a Dagster workspace. It uses the 'executable_path' key to specify distinct Python interpreters and the 'location_name' key for naming each location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nload_from:\n  - python_file: spark_repo.py\n    location_name: spark_repo\n    executable_path: /path/to/spark/venv/bin/python\n  - python_file: tensorflow_repo.py\n    location_name: tensorflow_repo\n    executable_path: /path/to/tensorflow/venv/bin/python\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Resource in Dagster\nDESCRIPTION: Python code snippet demonstrating how to configure the OpenAI resource in Dagster using environment variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/openai.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_openai import OpenAIResource\n\ndefs = Definitions(\n    resources={\n        \"openai\": OpenAIResource(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Resources with Environment Variables\nDESCRIPTION: Demonstrates how to use EnvVar to configure resources using environment variables. The example shows a DatabaseResource that uses environment variables for host, port, and password.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/configuring-resources.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import EnvVar, resource\n\n@resource\nclass DatabaseResource:\n    def __init__(self, host: str, port: int, password: str):\n        self.host = host\n        self.port = port\n        self.password = password\n\nmy_database = DatabaseResource(\n    host=EnvVar(\"DATABASE_HOST\"),\n    port=EnvVar(\"DATABASE_PORT\").int_value,\n    password=EnvVar(\"DATABASE_PASSWORD\")\n)\n```\n\n----------------------------------------\n\nTITLE: Materializing Power BI Semantic Models\nDESCRIPTION: Example demonstrating how to create executable asset definitions for refreshing Power BI semantic models\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/powerbi.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_powerbi import build_semantic_model_refresh_asset_definition\n\nsemantic_model_refresh = build_semantic_model_refresh_asset_definition(\n    \"my_semantic_model\",\n    semantic_model_id=\"your-semantic-model-id\",\n)\n\ndefs = Definitions(\n    assets=[semantic_model_refresh],\n    resources={\"power_bi\": power_bi_workspace},\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Workspace Configuration in YAML\nDESCRIPTION: A simple workspace.yaml file that loads a code location from a Python file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# workspace.yaml\n\nload_from:\n  - python_file: my_file.py\n```\n\n----------------------------------------\n\nTITLE: Logging Asset Observations in Dagster Assets\nDESCRIPTION: This example shows how to log Asset Observations in Dagster assets, tracking specific properties of an asset over time without signifying that the asset has been mutated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetObservation\n\n@asset\ndef my_table():\n    # ... do stuff\n    context.log_event(\n        AssetObservation(\n            asset_key=\"my_table\",\n            metadata={\"num_rows\": 1234},\n        )\n    )\n    return table\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Settings in dagster_cloud.yaml\nDESCRIPTION: Example of setting build directory and registry for a code location in dagster_cloud.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/dagster-cloud-yaml.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: data-eng-pipeline\n    code_source:\n      package_name: example_etl\n    build:\n      directory: ./\n      registry: your-docker-image-registry/image-name # e.g. localhost:5000/myimage\n```\n\n----------------------------------------\n\nTITLE: Azure IO Manager Documentation\nDESCRIPTION: ReStructuredText documentation for ADLS2PickleIOManager implementation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-azure.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable::  dagster_azure.adls2.ADLS2PickleIOManager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Defining PineconeResource Class in Python for Dagster\nDESCRIPTION: This code defines a PineconeResource class with methods to create and retrieve Pinecone indexes. It includes functionality to set up the index with specified dimensions and metric type, defaulting to AWS for cloud infrastructure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/vector-database.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass PineconeResource:\n    def __init__(self, api_key: str):\n        self._client = pinecone.init(api_key=api_key)\n\n    def create_index(\n        self,\n        index_name: str,\n        dimension: int = 1536,\n        metric: str = \"cosine\",\n        pod_type: str = \"p1\",\n    ):\n        if index_name not in pinecone.list_indexes():\n            pinecone.create_index(\n                index_name,\n                dimension=dimension,\n                metric=metric,\n                pod_type=pod_type,\n            )\n\n        return self.get_index(index_name)\n\n    def get_index(self, index_name: str):\n        return pinecone.Index(index_name)\n```\n\n----------------------------------------\n\nTITLE: Multiple Assets Selection Job\nDESCRIPTION: Creates a job selecting multiple specific assets using OR operator\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsummary_stats_assets_job = define_asset_job(\n    name=\"summary_stats_assets_job\", selection='key:\"summary_stats_1\" or key:\"summary_stats_2\"'\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster DuckDB Integration\nDESCRIPTION: Command to install the dagster-duckdb package via pip package manager\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-duckdb\n```\n\n----------------------------------------\n\nTITLE: Migrating from DbtCliClientResource to DbtCliResource\nDESCRIPTION: Examples showing how to update code that uses the deprecated DbtCliClientResource to the new DbtCliResource. This includes changes to both op implementations and asset definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# old\n@op\ndef my_dbt_op(dbt_resource: DbtCliClientResource):\n    dbt: DbtCliClient = dbt.get_client()\n\n    dbt.cli(\"run\")\n\n    dbt.cli(\"run\", full_refresh=True)\n\n    dbt.cli(\"test\")\n    manifest_json = dbt.get_manifest_json()\n\n# new\nwith Path(\"my/dbt/manifest\").open() as handle:\n    manifest = json.loads(dbt_manifest.read())\n\n@op\ndef my_dbt_op(dbt: DbtCliResource):\n   dbt.cli([\"run\"], manifest=manifest).stream()\n\n   dbt.cli([\"run\", \"--full-refresh\"], manifest=manifest).stream()\n\n   dbt_test_invocation = dbt.cli([\"test\"], manifest_manifest).stream()\n   manifest_json = dbt_test_invocation.get_artifact(\"manifest.json\")\n\n# old\ndbt_assets = load_assets_from_dbt_project(project_dir=\"my/dbt/project\")\n\ndefs = Definitions(\n    assets=dbt_assets,\n    resources={\n        \"dbt\": DbtCliClientResource(project_dir=\"my/dbt/project\")\n    },\n)\n\n# new\ndbt_assets = load_assets_from_dbt_project(project_dir=\"my/dbt/project\")\n\ndefs = Definitions(\n    assets=dbt_assets,\n    resources={\n        \"dbt\": DbtCliResource(project_dir=\"my/dbt/project\")\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Code Servers Configuration in YAML\nDESCRIPTION: Configuration for code server timeout settings in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ncode_servers:\n  local_startup_timeout: 360\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local Postgres Database\nDESCRIPTION: Commands to set up a local Postgres database using Docker, including creating a replica database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_modern_data_stack/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ docker pull postgres\n$ docker run --name mds-demo -p 5432:5432 -e POSTGRES_PASSWORD=password -d postgres\n$ PGPASSWORD=password psql -h localhost -p 5432 -U postgres -d postgres -c \"CREATE DATABASE postgres_replica;\"\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 I/O Manager for Dagster Job\nDESCRIPTION: This YAML configuration specifies the S3 bucket to be used by the s3_io_manager resource for a Dagster job. It sets up the storage location for data passed between ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  io_manager:\n    config:\n      bucket: \"my-cool-bucket\"\n```\n\n----------------------------------------\n\nTITLE: Creating Task IAM Role Policy for ECS Agent in AWS\nDESCRIPTION: This JSON policy defines the permissions required for the Task IAM role that allows containers running in the ECS task to interact with AWS. It includes permissions to describe and launch ECS tasks, as well as interact with related AWS services.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/amazon-ecs/manual-provision.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeNetworkInterfaces\",\n        \"ec2:DescribeRouteTables\",\n        \"ecs:CreateService\",\n        \"ecs:DeleteService\",\n        \"ecs:DescribeServices\",\n        \"ecs:DescribeTaskDefinition\",\n        \"ecs:DescribeTasks\",\n        \"ecs:ListAccountSettings\",\n        \"ecs:ListServices\",\n        \"ecs:ListTagsForResource\",\n        \"ecs:ListTasks\",\n        \"ecs:RegisterTaskDefinition\",\n        \"ecs:RunTask\",\n        \"ecs:StopTask\",\n        \"ecs:TagResource\",\n        \"ecs:UpdateService\",\n        \"iam:PassRole\",\n        \"logs:GetLogEvents\",\n        \"secretsmanager:DescribeSecret\",\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:ListSecrets\",\n        \"servicediscovery:CreateService\",\n        \"servicediscovery:DeleteService\",\n        \"servicediscovery:ListServices\",\n        \"servicediscovery:GetNamespace\",\n        \"servicediscovery:ListTagsForResource\",\n        \"servicediscovery:TagResource\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Location Settings in YAML\nDESCRIPTION: Example configuration showing available fields for Kubernetes location settings in dagster_cloud.yaml, including environment variables, secrets, volumes, and Kubernetes-specific configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/configuration.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      k8s:\n        env_config_maps:\n          - my_config_map\n        env_secrets:\n          - my_secret\n        env_vars:\n          - FOO_ENV_VAR=foo_value\n          - BAR_ENV_VAR\n        image_pull_policy: Always\n        image_pull_secrets:\n          - name: my_image_pull_secret\n        labels:\n          my_label_key: my_label_value\n        namespace: my_k8s_namespace\n        service_account_name: my_service_account_name\n        volume_mounts:\n          - mount_path: /opt/dagster/test_mount_path/volume_mounted_file.yaml\n            name: test-volume\n            sub_path: volume_mounted_file.yaml\n        volumes:\n          - name: test-volume\n            config_map:\n              name: test-volume-configmap\n        server_k8s_config:\n          pod_spec_config:\n            node_selector:\n              disktype: standard\n          pod_template_spec_metadata:\n            annotations:\n              mykey: myvalue\n          deployment_metadata:\n            annotations:\n              mykey: myvalue\n          service_metadata:\n            annotations:\n              mykey: myvalue\n          container_config:\n            resources:\n              limits:\n                cpu: 100m\n                memory: 128Mi\n        run_k8s_config:\n          pod_spec_config:\n            node_selector:\n              disktype: ssd\n          container_config:\n            resources:\n              limits:\n                cpu: 500m\n                memory: 1024Mi\n          pod_template_spec_metadata:\n            annotations:\n              mykey: myvalue\n          job_spec_config:\n            ttl_seconds_after_finished: 7200\n          job_metadata:\n            annotations:\n              mykey: myvalue\n```\n\n----------------------------------------\n\nTITLE: Setting Default Limit for Concurrency Pools in YAML\nDESCRIPTION: This YAML snippet shows how to set a default limit for concurrency pools in the Dagster configuration. It's added to the dagster.yaml file or deployment settings in Dagster+.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  pools:\n    default_limit: 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Asset Factory with Dependencies in Python\nDESCRIPTION: Python implementation of an asset factory that processes YAML configurations to create Dagster assets with defined dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/defining-dependencies-with-asset-factories.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\nfrom dagster import asset\n\ndef load_table_definitions():\n    with open(\"table_definitions.yaml\", \"r\") as f:\n        return yaml.safe_load(f)[\"tables\"]\n\ndef create_table_assets():\n    table_definitions = load_table_definitions()\n    \n    # Create a dictionary mapping table names to their asset ops\n    table_assets = {}\n    \n    # For each table definition, create an asset\n    for table_def in table_definitions:\n        # Get the upstream dependencies for this table\n        deps = [table_assets[dep] for dep in table_def[\"dependencies\"]]\n        \n        # Create the asset\n        @asset(name=table_def[\"name\"], deps=deps)\n        def table_asset():\n            # Execute the query\n            query = table_def[\"query\"]\n            # ... execute the query ...\n            pass\n        \n        # Store the asset in our dictionary\n        table_assets[table_def[\"name\"]] = table_asset\n    \n    return list(table_assets.values())\n```\n\n----------------------------------------\n\nTITLE: Applying Hooks to Specific Ops in Dagster\nDESCRIPTION: This example demonstrates how to apply hooks to specific ops within a Dagster job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-hooks.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@op(hooks={slack_message_on_failure, slack_message_on_success})\ndef op_a():\n    pass\n\n@op\ndef op_b():\n    pass\n\n@job\ndef my_job():\n    op_a()\n    op_b()\n```\n\n----------------------------------------\n\nTITLE: Implementing Kubernetes Pod Execution as a Dagster Asset\nDESCRIPTION: Example of creating a Dagster asset that executes a task within a Kubernetes pod using the PipesK8sClient. This demonstrates how to define the pod specification and execute commands in Kubernetes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/kubernetes-pod-operator.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster_k8s import PipesK8sClient\n\n@asset\ndef run_in_k8s():\n    # Create the K8s client\n    client = PipesK8sClient(\n        kubeconfig=\"path/to/kubeconfig\",\n        kubecontext=\"my-context\",\n        env={\"AWS_PROFILE\": \"my-profile\"},\n    )\n    \n    # Define the pod specification\n    pod_spec = {\n        \"containers\": [\n            {\n                \"name\": \"main\",\n                \"image\": \"python:3.9\",\n                \"command\": [\"python\", \"-c\"],\n                \"args\": [\"print('Hello from Kubernetes!')\"]\n            }\n        ]\n    }\n    \n    # Execute the command in the pod\n    result = client.execute(\n        pod_spec=pod_spec,\n        namespace=\"default\",\n    )\n    \n    return result.stdout\n```\n\n----------------------------------------\n\nTITLE: Implementing a Run Success Sensor in Python with Dagster\nDESCRIPTION: This snippet demonstrates how to create a run status sensor that triggers on successful job runs. It uses the @run_status_sensor decorator and checks for the DagsterRunStatus.SUCCESS status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-run-status-sensors.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@run_status_sensor(run_status=DagsterRunStatus.SUCCESS)\ndef my_run_success_sensor(context):\n    email_alert(f\"Job {context.dagster_run.job_name} succeeded\")\n```\n\n----------------------------------------\n\nTITLE: Paginated Runs Query in GraphQL\nDESCRIPTION: GraphQL query to retrieve Dagster runs with pagination support using cursor and limit parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/index.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nquery PaginatedRunsQuery($cursor: String) {\n  runsOrError(\n    cursor: $cursor\n    limit: 10\n  ) {\n    __typename\n    ... on Runs {\n      results {\n        runId\n        jobName\n        status\n        runConfigYaml\n        startTime\n        endTime\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining External Kubernetes Code with Dagster Pipes in Python\nDESCRIPTION: This Python script uses dagster-pipes to interact with Dagster from within a Kubernetes container. It logs information, retrieves parameters, and reports asset materialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/kubernetes-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# my_python_script.py\n\nfrom dagster_pipes import open_dagster_pipes\n\nwith open_dagster_pipes() as pipes:\n    # Stream log message back to Dagster\n    pipes.log.info(f\"Using some_parameter value: {pipes.get_extra('some_parameter')}\")\n\n    # ... your code that computes and persists the asset\n\n    pipes.report_asset_materialization(\n        metadata={\n            \"some_metric\": {\"raw_value\": 2, \"type\": \"int\"}\n        },\n        data_version=\"alpha\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Source in YAML\nDESCRIPTION: Example of defining a dbt source in a sources.yml file. This defines a source named 'jaffle_shop' with a table named 'orders'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: jaffle_shop\n    tables:\n      - name: orders\n```\n\n----------------------------------------\n\nTITLE: Customizing Fivetran Asset Materialization\nDESCRIPTION: Code demonstrating how to customize Fivetran connector syncs using the fivetran_assets decorator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npath=\"docs_snippets/docs_snippets/integrations/fivetran/customize_fivetran_asset_defs.py\"\n```\n\n----------------------------------------\n\nTITLE: Nesting Graphs in Dagster (Python)\nDESCRIPTION: This example shows how to nest graphs in Dagster by creating a sub-graph for Celsius to Fahrenheit conversion and using it within the main job graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@graph\ndef celsius_to_fahrenheit(number):\n    return add_thirty_two(multiply_by_one_point_eight(number))\n\n@job\ndef all_together_nested():\n    log_fahrenheit(celsius_to_fahrenheit(return_five()))\n```\n\n----------------------------------------\n\nTITLE: Importing EcsRunLauncher for AWS Deployment\nDESCRIPTION: This code imports the EcsRunLauncher, which is used for launching an Amazon ECS task per run when deploying Dagster to Amazon Web Services.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-launchers.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_aws.ecs import EcsRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Logging Basic Asset Observation\nDESCRIPTION: Example showing how to log a basic asset observation event from within an op using the context's log_event method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/asset-observations.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef observe_my_asset(context):\n    context.log_event(\n        AssetObservation(\n            asset_key=AssetKey(\"my_table\"),\n            description=\"Found 100 rows in my_table\"\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Nothing Dependencies in a Graph\nDESCRIPTION: Illustrates how to model explicit ordering dependencies using the Nothing type, without passing actual data between ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, graph, Nothing\n\n@op\ndef create_table_1():\n    # Create the first table\n    ...\n\n@op\ndef create_table_2(_dep=In(Nothing)):\n    # Create the second table\n    ...\n\n@graph\ndef create_tables_graph():\n    create_table_2(create_table_1())\n```\n\n----------------------------------------\n\nTITLE: Basic Integer Addition in Python\nDESCRIPTION: Performs addition of two integer literals (1 + 1) and assigns the result to variable x.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/integrations/dagstermill/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nx = 1 + 1\n```\n\n----------------------------------------\n\nTITLE: Configuring RetryPolicy with Parameters\nDESCRIPTION: Enhanced retry policy configuration with specific retry attempts and delay settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-retries.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@op(retry_policy=RetryPolicy(max_retries=3, delay=1.0))\ndef my_op():\n    if random() > 0.5:\n        raise Exception(\"Random failure!\")\n    return \"success\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Teradata Compute Cluster with dagster-teradata\nDESCRIPTION: This operation removes a compute cluster from Teradata VantageCloud Lake, with an option to delete the associated compute group. It can be executed directly from a Dagster workflow, allowing for complete lifecycle management of compute resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndrop_teradata_compute_cluster(\n    compute_profile_name: str,\n    compute_group_name: str,\n    delete_compute_group: bool = False\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgresScheduleStorage in Dagster\nDESCRIPTION: Autoconfigurable class for PostgreSQL schedule storage in Dagster. It provides options to configure PostgreSQL database connection and usage for storing schedule-related data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-postgres.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autoconfigurable:: PostgresScheduleStorage\n```\n\n----------------------------------------\n\nTITLE: Updating Dagster Asset to Use Observable Source Asset\nDESCRIPTION: This example demonstrates how to update a Dagster asset to use an observable source asset as an upstream dependency and read its value from a file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@asset(non_argument_deps={\"input_number\"})\ndef versioned_number():\n    with open(\"input_number.txt\", \"r\") as f:\n        result = int(f.read())\n    return Output(value=result, metadata={\"data_version\": DataVersion(str(result))})\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using SSH/SFTP Resources in Dagster\nDESCRIPTION: Example demonstrating how to configure and use SSH and SFTP resources in a Dagster job. It includes resource definitions, job configuration, and operations that utilize these resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/ssh-sftp.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, resource\nfrom dagster_ssh import ssh_resource, sftp_resource\n\n@resource(config_schema={\"remote_host\": str, \"remote_port\": int})\ndef my_ssh_resource(context):\n    return ssh_resource(\n        remote_host=context.resource_config[\"remote_host\"],\n        remote_port=context.resource_config[\"remote_port\"],\n    )\n\n@resource(config_schema={\"remote_host\": str, \"remote_port\": int})\ndef my_sftp_resource(context):\n    return sftp_resource(\n        remote_host=context.resource_config[\"remote_host\"],\n        remote_port=context.resource_config[\"remote_port\"],\n    )\n\n@op(required_resource_keys={\"ssh\"})\ndef ssh_op(context):\n    ssh_client = context.resources.ssh\n    stdin, stdout, stderr = ssh_client.exec_command(\"ls -la\")\n    context.log.info(stdout.read().decode())\n\n@op(required_resource_keys={\"sftp\"})\ndef sftp_op(context):\n    sftp_client = context.resources.sftp\n    sftp_client.put(\"local_file.txt\", \"remote_file.txt\")\n\n@job(resource_defs={\"ssh\": my_ssh_resource, \"sftp\": my_sftp_resource})\ndef my_job():\n    ssh_op()\n    sftp_op()\n\nmy_job.execute_in_process(\n    run_config={\n        \"resources\": {\n            \"ssh\": {\"config\": {\"remote_host\": \"example.com\", \"remote_port\": 22}},\n            \"sftp\": {\"config\": {\"remote_host\": \"example.com\", \"remote_port\": 22}},\n        }\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Model Inference on New Data using Dagster\nDESCRIPTION: Performs inference on new data using the trained model, predicting comment counts for new stories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/ml-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset(deps=[recent_stories, regressor_model, vectorized_data])\ndef predictions(recent_stories, regressor_model, vectorized_data) -> pd.DataFrame:\n    X = vectorized_data.vectorizer.transform(recent_stories[\"title\"]).toarray()\n    y_pred = regressor_model.regressor.predict(X)\n    recent_stories[\"predicted_comments\"] = y_pred\n    return recent_stories\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Docker Containers in Dagster+\nDESCRIPTION: This YAML configuration demonstrates how to include environment variables and secrets in Docker containers associated with a specific code location. Environment variables can be specified as just a key (which pulls the value from the local environment) or as key-value pairs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/docker/configuration.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      docker:\n        env_vars:\n          - DATABASE_NAME\n          - DATABASE_USERNAME=hooli_testing\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Definitions Object in Python\nDESCRIPTION: This snippet creates a Dagster Definitions object that includes the subprocess asset and the PipesSubprocessClient resource, making them accessible to Dagster tools.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/create-subprocess-asset.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[subprocess_asset],\n    resources={\n        \"pipes_subprocess_client\": PipesSubprocessClient(),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting Sink Assets Depending on a Group in Dagster\nDESCRIPTION: Shows how to select sink assets that depend on assets in the 'public_data' group but do not belong to that group. This syntax uses the 'sinks' function with a '+' operator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nsinks(group:\"public_data\"+)\n```\n\nLANGUAGE: python\nCODE:\n```\nsinks_feed_to_public_data_job = define_asset_job(\n    name=\"sinks_feed_to_public_data_job\", selection='sinks(group:\"public_data\"+)'\n)\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster asset list --select 'sinks(group:\"public_data\"+)'\ndagster asset materialize --select 'sinks(group:\"public_data\"+)'\n```\n\n----------------------------------------\n\nTITLE: Updating dbt Model Query\nDESCRIPTION: SQL query modification to use the newly defined source instead of the original seed data for the staging customers model.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/upstream-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith source as (\n\n    {#-\n    Use source instead of seed:\n    #}\n    select * from {{ source('jaffle_shop', 'raw_customers') }}\n\n),\n\nrenamed as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from source\n\n)\n\nselect * from renamed\n```\n\n----------------------------------------\n\nTITLE: Specifying Executor on Dagster Job\nDESCRIPTION: This snippet demonstrates how to specify an executor directly on a Dagster job using the 'executor_def' parameter of the @job decorator or GraphDefinition.to_job() method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/run-executors.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@job(executor_def=in_process_executor)\ndef my_job():\n    ...\n\n# or\n\nmy_job = GraphDefinition(...).to_job(executor_def=in_process_executor)\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Lake PyArrow I/O Manager in Python\nDESCRIPTION: Shows how to configure the Delta Lake PyArrow I/O Manager in a Dagster Definitions object for use with PyArrow Tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_deltalake import DeltaLakePyArrowIOManager\n\ndefs = Definitions(\n    assets=[iris_dataset],\n    resources={\n        \"io_manager\": DeltaLakePyArrowIOManager(\n            root_uri=\"s3://my-bucket/my-deltalake\"\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Organizing Dagster Assets with Tags in Python\nDESCRIPTION: This example shows how to attach tags to a Dagster asset for organization and filtering purposes. Tags are key-value pairs of strings passed to the 'tags' argument when defining an asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(tags={\"domain\": \"marketing\", \"pii\": \"true\"})\ndef marketing_asset():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Run Monitoring in dagster.yaml\nDESCRIPTION: YAML configuration for enabling run monitoring in a Dagster instance with timeout settings for run startup and cancellation processes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-monitoring.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrun_monitoring:\n  enabled: true\n  start_timeout_seconds: 60\n  cancel_timeout_seconds: 60\n```\n\n----------------------------------------\n\nTITLE: Initializing GitHub Resource with Token in Python\nDESCRIPTION: This code snippet demonstrates how to initialize the GithubResource using a GitHub token. The token is retrieved from an environment variable for security purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/sources.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ngithub_resource = GithubResource(github_token=os.environ[\"GITHUB_TOKEN\"])\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Upstream Partitions in Python\nDESCRIPTION: This snippet shows how to load multiple upstream partitions using the default partition mapping in Dagster. It defines assets with partitions and demonstrates accessing multiple partition values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/io-managers/defining-a-custom-io-manager.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2023-01-01\"))\ndef upstream_asset():\n    return 1\n\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2023-01-01\"))\ndef downstream_asset(upstream_asset: Dict[str, int]):\n    # upstream_asset is a dict mapping partition keys to their values\n    return sum(upstream_asset.values())\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Power BI Workspaces\nDESCRIPTION: Example showing how to combine assets from multiple Power BI workspaces in a single Dagster asset graph\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/powerbi.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nworkspace_1 = PowerBIWorkspace(\n    workspace_id=\"workspace-1-id\",\n    tenant_id=\"tenant-id\",\n    client_id=\"client-id\",\n    client_secret=EnvVar(\"POWERBI_CLIENT_SECRET_1\"),\n)\n\nworkspace_2 = PowerBIWorkspace(\n    workspace_id=\"workspace-2-id\",\n    tenant_id=\"tenant-id\",\n    client_id=\"client-id\",\n    client_secret=EnvVar(\"POWERBI_CLIENT_SECRET_2\"),\n)\n\ndefs = Definitions(\n    assets=[\n        *load_powerbi_asset_specs(workspace_1),\n        *load_powerbi_asset_specs(workspace_2),\n    ],\n    resources={\n        \"power_bi_1\": workspace_1,\n        \"power_bi_2\": workspace_2,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Cursor Implementation for High-Volume Events\nDESCRIPTION: Demonstrates using cursors to optimize sensor performance when dealing with large numbers of events, tracking the last processed state.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/index.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/automation/sensor-cursor.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Creating Downstream Asset\nDESCRIPTION: Example of creating a downstream asset that depends on the iris_dataset table\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset\ndef iris_setosa(duckdb, iris_dataset):\n    duckdb.execute_query(\n        \"\"\"CREATE TABLE IF NOT EXISTS iris.iris_setosa AS\n        SELECT * FROM iris.iris_dataset\n        WHERE species = 0\n        \"\"\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Logging Expectation Results in Dagster Ops\nDESCRIPTION: This snippet demonstrates how to log Expectation Results in Dagster ops, representing the results of a data quality test.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, ExpectationResult\n\n@op\ndef my_op():\n    df = pd.read_csv(\"data.csv\")\n    \n    # assert df.shape[0] > 0\n    context.log_event(\n        ExpectationResult(\n            success=df.shape[0] > 0,\n            description=\"Dataframe has rows\",\n            metadata={\"num_rows\": df.shape[0]},\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Outputting Dagster Logs to a File\nDESCRIPTION: Example of a Dagster job that uses the configured file handler to output logs to a file. This demonstrates how the custom log configuration is applied.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/python-logging.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@job\ndef my_job():\n    @op\n    def my_op(context):\n        context.log.info(\"Hello, world!\")\n\n    my_op()\n```\n\n----------------------------------------\n\nTITLE: Configuring Celery Executor for Dagster Job (Python)\nDESCRIPTION: Python code snippet demonstrating how to set up a Dagster job to use the Celery executor.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job\nfrom dagster_celery import celery_executor\n\n@job(executor_def=celery_executor)\ndef my_job():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Enabling isolated agents across multiple Kubernetes clusters\nDESCRIPTION: YAML configuration for enabling isolated agents to allow work distribution across multiple Kubernetes clusters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\n# values.yaml\nisolatedAgents:\n  enabled: true\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project with PySpark Example\nDESCRIPTION: Command-line instruction to create a new Dagster project using the PySpark example template. This helps developers quickly set up a starter project with pre-configured Spark integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_pyspark/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example with_pyspark\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory and Virtual Environment\nDESCRIPTION: Commands to create a new directory for the Dagster project and set up a virtual environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/quickstart.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir dagster-quickstart\ncd dagster-quickstart\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Configuring DLT Pipeline in Python\nDESCRIPTION: Definition of a dlt pipeline specifying the pipeline name, destination, and dataset configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = dlt.pipeline(\n    pipeline_name=\"example_pipeline\",\n    destination=\"snowflake\",\n    dataset_name=\"example_data\",\n    progress=\"log\",\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing ReplicationTranslator Class in Python\nDESCRIPTION: Defines a ReplicationTranslator class with a method to map table specifications to Dagster asset keys. This class serves as a foundation for customizing asset attributes in the integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, _check as check\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass ReplicationTranslator:\n    @public\n    def get_asset_key(self, table_definition: Mapping[str, str]) -> AssetKey:\n        return AssetKey(str(table_definition.get(\"name\")))\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Settings for Every Step in Python\nDESCRIPTION: This snippet demonstrates how to use the 'step_k8s_config' field on the k8s_job_executor to set Kubernetes configuration for every step pod. It includes settings for container resources, volumes, and metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@job(\n    executor_def=k8s_job_executor.configured(\n        {\n            \"step_k8s_config\": {\n                \"container_config\": {\n                    \"resources\": {\n                        \"requests\": {\"cpu\": \"250m\", \"memory\": \"64Mi\"},\n                        \"limits\": {\"cpu\": \"500m\", \"memory\": \"2560Mi\"},\n                    }\n                },\n                \"pod_spec_config\": {\n                    \"volumes\": [\n                        {\"name\": \"my-volume\", \"emptyDir\": {}}\n                    ]\n                },\n                \"pod_template_spec_metadata\": {\n                    \"annotations\": {\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\"}\n                },\n            }\n        }\n    )\n)\ndef my_job():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Fetching Materializations from Dagster OSS (Python)\nDESCRIPTION: This snippet shows how to fetch materializations for a specific asset from the Dagster OSS instance using the fetch_materializations method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/oss-metadata-to-plus/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult: EventRecordsResult = context.instance.fetch_materializations(\n    records_filter=AssetKey.from_user_string(asset_key),\n    limit=1,\n    cursor=cursor,\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Tags for dbt Nodes in Dagster\nDESCRIPTION: This snippet demonstrates how to create a custom DagsterDbtTranslator to override Dagster tags for all dbt nodes in a project. It converts dbt tags of the form 'foo=bar' to key/value pairs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass CustomDbtTranslator(DagsterDbtTranslator):\n    def get_tags(self, dbt_resource_props: Mapping[str, Any]) -> Mapping[str, str]:\n        tags = {}\n        for tag in dbt_resource_props.get(\"tags\", []):\n            if \"=\" in tag:\n                key, value = tag.split(\"=\")\n                tags[key] = value\n        return tags\n```\n\n----------------------------------------\n\nTITLE: Visualizing Project Structure for Airflow to Dagster Migration\nDESCRIPTION: This code snippet illustrates the directory structure of the tutorial project. It shows the organization of shared code, Dagster definitions, and Airflow DAGs, providing a clear overview of the project layout for the migration process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/airlift-migration-tutorial/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntutorial_example\n├── shared: Contains shared Python & SQL code used Airflow and proxied Dagster code\n│\n├── dagster_defs: Contains Dagster definitions\n│   ├── stages: Contains reference implementations of each stage of the migration process\n│   ├── definitions.py: Empty starter file for following along with the tutorial\n│\n├── airflow_dags: Contains the Airflow DAG and associated files\n│   ├── proxied_state: Contains migration state files for each DAG, see migration step below\n│   ├── dags.py: The Airflow DAG definition\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-embedded-elt Package\nDESCRIPTION: Command to install the dagster-embedded-elt package using pip. This package provides integrations with Sling and dlt for building ELT pipelines in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/embedded-elt.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-embedded-elt\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables with os.getenv in Dagster\nDESCRIPTION: A Python code snippet demonstrating how to access environment variables in Dagster code using the os.getenv method. This retrieves the DATABASE_NAME variable from the environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ndatabase_name = os.getenv(\"DATABASE_NAME\")\n```\n\n----------------------------------------\n\nTITLE: Generating Dagster Schedule Script with CLI\nDESCRIPTION: This command uses the Dagster CLI to scaffold a new schedule script named 'daily_jaffle.py'. The 'dg scaffold' command is used to create a template for a Dagster schedule, which can be customized for specific scheduling needs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/35-scaffold-daily-jaffle.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold dagster.schedule daily_jaffle.py\n```\n\n----------------------------------------\n\nTITLE: Displaying Local Development Project Structure with Multiple Code Locations\nDESCRIPTION: This snippet shows the file structure for a Dagster project with multiple code locations during local development. It includes the workspace.yaml file for defining multiple code locations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/dagster-project-file-reference.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n.\n├── README.md\n├── my_dagster_project\n│   ├── __init__.py\n│   ├──  assets.py\n│   └──  definitions.py\n├── my_dagster_project_tests\n├── dagster.yaml      ## optional, used for instance settings\n├── pyproject.toml\n├── setup.cfg\n├── setup.py\n├── tox.ini\n└── workspace.yaml    ## defines multiple code locations\n```\n\n----------------------------------------\n\nTITLE: Defining Hacker News Data Assets in Python\nDESCRIPTION: Python code defining three assets for Hacker News data: items (full dataset), comments, and stories. Uses Pandas DataFrames for data manipulation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef items():\n    url = \"https://hacker-news.firebaseio.com/v0/newstories.json\"\n    top_500_ids = requests.get(url).json()\n    items = []\n    for item_id in top_500_ids[:100]:\n        item = requests.get(\n            f\"https://hacker-news.firebaseio.com/v0/item/{item_id}.json\"\n        ).json()\n        items.append(item)\n\n    return pd.DataFrame(items)\n\n\n@asset\ndef comments(items: pd.DataFrame):\n    return items[items[\"type\"] == \"comment\"].drop(\"type\", axis=1)\n\n\n@asset\ndef stories(items: pd.DataFrame):\n    return items[items[\"type\"] == \"story\"].drop(\"type\", axis=1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Available Fuel Stations Asset in Python\nDESCRIPTION: Dagster asset that processes fuel station data, generates prompts for each station, and determines their availability using an AI model. Returns a list of available stations with their distances.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/additional-prompt.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\n@asset(\n    deps=[\"parsed_user_input\", \"fuel_stations\"]\n)\ndef available_fuel_stations(context, parsed_user_input, fuel_stations):\n    \"\"\"Get the fuel stations that are currently available.\"\"\"\n    current_time = datetime.now().strftime(\"%I:%M %p\")\n    \n    available_stations = []\n    for station in fuel_stations:\n        prompt = FUEL_STATION_PROMPT.format(\n            current_time=current_time,\n            hours_of_operation=station[\"hours_of_operation\"]\n        )\n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        is_open = json.loads(response.choices[0].message.content)[\"is_open\"]\n        if is_open:\n            available_stations.append(station)\n    \n    for station in available_stations:\n        context.log.info(\n            f\"{station['station_name']} at {station['street_address']} is {station['distance']} miles away\"\n        )\n\n```\n\n----------------------------------------\n\nTITLE: Customizing Upstream Dependencies for Looker Assets\nDESCRIPTION: Python code showing how to customize upstream dependencies for Looker assets using a custom DagsterLookerApiTranslator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/looker.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CustomLookerApiTranslator(DagsterLookerApiTranslator):\n    def translate_view(self, view):\n        asset_spec = super().translate_view(view)\n        if view.name == \"my_looker_view\":\n            asset_spec.upstream_assets.append(\"my_upstream_asset\")\n        return asset_spec\n\nlooker_asset_specs = load_looker_asset_specs(\n    looker_resource,\n    api_translator=CustomLookerApiTranslator(),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Non-Isolated Runs in Dagster+\nDESCRIPTION: Controls settings for non-isolated runs including enabling/disabling the feature and setting concurrent run limits. Helps manage resource utilization on the code location server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/deployment-settings-reference.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nnon_isolated_runs:\n  enabled: True\n  max_concurrent_non_isolated_runs: 1\n```\n\n----------------------------------------\n\nTITLE: Starting Celery Worker\nDESCRIPTION: Command to start a Celery worker that will execute Dagster tasks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/celery.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndagster-celery worker start -A dagster_celery.app\n```\n\n----------------------------------------\n\nTITLE: Defining External Delta Lake Asset\nDESCRIPTION: Configuration for making existing Delta Lake tables available as Dagster assets\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/using-deltalake-with-dagster.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsource_assets = build_source_assets(\n    [\n        AssetSpec(\n            group_name=\"iris\",\n            key=\"iris_harvest_data\",\n        )\n    ],\n    io_manager_key=\"io_manager\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining In Latest Time Window Scheduling Condition in Dagster\nDESCRIPTION: This snippet allows users to set a scheduling condition based on whether an asset is in the latest time window, enabling fresh data processing as assets are updated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.in_latest_time_window()\n```\n\n----------------------------------------\n\nTITLE: Displaying SQL Configuration Settings in Python\nDESCRIPTION: This Python code snippet illustrates how to display the entire list of SQL configuration settings using an existing SparkSession.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n# spark is an existing SparkSession\n\nspark.sql(\"SET -v\").show(n=200, truncate=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hacker News API Client Resource in Dagster\nDESCRIPTION: Demonstrates the implementation of a real Hacker News API client as a Dagster resource to be used by assets and ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstart_resource\n```\n\n----------------------------------------\n\nTITLE: Defining Root-Level Resources in Dagster\nDESCRIPTION: Example demonstrating how to define resources at the root level of a Dagster project. Shows resource definition structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/using-resources.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/dg/using-resources/2-resources-at-defs-root.py\" />\n```\n\n----------------------------------------\n\nTITLE: Disabling Telemetry in Dagster Configuration\nDESCRIPTION: YAML configuration to disable telemetry collection in Dagster by adding to the dagster.yaml file in the DAGSTER_HOME directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/telemetry.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntelemetry:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring EcsRunLauncher with Custom Task Definition\nDESCRIPTION: This YAML configuration sets up the EcsRunLauncher with a custom task definition for launching Dagster runs in ECS. It specifies the task definition ARN and container name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: 'dagster_aws.ecs'\n  class: 'EcsRunLauncher'\n  config:\n    task_definition: 'arn:aws:ecs:us-east-1:1234567890:task-definition/my-task-definition:1'\n    container_name: 'my_container_name'\n```\n\n----------------------------------------\n\nTITLE: Defining a Partitioned Asset in Dagster OSS (Python)\nDESCRIPTION: This snippet demonstrates how to define a daily partitioned asset in Dagster OSS. The asset generates random metadata for each materialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/oss-metadata-to-plus/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2023-10-01\"))\ndef my_daily_partitioned_asset() -> MaterializeResult:\n    some_metadata_value = random.randint(0, 100)\n\n    return MaterializeResult(metadata={\"foo\": some_metadata_value})\n```\n\n----------------------------------------\n\nTITLE: Configuring Asset Checks with Automation Conditions\nDESCRIPTION: Shows how to configure asset checks with automation conditions using either the @asset_check decorator or AssetCheckSpec object. Includes examples of daily cron scheduling for checks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset_check(asset=dg.AssetKey(\"orders\"), automation_condition=dg.AutomationCondition.on_cron(\"@daily\"))\ndef my_eager_check() -> dg.AssetCheckResult:\n    return dg.AssetCheckResult(passed=True)\n\n\ndg.AssetCheckSpec(\n    \"my_cron_check\",\n    asset=dg.AssetKey(\"orders\"),\n    automation_condition=dg.AutomationCondition.on_cron(\"@daily\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Ending Partition in Partition Set in Python\nDESCRIPTION: This snippet shows how to customize the ending partition in a partition set using the end_offset parameter in the partition's config in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/constructing-schedules-for-partitioned-assets-and-jobs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DailyPartitionsDefinition\n\nmy_partitions_def = DailyPartitionsDefinition(\n    start_date=\"2024-05-01\",\n    end_offset=1\n)\n```\n\n----------------------------------------\n\nTITLE: Final Definitions Configuration\nDESCRIPTION: Shows the final definitions.py file using complete autoloading.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import load_defs\n\ndefs = load_defs()\n```\n\n----------------------------------------\n\nTITLE: Creating Table event_logs in PostgreSQL\nDESCRIPTION: Defines the 'event_logs' table with columns for storing event data, including IDs and timestamps, primarily for logging both Dagster events and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key character varying,\n    asset_key character varying\n);\n\nALTER TABLE public.event_logs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Basic Sensor Testing in Python\nDESCRIPTION: Example of testing a sensor by directly invoking its Python function and validating run configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-sensors.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = my_sensor_definition.evaluator.evaluate_sensor() \nrun_requests = list(result.run_requests)\nassert len(run_requests) == 1\n\nvalidate_run_config(my_job, run_requests[0].run_config)\n```\n\n----------------------------------------\n\nTITLE: Defining External BigQuery Table Assets\nDESCRIPTION: Shows how to create an external asset definition for existing BigQuery tables, enabling Dagster to track data lineage and manage dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsource_assets = [\n    AssetSpec(\n        \"iris_harvest_data\",\n        description=\"Raw data from iris harvests stored in BigQuery\",\n        key=\"IRIS_HARVEST_DATA\",\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Yielding Asset Materializations in Dagster Ops\nDESCRIPTION: This example shows how to yield asset materializations in Dagster ops, along with yielding outputs via an Output object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-events.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, AssetMaterialization, AssetKey, Output\n\n@op\ndef my_op():\n    # ... persist some data to a file at a known path\n    path = \"/path/to/file.csv\"\n    file_size = 1337\n    yield AssetMaterialization(\n        asset_key=AssetKey(\"my_dataset\"),\n        description=\"Persisted data to a file.\",\n        metadata={\"file_path\": path, \"size (bytes)\": file_size},\n    )\n    yield Output(path)\n```\n\n----------------------------------------\n\nTITLE: Schema and Table Creation in PostgreSQL\nDESCRIPTION: This SQL snippet initializes a schema named 'test-schema' with ownership assigned to 'test'. It defines various tables across different schemas, including 'public' and 'test-schema', to handle functionalities like asset key management, event logging, job handling, and snapshots. Each table specifies columns, types, and default values as necessary.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: test-schema; Type: SCHEMA; Schema: -; Owner: test\nCREATE SCHEMA \"test-schema\";\n\nALTER SCHEMA \"test-schema\" OWNER TO test;\n\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\nALTER TABLE public.alembic_version OWNER TO test;\n\n-- Name: asset_keys; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    wipe_timestamp timestamp without time zone,\n    last_materialization_timestamp timestamp without time zone,\n    tags text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.asset_keys OWNER TO test;\n\n-- Further table and sequence definitions follow\n```\n\n----------------------------------------\n\nTITLE: Refreshing and Materializing Tableau Assets\nDESCRIPTION: Example showing how to refresh Tableau workbooks and materialize Tableau sheets and dashboards using Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/tableau.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_tableau import TableauCloudWorkspace, load_tableau_asset_specs\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Outputs in Dagster Graphs (Python)\nDESCRIPTION: This example shows how to define multiple outputs from a Dagster graph by returning a dictionary with named outputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@graph\ndef celcius_conversions(num):\n    fahrenheit = celcius_to_fahrenheit(num)\n    kelvin = celcius_to_kelvin(num)\n    return {\"fahrenheit\": fahrenheit, \"kelvin\": kelvin}\n```\n\n----------------------------------------\n\nTITLE: Adding Upstream Dependencies to Dagster Definitions\nDESCRIPTION: This Python code extends the Dagster definitions to include an upstream asset that will be used by downstream dbt models. It demonstrates how to create a raw data asset that dbt will use as a source.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom dagster import asset, Definitions\nfrom dagster_dbt import DbtCliResource, dbt_assets\n\n@asset\ndef raw_customers():\n    return pd.DataFrame(\n        {\n            \"customer_id\": [\"1\", \"2\", \"3\"],\n            \"first_name\": [\"Rachel\", \"Monica\", \"Ross\"],\n            \"last_name\": [\"Green\", \"Geller\", \"Geller\"],\n        }\n    )\n\ndbt_resource = DbtCliResource(project_dir=\"./basic-dbt-project\")\n\n@dbt_assets(manifest=dbt_resource.get_manifest())\ndef my_dbt_assets():\n    pass\n\ndefs = Definitions(\n    assets=[raw_customers, my_dbt_assets],\n    resources={\"dbt\": dbt_resource}\n)\n```\n\n----------------------------------------\n\nTITLE: Obtaining Dagstermill Execution Context in Python\nDESCRIPTION: This code snippet gets the execution context using Dagstermill. The context provides access to configuration and resources specified in the Dagster pipeline, allowing for dynamic adjustments in the Jupyter notebook.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_config.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncontext = dagstermill.get_context()\n```\n\n----------------------------------------\n\nTITLE: Configuring Execution Policies for Dagster Ops in Python\nDESCRIPTION: These snippets show classes used for configuring execution policies for Ops, including retry policies, backoff strategies, and jitter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/ops.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: RetryPolicy\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Backoff\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Jitter\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with pytest\nDESCRIPTION: Executes the unit tests located in the dagster_university_tests directory using the pytest framework.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_du_dbt_starter/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest dagster_university_tests\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom S3 Endpoint\nDESCRIPTION: Configuration for non-AWS S3 implementations by specifying a custom endpoint URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconfig = S3Config(endpoint=\"https://<my-s3-endpoint-url>\")\n```\n\n----------------------------------------\n\nTITLE: Testing Run Failure Sensor with Context in Python using Dagster\nDESCRIPTION: This example demonstrates how to execute a failing job, retrieve the failure event, and build a context for testing a run failure sensor. It uses build_run_status_sensor_context and for_run_failure to create the appropriate context object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-run-status-sensors.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import build_run_status_sensor_context\n\nresult = fail_job.execute_in_process(raise_on_error=False)\njob_failure_event = result.get_job_failure_event()\n\ncontext = build_run_status_sensor_context(\n    sensor_name=\"my_run_failure_sensor\",\n    dagster_instance=result.instance,\n    run=result.dagster_run,\n    job_state=job_failure_event.job_state,\n    run_status=DagsterRunStatus.FAILURE,\n).for_run_failure()\n\nmy_run_failure_sensor(context)\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon ECS Agent Replicas\nDESCRIPTION: CloudFormation template configuration for setting up multiple Amazon ECS agent replicas.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/multiple.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nDagsterCloudAgent:\n  Type: AWS::ECS::Service\n  Properties:\n    ...\n    DesiredCount: 2\n```\n\n----------------------------------------\n\nTITLE: Implementing DataFrame Level Validation with Row Count Constraint\nDESCRIPTION: Example of adding DataFrame-level constraints using RowCountConstraint to validate the number of rows in the DataFrame.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pandas.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nshape_constrained_trip_dataframe = create_dagster_pandas_dataframe_type(\n    name=\"ShapeConstrainedTripDataFrame\",\n    columns=[\n        PandasColumn.integer_column(\"bike_id\", min_value=0),\n        PandasColumn.integer_column(\"payment_id\", min_value=0),\n        PandasColumn.string_column(\"payment_type\"),\n        PandasColumn.float_column(\"amount\", min_value=0),\n        PandasColumn.datetime_column(\"started_at\"),\n        PandasColumn.datetime_column(\"ended_at\"),\n    ],\n    dataframe_constraints=[RowCountConstraint(10)],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating GitHub Issues with Dagster\nDESCRIPTION: Example showing how to create GitHub issues using the GithubResource in a Dagster job. Demonstrates configuring the GitHub app credentials and creating a basic issue.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-github.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom dagster import job, op\nfrom dagster_github import GithubResource\n\n\n@op\ndef github_op(github: GithubResource):\n    github.get_client().create_issue(\n        repo_name='dagster',\n        repo_owner='dagster-io',\n        title='Dagster\\'s first github issue',\n        body='this open source thing seems like a pretty good idea',\n    )\n\n@job(resource_defs={\n     'github': GithubResource(\n         github_app_id=os.getenv('GITHUB_APP_ID'),\n         github_app_private_rsa_key=os.getenv('GITHUB_PRIVATE_KEY'),\n         github_installation_id=os.getenv('GITHUB_INSTALLATION_ID')\n )})\ndef github_job():\n    github_op()\n\ngithub_job.execute_in_process()\n```\n\n----------------------------------------\n\nTITLE: Passing Additional Arguments with Dynamic Outputs in Python with Dagster\nDESCRIPTION: This example shows how to use a lambda or scoped function to pass non-dynamic outputs alongside dynamic ones in the map function of Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef downstream_op(dynamic_input, static_input):\n    return dynamic_input + static_input\n\n@job\ndef job_with_additional_args():\n    dynamic_outs = dynamic_outputs()\n    static_value = 5\n    results = dynamic_outs.map(lambda x: downstream_op(x, static_value))\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Runtime for All Runs\nDESCRIPTION: YAML configuration that sets a maximum runtime of 2 hours (7200 seconds) for all runs in a Dagster deployment, after which the runs will be marked as failed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-monitoring.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nrun_monitoring:\n  enabled: true\n  max_runtime_seconds: 7200\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Asset for Subprocess Execution in Python\nDESCRIPTION: This code defines a Dagster asset that uses the PipesSubprocessClient to execute external code in a subprocess. It includes setting up the command and invoking the subprocess.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/create-subprocess-asset.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(required_resource_keys={\"pipes_subprocess_client\"})\ndef subprocess_asset(context: AssetExecutionContext):\n    cmd = [\n        shutil.which(\"python\"),\n        os.path.join(os.path.dirname(__file__), \"external_code.py\"),\n    ]\n\n    with context.resources.pipes_subprocess_client.run(cmd) as invocation:\n        result = invocation.get_materialize_result()\n        return result\n```\n\n----------------------------------------\n\nTITLE: Using Dagster PipesK8sClient\nDESCRIPTION: Example of using the PipesK8sClient in Dagster to execute tasks within Kubernetes pods, providing equivalent functionality to Airflow's KubernetesPodOperator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/kubernetes-pod-operator.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The snippet shows how to use PipesK8sClient in Dagster\n```\n\n----------------------------------------\n\nTITLE: Multiple Fivetran Workspaces\nDESCRIPTION: Code demonstrating how to combine definitions from multiple Fivetran workspaces in a single asset graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npath=\"docs_snippets/docs_snippets/integrations/fivetran/multiple_fivetran_workspaces.py\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Test Job for Run Status Sensor in Python with Dagster\nDESCRIPTION: This code creates a simple Dagster job that always succeeds, to be used for testing run status sensors. It defines an op and a job using the @op and @job decorators.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-run-status-sensors.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op():\n    return 1\n\n@job\ndef my_job():\n    my_op()\n```\n\n----------------------------------------\n\nTITLE: Implementing Partitioned DLT Assets\nDESCRIPTION: Shows how to implement static named partitions with DLT assets, demonstrating partition configuration and handling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import StaticPartitionsDefinition\nfrom dagster_dlt import dlt_assets\n\nregions = StaticPartitionsDefinition([\"us\", \"eu\", \"asia\"])\n\n@dlt_assets(partitions_def=regions)\ndef load_data_by_region():\n    pipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\", dataset_name=\"example\")\n    data = get_data_for_region()\n    info = pipeline.run(data)\n    return info\n```\n\n----------------------------------------\n\nTITLE: Defining a Dagster Job Without Nesting (Python)\nDESCRIPTION: This snippet demonstrates a basic Dagster job without using nested graphs. It converts a number from Celsius to Fahrenheit using a series of ops.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op\n\n@op\ndef return_five():\n    return 5\n\n@op\ndef multiply_by_one_point_eight(number):\n    return number * 1.8\n\n@op\ndef add_thirty_two(number):\n    return number + 32\n\n@op\ndef log_fahrenheit(number):\n    print(f\"Fahrenheit: {number}\")\n\n@job\ndef all_together():\n    log_fahrenheit(add_thirty_two(multiply_by_one_point_eight(return_five())))\n```\n\n----------------------------------------\n\nTITLE: Using BigQueryPySparkIOManager with spark_resource in Dagster\nDESCRIPTION: Demonstrates how to use the BigQueryPySparkIOManager with the spark_resource in Dagster. This setup is required for working with PySpark DataFrames in BigQuery.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Definitions\nfrom dagster_spark import spark_resource\nfrom dagster_gcp_pyspark import BigQueryPySparkIOManager\n\n@asset\ndef my_asset(spark):\n    # Your asset logic here\n    ...\n\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"io_manager\": BigQueryPySparkIOManager(\n            project=\"my-project\", dataset=\"my_dataset\"\n        ),\n        \"spark\": spark_resource.configured(\n            {\n                \"spark_conf\": {\n                    \"spark.jars\": \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\",\n                    \"spark.hadoop.google.cloud.auth.service.account.enable\": \"true\",\n                    \"spark.hadoop.google.cloud.auth.service.account.json.keyfile\": \"/path/to/keyfile.json\",\n                }\n            }\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Providing Per-Input Config to Input Managers\nDESCRIPTION: Shows how to define and use per-input configuration for input managers, allowing parameterization of input loading.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/unconnected-inputs.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import InputManager, In, op, job\n\nclass ConfigurableInputManager(InputManager):\n    def load_input(self, context):\n        # Access the per-input config\n        file_name = context.config[\"file_name\"]\n        # Load data from the specified file\n        return [1, 2, 3]\n\n@op(ins={\"data\": In(input_manager_key=\"configurable_input_manager\")})\ndef process_data(data):\n    return data\n\n@job(resource_defs={\"configurable_input_manager\": ConfigurableInputManager()})\ndef my_job():\n    process_data()\n\nresult = my_job.execute_in_process(\n    run_config={\n        \"ops\": {\n            \"process_data\": {\n                \"inputs\": {\n                    \"data\": {\"file_name\": \"my_data.csv\"}\n                }\n            }\n        }\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Yielding Failure Event in Dagstermill\nDESCRIPTION: Generates a Failure event using Dagster's Failure class and Dagstermill's yield_event method to signal a notebook execution failure with a custom error message\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/yield_failure.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Failure\nimport dagstermill\n\ndagstermill.yield_event(Failure(\"bad bad notebook\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring workspace environment secrets in Helm values.yaml\nDESCRIPTION: Configuration for specifying environment secrets in the Helm values file that will be made available to Dagster using Kubernetes envFrom functionality.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\n# values.yaml\nworkspace:\n  envSecrets:\n    - name: database-password-kubernetes-secret\n```\n\n----------------------------------------\n\nTITLE: Defining Sling Connection Resources for S3 and Snowflake in Python\nDESCRIPTION: This snippet defines Sling connection resources for S3 and Snowflake. It sets up the necessary configurations for connecting to an S3 bucket and a Snowflake database, using environment variables for sensitive information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/sling_replicate_csv_files_to_snowflake.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_sling import SlingConnectionResource, SlingResource, sling_assets\n\nS3_SOURCE_BUCKET = \"elementl-data\"\n\n\ns3_connection = SlingConnectionResource(\n    name=\"SLING_S3_SOURCE\",\n    type=\"s3\",\n    bucket=S3_SOURCE_BUCKET,  # type: ignore\n    access_key_id=EnvVar(\"AWS_ACCESS_KEY_ID\"),  # type: ignore\n    secret_access_key=EnvVar(\"AWS_SECRET_ACCESS_KEY\"),  # type: ignore\n)\n\nsnowflake_connection = SlingConnectionResource(\n    name=\"SLING_SNOWFLAKE_DESTINATION\",\n    type=\"snowflake\",\n    host=EnvVar(\"SNOWFLAKE_ACCOUNT\"),  # type: ignore\n    user=EnvVar(\"SNOWFLAKE_USER\"),  # type: ignore\n    password=EnvVar(\"SNOWFLAKE_PASSWORD\"),  # type: ignore\n    warehouse=EnvVar(\"SNOWFLAKE_WAREHOUSE\"),  # type: ignore\n    database=EnvVar(\"SNOWFLAKE_DATABASE\"),  # type: ignore\n    schema=EnvVar(\"SNOWFLAKE_SCHEMA\"),  # type: ignore\n    role=EnvVar(\"SNOWFLAKE_ROLE\"),  # type: ignore\n)\n\n\nsling_resource = SlingResource(connections=[s3_connection, snowflake_connection])\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables and Secrets for a Code Location\nDESCRIPTION: Example showing how to configure environment variables and secrets for a specific code location in Dagster+ using the container_context.ecs properties in dagster_cloud.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/amazon-ecs/configuration-reference.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      ecs:\n        env_vars:\n          - DATABASE_NAME=testing\n          - DATABASE_PASSWORD\n        secrets:\n          - name: 'MY_API_TOKEN'\n            valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:token::'\n          - name: 'MY_PASSWORD'\n            valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:password::'\n        secrets_tags:\n          - 'my_tag_name'\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake I/O Manager in Dagster\nDESCRIPTION: Python code snippet demonstrating how to configure the Snowflake I/O manager with necessary connection details and optional parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster-io-managers.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_snowflake_pandas import SnowflakePandasIOManager\n\ndefs = Definitions(\n    resources={\n        \"io_manager\": SnowflakePandasIOManager(\n            account=\"your-account\",\n            user=\"${env:SNOWFLAKE_USER}\",\n            password=\"${env:SNOWFLAKE_PASSWORD}\",\n            database=\"PLANTS\",\n            warehouse=\"PLANTS\",\n            schema=\"FLOWERS\",\n            role=\"writer\",\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating dbt Model for Customer Data\nDESCRIPTION: This SQL code defines a dbt model that sources data from the upstream 'raw_customers' asset and performs a simple transformation. It demonstrates how to create a dbt model that depends on a Dagster asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nwith source as (\n    select * from {{ source('example', 'raw_customers') }}\n)\n\nselect *\nfrom source\n```\n\n----------------------------------------\n\nTITLE: Configuring NoOpComputeLogManager in dagster.yaml\nDESCRIPTION: Configuration for disabling compute log upload entirely by using the NoOpComputeLogManager in the dagster.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/managing-compute-logs-and-error-messages.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster.core.storage.noop_compute_log_manager\n  class: NoOpComputeLogManager\n```\n\n----------------------------------------\n\nTITLE: Migrating Type Hints from AssetExecutionContext to OpExecutionContext in Ops\nDESCRIPTION: Examples showing how to update type hints in @op decorators to respect the new class hierarchy in Dagster 1.5.0. The context parameter in ops should now use OpExecutionContext instead of AssetExecutionContext.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# old\n@op\ndef my_op(context: AssetExecutionContext):\n    ...\n\n# correct\n@op\ndef my_op(context: OpExecutionContext):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Accessing Dagster Context and Reading Data\nDESCRIPTION: This snippet retrieves the Dagster execution context using `dagstermill.get_context()` and reads a CSV file from a URL into a Pandas DataFrame. The context allows for logging and accessing other Dagster-specific features within the notebook. The Pandas DataFrame `df` stores the dataset for further analysis.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_LR.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontext = dagstermill.get_context()\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n```\n\n----------------------------------------\n\nTITLE: Making Op Jobs Available to Dagster Tools in Python\nDESCRIPTION: Shows how to make jobs available to Dagster tools by including them in a Definitions object at the top level of a Python module or file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/op-jobs.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, job, op\n\n@op\ndef hello():\n    pass\n\n@job\ndef my_job():\n    hello()\n\ndefs = Definitions(\n    jobs=[my_job]\n)\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Instance Migration Command\nDESCRIPTION: Command to migrate the Dagster instance to the latest schema version. This should be run when upgrading to newer Dagster versions to ensure compatibility with schema changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ndagster instance migrate\n```\n\n----------------------------------------\n\nTITLE: Configuring Op Jobs with Hardcoded Configuration in Python\nDESCRIPTION: Shows how to supply hardcoded configuration to an op job using either a RunConfig object or a raw config dictionary.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/op-jobs.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, Config\n\n@op(config_schema={\"x\": int})\ndef my_op(context):\n    return context.op_config[\"x\"]\n\n@job(config={\"ops\": {\"my_op\": {\"config\": {\"x\": 5}}}})\ndef my_job():\n    my_op()\n\n# Alternatively:\nmy_job = my_graph.to_job(\n    config={\"ops\": {\"my_op\": {\"config\": {\"x\": 5}}}}\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring EcsRunLauncher with Specific Secrets\nDESCRIPTION: This YAML configuration demonstrates how to pass specific AWS Secrets Manager secrets to the EcsRunLauncher for inclusion as environment variables in Dagster runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: 'dagster_aws.ecs'\n  class: 'EcsRunLauncher'\n  config:\n    secrets:\n      - name: 'MY_API_TOKEN'\n        valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:token::'\n      - name: 'MY_PASSWORD'\n        valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:password::'\n```\n\n----------------------------------------\n\nTITLE: Importing DagsterMill\nDESCRIPTION: This snippet imports the dagstermill library, which is necessary for integrating Jupyter notebooks into Dagster pipelines. It allows accessing Dagster's execution context and logging information within the notebook.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_LR.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Creating Jobs Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'jobs' table to store information about Dagster jobs. It includes columns for job identification, status, type, and related timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.jobs (\n    id bigint NOT NULL,\n    job_origin_id character varying(255),\n    selector_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Sending Metadata to Dagster+ via REST API (Python)\nDESCRIPTION: This snippet demonstrates how to send asset metadata to Dagster+ using the REST API. It constructs the payload with asset key, metadata, and partition information, and sends a POST request to the Dagster+ API.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/oss-metadata-to-plus/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://{organization}.dagster.cloud/{deployment_name}/report_asset_materialization/\".format(\n    organization=\"new_organization\", deployment_name=\"new_deployment\"\n)\n\npayload = {\n    \"asset_key\": asset_key,\n    \"metadata\": metadata,\n    \"partition\": partition,\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Dagster-Cloud-Api-Token\": new_dagster_cloud_api_token,\n}\n\nresponse = requests.request(\"POST\", url, json=payload, headers=headers)\nresponse.raise_for_status()\n```\n\n----------------------------------------\n\nTITLE: Starting RabbitMQ Broker with Docker Compose (Bash)\nDESCRIPTION: Command to start a local RabbitMQ broker using Docker Compose in the dagster-celery directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Shell commands to install the necessary Dagster libraries for BigQuery integration\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-gcp dagster-gcp-pandas\n```\n\n----------------------------------------\n\nTITLE: Specifying Schema in Asset Key for Dagster Assets\nDESCRIPTION: This example shows how to specify the Snowflake schema as part of the asset's key in Dagster. It allows storing different assets in different schemas.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@asset(key_prefix=[\"IRIS\"])\ndef iris_dataset():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n\n@asset(key_prefix=[\"DAFFODIL\"])\ndef daffodil_dataset():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n```\n\n----------------------------------------\n\nTITLE: Creating Branch Deployment with Dagster Cloud CLI in Shell\nDESCRIPTION: Uses the dagster-cloud CLI to create or update a branch deployment. Includes optional parameters for enhancing the Branch Deployments UI in Dagster+.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/using-branch-deployments-with-the-cli.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nBRANCH_DEPLOYMENT_NAME=$(\n    dagster-cloud branch-deployment create-or-update \\\n        --organization $ORGANIZATION_NAME \\\n        --api-token $DAGSTER_CLOUD_API_TOKEN \\\n        --git-repo-name $REPOSITORY_NAME \\\n        --branch-name $BRANCH_NAME \\\n        --commit-hash $COMMIT_SHA \\\n        --timestamp $TIMESTAMP\n        --code-review-url $PR_URL \\\n        --code-review-id $INPUT_PR \\\n        --pull-request-status $PR_STATUS \\\n        --commit-message $MESSAGE \\\n        --author-name $NAME \\\n        --author-email $EMAIL \\\n        --author-avatar-url $AVATAR_URL \\\n        --base-deployment-name $BASE_DEPLOYMENT_NAME \n)\n```\n\n----------------------------------------\n\nTITLE: Asset Metadata Recording in Dagster\nDESCRIPTION: Shows how to attach metadata to asset materializations to provide additional context and monitoring capabilities\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_snowflake/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef hackernews_topstories():\n    # Fetch and process stories\n    return output_dataframe.with_metadata({\n        \"num_records\": len(output_dataframe),\n        \"preview\": MetadataValue.md(output_dataframe.head().to_markdown())\n    })\n```\n\n----------------------------------------\n\nTITLE: Migrating from Pendulum to Standard Python Datetime Objects\nDESCRIPTION: Example showing how to modify asset code that previously relied on Pendulum datetime methods. This demonstrates converting standard datetime objects back to Pendulum instances to maintain compatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetExecutionContext\n\n@asset\ndef my_asset(context: AssetExecutionContext):\n  window_start, window_end = context.partition_time_window\n  in_an_hour = window_start.add(hours=1) # will break since add() is only defined in pendulum\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variables in .env File for Dagster\nDESCRIPTION: A sample .env file showing how to define environment variables for a Dagster application. These variables include database credentials that can be accessed by Dagster at runtime.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# .env\n\nDATABASE_NAME=staging\nDATABASE_SCHEMA=sales\nDATABASE_USERNAME=salesteam\nDATABASE_PASSWORD=supersecretstagingpassword\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions\nDESCRIPTION: Python code showing how to set up Dagster definitions with the PipesDataprocJobClient resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/gcp-dataproc-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/dagster/dagster_pipes/gcp/dataproc_job/dagster_code.py\" startAfter=\"start_definitions_marker\" endBefore=\"end_definitions_marker\" />\n```\n\n----------------------------------------\n\nTITLE: Constructing Assets for Airflow Tasks in Dagster\nDESCRIPTION: Python code to create asset specs corresponding to Airflow tasks, including dbt model assets. It uses assets_with_task_mappings and the dbt_assets decorator to define the assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/observe.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Code example omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Execution Properties\nDESCRIPTION: Configuration properties that control Spark's execution behavior including broadcast settings, executor configurations, file handling, and Hadoop integration parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_35\n\nLANGUAGE: properties\nCODE:\n```\nspark.broadcast.blockSize=4m\nspark.broadcast.checksum=true\nspark.executor.cores=1\nspark.default.parallelism=<dynamic>\nspark.executor.heartbeatInterval=10s\nspark.files.fetchTimeout=60s\nspark.files.useFetchCache=true\nspark.files.overwrite=false\nspark.files.maxPartitionBytes=134217728\nspark.files.openCostInBytes=4194304\nspark.hadoop.cloneConf=false\nspark.hadoop.validateOutputSpecs=true\nspark.storage.memoryMapThreshold=2m\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=1\n```\n\n----------------------------------------\n\nTITLE: Defining Replication Configuration in YAML\nDESCRIPTION: Example YAML configuration file for the replication tool, specifying source and destination databases, and tables to replicate.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconnections:\n  source:\n    type: duckdb\n    connection: example.duckdb\n  destination:\n    type: postgres\n    connection: postgresql://postgres:postgres@localhost/postgres\n\ntables:\n  - name: users\n    primary_key: id\n  - name: products\n    primary_key: id\n  - name: activity\n    primary_key: id\n```\n\n----------------------------------------\n\nTITLE: Storing Multi-Partitioned Assets in Snowflake\nDESCRIPTION: Illustrates how to store multi-partitioned assets in Snowflake using a dictionary of partition_expr metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, MultiPartitionsDefinition, DailyPartitionsDefinition, StaticPartitionsDefinition\n\ntime_partitions = DailyPartitionsDefinition(start_date=\"2023-01-01\")\nspecies_partitions = StaticPartitionsDefinition([\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\npartitions = MultiPartitionsDefinition({\n    \"time\": time_partitions,\n    \"species\": species_partitions,\n})\n\n@asset(\n    partitions_def=partitions,\n    metadata={\n        \"partition_expr\": {\n            \"time\": \"TO_TIMESTAMP(TIME::INT)\",\n            \"species\": \"SPECIES\",\n        }\n    }\n)\ndef iris_dataset():\n    return get_iris_data()\n\n@asset(partitions_def=partitions)\ndef species_daily_averages(iris_dataset):\n    return iris_dataset.mean()\n```\n\n----------------------------------------\n\nTITLE: Testing Partition Config Validation in Python\nDESCRIPTION: Example showing how to validate run configuration for a partitioned job using validate_run_config function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/testing-partitioned-config-and-jobs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import validate_run_config\n\ndef test_my_partitioned_config():\n    result = my_partitioned_config(\"2023-01-01\")\n    assert validate_run_config(my_job, result)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stub HN API Client for Testing\nDESCRIPTION: Shows the implementation of a stubbed version of the Hacker News API client for use in unit testing, implementing the same methods as the real client.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstart_mock\n```\n\n----------------------------------------\n\nTITLE: Configuring NoOpComputeLogManager in YAML\nDESCRIPTION: Configuration for NoOpComputeLogManager which does not store stdout and stderr logs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster._core.storage.noop_compute_log_manager\n  class: NoOpComputeLogManager\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: This command starts the Dagster development server. It sets the DAGSTER_HOME environment variable to the current directory before running the server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/google_drive_factory/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDAGSTER_HOME=$(pwd) dagster dev\n```\n\n----------------------------------------\n\nTITLE: Starting RabbitMQ Broker with Docker\nDESCRIPTION: Docker command to run RabbitMQ broker on port 5672 for Celery task queue management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/celery.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -p 5672:5672 rabbitmq:3.8.2\n```\n\n----------------------------------------\n\nTITLE: Querying Orders Table with DuckDB\nDESCRIPTION: This SQL command queries a DuckDB database file located at '/tmp/jaffle_platform.duckdb'. It selects all columns from the 'orders' table and limits the output to 5 rows. The query is executed directly from the command line using the DuckDB CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/26-duckdb-select-orders.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nduckdb /tmp/jaffle_platform.duckdb -c \"SELECT * FROM orders LIMIT 5;\"\n```\n\n----------------------------------------\n\nTITLE: Adding Summary Statistics to DataFrame Type\nDESCRIPTION: Demonstrates how to implement and add summary statistics functionality to a custom DataFrame type for monitoring data characteristics.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pandas.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef compute_summary_stats(dataframe):\n    return {\n        \"min_of_max\": dataframe.amount.min(),\n        \"max_of_max\": dataframe.amount.max(),\n        \"num_bikes\": dataframe.bike_id.nunique(),\n    }\n\n\nsummary_stats_trip_dataframe = create_dagster_pandas_dataframe_type(\n    name=\"SummaryStatsTripDataFrame\",\n    columns=[\n        PandasColumn.integer_column(\"bike_id\", min_value=0),\n        PandasColumn.integer_column(\"payment_id\", min_value=0),\n        PandasColumn.string_column(\"payment_type\"),\n        PandasColumn.float_column(\"amount\", min_value=0),\n        PandasColumn.datetime_column(\"started_at\"),\n        PandasColumn.datetime_column(\"ended_at\"),\n    ],\n    summary_statistics=compute_summary_stats,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Jupyter Notebook Asset with Input in Python\nDESCRIPTION: Creates a Dagster asset from a Jupyter notebook, specifying the notebook path and input dependencies. The 'iris_dataset' asset is provided as input to the notebook.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\niris_kmeans_jupyter_notebook = define_dagstermill_asset(\n    name=\"iris_kmeans_jupyter\",\n    notebook_path=file_relative_path(__file__, \"notebooks/iris-kmeans.ipynb\"),\n    group_name=\"template_tutorial\",\n    ins={\"iris\": AssetIn(\"iris_dataset\")},\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Dagster Definitions to Include Asset Checks\nDESCRIPTION: Updates the Dagster Definitions object to include both assets (products, sales_reps, sales_data, joined_data) and the newly created asset check (missing_dimension_check), along with a DuckDB resource configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/ensure-data-quality-with-asset-checks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefs = dg.Definitions(\n    assets=[products,\n        sales_reps,\n        sales_data,\n        joined_data,\n    ],\n    asset_checks=[missing_dimension_check],\n    resources={\"duckdb\": DuckDBResource(database=\"data/mydb.duckdb\")},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Database Password\nDESCRIPTION: Kubectl command to create a Kubernetes secret for storing a database password.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create secret generic database-password-kubernetes-secret \\\n    --from-literal=DATABASE_PASSWORD=your_password \\\n    --namespace dagster-plus\n```\n\n----------------------------------------\n\nTITLE: Viewing JSON-formatted Dagster Event Log Entries for Pipeline Execution\nDESCRIPTION: These code snippets represent JSON-formatted Dagster event log entries showing the full lifecycle of a pipeline execution. Each entry contains metadata about pipeline events, including timestamps, event types, process IDs, and contextual information related to step execution and resource initialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of run for \\\"__ASSET_JOB\\\".\", \"pid\": 82986, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": null, \"timestamp\": 1729528719.518539, \"user_message\": \"Started execution of run for \\\"__ASSET_JOB\\\".\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"82986\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['not_partitioned']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps using multiprocess executor: parent process (pid: 82986)\", \"pid\": 82986, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": null, \"timestamp\": 1729528719.574795, \"user_message\": \"Executing steps using multiprocess executor: parent process (pid: 82986)\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"step_process_start\", \"metadata_entries\": []}, \"event_type_value\": \"STEP_WORKER_STARTING\", \"logging_tags\": {\"job_name\": \"__ASSET_JOB\", \"op_name\": \"not_partitioned\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": \"not_partitioned\"}, \"message\": \"Launching subprocess for \\\"not_partitioned\\\".\", \"pid\": 82986, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"not_partitioned\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"not_partitioned\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"not_partitioned\", \"parent\": null}}, \"step_key\": \"not_partitioned\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": \"not_partitioned\", \"timestamp\": 1729528719.587751, \"user_message\": \"Launching subprocess for \\\"not_partitioned\\\".\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"step_process_start\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"83044\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"STEP_WORKER_STARTED\", \"logging_tags\": {}, \"message\": \"Executing step \\\"not_partitioned\\\" in subprocess.\", \"pid\": 83044, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": \"not_partitioned\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": \"not_partitioned\", \"timestamp\": 1729528720.677435, \"user_message\": \"Executing step \\\"not_partitioned\\\" in subprocess.\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"RESOURCE_INIT_STARTED\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [io_manager].\", \"pid\": 83044, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": null, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"not_partitioned\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"not_partitioned\", \"parent\": null}}, \"step_key\": \"not_partitioned\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": \"not_partitioned\", \"timestamp\": 1729528720.729564, \"user_message\": \"Starting initialization of resources [io_manager].\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"resources\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PythonArtifactMetadataEntryData\", \"module\": \"dagster._core.storage.fs_io_manager\", \"name\": \"PickledObjectFilesystemIOManager\"}, \"label\": \"io_manager\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"26ms\"}, \"label\": \"io_manager:init_time\"}]}, \"event_type_value\": \"RESOURCE_INIT_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished initialization of resources [io_manager].\", \"pid\": 83044, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": null, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"not_partitioned\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"not_partitioned\", \"parent\": null}}, \"step_key\": \"not_partitioned\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": \"not_partitioned\", \"timestamp\": 1729528720.773275, \"user_message\": \"Finished initialization of resources [io_manager].\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ComputeLogsCaptureData\", \"external_stderr_url\": null, \"external_stdout_url\": null, \"external_url\": null, \"log_key\": \"hpnzfxjp\", \"step_keys\": [\"not_partitioned\"]}, \"event_type_value\": \"LOGS_CAPTURED\", \"logging_tags\": {}, \"message\": \"Started capturing logs in process (pid: 83044).\", \"pid\": 83044, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": null, \"timestamp\": 1729528720.963532, \"user_message\": \"Started capturing logs in process (pid: 83044).\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"job_name\": \"__ASSET_JOB\", \"op_name\": \"not_partitioned\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": \"not_partitioned\"}, \"message\": \"Started execution of step \\\"not_partitioned\\\".\", \"pid\": 83044, \"pipeline_name\": \"__ASSET_JOB\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"not_partitioned\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"not_partitioned\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"not_partitioned\", \"parent\": null}}, \"step_key\": \"not_partitioned\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"__ASSET_JOB\", \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"step_key\": \"not_partitioned\", \"timestamp\": 1729528721.018494, \"user_message\": \"Started execution of step \\\"not_partitioned\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Combining Metadata Fetching Methods for dbt Models in Dagster\nDESCRIPTION: This example shows how to chain multiple metadata fetching methods for dbt models in Dagster. It combines fetch_row_counts() and fetch_column_metadata() on the DbtEventIterator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@dbt_assets(manifest=manifest)\ndef my_dbt_assets(context: AssetExecutionContext, dbt: DbtCliResource):\n    yield from dbt.cli(\n        [\n            \"build\",\n            \"--select\",\n            \"my_model\",\n            \"--profiles-dir\",\n            \".\",\n            \"--project-dir\",\n            \".\",\n        ],\n        context=context,\n    ).stream().fetch_row_counts().fetch_column_metadata()\n```\n\n----------------------------------------\n\nTITLE: Implementing Sensor-based External Asset Polling\nDESCRIPTION: Shows how to create a sensor that polls an SFTP server to detect changes in external assets and record materializations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/external-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import sensor, SensorResult, AssetMaterialization, EventMetadataEntry\nfrom datetime import datetime\n\n@sensor\ndef raw_transactions_sensor(context):\n    # In practice, you would check if the file has changed on SFTP\n    file_last_modified_at = datetime.now()\n\n    return SensorResult(\n        events=[\n            AssetMaterialization(\n                asset_key=\"raw_transactions\",\n                metadata={\n                    \"file_last_modified_at\": EventMetadataEntry.text(\n                        file_last_modified_at.isoformat()\n                    )\n                },\n            )\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Asset Tags in Python\nDESCRIPTION: This snippet demonstrates how to attach tags to an asset in Dagster using the 'tags' argument. It shows the structure of tags as a dictionary of key-value pairs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/tags.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(\n    tags={\n        \"domain\": \"marketing\",\n        \"pii\": \"true\",\n        \"private\": \"\",  # label\n    }\n)\ndef marketing_email_list():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions\nDESCRIPTION: Defines the complete Dagster pipeline configuration including assets, asset checks, and resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-partitioned-asset.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndefs = dg.Definitions(\n    assets=[products,\n        sales_reps,\n        sales_data,\n        joined_data,\n        monthly_sales_performance,\n        product_performance,\n    ],\n    asset_checks=[missing_dimension_check],\n    resources={\"duckdb\": DuckDBResource(database=\"data/mydb.duckdb\")},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Replication Configuration for S3 to Snowflake Transfer in Python\nDESCRIPTION: This snippet defines the replication configuration for transferring data from S3 to Snowflake. It specifies the source and target connections, as well as the default settings and stream configurations for the data transfer.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/sling_replicate_csv_files_to_snowflake.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nreplication_config = {\n    \"source\": \"SLING_S3_SOURCE\",\n    \"target\": \"SLING_SNOWFLAKE_DESTINATION\",\n    \"defaults\": {\"mode\": \"full-refresh\", \"object\": \"{stream_schema}_{stream_table}\"},\n    \"streams\": {\n        f\"s3://{S3_SOURCE_BUCKET}/staging\": {\n            \"object\": \"public.example_table\",\n            \"primary_key\": \"id\",\n        },\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Audit Logs with GraphQL in Dagster+\nDESCRIPTION: This GraphQL query demonstrates how to programmatically access audit logs in Dagster+ using the GraphQL API. It shows the structure required to retrieve audit log information that can be used for tracking changes to your Dagster deployment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/rbac/audit-logs.md#2025-04-22_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\ndocs_snippets/docs_snippets/dagster-plus/access/rbac/audit-logs.graphql\n```\n\n----------------------------------------\n\nTITLE: Training Linear Regression Model\nDESCRIPTION: This snippet defines the features (X) and target variable (y) for a linear regression model, creates a `LinearRegression` object, and fits the model to the data. It also logs a message to the Dagster context indicating that the model has been fitted.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_LR.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX = df[[\"sepal_length\", \"sepal_width\"]]\ny = df[\"petal_width\"]\nmodel = LinearRegression()\nfit = model.fit(X, y)\ncontext.log.info(\"Fitted linear regression model!\")\n```\n\n----------------------------------------\n\nTITLE: Yielding Dagstermill Computation Result\nDESCRIPTION: Uses Dagstermill to yield the computed result back to the pipeline context. This allows the result to be tracked and potentially used in subsequent pipeline steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/reimport.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n\ncontext = dagstermill.get_context()\ndagstermill.yield_result(result)\n```\n\n----------------------------------------\n\nTITLE: Declaring Warehouse Airflow Instance Reference in Python\nDESCRIPTION: Code snippet to create a reference to the 'warehouse' Airflow instance running at http://localhost:8081 using the AirflowInstance class.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_airlift import AirflowInstance\n\nwarehouse_airflow = AirflowInstance(\n    conn_id=\"warehouse\",\n    host=\"http://localhost:8081\",\n    username=\"airflow\",\n    password=\"airflow\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-meltano Package via pip\nDESCRIPTION: Command to install the dagster-meltano library using pip package manager. This is required to use Meltano with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/meltano.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-meltano\n```\n\n----------------------------------------\n\nTITLE: Specifying Schema as Metadata in Dagster Asset\nDESCRIPTION: This snippet demonstrates how to specify a Snowflake schema as metadata for a Dagster asset. It allows storing assets in different schemas within Snowflake.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"schema\": \"IRIS\"})\ndef iris_dataset():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n```\n\n----------------------------------------\n\nTITLE: Migrating Snowflake Tables with Timestamp Data in SQL\nDESCRIPTION: SQL script to migrate a Snowflake table created before dagster-snowflake 0.19.0 to use TIMESTAMP_NTZ(9) type instead of string for timestamp data. This script creates a new column, copies the data with appropriate conversion, and renames columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n// Add a column of type TIMESTAMP_NTZ(9)\nALTER TABLE database.schema.table\nADD COLUMN time_copy TIMESTAMP_NTZ(9)\n\n// copy the data from time and convert to timestamp data\nUPDATE database.schema.table\nSET time_copy = to_timestamp_ntz(time)\n\n// drop the time column\nALTER TABLE database.schema.table\nDROP COLUMN time\n\n// rename the time_copy column to time\nALTER TABLER database.schema.table\nRENAME COLUMN time_copy TO time\n```\n\n----------------------------------------\n\nTITLE: Sample Deployment Configuration in YAML for Dagster+\nDESCRIPTION: A comprehensive example of Dagster+ deployment settings in YAML format. This includes concurrency configuration for pools and runs, run monitoring timeouts, retry configuration, and SSO role settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/deployment-settings-reference.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  pools:\n    granularity: 'run'\n    default_limit: 1\n  runs:\n    max_concurrent_runs: 10\n    tag_concurrency_limits:\n      - key: 'database'\n        value: 'redshift'\n        limit: 5\n\nrun_monitoring:\n  start_timeout_seconds: 1200\n  cancel_timeout_seconds: 1200\n  max_runtime_seconds: 7200\n\nrun_retries:\n  max_retries: 0\n\nsso_default_role: EDITOR\n```\n\n----------------------------------------\n\nTITLE: Verifying Dagster Installation\nDESCRIPTION: Command to check the installed version of Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/installation.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndagster --version\n```\n\n----------------------------------------\n\nTITLE: Defining DuckDB Resource in Dagster\nDESCRIPTION: Adds a DuckDB resource to the Definitions object. This resource will be used as the storage backend for the ETL pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefs = dg.Definitions(\n    assets=[],\n    resources={\"duckdb\": DuckDBResource(database=\"data/mydb.duckdb\")},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Label Tag in Python\nDESCRIPTION: This example shows how to create a label tag by setting the tag value to an empty string. Labels are tags that only contain a key and are displayed differently in the UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/tags.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(\n    tags={\"private\":\"\"}\n)\ndef my_asset() -> None:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Adding Eager Automation to Monthly Sales Performance Asset in Dagster\nDESCRIPTION: Updates the monthly_sales_performance asset to include the eager automation condition, which makes the asset automatically run when all its dependencies are updated instead of using a separate schedule.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/automate-your-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(\n    compute_kind=\"duckdb\",\n    io_manager_def=dg.io_manager(\n        pickle_io_manager, base_dir=\"data\"\n    ),\n    name=\"monthly_sales_performance\",\n    group_name=\"analysis\",\n    metadata={\n        \"owner\": \"data_analytics_team\",\n        \"stage\": \"analysis\"\n    },\n    auto_materialize_policy=dg.AutoMaterializePolicy.eager(),\n    key_prefix=[\"analysis\"]\n)\ndef monthly_sales_performance(context, joined_data):\n    \"\"\"Monthly sales performance per sales rep.\"\"\"\n    # Group data by month and sales rep to calculate metrics\n    month_sales = (\n        joined_data\n        .groupby([fn.strftime(fn.cast(joined_data.date, types.TIMESTAMP), '%Y-%m')])\n        .agg([\n            fn.sum(joined_data.price).alias(\"total_sales\"),\n            fn.sum(fn.case([(joined_data.returned == True, joined_data.price)], 0))\n                .alias(\"total_returns\"),\n            fn.count(joined_data.id).alias(\"items_sold\"),\n            fn.count(fn.case([(joined_data.returned == True, 1)], None)).alias(\"items_returned\"),\n        ])\n        .sort(\"total_sales\", ascending=False)\n    )\n    \n    # Calculate return rate\n    month_sales = month_sales.select(\n        \"strftime\",\n        \"total_sales\",\n        \"total_returns\",\n        \"items_sold\",\n        \"items_returned\",\n        (fn.cast(month_sales.items_returned, types.FLOAT) / month_sales.items_sold).alias(\"return_rate\")\n    )\n    \n    context.log.info(f\"Calculated monthly sales performance\")\n    return month_sales\n```\n\n----------------------------------------\n\nTITLE: Starter Pack Snapshot Implementation\nDESCRIPTION: Implementation of the starter pack snapshot asset that extracts and stores member data in R2 storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/ingestion.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstart_starter_pack_func\n    client = atproto.get_client()\n    members = get_starter_pack_members(client, context.partition_key)\n    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    object_path = f\"starter_pack/{date_str}.json\"\n    s3.put_object(\n        Bucket=BUCKET,\n        Key=object_path,\n        Body=json.dumps(members).encode(),\n    )\n\n    # update dynamic partition\n    member_ids = [member[\"member\"].split(\".\")[0] for member in members]\n    context.instance.add_dynamic_partitions(MEMBER_PARTITION_KEY, member_ids)\n\n    return dg.MetadataValue.json({\"file\": object_path, \"count\": len(members)})\nend_starter_pack_func\n```\n\n----------------------------------------\n\nTITLE: Implementing Monthly Sales Performance Asset in Dagster\nDESCRIPTION: Creates a partitioned asset that calculates monthly sales performance per sales rep, using DuckDB for storage and computation. Includes partition-aware data management and metadata generation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-partitioned-asset.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(\n    partitions_def=monthly_partition,\n    compute_kind=\"duckdb\",\n    group_name=\"analysis\",\n    deps=[joined_data],\n)\ndef monthly_sales_performance(\n    context: dg.AssetExecutionContext, duckdb: DuckDBResource\n):\n    partition_date_str = context.partition_key\n    month_to_fetch = partition_date_str[:-3]\n\n    with duckdb.get_connection() as conn:\n        conn.execute(\n            f\"\"\"\n            create table if not exists monthly_sales_performance (\n                partition_date varchar,\n                rep_name varchar,\n                product varchar,\n                total_dollar_amount double\n            );\n\n            delete from monthly_sales_performance where partition_date = '{month_to_fetch}';\n\n            insert into monthly_sales_performance\n            select\n                '{month_to_fetch}' as partition_date,\n                rep_name,\n                product_name,\n                sum(dollar_amount) as total_dollar_amount\n            from joined_data where strftime(date, '%Y-%m') = '{month_to_fetch}'\n            group by '{month_to_fetch}', rep_name, product_name;\n            \"\"\"\n        )\n\n        preview_query = f\"select * from monthly_sales_performance where partition_date = '{month_to_fetch}';\"\n        preview_df = conn.execute(preview_query).fetchdf()\n        row_count = conn.execute(\n            f\"\"\"\n            select count(*)\n            from monthly_sales_performance\n            where partition_date = '{month_to_fetch}'\n            \"\"\"\n        ).fetchone()\n        count = row_count[0] if row_count else 0\n\n    return dg.MaterializeResult(\n        metadata={\n            \"row_count\": dg.MetadataValue.int(count),\n            \"preview\": dg.MetadataValue.md(preview_df.to_markdown(index=False)),\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Airbyte Cloud Assets from Multiple Workspaces\nDESCRIPTION: Combine definitions from multiple Airbyte Cloud workspaces by instantiating multiple AirbyteCloudWorkspace resources and merging their specs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_airbyte import AirbyteCloudWorkspace, load_airbyte_cloud_asset_specs\n\nworkspace_1 = AirbyteCloudWorkspace(\n    workspace_id=\"workspace_1_id\",\n    client_id=\"client_1_id\",\n    client_secret=\"client_1_secret\",\n)\n\nworkspace_2 = AirbyteCloudWorkspace(\n    workspace_id=\"workspace_2_id\",\n    client_id=\"client_2_id\",\n    client_secret=\"client_2_secret\",\n)\n\nairbyte_cloud_specs = [\n    *load_airbyte_cloud_asset_specs(workspace_1),\n    *load_airbyte_cloud_asset_specs(workspace_2),\n]\n\ndefs = Definitions(\n    assets=airbyte_cloud_specs,\n    resources={\n        \"airbyte_cloud_1\": workspace_1,\n        \"airbyte_cloud_2\": workspace_2,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill in Python\nDESCRIPTION: This snippet imports the Dagstermill module, which is essential for integrating Dagster with Jupyter notebooks. Dagstermill is required to access context and yield results in Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Configuring Question Input for Dagster RAG Retrieval Asset\nDESCRIPTION: Defines a configuration schema for the retrieval asset, allowing a question to be passed as input to the RAG system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/retrieval.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass QueryConfig(Config):\n    question: str\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Storage in Dagster YAML\nDESCRIPTION: Example configuration for using PostgreSQL storage in the dagster.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstorage:\n  postgres:\n    postgres_db:\n      hostname: localhost\n      username: dagster\n      password: \\{env:\\{env_var: DAGSTER_PG_PASSWORD\\}\\}\n      db_name: dagster\n      port: 5432\n```\n\n----------------------------------------\n\nTITLE: Instantiating the Dagster GraphQL Client in Python\nDESCRIPTION: This example demonstrates how to instantiate the DagsterGraphQLClient object to interact with the Dagster GraphQL API. The client connects to the local Dagster instance running on port 3000.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/graphql-client.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_graphql import DagsterGraphQLClient\n\n# Instantiate the client\nclient = DagsterGraphQLClient(\"http://localhost:3000\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Dagster API Lifecycle Warnings using Python warnings module\nDESCRIPTION: Demonstrates how to configure warning filters to control the display of different types of API lifecycle warnings in Dagster, including preview, beta, superseded, and deprecated APIs. Uses Python's built-in warnings module to manage these notifications.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/api-lifecycle/filtering-api-lifecycle-warnings.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\n# Filter out all API lifecycle warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n\n# Or filter specific categories\nwarnings.filterwarnings(\"ignore\", \".*preview.*\", category=ExperimentalWarning)\nwarnings.filterwarnings(\"ignore\", \".*beta.*\", category=ExperimentalWarning)\nwarnings.filterwarnings(\"ignore\", \".*superseded.*\", category=ExperimentalWarning)\nwarnings.filterwarnings(\"ignore\", \".*deprecated.*\", category=DeprecationWarning)\n```\n\n----------------------------------------\n\nTITLE: Basic DuckDB Asset Storage Example\nDESCRIPTION: Demonstrates creating a basic Dagster asset that fetches the Iris dataset as a Pandas DataFrame and stores it in DuckDB. The asset includes type hints to inform the I/O manager about data types.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n@asset\ndef iris_dataset() -> pd.DataFrame:\n    \"\"\"The iris dataset, renamed to have more readable column names.\"\"\"\n    iris = load_iris()\n    return pd.DataFrame(\n        {\n            \"sepal_length\": iris.data[:, 0],\n            \"sepal_width\": iris.data[:, 1],\n            \"petal_length\": iris.data[:, 2],\n            \"petal_width\": iris.data[:, 3],\n            \"species\": iris.target,\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Yielding Result with dagstermill in Python\nDESCRIPTION: This snippet shows how to use `dagstermill.yield_result()` to return a result named `obj` from within a Dagster-managed Jupyter notebook. The `dagstermill` library provides the necessary utilities to bridge the gap between Dagster's data pipeline framework and the interactive nature of Jupyter notebooks. The object `obj` should exist within the scope where `dagstermill.yield_result(obj)` is called inside a Jupyter notebook being executed by Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/yield_something.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n\ndagstermill.yield_result(obj)\n```\n\n----------------------------------------\n\nTITLE: Adding Assets to Dagster Definitions Object\nDESCRIPTION: Updates the Dagster Definitions object to include the new 'joined_data' asset along with the existing assets (products, sales_reps, sales_data). Also configures the DuckDB resource connection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-a-downstream-asset.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefs = dg.Definitions(\n  assets=[products,\n      sales_reps,\n      sales_data,\n      joined_data,\n  ],\n  resources={\"duckdb\": DuckDBResource(database=\"data/mydb.duckdb\")},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project in Dagster\nDESCRIPTION: Python code that sets up the dbt project configuration in Dagster. It specifies the project directory and other configuration parameters needed to run dbt commands.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/modeling.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndbt_project = with_resources(\n    DbtCliResource(\n        project_dir=os.path.join(os.path.dirname(__file__), \"../../dbt_project\")\n    ),\n    {\"dbt\": DbtCliResource},\n)\n```\n\n----------------------------------------\n\nTITLE: Representing Airbyte Cloud Assets in Dagster\nDESCRIPTION: Construct an AirbyteCloudWorkspace resource and load Airbyte Cloud asset specs into the Dagster asset graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_airbyte import AirbyteCloudWorkspace, load_airbyte_cloud_asset_specs\n\nairbyte_cloud_resource = AirbyteCloudWorkspace(\n    workspace_id=\"your_workspace_id\",\n    client_id=\"your_client_id\",\n    client_secret=\"your_client_secret\",\n)\n\nairbyte_cloud_specs = load_airbyte_cloud_asset_specs(airbyte_cloud_resource)\n\ndefs = Definitions(\n    assets=airbyte_cloud_specs,\n    resources={\"airbyte_cloud\": airbyte_cloud_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Modifying CI/CD Workflow for Multiple Code Locations Deployment\nDESCRIPTION: This YAML snippet demonstrates how to update a GitHub CI/CD workflow to deploy multiple code locations from a single repository. It uses the dagster-cloud-cli to set build outputs for each project location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/multi-tenancy.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# .github/workflows/dagster-cloud-deploy.yml\n\njobs:\n  dagster-cloud-deploy:\n    # ...\n    steps:\n      - name: Update build session with image tag for \"project_a\" code location\n        id: ci-set-build-output-project-a\n        if: steps.prerun.outputs.result != 'skip'\n        uses: dagster-io/dagster-cloud-action/actions/utils/dagster-cloud-cli@v0.1\n        with:\n          command: 'ci set-build-output --location-name=project_a --image-tag=$IMAGE_TAG'\n\n      - name: Update build session with image tag for \"project_b\" code location\n        id: ci-set-build-output-project-b\n        if: steps.prerun.outputs.result != 'skip'\n        uses: dagster-io/dagster-cloud-action/actions/utils/dagster-cloud-cli@v0.1\n        with:\n          command: 'ci set-build-output --location-name=project_b --image-tag=$IMAGE_TAG'\n      # ...\n```\n\n----------------------------------------\n\nTITLE: Loading from Python File with Options in YAML\nDESCRIPTION: Configures a code location to be loaded from a Python file with additional options like attribute, working directory, and executable path.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nload_from:\n  - python_file:\n      relative_path: hello_world_repository.py\n      attribute: hello_world_repository\n      working_directory: my_working_directory/\n      executable_path: venvs/path/to/python\n      location_name: my_location\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Tables in Downstream Assets\nDESCRIPTION: Example of loading data from Delta Lake tables into downstream Dagster assets\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/using-deltalake-with-dagster.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef iris_cleaned(iris_dataset: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean null values from the iris dataset.\"\"\"\n    return iris_dataset.dropna()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster RAG Query Asset Run\nDESCRIPTION: YAML configuration for materializing the query asset in Dagster, demonstrating how to provide a question as input to the RAG system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/retrieval.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nops:\n  query:\n    config:\n      question: What is Dagster?\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating a Branch Deployment in Dagster Cloud\nDESCRIPTION: This snippet demonstrates the basic command to create or update a branch deployment using the Dagster Cloud CLI. It includes required parameters such as organization name, API token, repository name, branch name, commit hash, and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nBRANCH_DEPLOYMENT_NAME=$(\n    dagster-cloud branch-deployment create-or-update \\\n        --organization $ORGANIZATION_NAME \\\n        --api-token $DAGSTER_CLOUD_API_TOKEN \\\n        --git-repo-name $REPOSITORY_NAME \\\n        --branch-name $BRANCH_NAME \\\n        --commit-hash $COMMIT_SHA \\ # Latest commit SHA on the branch\n        --timestamp $TIMESTAMP # UTC unixtime timestamp of the latest commit\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating Date Partition Range with Timezone Support\nDESCRIPTION: Examples showing how to update date_partition_range calls to use the new pendulum-based timezone-aware format in Dagster 0.10.0, replacing the datetime.timedelta delta parameter with a string-based delta_range parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndate_partition_range(\n    start=datetime.datetime(2018, 1, 1),\n    end=datetime.datetime(2019, 1, 1),\n    delta=datetime.timedelta(months=1)\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndate_partition_range(\n    start=datetime.datetime(2018, 1, 1),\n    end=datetime.datetime(2019, 1, 1),\n    delta_range=\"months\"\n)\n```\n\n----------------------------------------\n\nTITLE: Yielding DataFrame Result with Dagstermill\nDESCRIPTION: Uses Dagstermill to yield the processed DataFrame as a result, enabling integration with Dagster workflows and further processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-pandas/dagster_pandas/examples/notebooks/papermill_pandas_hello_world.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndagstermill.yield_result(df)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic RetryPolicy in Dagster\nDESCRIPTION: Implementation of a basic retry policy that automatically retries the op when an exception occurs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-retries.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@op(retry_policy=RetryPolicy())\ndef my_op():\n    if random() > 0.5:\n        raise Exception(\"Random failure!\")\n    return \"success\"\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Pipeline\nDESCRIPTION: Command to run the Dagster pipeline using the development server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/quickstart.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev -f quickstart/assets.py\n```\n\n----------------------------------------\n\nTITLE: Reviewing Project Structure for Airlift Federation Tutorial\nDESCRIPTION: Shows the directory and file structure of the tutorial project, including Dagster definition files and Airflow DAG directories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nairlift_federation_tutorial\n├── constants.py: Contains constant values used throughout both Airflow and Dagster\n├── dagster_defs: Contains Dagster definitions\n│   ├── definitions.py: Empty starter file for following along with the tutorial\n│   └── stages: Contains reference implementations for each stage of the migration process.\n├── metrics_airflow_dags: Contains the Airflow DAGs for the \"downstream\" Airflow instance\n└── warehouse_airflow_dags: Contains the Airflow DAGs for the \"upstream\" Airflow instance\n```\n\n----------------------------------------\n\nTITLE: Multiple Asset Sets Selection in Python\nDESCRIPTION: Creates a job that selects assets between multiple source and target assets\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nassets_on_path_job = define_asset_job(\n    name=\"assets_on_path_job\", selection='(key:\"raw_data_a\" or key:\"raw_data_b\")+ and +(key:\"a_b_c_for_sales\" or key:\"b_c_for_sales\")'\n)\n```\n\n----------------------------------------\n\nTITLE: Loading from Python Module in YAML\nDESCRIPTION: Configures a code location to be loaded from a Python module.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nload_from:\n  - python_module:\n      module_name: hello_world_module.definitions\n```\n\n----------------------------------------\n\nTITLE: Defining a Snowflake Resource in Dagster\nDESCRIPTION: Python code that creates a SnowflakeResource class, which is a Dagster resource representing a connection to a Snowflake database. It includes methods for querying and inserting data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-databases.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ConfigurableResource\nfrom snowflake import connector\n\nclass SnowflakeResource(ConfigurableResource):\n    account: str\n    user: str\n    password: str\n    database: str\n    schema: str\n    warehouse: str\n\n    def get_connection(self):\n        return connector.connect(\n            account=self.account,\n            user=self.user,\n            password=self.password,\n            database=self.database,\n            schema=self.schema,\n            warehouse=self.warehouse,\n        )\n\n    def execute_query(self, query):\n        with self.get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)\n            results = cursor.fetchall()\n        return results\n\n    def insert_dataframe(self, df, table_name):\n        with self.get_connection() as conn:\n            cursor = conn.cursor()\n            for _, row in df.iterrows():\n                placeholders = \", \".join([\"?\" for _ in row])\n                insert_query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n                cursor.execute(insert_query, tuple(row))\n            conn.commit()\n```\n\n----------------------------------------\n\nTITLE: Importing DagsterMill\nDESCRIPTION: This snippet imports the dagstermill library, which provides the necessary functions to interact with Dagster within a Jupyter notebook environment. It's the first step to use Dagster's features within a notebook.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Getting Context with Dagstermill\nDESCRIPTION: This snippet retrieves the execution context using Dagstermill. This context is crucial for running Dagster operations and is essential for further development within Dagstermill.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers_no_yield.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncontext = dagstermill.get_context()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Message Writer for Dagster Pipes\nDESCRIPTION: Example of creating a custom message writer that writes to a cloud service blob store with message chunking support.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/dagster-pipes-details-and-customization.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom cloud_service import write_to_blob_store\nfrom dagster_pipes import PipesBlobStoreMessageWriter\n\nclass CloudServiceMessageWriter(PipesBlobStoreMessageWriter):\n    def __init__(self, path):\n        self._path = path\n        super().__init__()\n\n    def _write_chunks(self, chunks: list[str]) -> None:\n        write_to_blob_store(self._path, chunks)\n```\n\n----------------------------------------\n\nTITLE: Migrating from Intermediate Storage to IO Manager in Pipeline Execution\nDESCRIPTION: Example showing how to migrate from using deprecated 'intermediate_storage' or 'storage' fields in run_config to using the new IO manager approach with resource definitions in Dagster 0.10.0.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n@pipeline\ndef my_pipeline():\n    ...\n\nexecute_pipeline(\n    my_pipeline,\n    run_config={\n        \"intermediate_storage\": {\n            \"filesystem\": {\"base_dir\": ...}\n        }\n    },\n)\n\nexecute_pipeline(\n    my_pipeline,\n    run_config={\n        \"storage\": {\n            \"filesystem\": {\"base_dir\": ...}\n        }\n    },\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n@pipeline(\n    mode_defs=[\n        ModeDefinition(\n            resource_defs={\"io_manager\": fs_io_manager}\n        )\n    ],\n)\ndef my_pipeline():\n    ...\n\nexecute_pipeline(\n    my_pipeline,\n    run_config={\n        \"resources\": {\n            \"io_manager\": {\"config\": {\"base_dir\": ...}}\n        }\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Loading Images into Kind Cluster\nDESCRIPTION: Shell commands to create a kind cluster and load Docker images into it for testing. This demonstrates how to manually prepare a kind cluster for dagster-k8s tests.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nkind create cluster --name kind-test\nkind load docker-image --name kind-test dagster.io/dagster-docker-buildkite:py310-latest\n```\n\n----------------------------------------\n\nTITLE: Updated Dagster Definitions with dg Autoloading\nDESCRIPTION: Modified Python code that uses load_defs to autoload definitions from the defs directory and merges them with existing definitions, enabling dg compatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\nfrom dagster._utils.load_defs import load_defs\nimport my_existing_project.defs\n\n\n@asset\ndef my_asset():\n    return 1\n\n\ndefs = Definitions.merge(\n    load_defs(my_existing_project.defs),\n    Definitions(assets=[my_asset]),\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing ReplicationResource in Python\nDESCRIPTION: A Dagster resource that encapsulates the replication logic, extending ConfigurableResource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/guides/multi-asset-integration.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ConfigurableResource\nfrom dagster._annotations import public\n\n\nclass ReplicationResource(ConfigurableResource):\n    @public\n    def run(\n        self, replication_project: ReplicationProject\n    ) -> Iterator[AssetMaterialization]:\n        results = replicate(Path(replication_project.replication_configuration_yaml))\n        for table in results:\n            if table.get(\"status\") == \"SUCCESS\":\n                # NOTE: this assumes that the table name is the same as the asset key\n                yield AssetMaterialization(\n                    asset_key=str(table.get(\"name\")), metadata=table\n                )\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Agent for Branch Deployments in YAML\nDESCRIPTION: Modifies the dagster.yaml file to enable branch deployments for a Docker agent. Sets the branch_deployments field to true and removes any deployment fields.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/using-branch-deployments-with-the-cli.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster.yaml\n\ninstance_class:\n  module: dagster_cloud.instance\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token: <YOUR_AGENT_TOKEN>\n  branch_deployments: true ## true enables branch deployments\n\nuser_code_launcher:\n  module: dagster_cloud.workspace.docker\n  class: DockerUserCodeLauncher\n  config:\n    networks:\n      - dagster_cloud_agent\n    server_ttl:\n      enabled: true\n      ttl_seconds: 7200 #2 hours\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Return Type Annotation in Python\nDESCRIPTION: Illustrates how to use Python's type annotations to specify the return type of an op for single output.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op() -> int:\n    return 1\n```\n\n----------------------------------------\n\nTITLE: Configuring GraphQL Client for Dagster+ API\nDESCRIPTION: This function sets up the GraphQL client with the necessary URL and authentication token for accessing the Dagster+ API. Users need to replace the placeholder values with their actual deployment URL and user token.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/export-metrics.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_client():\n    url = \"YOUR_ORG.dagster.cloud/prod\"  # Your deployment-scoped url\n    user_token = \"YOUR_TOKEN\"  # A token generated from Organization Settings > Tokens\n    return DagsterGraphQLClient(url, headers={\"Dagster-Cloud-Api-Token\": user_token})\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Log Handlers in Dagster YAML\nDESCRIPTION: YAML configuration for setting up custom log handlers, formatters, and filters in Dagster. This example sets up a file handler with a custom formatter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/python-logging.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\npython_logs:\n  handlers:\n    myHandler:\n      class: logging.FileHandler\n      filename: my_dagster_logs.log\n      formatter: timeFormatter\n  formatters:\n    timeFormatter:\n      format: \"%(asctime)s | %(name)s-%(levelname)s-%(message)s\"\n      datefmt: \"%Y-%m-%d %H:%M:%S\"\n```\n\n----------------------------------------\n\nTITLE: Configuring uv Project for dg in pyproject.toml\nDESCRIPTION: TOML configuration in pyproject.toml that sets up a Dagster project to work with dg, specifying the project type, root module, and location of the top-level Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[tool.dg]\ndirectory_type = \"project\"\nproject.root_module = \"my_existing_project\"\nproject.code_location_target_module = \"my_existing_project.definitions\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Executor Blacklisting\nDESCRIPTION: Parameters that control when and how executors or nodes are blacklisted from scheduling due to task failures, including timeouts and failure thresholds.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_41\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.enabled\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.timeout\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.task.maxTaskAttemptsPerExecutor\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.task.maxTaskAttemptsPerNode\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.stage.maxFailedTasksPerExecutor\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.stage.maxFailedExecutorsPerNode\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.application.maxFailedTasksPerExecutor\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.application.maxFailedExecutorsPerNode\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.killBlacklistedExecutors\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.blacklist.application.fetchFailure.enabled\n```\n\n----------------------------------------\n\nTITLE: Configuring Eager Materialization for ML Model Asset in Python\nDESCRIPTION: This snippet demonstrates how to set up eager materialization for a machine learning model asset in Dagster. It uses the AutomationCondition.eager setting to ensure the model is refreshed whenever upstream data is updated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/managing-ml.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    deps=[data_asset],\n    auto_materialize_policy=AutoMaterializePolicy.eager()\n)\ndef ml_model(data_asset):\n    # model training code\n    return model\n```\n\n----------------------------------------\n\nTITLE: Defining In Progress Scheduling Condition in Dagster\nDESCRIPTION: This snippet defines the in progress scheduling condition, which checks if a given asset computation is currently in progress, useful for handling concurrency and state management during execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.in_progress()\n```\n\n----------------------------------------\n\nTITLE: Configuring Graph with Config Mapping (YAML)\nDESCRIPTION: This YAML configuration demonstrates how to provide config for a graph that uses config mapping, specifying the 'from_unit' parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nops:\n  to_fahrenheit:\n    config:\n      from_unit: \"C\"\n```\n\n----------------------------------------\n\nTITLE: Loading an Asset as an Input in a Graph\nDESCRIPTION: Demonstrates how to supply an asset as an input to an op in a graph, using the asset's I/O manager to load the input value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSpec, asset, op, graph, In\n\n@asset\ndef upstream_asset():\n    return [1, 2, 3]\n\n@op\ndef downstream_op(my_input):\n    return sum(my_input)\n\n@graph\ndef my_graph():\n    downstream_op(In(AssetSpec(\"upstream_asset\")))\n\n@job\ndef my_job():\n    my_graph()\n```\n\n----------------------------------------\n\nTITLE: Yielding Named Output with Dagstermill\nDESCRIPTION: Demonstrates how to use Dagstermill's yield_result function to yield a value with a specified output name. The code imports the dagstermill library and yields the integer value 3 with the output name 'my_output'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/integrations/dagstermill/notebooks/my_notebook.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n\ndagstermill.yield_result(3, output_name=\"my_output\")\n```\n\n----------------------------------------\n\nTITLE: Back Compatibility Testing for SQLite Migration\nDESCRIPTION: This snippet details the process for testing back compatibility of schema migrations in SQLite. It highlights the steps to create a test for pre-and post-migration behavior using an instance loaded from a database snapshot.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/alembic/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p python_modules/dagster/dagster_tests/general_tests/compat_tests/<my_schema_change>/sqlite/history\n```\n\nLANGUAGE: bash\nCODE:\n```\ncp $DAGSTER_HOME/history/runs.db\\* python_modules/dagster/dagster_tests/general_tests/compat_tests/<my_schema_change>/sqlite/history/\n```\n\nLANGUAGE: bash\nCODE:\n```\ncp -R $DAGSTER_HOME/history/runs python_modules/dagster/dagster_tests/general_tests/compat_tests/<my_schema_change>/sqlite/history/\n```\n\n----------------------------------------\n\nTITLE: Configuring Isolated Kubernetes Agents\nDESCRIPTION: Helm command and values.yaml configuration for setting up isolated Kubernetes agents.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/multiple.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nhelm upgrade \\\n    ...\n    --set isolatedAgents.enabled=true \\\n    --set dagsterCloud.agentLabel=\"My agent\" # optional, only supported on 0.13.14 and later\n```\n\nLANGUAGE: yaml\nCODE:\n```\nisolatedAgents:\n  enabled: true\n\ndagsterCloud:\n  agentLabel: 'My agent' # optional, only supported on 0.13.14 and later\n```\n\n----------------------------------------\n\nTITLE: Deploying Code Location to Dagster+ Serverless\nDESCRIPTION: Command to deploy a Python executable as a code location to Dagster+ Serverless, specifying the deployment, location name, and module name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud serverless deploy-python-executable --deployment prod --location-name quickstart --module-name quickstart\n```\n\n----------------------------------------\n\nTITLE: Configuring load_assets_from_dbt_project with use_build Parameter in Python\nDESCRIPTION: Example of setting the use_build parameter to False in the load_assets_from_dbt_project function to maintain the pre-1.4 behavior, as the default has changed to True.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_dbt import group_from_dbt_resource_props_fallback_to_directory\n\nload_assets_from_dbt_project(\n    ...,\n    use_build=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Performing K-means Analysis on Iris Dataset\nDESCRIPTION: Python code to perform K-means clustering on the Iris dataset using scikit-learn.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nestimator = sklearn.cluster.KMeans(n_clusters=3)\nestimator.fit(\n    iris[[\"Sepal length (cm)\", \"Sepal width (cm)\", \"Petal length (cm)\", \"Petal width (cm)\"]]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pipes in PySpark Job\nDESCRIPTION: This code shows how to use Dagster Pipes in a PySpark script. It sets up a context for sending messages to Dagster and demonstrates log collection and data processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/pyspark-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\nfrom dagster_pipes import PipesCliArgsParamsLoader, PipesS3MessageWriter, PipesS3ContextLoader, open_pipes_session\n\nspark = SparkSession.builder.appName(\"PipesSpark\").getOrCreate()\n\nwith open_pipes_session(\n    params_loader=PipesCliArgsParamsLoader(),\n    message_writer=PipesS3MessageWriter(),\n    context_loader=PipesS3ContextLoader(),\n) as pipes:\n    pipes.log(\"Starting Spark job\")\n\n    # Your Spark job logic here\n    df = spark.createDataFrame([(1, \"foo\"), (2, \"bar\")], [\"id\", \"name\"])\n    count = df.count()\n    pipes.log(f\"DataFrame count: {count}\")\n\n    pipes.log(\"Spark job completed successfully\")\n\nspark.stop()\n```\n\n----------------------------------------\n\nTITLE: Configuring Historical Partition Updates with AutomationCondition.eager()\nDESCRIPTION: Demonstrates how to configure AutomationCondition.eager() to update historical time partitions by removing the latest_partition_only condition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/example-customizations.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(auto_materialize_policy=AutoMaterializePolicy(AutomationCondition.eager() & ~AutomationCondition.latest_partition_only()))\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Runs by Status and Timestamp in SQL\nDESCRIPTION: Creates a B-tree index on the runs table to optimize queries filtering by status and ordered by update_timestamp and create_timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_range ON public.runs USING btree (status, update_timestamp, create_timestamp);\n```\n\n----------------------------------------\n\nTITLE: Dagster Asset Using Shared Function\nDESCRIPTION: Implementing a Dagster asset that uses the same shared Python function that is used by Airflow, enabling gradual migration between the systems.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/python-operator.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom pathlib import Path\n\n# Import the shared function\nfrom shared_package.shared_module import write_to_db\n\n@asset\ndef db_tables():\n    \"\"\"Write processed data from local files to the database.\"\"\"\n    # Call the same function that is used in Airflow\n    write_to_db(directory=Path(\"/path/to/files\"))\n```\n\n----------------------------------------\n\nTITLE: Creating a Job with Configured dbt Assets in Python\nDESCRIPTION: This snippet shows how to create a job that uses the configured dbt assets and sets the full_refresh flag to True.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[dbt_project_assets],\n    jobs=[\n        define_asset_job(\n            \"materialize_dbt_models\",\n            selection=AssetSelection.all(),\n            config={\"ops\": {\"dbt_project_assets\": {\"config\": {\"full_refresh\": True}}}},\n        )\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Hooks in Dagster\nDESCRIPTION: This snippet shows how to test a hook by invoking it directly with a constructed HookContext.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-hooks.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import build_hook_context, op\n\ndef test_failure_hook():\n    @op\n    def my_op():\n        pass\n\n    mock_slack = MagicMock()\n    context = build_hook_context(\n        resources={\"slack\": mock_slack},\n        op=my_op,\n    )\n    slack_message_on_failure(context)\n\n    assert mock_slack.send_message.call_count == 1\n```\n\n----------------------------------------\n\nTITLE: Printing Operation Configurations in Dagstermill with Python\nDESCRIPTION: This snippet prints the operation configuration stored in the context. It uses the 'op_config' property of the context to output the current operation's configurations, which are vital for debugging and validation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/print_dagstermill_context_op_config.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nprint(context.op_config)\n```\n\n----------------------------------------\n\nTITLE: Filtering Sigma Assets by Workbooks\nDESCRIPTION: Python code demonstrating how to load a subset of Sigma assets by providing a SigmaFilter to the load_sigma_asset_specs function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sigma.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_sigma import SigmaOrganization, load_sigma_asset_specs, SigmaFilter\n\nsigma_organization = SigmaOrganization(\n    base_url=\"https://api.sigmacomputing.com\",\n    client_id=\"your-client-id\",\n    client_secret=\"your-client-secret\",\n)\n\nsigma_filter = SigmaFilter(\n    workbook_folders=[\"folder1\", \"folder2\"],\n)\n\nsigma_asset_specs = load_sigma_asset_specs(\n    sigma_organization=sigma_organization,\n    sigma_filter=sigma_filter,\n)\n\ndefs = Definitions(\n    assets=sigma_asset_specs,\n    resources={\n        \"sigma_organization\": sigma_organization,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Multiple Code Locations Configuration in YAML\nDESCRIPTION: Defines multiple code locations in a single workspace.yaml file, each with different configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nload_from:\n  - python_file:\n      relative_path: path/to/dataengineering_spark_team.py\n      location_name: dataengineering_spark_team_py_38_virtual_env\n      executable_path: venvs/path/to/dataengineering_spark_team/bin/python\n  - python_file:\n      relative_path: path/to/team_code_location.py\n      location_name: ml_team_py_36_virtual_env\n      executable_path: venvs/path/to/ml_tensorflow/bin/python\n```\n\n----------------------------------------\n\nTITLE: Downloading Jaffle Shop Sample CSV Files using curl\nDESCRIPTION: Downloads three CSV files (raw_customers.csv, raw_orders.csv, raw_payments.csv) from the dbt-labs jaffle-shop-classic GitHub repository using curl commands. The commands are chained together using && to execute sequentially.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/13-curl.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://raw.githubusercontent.com/dbt-labs/jaffle-shop-classic/refs/heads/main/seeds/raw_customers.csv &&\ncurl -O https://raw.githubusercontent.com/dbt-labs/jaffle-shop-classic/refs/heads/main/seeds/raw_orders.csv &&\ncurl -O https://raw.githubusercontent.com/dbt-labs/jaffle-shop-classic/refs/heads/main/seeds/raw_payments.csv\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Dagster-PySpark Environment\nDESCRIPTION: This Dockerfile sets up an environment with Spark, necessary Java dependencies for S3, and Python packages for Dagster and PySpark integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/pyspark-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nARG SPARK_VERSION=3.5.1\n\nFROM bitnami/spark:${SPARK_VERSION}\n\nUSER root\n\nARG SPARK_VERSION=3.4.1\n\nCOPY --from=ghcr.io/astral-sh/uv:0.5.11 /uv /uvx /bin/\n\nRUN install_packages curl\n\nRUN curl -fSL \"https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/$SPARK_VERSION/hadoop-aws-$SPARK_VERSION.jar\" \\\n    -o /opt/bitnami/spark/jars/hadoop-aws-$SPARK_VERSION.jar\n\nRUN curl -fSL \"https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.780/aws-java-sdk-bundle-1.12.780.jar\" \\\n    -o /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.780.jar\n\n# install Python dependencies\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install --system dagster dagster-webserver dagster-aws pyspark\n\nWORKDIR /src\nENV DAGSTER_HOME=/dagster_home\nCOPY dagster_code.py script.py ./\n```\n\n----------------------------------------\n\nTITLE: Migrating from Deprecated Python Environment Key in Workspace YAML\nDESCRIPTION: Examples showing how to update workspace YAML configuration by replacing the deprecated 'python_environment' key with direct executable_path specification in the python_package or python_file entry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\n- python_environment:\n    executable_path: \"/path/to/venvs/dagster-dev-3.7.6/bin/python\"\n    target:\n      python_package:\n        package_name: dagster_examples\n        location_name: dagster_examples\n```\n\nLANGUAGE: yaml\nCODE:\n```\n- python_package:\n    executable_path: \"/path/to/venvs/dagster-dev-3.7.6/bin/python\"\n    package_name: dagster_examples\n    location_name: dagster_examples\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Daemon Standalone\nDESCRIPTION: Starts only the Dagster daemon process, which is useful for custom setups or when the webserver is run separately.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/dagster-daemon.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster-daemon run\n```\n\n----------------------------------------\n\nTITLE: Running Dagster with Module\nDESCRIPTION: Shell command to start Dagster instance by loading definitions from a Python module.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/running-dagster-locally.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -m my_module\n```\n\n----------------------------------------\n\nTITLE: Merging Domain-Specific Definitions in Dagster\nDESCRIPTION: Combines multiple domain-specific definitions (data ingestion, transformation, and dashboard) into a single coherent definition for the entire pipeline. This approach leads to a clean, organized project structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/dashboard.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[*ingestion_def.get_asset_defs(), *dbt_def.get_asset_defs(), *dashboard_def.get_asset_defs()],\n    resources={\n        **ingestion_def.resources,\n        **dbt_def.resources,\n        **dashboard_def.resources,\n    },\n    schedules=dbt_def.get_schedule_defs(),\n)\n```\n\n----------------------------------------\n\nTITLE: dagster-dbt with Insights Tracking Implementation\nDESCRIPTION: Enhanced dbt asset configuration using the with_insights() method to track BigQuery usage metrics in Dagster+ Insights. This is the configuration after adding Insights integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/google-bigquery.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_dbt import DbtCliResource, dbt_assets\n\n@dbt_assets\ndef my_dbt_assets(dbt):\n    return dbt.cli([\"build\"], manifest=dbt.get_manifest()).with_insights().wait()\n\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\n        \"dbt\": DbtCliResource(\n            project_dir=\"path/to/dbt_project\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Type Resolution with Annotated and Resolver\nDESCRIPTION: Example showing how to provide custom resolution logic for non-standard types in a component, such as converting an API key to a client instance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import Annotated\nfrom dagster_components import Resolver\nfrom dagster import Config\n\nclass ApiClientConfig(Config):\n    api_key: str\n\nclass ApiClient:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n    \n    def query(self, q):\n        pass\n\ndef create_api_client(config: ApiClientConfig) -> ApiClient:\n    return ApiClient(api_key=config.api_key)\n\nclass ApiComponent(Resolvable, Component):\n    client: Annotated[ApiClient, Resolver(lambda config: create_api_client(config))]\n    \n    # The schema is inferred as:\n    # client:\n    #   api_key: str\n```\n\n----------------------------------------\n\nTITLE: Specifying Dask Resource Requirements for an Op in Python\nDESCRIPTION: This snippet demonstrates how to specify Dask resource requirements for a Dagster op using tags. It shows how to request GPU resources for an op that will be executed on a Dask cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/dask.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op(\n    ...\n    tags={'dagster-dask/resource_requirements': {\"GPU\": 1}},\n)\ndef my_op(...):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring DbtProject for Deferred Execution in Python\nDESCRIPTION: This code snippet shows how to set up a DbtProject object with a state path for deferred execution and use get_defer_args in the dbt command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndbt_project = DbtProject(\n    project_dir=\".\",\n    profiles_dir=\".\",\n    state_path=\"state/\",\n)\n\n@dbt_assets(\n    project=dbt_project,\n)\ndef dbt_project_assets(context: AssetExecutionContext):\n    # Run dbt build with defer args\n    yield from dbt_cli(\n        command=\"build\",\n        args=[\n            *get_defer_args(context),\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: YAML frontmatter configuration for a documentation page about resource authentication, specifying the title, sidebar position, and visibility status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/authenticating-to-a-resource.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Authenticating to a resource\nsidebar_position: 300\nunlisted: true\n---\n```\n\n----------------------------------------\n\nTITLE: Databricks Code Implementation\nDESCRIPTION: Example showing the Databricks side of the integration, demonstrating how to structure code that will be executed on Databricks platform.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/databricks.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/integrations/databricks/databricks_code.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Helm Chart from Source\nDESCRIPTION: Installs the Dagster Helm chart from the local source code. This command is similar to installing from remote but uses the current directory (`.`) as the chart location. It also creates a namespace named `dagster` and deploys a release named `my-release`.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-release . \\\n    --namespace dagster \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Compiling dbt Manifest with DbtProject in Python\nDESCRIPTION: Python code snippet demonstrating how to use the DbtProject class to handle the creation of a dbt manifest file at runtime during development. This approach is used in the scaffolded Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmy_dbt_project = DbtProject(\n    project_dir=\"path/to/dbt/project\",\n    profiles_dir=\"path/to/dbt/project\",  # optional\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Code Locations in dagster_cloud.yaml\nDESCRIPTION: Example of a dagster_cloud.yaml file defining multiple code locations with various configuration options including build settings, working directory, and container context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/dagster-cloud-yaml.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: data-eng-pipeline\n    code_source:\n      package_name: example_etl\n    build:\n      directory: ./example_etl\n      registry: localhost:5000/docker_image\n  - location_name: ml-pipeline\n    code_source:\n      package_name: example_ml\n    working_directory: ./ml_project\n    executable_path: venvs/path/to/ml_tensorflow/bin/python\n  - location_name: my_random_assets\n    code_source:\n      python_file: random_assets.py\n    container_context:\n      k8s:\n        env_vars:\n          - database_name\n          - database_username=hooli_testing\n        env_secrets:\n          - database_password\n```\n\n----------------------------------------\n\nTITLE: DBT with Snowflake and Insights Integration (After)\nDESCRIPTION: Enhanced configuration for DBT with Snowflake that includes Insights tracking. This implementation adds the .with_insights() method to the DbtCliResource and incorporates Snowflake insights assets and schedule.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/snowflake.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_dbt import DbtCliResource, dbt_assets\nfrom dagster_cloud.dagster_insights import create_snowflake_insights_asset_and_schedule\n\n@dbt_assets\ndef my_dbt_assets():\n    return ...\n\n# Create the asset and schedule definitions needed for collecting Snowflake metrics\nsnowflake_insights_defs = create_snowflake_insights_asset_and_schedule(\n    resources={\n        \"snowflake\": ...,  # your snowflake resource config\n    }\n)\n\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\n        \"dbt\": DbtCliResource(\n            project_dir=\"path/to/dbt/project\",\n            profiles_dir=\"path/to/profiles/dir\",\n        ).with_insights(),\n    },\n    # Incorporate the snowflake insights defs into your Definitions\n    **snowflake_insights_defs,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Code Locations in Dagster+ Hybrid\nDESCRIPTION: YAML configuration for defining multiple code locations in dagster_cloud.yaml file. This allows separate projects to be managed within the same repository while maintaining isolation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/managing-multiple-projects-and-teams.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yml\n\nlocations:\n  - location_name: project_a\n    code_source:\n      package_name: project_a\n    build:\n      # ...\n  - location_name: project_b\n    code_source:\n      package_name: project_b\n    build:\n      # ...\n```\n\n----------------------------------------\n\nTITLE: Creating asset_keys Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the asset_keys table in the public schema of the PostgreSQL database, which will store identifiers and timestamps associated with various assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: asset_keys; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.asset_keys OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Job Run Status with Dagster GraphQL Client\nDESCRIPTION: This example demonstrates how to use the client to fetch the status of a specific job run using the get_run_status method with a run_id parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/graphql-client.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get a run status\nstatus = client.get_run_status(\"my-run-id\")\nprint(f\"Status for run_id my-run-id: {status}\")\n```\n\n----------------------------------------\n\nTITLE: Report Asset Checks with Dagster Pipes\nDESCRIPTION: Demonstrates how to report asset quality checks from external code using Dagster Pipes. Includes check definition and execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/modify-external-code.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import open_dagster_pipes, PipesContext\n\nwith open_dagster_pipes():\n    orders = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n    context = PipesContext.get()\n    total = sum(quantity for _, quantity in orders)\n    context.log.info(f\"Total orders: {total}\")\n    context.report_asset_materialization(metadata={\"total_orders\": total})\n    context.report_asset_check(\n        name=\"total_orders_check\",\n        passed=total > 0,\n        metadata={\"total_orders\": total}\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Table in DuckDB\nDESCRIPTION: Example of creating a DuckDB table using the DuckDB resource in Dagster\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n@asset\ndef iris_dataset(duckdb):\n    iris = load_iris()\n    df = pd.DataFrame(\n        iris.data, columns=iris.feature_names\n    ).assign(species=iris.target)\n    \n    df.columns = [\n        \"sepal_length\",\n        \"sepal_width\", \n        \"petal_length\",\n        \"petal_width\",\n        \"species\"\n    ]\n    \n    duckdb.execute_query(\n        \"\"\"CREATE TABLE IF NOT EXISTS iris.iris_dataset AS\n        SELECT * FROM df\n        \"\"\",\n        source_data={\"df\": df}\n    )\n```\n\n----------------------------------------\n\nTITLE: Importing SnowflakePandasIOManager in Python\nDESCRIPTION: This snippet shows how to import the SnowflakePandasIOManager class, which is an IOManagerDefinition for handling Snowflake and Pandas data in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-snowflake-pandas.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake_pandas import SnowflakePandasIOManager\n```\n\n----------------------------------------\n\nTITLE: Updating an Asset's Code Version in Branch Deployment - Python\nDESCRIPTION: Example of the same asset with a modified code_version parameter (changed from 'v1' to 'v2') in the branch deployment. This change will be detected by Change Tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(code_version=\"v2\")\ndef customers(): ...\n```\n\n----------------------------------------\n\nTITLE: Dagster Definitions Configuration\nDESCRIPTION: Final Dagster definitions setup including assets and resources configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/ingestion.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstart_def\ndefs = Definitions(\n    assets=[starter_pack_snapshot, actor_feed_snapshot],\n    resources={\n        \"atproto\": ATProtoResource(\n            username=EnvVar(\"BLUESKY_USER\"),\n            password=EnvVar(\"BLUESKY_PASS\"),\n        ),\n        \"s3\": S3Resource(),\n    },\n)\nend_def\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Resources and Jobs for Different Environments\nDESCRIPTION: Defines resources and jobs for both production and branch deployment environments, using environment variables to determine the appropriate configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, load_assets_from_modules, ScheduleDefinition\nfrom dagster_snowflake import SnowflakeResource, SnowflakeIOManager\nimport os\n\nfrom .assets import assets\nfrom .jobs import clone_prod, drop_prod_clone\n\nPRODUCTION_SNOWFLAKE_CONFIG = {\n    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n    \"database\": \"PRODUCTION\",\n    \"schema\": \"HACKER_NEWS\",\n}\n\nBRANCH_SNOWFLAKE_CONFIG = {\n    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n    \"database\": f\"PRODUCTION_CLONE_{os.getenv('DAGSTER_CLOUD_PULL_REQUEST_ID')}\",\n    \"schema\": \"HACKER_NEWS\",\n}\n\nresources = {\n    \"snowflake\": SnowflakeResource(\n        BRANCH_SNOWFLAKE_CONFIG if os.getenv(\"DAGSTER_CLOUD_IS_BRANCH_DEPLOYMENT\") else PRODUCTION_SNOWFLAKE_CONFIG\n    ),\n    \"snowflake_io_manager\": SnowflakeIOManager(\n        BRANCH_SNOWFLAKE_CONFIG if os.getenv(\"DAGSTER_CLOUD_IS_BRANCH_DEPLOYMENT\") else PRODUCTION_SNOWFLAKE_CONFIG\n    ),\n}\n\ndefs = Definitions(\n    assets=load_assets_from_modules([assets]),\n    resources=resources,\n    jobs=[\n        clone_prod.to_job(resource_defs=resources),\n        drop_prod_clone.to_job(resource_defs=resources),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Downstream Dependencies for Airbyte Cloud Assets\nDESCRIPTION: Define assets that are downstream of specific Airbyte Cloud tables using their asset keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(deps=[my_airbyte_cloud_table])\ndef my_downstream_asset(context, my_airbyte_cloud_table):\n    # Process data from my_airbyte_cloud_table\n    pass\n```\n\n----------------------------------------\n\nTITLE: Asset Check Implementation in External Code\nDESCRIPTION: Shows how to report asset check results from external code using PipesContext\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncontext.report_asset_check(\n    asset_key=[\"my_table\"],\n    success=True,\n    description=\"Check passed successfully\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Schema via Asset Key Prefix in DuckDB\nDESCRIPTION: Shows how to specify DuckDB schemas using asset key prefixes for different datasets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\niris_dataset = asset(key_prefix=[\"IRIS\"])(lambda: pd.read_csv(\"iris.csv\"))\ndaffodil_dataset = asset(key_prefix=[\"DAFFODIL\"])(lambda: pd.read_csv(\"daffodil.csv\"))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dagster Job Components and Dependencies\nDESCRIPTION: Mermaid diagram illustrating Job's relationships with Assets, Graph, Config, Schedule, Sensor, and Definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_9\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Graph fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Config fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Schedule fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Sensor fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Asset -.-> Job\n    Config -.-> Job\n    Graph -.-> Job\n\n    Job(Job)\n\n    Job -.-> Schedule\n    Job -.-> Sensor\n    Job ==> Definitions\n```\n\n----------------------------------------\n\nTITLE: Configuring BoolSource in Dagster Python\nDESCRIPTION: Illustrates the use of BoolSource for configuring an operation with either a boolean literal or an environment variable selector for boolean values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/config.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, BoolSource\n\n@op(config_schema=BoolSource)\ndef secret_bool_op(context) -> bool:\n    return context.op_config\n\n@job\ndef secret_job():\n    secret_bool_op()\n\nsecret_job.execute_in_process(\n    run_config={\n        'ops': {'secret_bool_op': {'config': False}}\n    }\n)\n\nsecret_job.execute_in_process(\n    run_config={\n        'ops': {'secret_bool_op': {'config': {'env': 'VERY_SECRET_ENV_VARIABLE_BOOL'}}}\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring HN API Client in Dagster Definitions\nDESCRIPTION: Demonstrates how to add the Hacker News API client instance to the resources configuration in the Dagster Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstart_hn_resource\n```\n\n----------------------------------------\n\nTITLE: Importing pandera_schema_to_dagster_type function from dagster_pandera\nDESCRIPTION: Reference to the pandera_schema_to_dagster_type function which converts Pandera schemas to Dagster types for dataframe validation. This function is the main component of the dagster_pandera integration library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-pandera.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pandera import pandera_schema_to_dagster_type\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Agent for Branch Deployments in Dagster+\nDESCRIPTION: YAML configuration for a Kubernetes Helm values file that enables branch deployments in Dagster+. It sets dagsterCloudAgent.branchDeployments to true and disables user deployments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloudAgent:\n  branchDeployments: true\n  userDeployments:\n    enabled: false\n\n```\n\n----------------------------------------\n\nTITLE: Determining Deployment Type in Dagster+ Using Environment Variables\nDESCRIPTION: A function that detects whether the current execution is in a branch deployment or a full production deployment by checking the DAGSTER_CLOUD_IS_BRANCH_DEPLOYMENT environment variable. Returns 'branch' for branch deployments and 'prod' for full deployments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_current_env():\n  is_branch_depl = os.getenv(\"DAGSTER_CLOUD_IS_BRANCH_DEPLOYMENT\") == \"1\"\n  assert is_branch_depl != None  # env var must be set\n  return \"branch\" if is_branch_depl else \"prod\"\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Compute Log Storage for Dagster Cloud\nDESCRIPTION: YAML configuration to set up S3 storage for compute logs in Dagster Cloud.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ncomputeLogs:\n  enabled: true\n  custom:\n    module: dagster_aws.s3.compute_log_manager\n    class: S3ComputeLogManager\n    config:\n      show_url_only: true\n      bucket: your-compute-log-storage-bucket\n      region: your-bucket-region\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Logic for Bluesky API Requests in Python\nDESCRIPTION: This function uses tenacity to handle rate limiting when fetching feed data from Bluesky. It implements a retry mechanism that backs off when hitting API limits to ensure successful data extraction.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/rate-limiting.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_all_feed_items(atproto_client, feed_type, limit=None, time_limit_seconds=None):\n    \"\"\"Gets all possible feed items for a feed_type with respect to time and request limits.\n\n    Args:\n        atproto_client: The AT Protocol client.\n        feed_type: The feed type to query.\n        limit: Optional limit of number of records to get.\n        time_limit_seconds: Optional time limit in seconds.\n\n    Returns:\n        List of feed items.\n    \"\"\"\n    items = []\n    cursor = None\n    start_time = time.time()\n\n    while True:\n        # Check if we've hit our limits\n        if limit is not None and len(items) >= limit:\n            break\n        if time_limit_seconds is not None and time.time() - start_time > time_limit_seconds:\n            break\n\n        # Get the next page of results\n        result = _get_feed_with_retries(atproto_client, feed_type, cursor=cursor)\n        feed = result.feed\n        cursor = result.cursor\n\n        if not feed:\n            break\n\n        items.extend(feed)\n\n        if not cursor:\n            break\n\n    return items\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Fivetran Connectors\nDESCRIPTION: Example of using ConnectorSelectorFn to load assets for only selected Fivetran connectors.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npath=\"docs_snippets/docs_snippets/integrations/fivetran/select_fivetran_connectors.py\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Assets with Run Configuration in Python\nDESCRIPTION: This example demonstrates how to set up dbt assets with a configuration schema to allow passing additional flags like --full-refresh at runtime.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@dbt_assets(\n    project=dbt_project,\n    config_schema={\n        \"full_refresh\": Field(bool, default_value=False, is_required=False),\n    },\n)\ndef dbt_project_assets(context: AssetExecutionContext):\n    # Run dbt build with optional full-refresh flag\n    yield from dbt_cli(\n        \"build\",\n        args=[\"--full-refresh\"] if context.op_config[\"full_refresh\"] else [],\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Backfill Processing in Dagster YAML\nDESCRIPTION: This snippet demonstrates how to configure backfill processing in the Dagster instance YAML file. It shows the setup for parallel processing using threads and specifying the number of workers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nbackfills:\n  use_threads: true\n  num_workers: 4\n```\n\n----------------------------------------\n\nTITLE: Setting Additional Repositories for Spark Jars - Configuration\nDESCRIPTION: This property defines a comma-separated list of additional repositories to search for Maven coordinates set by 'spark.jars.packages'. It allows for flexible resolution of dependencies in Spark applications.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_9\n\nLANGUAGE: properties\nCODE:\n```\nspark.jars.repositories\n```\n\n----------------------------------------\n\nTITLE: Defining and Accessing Asset Configuration in Python\nDESCRIPTION: This snippet demonstrates how to define a Config subclass for an asset and access it through the config parameter in the asset function body. The Config class defines a person_name field that will be provided at runtime.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/run-configuration.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AssetConfig(Config):\n    person_name: str\n\n@asset\ndef asset_using_config(config: AssetConfig):\n    # Access the config value through the config parameter\n    return f\"Hello, {config.person_name}!\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Dagster Documentation\nDESCRIPTION: This snippet lists the required Python packages for Dagster's documentation. It includes Sphinx for documentation generation, Click for command-line interfaces, and Docutils for text processing. Specific version requirements are provided for Sphinx and Click.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/_ext/sphinx-click/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx>=4.0\nclick>=8.0\ndocutils\n```\n\n----------------------------------------\n\nTITLE: Enhanced Dagster Asset with Context\nDESCRIPTION: Updated Dagster asset that passes model configuration to the JavaScript process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/javascript-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster_pipes import PipesSubprocessClient\n\n@asset(required_resource_keys={\"pipes\"}, compute_kind=\"javascript\")\ndef tensorflow_model(context):\n    context.resources.pipes.run_python(\n        \"node tensorflow/main.js\",\n        dagster_context={\n            \"model_config\": {\n                \"epochs\": 10,\n                \"layers\": [\n                    {\"units\": 1, \"inputShape\": [1]},\n                    {\"units\": 64, \"activation\": \"relu\"},\n                    {\"units\": 64, \"activation\": \"relu\"},\n                    {\"units\": 1}\n                ]\n            }\n        }\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Dagster Cloud Build Output\nDESCRIPTION: Command to update the build session with the Docker image tag for a specific code location\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/configuring-ci-cd.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud ci set-build-output --location-name=code-location-a --image-tag=IMAGE_TAG\n```\n\n----------------------------------------\n\nTITLE: Detecting Anomalous Events in Stock Prices using Bollinger Bands in Python\nDESCRIPTION: This snippet identifies anomalous events in the stock prices based on the computed Bollinger Bands.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/notebooks/bollinger.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nANOMALIES = bol.compute_anomalous_events(PRICES, BOLL)\n```\n\n----------------------------------------\n\nTITLE: Configuring High Availability for Dagster Cloud Agent\nDESCRIPTION: YAML configuration to set up multiple replicas for the Dagster Cloud agent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloudAgent:\n  replicas: 2\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project from Example\nDESCRIPTION: Command to create a new Dagster project from the 'development_to_production' example and install dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example development_to_production\ncd my-dagster-project\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagstermill Context and Data\nDESCRIPTION: Sets up the Dagstermill context and defines a list of integers for processing. This snippet prepares the initial data for computational operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/reimport.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n\ncontext = dagstermill.get_context()\nl = [3, 4]\n```\n\n----------------------------------------\n\nTITLE: Creating asset_keys Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'asset_keys' table which stores asset-related identifiers and timestamps. This structure allows for the management and tracking of assets within the system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: asset_keys; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.asset_keys OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Querying Pinecone for Similar Vectors in Dagster RAG System\nDESCRIPTION: Embeds the input question using OpenAI, then queries Pinecone for similar vectors within specific namespaces. Demonstrates vector similarity search and namespace filtering.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/retrieval.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nembedded_question = openai.embeddings.create(\n    model=\"text-embedding-3-small\", input=[config.question]\n).data[0].embedding\n\nresults = index.query(\n    vector=embedded_question,\n    top_k=5,\n    namespace=[\"dagster-github\", \"dagster-docs\"],\n    include_metadata=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple I/O Managers with Delta Lake in Python\nDESCRIPTION: Demonstrates how to use the Delta Lake I/O manager alongside other I/O managers for different assets in a Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@asset(io_manager_key=\"warehouse_io_manager\")\ndef iris_dataset():\n    # Create iris dataset\n\n@asset(io_manager_key=\"blob_io_manager\")\ndef iris_plots(iris_dataset):\n    # Create plots\n\ndefs = Definitions(\n    assets=[iris_dataset, iris_plots],\n    resources={\n        \"warehouse_io_manager\": delta_lake_pandas_io_manager,\n        \"blob_io_manager\": fs_io_manager\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration in YAML\nDESCRIPTION: Configuration for SQLite storage backend in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstorage:\n  sqlite:\n    base_dir: /path/to/sqlite/storage\n```\n\n----------------------------------------\n\nTITLE: Defining a Schedule with Resources in Python\nDESCRIPTION: This example shows how to create a schedule that utilizes resources. It sets up a daily schedule for processing data and uses a date_formatter resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/testing-schedules.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@schedule(\n    cron_schedule=\"0 0 * * *\",\n    job=process_data_job,\n    execution_timezone=\"US/Central\",\n)\ndef process_data_schedule(context):\n    date_to_format = context.scheduled_execution_time.strftime(\"%Y-%m-%d\")\n    formatted_date = context.resources.date_formatter.format_date(date_to_format)\n    return RunConfig(ops={\"process_data\": {\"config\": {\"date\": formatted_date}}})\n```\n\n----------------------------------------\n\nTITLE: Configuring Unsafe Rename Operations for S3\nDESCRIPTION: Configuration for enabling unsafe rename operations in single-writer scenarios where atomic operations are not required.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_deltalake import S3Config\n\nconfig = S3Config(allow_unsafe_rename=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Eager Automation Condition in Python\nDESCRIPTION: This snippet demonstrates how to define an eager automation condition in Dagster. It uses the 'missing' operand with 'newly_true' and 'since_last_handled' operators to create a condition that triggers only when an asset partition becomes missing and hasn't been handled yet.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/automation-condition-operands-and-operators.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagster as dg\n\ndg.AutomationCondition.missing().newly_true().since_last_handled()\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Daemon\nDESCRIPTION: Command to start the Dagster daemon process which handles schedules, sensors, and run queues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/deploying-dagster-as-a-service.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster-daemon run\n```\n\n----------------------------------------\n\nTITLE: Testing a Cron Schedule with Context in Python\nDESCRIPTION: This code snippet shows how to test a schedule by building a ScheduleEvaluationContext, invoking the schedule function, and validating the resulting run configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/testing-schedules.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import build_schedule_context, validate_run_config\n\ndef test_configurable_job_schedule():\n    context = build_schedule_context(\n        scheduled_execution_time=pendulum.datetime(2019, 2, 27, tz=\"US/Central\")\n    )\n    run_config = configurable_job_schedule(context)\n    assert isinstance(run_config, RunConfig)\n    validate_run_config(configurable_job, run_config.to_dict())\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Retries Using Tags in Python\nDESCRIPTION: Python example showing how to configure run retries using tags on Job definitions. This allows setting retry parameters directly in code rather than through configuration files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-retries.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job\n\n\n@job(tags={\"dagster/max_retries\": 3, \"dagster/retry_on_asset_or_op_failure\": False})\ndef sample_job():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Using Context in K-means Clustering\nDESCRIPTION: Shows how to use the configured context value in a scikit-learn K-means clustering implementation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nestimator = sklearn.cluster.KMeans(n_clusters=context.op_config)\n```\n\n----------------------------------------\n\nTITLE: Syncing Alert Policies Command in Dagster+\nDESCRIPTION: Command to synchronize alert policy configurations from a YAML file to a Dagster+ deployment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/alerts/example-config.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud deployment alert-policies sync -a /path/to/alert_policies.yaml\n```\n\n----------------------------------------\n\nTITLE: Importing K8sRunLauncher for Kubernetes Deployment\nDESCRIPTION: This snippet shows how to import the K8sRunLauncher, which is used for allocating a Kubernetes job per run when deploying Dagster to Kubernetes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-launchers.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_k8s import K8sRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Virtual Environment for MacOS\nDESCRIPTION: Commands for creating and activating a Python virtual environment on MacOS. This isolates the dependencies for the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv dagster_tutorial source dagster_tutorial/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Queue Routing\nDESCRIPTION: Configuration for routing code location requests to specific agents using custom queues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/multiple.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: data-eng-pipeline\n    code_source:\n      package_name: quickstart_etl\n    executable_path: venvs/path/to/dataengineering_spark_team/bin/python\n    agent_queue: special-queue\n```\n\nLANGUAGE: yaml\nCODE:\n```\nagent_queues:\n  include_default_queue: True # Continue to handle requests for code locations that aren't annotated with a specific queue\n  additional_queues:\n    - special-queue\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration in YAML\nDESCRIPTION: Example showing how to use environment variables to override values in the dagster.yaml configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninstance:\n  module: dagster.core.instance\n  class: DagsterInstance\n  config:\n    some_key:\n      env: ME_ENV_VAR\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow Configuration\nDESCRIPTION: Example GitHub Actions workflow configuration for AWS ECR registry setup\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\njobs:\n  dagster-cloud-deploy:\n    steps:\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v1\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: ${{ secrets.AWS_REGION }}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpDefinition in Dagster Python\nDESCRIPTION: This snippet shows the OpDefinition class, which represents a defined Op. It includes a 'configured' method for creating configured instances of the Op.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/ops.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: OpDefinition\n    :members: configured\n```\n\n----------------------------------------\n\nTITLE: Concurrency Configuration in YAML\nDESCRIPTION: Configuration for default operation concurrency limits in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  default_op_concurrency_limit: 10\n```\n\n----------------------------------------\n\nTITLE: Converting dbt Asset Key Generation to Legacy Behavior\nDESCRIPTION: Example of how to revert to the legacy behavior for AssetKey generation when using dbt integration. This customizes the node_info_to_asset_key function parameter to use only the node name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndbt_assets = load_assets_from_dbt_project(..., node_info_to_asset_key=lambda node_info: AssetKey(node_info[\"name\"]))\n```\n\n----------------------------------------\n\nTITLE: Executing Custom SQL Queries with DuckDB Resource in Python\nDESCRIPTION: This snippet demonstrates how to use the DuckDB resource to execute a custom SQL query within a Dagster asset. It selects small petals from the iris_dataset table using a DuckDBPyConnection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(required_resource_keys={\"duckdb\"})\ndef small_petals(context):\n    with context.resources.duckdb.get_connection() as conn:\n        result = conn.execute(\n            \"\"\"SELECT * FROM iris_dataset\n            WHERE petal_length_cm < 1.5\n            AND petal_width_cm < 0.3\"\"\"\n        ).fetch_df()\n    return result\n```\n\n----------------------------------------\n\nTITLE: Querying Customer Records with DuckDB SQL\nDESCRIPTION: SQL query to select the first 5 records from the raw_customers table in a DuckDB database located at /tmp/jaffle_platform.duckdb. Returns customer ID, first name, last name, and loading timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/16-duckdb-select.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM raw_customers LIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables with Configuration in Python\nDESCRIPTION: This example demonstrates how to use environment variables in configuration by passing an EnvVar object when constructing a config object. This is useful for sensitive values or values that vary by environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/run-configuration.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresult = config_job.execute_in_process(\n    run_config=RunConfig(\n        ops={\n            \"op_using_config\": OpConfig(\n                person_name=EnvVar(\"USER_NAME\")\n            )\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Scaffolding for ShellCommandComponent\nDESCRIPTION: Enhanced ShellCommandComponent that includes custom scaffolding logic to create template files when a new component instance is scaffolded.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, Definitions, asset\nfrom dagster._core.definitions.asset_spec import AssetSpec\nfrom dagster_components import Component, ResolvedAssetSpec, Resolvable, ComponentTypeMetadata, scaffoldable\nfrom pydantic import model_validator\nfrom typing_extensions import Annotated, override\nimport os\nimport subprocess\n\nfrom typing import List\n\n\n@scaffoldable(\n    yaml_str=\"\"\"component_type: shell_command\n\nattributes:\n  script_path: script.sh\n  asset_specs:\n    - key: my_shell_asset\n\"\"\",\n    files={\n        \"script.sh\": \"\"\"#!/bin/bash\necho \"Hello, world!\"\n\"\"\"\n    },\n    executable_files=[\"script.sh\"],\n)\nclass ShellCommandComponent(Resolvable, Component):\n    \"\"\"A component that executes a shell command.\"\"\"\n\n    script_path: str\n    \"\"\"The path to the shell script to execute.\"\"\"\n\n    asset_specs: List[ResolvedAssetSpec]\n    \"\"\"The assets that this script produces.\"\"\"\n\n    def execute(self, context):\n        \"\"\"Execute the shell script.\"\"\"\n        context.log.info(f\"Executing {self.script_path}\")\n        result = subprocess.run([\"bash\", self.script_path], capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Script failed with error: {result.stderr}\")\n        context.log.info(f\"Script output: {result.stdout}\")\n        return result.stdout\n\n    @override\n    def build_defs(self) -> Definitions:\n        \"\"\"Build the definitions for this component.\"\"\"\n        @asset(name=\"shell_script_output\", key_prefix=self.get_component_key())\n        def shell_script_asset(context):\n            return self.execute(context)\n\n        return Definitions(\n            assets=[shell_script_asset],\n        )\n\n    @classmethod\n    @override\n    def get_component_type_metadata(cls) -> ComponentTypeMetadata:\n        return ComponentTypeMetadata(\n            owners=[\"data-platform-team\"],\n            tags={\"tier\": \"production\"},\n        )\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Compute Log Storage in YAML\nDESCRIPTION: This configuration sets up writing compute logs to an AWS S3 bucket using the S3ComputeLogManager. It includes options for streaming partial log files, setting the upload interval, and specifying various S3-related parameters such as bucket name, region, and encryption.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/settings/customizing-agent-settings.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster.yaml\ninstance_class:\n  module: dagster_cloud.instance\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token:\n    env: DAGSTER_CLOUD_AGENT_TOKEN\n  deployment: prod\n\nuser_code_launcher:\n  module: dagster_cloud.workspace.docker\n  class: DockerUserCodeLauncher\n\ncompute_logs:\n  module: dagster_aws.s3.compute_log_manager\n  class: S3ComputeLogManager\n  config:\n    bucket: 'mycorp-dagster-compute-logs'\n    local_dir: '/tmp/cool'\n    prefix: 'dagster-test-'\n    use_ssl: true\n    verify: true\n    verify_cert_path: '/path/to/cert/bundle.pem'\n    endpoint_url: 'http://alternate-s3-host.io'\n    skip_empty_files: true\n    upload_interval: 30\n    upload_extra_args:\n      ServerSideEncryption: 'AES256'\n    show_url_only: true\n    region: 'us-west-1'\n```\n\n----------------------------------------\n\nTITLE: Stream Logs to Dagster\nDESCRIPTION: Demonstrates how to use Dagster Pipes context to send log messages back to Dagster. Uses context.log to stream info level messages.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/modify-external-code.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import open_dagster_pipes, PipesContext\n\nwith open_dagster_pipes():\n    orders = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n    context = PipesContext.get()\n    total = sum(quantity for _, quantity in orders)\n    context.log.info(f\"Total orders: {total}\")\n```\n\n----------------------------------------\n\nTITLE: Dask Task Resource Allocation\nDESCRIPTION: Python code showing how to submit tasks to a Dask cluster with specific resource requirements.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/dask.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclient.submit(task, resources={'GPU': 1})\n```\n\n----------------------------------------\n\nTITLE: Local Dask Job Configuration in YAML\nDESCRIPTION: YAML configuration for executing a Dagster job using a local Dask executor setup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/dask.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexecution:\n  config:\n    cluster:\n      n_workers: 2\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Definitions\nDESCRIPTION: Definition of Dagster Definitions object to include DLT assets and resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[\n        dagster_github_assets,\n    ],\n    resources={\n        \"dlt\": dlt_resource,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration in YAML\nDESCRIPTION: Example showing how to configure environment variables and secrets for a specific code location in Dagster+.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/configuration.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocation:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      k8s:\n        env_vars:\n          - database_name\n          - database_username=hooli_testing\n        env_secrets:\n          - database_password\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Pyarrow IO Manager Configuration\nDESCRIPTION: Configuration definition for the DeltaLakePyarrowIOManager class which handles Delta Lake I/O operations using PyArrow in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: DeltaLakePyarrowIOManager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Submitting a Job Run with Automatic Repository Inference\nDESCRIPTION: This example demonstrates submitting a job run when the job name is unique across repositories, allowing the client to automatically infer the repository location and repository name without explicitly specifying them.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/graphql-client.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Submit a run with just a job name\nrun_id = client.submit_job_execution(job_name=\"my_job\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Pipes Session for PySpark in Dagster\nDESCRIPTION: This code snippet initializes a Pipes session for orchestrating PySpark jobs in Dagster. It sets up S3 message reading and context injection for communication between Dagster and Spark.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/pyspark-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster_aws.s3 import S3Resource\nfrom dagster_pipes import PipesS3MessageReader, PipesS3ContextInjector, open_pipes_session\n\n@asset\ndef run_spark_job(s3: S3Resource):\n    with open_pipes_session(\n        message_reader=PipesS3MessageReader(\n            s3=s3,\n            bucket_name=\"dagster-pipes\",\n            include_stdio_in_messages=True,\n        ),\n        context_injector=PipesS3ContextInjector(\n            s3=s3,\n            bucket_name=\"dagster-pipes\",\n        ),\n    ) as session:\n```\n\n----------------------------------------\n\nTITLE: Dagster Asset Equivalent of PythonOperator\nDESCRIPTION: The Dagster equivalent implementation using @multi_asset decorator to define assets that can replace the functionality of an Airflow PythonOperator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/python-operator.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@multi_asset(\n    outs={\n        \"table_a\": AssetOut(),\n        \"table_b\": AssetOut(),\n        \"reports\": AssetOut(),\n    },\n    compute_kind=\"python\",\n)\ndef write_to_db(context):\n    \"\"\"Write processed data from local files to DB tables and generate reports.\"\"\"\n    directory = Path(\"/path/to/files\")\n    for file in directory.glob(\"*.csv\"):\n        with open(file) as f:\n            write_to_table_from_csv(f)\n    \n    # Let's say the function returns values we want to track as assets:\n    return (\n        {\"data\": \"table_a_data\"},  # table_a\n        {\"data\": \"table_b_data\"},  # table_b\n        {\"data\": \"reports_data\"},  # reports\n    )\n```\n\n----------------------------------------\n\nTITLE: Jobs Query in GraphQL\nDESCRIPTION: GraphQL query to list all jobs within a specific repository, requiring repository location name and repository name as parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/index.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nquery JobsQuery(\n  $repositoryLocationName: String!\n  $repositoryName: String!\n) {\n  repositoryOrError(\n    repositorySelector: {\n      repositoryLocationName: $repositoryLocationName\n      repositoryName: $repositoryName\n    }\n  ) {\n    ... on Repository {\n      jobs {\n        name\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Code Location in Python\nDESCRIPTION: Example showing how to create a code location by defining a top-level Definitions object that contains assets, schedules, sensors, and resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/managing-code-locations-with-definitions.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# definitions.py\n\ndefs = Definitions(\n    assets=[dbt_customers_asset, dbt_orders_asset],\n    schedules=[bi_weekly_schedule],\n    sensors=[new_data_sensor],\n    resources=[dbt_resource],\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Schedule Timing for Partitioned Job in Python\nDESCRIPTION: This snippet demonstrates how to customize the timing of a schedule for a partitioned job using parameters like minute_of_hour and hour_of_day in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/constructing-schedules-for-partitioned-assets-and-jobs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@daily_partitioned_config\ndef my_partitioned_config(start_date: datetime, _config):\n    return {\"date\": start_date.strftime(\"%Y-%m-%d\")}\n\n@job(config=my_partitioned_config)\ndef my_daily_job():\n    my_op()\n\nmy_job_schedule = build_schedule_from_partitioned_job(\n    job=my_daily_job,\n    name=\"my_job_schedule\",\n    minute_of_hour=30,\n    hour_of_day=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Isolated Agents for Dagster Cloud\nDESCRIPTION: YAML configuration to set up isolated agents for Dagster Cloud.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nisolatedAgents: true\n```\n\n----------------------------------------\n\nTITLE: Listing Core Dagster Packages for CI Build in Markdown\nDESCRIPTION: This code snippet lists the core Dagster packages that are used to declare dependencies. These declarations enable conditional skipping of unaffected parts during the CI build process. The packages listed are dagster, dagster-graphql, dagster-test, and dagster-postgres.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/buildkite_deps.txt#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# This file is used to declare dependencies so we can\n# conditionally skip unaffected parts of the CI build\ndagster\ndagster-graphql\ndagster-test\ndagster-postgres\n```\n\n----------------------------------------\n\nTITLE: dbt_project.yml with Insights Tracking Configuration\nDESCRIPTION: Enhanced dbt_project.yml with query-comment configuration to enable attribution of cost metrics to the correct Dagster assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/google-bigquery.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_dbt_project\nversion: 1.0.0\n\nquery-comment:\n  comment: \"dagster insights tracking\"\n  append: true\n\nmodels:\n  my_dbt_project:\n    materialized: view\n```\n\n----------------------------------------\n\nTITLE: Filtered Runs Query in GraphQL\nDESCRIPTION: GraphQL query that demonstrates filtering runs by specific criteria (in this example, by failure status).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/index.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nquery FilteredRunsQuery($cursor: String) {\n  runsOrError(\n    filter: { statuses: [FAILURE] }\n    cursor: $cursor\n    limit: 10\n  ) {\n    __typename\n    ... on Runs {\n      results {\n        runId\n        jobName\n        status\n        runConfigYaml\n        startTime\n        endTime\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Subsettable Graph-backed Asset Op in Python\nDESCRIPTION: This code shows how to implement a subsettable graph-backed asset op named 'foo'. It uses context.selected_output_names to conditionally yield outputs based on the selected asset subset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/graph-backed-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@op(out={\"foo_1\": Out(is_required=False), \"foo_2\": Out()})\ndef foo(context):\n    if \"foo_1\" in context.selected_output_names:\n        yield Output(1, output_name=\"foo_1\")\n    yield Output(2, output_name=\"foo_2\")\n```\n\n----------------------------------------\n\nTITLE: Scheduling Mixed dbt and Non-dbt Asset Jobs in Python\nDESCRIPTION: This snippet shows how to create and schedule a job that includes both dbt assets and downstream non-dbt assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[dbt_project_assets, *my_other_assets],\n    schedules=[\n        build_schedule_from_dbt_selection(\n            [dbt_project_assets],\n            job_name=\"daily_dbt_and_downstream\",\n            cron_schedule=\"0 0 * * *\",\n            dbt_select=\"tag:daily\",\n            asset_selection=lambda: AssetSelection.groups(\"dbt\").downstream(),\n        )\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring DockerRunLauncher in YAML\nDESCRIPTION: Configuration for DockerRunLauncher which allocates a Docker container per run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: dagster_docker\n  class: DockerRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Modifying an Asset's Partition Definition in Branch Deployment - Python\nDESCRIPTION: Example of the same asset with a modified WeeklyPartitionsDefinition that starts one year earlier in the branch deployment. This change will be detected by Change Tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset(partitions_def=WeeklyPartitionsDefinition(start_date=\"2023-01-01\"))\ndef weekly_orders(): ...\n```\n\n----------------------------------------\n\nTITLE: Fetching Column Metadata\nDESCRIPTION: Code showing how to emit column-level metadata for Fivetran assets using FivetranEventIterator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npath=\"docs_snippets/docs_snippets/integrations/fivetran/fetch_column_metadata_fivetran_assets.py\"\n```\n\n----------------------------------------\n\nTITLE: Initialize Dagster Pipes Context\nDESCRIPTION: Shows how to initialize Dagster Pipes context to enable communication between external code and Dagster. Demonstrates basic context setup with open_dagster_pipes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/modify-external-code.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import open_dagster_pipes, PipesContext\n\nwith open_dagster_pipes():\n    orders = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\n    context = PipesContext.get()\n    total = sum(quantity for _, quantity in orders)\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Asset Definitions\nDESCRIPTION: This snippet shows the formatted output of the 'dg list defs' command in Dagster. It displays a table with sections for different types of definitions, focusing on Assets. The table includes columns for the asset key, group, dependencies, kinds, and description.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/5-list-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list defs\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Section ┃ Definitions                                         ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Assets  │ ┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┓ │\n│         │ ┃ Key      ┃ Group   ┃ Deps ┃ Kinds ┃ Description ┃ │\n│         │ ┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━┩ │\n│         │ │ my_asset │ default │      │       │             │ │\n│         │ └──────────┴─────────┴──────┴───────┴─────────────┘ │\n└─────────┴─────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Visualizing Clustering Results\nDESCRIPTION: Creates a complex visualization grid comparing the clustering results with actual species classifications across different feature combinations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/tutorial_template/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(ncols=3, nrows=3, figsize=(12, 12))\n\nsns.scatterplot(\n    x=\"Sepal width (cm)\",\n    y=\"Sepal length (cm)\",\n    data=iris,\n    ax=axs[0, 0],\n    hue=\"Species\",\n    style=\"K-means cluster assignment\",\n    legend=False,\n    marker=\"x\",\n    alpha=0.5,\n)\n# ... [Additional plotting code omitted for brevity]\n\nimport matplotlib.patches as mpatches\nimport matplotlib.lines as mlines\n\npalette = sns.color_palette()\nsetosa = mpatches.Patch(color=palette[0], label=\"Iris setosa\", alpha=0.5)\nversicolor = mpatches.Patch(color=palette[1], label=\"Iris versicolor\", alpha=0.5)\nvirginica = mpatches.Patch(color=palette[2], label=\"Iris virginica\", alpha=0.5)\n\nclass_0 = mlines.Line2D(\n    [], [], marker=\"o\", color=\"lightgrey\", linestyle=\"None\", markersize=10, label=\"Class 0\"\n)\nclass_1 = mlines.Line2D(\n    [], [], marker=\"X\", color=\"lightgrey\", linestyle=\"None\", markersize=10, label=\"Class 1\"\n)\nclass_2 = mlines.Line2D(\n    [], [], marker=\"s\", color=\"lightgrey\", linestyle=\"None\", markersize=10, label=\"Class 2\"\n)\n\naxs[2, 0].legend(handles=[setosa, versicolor, virginica, class_0, class_1, class_2])\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster Dependencies for CI Build Optimization\nDESCRIPTION: This snippet enumerates various Dagster packages and modules that are dependencies for the project. It's used to conditionally skip unaffected parts of the CI build process, improving build efficiency.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/celery-k8s-test-suite/buildkite_deps.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# This file is used to declare dependencies so we can\n# conditionally skip unaffected parts of the CI build\ndagster\ndagster-graphql\ndagster-test\ndagster-pandas\ndagster-k8s\ndagster-celery\ndagster-celery-k8s\ndagster-celery-docker\ndagster-postgres\ndagster-airflow\ndagster-docker\ndagster-aws\ndagster-gcp\ndagster-k8s-test-infra\n```\n\n----------------------------------------\n\nTITLE: Creating Event Logs Indexes in PostgreSQL\nDESCRIPTION: Creates indexes on the event_logs table for efficient querying by event type, asset key, run ID, and step key\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_event_type ON public.event_logs USING btree (dagster_event_type, id);\n\nCREATE INDEX idx_events_by_asset ON public.event_logs USING btree (asset_key, dagster_event_type, id) WHERE (asset_key IS NOT NULL);\n\nCREATE INDEX idx_events_by_asset_partition ON public.event_logs USING btree (asset_key, dagster_event_type, partition, id) WHERE ((asset_key IS NOT NULL) AND (partition IS NOT NULL));\n\nCREATE INDEX idx_events_by_run_id ON public.event_logs USING btree (run_id, id);\n\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n```\n\n----------------------------------------\n\nTITLE: Dagster Definitions Configuration\nDESCRIPTION: Python code configuring Dagster definitions with EMR-specific resources and assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-containers-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_aws.pipes import PipesEMRContainersClient, PipesS3MessageReader\n\ndefs = Definitions(\n    assets=[emr_containers_asset],\n    resources={\n        \"pipes_emr_containers\": PipesEMRContainersClient(\n            message_reader=PipesS3MessageReader(\n                include_stdio_in_messages=True\n            )\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Items Asset with HN API Client Resource\nDESCRIPTION: Shows how to modify an items asset to utilize the Hacker News API client resource for data fetching and processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dev-to-prod.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstart_items\n```\n\n----------------------------------------\n\nTITLE: Basic Snowflake Resource Configuration (Before Insights)\nDESCRIPTION: Example of a standard Snowflake resource configuration in Dagster without Insights tracking. This code defines resources for connecting to Snowflake and includes defs for working with these resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/snowflake.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_snowflake import SnowflakeResource\n\ndefs = Definitions(\n    assets=[...],\n    resources={\n        \"snowflake\": SnowflakeResource(\n            account=\"abc12345\",\n            user=\"user\",\n            password=\"password\",\n            database=\"database\",\n            warehouse=\"warehouse\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up Virtual Environment - Windows\nDESCRIPTION: Commands to create and activate a Python virtual environment using venv on Windows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/installation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv source venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Initial Definitions Configuration\nDESCRIPTION: Shows the original definitions.py file that manually loads definitions from multiple modules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom elt import elt_defs\nfrom ml import ml_defs\nfrom viz import viz_defs\n\ndefs = Definitions(\n    assets=[*elt_defs.assets, *ml_defs.assets, *viz_defs.assets],\n    jobs=[*elt_defs.jobs, *ml_defs.jobs, *viz_defs.jobs],\n    schedules=[*elt_defs.schedules, *ml_defs.schedules, *viz_defs.schedules],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster GCP Integration\nDESCRIPTION: Command to install the Dagster GCP integration package using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/gcs.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-gcp\n```\n\n----------------------------------------\n\nTITLE: Defining Python Executable for PySpark - Configuration\nDESCRIPTION: This property sets the Python binary to use for both the driver and executors in a PySpark application. This enables consistency in the Python environment across Spark jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_11\n\nLANGUAGE: properties\nCODE:\n```\nspark.pyspark.python\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery I/O Manager with GCP Credentials in Python\nDESCRIPTION: Example of providing base64 encoded GCP credentials to the BigQuery I/O manager in Dagster configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_gcp_pandas import BigQueryPandasIOManager\n\ndefs = Definitions(\n    resources={\n        \"io_manager\": BigQueryPandasIOManager(\n            project=\"my-project\",\n            gcp_credentials=\"${GCP_CREDS}\",\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing DBT Model Group Names in YAML\nDESCRIPTION: Example of overriding the Dagster group name for a dbt model using meta configuration. Sets the group name as 'marketing' for the customers model.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: customers\n    config:\n      meta:\n        dagster:\n          group: marketing\n```\n\n----------------------------------------\n\nTITLE: Removed Configuration in dagster.yaml for Backfill Settings\nDESCRIPTION: Example of removed backfill configuration in dagster.yaml that is no longer valid in Dagster 0.10.0. In this version, all backfills are now daemon-based and require dagster-daemon to be running.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\n# removed, no longer a valid setting in dagster.yaml\n\nbackfill:\n  daemon_enabled: true\n```\n\n----------------------------------------\n\nTITLE: Snowflake Resource with Insights Tracking (After)\nDESCRIPTION: Enhanced Snowflake resource configuration that implements Insights tracking. This code uses InsightsSnowflakeResource instead of the standard SnowflakeResource and adds the Snowflake insights assets and schedule to capture usage metrics.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/snowflake.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_cloud.dagster_insights import create_snowflake_insights_asset_and_schedule\nfrom dagster_cloud.dagster_insights.resources import InsightsSnowflakeResource\n\n# Create the asset and schedule definitions needed for collecting Snowflake metrics\nsnowflake_insights_defs = create_snowflake_insights_asset_and_schedule(\n    resources={\n        \"snowflake\": InsightsSnowflakeResource(\n            account=\"abc12345\",\n            user=\"user\",\n            password=\"password\",\n            database=\"database\",\n            warehouse=\"warehouse\",\n        )\n    }\n)\n\ndefs = Definitions(\n    assets=[...],\n    resources={\n        \"snowflake\": InsightsSnowflakeResource(\n            account=\"abc12345\",\n            user=\"user\",\n            password=\"password\",\n            database=\"database\",\n            warehouse=\"warehouse\",\n        )\n    },\n    # Incorporate the snowflake insights defs into your Definitions\n    **snowflake_insights_defs,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing PipesGlueClient in Dagster asset code\nDESCRIPTION: Use the PipesGlueClient resource in Dagster asset code to launch and monitor AWS Glue jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-glue-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef glue_pipes_asset(glue_client: PipesGlueClient):\n    glue_client.run_job(\n        job_name=\"my-glue-job\",\n        arguments={\n            \"--my-arg\": \"my-value\",\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Request Configuration for Borough Data Query\nDESCRIPTION: Example JSON configuration file showing the required fields for submitting a data request. The request specifies a date range and borough for data processing, which will generate results in the data/results directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_dagster_university_start/data/requests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"start_date\": \"2023-01-10\",\n  \"end_date\": \"2023-01-25\",\n  \"borough\": \"Staten Island\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Dagster Cloud Agent Helm Chart Repository\nDESCRIPTION: Adds the Dagster Cloud Helm chart repository and updates local Helm cache.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add dagster-cloud https://dagster-io.github.io/helm-user-cloud\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB PySpark I/O Manager\nDESCRIPTION: Configuration setup for DuckDB PySpark I/O Manager in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"io_manager\": DuckDBPySparkIOManager(\n            database=\"example.duckdb\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Step Success Event in Dagster Pipeline (JSON)\nDESCRIPTION: JSON log entry indicating the successful completion of the 'do_input.compute' step in the Dagster pipeline, showing execution duration and related metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 16.563892364501953}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Finished execution of step \\\"do_input.compute\\\" in 16ms.\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - STEP_SUCCESS - Finished execution of step \\\"do_input.compute\\\" in 16ms.\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.702914, \"user_message\": \"Finished execution of step \\\"do_input.compute\\\" in 16ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Key Constraint to Runs in PostgreSQL\nDESCRIPTION: Establishes a unique constraint for the 'run_id' column in the 'runs' table to ensure that each run is uniquely identifiable.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n```\n\n----------------------------------------\n\nTITLE: Amazon ECS Agent Per-Deployment Configuration in dagster.yaml\nDESCRIPTION: Complete configuration example for dagster.yaml used by Amazon ECS agents, showing instance class settings, API configuration, user code launcher settings, and agent properties. Typically created by the CloudFormation template.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/amazon-ecs/configuration-reference.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ninstance_class:\n  module: dagster_cloud\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token: <Agent Token String>\n  deployments:\n    - <Deployment Name>\n    - <Optional Additional Deployment Name>\n  branch_deployments: <true|false>\n\nuser_code_launcher:\n  module: dagster_cloud.workspace.ecs\n  class: EcsUserCodeLauncher\n  config:\n    cluster: <Cluster Name>\n    subnets:\n      - <Subnet Id 1>\n      - <Subnet Id 2>\n    security_group_ids:\n      - <Security Group ID>\n    service_discovery_namespace_id: <Service Discovery Namespace Id>\n    execution_role_arn: <Task Execution Role Arn>\n    task_role_arn: <Task Role Arn>\n    log_group: <Log Group Name>\n    launch_type: <\"FARGATE\"|\"EC2\">\n    server_process_startup_timeout: <Timeout in seconds>\n    server_resources:\n      cpu: <CPU value>\n      memory: <Memory value>\n    server_sidecar_containers:\n      - name: SidecarName\n        image: SidecarImage\n        <Additional container fields>\n    run_resources:\n      cpu: <CPU value>\n      memory: <Memory value>\n    run_sidecar_containers:\n      - name: SidecarName\n        image: SidecarImage\n        <Additional container fields>\n    mount_points:\n      - <List of mountPoints to pass into register_task_definition>\n    volumes:\n      - <List of volumes to pass into register_task_definition>\n    server_ecs_tags:\n      - key: MyEcsTagKey\n        value: MyEcsTagValue\n    run_ecs_tags:\n      - key: MyEcsTagKeyWithoutValue\n    repository_credentials: MyRepositoryCredentialsSecretArn\n\nisolated_agents:\n  enabled: <true|false>\nagent_queues:\n  include_default_queue: <true|false>\n  additional_queues:\n    - <queue name>\n    - <additional queue name>\n```\n\n----------------------------------------\n\nTITLE: Loading Airflow DAG Asset Specs in Python\nDESCRIPTION: Using the load_airflow_dag_asset_specs function to create asset representations of DAGs in the 'warehouse' Airflow instance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_airlift import load_airflow_dag_asset_specs\n\nall_warehouse_assets = load_airflow_dag_asset_specs(\n    warehouse_airflow,\n    prefix=[\"warehouse\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Cloud YAML\nDESCRIPTION: Example configuration for dagster_cloud.yaml file specifying the build registry settings\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/dagster-plus/deployment/branch-deployments/dagster_cloud.yaml\" language=\"yaml\" />\n```\n\n----------------------------------------\n\nTITLE: Testing Nested Resource Construction in Python\nDESCRIPTION: Shows how to test a ConfigurableResource that requires other resources by passing them as constructor arguments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/testing-configurable-resources.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef test_my_nested_resource():\n    inner_resource = InnerResource(baz=\"qux\")\n    resource = MyResource(foo=\"bar\", inner=inner_resource)\n    assert resource.foo == \"bar\"\n    assert resource.inner.baz == \"qux\"\n```\n\n----------------------------------------\n\nTITLE: Configuring K8sRunLauncher in YAML for Kubernetes Deployment\nDESCRIPTION: A YAML configuration example for the K8sRunLauncher that allows Dagster to launch runs as Kubernetes Jobs. This configuration specifies essential parameters like service account, job image, storage locations, and environment configurations needed for Kubernetes integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: dagster_k8s.launcher\n  class: K8sRunLauncher\n  config:\n    image_pull_secrets:\n    service_account_name: dagster\n    job_image: \"my-company.com/image:latest\"\n    dagster_home: \"/opt/dagster/dagster_home\"\n    postgres_password_secret: \"dagster-postgresql-secret\"\n    image_pull_policy: \"IfNotPresent\"\n    job_namespace: \"dagster\"\n    instance_config_map: \"dagster-instance\"\n    env_config_maps:\n      - \"dagster-k8s-job-runner-env\"\n    env_secrets:\n      - \"dagster-k8s-some-secret\"\n```\n\n----------------------------------------\n\nTITLE: Defining Hacker News Assets in Python for Dagster\nDESCRIPTION: Defines three assets (ITEMS, COMMENTS, STORIES) representing Hacker News data tables to be persisted in Snowflake using a custom I/O manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(io_manager_key=\"snowflake_io_manager\")\ndef items():\n    return get_items()\n\n@asset(io_manager_key=\"snowflake_io_manager\")\ndef comments(items):\n    return get_comments(items)\n\n@asset(io_manager_key=\"snowflake_io_manager\")\ndef stories(items):\n    return get_stories(items)\n```\n\n----------------------------------------\n\nTITLE: Adding Asset Checks to Peered Airflow DAGs\nDESCRIPTION: Python code that extends the Airflow-Dagster integration by adding asset checks to validate data quality. This example checks that the customers CSV output exists and has rows, providing additional observability on top of Airflow DAGs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/peer.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport urllib.parse\n\nfrom dagster import Definitions, AssetChecksDefinition, asset_check\nfrom dagster_airlift.core import build_defs_from_airflow_instance\n\nAIRFLOW_URL = os.getenv(\"AIRFLOW__API__URL\", \"http://localhost:8080/api/\")\n\ncore_defs = build_defs_from_airflow_instance(\n    url=AIRFLOW_URL,\n    # Credentials aren't required if the Airflow instance doesn't require auth\n    # username=urllib.parse.quote(os.environ.get(\"AIRFLOW_USERNAME\", \"airflow\")),\n    # password=urllib.parse.quote(os.environ.get(\"AIRFLOW_PASSWORD\", \"airflow\")),\n)\n\n\n@asset_check(asset=core_defs.assets.get_asset(\"rebuild_customers_list\"))\ndef check_customers_csv(context):\n    import pandas as pd\n\n    customers_file = \"tutorial_example/working_dir/customers.csv\"\n    assert os.path.exists(customers_file), f\"The customers CSV file at {customers_file} doesn't exist\"\n    df = pd.read_csv(customers_file)\n    assert len(df) > 0, \"The customers CSV has zero rows\"\n\n\ndefs = Definitions(\n    assets=core_defs.assets,\n    sensors=core_defs.sensors,\n    asset_checks=[check_customers_csv],\n    # If you want to use the UI, but don't want the sensor to monitor your Airflow instance,\n    # you can instead include an explicit empty list of sensors: sensors=[]\n)\n```\n\n----------------------------------------\n\nTITLE: Compute Logs Configuration in YAML\nDESCRIPTION: Configuration for local compute log storage in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster.core.storage.local_compute_log_manager\n  class: LocalComputeLogManager\n  config:\n    base_dir: /path/to/compute/logs\n```\n\n----------------------------------------\n\nTITLE: Starting Celery Worker with Custom Config\nDESCRIPTION: Command to start a Celery worker with a custom configuration YAML file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/celery.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndagster-celery worker start -y /path/to/celery_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install the necessary Python packages for Dagster-Snowflake integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-snowflake pandas\n```\n\n----------------------------------------\n\nTITLE: Executing a Job with Invalid Configuration in Python\nDESCRIPTION: This example shows what happens when invalid configuration is provided to a job. The execution will fail with validation errors because nonexistent_config_value is not defined in the Config schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/run-configuration.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# This will fail because \"nonexistent_config_value\" isn't defined in the schema\nresult = config_job.execute_in_process(\n    run_config=RunConfig(\n        ops={\n            \"op_using_config\": {\"nonexistent_config_value\": \"will_fail\"}\n        }\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring code location secrets in dagster_cloud.yaml\nDESCRIPTION: YAML configuration for adding environment secrets to a specific code location in the dagster_cloud.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nlocation:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      k8s:\n        env_secrets:\n          - database-password\n```\n\n----------------------------------------\n\nTITLE: Executing Dagit\nDESCRIPTION: This snippet shows how to execute the 'dagit' command with a specified port to run Dagster UI. 'dagit' is the command-line tool that brings up Dagster's web-based interface. This command requires the Dagster package to be installed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagit/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndagit -p 3333\n```\n\n----------------------------------------\n\nTITLE: Configuration YAML for Pipeline Execution\nDESCRIPTION: YAML configuration for the Dagster pipeline that specifies the user input location for the fuel station search.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/additional-prompt.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nops:\n  user_input_prompt:\n    config:\n      location: I'm near the The Art Institute of Chicago and driving a Kia EV9\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Instance\nDESCRIPTION: YAML configuration example for setting instance parameters like run coordinator limits in dagster.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/running-dagster-locally.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n~/.dagster_home/dagster.yaml\n```\n\n----------------------------------------\n\nTITLE: Updated Installation Pattern for Dagster with Extensions\nDESCRIPTION: The recommended way to install Dagster with extension libraries following the 1.0 release, as extension libraries remain on 0.16.x versioning track while being compatible with Dagster 1.x.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster=={DAGSTER_VERSION} dagster-somelibrary\n```\n\n----------------------------------------\n\nTITLE: Configuring ADLSComputeLogManager in YAML for Dagster\nDESCRIPTION: This snippet demonstrates various ways to configure the ADLSComputeLogManager, including authentication with access key, service principal, Azure CLI, and environment variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/obstore.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# there are multiples ways to configure the ADLSComputeLogManager\n# Authenticate with access key\ncompute_logs:\n  module:  dagster_obstore.azure.compute_log_manager\n  class: ADLSComputeLogManager\n  config:\n    storage_account: \"my-az-account\"\n    container: \"dagster-logs\"\n    access_key:\n      env: ACCESS_KEY\n    local_dir: \"/tmp/dagster-logs\"\n    allow_http: false\n    allow_invalid_certificates: false\n    timeout: \"60s\"  # Timeout for obstore requests\n# Authenticate with service principal\ncompute_logs:\n  module:  dagster_obstore.azure.compute_log_manager\n  class: ADLSComputeLogManager\n  config:\n    storage_account: \"my-az-account\"\n    container: \"dagster-logs\"\n    client_id: \"access-key-id\"\n    client_secret: \"my-key\"\n    tenant_id: \"tenant-id\"\n    local_dir: \"/tmp/dagster-logs\"\n# Authenticate with use_azure_cli\ncompute_logs:\n  module:  dagster_obstore.azure.compute_log_manager\n  class: ADLSComputeLogManager\n  config:\n    storage_account: \"my-az-account\"\n    container: \"dagster-logs\"\n    use_azure_cli: true\n    local_dir: \"/tmp/dagster-logs\"\n# Don't set secrets through config, but let obstore pick it up from ENV VARS\ncompute_logs:\n  module:  dagster_obstore.azure.compute_log_manager\n  class: ADLSComputeLogManager\n  config:\n    storage_account: \"my-az-account\"\n    container: \"dagster-logs\"\n    local_dir: \"/tmp/dagster-logs\"\n```\n\n----------------------------------------\n\nTITLE: Logging with Context in Dagstermill Python\nDESCRIPTION: This snippet demonstrates using the retrieved context to log a message. The context's log method is invoked to log informational messages, which can be useful for debugging and monitoring during pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_logging.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontext.log.info(\"Hello, there!\")\n```\n\n----------------------------------------\n\nTITLE: Deploying Code to a Branch Deployment in Dagster Cloud\nDESCRIPTION: This snippet shows the command to deploy code to a branch deployment within a CI/CD process. It includes parameters for specifying the organization, deployment name, API token, location file, location name, image, commit hash, and git URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud deployment add-location \\\n    --organization $ORGANIZATION_NAME \\\n    --deployment $BRANCH_DEPLOYMENT_NAME \\\n    --api-token $DAGSTER_CLOUD_API_TOKEN \\\n    --location-file $LOCATION_FILE \\\n    --location-name $LOCATION_NAME \\\n    --image \"${LOCATION_REGISTRY_URL}:${IMAGE_TAG}\" \\\n    --commit-hash \"${COMMIT_SHA}\" \\\n    --git-url \"${GIT_URL}\"\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository for Dagster Chart\nDESCRIPTION: This command retrieves the latest information about the Dagster Helm chart, which is updated with every Open Source (OSS) release.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/migrating-while-upgrading.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and AWS dependencies\nDESCRIPTION: Install the necessary packages for Dagster and AWS integration using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-glue-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-webserver dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Scraping for Individual Pages in Python\nDESCRIPTION: This code snippet defines a method in the SitemapScraper class to scrape content from individual web pages. It uses the requests library to fetch page content and BeautifulSoup for HTML parsing. The scraped content is converted into a LangChain Document for further processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/sources.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain.schema import Document\n\nclass SitemapScraper:\n    def scrape_page(self, url: str) -> Document:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n\n        # Find the main content of the page (adjust selector as needed)\n        content = soup.find(\"main\")\n\n        if content:\n            # Extract text content\n            text = content.get_text(strip=True)\n\n            # Create a Document\n            doc = Document(\n                page_content=text,\n                metadata={\"source\": url}\n            )\n\n            return doc\n        else:\n            return None\n```\n\n----------------------------------------\n\nTITLE: Representing Sigma Assets in Dagster Asset Graph\nDESCRIPTION: Python code to load Sigma assets into the Dagster asset graph using SigmaOrganization resource and load_sigma_asset_specs function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sigma.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_sigma import SigmaOrganization, load_sigma_asset_specs\n\nsigma_organization = SigmaOrganization(\n    base_url=\"https://api.sigmacomputing.com\",\n    client_id=\"your-client-id\",\n    client_secret=\"your-client-secret\",\n)\n\nsigma_asset_specs = load_sigma_asset_specs(\n    sigma_organization=sigma_organization,\n)\n\ndefs = Definitions(\n    assets=sigma_asset_specs,\n    resources={\n        \"sigma_organization\": sigma_organization,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Assets to the Dagster Definitions Object\nDESCRIPTION: Updates the Definitions object to include all the created assets (products, sales_reps, and sales_data), making them available to the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/create-and-materialize-assets.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndefs = dg.Definitions(\n    assets=[products,\n        sales_reps,\n        sales_data,\n    ],\n    resources={\"duckdb\": DuckDBResource(database=\"data/mydb.duckdb\")},\n)\n```\n\n----------------------------------------\n\nTITLE: Using Input Manager Decorator for Unconnected Inputs\nDESCRIPTION: Demonstrates how to use the input_manager decorator to load unconnected inputs from an external source.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/unconnected-inputs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import input_manager, In, op, job\n\n@input_manager\ndef table1_loader(context):\n    # do something to load data\n    return [1, 2, 3]\n\n@op(ins={\"table1\": In(input_manager_key=\"table1\")})\ndef process_table1(table1):\n    return table1\n\n@job(resource_defs={\"table1\": table1_loader})\ndef my_job():\n    process_table1()\n```\n\n----------------------------------------\n\nTITLE: Dask Worker Resource Configuration\nDESCRIPTION: Shell command to launch a Dask worker with specific GPU resource specifications.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/dask.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndask-worker scheduler:8786 --resources \"GPU=2\"\n```\n\n----------------------------------------\n\nTITLE: Defining Failed Scheduling Condition in Dagster\nDESCRIPTION: This snippet defines the failed scheduling condition, which indicates an asset's last computation attempt failed, allowing for conditional retries or alerts based on asset status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.failed()\n```\n\n----------------------------------------\n\nTITLE: Importing Data Science Libraries\nDESCRIPTION: This snippet imports common data science libraries: Pandas for data manipulation, scikit-learn for linear regression, and Matplotlib for plotting. These libraries are essential for the data analysis and modeling tasks performed in the notebook.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_LR.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Dagster Docker Integration Code Example\nDESCRIPTION: Reference to an example file showing Docker integration with Dagster. The example demonstrates how to use PipesDockerClient resource to interact with Docker containers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/docker.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndocs_snippets/docs_snippets/integrations/docker.py\n```\n\n----------------------------------------\n\nTITLE: Applying Scheduling Conditions to Dependencies in Dagster\nDESCRIPTION: This snippet demonstrates how to apply scheduling conditions to asset dependencies in Dagster, allowing for flexible scheduling based on related asset states.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.any_deps_match(a)\n```\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.all_deps_match(a)\n```\n\n----------------------------------------\n\nTITLE: Adding Tags to an Asset in Branch Deployment - Python\nDESCRIPTION: Example of the same asset with an additional tag 'type: perishable' in the branch deployment. Change Tracking will detect when tags are added, modified, or removed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@asset(tags={\"section\": \"produce\", \"type\": \"perishable\"})\ndef fruits_in_stock(): ...\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for project_fully_featured Example in Dagster\nDESCRIPTION: This snippet lists the dependencies for the project_fully_featured example, including various Dagster modules and libraries for AWS, dbt, DuckDB, pandas, PostgreSQL, GCP, PySpark, Slack, Spark, and Snowflake integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/alt-1/requirements.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n-e examples/project_fully_featured[tests]\n  -e python_modules/dagster[pyright,test]\n  -e python_modules/dagster-pipes\n  -e python_modules/libraries/dagster-shared\n  -e python_modules/dagster-webserver\n  -e python_modules/dagster-graphql\n  -e python_modules/libraries/dagster-aws[stubs]\n  -e python_modules/libraries/dagster-dbt/\n  -e python_modules/libraries/dagster-duckdb-pandas/\n    -e python_modules/libraries/dagster-duckdb/\n    -e python_modules/libraries/dagster-duckdb-pyspark/\n  -e python_modules/libraries/dagster-pandas/\n  -e python_modules/libraries/dagster-postgres/\n  -e python_modules/libraries/dagster-gcp[test,dataproc]\n  -e python_modules/libraries/dagster-pyspark/\n  -e python_modules/libraries/dagster-slack/\n  -e python_modules/libraries/dagster-spark/\n  -e python_modules/libraries/dagster-snowflake/\n    -e python_modules/libraries/dagster-snowflake-pandas/\n    -e python_modules/libraries/dagster-snowflake-pyspark/\n```\n\n----------------------------------------\n\nTITLE: Updating Dagster Cloud YAML Configuration for Azure Container Registry\nDESCRIPTION: YAML configuration update in dagster_cloud.yaml to use Azure Container Registry for building and storing images.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/acr-user-code.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: quickstart_etl\n    code_source:\n      package_name: quickstart_etl.definitions\n    build:\n      directory: ./\n      registry: <your-acr-name>.azurecr.io/<image-name>\n```\n\n----------------------------------------\n\nTITLE: Searching Dagster Helm Repository\nDESCRIPTION: Searches the Helm repository for Dagster charts. This command helps users find the latest version of the Dagster chart available in the repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm search repo dagster\n```\n\n----------------------------------------\n\nTITLE: Configuring Code Location Docker Image\nDESCRIPTION: Dockerfile setup for individual code locations that will be used for running Dagster jobs. Includes code server configuration and exposes necessary ports.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/docker.md#2025-04-22_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.10-slim\n\nRUN pip install \\\n    dagster \\\n    dagster-postgres \\\n    dagster-docker\n\n# Add code location code\nWORKDIR /opt/dagster/app\nCOPY directory/with/your/code/ /opt/dagster/app\n\n# Run dagster code server on port 4000\nEXPOSE 4000\n\n# CMD allows this to be overridden from run launchers or executors to execute runs and steps\nCMD [\"dagster\", \"code-server\", \"start\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-f\", \"definitions.py\"]\n```\n\n----------------------------------------\n\nTITLE: Updating Specific Time-Lagged Partitions with Cron\nDESCRIPTION: Shows how to configure a cron-based automation to target specific time-lagged partitions instead of the latest partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/example-customizations.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(auto_materialize_policy=AutoMaterializePolicy(AutomationCondition.partition_delay(days=1)))\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Partitioned Assets with Delta Lake I/O Manager in Python\nDESCRIPTION: Shows how to configure a static partitioned asset for storage in Delta Lake by specifying the partition expression metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"partition_expr\": \"species\"})\ndef iris_by_species(context: OpExecutionContext):\n    # Asset implementation\n```\n\n----------------------------------------\n\nTITLE: Configuring RDS for Dagster Run and Event Log Storage\nDESCRIPTION: This YAML configuration sets up a PostgreSQL database on RDS for storing Dagster run and event data. It specifies the connection details and storage settings for the Dagster instance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/aws.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nstorage:\n  postgres:\n    postgres_db:\n      username:\n        env: DAGSTER_PG_USERNAME\n      password:\n        env: DAGSTER_PG_PASSWORD\n      hostname:\n        env: DAGSTER_PG_HOST\n      db_name:\n        env: DAGSTER_PG_DB\n      port: 5432\n\nrun_storage:\n  module: dagster_postgres.run_storage\n  class: PostgresRunStorage\n\nevent_log_storage:\n  module: dagster_postgres.event_log\n  class: PostgresEventLogStorage\n\nschedule_storage:\n  module: dagster_postgres.schedule_storage\n  class: PostgresScheduleStorage\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint on snapshots.id\nDESCRIPTION: This SQL statement defines the primary key for the `snapshots` table, using the `id` column. This ensures that each snapshot has a unique identifier.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\n\"ALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sensor Evaluation Interval\nDESCRIPTION: Shows how to set the minimum interval between sensor evaluations using the minimum_interval_seconds parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Sensor will be evaluated at least every 30 seconds\n@dg.sensor(job=my_job, minimum_interval_seconds=30)\ndef new_file_sensor():\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining DBT Documentation Block with Jinja for Jaffle Shop Project\nDESCRIPTION: A Jinja template documentation block that provides an overview of the Jaffle Shop project. It explains that Jaffle Shop is a fictional ecommerce store used for testing DBT code and includes a link to the source code repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_python_interleaving/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Bootstrapping Dagster Project with OpenAI Example\nDESCRIPTION: This command creates a new Dagster project using the OpenAI example as a template. It sets up the project structure and initial files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_openai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example with_openai\n```\n\n----------------------------------------\n\nTITLE: Preserving Default Metadata for dbt Resources in Dagster\nDESCRIPTION: Example of creating a custom metadata function that preserves the default metadata from dbt resource properties while adding custom metadata, maintaining behavior similar to pre-1.4 versions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_dbt import default_metadata_from_dbt_resource_props\n\ndef my_metadata_from_dbt_resource_props(dbt_resource_props):\n    my_metadata = {...}\n    return {**default_metadata_from_dbt_resource_props(dbt_resource_props), **my_metadata}\n\nload_assets_from_dbt_manifest(\n    ...,\n    node_info_to_definition_metadata_fn=my_metadata_from_dbt_resource_props\n)\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Typed Input in Python\nDESCRIPTION: Illustrates how to define an op with a typed input using the 'ins' parameter of the @op decorator. This allows for runtime type checking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@op(ins={\"x\": In(int)})\ndef my_op(x):\n    return x * 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Isolated Docker Agents\nDESCRIPTION: Docker configuration for running isolated agents in different environments using dagster.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/multiple.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nisolated_agents:\n  enabled: true\n\ndagster_cloud_api:\n  # <your other config>\n  agent_label: 'My agent' # optional\n```\n\n----------------------------------------\n\nTITLE: Configuring Sling Replication for CSV to DuckDB\nDESCRIPTION: YAML configuration for Sling that defines how to replicate data from CSV files to DuckDB. It specifies the source files to load, the target database, and the schema to use.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/components-etl-pipeline-tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsource:\n  plugin: FILE\n  options:\n    files:\n      - path: ./data/jaffle_shop_customers.csv\n        format: CSV\n        schema_name: raw\n        table_name: customers\n\n      - path: ./data/jaffle_shop_orders.csv\n        format: CSV\n        schema_name: raw\n        table_name: orders\n        options:\n          null_if: [\"\"]\n\n      - path: ./data/jaffle_shop_payments.csv\n        format: CSV\n        schema_name: raw\n        table_name: payments\n\ntarget:\n  plugin: DUCKDB\n  options:\n    database: \"jaffle_shop.duckdb\"\n    output_mode: REPLACE\n    create_schemas: true\n\n```\n\n----------------------------------------\n\nTITLE: Defining Event Types for Dagster Ops in Python\nDESCRIPTION: These snippets show various event types that can be yielded or raised by Ops to communicate with the Dagster framework during execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/ops.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Output\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: AssetMaterialization\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: ExpectationResult\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: TypeCheck\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Failure\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: RetryRequested\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Launches the Dagster UI web server in development mode, making the project accessible through the browser at http://localhost:3000.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_du_dbt_starter/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Dependencies\nDESCRIPTION: Command to install Dagster and required dependencies using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/quickstart.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-webserver pandas\n```\n\n----------------------------------------\n\nTITLE: Bypassing Merge Threshold for Sort-based Shuffle - Shuffle Settings\nDESCRIPTION: This advanced setting defines the threshold for avoiding merge-sorting data in sort-based shuffle operations, which can enhance performance under certain conditions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_26\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.sort.bypassMergeThreshold\n```\n\n----------------------------------------\n\nTITLE: Creating Latest Feed Analysis Model\nDESCRIPTION: SQL model that extracts the latest feed data by leveraging the partitioning structure in R2. It uses a CTE to filter for the most recent data based on date partitioning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/modeling.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nwith latest_feed_cte as (\n    select\n        snapshot_date,\n        author_did,\n        actor_did,\n        value,\n        text\n    from {{ ref('stg_feed_snapshots') }}\n    where snapshot_date = (\n        select max(snapshot_date) \n        from {{ ref('stg_feed_snapshots') }}\n    )\n),\n```\n\n----------------------------------------\n\nTITLE: Accessing Dagster+ Built-in Environment Variables\nDESCRIPTION: A Python code snippet showing how to access Dagster+ specific built-in environment variables using os.getenv. This example retrieves the deployment name from the environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/using-environment-variables-and-secrets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\ndeployment_name = os.getenv(\"DAGSTER_CLOUD_DEPLOYMENT_NAME\")\n```\n\n----------------------------------------\n\nTITLE: Dagster Object Store GET Operation Event in JSON Format\nDESCRIPTION: JSON representation of a Dagster object store operation record. This event documents the retrieval of an intermediate object from a filesystem object store for the 'build_cost_dashboard' step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_costs/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_costs/result\"}, \"label\": \"key\"}], \"op\": \"GET_OBJECT\", \"value_name\": \"_\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_cost_dashboard - OBJECT_STORE_OPERATION - Retrieved intermediate object for input _ in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1608666933.7482061, \"user_message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Fivetran Dependencies\nDESCRIPTION: Command to install the required Python packages dagster and dagster-fivetran using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-fivetran\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Template File\nDESCRIPTION: Command to copy the environment template file to create a .env file. This file will store environment variables needed for the project configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/index.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Loading dbt Project Configuration\nDESCRIPTION: Python code that loads and prepares the dbt project manifest for use in Dagster, with development environment handling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/load-dbt-models.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\njaffle_shop_project = load_project_from_dir(\"../\")\njaffle_shop_project.prepare_if_dev()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies in setup.py for Dagster+ Serverless\nDESCRIPTION: Example setup.py file showing how to specify Python dependencies for a Dagster project deployed to Dagster+ Serverless. This configuration includes basic Dagster requirements along with additional packages.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"quickstart_etl\",\n    packages=find_packages(exclude=[\"quickstart_etl_tests\"]),\n    install_requires=[\n        \"dagster\",\n        \"boto3\",\n        \"pandas\",\n        \"matplotlib\",\n        'soda @ https://pypi.cloud.soda.io/packages/soda-1.6.2.tar.gz',\n        'soda-snowflake @ https://pypi.cloud.soda.io/packages/soda_snowflake-1.6.2.tar.gz'\n    ],\n    extras_require={\"dev\": [\"dagster-webserver\", \"pytest\"]},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema via Asset Metadata in DuckDB\nDESCRIPTION: Demonstrates how to specify a DuckDB schema using asset metadata configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"schema\": \"FLOWERS\"})\ndef iris_dataset() -> pd.DataFrame:\n    return pd.read_csv(\"iris.csv\")\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster DagsterEventRecord JSON for Pipeline Execution Logs\nDESCRIPTION: These JSON objects represent Dagster execution events, capturing the state and progress of pipeline runs. Each event contains metadata such as event type, timestamp, process ID, and detailed execution information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 16.563892364501953}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Finished execution of step \\\"do_input.compute\\\" in 16ms.\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - STEP_SUCCESS - Finished execution of step \\\"do_input.compute\\\" in 16ms.\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.702914, \"user_message\": \"Finished execution of step \\\"do_input.compute\\\" in 16ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint for Run Tags in PostgreSQL for Dagster\nDESCRIPTION: Adds a foreign key constraint between the run_tags and runs tables with cascade delete, ensuring that each tag references a valid run ID and is deleted when the run is deleted.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_62\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Using Standalone Patito Model with Custom Dagster Type\nDESCRIPTION: Shows how to create a custom Dagster type from a Patito model using patito_model_to_dagster_type function. This approach allows using the validation with any IOManager while working with regular Polars DataFrames.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/patito.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_polars.patito import patito_model_to_dagster_type\n\nuser_type = patito_model_to_dagster_type(User)\n\n\n@dg.asset(io_manager_key=\"polars_parquet_io_manager\", dagster_type=user_type)\ndef my_asset() -> pl.DataFrame:\n    my_data = ...\n    return pl.DataFrame(my_data)  # <- you better behave, mr. data!\n```\n\n----------------------------------------\n\nTITLE: Scaffolding Dagster Component Using CLI\nDESCRIPTION: This command uses the Dagster CLI to generate a scaffold for a new component named 'ShellCommand' within the 'my_component_library.lib' module. The scaffold provides a basic structure for the component, which can then be customized as needed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/shell-script-component/4-scaffold-instance-of-component.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold 'my_component_library.lib.ShellCommand' my_shell_command\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraints - PostgreSQL\nDESCRIPTION: SQL statements that add primary key constraints to various tables including event_logs, job_ticks, jobs, runs and others. Each constraint ensures unique identification of records in their respective tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.normalized_cereals\n    ADD CONSTRAINT normalized_cereals_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Adding Eager Automation to Product Performance Asset in Dagster\nDESCRIPTION: Updates the product_performance asset to include the eager automation condition, ensuring it automatically runs when all its dependencies have been successfully materialized.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/automate-your-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(\n    compute_kind=\"duckdb\",\n    io_manager_def=dg.io_manager(\n        pickle_io_manager, base_dir=\"data\"\n    ),\n    name=\"product_performance\",\n    group_name=\"analysis\",\n    metadata={\n        \"owner\": \"data_analytics_team\",\n        \"stage\": \"analysis\"\n    },\n    auto_materialize_policy=dg.AutoMaterializePolicy.eager(),\n    key_prefix=[\"analysis\"]\n)\ndef product_performance(context, joined_data):\n    \"\"\"Product performance analysis.\"\"\"\n    # Group data by product to calculate metrics\n    product_perf = (\n        joined_data\n        .groupby([joined_data.product_name, joined_data.product_category])\n        .agg([\n            fn.sum(joined_data.price).alias(\"total_sales\"),\n            fn.sum(fn.case([(joined_data.returned == True, joined_data.price)], 0))\n                .alias(\"total_returns\"),\n            fn.count(joined_data.id).alias(\"items_sold\"),\n            fn.count(fn.case([(joined_data.returned == True, 1)], None)).alias(\"items_returned\"),\n        ])\n        .sort(\"total_sales\", ascending=False)\n    )\n    \n    # Calculate return rate\n    product_perf = product_perf.select(\n        \"product_name\",\n        \"product_category\",\n        \"total_sales\",\n        \"total_returns\",\n        \"items_sold\",\n        \"items_returned\",\n        (fn.cast(product_perf.items_returned, types.FLOAT) / product_perf.items_sold).alias(\"return_rate\")\n    )\n    \n    context.log.info(f\"Calculated product performance metrics\")\n    return product_perf\n```\n\n----------------------------------------\n\nTITLE: Creating a Dagster Project from Example\nDESCRIPTION: Command to initialize a Dagster project using a pre-built example template for ETL tutorials. This sets up the necessary directory structure and files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/index.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --example getting_started_etl_tutorial\n```\n\n----------------------------------------\n\nTITLE: Generating Dagster Assets from dbt Models\nDESCRIPTION: Python code that uses DagsterDbtTranslator to map ingestion assets to dbt sources and generate Dagster assets from dbt models. This creates the DAG structure and lineage in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/modeling.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndbt_assets = load_assets_from_dbt_project(\n    project_dir=os.path.join(os.path.dirname(__file__), \"../../dbt_project\"),\n    node_info_to_group_fn=lambda node_info: \"dbt\",\n    dagster_dbt_translator=DagsterDbtTranslator(\n        dbt_resource_key=\"dbt\",\n        create_asset_inputs_fn=lambda source_asset_key, _dbt_resource_props: {\n            AssetKey(\n                [\n                    source_asset_key[-2],\n                    f\"{source_asset_key[-2]}_{source_asset_key[-1]}_snapshot\",\n                ]\n            ): AssetKey([\"ingestion\", f\"{source_asset_key[-1]}_snapshot\"])\n        },\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Asset Definition Metadata\nDESCRIPTION: Example of customizing Fivetran asset properties using a custom DagsterFivetranTranslator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/fivetran.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npath=\"docs_snippets/docs_snippets/integrations/fivetran/customize_fivetran_translator_asset_spec.py\"\n```\n\n----------------------------------------\n\nTITLE: Dagster Code Implementation\nDESCRIPTION: Example showing the Dagster code configuration to set up Databricks integration. Shows how to define assets and ops that interact with Databricks jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/databricks.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/integrations/databricks/dagster_code.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Testing Sensor with Resources\nDESCRIPTION: Example demonstrating how to test a sensor that uses external resources by mocking the resource implementation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-sensors.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MockUsersApi:\n    def __init__(self):\n        self._users = []\n\n    def get_new_users(self):\n        return self._users\n\n    def add_user(self, user):\n        self._users.append(user)\n\ntest_users_api = MockUsersApi()\nwith instance_for_test() as instance:\n    context = build_sensor_context(\n        instance=instance,\n        resources={\"users_api\": test_users_api},\n    )\n\n    # No users yet\n    result = process_new_users_sensor(context)\n    assert result.skip_reason == \"No new users to process\"\n\n    # Add a user\n    test_users_api.add_user({\"name\": \"alice\"})\n    result = process_new_users_sensor(context)\n    assert len(result.run_requests) == 1\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Actions Workflow for Azure Container Registry\nDESCRIPTION: YAML configuration for GitHub Actions workflow to build and push Docker images to Azure Container Registry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/acr-user-code.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Login to Azure Container Registry\n  if: steps.prerun.outputs.result != 'skip'\n  uses: docker/login-action@v3\n  with:\n    registry: ${{ env.IMAGE_REGISTRY }}\n    username: ${{ secrets.AZURE_CLIENT_ID }}\n    password: ${{ secrets.AZURE_CLIENT_SECRET }}\n```\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Build and upload Docker image for \"quickstart_etl\"\n  if: steps.prerun.outputs.result != 'skip'\n  uses: docker/build-push-action@v4\n  with:\n    context: .\n    push: true\n    tags: ${{ env.IMAGE_REGISTRY }}/<image-name>:${{ env.IMAGE_TAG }}\n    cache-from: type=gha\n    cache-to: type=gha,mode=max\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Schedule for Daily Job\nDESCRIPTION: Demonstrates how to create a daily schedule for automatically running asset jobs in Dagster\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_snowflake/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nall_assets_job = define_asset_job(\n    name=\"all_assets_job\", \n    selection=\"*\"\n)\n\ndaily_schedule = ScheduleDefinition(\n    job=all_assets_job,\n    cron_schedule=\"0 0 * * *\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Daemon and Webserver Locally\nDESCRIPTION: Launches both the Dagster webserver and daemon using a single command for local development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/dagster-daemon.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-evidence package with uv\nDESCRIPTION: Command to install the dagster-evidence Python package using the uv package manager. This installs the Dagster integration with Evidence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/27-uv-add-evidence.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nuv add dagster-evidence\n```\n\n----------------------------------------\n\nTITLE: Defining Inputs and Outputs for Dagster Ops in Python\nDESCRIPTION: These snippets show the In and Out classes used to define inputs and outputs for Ops in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/ops.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: In\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: Out\n```\n\n----------------------------------------\n\nTITLE: Starting the Dagster Docker Agent\nDESCRIPTION: Docker run command that starts the Dagster agent container. It mounts the configuration file and Docker socket as volumes, allowing the agent to communicate with the Docker daemon and launch containers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/docker/setup.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n  --network=dagster_cloud_agent \\\n  --volume $PWD/dagster.yaml:/opt/dagster/app/dagster.yaml:ro \\\n  --volume /var/run/docker.sock:/var/run/docker.sock \\\n  -it docker.io/dagster/dagster-cloud-agent:latest \\\n  dagster-cloud agent run /opt/dagster/app\n```\n\n----------------------------------------\n\nTITLE: Downloading and Setting Up the Tutorial Project\nDESCRIPTION: Commands to download the tutorial project, change to the project directory, and install dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster project from-example --name tutorial_notebook_assets --example tutorial_notebook_assets\ncd tutorial_notebook_assets\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Available Top-Level Queries for Dagster+ GraphQL API\nDESCRIPTION: This GraphQL schema snippet shows the available top-level queries for retrieving reporting metrics by job, asset, and asset group. It includes the query names and their required parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/export-metrics.md#2025-04-22_snippet_2\n\nLANGUAGE: graphql\nCODE:\n```\nreportingMetricsByJob(\n  metricsFilter: JobReportingMetricsFilter\n  metricsSelector: ReportingMetricsSelector!\n): ReportingMetricsOrError!\n\nreportingMetricsByAsset(\n  metricsFilter: AssetReportingMetricsFilter\n  metricsSelector: ReportingMetricsSelector!\n): ReportingMetricsOrError!\n\nreportingMetricsByAssetGroup(\n  metricsFilter: AssetGroupReportingMetricsFilter\n  metricsSelector: ReportingMetricsSelector!\n): ReportingMetricsOrError!\n```\n\n----------------------------------------\n\nTITLE: Setting Pool Granularity to Run-Level in YAML Configuration\nDESCRIPTION: This YAML configuration sets the pool enforcement granularity to 'run', which limits the total number of runs containing specific ops at any given time, rather than limiting the total number of ops actively executing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/managing-concurrency.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconcurrency:\n  pools:\n    granularity: 'run'\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Tableau Dependencies\nDESCRIPTION: Commands to install the required Dagster and Tableau integration packages via pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/tableau.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-tableau\n```\n\n----------------------------------------\n\nTITLE: Setting Run Priority in Python using Dagster Tags\nDESCRIPTION: Example showing how to set a negative priority (-1) for a Dagster run using the dagster/priority tag in Python code.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/customizing-run-queue-priority.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@job(tags={\"dagster/priority\": \"-1\"})\ndef my_job():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Adding dbt Preparation Step to CI/CD Workflow in YAML\nDESCRIPTION: This YAML snippet adds a step to the CI/CD workflow file to prepare the dbt project for deployment. It installs necessary dependencies and packages the dbt project for use with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster-plus/serverless.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Prepare DBT project for deployment\n  if: steps.prerun.outputs.result == 'pex-deploy'\n  run: |\n    python -m pip install pip --upgrade\n    cd project-repo\n    pip install . --upgrade --upgrade-strategy eager\n    dagster-dbt project prepare-and-package --file <DAGSTER_PROJECT_FOLDER>/project.py\n  shell: bash\n```\n\n----------------------------------------\n\nTITLE: Terminate Run Mutation in GraphQL\nDESCRIPTION: GraphQL mutation to stop execution of an in-progress Dagster run, requiring only the run ID as a parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/index.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmutation TerminateRun($runId: String!) {\n  terminateRun(runId: $runId){\n    __typename\n    ... on TerminateRunSuccess{\n      run {\n        runId\n      }\n    }\n    ... on TerminateRunFailure {\n      message\n    }\n    ... on RunNotFoundError {\n      runId\n    }\n    ... on PythonError {\n      message\n      stack\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Fitting Random Forest Regressor Model\nDESCRIPTION: This snippet creates a Random Forest Regressor model, fits it to the provided features and target, and logs the fitting event.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = sklearn.ensemble.RandomForestRegressor()\nfit = model.fit(X, y)\ncontext.log.info(\"Fitted random forest model!\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Teradata Provider Package\nDESCRIPTION: Commands to install the required packages including dagster, dagster-webserver, and dagster-teradata. Optional dependencies for AWS and Azure are also mentioned.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-webserver dagster-teradata\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-teradata[aws]\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-teradata[azure]\n```\n\n----------------------------------------\n\nTITLE: Installing and Verifying dbt CLI\nDESCRIPTION: Commands to install the dbt CLI package via pip and verify the installation version. Requires dbt version 0.19.0 or higher.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/dbt_project/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dbt\ndbt --version  # Check your dbt version. You should have dbt>=0.19.0\n```\n\n----------------------------------------\n\nTITLE: Handling Multiple Dynamic Outputs in Python with Dagster\nDESCRIPTION: This snippet demonstrates how to handle multiple dynamic outputs using a namedtuple in Dagster. Each entry in the namedtuple can be used via map or collect functions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/dynamic-graphs.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DynamicOut, DynamicOutput, job, op\nfrom collections import namedtuple\n\nDynamicOutputs = namedtuple(\"DynamicOutputs\", [\"evens\", \"odds\"])\n\n@op(out={\"evens\": DynamicOut(), \"odds\": DynamicOut()})\ndef dynamic_outs():\n    for i in range(10):\n        if i % 2 == 0:\n            yield DynamicOutput(value=i, mapping_key=str(i), output_name=\"evens\")\n        else:\n            yield DynamicOutput(value=i, mapping_key=str(i), output_name=\"odds\")\n\n@job\ndef multiple_dynamic_outputs_job():\n    dynamic_stuff = dynamic_outs()\n    dynamic_stuff.evens.map(lambda x: x)\n    dynamic_stuff.odds.map(lambda x: x)\n```\n\n----------------------------------------\n\nTITLE: Creating a Failing Test Job for Run Failure Sensor in Python with Dagster\nDESCRIPTION: This code snippet defines a Dagster job that always fails, to be used for testing run failure sensors. It includes an op that raises an exception and a job that uses this op.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-run-status-sensors.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef fail_op():\n    raise Exception(\"Deliberate failure\")\n\n@job\ndef fail_job():\n    fail_op()\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Materialization to Open Source Deployment using Python\nDESCRIPTION: Python example using the requests library to report an asset materialization to a self-hosted Dagster deployment. Uses a simple GET request with the asset key in the URL path.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nurl = f\"{DAGSTER_WEBSERVER_HOST}/report_asset_materialization/{ASSET_KEY}\"\nresponse = requests.request(\"POST\", url)\nresponse.raise_for_status()\n```\n\n----------------------------------------\n\nTITLE: Configuring SQLite Storage in Dagster YAML\nDESCRIPTION: Example configuration for using SQLite storage in the dagster.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nstorage:\n  sqlite:\n    base_dir: \"/path/to/dagster/home/storage\"\n```\n\n----------------------------------------\n\nTITLE: Creating Schedules in Dagster ETL Project\nDESCRIPTION: This file defines the weekly_update_schedule for the ETL pipeline, which determines when pipeline runs are automatically triggered on a weekly basis.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/refactor-your-project.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Code placeholder for schedules.py\n# The actual implementation would be shown in the CodeExample component\n```\n\n----------------------------------------\n\nTITLE: Capturing Python Logs in Dagster\nDESCRIPTION: Example of using a custom Python logger that won't be captured by Dagster by default. This demonstrates the need for configuring managed_python_loggers in dagster.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/python-logging.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogger = logging.getLogger(\"my_logger\")\nlogger.setLevel(logging.INFO)\n\n@asset\ndef my_asset():\n    logger.info(\"This is a log from a custom Python logger\")\n    return 5\n```\n\n----------------------------------------\n\nTITLE: Creating Fuel Station Availability Prompt in Python\nDESCRIPTION: Generates a prompt template that combines current time with fuel station operating hours to determine if a station is currently open. The prompt is structured to return JSON output.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/additional-prompt.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\nIt is {current_time} right now. The fuel station is open from {hours_of_operation}. Based on this, is the fuel station currently open?\n\nProvide the response in JSON with a property \"is_open\" that is either true or false.\n\n```\n\n----------------------------------------\n\nTITLE: Creating schedules Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the schedules table that holds information about scheduled jobs and their statuses, allowing better management of periodic tasks within Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: schedules; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.schedules (\n    id integer NOT NULL,\n    schedule_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    schedule_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.schedules OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Running Jupyter Notebook from Command Line\nDESCRIPTION: Shell command to start a Jupyter notebook server and open a specific notebook file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\njupyter notebook /path/to/new/notebook.ipynb\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for CeleryK8sRunLauncher and Executor\nDESCRIPTION: ReStructuredText documentation defining the module imports and configurations for Celery-Kubernetes integration in Dagster, including the CeleryK8sRunLauncher and celery_k8s_job_executor components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery-k8s.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: dagster_celery_k8s\n\n.. autoconfigurable:: CeleryK8sRunLauncher\n  :annotation: RunLauncher\n\n.. autoconfigurable:: celery_k8s_job_executor\n  :annotation: ExecutorDefinition\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster GraphQL Client and Automation Module\nDESCRIPTION: The snippet provides the commands to install the Dagster GraphQL client along with related automation modules. Dependencies include the 'pip' tool and access to the 'python_modules' directory. This setup allows the 'dagster-graphql-client' to be accessible from the command line for subsequent operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation/graphql/python_client/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \\\n-e python_modules/dagster-graphql \\\n-e python_modules/automation\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Docker Integration\nDESCRIPTION: Command to install the dagster-docker package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/docker.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-docker\n```\n\n----------------------------------------\n\nTITLE: Creating test-schema.sort_by_calories View\nDESCRIPTION: This snippet creates a view named 'sort_by_calories' within the 'test-schema' schema. The view selects all columns from the 'cereals' table and orders the results by the 'calories' column. This view provides a sorted list of cereals based on their calorie content.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE VIEW \\\"test-schema\\\".sort_by_calories AS\n SELECT cereals.name,\n    cereals.mfr,\n    cereals.type,\n    cereals.calories,\n    cereals.protein,\n    cereals.fat,\n    cereals.sodium,\n    cereals.fiber,\n    cereals.carbo,\n    cereals.sugars,\n    cereals.potass,\n    cereals.vitamins,\n    cereals.shelf,\n    cereals.weight,\n    cereals.cups,\n    cereals.rating\n   FROM \\\"test-schema\\\".cereals\n  ORDER BY cereals.calories;\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Agent for Branch Deployments in YAML\nDESCRIPTION: Modifies the Helm values file to enable branch deployments for a Kubernetes agent. Sets branchDeployments to true and configures server TTL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/using-branch-deployments-with-the-cli.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloud:\n  branchDeployments: true\n---\nworkspace:\n  serverTTL:\n    enabled: true\n    ttlSeconds: 7200\n```\n\n----------------------------------------\n\nTITLE: Generating and Applying Dagster Helm JSON Schema\nDESCRIPTION: These commands generate and apply the JSON schema for the Dagster Helm chart. The schema is generated from a Pydantic model of the chart's values.  The first command installs the necessary CLI tools, the second displays the schema, and the third updates the existing schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Install the cli to generate the JSON Schema\npip install -e ./schema\n\n# Display the resulting schema from the Dagster chart values Pydantic model\ndagster-helm schema show\n\n# Update the existing schema\ndagster-helm schema apply\n```\n\n----------------------------------------\n\nTITLE: Creating Table: runs in PostgreSQL\nDESCRIPTION: Defines 'runs' table to store metadata about execution runs, including pipeline name, run ID, and timestamps. Features fields for start and end times, providing comprehensive details about each run's execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    mode text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    start_time double precision,\n    end_time double precision\n);\nALTER TABLE public.runs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Visualizing Model Predictions\nDESCRIPTION: This snippet visualizes the true petal widths versus the predicted widths using a scatter plot, which helps in assessing model accuracy visually.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npred_y = fit.predict(X)\nplt.scatter(y, pred_y)\nplt.title(\"True vs. predicted\")\nplt.xlabel(\"True petal width\")\nplt.ylabel(\"Predicted petal width\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project in Editable Mode\nDESCRIPTION: Installs the Dagster project as a Python package in editable mode, allowing local code changes to apply automatically during development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_du_dbt_starter/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Setting up environment and running Dagster\nDESCRIPTION: Bash commands to set up environment variables and start Dagster pointing to the asset created from the Airflow DAG.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/peer.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport TUTORIAL_EXAMPLE_DIR=$(pwd)\nexport TUTORIAL_DBT_PROJECT_DIR=\"$TUTORIAL_EXAMPLE_DIR/tutorial_example/shared/dbt\"\nexport AIRFLOW_HOME=\"$TUTORIAL_EXAMPLE_DIR/.airflow_home\"\ndagster dev -f tutorial_example/dagster_defs/definitions.py\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server\nDESCRIPTION: This command starts the Dagster development server, allowing you to view and interact with the example in Dagster's UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_pyspark/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Process Exit Engine Event in Dagster (JSON)\nDESCRIPTION: JSON log entry indicating the exit of a Dagster pipeline process, including the process ID that was terminated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 80630).\", \"pid\": null, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Process for pipeline exited (pid: 80630).\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": null, \"timestamp\": 1610466063.748248, \"user_message\": \"Process for pipeline exited (pid: 80630).\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration Package\nDESCRIPTION: This command installs the dagster-aws package, which is required for integrating Dagster with AWS CloudWatch.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/cloudwatch.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Job Ticks in PostgreSQL\nDESCRIPTION: This command adds a primary key on the 'id' column of the 'job_ticks' table, ensuring each job tick entry remains unique.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Create runs Table\nDESCRIPTION: This SQL statement creates the `runs` table for storing information about Dagster runs. It includes fields for run ID, snapshot ID, pipeline name, status, run body, partition, partition set, creation timestamp, and update timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\"\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server\nDESCRIPTION: Command to start the Dagster development server, which generates the dbt manifest at runtime for local development of the integrated dbt and Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Configuring S3ComputeLogManager in Helm values.yaml\nDESCRIPTION: Configuration for storing compute logs in your own S3 bucket when deploying the Kubernetes agent using Helm.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/managing-compute-logs-and-error-messages.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncomputeLogs:\n  enabled: true\n  custom:\n    module: dagster_aws.s3.compute_log_manager\n    class: S3ComputeLogManager\n    config:\n      show_url_only: true\n      bucket: your-compute-log-storage-bucket\n      region: your-bucket-region\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Cloud YAML for Monorepo\nDESCRIPTION: Example YAML configuration for defining multiple code locations in a monorepo setup, including location names, package names, and build directories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/index.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: core\n    code_source:\n      package_name: core\n    build:\n      directory: ./core\n      registry: your-registry/image # eg 764506304434.dkr.ecr.us-west-2.amazonaws.com/core\n  - location_name: new\n    code_source:\n      package_name: new\n    build:\n      directory: ./new\n      registry: your-registry/image # eg 764506304434.dkr.ecr.us-west-2.amazonaws.com/new\n```\n\n----------------------------------------\n\nTITLE: Importing CeleryK8sRunLauncher for Celery with Kubernetes\nDESCRIPTION: This code imports the CeleryK8sRunLauncher, which is used for launching runs as single Kubernetes jobs with extra configuration to support the celery_k8s_job_executor.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-launchers.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_celery_k8s import CeleryK8sRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Polars DataFrames with Dagster Assets\nDESCRIPTION: Example showing how to use Polars DataFrames with Dagster assets using the PolarsParquetIOManager. The default behavior loads eager DataFrames but can return LazyFrames.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-polars.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nimport polars as pl\n\n@asset(io_manager_key=\"polars_parquet_io_manager\")\ndef upstream():\n    return DataFrame({\"foo\": [1, 2, 3]})\n\n@asset(io_manager_key=\"polars_parquet_io_manager\")\ndef downstream(upstream) -> pl.LazyFrame:\n    assert isinstance(upstream, pl.DataFrame)\n    return upstream.lazy()  # LazyFrame will be sinked\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment on Windows\nDESCRIPTION: Commands to create and activate a Python virtual environment using uv on Windows systems.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example dagster_example\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Setting Cache Size for Shuffle Service Index - Shuffle Settings\nDESCRIPTION: This configuration defines the maximum memory footprint for cache entries in the shuffle service index, helping to optimize resource usage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_24\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.service.index.cache.size\n```\n\n----------------------------------------\n\nTITLE: Emitting Log Messages from Schedule Evaluation in Dagster\nDESCRIPTION: This snippet demonstrates how to emit log messages from a schedule during its evaluation function. These logs will be visible in the UI when inspecting a tick in the schedule's tick history.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/defining-schedules.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@schedule(cron_schedule=\"0 0 * * *\", job=my_job)\ndef my_schedule(context):\n    context.log.info(\"Evaluating my schedule\")\n    should_run = evaluate_something()\n    if should_run:\n        context.log.info(\"Launching my job\")\n        return RunRequest(run_key=None, run_config={})\n    else:\n        context.log.info(\"Skipping launch\")\n        return SkipReason(\"Condition not met\")\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint Between runs and snapshots Tables\nDESCRIPTION: SQL command to add a foreign key constraint to the runs table referencing the snapshots table on snapshot_id to maintain referential integrity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration Package\nDESCRIPTION: This command installs the dagster-aws package, which is required for using AWS Lambda with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/lambda.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster Webserver\nDESCRIPTION: Command to start the Dagster webserver for development and monitoring.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/index.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration in Dagster\nDESCRIPTION: Implementation showing how to configure the SunResource using environment variables\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-apis.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, Definitions, Config, EnvVar\n\n@asset\ndef home_sunrise(sun_resource: SunResource):\n    return sun_resource.get_sunrise()\n\ndefs = Definitions(\n    assets=[home_sunrise],\n    resources={\n        \"sun_resource\": SunResource(\n            config=SunResourceConfig(\n                latitude=EnvVar(\"HOME_LATITUDE\"),\n                longitude=EnvVar(\"HOME_LONGITUDE\"),\n                timezone=EnvVar(\"HOME_TIMEZONE\"),\n            )\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Disabling Compute Logs in YAML\nDESCRIPTION: This configuration disables forwarding compute logs to Dagster+ by using the NoOpComputeLogManager. This can be useful in scenarios where compute log forwarding is not required or needs to be temporarily disabled.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/settings/customizing-agent-settings.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster.yaml\ninstance_class:\n  module: dagster_cloud.instance\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token:\n    env: DAGSTER_CLOUD_AGENT_TOKEN\n  deployment: prod\n\nuser_code_launcher:\n  module: dagster_cloud.workspace.docker\n  class: DockerUserCodeLauncher\n\ncompute_logs:\n  module: dagster.core.storage.noop_compute_log_manager\n  class: NoOpComputeLogManager\n```\n\n----------------------------------------\n\nTITLE: Installing Steampipe and GitHub Plugin\nDESCRIPTION: Commands to install Steampipe and its GitHub plugin for local development on macOS.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_analytics/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install steampipe\nsteampipe plugin install github\n```\n\n----------------------------------------\n\nTITLE: Create secondary_indexes Table\nDESCRIPTION: This SQL statement creates the `secondary_indexes` table, which tracks secondary indexes created by migrations. It stores the name of the index and creation/completion timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\"\n```\n\n----------------------------------------\n\nTITLE: Deploying a Dagster Project to Serverless Using CLI\nDESCRIPTION: Command to deploy a Dagster project to Dagster+ Serverless using the dagster-cloud CLI. This deploys the Python code to a Serverless environment with specified configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/ci-cd-in-serverless.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud serverless deploy-python-executable ./my-dagster-project \\\n  --location-name example \\\n  --package-name quickstart_etl \\\n  --python-version 3.12\n```\n\n----------------------------------------\n\nTITLE: Customizing Looker Asset Metadata\nDESCRIPTION: Python code showing how to customize asset definition metadata for Looker assets using a custom DagsterLookerApiTranslator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/looker.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_looker import LookerResource, load_looker_asset_specs, DagsterLookerApiTranslator\n\nclass CustomLookerApiTranslator(DagsterLookerApiTranslator):\n    def translate_view(self, view):\n        asset_spec = super().translate_view(view)\n        asset_spec.metadata[\"custom_field\"] = \"custom_value\"\n        return asset_spec\n\n    def translate_explore(self, explore):\n        asset_spec = super().translate_explore(explore)\n        asset_spec.metadata[\"custom_field\"] = \"custom_value\"\n        return asset_spec\n\n    def translate_dashboard(self, dashboard):\n        asset_spec = super().translate_dashboard(dashboard)\n        asset_spec.metadata[\"custom_field\"] = \"custom_value\"\n        return asset_spec\n\nlooker_resource = LookerResource(\n    base_url=EnvVar(\"LOOKER_BASE_URL\"),\n    client_id=EnvVar(\"LOOKER_CLIENT_ID\"),\n    client_secret=EnvVar(\"LOOKER_CLIENT_SECRET\"),\n)\n\nlooker_asset_specs = load_looker_asset_specs(\n    looker_resource,\n    api_translator=CustomLookerApiTranslator(),\n)\n\ndefs = Definitions(\n    assets=looker_asset_specs,\n    resources={\"looker\": looker_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Standard DBT Project Configuration (Before Insights)\nDESCRIPTION: Basic DBT project configuration file without Insights tracking. This YAML file contains standard DBT project settings including name, version, and profile configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/snowflake.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nname: 'my_dbt_project'\nversion: '1.0.0'\nconfig-version: 2\n\nprofile: 'my_profile'\n\nmodel-paths: [\"models\"]\nseed-paths: [\"data\"]\ntest-paths: [\"tests\"]\nanalysis-paths: [\"analyses\"]\nmacro-paths: [\"macros\"]\n\ntarget-path: \"target\"\nclean-targets: [\"target\", \"dbt_packages\"]\n\nmodels:\n  my_dbt_project:\n    +materialized: view\n```\n\n----------------------------------------\n\nTITLE: Inserting Dagster Pipeline Run Data in SQL\nDESCRIPTION: SQL insert statements for Dagster pipeline run data. Includes run IDs, pipeline names, statuses, and serialized run configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_62\n\nLANGUAGE: SQL\nCODE:\n```\n2\td45fa10b-121d-4051-9358-cfd3dfcbc878\t47992fd61f2aecc862f741e1355e2b910d5850de\tfoo\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"53f52f323e0e20f0b8ec27a3d698335b3e0918fb\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": \"bar\", \"executable_path\": \"/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\", \"module_name\": null, \"package_name\": null, \"python_file\": \"/Users/dgibson/dagster/python_modules/dagit/dagit_tests/toy/bar_repo.py\", \"working_directory\": \"/Users/dgibson/dagster-home\"}, \"location_name\": \"bar_repo.py:bar\"}, \"repository_name\": \"bar\"}, \"pipeline_name\": \"foo\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"foo\", \"pipeline_snapshot_id\": \"47992fd61f2aecc862f741e1355e2b910d5850de\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\"}}\t2021-01-12 10:41:03.336991\t2021-01-12 09:41:03.737958\n3\t9f130936-1409-4bd0-b15d-67cf6e7a67ec\t47992fd61f2aecc862f741e1355e2b910d5850de\tfoo\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"53f52f323e0e20f0b8ec27a3d698335b3e0918fb\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": \"bar\", \"executable_path\": \"/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\", \"module_name\": null, \"package_name\": null, \"python_file\": \"/Users/dgibson/dagster/python_modules/dagit/dagit_tests/toy/bar_repo.py\", \"working_directory\": \"/Users/dgibson/dagster-home\"}, \"location_name\": \"bar_repo.py:bar\"}, \"repository_name\": \"bar\"}, \"pipeline_name\": \"foo\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"foo\", \"pipeline_snapshot_id\": \"47992fd61f2aecc862f741e1355e2b910d5850de\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\"}}\t2021-01-12 10:42:03.334236\t2021-01-12 09:42:03.702535\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating Runs Table in PostgreSQL for Dagster\nDESCRIPTION: SQL code to create the runs table which stores metadata for Dagster pipeline runs. It includes columns for run ID, pipeline name, status, run body (as JSON), and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_56\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id text,\n    pipeline_name text,\n    status text,\n    run_body jsonb,\n    create_timestamp timestamp without time zone,\n    update_timestamp timestamp without time zone,\n    CONSTRAINT runs_pkey PRIMARY KEY (id),\n    CONSTRAINT runs_run_id_key UNIQUE (run_id)\n);\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Materialization to Dagster+ with Metadata\nDESCRIPTION: Example of reporting an asset materialization to Dagster+ with custom metadata using cURL. Includes proper authentication and content type headers with a JSON payload.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n    --url https://{ORGANIZATION}.dagster.cloud/{DEPLOYMENT_NAME}/report_asset_materialization/ \\\n    --header 'Content-Type: application/json' \\\n    --header 'Dagster-Cloud-Api-Token: {TOKEN}' \\\n    --data '{\n        \"asset_key\": \"{ASSET_KEY}\",\n        \"metadata\": {\n            \"rows\": 10\n        },\n    }'\n```\n\n----------------------------------------\n\nTITLE: Using dagster-pipes in EMR Serverless Job Script\nDESCRIPTION: This Python script demonstrates how to use dagster-pipes in an EMR Serverless job. It opens a Dagster Pipes context, performs some Spark operations, and sends metadata and asset materializations back to Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-serverless-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\nfrom dagster_pipes import open_dagster_pipes\n\nspark = SparkSession.builder.appName(\"dagster-emr-serverless\").getOrCreate()\n\nwith open_dagster_pipes() as context:\n    df = spark.createDataFrame([(1, \"foo\"), (2, \"bar\")], [\"id\", \"name\"])\n    count = df.count()\n    context.log.info(f\"Count: {count}\")\n    context.add_output_metadata({\"row_count\": {\"raw_value\": count, \"type\": \"integer\"}})\n    context.materialize_asset(\"my_asset\", {\"path\": \"/path/to/asset\"})\n\nspark.stop()\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Templating Scope to Component\nDESCRIPTION: Shows how to add custom templating scope to a component by implementing the get_additional_scope method to provide custom automation conditions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/customizing-components.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/components/custom-subclass/custom-scope.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Launching Airflow with Sample DAGs\nDESCRIPTION: This command starts Airflow with two pre-loaded DAGs: 'load_lakehouse' for ingesting CSV data into DuckDB, and 'dbt_dag' for running a modified jaffle shop project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/starlift-demo/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake run_airflow\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Entry Point in pyproject.toml\nDESCRIPTION: This TOML configuration shows the complete pyproject.toml setup for a package with a 'dg' plugin entry point. It includes build system requirements and the entry point registration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-dg-plugin.md#2025-04-22_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-library\"\nversion = \"0.1.0\"\ndescription = \"A sample library\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\n\n[project.entry-points]\n\"dagster_dg.plugin\" = { my_library = \"my_library\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Plotly Package\nDESCRIPTION: Command to install the plotly Python package as a dependency for creating visualizations\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/downstream-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install plotly\n```\n\n----------------------------------------\n\nTITLE: Limiting Remote Fetch Requests During Shuffle - Shuffle Settings\nDESCRIPTION: This setting limits the number of remote requests to fetch blocks simultaneously. It helps in managing load and avoids overloading nodes with inbound connections.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_13\n\nLANGUAGE: properties\nCODE:\n```\nspark.reducer.maxReqsInFlight\n```\n\n----------------------------------------\n\nTITLE: Building and Deploying PEX Package to S3\nDESCRIPTION: Shell commands for building the Docker image, extracting the PEX file, and uploading it to an S3 bucket. Also includes the spark-submit command configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nDOCKER_BUILDKIT=1 docker build --output type=local,dest=./output .\n```\n\nLANGUAGE: shell\nCODE:\n```\naws s3 cp output/venv.pex s3://your-bucket/venv.pex\n```\n\nLANGUAGE: shell\nCODE:\n```\nspark-submit ... --files s3://your-bucket/venv.pex --conf spark.pyspark.python=./venv.pex\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI Web Server\nDESCRIPTION: Command to start the Dagster UI web server for local development and monitoring of the ETL pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_aws/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Markdown Table: Built-in Dagster+ Insights Metrics\nDESCRIPTION: Table documenting the built-in metrics available in Dagster+ Insights, including credit usage, compute duration, materializations, observations, failures, retries, and asset checks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric               | Description                                                                                                                                                                                                                      |\n| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Dagster credits      | The Dagster credit cost associated with computing this object. Dagster credits are charged for every step that's run, and for every asset that's materialized. For more information, [refer to the pricing FAQ](https://dagster.io/pricing#faq). |\n| Compute duration     | The time spent computing steps. For jobs that run steps in parallel, the compute duration may be longer than the wall clock time it takes for the run to complete.                                                                               |\n| Materializations     | The number of asset materializations associated with computing this object.                                                                                                                                                                      |\n| Observations         | The number of [asset observations](/guides/build/assets/metadata-and-tags/asset-observations) associated with computing this object.                                                                                                             |\n| Step failures        | The number of times steps failed when computing this object. **Note**: Steps that retry and succeed aren't included in this metric.                                                                                                              |\n| Step retries         | The number of times steps were retried when computing this object.                                                                                                                                                                               |\n| Asset check warnings | The number of [asset checks](/guides/test/asset-checks) that produced warnings.                                                                                                                                                                  |\n| Asset check errors   | The number of [asset checks](/guides/test/asset-checks) that produced errors.                                                                                                                                                                    |\n| Retry compute        | The time spent computing steps, including time spent retrying failed steps. For jobs that run steps in parallel, the compute duration may be longer than the wall clock time it takes for the run to complete.                                   |\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Tables from Dagster Assets\nDESCRIPTION: Python code demonstrating how to create a Snowflake table using the Snowflake resource within a Dagster asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef iris_dataset(snowflake):\n    iris_df = pd.read_csv(\n        \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n    )\n    snowflake.execute_query(\n        query=\"CREATE OR REPLACE TABLE IRIS_DATASET AS SELECT * FROM VALUES \",\n        parameters={\"df\": iris_df},\n        quote_identifiers=False,\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Dagstermill Context with Resource Definition\nDESCRIPTION: This snippet retrieves the Dagstermill context, defining resource dependencies for the workflow. It initializes a resource named 'list' that is an empty list.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource_with_exception.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ResourceDefinition\n\ncontext = dagstermill.get_context(resource_defs={\"list\": ResourceDefinition(lambda _: [])})\n```\n\n----------------------------------------\n\nTITLE: Schedules Configuration in YAML\nDESCRIPTION: Configuration for schedule evaluation settings in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nschedules:\n  use_threads: true\n  num_workers: 8\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxied State YAML for Airflow Tasks\nDESCRIPTION: YAML configuration that tracks which Airflow tasks have been proxied to Dagster. This file serves as the source of truth for proxying state and allows toggling tasks between Airflow and Dagster execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/migrate.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndag_id: rebuild_customers_list\ntasks:\n  load_raw_customers:\n    proxied: false\n  build_dbt_models:\n    proxied: false\n  export_customers:\n    proxied: false\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns in Downstream Asset with DuckDB I/O Manager\nDESCRIPTION: This example shows how to select specific columns when loading data from an upstream asset using the DuckDB I/O manager. It demonstrates fetching only sepal-related columns from the iris_dataset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    ins={\n        \"iris_dataset\": AssetIn(\n            metadata={\"columns\": [\"sepal_length_cm\", \"sepal_width_cm\"]}\n        )\n    }\n)\ndef sepal_data(iris_dataset):\n    return iris_dataset\n```\n\n----------------------------------------\n\nTITLE: Loading Python Module Definitions\nDESCRIPTION: Example of Python module definitions for Dagster assets and jobs that can be loaded using the -m argument.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/running-dagster-locally.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmy_module/__init__.py\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration Package\nDESCRIPTION: Command to install the dagster-aws package which provides AWS Athena integration capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/athena.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Customizing Upstream Dependencies for DLT Assets\nDESCRIPTION: Shows how to customize upstream dependencies for DLT assets by creating a custom DagsterDltTranslator class and overriding its methods.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_dlt import DagsterDltTranslator\n\nclass CustomDltTranslator(DagsterDltTranslator):\n    def get_asset_spec(self, *args, **kwargs):\n        asset_spec = super().get_asset_spec(*args, **kwargs)\n        # Customize asset_spec here\n        return asset_spec\n\n    def get_op_spec(self, *args, **kwargs):\n        op_spec = super().get_op_spec(*args, **kwargs)\n        # Customize op_spec here\n        return op_spec\n```\n\n----------------------------------------\n\nTITLE: Defining Data Ingestion Asset with Sling in Python\nDESCRIPTION: This code defines an asset that runs the Sling replication job using the @sling_assets decorator. It configures the replication settings to specify the source (S3) and target (Snowflake), including stream configuration for data ingestion.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/snowflake_to_s3_sling.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_sling import sling_assets\n\nreplication_config = {\n    \"SOURCE\": \"MY_S3\",\n    \"TARGET\": \"MY_SNOWFLAKE\",\n    \"defaults\": {\"mode\": \"full-refresh\", \"object\": \"{stream_schema}_{stream_table}\"},\n    \"streams\": {\n        \"s3://your-s3-bucket/your-file.csv\": {\n            \"object\": \"your_snowflake_schema.your_table\",\n            \"primary_key\": \"id\",\n        },\n    },\n}\n\n@sling_assets(replication_config=replication_config)\ndef ingest_s3_to_snowflake(context, sling: SlingResource):\n    yield from sling.replicate(context=context)\n\ndefs = Definitions(assets=[ingest_s3_to_snowflake], resources={\"sling\": sling_resource})\n```\n\n----------------------------------------\n\nTITLE: Creating Schedule from Partition Set in Python\nDESCRIPTION: Example showing how to create a schedule from a daily partition set that runs at 10AM using the new partition_selector parameter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npartition_set = PartitionSetDefinition(\n    name='hello_world_partition_set',\n    pipeline_name='hello_world_pipeline',\n    partition_fn= date_partition_range(\n        start=datetime.datetime(2021, 1, 1),\n        delta_range=\"days\",\n        timezone=\"US/Central\",\n    )\n    run_config_fn_for_partition=my_run_config_fn,\n)\n\nschedule_definition = partition_set.create_schedule_definition(\n    \"daily_10am_schedule\",\n    \"0 10 * * *\",\n    partition_selector=create_offset_partition_selector(lambda d: d.subtract(hours=10, days=1))\n    execution_timezone=\"US/Central\",\n)\n```\n\n----------------------------------------\n\nTITLE: Allowing Specific Dependencies in Cron-based Automation\nDESCRIPTION: Shows how to specify which upstream dependencies to allow when using AutomationCondition.on_cron() using AssetSelection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/example-customizations.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset(auto_materialize_policy=AutoMaterializePolicy(AutomationCondition.on_cron(\"0 0 * * *\", allow_deps=AssetSelection.groups(\"upstream_group\"))))\n```\n\n----------------------------------------\n\nTITLE: Running Customized Celery Worker (Bash)\nDESCRIPTION: Command to run a Celery worker using a custom Python module for configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncelery -A my_module worker --loglevel=info\n```\n\n----------------------------------------\n\nTITLE: Executing GitHub GraphQL Queries\nDESCRIPTION: Example demonstrating how to execute custom GraphQL queries against the GitHub API using the GithubResource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-github.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom dagster import job, op\nfrom dagster_github import github_resource\n\n\n@op\ndef github_op(github: GithubResource):\n    github.get_client().execute(\n        query=\"\"\"\n        query get_repo_id($repo_name: String!, $repo_owner: String!) {\n            repository(name: $repo_name, owner: $repo_owner) {\n                id\n            }\n        }\n        \"\"\",\n        variables={\"repo_name\": repo_name, \"repo_owner\": repo_owner},\n    )\n\n@job(resource_defs={\n     'github': GithubResource(\n         github_app_id=os.getenv('GITHUB_APP_ID'),\n         github_app_private_rsa_key=os.getenv('GITHUB_PRIVATE_KEY'),\n         github_installation_id=os.getenv('GITHUB_INSTALLATION_ID')\n )})\ndef github_job():\n    github_op()\n\ngithub_job.execute_in_process()\n```\n\n----------------------------------------\n\nTITLE: Creating daemon_heartbeats Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the daemon_heartbeats table, which stores heartbeats from various daemons, facilitating monitoring and synchronization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: daemon_heartbeats; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    info character varying\n);\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Configuring Key Vault Access for AKS\nDESCRIPTION: Commands to grant AKS cluster access to Key Vault and create secrets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/key-vault.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport CLIENT_ID=$(az identity show -g <resource-group> -n <agent-identity> --query 'clientId' -otsv)\nexport KEYVAULT_SCOPE=$(az keyvault show --name <vault-name> --query id -o tsv)\naz role assignment create --role \"Key Vault Secrets User\" --assignee $CLIENT_ID --scope $KEYVAULT_SCOPE\n```\n\nLANGUAGE: bash\nCODE:\n```\naz keyvault secret set --name dagsterAgentToken --vault-name <vault-name> --value <dagster-token>\n```\n\n----------------------------------------\n\nTITLE: Legacy DuckDB IO Manager Builder\nDESCRIPTION: Legacy configuration for building DuckDB IO manager with IOManagerDefinition annotation\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-duckdb.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: build_duckdb_io_manager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Creating Run Status Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the status column of the runs table to optimize queries that filter by run status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_56\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Enabling Azure Key Vault Provider in AKS\nDESCRIPTION: Commands to enable and validate the Azure Key Vault CSI driver and provider installation in AKS cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/key-vault.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naz aks enable-addons --resource-group <resource-group> --name <cluster-name> --addons azure-keyvault-secrets-provider\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n kube-system -l 'app in (secrets-store-csi-driver,secrets-store-provider-azure)' -o wide\n```\n\n----------------------------------------\n\nTITLE: Running Local Agent with Token Parameter in Bash\nDESCRIPTION: Alternative command to run the Dagster Cloud agent, passing the agent token directly as a command-line parameter instead of using an environment variable in the configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/local.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud agent run ~/dagster_home/ --agent-token <AGENT_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Creating public.daemon_heartbeats Table\nDESCRIPTION: This snippet creates the 'daemon_heartbeats' table in the 'public' schema to store heartbeat information from Dagster daemons. The table includes columns for daemon type, daemon ID, timestamp, and the heartbeat's body. This data is used to monitor the health and activity of the daemons.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \\\"timestamp\\\" timestamp without time zone NOT NULL,\n    body text\n);\"\n```\n\n----------------------------------------\n\nTITLE: Reading CSV File into DataFrame\nDESCRIPTION: This snippet utilizes Dagstermill to get the current context and reads a CSV file containing iris data into a pandas DataFrame.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontext = dagstermill.get_context()\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n```\n\n----------------------------------------\n\nTITLE: Configuring agent queues for multi-cluster deployment\nDESCRIPTION: YAML configuration for setting up agent queues to route work to specific Kubernetes clusters based on queue assignment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloud:\n  agentQueues:\n    additionalQueues:\n      - on-prem-agent-queue\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Lake Dependencies\nDESCRIPTION: Commands to install required Dagster Delta Lake packages for integration\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/using-deltalake-with-dagster.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-deltalake dagster-deltalake-pandas\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster STEP_SUCCESS Event\nDESCRIPTION: This JSON snippet represents a Dagster event log entry indicating the successful completion of a step named 'ingest_costs' within the 'longitudinal_pipeline'. The event includes the duration of the step execution in milliseconds and logging tags for context. This allows to analyze performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 5337.012522970326}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_costs\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_costs\"}, \"message\": \"Finished execution of step \\\"ingest_costs\\\" in 5.34s.\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_costs\", \"parent\": null}, \"step_key\": \"ingest_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ingest_costs - STEP_SUCCESS - Finished execution of step \\\"ingest_costs\\\" in 5.34s.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"ingest_costs\", \"timestamp\": 1609894312.569429, \"user_message\": \"Finished execution of step \\\"ingest_costs\\\" in 5.34s.\"\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-gemini via pip\nDESCRIPTION: Command to install the dagster and dagster-gemini packages using pip. This is a prerequisite for using Gemini with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gemini.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-gemini\n```\n\n----------------------------------------\n\nTITLE: Creating Table: bulk_actions in PostgreSQL\nDESCRIPTION: Defines 'bulk_actions' table for managing bulk operation actions within the system. Includes columns for action details such as key, status, and timestamp, used for tracking status and body of bulk operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.bulk_actions (\n    id integer NOT NULL,\n    key character varying(32) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text,\n    action_type character varying(32),\n    selector_id text\n);\nALTER TABLE public.bulk_actions OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Postgres Migration Configuration in dagster.yaml\nDESCRIPTION: This snippet provides a configuration setup for Postgres in the 'dagster.yaml' file used for running migrations. It outlines the necessary modules and classes for event log and run storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/alembic/README.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nevent_log_storage:\n  module: dagster_postgres.event_log\n  class: PostgresEventLogStorage\n  config:\n    postgres_url: \"postgresql://test:test@localhost:5432/test\"\nrun_storage:\n  module: dagster_postgres.run_storage\n  class: PostgresRunStorage\n  config:\n    postgres_url: \"postgresql://test:test@localhost:5432/test\"\nschedule_storage:\n  module: dagster_postgres.schedule_storage\n  class: PostgresScheduleStorage\n  config:\n    postgres_url: \"postgresql://test:test@localhost:5432/test\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project from Example\nDESCRIPTION: Command to bootstrap a new Dagster project using the quickstart_etl example. This creates a new project with the specified name based on the example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_etl/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example quickstart_etl\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with Notebook I/O Manager\nDESCRIPTION: Python code to create a Dagster Definitions object with assets and a ConfigurableLocalOutputNotebookIOManager for notebook storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import load_assets_from_modules, Definitions\nfrom dagstermill import ConfigurableLocalOutputNotebookIOManager\n\nfrom . import assets\n\ndefs = Definitions(\n    assets=load_assets_from_modules([assets]),\n    resources={\n        \"output_notebook_io_manager\": ConfigurableLocalOutputNotebookIOManager()\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Pipeline Definitions in Dagster\nDESCRIPTION: Shows how to combine multiple pipeline definitions into a single Dagster project definition. Demonstrates the pattern for merging factory-generated definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/factory-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions.combine(definitions_list)\n```\n\n----------------------------------------\n\nTITLE: Configuring GCS for Compute Logs\nDESCRIPTION: Configuration for GCSComputeLogManager which writes logs to Google Cloud Storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster_gcp.gcs\n  class: GCSComputeLogManager\n  config:\n    bucket: \"my-bucket\"\n```\n\n----------------------------------------\n\nTITLE: Loading Workspace File with CLI Command\nDESCRIPTION: Command to load a workspace.yaml file from a specific path using the Dagster CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -w path/to/workspace.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Verifying CLI Availability\nDESCRIPTION: Commands to install Dagster using UV and verify that the dagster CLI is correctly installed and accessible.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/setup.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install dagster\ndagster --version\n```\n\n----------------------------------------\n\nTITLE: Defining SlingConnectionResource Class in Python\nDESCRIPTION: This snippet defines the SlingConnectionResource class, which is likely used to manage connections to Sling within Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-sling.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: SlingConnectionResource\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Commands for installing the necessary Python packages in the Dagster environment\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/databricks-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-webserver dagster-databricks\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Chunks for Shuffle Transfer - Shuffle Settings\nDESCRIPTION: This property limits the maximum number of chunks that can be transferred simultaneously in the shuffle service. Exceeding this limit will result in new connections being closed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_25\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.maxChunksBeingTransferred\n```\n\n----------------------------------------\n\nTITLE: Visualizing Config Relationships in Dagster\nDESCRIPTION: This mermaid diagram shows how Config is used in Dagster, illustrating its relationships with Assets, Jobs, Schedules, and Sensors. It demonstrates how Config can be applied to parameterize and reuse these components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_5\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    Config(Config)\n\n    style Job fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Schedule fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Sensor fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Config -.-> Asset\n    Config -.-> Job\n    Config -.-> Schedule\n    Config -.-> Sensor\n```\n\n----------------------------------------\n\nTITLE: Starting Standalone Dagster Webserver\nDESCRIPTION: Command to launch only the Dagster webserver without the daemon. Note that some features like schedules and sensors require the daemon to be running separately.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/webserver.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Dagster Step Success Event Log in JSON Format\nDESCRIPTION: A JSON representation of a Dagster event record indicating successful step completion. This log captures the completion of the 'sleeper' step, including its execution duration of 1.04 seconds.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 1044.7476819972508}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline\": \"sleepy_pipeline\", \"solid\": \"sleeper\", \"solid_definition\": \"sleeper\", \"step_key\": \"sleeper.compute\"}, \"message\": \"Finished execution of step \\\"sleeper.compute\\\" in 1.04s.\", \"pipeline_name\": \"sleepy_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"sleeper\", \"name\": \"sleeper\", \"parent\": null}, \"step_key\": \"sleeper.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - STEP_SUCCESS - Finished execution of step \\\"sleeper.compute\\\" in 1.04s.\\n event_specific_data = {\\\"duration_ms\\\": 1044.7476819972508}\\n               solid = \\\"sleeper\\\"\\n    solid_definition = \\\"sleeper\\\"\\n            step_key = \\\"sleeper.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project with uv\nDESCRIPTION: Commands to create a new Dagster project using dg with uv, including project initialization and environment activation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/scaffolding-a-project.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndg init my-project\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd my-project && uv venv .venv && source .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project Structure (uv)\nDESCRIPTION: Command to show the directory structure of a Dagster project initialized with uv.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/scaffolding-a-project.md#2025-04-22_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\nmy-project\n├── pyproject.toml\n├── README.md\n├── src\n│   └── my_project\n│       ├── defs\n│       │   └── __init__.py\n│       ├── definitions.py\n│       ├── __init__.py\n│       └── lib\n│           └── __init__.py\n├── tests\n│   └── __init__.py\n└── uv.lock\n```\n\n----------------------------------------\n\nTITLE: Disabling Compute Logs in Helm values.yaml\nDESCRIPTION: Configuration for disabling compute log upload when deploying the Kubernetes agent using Helm.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/managing-compute-logs-and-error-messages.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncomputeLogs:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare R2 Storage\nDESCRIPTION: Configuration for Cloudflare R2 storage using atomic copy operations with specific headers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_deltalake import S3Config\n\nconfig = S3Config(copy_if_not_exists=\"header: cf-copy-destination-if-none-match: *\")\n```\n\n----------------------------------------\n\nTITLE: Loading Dagster Definitions from File\nDESCRIPTION: Example of loading Dagster definitions directly from a Python file using the -f argument.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/running-dagster-locally.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -f defs.py\n```\n\n----------------------------------------\n\nTITLE: Creating public.run_tags Table and Sequence\nDESCRIPTION: This snippet creates the 'run_tags' table in the 'public' schema for storing tags associated with Dagster runs. It contains columns for ID, run ID, tag key, and tag value.  An associated sequence 'run_tags_id_seq' is also created to automatically generate unique IDs for each run tag.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\n\n\nALTER TABLE public.run_tags OWNER TO test;\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dagster Cloud Agent with Helm\nDESCRIPTION: Shell command to upgrade the Dagster Cloud agent using Helm with custom values from a values.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nhelm --namespace dagster-cloud upgrade agent \\\n    dagster-cloud/dagster-cloud-agent \\\n    --values ./values.yaml\n```\n\n----------------------------------------\n\nTITLE: Exporting Asset Metrics using GraphQL in Python\nDESCRIPTION: This snippet demonstrates how to use the GraphQL Python Client to export the Dagster credits metric for all assets for a specific time period. It includes the GraphQL query definition and a function to execute the query.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/export-metrics.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom dagster_graphql import DagsterGraphQLClient\n\nASSET_METRICS_QUERY = \"\"\"\nquery AssetMetrics($metricName: String, $after: Float, $before: Float) {\n  reportingMetricsByAsset(\n    metricsSelector: {\n      metricName: $metricName\n      after: $after\n      before: $before\n      sortAggregationFunction: SUM\n      granularity: DAILY\n    }\n  ) {\n    __typename\n    ... on ReportingMetrics {\n      metrics {\n        values\n        entity {\n          ... on ReportingAsset {\n            assetKey {\n              path\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\"\"\"\n\n\ndef get_client():\n    url = \"YOUR_ORG.dagster.cloud/prod\"  # Your deployment-scoped url\n    user_token = \"YOUR_TOKEN\"  # A token generated from Organization Settings > Tokens\n    return DagsterGraphQLClient(url, headers={\"Dagster-Cloud-Api-Token\": user_token})\n\n\nif __name__ == \"__main__\":\n    client = get_client()\n    result = client._execute(\n        ASSET_METRICS_QUERY,\n        {\n            \"metricName\": \"__dagster_dagster_credits\",\n            \"after\": datetime(2023, 9, 1).timestamp(),\n            \"before\": datetime(2023, 10, 1).timestamp(),\n        },\n    )\n\n    for asset_series in result[\"reportingMetricsByAsset\"][\"metrics\"]:\n        print(\"Asset key:\", asset_series[\"entity\"][\"assetKey\"][\"path\"])\n        print(\"Daily values:\", asset_series[\"values\"])\n\n```\n\n----------------------------------------\n\nTITLE: Creating Staging Models for Bluesky Feed Data\nDESCRIPTION: SQL model that extracts and transforms feed snapshot data from the raw source into a structured format. The model parses nested JSON fields and flattens the data for analysis.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/modeling.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nwith feeds_cte as (\n  select\n    snapshot_date,\n    replace(uri, 'at://did:plc:','' ) as author_did,\n    replace(author, 'at://did:plc:','' ) as actor_did,\n    type, value, subscribed, snapshot_date as ingested_at\n  from {{ source('atproto', 'actor_feed_snapshot') }}\n  where type = 'app.bsky.feed.post'\n),\n\nfeed_model as (\n  select\n    *,\n    json_extract_string(value, '$.text') as text,\n    replace(json_extract_string(value, '$.reply.parent.uri'), 'at://did:plc:','' ) as parent_post_author,\n    replace(json_extract_string(value, '$.reply.root.uri'), 'at://did:plc:','' ) as root_post_author\n  from feeds_cte\n)\n\nselect\n  *,\n  case\n    when parent_post_author is not null and root_post_author is not null then 'reply'\n    when json_extract_string(value, '$.embed.\"$type\"') = 'app.bsky.embed.record' then 'quote'\n    when json_extract_string(value, '$.embed.\"$type\"') = 'app.bsky.embed.recordWithMedia' then 'quote_with_media'\n    when json_extract_string(value, '$.embed.\"$type\"') = 'app.bsky.embed.images' then 'post_with_image'\n    when json_extract_string(value, '$.embed.\"$type\"') = 'app.bsky.embed.external' then 'post_with_external'\n    else 'post'\n  end as post_type\nfrom feed_model\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraints - SQL\nDESCRIPTION: SQL commands that add primary key constraints to various tables including event_logs, job_ticks, jobs, normalized_cereals, run_tags, runs, schedule_ticks, schedules, secondary_indexes, and snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_65\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.normalized_cereals\n    ADD CONSTRAINT normalized_cereals_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.schedule_ticks\n    ADD CONSTRAINT schedule_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.schedules\n    ADD CONSTRAINT schedules_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraints - SQL\nDESCRIPTION: SQL commands that add unique constraints to jobs, runs, schedules, secondary_indexes, and snapshots tables to ensure data integrity for specific columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_66\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.schedules\n    ADD CONSTRAINT schedules_schedule_origin_id_key UNIQUE (schedule_origin_id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster GraphQL Client for Dagster+ in Python\nDESCRIPTION: This code shows how to configure the DagsterGraphQLClient for use with Dagster+ by specifying a deployment-specific URL and a User Token for authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/graphql-client.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_graphql import DagsterGraphQLClient\n\n# Dagster+ usage\nclient = DagsterGraphQLClient(\n    \"https://dagster.cloud/my-org/my-deployment/graphql\",\n    headers={\"Dagster-Cloud-Api-Token\": \"my-token\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating Multiple Podcast Pipelines Using Factory\nDESCRIPTION: Demonstrates how to use the pipeline factory to create multiple podcast processing pipelines. Shows instantiation for three different podcast feeds with unique URLs and names.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/factory-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefinitions_list = [\n    rss_pipeline_factory(\n        \"https://corecursive.com/feed\", \"corecursive\"\n    ),\n    rss_pipeline_factory(\n        \"https://changelog.com/podcast/feed\", \"changelog\"\n    ),\n    rss_pipeline_factory(\n        \"https://feeds.simplecast.com/7PvD7RPL\", \"software_sessions\"\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Run Retries in Dagster+\nDESCRIPTION: Defines settings for handling failed run retries in Dagster+. The max_retries property determines how many retry attempts are allowed for failed runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/deployment-settings-reference.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nrun_retries:\n  max_retries: 0\n```\n\n----------------------------------------\n\nTITLE: RSS Feed Monitoring Sensor in Python\nDESCRIPTION: Implements a Dagster sensor to monitor RSS feeds for new content using etag tracking. Maintains cursor state to track processed items and trigger runs for new content.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/rss-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstart_sensor\n```\n\n----------------------------------------\n\nTITLE: Setting Up Persistent Storage\nDESCRIPTION: Shell commands to create and configure a permanent storage location for Dagster using DAGSTER_HOME environment variable.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/running-dagster-locally.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p ~/.dagster_home\nexport DAGSTER_HOME=~/.dagster_home\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Branch Deployments for Dagster Cloud Agent\nDESCRIPTION: YAML configuration to enable branch deployments for the Dagster Cloud agent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloud:\n  branchDeployment: true\n```\n\n----------------------------------------\n\nTITLE: Creating an Empty SparkContext for Runtime Configuration\nDESCRIPTION: Demonstrates creating an empty SparkContext that allows dynamic configuration at runtime, enabling more flexible application deployment across different environments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval sc = new SparkContext(new SparkConf())\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Artifact Storage\nDESCRIPTION: Configuration for LocalArtifactStorage which specifies the directory for storing local artifacts.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nlocal_artifact_storage:\n  module: dagster._core.storage.root\n  class: LocalArtifactStorage\n  config:\n    base_dir: \"/path/to/storage\"\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Asset to Invoke External Code\nDESCRIPTION: Implementation of a Dagster asset that uses Pipes client resource to invoke external code and handle its execution within the Dagster ecosystem.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess\nfrom dagster import asset\n\n@asset\ndef my_external_asset():\n    # Invoke external code\n    result = subprocess.check_output([\"python\", \"/usr/bin/external_code.py\"])\n    return result\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Networking Properties\nDESCRIPTION: Configuration properties that control Spark's networking behavior including RPC settings, port configurations, timeout values, and connection management parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_36\n\nLANGUAGE: properties\nCODE:\n```\nspark.rpc.message.maxSize=128\nspark.blockManager.port=<random>\nspark.driver.blockManager.port=<inherit>\nspark.driver.bindAddress=<value of spark.driver.host>\nspark.driver.host=<local hostname>\nspark.driver.port=<random>\nspark.network.timeout=120s\nspark.port.maxRetries=16\nspark.rpc.numRetries=3\nspark.rpc.retry.wait=3s\nspark.rpc.askTimeout=<spark.network.timeout>\nspark.rpc.lookupTimeout=120s\nspark.core.connection.ack.wait.timeout=<spark.network.timeout>\n```\n\n----------------------------------------\n\nTITLE: Creating Dockerfile for Dagster Pipes Kubernetes Container\nDESCRIPTION: This Dockerfile sets up a Python environment, installs dagster-pipes, copies the Python script, and sets the entrypoint to run the script.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/kubernetes-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.10-slim\n\nRUN pip install dagster-pipes\n\nCOPY my_python_script.py .\n\nENTRYPOINT [ \"python\",\"my_python_script.py\" ]\n```\n\n----------------------------------------\n\nTITLE: Running Project Tests\nDESCRIPTION: Command to execute the project's test suite using pytest.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dbt_python/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest assets_dbt_python_tests\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Delta Lake Dependencies\nDESCRIPTION: Commands to install the required Python packages for using Delta Lake with Dagster. Includes the core Delta Lake integration package and additional connectors for pandas and polars dataframes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-deltalake\npip install dagster-deltalake-pandas\npip install dagster-deltalake-polars\n```\n\n----------------------------------------\n\nTITLE: Supporting Optional DataFrames in Dagster Assets\nDESCRIPTION: Example showing how to handle optional Polars DataFrames in Dagster assets, where None values can be returned or handled gracefully.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-polars.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef upstream() -> Optional[pl.DataFrame]:\n    if has_data:\n        return DataFrame({\"foo\": [1, 2, 3]})  # type check will pass\n    else:\n        return None  # type check will pass and `dagster_polars` will skip writing the output completely\n\n@asset\ndef downstream(upstream: Optional[pl.LazyFrame]):  # upstream will be None if it doesn't exist in storage\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster via pip\nDESCRIPTION: This command installs the 'dagster-dg' package using pip, which is likely a specific distribution or version of Dagster. It's used to set up Dagster in a Python environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-d-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install dagster-dg\n```\n\n----------------------------------------\n\nTITLE: Configuring Nested Graphs in Dagster (YAML)\nDESCRIPTION: This YAML configuration specifies the config for nested ops within a sub-graph when launching a Dagster job run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/nesting-graphs.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nops:\n  add_and_multiply:\n    ops:\n      add_n:\n        config:\n          n: 5\n      multiply_by_m:\n        config:\n          m: 3\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Tests\nDESCRIPTION: Command to run both schema tests and data tests defined in the dbt project. Schema tests validate column specifications while data tests run custom SQL validations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/dbt_project/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndbt test\n```\n\n----------------------------------------\n\nTITLE: Tag Exclusion Asset Selection\nDESCRIPTION: Creates a job selecting assets by owner while excluding specific tags\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndata_eng_not_private_job = define_asset_job(\n    name=\"data_eng_not_private_job\", selection='owner:\"nora.dagster@example.com\" and not tag:\"customer_data\"'\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Runs Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'runs' table to store information about Dagster pipeline runs. It includes columns for run identification, status, pipeline details, and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.runs (\n    id bigint NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    mode text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    start_time double precision,\n    end_time double precision\n);\n```\n\n----------------------------------------\n\nTITLE: Installing UV Package Manager for Dagster Development\nDESCRIPTION: Command to install the UV package manager using a curl script, which is required for setting up the Dagster development environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Installing dg globally using uv\nDESCRIPTION: Command to install the dagster-dg package as a globally available tool using the uv package manager. This makes the dg command available system-wide regardless of virtual environment activation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install dagster-dg\n```\n\n----------------------------------------\n\nTITLE: Dagster W&B Integration Components\nDESCRIPTION: Lists the main components provided by the Dagster-W&B integration: wandb_resource for API authentication and wandb_artifacts_io_manager for artifact management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/wandb.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- `wandb_resource`: a Dagster resource used to authenticate and communicate to the W&B API.\n- `wandb_artifacts_io_manager`: a Dagster I/O Manager used to consume W&B Artifacts.\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration\nDESCRIPTION: This command installs the dagster-aws package, which provides the integration between Dagster and AWS services, including EMR.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/emr.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Updated ModeDefinition Configuration\nDESCRIPTION: Example showing the migration from system_storage_defs to intermediate_storage_defs in ModeDefinition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_aws.s3 import s3_plus_default_intermediate_storage_defs\n\nModeDefinition(intermediate_storage_defs=s3_plus_default_intermediate_storage_defs)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-AWS Integration for Secrets Manager\nDESCRIPTION: This command installs the Dagster-AWS package, which includes the integration with AWS Secrets Manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/secretsmanager.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Asset Definitions in Table Format\nDESCRIPTION: This snippet shows the output of the 'dg list defs' command, which displays a table of defined assets in a Dagster project. The table includes columns for Key, Group, Dependencies, Kinds, and Description for each asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/adding-attributes-to-assets/4-list-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list defs\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Section ┃ Definitions                                    ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Assets  │ ┏━━━━━┳━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┓ │\n│         │ ┃ Key ┃ Group   ┃ Deps ┃ Kinds ┃ Description ┃ │\n│         │ ┡━━━━━╇━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━┩ │\n│         │ │ a   │ team_a  │      │       │             │ │\n│         │ ├─────┼─────────┼──────┼───────┼─────────────┤ │\n│         │ │ b   │ team_a  │      │       │             │ │\n│         │ ├─────┼─────────┼──────┼───────┼─────────────┤ │\n│         │ │ c   │ default │      │       │             │ │\n│         │ └─────┴─────────┴──────┴───────┴─────────────┘ │\n└─────────┴────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dagster IO Manager Relationships\nDESCRIPTION: Mermaid diagram showing IO Manager's relationships with Assets and Definitions, including style configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_8\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    IOManager(IO Manager)\n\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    IOManager -.-> Asset\n    IOManager ==> Definitions\n```\n\n----------------------------------------\n\nTITLE: Fetching Iris Dataset in Jupyter Notebook\nDESCRIPTION: Python code to fetch the Iris dataset using pandas in a Jupyter notebook.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\niris = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n    names=[\n        \"Sepal length (cm)\",\n        \"Sepal width (cm)\",\n        \"Petal length (cm)\",\n        \"Petal width (cm)\",\n        \"Species\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: DBT Project Configuration with Insights Tracking (After)\nDESCRIPTION: Enhanced DBT project configuration that enables Insights tracking. This YAML includes additional query-comment settings that allow Dagster+ to attribute cost metrics to the correct assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/snowflake.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nname: 'my_dbt_project'\nversion: '1.0.0'\nconfig-version: 2\n\nprofile: 'my_profile'\n\nmodel-paths: [\"models\"]\nseed-paths: [\"data\"]\ntest-paths: [\"tests\"]\nanalysis-paths: [\"analyses\"]\nmacro-paths: [\"macros\"]\n\ntarget-path: \"target\"\nclean-targets: [\"target\", \"dbt_packages\"]\n\nquery-comment:\n  comment: \"dagster-insights: {{ node.unique_id }}\"\n  append: true\n\nmodels:\n  my_dbt_project:\n    +materialized: view\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Dagster Cloud Agent Token\nDESCRIPTION: Sets up a Kubernetes secret to store the Dagster Cloud agent token.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl --namespace dagster-cloud create secret generic dagster-cloud-agent-token --from-literal=DAGSTER_CLOUD_AGENT_TOKEN=<token>\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Base Image in GitHub Workflow (YAML)\nDESCRIPTION: This snippet shows how to specify a custom base image for Docker deployments in a GitHub workflow by setting the SERVERLESS_BASE_IMAGE_TAG environment variable.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  SERVERLESS_BASE_IMAGE_TAG: \"my-custom-image:latest\"\n```\n\n----------------------------------------\n\nTITLE: Default Argument Handling with Check Module\nDESCRIPTION: This snippet illustrates how to set default dictionary parameters in functions using the Check module, avoiding the pitfalls of mutable default arguments by ensuring unique instance creation when needed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-shared/dagster_shared/check/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef another_function(optional_dict=None):\n  # if optional_dict is None, it will return a unique instance of {} instead\n  the_dict = check.opt_dict_param(optional_dict, 'optional_dict')\n\n```\n\n----------------------------------------\n\nTITLE: Creating job_ticks Table in PostgreSQL\nDESCRIPTION: This SQL snippet sets up the 'job_ticks' table, which is useful for tracking execution ticks of jobs, including their status, type, and creation/update timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: job_ticks; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.job_ticks OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Logging Step Output Event in Dagster Pipeline\nDESCRIPTION: A JSON log entry capturing the completion of a step output event in a Dagster pipeline. The log shows a successful type check for the 'result' output from the 'build_model' step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"build_model\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_model - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_model\", \"timestamp\": 1608666998.749526, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Database Clone in GitHub Actions Workflow\nDESCRIPTION: GitHub Actions workflow configuration that triggers database cloning when a branch deployment is created. The job runs on pull request events and includes conditions to prevent execution on PR closure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: Queue clone_prod job after deployment\\n      if: github.event.action != 'closed'\\n      env:\\n        DAGSTER_CLOUD_API_TOKEN: ${{ secrets.DAGSTER_CLOUD_API_TOKEN }}\\n      run: |\\n        dagster-cloud job launch clone_prod\n```\n\n----------------------------------------\n\nTITLE: Disabling PEX Deploys in GitHub Workflow (YAML)\nDESCRIPTION: This snippet shows how to disable PEX-based deploys in a GitHub workflow by setting the ENABLE_FAST_DEPLOYS environment variable to false.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  ENABLE_FAST_DEPLOYS: \"false\"\n```\n\n----------------------------------------\n\nTITLE: Dagster Hook Definition Classes and Decorators - Python\nDESCRIPTION: Core hook-related components including success_hook and failure_hook decorators, HookDefinition class, HookContext class, and build_hook_context function. These components enable defining execution hooks for Dagster operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/hooks.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Hook Decorators\n@success_hook\n@failure_hook\n\n# Hook Classes\nclass HookDefinition:\n    pass\n\nclass HookContext:\n    pass\n\n# Hook Context Builder\ndef build_hook_context():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Storing Time Partitioned Assets in BigQuery with Dagster\nDESCRIPTION: Demonstrates how to define time partitioned assets in Dagster for storage in BigQuery, using partition_expr metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, DailyPartitionsDefinition\nimport pandas as pd\n\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2023-01-01\"),\n       metadata={\"partition_expr\": \"TIMESTAMP_SECONDS(TIME)\"})\ndef iris_data() -> pd.DataFrame:\n    ...\n\n@asset\ndef downstream_asset(iris_data: pd.DataFrame):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Databricks Integration\nDESCRIPTION: Command to install the dagster-databricks package via pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/databricks.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-databricks\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI\nDESCRIPTION: Command to start the Dagster UI and daemon processes locally.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/creating-a-new-project.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: bulk_actions_id_seq in PostgreSQL\nDESCRIPTION: Creates a sequence 'bulk_actions_id_seq' to auto-generate unique identifiers for the 'id' column in 'bulk_actions'. Incremented by 1 and cached with 1, ensuring unique identification for each bulk action record.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.bulk_actions_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.bulk_actions_id_seq OWNER TO test;\nALTER SEQUENCE public.bulk_actions_id_seq OWNED BY public.bulk_actions.id;\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Expectation Result Event in JSON\nDESCRIPTION: This snippet represents a Dagster event record for a step expectation result. It includes metadata about the expectations run on a 'groups' table, including column statistics.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepExpectationResultData\",\n      \"expectation_result\": {\n        \"__class__\": \"ExpectationResult\",\n        \"description\": \"Battery of expectations for groups\",\n        \"label\": \"groups_expectations\",\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"JsonMetadataEntryData\",\n              \"data\": {\n                \"columns\": {\n                  \"name\": {\n                    \"average_length\": 3.394893,\n                    \"empty\": 0,\n                    \"nulls\": 1,\n                    \"values\": 122\n                  },\n                  \"time_created\": {\n                    \"average\": 1231283,\n                    \"empty\": 2,\n                    \"nulls\": 1,\n                    \"values\": 120\n                  }\n                }\n              }\n            },\n            \"label\": \"table_summary\"\n          }\n        ],\n        \"success\": false\n      }\n    },\n    \"event_type_value\": \"STEP_EXPECTATION_RESULT\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"check_users_and_groups_one_fails_one_succeeds\",\n      \"solid_definition\": \"check_users_and_groups_one_fails_one_succeeds\",\n      \"step_key\": \"check_users_and_groups_one_fails_one_succeeds.compute\"\n    },\n    \"message\": \"Battery of expectations for groups\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"check_users_and_groups_one_fails_one_succeeds\",\n      \"name\": \"check_users_and_groups_one_fails_one_succeeds\",\n      \"parent\": null\n    },\n    \"step_key\": \"check_users_and_groups_one_fails_one_succeeds.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Battery of expectations for groups\\n event_specific_data = {\\\"expectation_result\\\": [false, \\\"groups_expectations\\\", \\\"Battery of expectations for groups\\\", [[\\\"table_summary\\\", null, [{\\\"columns\\\": {\\\"name\\\": {\\\"average_length\\\": 3.394893, \\\"empty\\\": 0, \\\"nulls\\\": 1, \\\"values\\\": 122}, \\\"time_created\\\": {\\\"average\\\": 1231283, \\\"empty\\\": 2, \\\"nulls\\\": 1, \\\"values\\\": 120}}}]]]]}\n               solid = \\\"check_users_and_groups_one_fails_one_succeeds\\\"\n    solid_definition = \\\"check_users_and_groups_one_fails_one_succeeds\\\"\n            step_key = \\\"check_users_and_groups_one_fails_one_succeeds.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"check_users_and_groups_one_fails_one_succeeds.compute\",\n  \"timestamp\": 1576110683.97454,\n  \"user_message\": \"Battery of expectations for groups\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to Dagster Database Tables in PostgreSQL\nDESCRIPTION: These SQL commands add primary key and unique constraints to various Dagster-related tables in the PostgreSQL database. This ensures data integrity and proper relationships between tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\nALTER TABLE ONLY public.asset_check_executions\n    ADD CONSTRAINT asset_check_executions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.asset_daemon_asset_evaluations\n    ADD CONSTRAINT asset_daemon_asset_evaluations_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.asset_event_tags\n    ADD CONSTRAINT asset_event_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.backfill_tags\n    ADD CONSTRAINT backfill_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.concurrency_limits\n    ADD CONSTRAINT concurrency_limits_concurrency_key_key UNIQUE (concurrency_key);\n\nALTER TABLE ONLY public.concurrency_limits\n    ADD CONSTRAINT concurrency_limits_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.concurrency_slots\n    ADD CONSTRAINT concurrency_slots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.dynamic_partitions\n    ADD CONSTRAINT dynamic_partitions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instance_info\n    ADD CONSTRAINT instance_info_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_selector_id_key UNIQUE (selector_id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.kvs\n    ADD CONSTRAINT kvs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.pending_steps\n    ADD CONSTRAINT pending_steps_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Create event_logs Table\nDESCRIPTION: This SQL statement creates the `event_logs` table for storing event logs related to Dagster runs. It includes fields for the run ID, event data, event type, timestamp, step key, asset key, and partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \\\"timestamp\\\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\"\n```\n\n----------------------------------------\n\nTITLE: Yielding Asset Materialization Event in Dagster - Python\nDESCRIPTION: This code snippet imports the Dagstermill module and the AssetMaterialization class, then yields an asset materialization event with a specified asset key. It demonstrates how to integrate Dagstermill with Dagster to track asset creations during processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/yield_event.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\\nfrom dagster import AssetMaterialization\\n\\ndagstermill.yield_event(AssetMaterialization(asset_key=\"my_asset\"))\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Template in Bash\nDESCRIPTION: This command copies the example environment file to create a new .env file for configuration. The .env file should be populated with necessary environment variables for the project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/google_drive_factory/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Asset from Jupyter Notebook\nDESCRIPTION: Python code to create a Dagster asset from a Jupyter notebook using define_dagstermill_asset function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagstermill import define_dagstermill_asset\nfrom dagster import file_relative_path\n\niris_kmeans_jupyter_notebook = define_dagstermill_asset(\n    name=\"iris_kmeans_jupyter\",\n    notebook_path=file_relative_path(__file__, \"notebooks/iris-kmeans.ipynb\"),\n    group_name=\"template_tutorial\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Source Definition\nDESCRIPTION: YAML configuration for defining a dbt source that corresponds to the upstream Dagster asset, including metadata to specify the asset relationship.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/upstream-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: jaffle_shop\n    tables:\n      - name: raw_customers\n        meta:\n          dagster:\n            asset_key: ['raw_customers']\n```\n\n----------------------------------------\n\nTITLE: Configuring custom service account for a code location\nDESCRIPTION: YAML configuration for specifying a custom Kubernetes service account for a specific code location in dagster_cloud.yaml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      k8s:\n        service_account_name: my_service_account_name\n```\n\n----------------------------------------\n\nTITLE: Importing DefaultRunLauncher in Python\nDESCRIPTION: This code imports the DefaultRunLauncher class, which is the simplest built-in run launcher in Dagster. It spawns a new process per run on the same node as the job's code location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-launchers.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster._core.launcher import DefaultRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Dagster and GCP\nDESCRIPTION: Shell commands to install the necessary Python packages for working with Dagster and GCP Dataproc.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/gcp-dataproc-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-webserver 'dagster-gcp[dataproc]'\n```\n\n----------------------------------------\n\nTITLE: Defining Partitions in Dagster ETL Project\nDESCRIPTION: This file contains partition definitions for the data pipeline, specifically monthly_partition and product_category_partition. These partitions help organize data processing by time periods and product categories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/refactor-your-project.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Code placeholder for partitions.py\n# The actual implementation would be shown in the CodeExample component\n```\n\n----------------------------------------\n\nTITLE: Starting Steampipe Service\nDESCRIPTION: Command to start the Steampipe service, which is required for fetching GitHub data in the production environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_analytics/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsteampipe service start\n```\n\n----------------------------------------\n\nTITLE: Limiting Remote Blocks Fetched from a Host - Shuffle Settings\nDESCRIPTION: This configuration restricts the number of blocks fetched from a host per reduce task, helping to prevent overload on individual hosts and ensuring stability during shuffles.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_14\n\nLANGUAGE: properties\nCODE:\n```\nspark.reducer.maxBlocksInFlightPerAddress\n```\n\n----------------------------------------\n\nTITLE: Visualizing Component Relationships in Dagster\nDESCRIPTION: This mermaid diagram illustrates the concept of a Component in Dagster, showing how it encapsulates Definitions to accomplish specific tasks or integrate with other systems.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_4\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    subgraph Component\n    Definitions(Definitions)\n    end\n```\n\n----------------------------------------\n\nTITLE: Running Basic DBT Commands\nDESCRIPTION: Core dbt CLI commands for executing models and running tests in a dbt project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dbt_python/dbt_project/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt run\ndbt test\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence for Snapshots ID in PostgreSQL for Dagster\nDESCRIPTION: Creates a sequence named 'snapshots_id_seq' for generating unique IDs for the snapshots table. The sequence starts at 1 and increments by 1 for each new entry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n```\n\n----------------------------------------\n\nTITLE: Yielding a Result with Dagstermill\nDESCRIPTION: This snippet demonstrates how to yield a simple result string from a Dagstermill-executed notebook.  The `dagstermill.yield_result` function is used to send the result to Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_explicit_yield.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndagstermill.yield_result(\"A result\")\n```\n\n----------------------------------------\n\nTITLE: Bootstrapping a Dagster Project from Example\nDESCRIPTION: Command to create a new Dagster project based on the assets_pandas_type_metadata example. This creates a project with the specified name that includes all the example code.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example assets_pandas_type_metadata\n```\n\n----------------------------------------\n\nTITLE: Installing Yarn Dependencies for Dagster Docs\nDESCRIPTION: Command to install project dependencies using Yarn package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding and Connecting to Redis\nDESCRIPTION: Shell commands to set up port forwarding to a Redis service running in Kubernetes and connect to it using redis-cli, useful for testing and debugging.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward --namespace default svc/dagredis-master 6379:6379\nredis-cli -h 127.0.0.1 -p 6379\n```\n\n----------------------------------------\n\nTITLE: Starter Pack Asset Decorator Configuration\nDESCRIPTION: Asset decorator configuration for the starter pack snapshot, including partitioning and automation conditions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/ingestion.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstart_starter_pack_dec\n@asset(\n    partitions=StaticPartitionsDefinition([\"3l7cddlz5ja24\"]),\n    auto_materialize_condition=dg.AutomationCondition.daily(at_time=time(0, 0)),\n    kinds=[\"Python\"],\n    group_name=\"ingestion\",\n)\ndef starter_pack_snapshot(\n    context,\n    atproto: ATProtoResource,\n    s3: S3Resource,\n) -> None:\nend_starter_pack_dec\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI Web Server\nDESCRIPTION: Command to start the Dagster UI web server for local development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_modern_data_stack/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Pull Secrets for Dagster Cloud Agent\nDESCRIPTION: YAML configuration to specify image pull secrets for the Dagster Cloud agent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nimagePullSecrets: [regCred]\n```\n\n----------------------------------------\n\nTITLE: Implementing build_defs Method for ShellCommandComponent\nDESCRIPTION: Complete implementation of the ShellCommandComponent with the build_defs method that creates assets from the specified shell script.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, Definitions, asset\nfrom dagster._core.definitions.asset_spec import AssetSpec\nfrom dagster_components import Component, ResolvedAssetSpec, Resolvable, ComponentTypeMetadata\nfrom pydantic import model_validator\nfrom typing_extensions import Annotated, override\nimport os\nimport subprocess\n\nfrom typing import List\n\n\nclass ShellCommandComponent(Resolvable, Component):\n    \"\"\"A component that executes a shell command.\"\"\"\n\n    script_path: str\n    \"\"\"The path to the shell script to execute.\"\"\"\n\n    asset_specs: List[ResolvedAssetSpec]\n    \"\"\"The assets that this script produces.\"\"\"\n\n    def execute(self, context):\n        \"\"\"Execute the shell script.\"\"\"\n        context.log.info(f\"Executing {self.script_path}\")\n        result = subprocess.run([\"bash\", self.script_path], capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Script failed with error: {result.stderr}\")\n        context.log.info(f\"Script output: {result.stdout}\")\n        return result.stdout\n\n    @override\n    def build_defs(self) -> Definitions:\n        \"\"\"Build the definitions for this component.\"\"\"\n        @asset(name=\"shell_script_output\", key_prefix=self.get_component_key())\n        def shell_script_asset(context):\n            return self.execute(context)\n\n        return Definitions(\n            assets=[shell_script_asset],\n        )\n\n    @classmethod\n    @override\n    def get_component_type_metadata(cls) -> ComponentTypeMetadata:\n        return ComponentTypeMetadata(\n            owners=[\"data-platform-team\"],\n            tags={\"tier\": \"production\"},\n        )\n```\n\n----------------------------------------\n\nTITLE: Dagster-Celery CLI Commands\nDESCRIPTION: Common CLI commands for managing Celery workers in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/celery.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n## Start new workers\ndagster-celery worker start\n\n## View running workers\ndagster-celery worker list\n\n## Terminate workers\ndagster-celery worker terminate\n```\n\n----------------------------------------\n\nTITLE: Asset Check Executions Table Definition\nDESCRIPTION: Table for storing details of asset check executions, including status, timestamps, and related event information\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.asset_check_executions (\n    id bigint NOT NULL,\n    asset_key text,\n    check_name text,\n    partition text,\n    run_id character varying(255),\n    execution_status character varying(255),\n    evaluation_event text,\n    evaluation_event_timestamp timestamp without time zone,\n    evaluation_event_storage_id bigint,\n    materialization_event_storage_id bigint,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Agent YAML File\nDESCRIPTION: Creates a configuration file for the Dagster agent that includes instance settings, API configuration with authentication token, and Docker user code launcher configuration. This file defines how the agent will connect to Dagster+ and launch Docker containers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/docker/setup.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninstance_class:\n  module: dagster_cloud.instance\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token: <YOUR_AGENT_TOKEN>\n  branch_deployments: true # enables branch deployments\n  deployment: prod\n\nuser_code_launcher:\n  module: dagster_cloud.workspace.docker\n  class: DockerUserCodeLauncher\n  config:\n    networks:\n      - dagster_cloud_agent\n```\n\n----------------------------------------\n\nTITLE: Helm Chart Pod Spec Configuration Example\nDESCRIPTION: Example of pod specification configuration in a Helm chart showing node selector, DNS policy, and image pull secrets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"node_selector\": {\"disktype\": \"ssd\"},\n  \"dns_policy\": \"ClusterFirst\",\n  \"image_pull_secrets\": [{\"name\": \"my-secret\"}]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Asset Attributes with component.yaml in Dagster\nDESCRIPTION: This YAML configuration sets the group of all assets within the directory to 'team_a' using the Dagster DefsFolderComponent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/adding-attributes-to-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndagster:\n  component:\n    class: dagster.components.DefsFolderComponent\n    config:\n      transforms:\n        - set_asset_group: team_a\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Instance Migration\nDESCRIPTION: This command uses 'helm template' to generate a Kubernetes job that runs the 'dagster instance migrate' command. It applies the generated job to the cluster to perform the migration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/migrating-while-upgrading.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Run `helm list` and save your Dagster Helm release name\nexport HELM_DAGSTER_RELEASE_NAME=<DAGSTER_RELEASE_NAME>\n\n# The `helm template` command must be run from the directory containing the\n# `values.yaml` file you used to install the Dagster Helm chart.\n#\n# If needed, you can retrieve the currently applied `values.yaml` file\n# from the cluster by running:\n#\n# `helm get values $HELM_DAGSTER_RELEASE_NAME > values.yaml`\n#\nhelm template $HELM_DAGSTER_RELEASE_NAME dagster/dagster \\\n    --set \"migrate.enabled=true\" \\\n    --show-only templates/job-instance-migrate.yaml \\\n    --values values.yaml \\\n    | kubectl apply -f -\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster repository in editable mode - Bash\nDESCRIPTION: This command installs the Dagster repository as a Python package in editable mode, allowing local changes to be reflected without reinstallation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_gcp/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultRunCoordinator in YAML\nDESCRIPTION: Configuration for DefaultRunCoordinator which immediately sends runs to the run launcher.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nrun_coordinator:\n  module: dagster._core.run_coordinator\n  class: DefaultRunCoordinator\n```\n\n----------------------------------------\n\nTITLE: Yielding Result with Dagstermill in Python\nDESCRIPTION: This snippet computes the sum of `a` and `b`, then utilizes `dagstermill.yield_result()` to yield the result within a Dagster pipeline. This process is key for producing output from computations in a compatible format for Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nresult = a + b\ndagstermill.yield_result(result)\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset with Tags in Main Deployment - Python\nDESCRIPTION: Example of an asset definition with a tag specifying the section as 'produce' in the main deployment. Tags are used to categorize and organize assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@asset(tags={\"section\": \"produce\"})\ndef fruits_in_stock(): ...\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Definition\nDESCRIPTION: Configuration for dynamic partitioning of member data processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/ingestion.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstart_dynamic_partition\nMEMBER_PARTITION_KEY = \"member_partitions\"\nmember_partitions = dg.DynamicPartitionsDefinition(name=MEMBER_PARTITION_KEY)\nend_dynamic_partition\n```\n\n----------------------------------------\n\nTITLE: Setting Up the Dagster Development Environment\nDESCRIPTION: Command to run the development installation script, which sets up a full Dagster development environment with all modules and runs basic tests.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Installing DuckDB Polars Integration\nDESCRIPTION: Command to install the DuckDB Polars integration package.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-duckdb-polars\n```\n\n----------------------------------------\n\nTITLE: Initializing SitemapScraper Resource in Python\nDESCRIPTION: This code snippet demonstrates how to initialize the SitemapScraper resource. It creates an instance of the SitemapScraper class, which can be used to parse sitemaps and scrape web pages.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/sources.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsitemap_scraper = SitemapScraper()\n```\n\n----------------------------------------\n\nTITLE: Listing Available Component Types in Dagster\nDESCRIPTION: Command to view all component types available in your Dagster environment. This helps you discover what components you can add to your project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/adding-components.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndg list plugins --feature component\n```\n\n----------------------------------------\n\nTITLE: Enabling DBT Code References in Python\nDESCRIPTION: Example of enabling code reference metadata for dbt assets in Dagster by configuring DagsterDbtTranslatorSettings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_dbt import DagsterDbtTranslator, DagsterDbtTranslatorSettings\n\ndbt_assets = load_assets_from_dbt_project(\n    project_dir=\"path/to/dbt/project\",\n    dagster_dbt_translator=DagsterDbtTranslator(\n        settings=DagsterDbtTranslatorSettings(enable_code_references=True)\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment with UV on Windows\nDESCRIPTION: Command to create and activate a Python virtual environment using UV on Windows. This isolates the project dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example dagster_example\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Creating Job Ticks Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'job_ticks' table to store information about Dagster job ticks. It includes columns for job identification, status, type, and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.job_ticks (\n    id bigint NOT NULL,\n    job_origin_id character varying(255),\n    selector_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns in Downstream Asset with Delta Lake I/O Manager in Python\nDESCRIPTION: Demonstrates how to select specific columns from an upstream Delta Lake table when loading it into a downstream asset using metadata configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    ins={\n        \"iris_dataset\": AssetIn(\n            metadata={\"columns\": [\"sepal_length_cm\", \"sepal_width_cm\"]}\n        )\n    }\n)\ndef sepal_data(iris_dataset):\n    return iris_dataset\n```\n\n----------------------------------------\n\nTITLE: Logging into AKS Cluster using Azure CLI\nDESCRIPTION: These commands use the Azure CLI to log into an AKS cluster. The first command initiates the Azure login process, while the second retrieves the credentials for the specified AKS cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/aks-agent.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naz login\naz aks get-credentials --resource-group <your-resource-group> --name <your-aks-cluster>\n```\n\n----------------------------------------\n\nTITLE: Defining dbt Sources for DuckDB R2 Data Access\nDESCRIPTION: Sets up sources.yml to map R2 objects to DuckDB tables using read_ndjson_objects. This configuration enables dbt to read JSON data stored in R2 buckets as structured tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/modeling.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2\n\nsources:\n  - name: atproto\n    schema: '{{ target.schema }}'\n    tables:\n      - name: actor_feed_snapshot\n        identifier: |\n          read_ndjson_objects(\n            '{{ target.external_root }}/atproto_actor_feed_snapshot/*/*/*/*.ndjson'\n          )\n      - name: starter_pack_snapshot\n        identifier: |\n          read_ndjson_objects(\n            '{{ target.external_root }}/atproto_starter_pack_snapshot/*/*/*/*.ndjson'\n          )\n```\n\n----------------------------------------\n\nTITLE: Using LazyFrames in Dagster Assets\nDESCRIPTION: Example showing how to use Polars LazyFrames with Dagster assets, where type annotations control whether to load an eager or lazy DataFrame.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-polars.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(io_manager_key=\"polars_parquet_io_manager\")\ndef downstream(upstream: pl.LazyFrame) -> pl.LazyFrame:\n    assert isinstance(upstream, pl.LazyFrame)\n    return upstream\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Base Image in GitHub Workflow for Dagster+ Serverless\nDESCRIPTION: GitHub workflow configuration showing how to specify a custom base Docker image for Dagster+ Serverless deployment using the SERVERLESS_BASE_IMAGE_TAG environment variable.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: Dagster Cloud Serverless Deploy\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  deploy:\n    name: Serverless Deploy\n    runs-on: ubuntu-latest\n    env:\n      PYTHON_VERSION: 3.10\n      SERVERLESS_BASE_IMAGE_TAG: sha256_518ad2f92b078c63c60e89f0310f13f19d3a1c7ea9e1976d67d59fcb7040d0d6\n    # rest of your deploy.yml...\n```\n\n----------------------------------------\n\nTITLE: Creating Teradata Compute Cluster with dagster-teradata\nDESCRIPTION: This operation provisions a new compute cluster in Teradata VantageCloud Lake. It allows users to specify cluster configuration including compute profiles, resource allocation, and query execution strategies within a Dagster job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncreate_teradata_compute_cluster(\n    compute_profile_name: str,\n    compute_group_name: str,\n    query_strategy: str = \"STANDARD\",\n    compute_map: Optional[str] = None,\n    compute_attribute: Optional[str] = None,\n    timeout: int = constants.CC_OPR_TIME_OUT\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Directory Tree Structure\nDESCRIPTION: Shows the hierarchical directory structure of a Dagster project using tree command output. Illustrates the main project components including source code organization, test directory, and project configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/3-pip-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── pyproject.toml\n├── src\n│   └── jaffle_platform\n│       ├── __init__.py\n│       ├── definitions.py\n│       ├── defs\n│       │   └── __init__.py\n│       └── lib\n│           └── __init__.py\n└── tests\n    └── __init__.py\n\n6 directories, 6 files\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB Resource\nDESCRIPTION: Code to configure the DuckDB resource in Dagster with database path settings\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_duckdb import DuckDBResource\n\ndefs = Definitions(\n    resources={\n        \"duckdb\": DuckDBResource(\n            database=\"data.duckdb\"\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for jobs.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `jobs` table to be generated by the `jobs_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_53\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Airflow Instance and dbt Project\nDESCRIPTION: Command to scaffold the Airflow instance and initialize the dbt project for the tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/setup.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_setup\n```\n\n----------------------------------------\n\nTITLE: Finding Multiple Browser Elements with CSS Selector\nDESCRIPTION: Utility function to find multiple elements with a CSS selector and return them as a list, useful for tests that need to interact with repeated UI elements.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/cli_test_scaffold.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef find_elements(browser, selector):\n    return browser.find_elements_by_css_selector(selector)\n```\n\n----------------------------------------\n\nTITLE: Storing Static Partitioned Assets in DuckDB\nDESCRIPTION: This snippet illustrates how to store static partitioned assets in DuckDB using the DuckDB I/O manager. It uses the 'partition_expr' metadata to specify the column for partitioning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"partition_expr\": \"SPECIES\"})\ndef iris_by_species():\n    return pd.DataFrame({\n        \"SPECIES\": [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"],\n        \"sepal_length_cm\": [5.1, 7.0, 6.3],\n        \"sepal_width_cm\": [3.5, 3.2, 3.3],\n        \"petal_length_cm\": [1.4, 4.7, 6.0],\n        \"petal_width_cm\": [0.2, 1.4, 2.5],\n    })\n```\n\n----------------------------------------\n\nTITLE: Creating Asset Keys Table in Dagster Schema\nDESCRIPTION: Defines a table to track asset keys, including metadata about asset materialization, last run, and timestamps for tracking data lineage and history\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    wipe_timestamp timestamp without time zone,\n    last_materialization_timestamp timestamp without time zone,\n    tags text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: JSON Structure of json_console_logger Output in Dagster\nDESCRIPTION: Example of the JSON structure output by Dagster's json_console_logger. This shows the full log record including Dagster metadata fields and provides insight into available logging data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/custom-logging.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"args\": [],\n  \"created\": 1725455358.2311811,\n  \"dagster_meta\": {\n    \"dagster_event\": null,\n    \"dagster_event_batch_metadata\": null,\n    \"job_name\": \"hackernews_topstory_ids_job\",\n    \"job_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/5b/t062dlpj3j716l4w1d3yq6vm0000gn/T/tmpds_hvzm9\\\"}\",\n      \"dagster/preset_name\": \"default\",\n      \"dagster/solid_selection\": \"*\"\n    },\n    \"log_message_id\": \"3850cfb8-f9fb-458a-a986-3efd26e4b859\",\n    \"log_timestamp\": \"2024-09-04T13:09:18.225289\",\n    \"op_name\": \"get_hackernews_topstory_ids\",\n    \"orig_message\": \"Compute Logger - Got 500 top stories.\",\n    \"resource_fn_name\": null,\n    \"resource_name\": null,\n    \"run_id\": \"11528a21-38d5-43e7-8b13-993e47ce7f7d\",\n    \"step_key\": \"get_hackernews_topstory_ids\"\n  },\n  \"exc_info\": null,\n  \"exc_text\": null,\n  \"filename\": \"log_manager.py\",\n  \"funcName\": \"emit\",\n  \"levelname\": \"INFO\",\n  \"levelno\": 20,\n  \"lineno\": 272,\n  \"module\": \"log_manager\",\n  \"msecs\": 231.0,\n  \"msg\": \"hackernews_topstory_ids_job - 11528a21-38d5-43e7-8b13-993e47ce7f7d - get_hackernews_topstory_ids - Compute Logger - Got 500 top stories.\",\n  \"name\": \"dagster\",\n  \"pathname\": \"/home/dagster/workspace/quickstart-etl/.venv/lib/python3.11/site-packages/dagster/_core/log_manager.py\",\n  \"process\": 35373,\n  \"processName\": \"SpawnProcess-2:1\",\n  \"relativeCreated\": 813.4410381317139,\n  \"stack_info\": null,\n  \"thread\": 8584465408,\n  \"threadName\": \"MainThread\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dagster+ Values Configuration\nDESCRIPTION: YAML configuration for updating Dagster+ deployment to use Key Vault secrets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/key-vault.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloud:\n  agentTokenSecretName: dagster\n  agentTokenEnvVarName: dagsterAgentToken\n\ndagsterCloudAgent:\n  volumeMounts:\n    - name: dagster-token\n      mountPath: /mnt/dagster-token\n      readOnly: true\n  volumes:\n    - name: dagster-token\n      csi:\n        driver: secrets-store.csi.k8s.io\n        readOnly: true\n        volumeAttributes:\n          secretProviderClass: 'azure-kv-dagster-agent-token'\n\nworkspace:\n  envSecrets:\n    - name: dagster\n      optional: false\n  volumeMounts:\n    - name: dagster-token\n      mountPath: /mnt/dagster-token\n      readOnly: true\n  volumes:\n    - name: dagster-token\n      csi:\n        driver: secrets-store.csi.k8s.io\n        readOnly: true\n        volumeAttributes:\n          secretProviderClass: 'azure-kv-dagster-agent-token'\n```\n\n----------------------------------------\n\nTITLE: Updating Asset Check for customers_csv Asset\nDESCRIPTION: Python code snippet showing how to update the asset check to target the customers_csv asset after introducing explicit assets for Airflow task outputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/observe.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Code example omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Filtering Looker Assets\nDESCRIPTION: Python code demonstrating how to filter Looker assets by specifying dashboard folders and explore usage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/looker.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_looker import LookerResource, load_looker_asset_specs, LookerFilter\n\nlooker_resource = LookerResource(\n    base_url=EnvVar(\"LOOKER_BASE_URL\"),\n    client_id=EnvVar(\"LOOKER_CLIENT_ID\"),\n    client_secret=EnvVar(\"LOOKER_CLIENT_SECRET\"),\n)\n\nlooker_filter = LookerFilter(\n    dashboard_folders=[\"Folder 1\", \"Folder 2\"],\n    only_fetch_explores_used_in_dashboards=True,\n)\n\nlooker_asset_specs = load_looker_asset_specs(looker_resource, looker_filter=looker_filter)\n\ndefs = Definitions(\n    assets=looker_asset_specs,\n    resources={\"looker\": looker_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Airflow PythonOperator Example\nDESCRIPTION: An example of how to use the PythonOperator in Airflow to run a Python function that processes files and writes them to a database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/python-operator.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef write_to_db(directory, **context):\n    for file in directory.glob(\"*.csv\"):\n        with open(file) as f:\n            write_to_table_from_csv(f)\n\n\nwrite_to_db_task = PythonOperator(\n    task_id=\"write_to_db\",\n    python_callable=write_to_db,\n    op_kwargs={\"directory\": Path(\"/path/to/files\")},\n    dag=dag,\n)\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with secondary_indexes ID in PostgreSQL\nDESCRIPTION: Associates the 'secondary_indexes_id_seq' sequence with 'secondary_indexes.id', managing ID assignment and ensuring unique index identification within the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.secondary_indexes_id_seq OWNED BY public.secondary_indexes.id;\n```\n\n----------------------------------------\n\nTITLE: Creating secondary_indexes Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the secondary_indexes table to manage various additional indexes that might be required for optimizing data queries and structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: secondary_indexes; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\nALTER TABLE public.secondary_indexes OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Event Logs by Asset Partition in SQL\nDESCRIPTION: Creates a conditional B-tree index on the event_logs table for records with non-null asset_key and partition values to optimize partitioned asset queries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_events_by_asset_partition ON public.event_logs USING btree (asset_key, dagster_event_type, partition, id) WHERE ((asset_key IS NOT NULL) AND (partition IS NOT NULL));\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill Context\nDESCRIPTION: This snippet demonstrates how to import the Dagstermill library for integration with Dagster. The import statement is necessary to leverage Dagstermill's functionality in the code that follows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers_no_yield.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server\nDESCRIPTION: This command starts the Dagster development server, allowing you to view and interact with the project through the Dagster UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Adding Code Location to Dagster+ Hybrid\nDESCRIPTION: Command to add a code location to Dagster+ Hybrid, specifying the deployment, location name, module name, and Docker image URI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud deployment add-location --deployment prod --location-name quickstart --module-name quickstart --image 764506304434.dkr.ecr.us-west-2.amazonaws.com/hooli-data-science-prod:latest\n```\n\n----------------------------------------\n\nTITLE: Loading Sigma Assets from Multiple Organizations\nDESCRIPTION: Python code demonstrating how to combine Sigma assets from multiple organizations into a single Dagster asset graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sigma.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_sigma import SigmaOrganization, load_sigma_asset_specs\n\nsigma_org_1 = SigmaOrganization(\n    base_url=\"https://api.sigmacomputing.com\",\n    client_id=\"your-client-id-1\",\n    client_secret=\"your-client-secret-1\",\n)\n\nsigma_org_2 = SigmaOrganization(\n    base_url=\"https://api.sigmacomputing.com\",\n    client_id=\"your-client-id-2\",\n    client_secret=\"your-client-secret-2\",\n)\n\nsigma_asset_specs_1 = load_sigma_asset_specs(\n    sigma_organization=sigma_org_1,\n)\n\nsigma_asset_specs_2 = load_sigma_asset_specs(\n    sigma_organization=sigma_org_2,\n)\n\ndefs = Definitions(\n    assets=[*sigma_asset_specs_1, *sigma_asset_specs_2],\n    resources={\n        \"sigma_organization_1\": sigma_org_1,\n        \"sigma_organization_2\": sigma_org_2,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Filtered Downstream Asset Selection\nDESCRIPTION: Defines a job selecting downstream CSV assets from a specific starting point\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndownstream_csv_job = define_asset_job(\n    name=\"downstream_csv_job\", selection='key:\"combo_a_b_c_data\"+ and kind:\"csv\"'\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project from Example\nDESCRIPTION: Command to download and set up the assets_dbt_python example project into your working directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dbt_python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --example assets_dbt_python --name assets_dbt_python\n```\n\n----------------------------------------\n\nTITLE: Activating Python Virtual Environment using Shell Command\nDESCRIPTION: This command activates a Python virtual environment located in the .venv directory. It's typically used in shell scripts or run manually to set up the correct Python environment for a project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/2-b-pip-venv.txt#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster HashiCorp Integration Package\nDESCRIPTION: Command to install the dagster-hashicorp package via pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/hashicorp.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-hashicorp\n```\n\n----------------------------------------\n\nTITLE: Reusing Existing Kind Cluster for Integration Tests\nDESCRIPTION: Shell command showing how to run tests against an existing kind cluster and Helm namespace, bypassing cluster creation and setup for faster development cycles.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npytest --kind-cluster=\"cluster-d9971c84d44d47f382a2928c8c161faa\" --existing-helm-namespace=\"dagster-test-95590a\" -s -vvv -m \"not integration\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Content Security Policy Headers\nDESCRIPTION: Defines CSP rules that restrict content sources for enhanced security. Includes base-uri and object-src restrictions, script source controls with nonce validation and dynamic execution, and style source permissions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/app-oss/csp-header.txt#2025-04-22_snippet_0\n\nLANGUAGE: csp\nCODE:\n```\nbase-uri 'none'; object-src 'none'; script-src 'nonce-NONCE-PLACEHOLDER' 'unsafe-inline' https: http: 'strict-dynamic'; style-src 'unsafe-inline' 'self' 'unsafe-eval';\n```\n\n----------------------------------------\n\nTITLE: Auto Materialize Configuration in YAML\nDESCRIPTION: Configuration for automatic asset materialization settings in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nauto_materialize:\n  enabled: true\n  minimum_interval_seconds: 3600\n  run_tags:\n    key: 'value'\n  respect_materialization_data_versions: true\n  max_tick_retries: 3\n  use_sensors: false\n  use_threads: false\n  num_workers: 4\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster UI for Kubernetes Pipes Integration\nDESCRIPTION: This shell command starts the Dagster UI for the Kubernetes Pipes integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/kubernetes-pipeline.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -f dagster_k8s_pipes.py\n```\n\n----------------------------------------\n\nTITLE: Running Local Agent from Configuration in Bash\nDESCRIPTION: Command to start the Dagster Cloud agent using the configuration from dagster.yaml. This command assumes the agent token is already properly configured in the YAML file or environment variable.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/local.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud agent run ~/dagster_home/\n```\n\n----------------------------------------\n\nTITLE: Scaling Down Dagster Components\nDESCRIPTION: These commands scale down the Dagster webserver and daemon deployments to ensure no ongoing job runs are writing to the database during migration. It also saves the current replica counts for scaling back up later.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/migrating-while-upgrading.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Get the names of Dagster's webserver and daemon deployments created by Helm\nexport WEBSERVER_DEPLOYMENT_NAME=`kubectl get deploy \\\n    --selector=component=dagster-webserver -o jsonpath=\"{.items[0].metadata.name}\"`\nexport DAEMON_DEPLOYMENT_NAME=`kubectl get deploy \\\n    --selector=component=dagster-daemon -o jsonpath=\"{.items[0].metadata.name}\"`\n\n# Save each deployment's replica count to scale back up after migrating\nexport WEBSERVER_DEPLOYMENT_REPLICA_COUNT=`kubectl get deploy \\\n    --selector=component=dagster-webserver -o jsonpath=\"{.items[0].status.replicas}\"`\nexport DAEMON_DEPLOYMENT_REPLICA_COUNT=`kubectl get deploy \\\n    --selector=component=dagster-daemon -o jsonpath=\"{.items[0].status.replicas}\"`\n\n# Scale down the Deployments\nkubectl scale deploy $WEBSERVER_DEPLOYMENT_NAME --replicas=0\nkubectl scale deploy $DAEMON_DEPLOYMENT_NAME --replicas=0\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies with pip\nDESCRIPTION: Command to install the requests library needed for the API examples\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-resources/connecting-to-apis.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install requests\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster with Consistent Versioning\nDESCRIPTION: Example showing how to modify pip install commands when working with Dagster integration libraries, which remain on a pre-1.0 version track while being compatible with Dagster 1.x.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster=={DAGSTER_VERSION} dagster-somelibrary=={DAGSTER_VERSION}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster PIPELINE_SUCCESS Event JSON in Python\nDESCRIPTION: JSON representation of a Dagster pipeline success event. This event indicates that the entire 'foo' pipeline completed successfully. The record includes pipeline execution context information and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_59\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of pipeline \\\"foo\\\".\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"foo\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.684907, \"user_message\": \"Finished execution of pipeline \\\"foo\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Cloud Configuration Check\nDESCRIPTION: Command to validate the dagster_cloud.yaml configuration and connection to Dagster+\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/configuring-ci-cd.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud ci check --project-dir=.\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Observation to Dagster+ Cloud using cURL (Bash)\nDESCRIPTION: This example shows how to report an asset observation to Dagster+ cloud deployment using cURL. It sends a POST request with a JSON body containing the asset key, metadata, and data version. The request includes necessary headers for content type and authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n    --url https://{ORGANIZATION}.dagster.cloud/{DEPLOYMENT_NAME}/report_asset_observation/ \\\n    --header 'Content-Type: application/json' \\\n    --header 'Dagster-Cloud-Api-Token: {TOKEN}' \\\n    --data '{\n        \"asset_key\": \"{ASSET_KEY}\",\n        \"metadata\": {\n            \"rows\": 10\n        },\n        \"data_version\": \"{VERSION}\",\n    }'\n```\n\n----------------------------------------\n\nTITLE: Setting up Virtual Environment - MacOS\nDESCRIPTION: Commands to create and activate a Python virtual environment using venv on MacOS.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv source venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Configuring Working Directory for Relative Imports in Dagster Workspace\nDESCRIPTION: This YAML snippet demonstrates how to specify a custom working directory for resolving relative imports in a Dagster workspace configuration. It uses the 'working_directory' key to set the base path for import resolution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nload_from:\n  - python_file: repo.py\n    working_directory: /path/to/working/directory\n```\n\n----------------------------------------\n\nTITLE: Preparing DBT Project for Deployment in CI/CD Workflow\nDESCRIPTION: This YAML snippet adds a step to a GitHub Actions workflow to prepare a dbt project for deployment. It installs dependencies, runs the dagster-dbt prepare-and-package command, and should be placed before the Docker image build step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster-plus/hybrid.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Prepare DBT project for deployment\n  run: |\n    python -m pip install pip --upgrade\n    pip install . --upgrade --upgrade-strategy eager\n    dagster-dbt project prepare-and-package --file <DAGSTER_PROJECT_FOLDER>/project.py\n  shell: bash\n```\n\n----------------------------------------\n\nTITLE: Configuring Run Retries to Ignore Op Failures\nDESCRIPTION: YAML configuration for run retries that will only trigger on system failures like crashes but not on operational failures. This prevents duplicate retries when op-level retries are also configured.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-retries.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nrun_retries:\n  enabled: true # Omit this key if using Dagster+, since run retries are enabled by default\n  max_retries: 3\n  retry_on_asset_or_op_failure: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in AWS ECS Agent Template\nDESCRIPTION: Configuration for setting environment variables in the AWS ECS agent's CloudFormation template under the user_code_launcher config section. Demonstrates how to specify environment variables that can be hardcoded or pulled from the agent environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/environment-variables/agent-config.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nuser_code_launcher:\n  module: dagster_cloud.workspace.ecs\n  class: EcsUserCodeLauncher\n  config:\n    cluster: ${ConfigCluster}\n    subnets: [${ConfigSubnet}]\n    service_discovery_namespace_id: ${ServiceDiscoveryNamespace}\n    execution_role_arn: ${TaskExecutionRole.Arn}\n    task_role_arn: ${AgentRole}\n    log_group: ${AgentLogGroup}\n    env_vars:\n      - SNOWFLAKE_USERNAME=dev\n      - SNOWFLAKE_PASSWORD       ## pulled from agent environment\n' > $DAGSTER_HOME/dagster.yaml && cat $DAGSTER_HOME/dagster.yaml && dagster-cloud agent run\"\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project Component in YAML\nDESCRIPTION: YAML configuration for the dagster_dbt.DbtProjectComponent, specifying the project directory and output schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/components-etl-pipeline-tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\ndagster_dbt:\n  project_dir: ../../../../dbt/jdbt\n  output_schema:\n    target: main\n  selected: [\"*\"]\n  output_key_prefix: [\"target\"]\n```\n\n----------------------------------------\n\nTITLE: Local Artifact Storage Configuration in YAML\nDESCRIPTION: Configuration for local artifact storage in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nlocal_artifact_storage:\n  module: dagster.core.storage.root\n  class: LocalArtifactStorage\n  config:\n    base_dir: /path/to/artifact/storage\n```\n\n----------------------------------------\n\nTITLE: Creating Spark Configuration with Hadoop Properties in Scala\nDESCRIPTION: Demonstrates how to create a SparkConf object and set Hadoop properties programmatically. This approach allows for dynamic configuration without hardcoding values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_53\n\nLANGUAGE: scala\nCODE:\n```\nval conf = new SparkConf().set(\"spark.hadoop.abc.def\",\"xyz\")\nval sc = new SparkContext(conf)\n```\n\n----------------------------------------\n\nTITLE: Specifying ACS URLs and Entity ID for Dagster+ in PingOne\nDESCRIPTION: This snippet shows the URL format for configuring the ACS URLs and Entity ID in PingOne for Dagster+ integration. It requires replacing the placeholder with the actual Dagster+ organization name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/pingone-sso.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://<organization_name>.dagster.cloud/auth/saml/consume\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Server Timeout\nDESCRIPTION: Configuration for setting the timeout duration for gRPC server startup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\ncode_servers:\n  local_startup_timeout: 300\n```\n\n----------------------------------------\n\nTITLE: Serving Component Documentation\nDESCRIPTION: Command to display a documentation webpage containing detailed information about available component types in your Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/adding-components.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndg docs serve\n```\n\n----------------------------------------\n\nTITLE: Creating Mermaid.js Diagrams in Markdown\nDESCRIPTION: Shows how to create simple flowchart diagrams using Mermaid.js syntax in Markdown documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n```mermaid\nflowchart LR\n    Start --> Stop\n```\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Check Evaluation - Local Webserver (Bash)\nDESCRIPTION: Example of reporting a successful asset check named 'check_null_ids' against a locally running Dagster webserver using cURL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST localhost:3000/report_asset_check/{ASSET_KEY}?check_name=check_null_ids&passed=true\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration Package\nDESCRIPTION: Command to install the dagster-aws package which provides AWS Redshift integration capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/redshift.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Visualizing Model Predictions\nDESCRIPTION: This snippet uses Matplotlib to create a scatter plot of the true petal widths against the predicted petal widths.  It predicts the `y` values using `fit.predict(X)`, then plots the true vs. predicted values for visual inspection of the model's performance.  The plot is displayed using `plt.show()`.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_LR.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npred_y = fit.predict(X)\nplt.scatter(y, pred_y)\nplt.title(\"True vs. predicted\")\nplt.xlabel(\"True petal width\")\nplt.ylabel(\"Predicted petal width\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Reporting Asset Check Evaluation - Dagster+ (Bash/JSON)\nDESCRIPTION: Example of reporting a failed asset check named 'check_null_ids' against Dagster+ using cURL with a JSON body. Includes authentication header and metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/external-assets-rest-api.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n    --url https://{ORGANIZATION}.dagster.cloud/{DEPLOYMENT_NAME}/report_asset_check/ \\\n    --header 'Content-Type: application/json' \\\n    --header 'Dagster-Cloud-Api-Token: {TOKEN}' \\\n    --data '{\n        \"asset_key\": \"{ASSET_KEY}\",\n        \"check_name\": \"check_null_ids\",\n        \"passed\": false,\n        \"metadata\": {\n            \"null_rows\": 3\n        },\n    }'\n```\n\n----------------------------------------\n\nTITLE: Logical Operations with Scheduling Conditions in Dagster\nDESCRIPTION: This snippet shows how to use logical operators to combine scheduling conditions in Dagster, enabling complex scheduling scenarios based on multiple conditions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\na | b\n```\n\nLANGUAGE: python\nCODE:\n```\na & b\n```\n\nLANGUAGE: python\nCODE:\n```\n~a\n```\n\n----------------------------------------\n\nTITLE: Configuring Project-level Settings for dg in TOML\nDESCRIPTION: This snippet demonstrates a comprehensive example of the dg-scoped part of a pyproject.toml file for a project named 'my-project'. It includes all supported settings under the 'tool.dg.project' section.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/configuring-dg.md#2025-04-22_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[tool.dg.project]\nname = \"my-project\"\ntype = \"python\"\n\n[tool.dg.project.python]\nsource_dir = \"src\"\ntest_dir = \"tests\"\ndependency_file = \"requirements.txt\"\nmax_workers = 4\n\n[tool.dg.project.lint]\nmax_line_length = 100\nignore = [\"E203\", \"E266\", \"E501\", \"W503\"]\nselect = [\"B\", \"C\", \"E\", \"F\", \"W\", \"T4\", \"B9\"]\n\n[tool.dg.project.test]\nextra_args = [\"-v\", \"--strict-markers\"]\n\n[tool.dg.project.format]\nextra_args = [\"--aggressive\", \"--in-place\"]\n\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data with Pandas\nDESCRIPTION: Reads a CSV file named 'num.csv' into a Pandas DataFrame. Prepares the data for subsequent processing and analysis.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-pandas/dagster_pandas/examples/notebooks/papermill_pandas_hello_world.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"num.csv\")\n```\n\n----------------------------------------\n\nTITLE: Asset Path Selection via CLI\nDESCRIPTION: CLI commands to list and materialize assets between raw_data_b and summary_stats_2\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndagster asset list --select 'key:\"raw_data_b\"+ and +key:\"summary_stats_2\"'\ndagster asset materialize --select 'key:\"raw_data_b\"+ and +key:\"summary_stats_2\"'\n```\n\n----------------------------------------\n\nTITLE: Yielding Result with dagstermill in Python\nDESCRIPTION: This snippet uses the dagstermill library to yield a result, demonstrating how to output data from a Jupyter notebook as a part of a Dagster pipeline. The function `yield_result` takes the result to be output. Ensure that dagstermill is imported and initialized correctly.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_output.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndagstermill.yield_result(\"hello, world\")\n```\n\n----------------------------------------\n\nTITLE: Creating Bar Plot from DataFrame\nDESCRIPTION: Generates a bar plot from the transformed DataFrame. Visualizes the data using Pandas plotting functionality.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-pandas/dagster_pandas/examples/notebooks/papermill_pandas_hello_world.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsome_plot = df.plot(kind=\"bar\")\nsome_plot\n```\n\n----------------------------------------\n\nTITLE: Dagster Version Output Example\nDESCRIPTION: Example output showing the installed Dagster version number.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/installation.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n> dagster --version\ndagster, version 1.8.4\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Jobs for Snowflake Database Cloning\nDESCRIPTION: Defines ops and jobs to clone the production database for each branch deployment and drop the clone when the branch is merged, using the Snowflake resource for query execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, job, graph\nfrom dagster_snowflake import SnowflakeResource\nimport os\n\n@op(required_resource_keys={\"snowflake\"})\ndef drop_database_clone(context):\n    context.resources.snowflake.execute_query(\n        f\"DROP DATABASE IF EXISTS PRODUCTION_CLONE_{os.getenv('DAGSTER_CLOUD_PULL_REQUEST_ID')}\"\n    )\n\n@op(required_resource_keys={\"snowflake\"})\ndef clone_production_database(context):\n    context.resources.snowflake.execute_query(\n        f\"CREATE DATABASE PRODUCTION_CLONE_{os.getenv('DAGSTER_CLOUD_PULL_REQUEST_ID')} CLONE PRODUCTION\"\n    )\n\n@graph\ndef clone_prod():\n    clone_production_database(drop_database_clone())\n\n@graph\ndef drop_prod_clone():\n    drop_database_clone()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Authorization in Dagster-Airlift\nDESCRIPTION: Example demonstrating how to create a custom BaseProxyTasktoDagsterOperator subclass that retrieves an API key from Airflow Variables and sets it in the session for authentication with Dagster's GraphQL API.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/migration-reference.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"airlift-migration-tutorial/tutorial_example/snippets/custom_operator_examples/custom_proxy.py\" />\n```\n\n----------------------------------------\n\nTITLE: Deploying Docker Image via Dagster Cloud CLI (Bash)\nDESCRIPTION: This command demonstrates how to deploy a Docker image instead of a PEX using the Dagster Cloud CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud serverless deploy --location-name=my_location\n```\n\n----------------------------------------\n\nTITLE: Configuring All Tasks as Proxied in YAML\nDESCRIPTION: Final YAML configuration with all Airflow tasks set to proxied: True. This completes the migration of task execution to Dagster while Airflow still handles orchestration and scheduling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/migrate.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndag_id: rebuild_customers_list\ntasks:\n  load_raw_customers:\n    proxied: true\n  build_dbt_models:\n    proxied: true\n  export_customers:\n    proxied: true\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster DBT Dependencies with UV\nDESCRIPTION: Command to install dagster-dbt and dbt-duckdb packages using the UV package manager. This installs the necessary dependencies for using DBT with Dagster and DuckDB as the database backend.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/18-uv-add-dbt.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nuv add dagster-dbt dbt-duckdb\n```\n\n----------------------------------------\n\nTITLE: Creating Airflow Run Sensor in Python\nDESCRIPTION: Creating a sensor to poll the 'warehouse' Airflow instance for new runs of the 'load_customers' DAG and materialize them in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import sensor, RunRequest, SensorResult\nfrom dagster_airlift import build_airflow_run_materialization\n\n@sensor(asset_selection=[load_customers_asset])\ndef warehouse_sensor():\n    last_run = warehouse_airflow.get_last_successful_run(\"load_customers\")\n    if last_run:\n        return SensorResult(\n            run_requests=[RunRequest(run_key=last_run.run_id)],\n            dynamic_materializations=[\n                build_airflow_run_materialization(\n                    run=last_run,\n                    asset=load_customers_asset,\n                )\n            ],\n        )\n    return SensorResult(skip_reason=\"No new successful runs\")\n```\n\n----------------------------------------\n\nTITLE: Basic Arithmetic Operations in Python\nDESCRIPTION: This snippet executes simple arithmetic by assigning numerical values to variables `a` and `b`. It demonstrates variable initialization and simple mathematical operations in Python.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\na = 3\nb = 4\n```\n\n----------------------------------------\n\nTITLE: Alter secondary_indexes_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `secondary_indexes_id_seq` sequence to the `id` column of the `secondary_indexes` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.secondary_indexes_id_seq OWNED BY public.secondary_indexes.id;\"\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Commands to navigate to the Dagster project directory and start the development server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/load-dbt-models.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd jaffle_dagster/\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Loading Additional Images for Postgres and RabbitMQ Services\nDESCRIPTION: Shell commands to pull and load PostgreSQL and RabbitMQ images into a kind cluster, which are required for running Dagster with in-cluster services.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull docker.io/bitnami/rabbitmq\ndocker pull docker.io/bitnami/postgresql\n\nkind load docker-image --name kind-test docker.io/bitnami/rabbitmq:latest\nkind load docker-image --name kind-test docker.io/bitnami/postgresql:latest\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-polars package\nDESCRIPTION: Command to install the dagster-polars integration package using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/polars.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-polars\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for snapshots ID in PostgreSQL\nDESCRIPTION: Applies 'snapshots_id_seq' automatically for default ID setting in 'snapshots', ensuring unique snapshot entry identification and orderly structuring within the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence schedule_ticks_id_seq in PostgreSQL\nDESCRIPTION: This sequence, named 'schedule_ticks_id_seq', automatically generates IDs for the 'schedule_ticks' table, critical for tracing and differentiating between schedule events.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.schedule_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.schedule_ticks_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Table: dynamic_partitions in PostgreSQL\nDESCRIPTION: Defines 'dynamic_partitions' table to manage partition definitions. Includes columns for partition identification, name, and creation timestamp, providing a system to handle partitions dynamically within the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.dynamic_partitions (\n    id integer NOT NULL,\n    partitions_def_name text NOT NULL,\n    partition text NOT NULL,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.dynamic_partitions OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Secret Provider Configuration\nDESCRIPTION: YAML configuration for the Secret Provider Class to access Key Vault secrets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/key-vault.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: azure-kv-dagster-agent-token\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"false\"\n    clientID: \"${CLIENT_ID}\"\n    keyvaultName: ${KEYVAULT_NAME}\n    cloudName: \"\"\n    objects:  |\n      array:\n        - |\n          objectName: dagsterAgentToken\n          objectType: secret\n          objectVersion: \"\"\n    tenantId: \"${IDENTITY_TENANT}\"\n  secretObjects:\n  - data:\n    - key: dagsterAgentToken\n      objectName: dagsterAgentToken\n    secretName: dagster\n    type: Opaque\n```\n\n----------------------------------------\n\nTITLE: Creating an Op Factory Pattern in Python\nDESCRIPTION: This snippet illustrates how to create an op factory function that generates parameterized ops. It defines a function that returns an OpDefinition with customizable name and message.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import op, OpDefinition\n\ndef create_hello_op(name: str, message: str) -> OpDefinition:\n    @op(name=name)\n    def _hello():\n        return message\n\n    return _hello\n\nhello_dagster = create_hello_op(\"hello_dagster\", \"Hello, Dagster!\")\nhello_world = create_hello_op(\"hello_world\", \"Hello, World!\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Upstream Dependencies for Sigma Assets\nDESCRIPTION: Python code showing how to customize upstream dependencies for Sigma assets using a custom DagsterSigmaTranslator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sigma.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CustomSigmaTranslator(DagsterSigmaTranslator):\n    def translate_workbook(self, workbook):\n        asset_spec = super().translate_workbook(workbook)\n        if workbook.name == \"my_sigma_workbook\":\n            asset_spec.upstream_dependencies = {AssetKey(\"my_upstream_asset\")}\n        return asset_spec\n```\n\n----------------------------------------\n\nTITLE: Enabling Non-Isolated Runs in Dagster+ Deployment Settings (YAML)\nDESCRIPTION: This YAML configuration enables non-isolated runs in the Dagster+ deployment settings. It allows for faster start times but with shared resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/run-isolation.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nnon_isolated_runs:\n  enabled: True\n```\n\n----------------------------------------\n\nTITLE: Logging Engine Event for Parent Process Exit - JSON\nDESCRIPTION: This snippet represents a log entry for an engine event, indicating that the parent process has exited after a specified duration. It includes metadata about the process execution and the corresponding PID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"46268\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Multiprocess executor: parent process exiting after 4.24s (pid: 46268)\", \"pid\": 46268, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"3a329e81-04ee-493a-b376-fb5d04d4068b\", \"step_key\": null, \"timestamp\": 1668643924.5049868, \"user_message\": \"Multiprocess executor: parent process exiting after 4.24s (pid: 46268)\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Prometheus Package\nDESCRIPTION: Command to install the dagster-prometheus integration package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/prometheus.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-prometheus\n```\n\n----------------------------------------\n\nTITLE: Create jobs_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `jobs_id_seq` to generate unique IDs for the `jobs` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.jobs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Values for Database Tables in PostgreSQL\nDESCRIPTION: SQL commands that set sequence values for event_logs_id_seq, run_tags_id_seq, and runs_id_seq sequences in the public schema. These sequences are used for auto-incrementing IDs in their respective tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.event_logs_id_seq', 168, true);\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.run_tags_id_seq', 11, true);\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.runs_id_seq', 12, true);\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster CLI Help in Bash\nDESCRIPTION: This command displays the help information for the Dagster CLI (dg). It shows usage instructions, available options, global options, and a list of subcommands for managing Dagster projects.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/1-help.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndg --help\n```\n\n----------------------------------------\n\nTITLE: Executing Custom SQL with Snowflake Resource\nDESCRIPTION: Demonstrates how to use a Snowflake resource to execute custom SQL queries on a Snowflake database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\nfrom dagster_snowflake import SnowflakeResource\n\n@asset(required_resource_keys={\"snowflake\"})\ndef small_petals(context: AssetExecutionContext):\n    with context.resources.snowflake.get_connection() as conn:\n        results = conn.cursor().execute(\n            \"SELECT COUNT(*) FROM FLOWERS.IRIS.IRIS_DATASET WHERE PETAL_LENGTH_CM < 2\"\n        ).fetchone()\n    return results[0]\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to an Asset in Branch Deployment - Python\nDESCRIPTION: Example of an asset definition with dependencies on 'orders' and 'customers' assets. Change Tracking will detect modifications to an asset's upstream dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(deps=[orders, customers])\ndef returns(): ...\n```\n\n----------------------------------------\n\nTITLE: Running dbt Models\nDESCRIPTION: Command to execute dbt models which create tables, views, and materialized views in the SQL database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/dbt_project/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Adding dagster-sling Package with uv in Shell\nDESCRIPTION: This command uses the uv package manager to add the dagster-sling package to a Dagster project. The uv tool is likely being used as an alternative to pip for faster Python package management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/8-uv-add-sling.txt#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nuv add dagster-sling\n```\n\n----------------------------------------\n\nTITLE: Verifying kubectl Context for AKS Cluster\nDESCRIPTION: This command checks the current context of the kubectl installation, which should output the name of the AKS cluster if the login was successful.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/aks-agent.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl config current-context\n```\n\n----------------------------------------\n\nTITLE: Configuring External Redis Message Broker in Helm Values\nDESCRIPTION: This YAML configuration in values.yaml disables RabbitMQ and sets up an external Redis instance as the message broker for Celery in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/kubernetes-and-celery.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nrabbitmq:\n  enabled: false\n\nredis:\n  enabled: true\n  internal: false\n  host: 'redisHost'\n  port: 6379\n  brokerDbNumber: 0\n  backendDbNumber: 0\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Event Logs in PostgreSQL\nDESCRIPTION: Inserts a primary key constraint on the 'id' column of the 'event_logs' table to maintain the integrity and uniqueness of log entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Power BI Dependencies\nDESCRIPTION: Command to install the required Dagster and Power BI integration packages using pip\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/powerbi.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-powerbi\n```\n\n----------------------------------------\n\nTITLE: Viewing Scaffolded Asset File Contents\nDESCRIPTION: Command to display the contents of the newly created asset file, showing the template code generated by the dg scaffold command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/dagster-definitions.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncat defs/assets/my_asset.py\n\nfrom dagster import asset\n\n\n@asset\ndef my_asset():\n    \"\"\"\n    This asset is an example of a scaffolded asset. Replace this docstring with a\n    description of the asset.\n    \"\"\"\n    # Replace this with the logic for your asset\n    pass\n```\n\n----------------------------------------\n\nTITLE: Uploading SAML Metadata to Dagster+ Using CLI\nDESCRIPTION: This command uploads the downloaded SAML metadata file from Google Workspace to Dagster+ using the dagster-cloud CLI. It requires a user token and your organization's URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/google-workspace-sso.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud organization settings saml upload-identity-provider-metadata <the_path/to/metadata> \\\n   --api-token=<user_token> \\\n   --url https://<your_organization_name>.dagster.cloud\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in React/MDX\nDESCRIPTION: Imports and renders a DocCardList component from the theme directory to display documentation cards. The page is configured with frontmatter for sidebar visibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/dagster/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nsidebar_class_name: hidden\ntitle: Dagster core\n---\n```\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Core Allocation with spark.cores.max\nDESCRIPTION: Parameter used to set the maximum amount of CPU cores to request for the application across a Spark standalone cluster or Mesos cluster in coarse-grained sharing mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_37\n\nLANGUAGE: properties\nCODE:\n```\nspark.cores.max\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill Module Functions\nDESCRIPTION: Reference to core Dagstermill functions for notebook integration and execution control in Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagstermill.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: dagstermill\n\n.. autofunction:: define_dagstermill_asset\n\n.. autofunction:: define_dagstermill_op\n\n.. autoclass:: ConfigurableLocalOutputNotebookIOManager\n\n.. autofunction:: get_context\n\n.. autofunction:: yield_event\n\n.. autofunction:: yield_result\n\n.. autoclass:: DagstermillExecutionContext\n\n.. autoclass:: DagstermillError\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill\nDESCRIPTION: This snippet imports the dagstermill library, which is used for integrating Jupyter notebooks with Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Initializing gRPC Server with Python File\nDESCRIPTION: Command to initialize a Dagster gRPC server using a Python file, specifying host and port.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndagster api grpc --python-file /path/to/file.py --host 0.0.0.0 --port 4266\n```\n\n----------------------------------------\n\nTITLE: Customizing DBT Source Asset Keys in YAML\nDESCRIPTION: Example of overriding the asset key for a dbt source and table using meta configuration in YAML. Sets the asset key as 'snowflake/jaffle_shop/orders'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: jaffle_shop\n    tables:\n      - name: orders\n        meta:\n          dagster:\n            asset_key: ['snowflake', 'jaffle_shop', 'orders']\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Command to start the Dagster development server with a specific job file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/celery.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -f celery_job.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Resource for Hooks in YAML\nDESCRIPTION: This YAML configuration snippet shows how to provide the Slack token for the Slack resource used by hooks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-hooks.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  slack:\n    config:\n      token: \"xoxp-1234123412341234-12341234-1234\"\n```\n\n----------------------------------------\n\nTITLE: Setting Working Directory in dagster_cloud.yaml\nDESCRIPTION: Example of using the working_directory setting to load Dagster code from a specific directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/dagster-cloud-yaml.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yaml\n\nlocations:\n  - location_name: data-eng-pipeline\n    code_source:\n      package_name: example_etl\n    working_directory: ./project_directory\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Scheduler Mode and Behavior\nDESCRIPTION: Parameters that control the job scheduling mode (FIFO vs FAIR) and resource revive interval for task scheduling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_40\n\nLANGUAGE: properties\nCODE:\n```\nspark.scheduler.mode\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.scheduler.revive.interval\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.scheduler.listenerbus.eventqueue.capacity\n```\n\n----------------------------------------\n\nTITLE: Running Metrics Airflow Instance\nDESCRIPTION: Starts the \"downstream\" Airflow instance with required environment variables, accessible on port 8082.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake metrics_airflow_run\n```\n\n----------------------------------------\n\nTITLE: Setting Python Version in GitLab CI/CD for Dagster+ Serverless\nDESCRIPTION: GitLab CI/CD configuration showing how to set a custom Python version for Dagster+ Serverless deployments. This sets the PYTHON_VERSION environment variable for the deployment process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nvariables:\n  ORGANIZATION_ID: \"my-org\"\n  DAGSTER_CLOUD_API_TOKEN: $DAGSTER_CLOUD_API_TOKEN\n  ENABLE_FAST_DEPLOYS: \"true\"\n  PYTHON_VERSION: 3.11\n\ninclude:\n  - remote: 'https://dagster-cloud-ci-public.s3.us-west-2.amazonaws.com/gitlab-template-gitlab-v0.yml'\n```\n\n----------------------------------------\n\nTITLE: Data Retention Configuration in YAML\nDESCRIPTION: Configuration for data retention policies in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nretention:\n  schedule:\n    purge_after_days: 90\n  sensor:\n    purge_after_days:\n      skipped: 7\n      failure: 30\n      success: -1\n```\n\n----------------------------------------\n\nTITLE: Test Pod Configuration for Secret Validation\nDESCRIPTION: YAML configuration for a test pod to verify secret mounting and access.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/key-vault.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Pod\napiVersion: v1\nmetadata:\n  name: test-akv-secret-mount\n  labels:\n    azure.workload.identity/use: \"true\"\nspec:\n  serviceAccountName: \"user-cloud-dagster-cloud-agent\"\n  containers:\n    - name: busybox\n      image: registry.k8s.io/e2e-test-images/busybox:1.29-4\n      command:\n        - \"/bin/sleep\"\n        - \"10000\"\n      volumeMounts:\n      - name: secrets-store01-inline\n        mountPath: \"/mnt/secrets-store\"\n        readOnly: true\n      env:\n      - name: DAGSTER_CLOUD_AGENT_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: dagster\n            key: dagsterAgentToken\n      envFrom:\n      - secretRef:\n          name: dagster\n  volumes:\n    - name: secrets-store01-inline\n      csi:\n        driver: secrets-store.csi.k8s.io\n        readOnly: true\n        volumeAttributes:\n          secretProviderClass: \"azure-kv-dagster-agent-token\"\n```\n\n----------------------------------------\n\nTITLE: Resource Relationship Diagram in Mermaid\nDESCRIPTION: Mermaid flowchart showing how Resources connect to Assets, Schedules, Sensors and Definitions, with custom theme configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_12\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    Resource(Resource)\n\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Schedule fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Sensor fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Resource -.-> Asset\n    Resource -.-> Schedule\n    Resource -.-> Sensor\n    Resource ==> Definitions\n```\n\n----------------------------------------\n\nTITLE: Scaffolding Dagster DBT Project Component in Shell\nDESCRIPTION: This command uses the Dagster CLI to scaffold a new DBT project component. It creates a component named 'jdbt' with the project path set to 'dbt/jdbt'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/20-dg-scaffold-jdbt.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold dagster_dbt.DbtProjectComponent jdbt --project-path dbt/jdbt\n```\n\n----------------------------------------\n\nTITLE: Listing Core Dagster Components\nDESCRIPTION: A plain text listing of the three main Dagster packages: the core dagster library, the web server UI component, and the Docker integration package.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/deploy_docker/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndagster\ndagster-webserver\ndagster-docker\n```\n\n----------------------------------------\n\nTITLE: Customizing Airbyte Cloud Asset Metadata\nDESCRIPTION: Customize asset properties for Airbyte Cloud assets by creating a custom DagsterAirbyteTranslator and passing it to load_airbyte_cloud_asset_specs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_airbyte import AirbyteCloudWorkspace, load_airbyte_cloud_asset_specs, DagsterAirbyteTranslator\n\nclass CustomAirbyteTranslator(DagsterAirbyteTranslator):\n    def get_asset_key(self, connection):\n        asset_key = super().get_asset_key(connection)\n        return asset_key.with_prefix(\"custom_prefix\")\n\n    def get_asset_metadata(self, connection):\n        metadata = super().get_asset_metadata(connection)\n        metadata[\"custom_field\"] = \"custom_value\"\n        return metadata\n\nairbyte_cloud_resource = AirbyteCloudWorkspace(\n    workspace_id=\"your_workspace_id\",\n    client_id=\"your_client_id\",\n    client_secret=\"your_client_secret\",\n)\n\ncustom_translator = CustomAirbyteTranslator()\nairbyte_cloud_specs = load_airbyte_cloud_asset_specs(airbyte_cloud_resource, translator=custom_translator)\n\ndefs = Definitions(\n    assets=airbyte_cloud_specs,\n    resources={\"airbyte_cloud\": airbyte_cloud_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Ops in Dagster using Python Decorator\nDESCRIPTION: This snippet demonstrates the use of the 'op' decorator for defining Ops in Dagster. Ops are the basic units of computation in the framework.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/ops.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autodecorator:: op\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame using Pandas in Python\nDESCRIPTION: This snippet applies a filter to the 'iris' DataFrame to select rows where the 'sepal_length' column is between 4.5 and 5.0. The result is stored in the 'filtered' DataFrame.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/clean_data.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfiltered = iris[(iris.sepal_length > 4.5) & (iris.sepal_length < 5.0)]\n```\n\n----------------------------------------\n\nTITLE: Repositories Query in GraphQL\nDESCRIPTION: GraphQL query to retrieve all repositories currently loaded in Dagster, showing their names and location names.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/index.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nquery RepositoriesQuery {\n  repositoriesOrError {\n    ... on RepositoryConnection {\n      nodes {\n        name\n        location {\n          name\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment and Importing Libraries for Bollinger Bands Analysis in Python\nDESCRIPTION: This snippet sets up the Jupyter environment, imports necessary libraries, and configures warnings. It includes custom modules for Bollinger Bands analysis.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/notebooks/bollinger.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 1\n%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'\n\nimport sys\nimport warnings\n\nsys.path.append(\"..\")\n\nimport dagster\n\nwarnings.filterwarnings(\"ignore\", category=dagster.BetaWarning)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport pandera as pa\n\nimport bollinger.lib as bol\n\n%aimport bollinger.lib\n```\n\n----------------------------------------\n\nTITLE: Authenticating and Publishing Docker Images\nDESCRIPTION: Commands to authenticate to AWS ECR and push built images. Requires AWS SSO login and accessing ECR to publish images to private repositories, viewable in the AWS Management Console under ECR.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation/docker/images/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\naws sso login\naws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 968703565975.dkr.ecr.us-west-2.amazonaws.com\ndagster-image push-all --name <IMAGE NAME>\n```\n\n----------------------------------------\n\nTITLE: Final Project Structure\nDESCRIPTION: Shows the final project structure after migrating all modules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmy_existing_project/\n├── definitions.py\n└── defs/\n    ├── elt/\n    │   ├── __init__.py\n    │   └── assets.py\n    ├── ml/\n    │   ├── __init__.py\n    │   └── assets.py\n    └── viz/\n        ├── __init__.py\n        └── assets.py\n```\n\n----------------------------------------\n\nTITLE: Using Check Module for Runtime Type Checking in Python\nDESCRIPTION: This snippet demonstrates the use of the Check module to verify that function parameters conform to expected types, such as integers. It simplifies error handling and ensures consistent exception behavior.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-shared/dagster_shared/check/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import check\n\ndef some_function(should_be_int, optional_int):\n  check.int_param(should_be_int, 'should_be_int')\n  check.opt_int_param(optional_int, 'optional_int')\n\n```\n\n----------------------------------------\n\nTITLE: Partitioned Asset Observation\nDESCRIPTION: Example showing how to log an observation for a specific partition of an asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/asset-observations.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef observe_my_partitioned_asset(context):\n    context.log_event(\n        AssetObservation(\n            asset_key=AssetKey(\"my_partitioned_table\"),\n            partition=\"2022-12-25\"\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Customizing Sigma Asset Definition Metadata\nDESCRIPTION: Python code showing how to customize asset properties for Sigma assets by subclassing DagsterSigmaTranslator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sigma.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, AssetKey\nfrom dagster_sigma import SigmaOrganization, load_sigma_asset_specs, DagsterSigmaTranslator\n\nclass CustomSigmaTranslator(DagsterSigmaTranslator):\n    def translate_workbook(self, workbook):\n        asset_spec = super().translate_workbook(workbook)\n        asset_spec.metadata[\"custom_field\"] = \"custom_value\"\n        return asset_spec\n\n    def translate_dataset(self, dataset):\n        asset_spec = super().translate_dataset(dataset)\n        asset_spec.keys = [AssetKey([\"custom_key\", dataset.name])]\n        return asset_spec\n\nsigma_organization = SigmaOrganization(\n    base_url=\"https://api.sigmacomputing.com\",\n    client_id=\"your-client-id\",\n    client_secret=\"your-client-secret\",\n)\n\nsigma_asset_specs = load_sigma_asset_specs(\n    sigma_organization=sigma_organization,\n    translator=CustomSigmaTranslator(),\n)\n\ndefs = Definitions(\n    assets=sigma_asset_specs,\n    resources={\n        \"sigma_organization\": sigma_organization,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Index on runs.status\nDESCRIPTION: This SQL statement creates an index on the `status` column of the `runs` table. This speeds up queries that filter or sort by run status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_run_status ON public.runs USING btree (status);\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning Airflow and Dagster Run History\nDESCRIPTION: Executes a make command to clean both Airflow run history and Dagster asset materializations, preparing the environment for future runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/peer.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake clean\n```\n\n----------------------------------------\n\nTITLE: Verifying Migration Success\nDESCRIPTION: These commands check the status of the migration job by getting the pod name and inspecting its details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/migrating-while-upgrading.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -l job-name=dagster-instance-migrate\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe pod <POD_NAME>\n```\n\n----------------------------------------\n\nTITLE: Running Project Tests\nDESCRIPTION: Command to execute the project's test suite using pytest.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/creating-a-new-project.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest my_dagster_project_tests\n```\n\n----------------------------------------\n\nTITLE: Creating Index on job_ticks.job_origin_id\nDESCRIPTION: This SQL statement creates an index on the `job_origin_id` column of the `job_ticks` table. This speeds up queries that filter or sort by job origin ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\"\n```\n\n----------------------------------------\n\nTITLE: Linting Dagster UI Code\nDESCRIPTION: Command to run eslint, including prettier and autofixes, across all packages in the Dagster UI project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Importing PrometheusClient in Python\nDESCRIPTION: This snippet demonstrates how to import the PrometheusClient class from the dagster_prometheus.resources module. PrometheusClient is likely used for interacting with Prometheus from within Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-prometheus.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_prometheus.resources import PrometheusClient\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-pagerduty Package\nDESCRIPTION: Command to install the dagster-pagerduty package using pip. This package is required to use the PagerDuty integration with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/pagerduty.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-pagerduty\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in JSX\nDESCRIPTION: This snippet imports the DocCardList component from the theme and renders it within the page. It's likely used to generate a list of configuration-related documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Creating Instigators Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'instigators' table to store information about Dagster instigators. It includes columns for identification, status, type, and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.instigators (\n    id bigint NOT NULL,\n    selector_id character varying(255),\n    repository_selector_id character varying(255),\n    status character varying(63),\n    instigator_type character varying(63),\n    instigator_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Configure Azure Blob Storage for Compute Logs\nDESCRIPTION: YAML configuration for setting up Azure Blob Storage as the compute log manager in Dagster via Helm values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/blob-compute-logs.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ncomputeLogs:\n  enabled: true\n  custom:\n    module: dagster_azure.blob.compute_log_manager\n    class: AzureBlobComputeLogManager\n    config:\n      storage_account: mystorageaccount\n      container: mycontainer\n      default_azure_credential:\n        exclude_environment_credential: false\n      prefix: dagster-logs\n      local_dir: '/tmp/cool'\n      upload_interval: 30\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log JSON Entries\nDESCRIPTION: JSON-formatted log entries showing the execution flow of a Dagster pipeline, including step outputs, asset materializations, and engine events. Each entry contains event metadata, timestamps, and execution details for tracking pipeline progress.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepOutputData\",\n      \"metadata_entries\": [],\n      \"step_output_handle\": {\n        \"__class__\": \"StepOutputHandle\",\n        \"mapping_key\": null,\n        \"output_name\": \"result\",\n        \"step_key\": \"not_partitioned\"\n      }\n    }\n  },\n  \"pipeline_name\": \"__ASSET_JOB\",\n  \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\",\n  \"step_key\": \"not_partitioned\",\n  \"timestamp\": 1729528721.040823\n}\n```\n\n----------------------------------------\n\nTITLE: Assigning code locations to specific agent queues\nDESCRIPTION: YAML configuration in dagster_cloud.yaml for routing different code locations to specific agent queues in different Kubernetes clusters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: project-a\n    ...\n    agent_queue: on-prem-agent-queue\n  - location_name: project-b\n    ...\n    agent_queue: aws-agent-queue\n```\n\n----------------------------------------\n\nTITLE: Setting Default Column Values with Sequences in PostgreSQL\nDESCRIPTION: SQL statements that configure default values for ID columns across multiple tables in the Dagster database. Each ALTER TABLE statement sets the default value for an ID column to use the nextval function with its corresponding sequence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n\nALTER TABLE ONLY public.backfills ALTER COLUMN id SET DEFAULT nextval('public.backfills_id_seq'::regclass);\n\nALTER TABLE ONLY public.bulk_actions ALTER COLUMN id SET DEFAULT nextval('public.bulk_actions_id_seq'::regclass);\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n\nALTER TABLE ONLY public.normalized_cereals ALTER COLUMN id SET DEFAULT nextval('public.normalized_cereals_id_seq'::regclass);\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with runs ID in PostgreSQL\nDESCRIPTION: The association between 'runs_id_seq' and 'runs.id' allows automated numerical ID assignment for new entries, essential for maintaining record order and uniqueness.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in JSX\nDESCRIPTION: This snippet imports the DocCardList component from the '@theme/DocCardList' module. It is typically used in documentation frameworks to render a list of related documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n```\n\n----------------------------------------\n\nTITLE: Generating Docker Image Tag from Git Commit\nDESCRIPTION: Command to create a unique Docker image tag using the git commit SHA\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/configuring-ci-cd.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport IMAGE_TAG=`git log --format=format:%H -n 1`\n```\n\n----------------------------------------\n\nTITLE: Defining Partitioned Asset for Data Generation in Python\nDESCRIPTION: This code snippet defines a Dagster asset with daily partitions. It creates data files for each partition and a snapshot file with the current timestamp. The asset uses DailyPartitionsDefinition and writes data to CSV files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/_TEMPLATE.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import (\n    asset,\n    DailyPartitionsDefinition,\n    AssetExecutionContext,\n    Definitions,\n)\n\nimport datetime\n\n# Define the partitions\npartitions_def = DailyPartitionsDefinition(start_date=\"2023-01-01\")\n\n\n@asset(partitions_def=partitions_def)\ndef upstream_asset(context: AssetExecutionContext):\n    with open(f\"data-{partition_date}.csv\", \"w\") as f:\n        f.write(f\"Data for partition {partition_date}\")\n\n    snapshot_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    with open(f\"data-{snapshot_date}.csv\", \"w\") as f:\n        f.write(f\"Data for partition {partition_date}\")\n\n\ndefs = Definitions(assets=[upstream_asset])\n```\n\n----------------------------------------\n\nTITLE: Project Structure Overview\nDESCRIPTION: Directory structure of the tutorial example showing shared code, Dagster definitions, and Airflow DAGs for the migration process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/setup.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\ntutorial_example\n├── shared: Contains shared Python & SQL code used Airflow and proxied Dagster code\n│\n├── dagster_defs: Contains Dagster definitions\n│   ├── stages: Contains reference implementations of each stage of the migration process\n│   ├── definitions.py: Empty starter file for following along with the tutorial\n│\n├── airflow_dags: Contains the Airflow DAG and associated files\n│   ├── proxied_state: Contains migration state files for each DAG, see migration step below\n│   ├── dags.py: The Airflow DAG definition\n```\n\n----------------------------------------\n\nTITLE: Modifying Airflow DAG to Support Task Proxying\nDESCRIPTION: Python code for modifying an Airflow DAG to be aware of the proxied state. The DAG checks a PROXYING flag and uses a method to read the proxied state from the YAML file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/migrate.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import the `read_proxied_tasks` function from the dagster_airlift library\nfrom dagster_airlift.airlift import read_proxied_tasks\n\n# Set PROXYING to True to enable the proxying\nPROXYING = False\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args=default_args,\n    schedule_interval=\"@daily\",\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # Set proxied_task_state on the DAG\n    if PROXYING:\n        dag.proxied_task_state = read_proxied_tasks(\n            \"proxied_state\", f\"{DAG_ID}.yaml\", DAG_ID\n        )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Running Dagster with Airflow Integration\nDESCRIPTION: Sets environment variables to configure the tutorial directories and launches Dagster pointing to the definitions file that contains Airflow integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/peer.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Set up environment variables to point to the airlift-migration-tutorial directory on your machine\nexport TUTORIAL_EXAMPLE_DIR=$(pwd)\nexport TUTORIAL_DBT_PROJECT_DIR=\"$TUTORIAL_EXAMPLE_DIR/tutorial_example/shared/dbt\"\nexport AIRFLOW_HOME=\"$TUTORIAL_EXAMPLE_DIR/.airflow_home\"\ndagster dev -f tutorial_example/dagster_defs/definitions.py\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Check Definitions Command\nDESCRIPTION: Command output showing validation of components and running dbt parse command\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/34-dg-component-check-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndg check defs\n```\n\n----------------------------------------\n\nTITLE: Including GitLab CI/CD Template for Dagster+ Serverless\nDESCRIPTION: Example of including a Dagster-provided CI/CD template in a GitLab configuration file, which is used to set up the GitLab CI/CD pipeline for Dagster+ Serverless deployments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ninclude:\n  - remote: 'https://dagster-cloud-ci-public.s3.us-west-2.amazonaws.com/gitlab-template-gitlab-v0.yml'\n```\n\n----------------------------------------\n\nTITLE: Other Python Dependencies for Quickstarts\nDESCRIPTION: Additional packages required for certain Dagster quickstart guides, which do not fit into typical dependencies. They are mainly for enhanced functionality in quickstart tutorials.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/requirements.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npandas_gbq  # (quickstart_gcp)\nwordcloud  # (quickstart-*)\n```\n\n----------------------------------------\n\nTITLE: Creating Database Indexes for Performance Optimization in Dagster\nDESCRIPTION: SQL CREATE INDEX commands that establish indexes on specific columns in the Dagster database to improve query performance. These indexes target frequently queried columns like asset_key, event_type, and status fields.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_53\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\nCREATE INDEX idx_event_type ON public.event_logs USING btree (dagster_event_type, id);\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (timestamp);\n```\n\n----------------------------------------\n\nTITLE: Structuring a Dagster+ Project with dagster_cloud.yaml\nDESCRIPTION: Demonstrates the typical file structure of a Dagster+ project including the placement of the dagster_cloud.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/dagster-cloud-yaml.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexample_etl\n├── README.md\n├── example_etl\n│   ├── __init__.py\n│   ├── assets\n│   ├── docker_image\n├── ml_project\n│   ├── example_ml\n│     ├── __init__.py\n│     ├── ml_assets\n├── random_assets.py\n├── example_etl_tests\n├── dagster_cloud.yaml\n├── pyproject.toml\n├── setup.cfg\n└── setup.py\n```\n\n----------------------------------------\n\nTITLE: Definition File Reference\nDESCRIPTION: Reference to the file containing schedule and job definitions\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_aws/README.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nquickstart_aws/definitions.py\n```\n\n----------------------------------------\n\nTITLE: Installing Airflow Required Packages\nDESCRIPTION: Command to install the required Python packages for setting up the local Airflow instance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/setup.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_install\n```\n\n----------------------------------------\n\nTITLE: Scaffolding Dagster Evidence Project via CLI\nDESCRIPTION: Command to create a new Evidence project structure using Dagster's CLI scaffold command. Creates a project named jaffle_dashboard using the EvidenceProject template.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/30-scaffold-jaffle-dashboard.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold dagster_evidence.EvidenceProject jaffle_dashboard\n```\n\n----------------------------------------\n\nTITLE: Defining Spark Driver Extra Java Options in HTML Table\nDESCRIPTION: HTML table row defining the spark.driver.extraJavaOptions property, which allows passing extra JVM options to the driver. It includes usage notes and limitations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<tr>\n  <td><code>spark.driver.extraJavaOptions</code></td>\n  <td>(none)</td>\n  <td>\n    A string of extra JVM options to pass to the driver. For instance, GC settings or other logging.\n    Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap\n    size settings can be set with <code>spark.driver.memory</code> in the cluster mode and through\n    the <code>--driver-memory</code> command line option in the client mode.\n\n    <br /><em>Note:</em> In client mode, this config must not be set through the <code>SparkConf</code>\n    directly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the <code>--driver-java-options</code> command line option or in\n    your default properties file.\n\n  </td>\n</tr>\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries in Python\nDESCRIPTION: This snippet imports the 'dagstermill' and 'pandas' libraries required for data manipulation and pipeline execution. 'dagstermill' is used for interfacing Jupyter notebooks with Dagster, while 'pandas' is employed for data handling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/clean_data.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dagstermill\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Raising an Exception\nDESCRIPTION: This snippet demonstrates exception handling by raising a generic exception, which could be used to trigger error logging or testing error flows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource_with_exception.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nraise Exception()\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding to Dagster Webserver Pod in Kubernetes\nDESCRIPTION: This bash script retrieves the Dagster webserver pod name and sets up port forwarding to access the Dagster UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/kubernetes-and-celery.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDAGSTER_WEBSERVER_POD_NAME=$(kubectl get pods --namespace default \\\n  -l \"app.kubernetes.io/name=dagster,app.kubernetes.io/instance=dagster,component=webserver\" \\\n  -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl --namespace default port-forward $DAGSTER_WEBSERVER_POD_NAME 8080:80\n```\n\n----------------------------------------\n\nTITLE: Default Local Dagster File Structure\nDESCRIPTION: Shows the default file structure for local Dagster storage when DAGSTER_HOME is set but dagster.yaml is not present or empty.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$DAGSTER_HOME\n├── dagster.yaml\n├── history\n│   ├── runs\n│   │   ├── 00636713-98a9-461c-a9ac-d049407059cd.db\n│   │   └── ...\n│   └── runs.db\n└── storage\n    ├── 00636713-98a9-461c-a9ac-d049407059cd\n    │   └── compute_logs\n    │       └── ...\n    └── ...\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI Web Server\nDESCRIPTION: Launches the Dagster UI web server for local development and interaction with the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_dagster_university_start/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Font Styles in React Application\nDESCRIPTION: Setup for adding font styles to the application root using styled-components with the Inter font family as default.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/ui-components/README.md#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\nimport {FontFamily, GlobalInter} from '@dagster-io/ui-components`;\nimport {createGlobalStyle} from 'styled-components';\n\nconst GlobalStyle = createGlobalStyle`\n  * {\n    font-family: ${FontFamily.default}; /* Default font is Inter */\n  }\n`;\n\nconst MyAppRoot = () => (\n  <>\n    <GlobalInter />\n    <GlobalStyle />\n    <div>\n      /* Your app */\n    </div>\n  </>\n);\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Run Coordinators\nDESCRIPTION: Core run coordinator classes from Dagster's internals for managing run execution policies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-coordinators.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster._core.run_coordinator import DefaultRunCoordinator\nfrom dagster._core.run_coordinator import QueuedRunCoordinator\n```\n\n----------------------------------------\n\nTITLE: Updated Definitions Configuration\nDESCRIPTION: Shows the modified definitions.py file after moving the elt module, now using autoloading.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom ml import ml_defs\nfrom viz import viz_defs\n\ndefs = Definitions(\n    assets=[*ml_defs.assets, *viz_defs.assets],\n    jobs=[*ml_defs.jobs, *viz_defs.jobs],\n    schedules=[*ml_defs.schedules, *viz_defs.schedules],\n)\n```\n\n----------------------------------------\n\nTITLE: ETL Jobs Configuration in YAML\nDESCRIPTION: YAML configuration file defining ETL jobs with source and destination S3 paths and SQL transformations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/creating-asset-factories.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ns3:\n  bucket: my-bucket\n\njobs:\n  - name: job1\n    source: path/to/source1.csv\n    destination: path/to/destination1.csv\n    query: SELECT * FROM source WHERE value > 100\n  - name: job2\n    source: path/to/source2.csv\n    destination: path/to/destination2.csv\n    query: SELECT * FROM source WHERE value < 0\n```\n\n----------------------------------------\n\nTITLE: Specifying Compression Codec in Spark Configuration\nDESCRIPTION: Example of how to specify a custom compression codec in Spark configuration using fully qualified class names.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_32\n\nLANGUAGE: java\nCODE:\n```\norg.apache.spark.io.LZ4CompressionCodec\norg.apache.spark.io.LZFCompressionCodec\norg.apache.spark.io.SnappyCompressionCodec\norg.apache.spark.io.ZStdCompressionCodec\n```\n\n----------------------------------------\n\nTITLE: Configuring S3ComputeLogManager in YAML for Dagster\nDESCRIPTION: This snippet shows multiple ways to configure the S3ComputeLogManager, including setting explicit S3 secrets, using a custom endpoint, and relying on environment variables for authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/obstore.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# there are multiples ways to configure the S3ComputeLogManager\n# Explicitly set S3 secrets\ncompute_logs:\n  module:  dagster_obstore.s3.compute_log_manager\n  class: S3ComputeLogManager\n  config:\n    bucket: \"dagster-logs\"\n    access_key_id:\n      env: ACCESS_KEY_ID\n    secret_access_key:\n      env: SECRET_KEY\n    local_dir: \"/tmp/dagster-logs\"\n    allow_http: false\n    allow_invalid_certificates: false\n    timeout: \"60s\"  # Timeout for obstore requests\n    region: \"us-west-1\"\n# Use S3 with custom endpoint (Minio, Cloudflare R2 etc.)\ncompute_logs:\n  module:  dagster_obstore.s3.compute_log_manager\n  class: S3ComputeLogManager\n  config:\n    bucket: \"dagster-logs\"\n    access_key_id: \"access-key-id\"\n    secret_access_key: \"my-key\"\n    local_dir: \"/tmp/dagster-logs\"\n    endpoint: \"http://alternate-s3-host.io\"\n    region: \"us-west-1\"\n# Don't set secrets through config, but let obstore pick it up from ENV VARS\ncompute_logs:\n  module:  dagster_obstore.s3.compute_log_manager\n  class: S3ComputeLogManager\n  config:\n    bucket: \"dagster-logs\"\n    local_dir: \"/tmp/dagster-logs\"\n```\n\n----------------------------------------\n\nTITLE: Spark Deploy Configuration Properties\nDESCRIPTION: Deployment configuration properties for Spark including recovery mode and ZooKeeper settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_52\n\nLANGUAGE: properties\nCODE:\n```\nspark.deploy.recoveryMode=NONE\nspark.deploy.zookeeper.url=None\nspark.deploy.zookeeper.dir=None\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Memory Management Properties in HTML\nDESCRIPTION: This HTML table defines various Spark memory management configuration properties, their default values, and detailed explanations of their purposes. It includes settings for memory fractions, off-heap memory, legacy mode, and cleaning processes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_34\n\nLANGUAGE: HTML\nCODE:\n```\n<table class=\"table\">\n<tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr>\n<tr>\n  <td><code>spark.memory.fraction</code></td>\n  <td>0.6</td>\n  <td>\n    Fraction of (heap space - 300MB) used for execution and storage. The lower this is, the\n    more frequently spills and cached data eviction occur. The purpose of this config is to set\n    aside memory for internal metadata, user data structures, and imprecise size estimation\n    in the case of sparse, unusually large records. Leaving this at the default value is\n    recommended. For more detail, including important information about correctly tuning JVM\n    garbage collection when increasing this value, see\n    <a href=\"tuning.html#memory-management-overview\">this description</a>.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.memory.storageFraction</code></td>\n  <td>0.5</td>\n  <td>\n    Amount of storage memory immune to eviction, expressed as a fraction of the size of the\n    region set aside by <code>spark.memory.fraction</code>. The higher this is, the less\n    working memory may be available to execution and tasks may spill to disk more often.\n    Leaving this at the default value is recommended. For more detail, see\n    <a href=\"tuning.html#memory-management-overview\">this description</a>.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.memory.offHeap.enabled</code></td>\n  <td>false</td>\n  <td>\n    If true, Spark will attempt to use off-heap memory for certain operations. If off-heap memory \n    use is enabled, then <code>spark.memory.offHeap.size</code> must be positive.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.memory.offHeap.size</code></td>\n  <td>0</td>\n  <td>\n    The absolute amount of memory in bytes which can be used for off-heap allocation.\n    This setting has no impact on heap memory usage, so if your executors' total memory consumption \n    must fit within some hard limit then be sure to shrink your JVM heap size accordingly.\n    This must be set to a positive value when <code>spark.memory.offHeap.enabled=true</code>.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.memory.useLegacyMode</code></td>\n  <td>false</td>\n  <td>\n    Whether to enable the legacy memory management mode used in Spark 1.5 and before.\n    The legacy mode rigidly partitions the heap space into fixed-size regions,\n    potentially leading to excessive spilling if the application was not tuned.\n    The following deprecated memory fraction configurations are not read unless this is enabled:\n    <code>spark.shuffle.memoryFraction</code><br>\n    <code>spark.storage.memoryFraction</code><br>\n    <code>spark.storage.unrollFraction</code>\n  </td>\n</tr>\n<tr>\n  <td><code>spark.shuffle.memoryFraction</code></td>\n  <td>0.2</td>\n  <td>\n    (deprecated) This is read only if <code>spark.memory.useLegacyMode</code> is enabled.\n    Fraction of Java heap to use for aggregation and cogroups during shuffles.\n    At any given time, the collective size of\n    all in-memory maps used for shuffles is bounded by this limit, beyond which the contents will\n    begin to spill to disk. If spills are often, consider increasing this value at the expense of\n    <code>spark.storage.memoryFraction</code>.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.storage.memoryFraction</code></td>\n  <td>0.6</td>\n  <td>\n    (deprecated) This is read only if <code>spark.memory.useLegacyMode</code> is enabled.\n    Fraction of Java heap to use for Spark's memory cache. This should not be larger than the \"old\"\n    generation of objects in the JVM, which by default is given 0.6 of the heap, but you can\n    increase it if you configure your own old generation size.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.storage.unrollFraction</code></td>\n  <td>0.2</td>\n  <td>\n    (deprecated) This is read only if <code>spark.memory.useLegacyMode</code> is enabled.\n    Fraction of <code>spark.storage.memoryFraction</code> to use for unrolling blocks in memory.\n    This is dynamically allocated by dropping existing blocks when there is not enough free\n    storage space to unroll the new block in its entirety.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.storage.replication.proactive</code></td>\n  <td>false</td>\n  <td>\n    Enables proactive block replication for RDD blocks. Cached RDD block replicas lost due to\n    executor failures are replenished if there are any existing available replicas. This tries\n    to get the replication level of the block to the initial number.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.cleaner.periodicGC.interval</code></td>\n  <td>30min</td>\n  <td>\n    Controls how often to trigger a garbage collection.<br><br>\n    This context cleaner triggers cleanups only when weak references are garbage collected.\n    In long-running applications with large driver JVMs, where there is little memory pressure\n    on the driver, this may happen very occasionally or not at all. Not cleaning at all may\n    lead to executors running out of disk space after a while.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.cleaner.referenceTracking</code></td>\n  <td>true</td>\n  <td>\n    Enables or disables context cleaning.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.cleaner.referenceTracking.blocking</code></td>\n  <td>true</td>\n  <td>\n    Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by\n    <code>spark.cleaner.referenceTracking.blocking.shuffle</code> Spark property).\n  </td>\n</tr>\n<tr>\n  <td><code>spark.cleaner.referenceTracking.blocking.shuffle</code></td>\n  <td>false</td>\n  <td>\n    Controls whether the cleaning thread should block on shuffle cleanup tasks.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.cleaner.referenceTracking.cleanCheckpoints</code></td>\n  <td>false</td>\n  <td>\n    Controls whether to clean checkpoint files if the reference is out of scope.\n  </td>\n</tr>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Defining External Asset for Existing Table\nDESCRIPTION: Code to create an external asset definition for an existing DuckDB table\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSpec, SourceAsset\n\niris_harvest_data = SourceAsset(\n    AssetSpec(\"iris_harvest_data\", group_name=\"iris\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Workspace Configuration in YAML\nDESCRIPTION: The workspace.yaml file defines how to access and load your code in a Dagster deployment. This snippet shows a placeholder for the file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/oss-deployment-architecture.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nworkspace.yaml\n```\n\n----------------------------------------\n\nTITLE: Create Managed Identity for AKS Agent\nDESCRIPTION: Azure CLI command to create a new managed identity that will be used by the AKS agent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/blob-compute-logs.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naz identity create --resource-group <resource-group> --name agent-identity\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster Plugins via CLI\nDESCRIPTION: This command lists the plugins for a specific library in Dagster. It displays a table showing the symbol name, summary, and features for each component in the plugin.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/creating-dg-plugin/5-list-plugins.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list plugins --plugin my_library\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Plugin List and Component Details\nDESCRIPTION: This snippet shows the output of the 'dg list plugins' command, which lists available Dagster plugins and their associated objects, including symbols, summaries, and features.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/shell-script-component/3-dg-list-plugins.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list plugins\n\nUsing /.../my-component-library/.venv/bin/dagster-components\n┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Plugin               ┃ Objects                                                                                       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ dagster              │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓ │\n│                      │ ┃ Symbol                                                      ┃ Summary      ┃ Features     ┃ │\n│                      │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩ │\n│                      │ │ dagster.asset                                               │ Create a     │ [scaffold-t… │ │\n│                      │ │                                                             │ definition   │              │ │\n│                      │ │                                                             │ for how to   │              │ │\n│                      │ │                                                             │ compute an   │              │ │\n│                      │ │                                                             │ asset.       │              │ │\n│                      │ ├─────────────────────────────────────────────────────────────┼──────────────┼──────────────┤ │\n│                      │ │ dagster.components.DefinitionsComponent                     │ An arbitrary │ [component,  │ │\n│                      │ │                                                             │ set of       │ scaffold-ta… │ │\n│                      │ │                                                             │ dagster      │              │ │\n│                      │ │                                                             │ definitions. │              │ │\n│                      │ ├─────────────────────────────────────────────────────────────┼──────────────┼──────────────┤ │\n│                      │ │ dagster.components.DefsFolderComponent                      │ A folder     │ [component,  │ │\n│                      │ │                                                             │ which may    │ scaffold-ta… │ │\n│                      │ │                                                             │ contain      │              │ │\n│                      │ │                                                             │ multiple     │              │ │\n│                      │ │                                                             │ submodules,  │              │ │\n│                      │ │                                                             │ each         │              │ │\n│                      │ │                                                             │ which define │              │ │\n│                      │ │                                                             │ components.  │              │ │\n│                      │ ├─────────────────────────────────────────────────────────────┼──────────────┼──────────────┤ │\n│                      │ │ dagster.components.PipesSubprocessScriptCollectionComponent │ Assets that  │ [component,  │ │\n│                      │ │                                                             │ wrap Python  │ scaffold-ta… │ │\n│                      │ │                                                             │ scripts      │              │ │\n│                      │ │                                                             │ executed     │              │ │\n│                      │ │                                                             │ with         │              │ │\n│                      │ │                                                             │ Dagster's    │              │ │\n│                      │ │                                                             │ PipesSubpro… │              │ │\n│                      │ ├─────────────────────────────────────────────────────────────┼──────────────┼──────────────┤ │\n│                      │ │ dagster.schedule                                            │ Creates a    │ [scaffold-t… │ │\n│                      │ │                                                             │ schedule     │              │ │\n│                      │ │                                                             │ following    │              │ │\n│                      │ │                                                             │ the provided │              │ │\n│                      │ │                                                             │ cron         │              │ │\n│                      │ │                                                             │ schedule and │              │ │\n│                      │ │                                                             │ requests     │              │ │\n│                      │ │                                                             │ runs for the │              │ │\n│                      │ │                                                             │ provided     │              │ │\n│                      │ │                                                             │ job.         │              │ │\n│                      │ ├─────────────────────────────────────────────────────────────┼──────────────┼──────────────┤ │\n│                      │ │ dagster.sensor                                              │ Creates a    │ [scaffold-t… │ │\n│                      │ │                                                             │ sensor where │              │ │\n│                      │ │                                                             │ the          │              │ │\n│                      │ │                                                             │ decorated    │              │ │\n│                      │ │                                                             │ function is  │              │ │\n│                      │ │                                                             │ used as the  │              │ │\n│                      │ │                                                             │ sensor's     │              │ │\n│                      │ │                                                             │ evaluation   │              │ │\n│                      │ │                                                             │ function.    │              │ │\n│                      │ └─────────────────────────────────────────────────────────────┴──────────────┴──────────────┘ │\n│ my_component_library │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓ │\n│                      │ ┃ Symbol                                ┃ Summary                 ┃ Features                ┃ │\n│                      │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩ │\n│                      │ │ my_component_library.lib.ShellCommand │ Models a shell script   │ [component,             │ │\n│                      │ │                                       │ as a Dagster asset.     │ scaffold-target]        │ │\n│                      │ └───────────────────────────────────────┴─────────────────────────┴─────────────────────────┘ │\n└──────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Raising a Dagster Failure\nDESCRIPTION: This code snippet demonstrates how to raise a `Failure` exception in Dagster. Raising a `Failure` signals that the step has failed, but the pipeline execution may continue. It requires the `dagster` library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/raise_failure.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Failure\n\nraise Failure(\"bad bad notebook\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Agent in YAML\nDESCRIPTION: YAML configuration for the local agent in dagster.yaml, specifying the agent token environment variable and deployment name. This file should be created in your designated Dagster home directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/local.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninstance_class:\n  module: dagster_cloud.instance\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token:\n    env: DAGSTER_AGENT_TOKEN\n  deployment: prod\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagstermill in Python\nDESCRIPTION: This snippet imports the Dagstermill library to enable context management functionality. It is necessary to have the Dagstermill library installed to use the provided context methods.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_logging.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Webserver\nDESCRIPTION: Command to start the Dagster webserver, configured to listen on all interfaces (0.0.0.0) on port 3000.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/deploying-dagster-as-a-service.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster-webserver -h 0.0.0.0 -p 3000\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Dagster Development Project\nDESCRIPTION: Commands to create a new Dagster project from an example template, install dependencies, and launch the web interface. The example demonstrates development-to-production pipeline transitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/development_to_production/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example development_to_production\ncd my-dagster-project\npip install -e \".[dev]\"\n\n# Load it in the web UI\ndagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Adding Dagster Helm Repository\nDESCRIPTION: Adds the Dagster Helm repository to the Helm client, allowing users to download and install Dagster charts. This is a prerequisite for installing Dagster using Helm.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add dagster https://dagster-io.github.io/helm\n```\n\n----------------------------------------\n\nTITLE: Defining Workspace Configuration in YAML\nDESCRIPTION: Example of a workspace.yaml file that replaces the old repository.yaml format. It demonstrates loading repositories from Python modules, files, and environments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_33\n\nLANGUAGE: yaml\nCODE:\n```\nload_from:\n  - python_module:\n      module_name: dagster_examples\n      attribute: define_internal_dagit_repository\n  - python_module: dagster_examples.intro_tutorial.repos\n  - python_file: repos.py\n  - python_environment:\n      executable_path: \"/path/to/venvs/dagster-dev-3.7.6/bin/python\"\n      target:\n        python_module:\n          module_name: dagster_examples\n          location_name: dagster_examples\n          attribute: define_internal_dagit_repository\n```\n\n----------------------------------------\n\nTITLE: Helm RunLauncher Configuration Example\nDESCRIPTION: YAML configuration showing the updated structure for configuring K8sRunLauncher in Helm values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_30\n\nLANGUAGE: yaml\nCODE:\n```\nrunLauncher:\n  type: K8sRunLauncher\n  config:\n    k8sRunLauncher:\n      jobNamespace: ~\n      loadInclusterConfig: true\n      kubeconfigFile: ~\n      envConfigMaps: []\n      envSecrets: []\n```\n\n----------------------------------------\n\nTITLE: Defining Pipelines, Schedules, and Repositories in Python (Pre-0.8.0)\nDESCRIPTION: Example of how pipelines, schedules, and repositories were defined in Dagster prior to version 0.8.0, using separate decorators and functions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n@pipeline\ndef test():\n    ...\n\n@daily_schedule(\n    pipeline_name='test',\n    start_date=datetime.datetime(2020, 1, 1),\n)\ndef daily_test_schedule(_):\n    return {}\n\ntest_partition_set = PartitionSetDefinition(\n    name=\"test\",\n    pipeline_name=\"test\",\n    partition_fn=lambda: [\"test\"],\n    environment_dict_fn_for_partition=lambda _: {},\n)\n\n@schedules\ndef define_schedules():\n    return [daily_test_schedule]\n\n@repository_partitions\ndef define_partitions():\n    return [test_partition_set]\n\ndef define_repository():\n    return RepositoryDefinition('test', pipeline_defs=[test])\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment with UV\nDESCRIPTION: Commands to install UV, create a new virtual environment, and activate it to prepare for installing Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/setup.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install uv\nuv venv\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Defining Genre Categories in Python\nDESCRIPTION: Python code that defines a list of relevant genre categories for book classification, focusing on popular genres found in the Goodreads shelves data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/feature-engineering.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nCATEGORIES = [\n    \"fantasy\", \"horror\", \"humor\", \"adventure\",\n    \"action\", \"romance\", \"ya\", \"superheroes\",\n    \"comedy\", \"mystery\", \"supernatural\", \"drama\",\n]\n```\n\n----------------------------------------\n\nTITLE: Customizing Power BI Asset Definitions\nDESCRIPTION: Example demonstrating how to customize asset properties for Power BI assets using a custom DagsterPowerBITranslator\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/powerbi.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_powerbi import DagsterPowerBITranslator\n\nclass CustomPowerBITranslator(DagsterPowerBITranslator):\n    def get_semantic_model_asset_spec(self, semantic_model):\n        asset_spec = super().get_semantic_model_asset_spec(semantic_model)\n        asset_spec.metadata[\"owner\"] = \"data_team\"\n        return asset_spec\n\n    def get_report_asset_spec(self, report):\n        asset_spec = super().get_report_asset_spec(report)\n        asset_spec.metadata[\"criticality\"] = \"high\"\n        return asset_spec\n```\n\n----------------------------------------\n\nTITLE: Setting Databricks Environment Variables\nDESCRIPTION: Shell commands for setting up required Databricks environment variables for host URL and access token\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/databricks-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport DATABRICKS_HOST=<your-host-url>\nexport DATABRICKS_TOKEN<your-personal-access-token>\n```\n\n----------------------------------------\n\nTITLE: Creating Database Clone in GitLab CI/CD Pipeline\nDESCRIPTION: GitLab CI/CD pipeline configuration for triggering database cloning when a branch deployment is created. The job executes when merge requests are created or updated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndeploy:\\n  script:\\n    - dagster-cloud job launch clone_prod\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Asset Daemon Evaluations by Evaluation ID in SQL\nDESCRIPTION: Creates a B-tree index on the asset_daemon_asset_evaluations table to optimize queries filtering by evaluation_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_asset_daemon_asset_evaluations_evaluation_id ON public.asset_daemon_asset_evaluations USING btree (evaluation_id);\n```\n\n----------------------------------------\n\nTITLE: Running the Dagster UI in Development Mode\nDESCRIPTION: Commands to run the local development version of the Dagster web application with autoreloading for UI development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd dagster/js_modules/dagster-ui\nmake dev_webapp\n```\n\n----------------------------------------\n\nTITLE: Setting PostgreSQL Parameters\nDESCRIPTION: These SQL statements configure various PostgreSQL parameters such as statement timeout, lock timeout, client encoding, and search path. These settings ensure proper database behavior and compatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n\"SET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster UI Components Package\nDESCRIPTION: Command to install the Dagster UI component library using yarn package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/ui-components/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @dagster-io/ui-components\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Pipes Module in Python\nDESCRIPTION: This snippet shows how to import the dagster_pipes module, which is the main module for the Dagster Pipes library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-pipes.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster_pipes\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project Dependencies\nDESCRIPTION: Installs the project and its development dependencies using pip in editable mode. The [dev] flag includes additional development-specific packages.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/assets_yaml_dsl/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Asset Keys in PostgreSQL\nDESCRIPTION: This SQL command adds a primary key constraint on the 'id' column of the 'asset_keys' table, ensuring the uniqueness of each asset key record.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Creating event_logs Table - SQL\nDESCRIPTION: This snippet defines the 'event_logs' table to store logs of events in the orchestration system. It includes fields for identifier, run ID, event description, and timestamps to track execution times.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: event_logs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key character varying\n);\nALTER TABLE public.event_logs OWNER TO test;\n\n```\n\n----------------------------------------\n\nTITLE: Trust Relationship for ECS IAM Roles in AWS\nDESCRIPTION: This JSON policy document defines the trust relationship that allows Amazon ECS to assume the IAM roles. This trust relationship must be included in both the Task Execution IAM role and the Task IAM role.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/amazon-ecs/manual-provision.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ecs-tasks.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Multi-Component Asset Key in Python\nDESCRIPTION: Demonstrates how to define an asset with a multi-component key prefix and a downstream asset that references it in Python.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetIn, asset\n\n\n# start_marker\n@asset(key_prefix=[\"one\", \"two\", \"three\"])\ndef upstream_asset():\n    return [1, 2, 3]\n# end_marker\n\n\n@asset(ins={\"upstream_asset\": AssetIn(key_prefix=[\"one\", \"two\", \"three\"])})\ndef downstream_asset(upstream_asset):\n    return upstream_asset + [4]\n\n```\n\n----------------------------------------\n\nTITLE: Dynamically Loading Spark Properties via spark-submit\nDESCRIPTION: Shows how to configure Spark application properties using command-line options with spark-submit, including setting application name, master URL, and additional Java options.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/spark-submit --name \"My app\" --master local[4] --conf spark.eventLog.enabled=false --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" myApp.jar\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Definitions Directory in Shell\nDESCRIPTION: This command creates a 'defs' directory within an existing project folder named 'my_existing_project'. In Dagster, the 'defs' directory typically contains asset and job definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/5-mkdir-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmkdir my_existing_project/defs\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Expectation Result Event for Raw File Friends Step\nDESCRIPTION: JSON representation of a DagsterEventRecord for a STEP_EXPECTATION_RESULT event. This record indicates a successful check that the 'raw_file_friends' table exists as part of step execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Checked raw_file_friends exists\", \"label\": \"output_table_exists\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_friends\", \"solid_definition\": \"raw_file_friends\", \"step_key\": \"raw_file_friends.compute\"}, \"message\": \"Checked raw_file_friends exists\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_friends\", \"name\": \"raw_file_friends\", \"parent\": null}, \"step_key\": \"raw_file_friends.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Checked raw_file_friends exists\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"output_table_exists\\\", \\\"Checked raw_file_friends exists\\\", []]}\\n               solid = \\\"raw_file_friends\\\"\\n    solid_definition = \\\"raw_file_friends\\\"\\n            step_key = \\\"raw_file_friends.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_friends.compute\", \"timestamp\": 1576110683.040704, \"user_message\": \"Checked raw_file_friends exists\"}\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in React for Dagster Documentation\nDESCRIPTION: This code snippet imports the DocCardList component from the '@theme/DocCardList' module. It is then used to render a list of documentation cards related to Dagster deployment types.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Mill\nDESCRIPTION: This snippet imports the dagstermill library, which enables using Jupyter notebooks as Dagster ops. This import statement is the prerequisite for using any other dagstermill functions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_config_struct.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Key Constraint to Snapshots in PostgreSQL\nDESCRIPTION: Implements a unique constraint on the 'snapshot_id' column of the 'snapshots' table to prevent duplicate snapshot identifiers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Setting up dbt documentation overview with Jinja templating\nDESCRIPTION: This snippet defines the documentation overview section for a fictional ecommerce store called 'jaffle_shop'. It explains that the project is used for testing dbt code and includes a link to the source repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/components/code_locations/dbt_project_location/defs/jaffle_shop_dbt/jaffle_shop/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Setting Snowflake Environment Variables\nDESCRIPTION: Shell commands to set up Snowflake authentication credentials as environment variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport SNOWFLAKE_USER=<your username>\nexport SNOWFLAKE_PASSWORD=<your password>\n```\n\n----------------------------------------\n\nTITLE: Configuring User-level Settings for dg in TOML\nDESCRIPTION: This snippet shows a comprehensive example of a user configuration file for dg. It includes application-level settings in the 'cli' section, which is the only permitted section in this file type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/configuring-dg.md#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[cli]\nlog_level = \"INFO\"\ncolor = true\ntelemetry_enabled = true\nlatest_version_check = true\n\n```\n\n----------------------------------------\n\nTITLE: AWS Service Account Configuration in YAML\nDESCRIPTION: Configuration for associating an IAM role with a Kubernetes service account for AWS permissions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/configuration.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nserviceAccount:\n    create: true\n    annotations:\n      eks.amazonaws.com/role-arn: \"arn:aws:iam::1234567890:role/my_service_account_role\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in Kubernetes Agent\nDESCRIPTION: Configuration for setting environment variables in a Kubernetes agent's values.yaml file. Demonstrates how to specify environment variables under the workspace section that can be hardcoded or pulled from the agent's environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/environment-variables/agent-config.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nworkspace:\n  envVars:\n    - SNOWFLAKE_PASSWORD # value pulled from agent's environment\n    - SNOWFLAKE_USERNAME=dev\n```\n\n----------------------------------------\n\nTITLE: Scaffolding an Evidence Project using Dagster CLI\nDESCRIPTION: Command that uses the Dagster CLI to generate a new Evidence dashboard project structure named 'jaffle_dashboard'. The command references the EvidenceProject component which provides the template for the project scaffolding.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/29-scaffold-jaffle-dashboard.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndg scaffold dagster_evidence.EvidenceProject jaffle_dashboard\n```\n\n----------------------------------------\n\nTITLE: Importing PrometheusResource in Python\nDESCRIPTION: This snippet shows how to import the PrometheusResource from the dagster_prometheus module. PrometheusResource is a ResourceDefinition for configuring Prometheus integration in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-prometheus.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_prometheus import PrometheusResource\n```\n\n----------------------------------------\n\nTITLE: Community Library Dependencies\nDESCRIPTION: Enlists dagster-specific community packages without any special configurations. Each package facilitates integration with community-contributed services or frameworks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/requirements.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndagster-anthropic\ndagster-chroma\ndagster-gemini\ndagster-weaviate\ndagster-qdrant\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Jobs by Type in SQL\nDESCRIPTION: Creates a B-tree index on the jobs table to optimize queries filtering by job_type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_55\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-hightouch Package\nDESCRIPTION: Command to install the dagster-hightouch package using pip. This is required to use the Hightouch integration with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/hightouch.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-hightouch\n```\n\n----------------------------------------\n\nTITLE: Creating and Entering Jaffle Platform Directory in Bash\nDESCRIPTION: This command creates a new directory named 'jaffle-platform' and immediately changes the current working directory to the newly created directory. It combines two shell commands using the && operator to execute them sequentially.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-a-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir jaffle-platform && cd jaffle-platform\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for BigQuery Insights Integration\nDESCRIPTION: Command to install the necessary packages for tracking BigQuery usage with Dagster+ Insights.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/google-bigquery.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-cloud\n```\n\n----------------------------------------\n\nTITLE: Applying RetryPolicy at Different Levels\nDESCRIPTION: Demonstration of applying retry policies at different levels - op definition, op invocation, and job level.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/op-retries.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef my_op():\n    if random() > 0.5:\n        raise Exception(\"Random failure!\")\n    return \"success\"\n\n@job(op_retry_policy=RetryPolicy())\ndef my_job():\n    my_op.with_retry_policy(RetryPolicy(max_retries=3))()\n```\n\n----------------------------------------\n\nTITLE: Defining Spark Driver Extra Classpath in HTML Table\nDESCRIPTION: HTML table row defining the spark.driver.extraClassPath property, which prepends extra classpath entries to the driver's classpath. It includes usage notes for client mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<tr>\n  <td><code>spark.driver.extraClassPath</code></td>\n  <td>(none)</td>\n  <td>\n    Extra classpath entries to prepend to the classpath of the driver.\n\n    <br /><em>Note:</em> In client mode, this config must not be set through the <code>SparkConf</code>\n    directly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the <code>--driver-class-path</code> command line option or in\n    your default properties file.\n\n  </td>\n</tr>\n```\n\n----------------------------------------\n\nTITLE: Getting Dagstermill Context in Python\nDESCRIPTION: The snippet retrieves the context using `dagstermill.get_context()`, which provides access to the pipeline's run context. There are no required parameters, and the output is a context object used for capturing and managing notebook state.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncontext = dagstermill.get_context()\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill Library in Python\nDESCRIPTION: This snippet imports the dagstermill library, a tool for integrating Jupyter Notebooks with Dagster workflows. This import is necessary to access functionalities provided by dagstermill.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/print_dagstermill_context_op_config.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Pipes Session in External Process\nDESCRIPTION: Example showing how to open and manage a Dagster Pipes session in an external process using PipesContext and related components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/dagster-pipes-details-and-customization.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nwith open_dagster_pipes(\n    params_loader=PipesEnvVarParamsLoader(),\n    context_loader=PipesDefaultContextLoader(),\n    message_writer=PipesDefaultMessageWriter(),\n) as pipes:\n    # read inputs\n    input_value = pipes.context.inputs[\"input_name\"]\n    print(f\"Input value: {input_value}\")\n\n    # write outputs\n    pipes.write_output(\"output_name\", {\"raw_value\": \"foo\", \"type\": \"string\"})\n\n    # write metadata\n    pipes.write_metadata({\"raw_value\": \"bar\", \"type\": \"string\"})\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster Definitions with dg CLI\nDESCRIPTION: Using the dg list defs command to verify that the newly created asset has been automatically added to the project's Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/dagster-definitions.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndg list defs\n\nFound 1 definitions in your project.\n\n┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Type                  ┃ Name                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ AssetDefinition       │ stock_prices          │\n└────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Docker Agent\nDESCRIPTION: Example configuration in dagster_cloud.yaml for setting environment variables using Docker agent. Demonstrates how to specify direct values and pull from local environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/environment-variables/agent-config.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      docker:\n        env_vars:\n          - DATABASE_NAME\n          - DATABASE_USERNAME=hooli_testing\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster asset definitions using CLI command\nDESCRIPTION: Output of the 'dg list defs' command which lists all available asset definitions in a Dagster project, organized in a tabular format showing keys, groups, dependencies, kinds, and descriptions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/adding-attributes-to-assets/2-list-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list defs\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Section ┃ Definitions                                    ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Assets  │ ┏━━━━━┳━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┓ │\n│         │ ┃ Key ┃ Group   ┃ Deps ┃ Kinds ┃ Description ┃ │\n│         │ ┡━━━━━╇━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━┩ │\n│         │ │ a   │ default │      │       │             │ │\n│         │ ├─────┼─────────┼──────┼───────┼─────────────┤ │\n│         │ │ b   │ default │      │       │             │ │\n│         │ ├─────┼─────────┼──────┼───────┼─────────────┤ │\n│         │ │ c   │ default │      │       │             │ │\n│         │ └─────┴─────────┴──────┴───────┴─────────────┘ │\n└─────────┴────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Configuring Databricks Spark Python Task\nDESCRIPTION: Defines the configuration for a Databricks Spark Python task, specifying the Python file location in DBFS.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/databricks-pipeline.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"spark_python_task\": {\n   \"python_file\": \"dbfs:/my_python_script.py\",   # location of target code file\n   \"source\": jobs.Source.WORKSPACE,\n}\n```\n\n----------------------------------------\n\nTITLE: Adding dagster-cloud Dependency to Project Setup\nDESCRIPTION: Example setup.py modification to include dagster-cloud as a dependency for a Dagster project. This is required before deploying to Dagster+ Serverless.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/ci-cd-in-serverless.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninstall_requires=[\n    \"dagster\",\n    \"dagster-cloud\",    # add this line\n    ...\n]\n```\n\n----------------------------------------\n\nTITLE: Spark Streaming Configuration Properties\nDESCRIPTION: Configuration properties for Spark Streaming including backpressure, receiver rates, and write-ahead logging settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_49\n\nLANGUAGE: properties\nCODE:\n```\nspark.streaming.backpressure.enabled=false\nspark.streaming.blockInterval=200ms\nspark.streaming.receiver.maxRate=not set\nspark.streaming.unpersist=true\nspark.streaming.stopGracefullyOnShutdown=false\nspark.streaming.kafka.maxRatePerPartition=not set\nspark.streaming.kafka.minRatePerPartition=1\nspark.streaming.kafka.maxRetries=1\nspark.streaming.ui.retainedBatches=1000\n```\n\n----------------------------------------\n\nTITLE: Uploading SAML Metadata to Dagster+ Using CLI\nDESCRIPTION: Shell command for uploading the SAML metadata file to Dagster+ using the dagster-cloud CLI. Requires a user token with appropriate permissions and your organization URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/azure-ad-sso.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud organization settings saml upload-identity-provider-metadata <path/to/metadata> \\\n   --api-token=<user_token> \\\n   --url https://<organization_name>.dagster.cloud\n```\n\n----------------------------------------\n\nTITLE: Scaffolding an Instance of the Component\nDESCRIPTION: Command-line example showing how to scaffold an instance of the ShellCommandComponent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ dg scaffold shell_command my_shell_command\nCreated my_component_library/components/my_shell_command\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project from Example\nDESCRIPTION: This command creates a new Dagster project based on the graph-backed assets example. It sets up the project structure and necessary files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/feature_graph_backed_assets/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example feature_graph_backed_assets\n```\n\n----------------------------------------\n\nTITLE: Implementing Blueprint CSS in React\nDESCRIPTION: Example of importing and using Blueprint CSS in a React application root component.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/ui-components/README.md#2025-04-22_snippet_4\n\nLANGUAGE: jsx\nCODE:\n```\nimport './blueprint.css';\n\nexport const MyAppRoot = () => ...;\n```\n\n----------------------------------------\n\nTITLE: Running Celery Worker (Bash)\nDESCRIPTION: Command to run a Celery worker for Dagster tasks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncelery -A dagster_celery.app worker -l info\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Environment File\nDESCRIPTION: Example of setting the OpenAI API key in a .env file for secure credential management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/openai.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# .env\n\nOPENAI_API_KEY=...\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill Library\nDESCRIPTION: This code snippet imports the Dagstermill library to allow for access to its functionality related to Jupyter notebooks and workflows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource_with_exception.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Structure for Airlift Federation Tutorial\nDESCRIPTION: This code snippet shows the directory structure of the Airlift federation tutorial project. It includes the main components such as constants, Dagster definitions, and Airflow DAGs for both upstream and downstream instances.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/airlift-federation-tutorial/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nairlift_federation_tutorial\n├── constants.py: Contains constant values used throughout both Airflow and Dagster\n├── dagster_defs: Contains Dagster definitions\n│   ├── definitions.py: Empty starter file for following along with the tutorial\n│   └── stages: Contains reference implementations for each stage of the migration process.\n├── metrics_airflow_dags: Contains the Airflow DAGs for the \"downstream\" airflow instance\n└── warehouse_airflow_dags: Contains the Airflow DAGs for the \"upstream\" airflow instance\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-airlift Package for Airflow Integration\nDESCRIPTION: Installs the dagster-airlift package along with core Dagster components using uv package manager in a virtual environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/peer.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\nuv pip install 'dagster-airlift[core]' dagster-webserver dagster\n```\n\n----------------------------------------\n\nTITLE: Prefixed Asset Definition\nDESCRIPTION: Example of defining an asset with a key prefix in Python\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset(\n    key_prefix=\"my_prefix\",\n    ...\n)\ndef raw_data_a() -> None:\n    ...\n```\n\n----------------------------------------\n\nTITLE: ECR Registry Authentication\nDESCRIPTION: Commands to set up AWS account variables and authenticate with ECR registry using AWS CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/deploy_ecs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity --output text | cut -f1)\nexport REGISTRY_URL=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $REGISTRY_URL\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster GRPC Server in Bash\nDESCRIPTION: Command to start a Dagster GRPC server pointing to a specific Python file, used for debugging backcompatibility issues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster api grpc --python-file webserver_service/repo.py --host 0.0.0.0 --port 4266\n```\n\n----------------------------------------\n\nTITLE: Configuring uv-Managed Python Environment in TOML\nDESCRIPTION: Configuration for using uv-managed Python environments in dg.toml and pyproject.toml files. This setting allows dg to manage project-specific environments using uv.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/python-environment-management-and-uv-integration.md#2025-04-22_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[project.python_environment]\nuv_managed = true\n```\n\nLANGUAGE: toml\nCODE:\n```\n[tool.dg.project.python_environment]\nuv_managed = true\n```\n\n----------------------------------------\n\nTITLE: External Python Script Implementation\nDESCRIPTION: Sample Python script that will be invoked as a subprocess by Dagster. Demonstrates basic functionality of generating and printing numbers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/index.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport random\n\nfor i in range(10):\n    number = random.randint(1, 100)\n    print(f\"Generated number {i + 1}: {number}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project in Editable Mode\nDESCRIPTION: Command to install the Dagster project in editable mode using pip. This allows local code changes to automatically apply during development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dynamic_partitions/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Job Ticks by Selector ID and Timestamp in SQL\nDESCRIPTION: Creates a B-tree index on the job_ticks table to optimize queries filtering by selector_id and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_tick_selector_timestamp ON public.job_ticks USING btree (selector_id, \"timestamp\");\n```\n\n----------------------------------------\n\nTITLE: Uploading SAML Metadata to Dagster+ using CLI\nDESCRIPTION: Command to upload the Okta SAML metadata file to Dagster+ using the dagster-cloud CLI. Requires a user token and organization URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/okta-sso.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud organization settings saml upload-identity-provider-metadata <path/to/metadata> \\\n   --api-token=<user_token> \\\n   --url https://<organization_name>.dagster.cloud\n```\n\n----------------------------------------\n\nTITLE: Creating Event Type Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the dagster_event_type and id columns of the event_logs table to optimize queries that filter by event type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_event_type ON public.event_logs USING btree (dagster_event_type, id);\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Chart Configuration from Dagit to DagsterWebserver\nDESCRIPTION: Example showing how to update Helm chart configuration to use the new dagsterWebserver naming convention instead of the deprecated dagit naming.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n# no (deprecated)\ndagit:\n  ...\n  # ...\ningress:\n  dagit: ...\n  readOnlyDagit: ...\n\n# yes\ndagsterWebserver:\n  ...\n  # ...\ningress:\n  dagsterWebserver: ...\n  readOnlyDagsterWebserver: ...\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment on MacOS\nDESCRIPTION: Commands to create and activate a Python virtual environment using uv on MacOS systems.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example source dagster_example/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Submitting Pipeline Execution with Python DagsterGraphQLClient\nDESCRIPTION: Python code to create a DagsterGraphQLClient and submit a pipeline execution, used for debugging backcompatibility issues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_graphql import DagsterGraphQLClient\n\nclient = DagsterGraphQLClient(\"localhost\", port_number=3000)\nclient.submit_pipeline_execution(pipeline_name=\"the_job\", mode=\"default\", run_config={})\n```\n\n----------------------------------------\n\nTITLE: Creating a Bash script for CLI command execution\nDESCRIPTION: This snippet defines a simple Bash script that echoes a greeting and an environment variable. It demonstrates the structure of the external script that will be called by the Dagster asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/pipes_cli_command.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\necho \"Hello from CLI\"\necho \"My env var is: ${MY_ENV_VAR}\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable and Accessing Dagster UI - Helm/Shell\nDESCRIPTION: This snippet sets an environment variable that retrieves the Dagster webserver pod name and forwards its port for local access. It requires a running Kubernetes cluster, correctly configured Helm chart for Dagster, and kubectl access. The expected input is the namespace and Helm release name, and the output is access to the Dagster UI via localhost.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/templates/NOTES.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport DAGSTER_WEBSERVER_POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"dagster.name\" . }},app.kubernetes.io/instance={{ .Release.Name }},component={{ include \"dagster.webserver.componentName\" . }}\" -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Visit http://127.0.0.1:8080 to open the Dagster UI\"\nkubectl --namespace {{ .Release.Namespace }} port-forward $DAGSTER_WEBSERVER_POD_NAME 8080:{{ $_.Values.dagsterWebserver.service.port }}\n```\n\n----------------------------------------\n\nTITLE: Starting Multiple Celery Workers (Bash)\nDESCRIPTION: Command to start multiple Celery workers in the background for Dagster tasks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncelery multi start w2 -A dagster_celery.app -l info\n```\n\n----------------------------------------\n\nTITLE: Installing a Specific Dagster Helm Chart Version\nDESCRIPTION: Installs a specific version of the Dagster Helm chart. The `--version` flag allows users to specify the desired chart version during installation.  The DAGSTER_VERSION environment variable must be set before executing the command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport VERSION=<DAGSTER_VERSION>\nhelm install my-release . \\\n    --namespace dagster \\\n    --create-namespace \\\n    --version $VERSION\n```\n\n----------------------------------------\n\nTITLE: Creating Table runs in PostgreSQL\nDESCRIPTION: Defines 'runs' table structure, including run IDs, pipeline names, and timestamps. It's used for logging information about each pipeline execution, facilitating tracking and management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name character varying,\n    status character varying(63),\n    run_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.runs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Dependencies\nDESCRIPTION: Command to install Dagster and required dependencies using uv pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project from Example in Bash\nDESCRIPTION: Command to bootstrap a new Dagster project using the fully featured example. It creates a project named 'my-dagster-project' based on the 'project_fully_featured' example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example project_fully_featured\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Cloud Build Session\nDESCRIPTION: Command to initialize the build session by reading the dagster_cloud.yaml configuration\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/configuring-ci-cd.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud ci init --project-dir=.\n```\n\n----------------------------------------\n\nTITLE: Creating Cold Cereals View by Calories in SQL\nDESCRIPTION: This snippet creates a view called 'sort_cold_cereals_by_calories' which filters cereals of type 'C' based on calorie count. It utilizes the 'sort_by_calories' data view to pull relevant cereal data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: sort_cold_cereals_by_calories; Type: VIEW; Schema: test-schema; Owner: test\n\nCREATE VIEW \"test-schema\".sort_cold_cereals_by_calories AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n  WHERE (sort_by_calories.type = 'C'::text);\n\nALTER TABLE \"test-schema\".sort_cold_cereals_by_calories OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Migrating Custom Intermediate Storage to IO Manager\nDESCRIPTION: Example demonstrating how to convert a custom intermediate storage definition to an IO manager using the helper method io_manager_from_intermediate_storage in Dagster 0.10.0.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nmy_io_manager_def = io_manager_from_intermediate_storage(\n    my_intermediate_storage_def\n)\n\n@pipeline(\n    mode_defs=[\n        ModeDefinition(\n            resource_defs={\n                \"io_manager\": my_io_manager_def\n            }\n        ),\n    ],\n)\ndef my_pipeline():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Copying Data into alembic_version Table - SQL\nDESCRIPTION: This snippet facilitates the bulk import of existing version numbers into the 'alembic_version' table, enabling the application to recognize and apply database migrations appropriately.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n-- Data for Name: alembic_version; Type: TABLE DATA; Schema: public; Owner: test\nCOPY public.alembic_version (version_num) FROM stdin;\n8f8dba68fd3b\n\\.\n\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Dagster Automation Methods\nDESCRIPTION: A comprehensive table comparing different automation methods in Dagster, including their descriptions, best use cases, and compatibility with various Dagster components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Method                                            | Description                                                                                                                  | Best for                                                     | Works with          |\n| ------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ | ------------------- |\n| [Schedules](schedules/)                           | Run a [selection of assets](/guides/build/assets/asset-selection-syntax) at specified times with a cron expression           | Regular, time-based job runs and basic time-based automation | Assets, Ops, Graphs |\n| [Declarative automation](declarative-automation/) | A framework that allows you to set automation conditions on assets and asset checks                                          | Asset-centric, condition-based updates                       | Assets only         |\n| [Sensors](sensors/)                               | Trigger runs based on events or conditions that you define, like the arrival of a new file or a change to an external system | Event-driven automation                                      | Assets, Ops, Graphs |\n| [Asset sensors](/guides/automate/asset-sensors)   | Trigger jobs when specified assets are materialized, allowing you to create dependencies between jobs or code locations.     | Cross-job/location asset dependencies                        | Assets only         |\n| [GraphQL triggers](/guides/operate/graphql/)      | Trigger materializations and jobs from the GraphQL endpoint                                                                  | Event triggers from external systems                         | Assets, Ops, Jobs   |\n```\n\n----------------------------------------\n\nTITLE: Configuring Asset Concurrency Control for Rate Limiting in Dagster\nDESCRIPTION: This asset decorator applies concurrency controls to the 'actor_feed_snapshot' asset. It uses the 'concurrency' parameter to limit concurrent executions based on the settings defined in dagster.yaml, preventing API rate limit issues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/rate-limiting.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@asset(\n    group_name=\"atproto\",\n    io_manager_key=\"data_warehouse_io_manager\",\n    key_prefix=[\"atproto\"],\n    concurrency=configs.Concurrency(),\n    partitions_def=actor_dp,\n)\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry in JSON\nDESCRIPTION: This snippet represents a single event log entry in JSON format for a Dagster pipeline execution. It includes details such as event type, timestamp, pipeline name, and step information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"EngineEventData\",\n      \"error\": null,\n      \"marker_end\": \"resources\",\n      \"marker_start\": null,\n      \"metadata_entries\": [\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": \"Initialized in 0.41ms\",\n          \"entry_data\": {\n            \"__class__\": \"PythonArtifactMetadataEntryData\",\n            \"module\": \"dagster.core.storage.mem_io_manager\",\n            \"name\": \"InMemoryIOManager\"\n          },\n          \"label\": \"io_manager\"\n        }\n      ]\n    },\n    \"event_type_value\": \"ENGINE_EVENT\",\n    \"logging_tags\": {},\n    \"message\": \"Finished initialization of resources [io_manager].\",\n    \"pid\": 58212,\n    \"pipeline_name\": \"composition\",\n    \"solid_handle\": null,\n    \"step_handle\": null,\n    \"step_key\": null,\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - ENGINE_EVENT - Finished initialization of resources [io_manager].\",\n  \"pipeline_name\": \"composition\",\n  \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n  \"step_key\": null,\n  \"timestamp\": 1640037521.112894,\n  \"user_message\": \"Finished initialization of resources [io_manager].\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Command to start the Dagster UI web server for local development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dbt_python/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Sling Dependencies\nDESCRIPTION: Command to install the required Python packages for using Dagster with Sling\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sling.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-sling\n```\n\n----------------------------------------\n\nTITLE: Reference to External Snowflake Example\nDESCRIPTION: Reference to an external Python code example showing Snowflake integration, located at docs_snippets/docs_snippets/integrations/snowflake.py\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example code referenced externally at:\n# docs_snippets/docs_snippets/integrations/snowflake.py\n```\n\n----------------------------------------\n\nTITLE: Creating Table: instigators in PostgreSQL\nDESCRIPTION: Defines 'instigators' table for tracking status and details of instigators in the system. Important columns include selector IDs and timestamps, useful for managing triggers and jobs originating from instigators.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.instigators (\n    id integer NOT NULL,\n    selector_id character varying(255),\n    repository_selector_id character varying(255),\n    status character varying(63),\n    instigator_type character varying(63),\n    instigator_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.instigators OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Defining DagsterSlingTranslator Class in Python\nDESCRIPTION: This snippet defines the DagsterSlingTranslator class, which is likely used to translate between Dagster and Sling concepts or data structures.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-sling.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: DagsterSlingTranslator\n```\n\n----------------------------------------\n\nTITLE: Importing Compute Log Manager Module in Python\nDESCRIPTION: Imports the compute log manager module from Dagster's core storage package. This module contains classes for managing compute logs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.compute_log_manager\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Bulk Actions by Action Type in SQL\nDESCRIPTION: Creates a B-tree index on the bulk_actions table to optimize queries filtering by action_type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions_action_type ON public.bulk_actions USING btree (action_type);\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Azure Integration Package\nDESCRIPTION: Command to install the dagster-azure package using pip package manager\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/azure-adls2.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-azure\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in Docker Agent\nDESCRIPTION: Configuration for setting environment variables in a Docker agent's dagster.yaml file. Shows how to specify environment variables that can be either hardcoded or pulled from the agent's environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/environment-variables/agent-config.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nuser_code_launcher:\n  module: dagster_cloud.workspace.docker\n  class: DockerUserCodeLauncher\n  config:\n    networks:\n      - dagster_cloud_agent\n    env_vars:\n      - SNOWFLAKE_PASSWORD # value pulled from agent's environment\n      - SNOWFLAKE_USERNAME=dev\n```\n\n----------------------------------------\n\nTITLE: Configuring SAML Reply URL for Dagster+ in Azure AD\nDESCRIPTION: URL format to use when configuring the SAML Identifier and Reply URL fields in Azure AD. The organization name placeholder needs to be replaced with your actual Dagster+ organization name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/azure-ad-sso.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://<organization_name>.dagster.cloud/auth/saml/consume\n```\n\n----------------------------------------\n\nTITLE: Importing SnowflakePandasTypeHandler in Python\nDESCRIPTION: This snippet demonstrates how to import the SnowflakePandasTypeHandler class, which is used for type handling in the Snowflake-Pandas integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-snowflake-pandas.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake_pandas import SnowflakePandasTypeHandler\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment on Windows\nDESCRIPTION: Create a new virtual environment named 'dagster_example' and activate it on Windows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/index.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example dagster_example\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Importing Runs Storage Module in Python\nDESCRIPTION: Imports the runs storage module from Dagster's core storage package. This module contains classes for managing run storage, including SQL-based implementations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.runs\n```\n\n----------------------------------------\n\nTITLE: Raising an Exception\nDESCRIPTION: This snippet raises an exception, simulating an error condition.  It's a simple way to test exception handling mechanisms or to halt execution under certain circumstances.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/error_notebook.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nraise Exception(\"Someone set up us the bomb\")\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-github Package\nDESCRIPTION: Command to install the dagster-github integration package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/github.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-github\n```\n\n----------------------------------------\n\nTITLE: Displaying the Tutorial Project Structure\nDESCRIPTION: A directory tree showing the structure of the tutorial example project, including shared code, Dagster definitions, and Airflow DAGs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/setup.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\ntutorial_example\n├── shared: Contains shared Python & SQL code used Airflow and proxied Dagster code\n│\n├── dagster_defs: Contains Dagster definitions\n│   ├── stages: Contains reference implementations of each stage of the migration process\n│   ├── definitions.py: Empty starter file for following along with the tutorial\n│\n├── airflow_dags: Contains the Airflow DAG and associated files\n│   ├── proxied_state: Contains migration state files for each DAG, see migration step below\n│   ├── dags.py: The Airflow DAG definition\n```\n\n----------------------------------------\n\nTITLE: Creating Docker ECS Context\nDESCRIPTION: Command to create a new Docker ECS context for Dagster deployment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/deploy_ecs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndocker context create ecs dagster-ecs\n```\n\n----------------------------------------\n\nTITLE: Parsing DagsterEventRecord JSON for Step Materialization Event (Users Table)\nDESCRIPTION: JSON structure representing a DagsterEventRecord for a STEP_MATERIALIZATION event. This captures the materialization of a table_info value with metadata about the users table path in a Dagster pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/users.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/users.raw\\\"]]]]}}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.6600409, \"user_message\": \"Materialized value table_info.\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultRunLauncher in YAML\nDESCRIPTION: Configuration for DefaultRunLauncher which spawns a new process in the same node as a job's code location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: dagster._core.launcher\n  class: DefaultRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Using dbt Source in SQL Model\nDESCRIPTION: Example of using a defined dbt source in a SQL model. This selects all columns from the 'jaffle_shop.orders' source where foo equals 1.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nselect *\n  from {{ source(\"jaffle_shop\", \"orders\") }}\n where foo=1\n```\n\n----------------------------------------\n\nTITLE: JavaScript Pipes Utility Functions\nDESCRIPTION: Helper functions for reading environment variables and communicating with Dagster from JavaScript.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/javascript-pipeline.md#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { gunzipSync } = require('zlib');\nconst { writeFileSync } = require('fs');\n\nfunction getDagsterContext() {\n    const context = process.env.DAGSTER_PIPES_CONTEXT;\n    if (!context) return null;\n    const decompressed = gunzipSync(Buffer.from(context, 'base64'));\n    return JSON.parse(decompressed.toString());\n}\n\nfunction writeDagsterMessage(message) {\n    const path = process.env.DAGSTER_PIPES_MESSAGES;\n    if (!path) return;\n    writeFileSync(path, JSON.stringify(message));\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Compression During Shuffle Spill - Shuffle Settings\nDESCRIPTION: This configuration specifies whether to compress data that is spilled during shuffle operations, which typically uses the codec defined in 'spark.io.compression.codec'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_27\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.spill.compress\n```\n\n----------------------------------------\n\nTITLE: Using Custom Scope in Component Configuration\nDESCRIPTION: Demonstrates how to use custom templating scope functions in the component.yaml configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/customizing-components.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntype: my_project.defs.my_def.CustomSubclass\n\nattributes:\n    ...\n    asset_post_processors:\n        - attributes:\n            automation_condition: \"{{ custom_cron('@daily') }}\"\n```\n\n----------------------------------------\n\nTITLE: Updating Sling Component with DuckDB Connection\nDESCRIPTION: Modified YAML configuration for the Sling component that includes connection details for the DuckDB database. This configuration specifies where replicated data should be written.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/components-etl-pipeline-tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncomponent_type: dagster_sling.SlingReplicationCollectionComponent\nreplications:\n  - path: replication.yaml\nconnections:\n  DUCKDB:\n    database: \"jaffle_shop.duckdb\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Task Reaper Settings\nDESCRIPTION: Parameters that control monitoring of killed or interrupted tasks, including polling frequency and thread dump generation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_44\n\nLANGUAGE: properties\nCODE:\n```\nspark.task.reaper.enabled\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.task.reaper.pollingInterval\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.task.reaper.threadDump\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Anthropic Integration\nDESCRIPTION: Command to install the dagster and dagster-anthropic packages via pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/anthropic.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-anthropic\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-airlift and Dependencies in Python Virtual Environment\nDESCRIPTION: Commands to activate a virtual environment and install the necessary packages including dagster-airlift, dagster-webserver, and dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\nuv pip install 'dagster-airlift[core]' dagster-webserver dagster\n```\n\n----------------------------------------\n\nTITLE: Defining SlingResource Class with Replication Method in Python\nDESCRIPTION: This snippet defines the SlingResource class, which includes a replicate method. This resource likely handles Sling-related operations within Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-sling.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: SlingResource\n    :members: replicate\n```\n\n----------------------------------------\n\nTITLE: Creating dbt Profile Configuration\nDESCRIPTION: Commands to create and set up the dbt profiles configuration file. Creates the profiles.yml file in the .dbt directory and requires copying contents from config/profiles.yml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/dbt_project/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntouch ~/.dbt/profiles.yml\n# Copy the contents of `config/profiles.yml` into `~/.dbt/profiles.yml`.\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server for Anthropic Integration Demo\nDESCRIPTION: Command to start the Dagster development server locally. This launches the Dagster UI where you can interact with the assets and pipeline that leverage Anthropic for prompt engineering.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_prompt_eng/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Loading Definitions from Multiple Modules\nDESCRIPTION: Function to load Dagster definitions from multiple modules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/definitions.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nload_definitions_from_modules\n```\n\n----------------------------------------\n\nTITLE: Type Checking with AssetExecutionContext and OpExecutionContext\nDESCRIPTION: Example showing type checking errors when using AssetExecutionContext with OpExecutionContext. This demonstrates the breaking change in Dagster 1.5.0 where AssetExecutionContext is now a subclass of OpExecutionContext rather than a type alias.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef my_helper_function(context: AssetExecutionContext):\n    ...\n\n@op\ndef my_op(context: OpExecutionContext):\n    my_helper_function(context)\n```\n\n----------------------------------------\n\nTITLE: Webserver Workspace Configuration\nDESCRIPTION: YAML configuration for setting up the Dagster webserver workspace with user code servers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterWebserver:\n  workspace:\n    enabled: true\n    servers:\n      - host: 'k8s-example-user-code-1'\n        port: 3030\n      - ...\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Asset Selection Filters in Markdown\nDESCRIPTION: This snippet shows a markdown table that lists various filters used for asset selection in Dagster. It includes filters for keys, tags, owners, groups, kinds, code locations, and more, along with their syntax and descriptions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Filter | Syntax | Description | Supported views |\n|--------|--------|-------------|------------------|\n| **Key (exact)** | `key:\"my_asset\"` | Selects assets with the exact key `my_asset`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Key with one wildcard** | `key:\"prefix_*\"`| Selects assets whose key starts with `prefix`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Key with multiple wildcards** | `key:\"prefix_*_middlefix_*_suffix\"` | Selects assets whose key starts with `prefix`, contains `middlefix`, and ends with `suffix`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Tag (exact)** | `tag:\"stage\"` | Selects assets tagged with `stage`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Tag (with value)** | `tag:\"stage\"=\"value\"` | Selects assets tagged with `stage` having a specific `value`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Owner** | `owner:\"alice\"` | Selects assets owned by `alice`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Group** | `group:\"team1\"` | Selects assets in the group `team1`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Kind** |  `kind:\"table\"` |  Selects assets of kind `table`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Code location** | `code_location:\"repo1\"` | Selects assets located in code location `repo1`. | OSS, Dagster+, Dagster+ branch deployments |\n| **Column tag** | `column_tag: \"my_tag\"` | Selects assets tagged with `my_tag`. | Dagster+ only |\n| **Columns** | `columns: \"my_column\"` | Selects assets with a column named `my_column`. | Dagster+ only |\n| **Table name** | `table_name: \"my_table\"` | Selects assets with a table named `my_table`. | Dagster+ only |\n```\n\n----------------------------------------\n\nTITLE: Yielding Results with Dagstermill in Python\nDESCRIPTION: This snippet uses 'dagstermill.yield_result' to output the 'filtered' DataFrame. This is part of Dagstermill's integration allowing dataframes or results to be passed back to the Dagster pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/clean_data.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndagstermill.yield_result(filtered)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Daemon\nDESCRIPTION: Command to install the Dagster daemon component using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/deploying-dagster-as-a-service.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster\n```\n\n----------------------------------------\n\nTITLE: Installing the Dagster Project Locally\nDESCRIPTION: Commands to install the Dagster project locally and start the Dagster webserver for development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_analytics/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Creating Run Range Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on status, update_timestamp, and create_timestamp columns of the runs table to optimize time-range and status-based queries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_55\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_range ON public.runs USING btree (status, update_timestamp, create_timestamp);\n```\n\n----------------------------------------\n\nTITLE: Adding Sensor to Dagster Definitions in Python\nDESCRIPTION: Updating the Dagster Definitions object to include the created sensor for monitoring Airflow runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[load_customers_asset],\n    sensors=[warehouse_sensor],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-GCP Integration for BigQuery\nDESCRIPTION: Command to install the Dagster-GCP package using pip. This package is required to use the BigQuery integration with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-gcp\n```\n\n----------------------------------------\n\nTITLE: Creating Database Indexes - SQL\nDESCRIPTION: SQL commands that create various indexes on tables for optimizing query performance. Includes btree indexes for asset keys, job ticks, run IDs, and run tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_67\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n\nCREATE INDEX ix_schedule_ticks_schedule_origin_id ON public.schedule_ticks USING btree (schedule_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Foreign Key Constraints in PostgreSQL\nDESCRIPTION: SQL statements defining foreign key constraints to maintain referential integrity between related tables. These constraints link runs to snapshots by snapshot_id, and run_tags to runs by run_id with cascading delete to maintain data consistency.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_69\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Stopping Schedules When Migrating Schedulers\nDESCRIPTION: Bash command to stop running schedules before changing schedulers to prevent dangling cron jobs. This is a required step when migrating to the new DagsterDaemonScheduler.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ndagster schedule stop --location {repository_location_name} {schedule_name}\n```\n\n----------------------------------------\n\nTITLE: Configuring Active Python Environment in TOML\nDESCRIPTION: Configuration for using the active Python environment in dg.toml and pyproject.toml files. This setting relies on user-managed environments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/python-environment-management-and-uv-integration.md#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[project.python_environment]\nactive = true\n```\n\nLANGUAGE: toml\nCODE:\n```\n[tool.dg.project.python_environment]\nactive = true\n```\n\n----------------------------------------\n\nTITLE: Empty Shell Command Component Class Structure\nDESCRIPTION: Initial scaffolded structure for a shell command component in Dagster, showing the basic class definition that inherits from Component and Model.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetKey, Definitions, asset\nfrom dagster._core.definitions.asset_spec import AssetSpec\nfrom dagster_components import Component\nfrom pydantic import model_validator\nfrom typing_extensions import Annotated, override\n\nfrom dagster_components import Model\n\n\nclass ShellCommandComponent(Model, Component):\n    \"\"\"A component that executes a shell command.\"\"\"\n\n    @override\n    def build_defs(self) -> Definitions:\n        \"\"\"Build the definitions for this component.\"\"\"\n        return Definitions()\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Tests Manually\nDESCRIPTION: Command to manually run the Dagster core tests to verify that the development setup is working correctly.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest python_modules/dagster/dagster_tests\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence schedules_id_seq in PostgreSQL\nDESCRIPTION: 'schedules_id_seq' is a sequence used to auto-generate IDs for the 'schedules' table, necessary for maintaining a unique identifier for each schedule entry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.schedules_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.schedules_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in JSX\nDESCRIPTION: This snippet imports the DocCardList component from the theme and renders it to display a list of Dagster examples. The DocCardList component is likely configured to automatically populate with example cards based on the project structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Managing dbt Project State in CI/CD Pipeline\nDESCRIPTION: This bash command fetches the manifest.json file from the production branch and saves it to a state directory for use with dbt defer.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud ci dagster-dbt project manage-state --file path/to/project.py\n```\n\n----------------------------------------\n\nTITLE: Initializing SparkConf with Local Thread Configuration\nDESCRIPTION: Configure a Spark application with a local master URL and two threads, which helps detect distributed context bugs. This minimal configuration demonstrates setting basic Spark properties programmatically.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval conf = new SparkConf()\n.setMaster(\"local[2]\")\n.setAppName(\"CountingSheep\")\nval sc = new SparkContext(conf)\n```\n\n----------------------------------------\n\nTITLE: Sequence Setup for Run Tags\nDESCRIPTION: Establishes a sequence 'run_tags_id_seq' to auto-generate IDs for 'run_tags', facilitating entries uniquely identified by an incrementing integer. This is linked to the 'id' column of 'run_tags' table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n\n\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Run Tags Table\nDESCRIPTION: Defines the 'run_tags' table with fields for 'id', 'run_id', 'key', and 'value'. It sets the table owner and ensures 'id' is a non-null field.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key character varying,\n    value character varying\n);\n\nALTER TABLE public.run_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Running Instance Migration Command for 0.10.0\nDESCRIPTION: The Bash command to migrate Dagster instance storage schema after upgrading to version 0.10.0. This is required to support new features like sensors and run queueing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n# Run after migrating to 0.10.0\n\n$ dagster instance migrate\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Weaviate Integration\nDESCRIPTION: Command to install both Dagster and the Weaviate integration package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/weaviate.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-weaviate\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Index for Dynamic Partitions in SQL\nDESCRIPTION: Creates a unique B-tree index on the dynamic_partitions table to ensure uniqueness of the combination of partitions_def_name and partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\nCREATE UNIQUE INDEX idx_dynamic_partitions ON public.dynamic_partitions USING btree (partitions_def_name, partition);\n```\n\n----------------------------------------\n\nTITLE: Defining Materialized Scheduling Condition in Dagster\nDESCRIPTION: This snippet demonstrates the usage of the materialized scheduling condition in Dagster to trigger computations when an asset is materialized. It is part of the scheduling framework within Dagster and is essential for automatic asset execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.materialized()\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Cereal Data Entries in normalized_cereals Table\nDESCRIPTION: Records from a normalized_cereals table containing nutritional data for various cereal products with columns for different attributes like name, manufacturer, type, calories, protein, fat, etc.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\n42\tLife\tQ\tC\t149.25373134328356\t5.970149253731343\t2.9850746268656714\t223.88059701492534\t2.9850746268656714\t17.91044776119403\t8.955223880597014\t141.79104477611938\t37.31343283582089\t2\t1.4925373134328357\t0.67\t45.328074\n43\tLucky Charms\tG\tC\t110\t2\t1\t180\t0\t12\t12\t55\t25\t2\t1\t1\t26.734515\n44\tMaypo\tA\tH\t100\t4\t1\t0\t0\t16\t3\t95\t25\t2\t1\t1\t54.850917\n45\tMuesli Raisins; Dates; & Almonds\tR\tC\t150\t4\t3\t95\t3\t16\t11\t170\t25\t3\t1\t1\t37.136863\n46\tMuesli Raisins; Peaches; & Pecans\tR\tC\t150\t4\t3\t150\t3\t16\t11\t170\t25\t3\t1\t1\t34.139765\n47\tMueslix Crispy Blend\tK\tC\t238.8059701492537\t4.477611940298507\t2.9850746268656714\t223.88059701492534\t4.477611940298507\t25.373134328358205\t19.402985074626862\t238.8059701492537\t37.31343283582089\t3\t2.2388059701492535\t0.67\t30.313351\n48\tMulti-Grain Cheerios\tG\tC\t100\t2\t1\t220\t2\t15\t6\t90\t25\t1\t1\t1\t40.105965\n49\tNut&Honey Crunch\tK\tC\t179.1044776119403\t2.9850746268656714\t1.4925373134328357\t283.58208955223876\t0\t22.388059701492537\t13.432835820895521\t59.70149253731343\t37.31343283582089\t2\t1.4925373134328357\t0.67\t29.924285\n50\tNutri-Grain Almond-Raisin\tK\tC\t208.955223880597\t4.477611940298507\t2.9850746268656714\t328.35820895522386\t4.477611940298507\t31.343283582089548\t10.44776119402985\t194.02985074626864\t37.31343283582089\t3\t1.9850746268656716\t0.67\t40.69232\n51\tNutri-grain Wheat\tK\tC\t90\t3\t0\t170\t3\t18\t2\t90\t25\t3\t1\t1\t59.642837\n52\tOatmeal Raisin Crisp\tG\tC\t260\t6\t4\t340\t3\t27\t20\t240\t50\t3\t2.5\t0.5\t30.450843\n53\tPost Nat. Raisin Bran\tP\tC\t179.1044776119403\t4.477611940298507\t1.4925373134328357\t298.5074626865671\t8.955223880597014\t16.417910447761194\t20.8955223880597\t388.0597014925373\t37.31343283582089\t3\t1.9850746268656716\t0.67\t37.840594\n54\tProduct 19\tK\tC\t100\t3\t0\t320\t1\t20\t3\t45\t100\t3\t1\t1\t41.50354\n55\tPuffed Rice\tQ\tC\t50\t1\t0\t0\t0\t13\t0\t15\t0\t3\t0.5\t1\t60.756112\n56\tPuffed Wheat\tQ\tC\t50\t2\t0\t0\t1\t10\t0\t50\t0\t3\t0.5\t1\t63.005645\n57\tQuaker Oat Squares\tQ\tC\t200\t8\t2\t270\t4\t28\t12\t220\t50\t3\t2\t0.5\t49.511874\n58\tQuaker Oatmeal\tQ\tH\t149.25373134328356\t7.462686567164178\t2.9850746268656714\t0\t4.029850746268656\t-1.4925373134328357\t-1.4925373134328357\t164.17910447761193\t0\t1\t1.4925373134328357\t0.67\t50.828392\n59\tRaisin Bran\tK\tC\t160\t4\t1.3333333333333333\t280\t6.666666666666666\t18.666666666666664\t16\t320\t33.33333333333333\t2\t1.7733333333333334\t0.75\t39.259197\n60\tRaisin Nut Bran\tG\tC\t200\t6\t4\t280\t5\t21\t16\t280\t50\t3\t2\t0.5\t39.7034\n61\tRaisin Squares\tK\tC\t180\t4\t0\t0\t4\t30\t12\t220\t50\t3\t2\t0.5\t55.333142\n62\tRice Chex\tR\tC\t97.34513274336284\t0.8849557522123894\t0\t212.38938053097345\t0\t20.353982300884958\t1.7699115044247788\t26.548672566371682\t22.123893805309734\t1\t0.8849557522123894\t1.13\t41.998933\n63\tRice Krispies\tK\tC\t110\t2\t0\t290\t0\t22\t3\t35\t25\t1\t1\t1\t40.560159\n64\tShredded Wheat\tN\tC\t80\t2\t0\t0\t3\t16\t0\t95\t0\t1\t0.83\t1\t68.235885\n65\tShredded Wheat 'n'Bran\tN\tC\t134.32835820895522\t4.477611940298507\t0\t0\t5.970149253731343\t28.35820895522388\t0\t208.955223880597\t0\t1\t1.4925373134328357\t0.67\t74.472949\n66\tShredded Wheat spoon size\tN\tC\t134.32835820895522\t4.477611940298507\t0\t0\t4.477611940298507\t29.850746268656714\t0\t179.1044776119403\t0\t1\t1.4925373134328357\t0.67\t72.801787\n67\tSmacks\tK\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t93.33333333333333\t1.3333333333333333\t12\t20\t53.33333333333333\t33.33333333333333\t2\t1.3333333333333333\t0.75\t31.230054\n68\tSpecial K\tK\tC\t110\t6\t0\t230\t1\t16\t3\t55\t25\t1\t1\t1\t53.131324\n69\tStrawberry Fruit Wheats\tN\tC\t90\t2\t0\t15\t3\t15\t5\t90\t25\t2\t1\t1\t59.363993\n70\tTotal Corn Flakes\tG\tC\t110\t2\t1\t200\t0\t21\t3\t35\t100\t3\t1\t1\t38.839746\n71\tTotal Raisin Bran\tG\tC\t140\t3\t1\t190\t4\t15\t14\t230\t100\t3\t1.5\t1\t28.592785\n72\tTotal Whole Grain\tG\tC\t100\t3\t1\t200\t3\t16\t3\t110\t100\t3\t1\t1\t46.658844\n73\tTriples\tG\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t333.3333333333333\t0\t28\t4\t80\t33.33333333333333\t3\t1.3333333333333333\t0.75\t39.106174\n74\tTrix\tG\tC\t110\t1\t1\t140\t0\t13\t12\t25\t25\t2\t1\t1\t27.753301\n75\tWheat Chex\tR\tC\t149.25373134328356\t4.477611940298507\t1.4925373134328357\t343.2835820895522\t4.477611940298507\t25.373134328358205\t4.477611940298507\t171.6417910447761\t37.31343283582089\t1\t1.4925373134328357\t0.67\t49.787445\n76\tWheaties\tG\tC\t100\t3\t1\t200\t3\t17\t3\t110\t25\t1\t1\t1\t51.592193\n77\tWheaties Honey Gold\tG\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t266.66666666666663\t1.3333333333333333\t21.333333333333332\t10.666666666666666\t80\t33.33333333333333\t1\t1.3333333333333333\t0.75\t36.187559\n\\.\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values for Table Columns in Dagster Database\nDESCRIPTION: This SQL snippet sets default values for ID columns in multiple tables using sequences. It's part of the database schema setup for Dagster, ensuring that ID columns are automatically populated with unique values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\n\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n\nALTER TABLE ONLY public.bulk_actions ALTER COLUMN id SET DEFAULT nextval('public.bulk_actions_id_seq'::regclass);\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\nALTER TABLE ONLY public.instigators ALTER COLUMN id SET DEFAULT nextval('public.instigators_id_seq'::regclass);\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Importing DeltaLakePolarsTypeHandler in Python\nDESCRIPTION: This snippet demonstrates the import of the DeltaLakePolarsTypeHandler class, which is used for type handling in the Delta Lake and Polars integration for Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake-polars.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_deltalake_polars import DeltaLakePolarsTypeHandler\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-pipes in Docker for EMR Serverless\nDESCRIPTION: This Dockerfile snippet shows how to install the dagster-pipes module in a Docker image used for an EMR Serverless job. It starts from the EMR base image, installs dagster-pipes using pip, and copies the job script into the image.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-serverless-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# start from EMR image\nFROM public.ecr.aws/emr-serverless/spark/emr-7.2.0:latest\n\nUSER root\n\nRUN python -m pip install dagster-pipes\n\n# copy the job script\nCOPY . .\n\nUSER hadoop\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Locality Wait Parameters\nDESCRIPTION: Group of parameters that control how long Spark will wait to launch tasks at different locality levels (process, node, rack) before giving up and launching on less-local nodes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_38\n\nLANGUAGE: properties\nCODE:\n```\nspark.locality.wait\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.locality.wait.node\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.locality.wait.process\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.locality.wait.rack\n```\n\n----------------------------------------\n\nTITLE: Dagster GCP Dataproc Example Usage\nDESCRIPTION: Example code showing how to use Dagster with GCP Dataproc. This is referenced in the documentation but the actual code content is stored in an external file at docs_snippets/docs_snippets/integrations/gcp-dataproc.py.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/dataproc.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Code content is referenced externally at:\n# docs_snippets/docs_snippets/integrations/gcp-dataproc.py\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Sigma Integration Packages\nDESCRIPTION: Command to install the required Python packages for Dagster and Sigma integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sigma.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-sigma\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project File Tree Structure\nDESCRIPTION: This snippet shows the directory structure of a Dagster project, including the pyproject.toml file, src directory with project code, and tests directory. It illustrates the typical layout of a Dagster project with assets and library code.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/dagster-definitions/2-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── pyproject.toml\n├── src\n│   └── my_project\n│       ├── __init__.py\n│       ├── definitions.py\n│       ├── defs\n│       │   ├── __init__.py\n│       │   └── assets\n│       │       └── my_asset.py\n│       └── lib\n│           └── __init__.py\n├── tests\n│   └── __init__.py\n└── uv.lock\n\n7 directories, 8 files\n```\n\n----------------------------------------\n\nTITLE: Creating Table: job_ticks in PostgreSQL\nDESCRIPTION: Defines 'job_ticks' table for managing job execution ticks, including columns for job ID, selector ID, status, and timestamps. Useful for logging and controlling periodic execution of jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    selector_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.job_ticks OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Documenting Dagster Repository Class and Methods in RST\nDESCRIPTION: ReStructuredText documentation defining the structure and formatting for Dagster's repository-related components. Includes directives for documenting the repository decorator, RepositoryDefinition class with its job management methods, and RepositoryData class.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/repositories.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: dagster\n\nRepositories\n============\n\n.. autodata:: repository\n  :annotation: RepositoryDefinition\n\n.. autoclass:: RepositoryDefinition\n    :members: get_all_jobs, get_job, has_job, job_names\n\n.. autoclass:: RepositoryData\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Module in Python\nDESCRIPTION: This snippet shows how to import the dagster module, which is necessary for using Dagster's scheduling and sensor features.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/schedules-sensors.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Index on key and value Columns in run_tags Table\nDESCRIPTION: SQL command to create a composite index on key and value columns in the run_tags table to optimize tag-based queries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Cloud YAML for Code Locations\nDESCRIPTION: Example YAML configuration for defining code locations in a dagster_cloud.yaml file, including location name and package name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/index.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n- location_name: quickstart\n    code_source:\n    package_name: quickstart\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Storage Emulator\nDESCRIPTION: Configuration for local storage emulators like Azurite or Localstack, enabling HTTP traffic for testing environments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nconfig = AzureConfig(use_emulator=True, client=ClientConfig(allow_http=True))\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Registry Secret for GCR Access from Minikube\nDESCRIPTION: Shell command to create a Kubernetes secret for accessing Google Container Registry from a Minikube cluster, using gcloud authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create secret docker-registry element-dev-key \\\n    --docker-server=https://gcr.io \\\n    --docker-username=oauth2accesstoken \\\n    --docker-password=\"$(gcloud auth print-access-token)\" \\\n    --docker-email=my@email.com\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Model Group Assignment in Dagster\nDESCRIPTION: Example of using the group_from_dbt_resource_props_fallback_to_directory function to maintain the pre-1.4 behavior of assigning groups to dbt models based on subdirectories instead of dbt groups.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_dbt import group_from_dbt_resource_props_fallback_to_directory\n\nload_assets_from_dbt_project(\n    ...,\n    node_info_to_group_fn=group_from_dbt_resource_props_fallback_to_directory,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Container Registry with Azure CLI\nDESCRIPTION: Azure CLI commands to create an Azure Container Registry and attach it to an AKS cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/acr-user-code.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naz login\naz acr create --resource-group <your_resource_group> --name <your-acr-name> --sku Basic\n```\n\nLANGUAGE: bash\nCODE:\n```\naz aks update -n <your-cluster-name> -g <your_resource_group> --attach-acr <your-acr-name>\n```\n\n----------------------------------------\n\nTITLE: Getting Dagster Context with Resource Definition\nDESCRIPTION: This snippet retrieves the Dagster execution context using `dagstermill.get_context()`.  It defines a resource named \"list\" with a `ResourceDefinition` that initializes it to an empty list.  The context object allows access to logging and resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import ResourceDefinition\n\ncontext = dagstermill.get_context(resource_defs={\"list\": ResourceDefinition(lambda _: [])})\n```\n\n----------------------------------------\n\nTITLE: Cloning Dagster Repository\nDESCRIPTION: Command to clone the Dagster repository and navigate to the prompt engineering project directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/docs_projects/project_prompt_eng\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Slack Integration Package\nDESCRIPTION: Command to install the dagster-slack package using pip. This package is required to use Slack integration with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/slack.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-slack\n```\n\n----------------------------------------\n\nTITLE: Configuring Workspace Root with dg.toml\nDESCRIPTION: The root dg.toml file for a Dagster workspace that specifies directory_type as 'workspace' to mark this directory as a workspace container.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/multiple-projects.md#2025-04-22_snippet_0\n\nLANGUAGE: TOML\nCODE:\n```\ndirectory_type = \"workspace\"\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Virtual Environment for Dagster Development\nDESCRIPTION: Commands to create and activate a virtual environment using UV with Python 3.12, which is one of the supported Python versions for Dagster development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv venv --python 3.12\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Start Event for Raw File Friends Step\nDESCRIPTION: JSON representation of a DagsterEventRecord for a STEP_START event. This record indicates the beginning of execution for the 'raw_file_friends.compute' step in the pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_friends\", \"solid_definition\": \"raw_file_friends\", \"step_key\": \"raw_file_friends.compute\"}, \"message\": \"Started execution of step \\\"raw_file_friends.compute\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_friends\", \"name\": \"raw_file_friends\", \"parent\": null}, \"step_key\": \"raw_file_friends.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"raw_file_friends.compute\\\".\\n               solid = \\\"raw_file_friends\\\"\\n    solid_definition = \\\"raw_file_friends\\\"\\n            step_key = \\\"raw_file_friends.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_friends.compute\", \"timestamp\": 1576110683.0005178, \"user_message\": \"Started execution of step \\\"raw_file_friends.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Lake I/O Manager\nDESCRIPTION: Configuration setup for Delta Lake I/O manager specifying root path and schema for Delta tables\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/using-deltalake-with-dagster.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[iris_dataset],\n    resources={\n        \"io_manager\": DeltaLakePandasIOManager(\n            root_path=\"path/to/deltalake\",\n            schema=\"iris\"\n        )\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Default SSO Role in Dagster+\nDESCRIPTION: Configures the default role assigned to new users logging in via SSO. Can be set to specific roles like EDITOR or NO_ACCESS to control automatic account creation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/deployment-settings-reference.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsso_default_role: EDITOR\n```\n\n----------------------------------------\n\nTITLE: Customizing DBT Model Owners in YAML\nDESCRIPTION: Example of overriding the Dagster owners for a dbt model using meta configuration. Assigns multiple owners to the customers model.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: customers\n    config:\n      meta:\n        dagster:\n          owners: ['owner@company.com', 'team:data@company.com']\n```\n\n----------------------------------------\n\nTITLE: Deploying with Custom Base Image via Dagster Cloud CLI (Bash)\nDESCRIPTION: This command demonstrates how to deploy code with a custom base image using the Dagster Cloud CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud serverless deploy --base-image=my_base_image:latest --location-name=my_location\n```\n\n----------------------------------------\n\nTITLE: Cloning Jaffle Dashboard with Git\nDESCRIPTION: Shell command that performs a shallow clone of the jaffle-dashboard repository and removes its Git history. The --depth=1 flag creates a shallow clone with only the latest commit to reduce download size.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/28-jaffle-dashboard-clone.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --depth=1 https://github.com/dagster-io/jaffle-dashboard.git jaffle_dashboard && rm -rf jaffle_dashboard/.git\n```\n\n----------------------------------------\n\nTITLE: Importing RunLauncher Class in Python\nDESCRIPTION: This snippet shows how to import the base RunLauncher class from Dagster's core launcher module. The RunLauncher is the base class for all run launchers in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-launchers.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster._core.launcher import RunLauncher\n```\n\n----------------------------------------\n\nTITLE: Importing Sling Assets Decorator in Python\nDESCRIPTION: This snippet shows the import statement for the sling_assets decorator from the dagster_sling module. The sling_assets decorator is used to define Sling assets in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-sling.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autodecorator:: sling_assets\n```\n\n----------------------------------------\n\nTITLE: SparkR Configuration Properties\nDESCRIPTION: Settings for SparkR including thread configuration, R script execution, and connection timeout parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_50\n\nLANGUAGE: properties\nCODE:\n```\nspark.r.numRBackendThreads=2\nspark.r.command=Rscript\nspark.r.driver.command=spark.r.command\nspark.r.shell.command=R\nspark.r.backendConnectionTimeout=6000\nspark.r.heartBeatInterval=100\n```\n\n----------------------------------------\n\nTITLE: Running the Dagster Web Server for UI Development\nDESCRIPTION: Commands to run a Dagster webserver instance on port 3333 using a sample job file, which is needed for UI development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd dagster/examples/docs_snippets/docs_snippets/intro_tutorial/basics/connecting_ops/\ndagster-webserver -p 3333 -f complex_job.py\n```\n\n----------------------------------------\n\nTITLE: Importing Proxying Functions and Classes in Python\nDESCRIPTION: Imports various functions and classes related to proxying between Dagster and Airflow. These components enable the execution of Dagster assets within Airflow workflows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-airlift.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_airlift.in_airflow import proxying_to_dagster, BaseDagsterAssetsOperator, DefaultProxyTaskToDagsterOperator, DefaultProxyDAGToDagsterOperator\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster Projects using CLI\nDESCRIPTION: This command uses the Dagster CLI to list all available projects. It displays the paths of two projects: 'project-1' and 'project-2'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/workspace/6-project-list.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg list project\n```\n\n----------------------------------------\n\nTITLE: Calculating Sum of Two Variables\nDESCRIPTION: This snippet calculates the sum of two previously assigned variables, 'a' and 'b'. This operation showcases basic Python functionality and can be expanded for more complex calculations if needed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers_no_yield.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult = a + b\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in JSX\nDESCRIPTION: This code snippet imports the DocCardList component from the '@theme/DocCardList' module. It is then used to render a list of documentation cards on the page.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Creating public.instance_info Table\nDESCRIPTION: This snippet creates the 'instance_info' table in the 'public' schema. This table is used to store instance-level information, in this case, the 'run_storage_id'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.instance_info (\n    run_storage_id text\n);\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Flower Interface for Celery Tasks - Helm/Shell\nDESCRIPTION: This snippet allows accessing the Flower interface for Celery task monitoring by setting an environment variable and port forwarding. It is a conditional operation dependent on the Flower service being enabled within the Helm chart. The snippet requires Kubernetes and Helm setup with appropriate permissions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/templates/NOTES.txt#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport FLOWER_POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"dagster.name\" . }},app.kubernetes.io/instance={{ .Release.Name }},component=flower\" -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Visit http://127.0.0.1:5555 to open Flower\"\nkubectl --namespace {{ .Release.Namespace }} port-forward $FLOWER_POD_NAME 5555:5555\n```\n\n----------------------------------------\n\nTITLE: Creating Table: daemon_heartbeats in PostgreSQL\nDESCRIPTION: Defines 'daemon_heartbeats' table for storing heartbeat information of system daemons. Contains columns for daemon type, ID, and timestamp, used for monitoring the health and activity of daemon processes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Branch Deployment with Git State in Shell\nDESCRIPTION: Uses the dagster-cloud CLI to create or update a branch deployment, reading git state automatically from the repository context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/using-branch-deployments-with-the-cli.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nBRANCH_DEPLOYMENT_NAME=$(\n    dagster-cloud branch-deployment create-or-update \\\n        --organization $ORGANIZATION_NAME \\\n        --api-token $DAGSTER_CLOUD_API_TOKEN \\\n        --git-repo-name $REPOSITORY_NAME \\\n        --branch-name $BRANCH_NAME \\\n        --read-git-state\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for Dagster-Airflow Integration\nDESCRIPTION: This requirements specification lists the necessary dependencies for integrating Dagster with Apache Airflow. It pins specific version ranges for each package to ensure compatibility, including Apache Airflow itself, Flask-session, Connexion, Pendulum, and the Dagster-airlift package with the 'in-airflow' extra.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/airlift-mwaa-example/dags/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\napache-airflow>=2.0.0,<2.8\nflask-session<0.6.0\nconnexion<3.0.0\npendulum>=2.0.0,<3.0.0\ndagster-airlift[in-airflow]\n```\n\n----------------------------------------\n\nTITLE: Installing DuckDB Dependencies\nDESCRIPTION: Command to install the required DuckDB integration packages for Dagster\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/using-duckdb-with-dagster.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-duckdb dagster-duckdb-pandas\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dagster Cloud Agent with Custom Values\nDESCRIPTION: Helm command to upgrade the Dagster Cloud agent with custom values from a YAML file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nhelm --namespace dagster-cloud upgrade agent \\\n    dagster-cloud/dagster-cloud-agent \\\n    --values ./values.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating job_ticks Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'job_ticks' table to manage job execution timestamps and statuses. It records necessary information for each job tick, including job identifiers and timestamps for creative and update actions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: job_ticks; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.job_ticks OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project with PySpark EMR Example\nDESCRIPTION: The snippet provides a command to initialize a Dagster project using a predefined example. This command sets up a new Dagster project named 'my-dagster-project' with the PySpark EMR integration example. It requires Dagster CLI tools to be installed and configured.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_pyspark_emr/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example with_pyspark_emr\n```\n\n----------------------------------------\n\nTITLE: Activating Python Virtual Environment\nDESCRIPTION: Sources the activation script from a Python virtual environment located in the .venv directory. This command makes the virtual environment's Python interpreter and installed packages available in the current shell session.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/2-b-uv-venv.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Displaying Directory Tree Structure for Dagster Project\nDESCRIPTION: Tree view representation of a typical Dagster project structure showing configuration files (pyproject.toml, uv.lock), source code organization in src/my_project, test directory setup, and required Python package files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n└── my-project\n    ├── pyproject.toml\n    ├── src\n    │   └── my_project\n    │       ├── __init__.py\n    │       ├── definitions.py\n    │       ├── defs\n    │       │   └── __init__.py\n    │       └── lib\n    │           └── __init__.py\n    ├── tests\n    │   └── __init__.py\n    └── uv.lock\n\n7 directories, 7 files\n```\n\n----------------------------------------\n\nTITLE: Create run_tags Table\nDESCRIPTION: This SQL statement creates the `run_tags` table for storing tags associated with Dagster runs. It includes fields for the run ID, tag key, and tag value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\"\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in React/JSX\nDESCRIPTION: Imports the DocCardList component from the theme and renders it to display a list of child documentation pages related to debugging in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/debugging/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Initializing New Dagster Project in Current Directory\nDESCRIPTION: Uses the Dagster CLI command 'dg init' to create a new Dagster project scaffold in the current directory, denoted by the period\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-e-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg init .\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Definitions with Downstream Asset\nDESCRIPTION: Updated Dagster definitions configuration that includes the new order_count_chart asset alongside existing dbt assets\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/downstream-assets.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstart_defs\\nend_defs\n```\n\n----------------------------------------\n\nTITLE: Creating Index on job_ticks.job_origin_id, status\nDESCRIPTION: This SQL statement creates a composite index on the `job_origin_id` and `status` columns of the `job_ticks` table. This is likely to improve performance for queries filtering by job origin and its status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\"\n```\n\n----------------------------------------\n\nTITLE: Workspace YAML Reference\nDESCRIPTION: Reference to the optional workspace.yaml configuration file that can be used for local Dagster instances but doesn't affect Dagster+ deployment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-requirements.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nworkspace.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Deployment\nDESCRIPTION: Command to start the Dagster deployment using Docker Compose.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/docker.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Asset Check Executions in SQL\nDESCRIPTION: Creates a B-tree index on the asset_check_executions table for improved query performance when searching by asset_key, check_name, materialization_event_storage_id, and partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_check_executions ON public.asset_check_executions USING btree (asset_key, check_name, materialization_event_storage_id, partition);\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster Engine Event Data Structure\nDESCRIPTION: This JSON structure represents an engine event in Dagster, containing metadata about process execution. It includes information like process IDs, step keys being executed, and execution timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80630\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['do_something.compute', 'do_input.compute']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 80630) in 168ms\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - ENGINE_EVENT - Finished steps in process (pid: 80630) in 168ms\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": null, \"timestamp\": 1610466063.712225, \"user_message\": \"Finished steps in process (pid: 80630) in 168ms\"}\n```\n\n----------------------------------------\n\nTITLE: Starting Pipeline Execution in Dagster\nDESCRIPTION: JSON log entry marking the beginning of pipeline execution for 'longitudinal_pipeline', which serves as the initial event in the pipeline's execution lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\"longitudinal_pipeline\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - PIPELINE_START - Started execution of pipeline \\\"longitudinal_pipeline\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608666901.056614, \"user_message\": \"Started execution of pipeline \\\"longitudinal_pipeline\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Database Indexes - PostgreSQL\nDESCRIPTION: SQL statements that create various indexes to optimize query performance on frequently accessed columns like asset_key, run_id, and job_origin_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\n\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n```\n\n----------------------------------------\n\nTITLE: Using Reconstructable Pipelines for Multi-Process Execution\nDESCRIPTION: Example of how to use the new reconstructable helper to enable multi-process execution of pipelines in Dagster 0.8.0.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n@pipeline(...)\ndef some_pipeline():\n    ...\n\nexecute_pipeline(reconstructable(some_pipeline), {'execution': {'multiprocess': {}}})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Context in Notebook\nDESCRIPTION: Example of setting up a development context in a Jupyter notebook for testing and development purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/reference.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n\ncontext = dagstermill.get_context(op_config=3)\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Virtual Environment with UV\nDESCRIPTION: Commands to create a fresh virtual environment using UV and activate it for the tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/setup.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install uv\nuv venv\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Project Structure\nDESCRIPTION: Commands to create the necessary directories and files for the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/quickstart.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir quickstart data\ntouch quickstart/__init__.py quickstart/assets.py\ntouch data/sample_data.csv\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration Package\nDESCRIPTION: Command to install the dagster-aws package which provides AWS ECR integration capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/ecr.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Open Source Deployment Project Structure\nDESCRIPTION: This snippet illustrates the file structure for a Dagster project deployed to infrastructure. It includes the workspace.yaml file for defining multiple code locations in a deployment environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/dagster-project-file-reference.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n.\n├── README.md\n├── my_dagster_project\n│   ├── __init__.py\n│   ├──  assets.py\n│   └──  definitions.py\n├── my_dagster_project_tests\n├── dagster.yaml      ## optional, used for instance settings\n├── pyproject.toml\n├── setup.cfg\n├── setup.py\n├── tox.ini\n└── workspace.yaml    ## defines multiple code locations\n```\n\n----------------------------------------\n\nTITLE: Importing Blueprint CSS Dependencies\nDESCRIPTION: CSS imports required for Blueprint component styling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/ui-components/README.md#2025-04-22_snippet_3\n\nLANGUAGE: css\nCODE:\n```\n@import '@blueprintjs/core/lib/css/blueprint.css';\n@import '@blueprintjs/icons/lib/css/blueprint-icons.css';\n@import '@blueprintjs/select/lib/css/blueprint-select.css';\n@import '@blueprintjs/table/lib/css/table.css';\n@import '@blueprintjs/popover2/lib/css/blueprint-popover2.css';\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Microsoft Teams Integration\nDESCRIPTION: Command to install the dagster-msteams package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/microsoft-teams.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-msteams\n```\n\n----------------------------------------\n\nTITLE: In-Process Execution YAML Config\nDESCRIPTION: YAML configuration for executing jobs in-process using the default executor\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/job-execution.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstart_ip_yaml\nresources:\n  io_manager:\n    config:\n      base_dir: '/tmp'\nexecution:\n  config:\n    in_process: {}\nend_ip_yaml\n```\n\n----------------------------------------\n\nTITLE: Running Airflow for Kitchen Sink Project in Bash\nDESCRIPTION: This command starts the Airflow instance for the Kitchen Sink project. It should be run in a separate shell after the installation and setup steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/dagster-dlift/kitchen-sink/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake run_airflow\n```\n\n----------------------------------------\n\nTITLE: EMR Environment Dockerfile Configuration\nDESCRIPTION: Dockerfile that sets up a custom EMR environment with Python 3.9 and required Dagster dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-containers-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM public.ecr.aws/emr-on-eks/spark/emr-6.9.0:latest\n\nUSER root\n\n# Install Python 3.9\nRUN amazon-linux-extras enable python3.9 && \\\n    yum install -y python39 python39-pip python39-devel && \\\n    alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1 && \\\n    alternatives --set python3 /usr/bin/python3.9\n\n# Install Python dependencies\nRUN python3 -m pip install dagster-pipes dagster-aws boto3\n\n# Copy EMR job script\nCOPY script.py /opt/script.py\n```\n\n----------------------------------------\n\nTITLE: Importing Legacy snowflake_pandas_io_manager in Python\nDESCRIPTION: This snippet shows how to import the legacy snowflake_pandas_io_manager, which is an IOManagerDefinition for backward compatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-snowflake-pandas.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_snowflake_pandas import snowflake_pandas_io_manager\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster, dbt, and DuckDB packages\nDESCRIPTION: This command installs the dagster-dbt package for Dagster integration with dbt, as well as the dbt-duckdb package for using DuckDB as a database backend with dbt. This setup allows for data pipeline orchestration with Dagster, transformation with dbt, and storage/querying with DuckDB.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/18-pip-add-dbt.txt#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install dagster-dbt dbt-duckdb\n```\n\n----------------------------------------\n\nTITLE: Sensor Relationship Diagram in Mermaid\nDESCRIPTION: Mermaid flowchart showing how Sensors connect to Assets, Config, Jobs and Definitions, with custom theme configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_15\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Config fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Job fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Sensor(Sensor)\n\n    Asset -.-> Sensor\n    Config -.-> Sensor\n    Job -.-> Sensor\n    Sensor ==> Definitions\n```\n\n----------------------------------------\n\nTITLE: Running Dagster at Different Migration Stages\nDESCRIPTION: These commands allow running Dagster at different stages of the Airflow to Dagster migration process: peer, observe, or migrate. Note that for the observation step, 'proxied' must be set to 'False' for each task in the DAG configuration files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/starlift-demo/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake run_peer\nmake run_observe\nmake run_migrate\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Twilio Package\nDESCRIPTION: Command to install the dagster-twilio integration package via pip\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/twilio.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-twilio\n```\n\n----------------------------------------\n\nTITLE: External Secret Configuration\nDESCRIPTION: YAML configuration for using externally managed secrets with PostgreSQL in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/customizing-your-deployment.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nglobal:\n  postgresqlSecretName: 'dagster-postgresql-secret'\n\ngeneratePostgresqlPasswordSecret: false\n```\n\n----------------------------------------\n\nTITLE: Moving ELT Directory in Dagster Project\nDESCRIPTION: This command moves all contents from the 'elt' directory to a new 'defs/elt' directory within the existing project structure. It's likely part of a project restructuring or organization process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-definitions/3-mv.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmv my_existing_project/elt/* my_existing_project/defs/elt\n```\n\n----------------------------------------\n\nTITLE: Logging Step Input in Dagster Pipeline (JSON)\nDESCRIPTION: Log entry for a step input event in a Dagster pipeline. It records receiving an input of type 'Any' for the 'persist_traffic' solid.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepInputData\",\n      \"input_name\": \"_\",\n      \"type_check_data\": {\n        \"__class__\": \"TypeCheckData\",\n        \"description\": null,\n        \"label\": \"_\",\n        \"metadata_entries\": [],\n        \"success\": true\n      }\n    },\n    \"event_type_value\": \"STEP_INPUT\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"persist_traffic\",\n      \"solid_definition\": \"base_one_input\",\n      \"step_key\": \"persist_traffic\"\n    },\n    \"message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\",\n    \"pid\": 18688,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"persist_traffic\",\n      \"parent\": null\n    },\n    \"step_key\": \"persist_traffic\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_traffic - STEP_INPUT - Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\",\n  \"step_key\": \"persist_traffic\",\n  \"timestamp\": 1608666924.481046,\n  \"user_message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating alembic_version Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'alembic_version' table to track database migrations. It includes a version number which is a non-null character varying type. This table is critical for the version control of the database schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\nALTER TABLE public.alembic_version OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Visualizing Asset Check Relationships in Dagster\nDESCRIPTION: This mermaid diagram illustrates the relationships between Asset Checks and other Dagster concepts. It shows how Asset Checks are associated with Assets and how they are included in Definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    AssetCheck(Asset Check)\n\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    AssetCheck -.-> Asset\n    AssetCheck ==> Definitions\n```\n\n----------------------------------------\n\nTITLE: Creating Op Jobs from a Graph in Python\nDESCRIPTION: Demonstrates how to create multiple op jobs from a single graph using the to_job() method, allowing for environment-specific customization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/op-jobs.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndev_job = my_graph.to_job(name=\"dev_job\", config={\"ops\": {\"add_one\": {\"config\": {\"num\": 2}}}})\nprod_job = my_graph.to_job(name=\"prod_job\", config={\"ops\": {\"add_one\": {\"config\": {\"num\": 4}}}})\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Speculative Execution\nDESCRIPTION: Parameters that control speculative execution of tasks, which relaunches tasks that are running slower than expected.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_42\n\nLANGUAGE: properties\nCODE:\n```\nspark.speculation\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.speculation.interval\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.speculation.multiplier\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.speculation.quantile\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Templating in component.yaml\nDESCRIPTION: Example of using Jinja2 templating to include environment variables in a component.yaml file. This approach helps keep sensitive information like credentials out of your configuration files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/adding-components.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncomponent_type: my_snowflake_component\n\nattributes:\n  account: \"{{ env('SNOWFLAKE_ACCOUNT') }}\"\n  password: \"{{ env('SNOWFLAKE_PASSWORD') }}\"\n```\n\n----------------------------------------\n\nTITLE: Including Dagster CI/CD Template in GitLab (YAML)\nDESCRIPTION: This snippet demonstrates how to include a Dagster-provided CI/CD template in a GitLab CI/CD configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ninclude:\n  - https://dagster.io/path/to/serverless/gitlab-ci-template.yml\n```\n\n----------------------------------------\n\nTITLE: EMR Configuration Utility\nDESCRIPTION: Utility function for configuring EMR settings in Dagster Pipes implementation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/migrating-from-step-launchers-to-pipes.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_emr_config(job_script_s3_path: str, extras: dict | None = None) -> dict:\n    return {\n        \"Name\": \"My EMR job\",\n        \"ReleaseLabel\": \"emr-6.10.0\",\n        \"Applications\": [{\"Name\": \"Spark\"}],\n        \"LogUri\": \"s3://my-bucket/emr-logs\",\n        \"Instances\": {\n            \"InstanceGroups\": [\n                {\n                    \"Name\": \"Primary node\",\n                    \"Market\": \"ON_DEMAND\",\n                    \"InstanceRole\": \"MASTER\",\n                    \"InstanceType\": \"m5.xlarge\",\n                    \"InstanceCount\": 1,\n                }\n            ],\n            \"KeepJobFlowAliveWhenNoSteps\": False,\n            \"TerminationProtected\": False,\n        },\n        \"Steps\": [\n            {\n                \"Name\": \"Spark application\",\n                \"ActionOnFailure\": \"TERMINATE_CLUSTER\",\n                \"HadoopJarStep\": {\n                    \"Jar\": \"command-runner.jar\",\n                    \"Args\": [\n                        \"spark-submit\",\n                        \"--py-files\",\n                        \"s3://my-bucket/pyspark-deps.zip\",\n                        job_script_s3_path,\n                        *([\"--extras\", json.dumps(extras)] if extras else []),\n                    ],\n                },\n            }\n        ],\n        \"JobFlowRole\": \"EMR_EC2_DefaultRole\",\n        \"ServiceRole\": \"EMR_DefaultRole\",\n    }\n```\n\n----------------------------------------\n\nTITLE: Directory Tree Structure for Dagster Jaffle Platform\nDESCRIPTION: This tree command output shows the directory structure of a Dagster project named 'jaffle_platform'. The structure includes Python module initialization files, definitions, and configuration files in YAML format for data ingestion and replication components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/11-tree-jaffle-platform.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsrc/jaffle_platform\n├── __init__.py\n├── definitions.py\n├── defs\n│   ├── __init__.py\n│   └── ingest_files\n│       ├── component.yaml\n│       └── replication.yaml\n└── lib\n    └── __init__.py\n\n4 directories, 6 files\n```\n\n----------------------------------------\n\nTITLE: Filtering Airflow DAG Assets in Python\nDESCRIPTION: Code to filter the assets to only include the 'load_customers' DAG from the 'warehouse' Airflow instance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nload_customers_asset = next(\n    asset for asset in all_warehouse_assets if asset.key == [\"warehouse\", \"load_customers\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Pandas Type Handler Class Documentation\nDESCRIPTION: ReStructuredText directive for documenting the DeltaLakePandasTypeHandler class\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake-pandas.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: DeltaLakePandasTypeHandler\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables and Starting Dagster Dev Server\nDESCRIPTION: Bash commands to set up environment variables and start the Dagster development server, pointing to the created definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/observe.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport TUTORIAL_EXAMPLE_DIR=$(pwd)\nexport DAGSTER_HOME=\"$TUTORIAL_EXAMPLE_DIR/.dagster_home\"\ndagster dev -f airlift_federation_tutorial/dagster_defs/definitions.py\n```\n\n----------------------------------------\n\nTITLE: Logging Information Using Context\nDESCRIPTION: This snippet utilizes the logging functionality of the Dagstermill context to log a sample message for debugging or informational purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource_with_exception.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontext.log.info(\"Test logging information\")\n```\n\n----------------------------------------\n\nTITLE: Importing Base Storage Module in Python\nDESCRIPTION: Imports the base storage module from Dagster's core storage package. This module likely contains base classes for various storage implementations in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.base_storage\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project Directory Structure\nDESCRIPTION: This code snippet shows the output of running the 'tree' command on the 'my_project/defs' directory. It reveals the hierarchical structure of the project, including Python files and a YAML configuration file organized into team-specific subdirectories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/adding-attributes-to-assets/3-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_project/defs\n├── __init__.py\n├── team_a\n│   ├── b.py\n│   ├── component.yaml\n│   └── subproject\n│       └── a.py\n└── team_b\n    └── c.py\n\n4 directories, 5 files\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Dependencies\nDESCRIPTION: Command to install Dagster and required development dependencies from the current directory using the uv package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Dagster CLI Command with Click in reStructuredText\nDESCRIPTION: This snippet shows how to document a Dagster CLI command named 'greet' using reStructuredText directives. It utilizes the Click library's documentation features to generate CLI documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/_ext/sphinx-click/tests/roots/basics/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. click:: greet:greet\n   :prog: greet\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Snapshots in PostgreSQL\nDESCRIPTION: Adds a primary key constraint on the 'id' column of the 'snapshots' table to ensure unique records are maintained.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon ECS Agent in dagster_cloud.yaml\nDESCRIPTION: Example configuration for Dagster+ with Amazon ECS agent showing all available container context fields including environment variables, secrets, resources, roles, volumes, and sidecar containers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/amazon-ecs/configuration-reference.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      ecs:\n        env_vars:\n          - DATABASE_NAME=staging\n          - DATABASE_PASSWORD\n        secrets:\n          - name: 'MY_API_TOKEN'\n            valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:token::'\n          - name: 'MY_PASSWORD'\n            valueFrom: 'arn:aws:secretsmanager:us-east-1:123456789012:secret:FOO-AbCdEf:password::'\n        secrets_tags:\n          - 'my_tag_name'\n        server_resources: # Resources for code servers launched by the agent for this location\n          cpu: 256\n          memory: 512\n          replica_count: 1\n        run_resources: # Resources for runs launched by the agent for this location\n          cpu: 4096\n          memory: 16384\n        execution_role_arn: arn:aws:iam::123456789012:role/MyECSExecutionRole\n        task_role_arn: arn:aws:iam::123456789012:role/MyECSTaskRole\n        mount_points:\n          - sourceVolume: myEfsVolume\n            containerPath: '/mount/efs'\n            readOnly: True\n        volumes:\n          - name: myEfsVolume\n            efsVolumeConfiguration:\n              fileSystemId: fs-1234\n              rootDirectory: /path/to/my/data\n        server_sidecar_containers:\n          - name: DatadogAgent\n            image: public.ecr.aws/datadog/agent:latest\n            environment:\n              - name: ECS_FARGATE\n                value: true\n        run_sidecar_containers:\n          - name: DatadogAgent\n            image: public.ecr.aws/datadog/agent:latest\n            environment:\n              - name: ECS_FARGATE\n                value: true\n        server_ecs_tags:\n          - key: MyEcsTagKey\n            value: MyEcsTagValue\n        run_ecs_tags:\n          - key: MyEcsTagKeyWithoutValue\n        repository_credentials: MyRepositoryCredentialsSecretArn\n```\n\n----------------------------------------\n\nTITLE: Creating Branch Deployment with Git Repository Context\nDESCRIPTION: This snippet demonstrates how to create or update a branch deployment using metadata from the git repository context. It uses the --read-git-state flag to automatically populate commit hash, timestamp, commit message, and author details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nBRANCH_DEPLOYMENT_NAME=$(\n    dagster-cloud branch-deployment create-or-update \\\n        --organization $ORGANIZATION_NAME \\\n        --api-token $DAGSTER_CLOUD_API_TOKEN \\\n        --git-repo-name $REPOSITORY_NAME \\\n        --branch-name $BRANCH_NAME \\\n        --read-git-state # Equivalent to passing --commit-hash, --timestamp\n                        # --commit-message, --author-name, --author-email\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Asset Event Tags in SQL\nDESCRIPTION: Creates a B-tree index on the asset_event_tags table to optimize queries filtering by asset_key, key, and value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_event_tags ON public.asset_event_tags USING btree (asset_key, key, value);\n```\n\n----------------------------------------\n\nTITLE: Setting Default Deployment for Dagster-Cloud CLI\nDESCRIPTION: Changes the default deployment for the dagster-cloud CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-cloud-cli/installing-and-configuring.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud config set-deployment <deployment_name>\n```\n\n----------------------------------------\n\nTITLE: Getting Dagster Context\nDESCRIPTION: This code retrieves the Dagster execution context within the notebook. The context object provides access to configuration, resources, and other information relevant to the Dagster op execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_config_struct.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncontext = dagstermill.get_context()\n```\n\n----------------------------------------\n\nTITLE: Displaying First Rows of DataFrame\nDESCRIPTION: This snippet displays the first few rows of the DataFrame, which helps in understanding the structure and contents of the data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Settings with pyproject.toml\nDESCRIPTION: The pyproject.toml file for a project within a Dagster workspace that sets up the project configuration, including its directory type, name, and Python environment management using uv.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/multiple-projects.md#2025-04-22_snippet_1\n\nLANGUAGE: TOML\nCODE:\n```\n[build-system]\nrequires = [\"setuptools>=42.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.dg]\ndirectory_type = \"project\"\nname = \"project-1\"\n\n[tool.dg.project]\npython_environment = { uv_managed = true }\n\n```\n\n----------------------------------------\n\nTITLE: Creating Snapshots Table in PostgreSQL\nDESCRIPTION: Defines a table for storing snapshots with unique identifiers, snapshot body, and type in the public schema\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Waiting for Browser Element Visibility\nDESCRIPTION: Example showing how to use WebDriverWait to wait until a specific element is visible on the page, which helps ensure tests don't fail due to timing issues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/cli_test_scaffold.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nWebDriverWait(browser, 10).until(\n    lambda _: find_element(browser, selector=\"some selector\").is_displayed()\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Tutorial Example Code\nDESCRIPTION: Command to install the Airlift migration tutorial example code using Dagster's project command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/setup.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name airlift-migration-tutorial --example airlift-migration-tutorial\n```\n\n----------------------------------------\n\nTITLE: Configuring Code Locations in dagster_cloud.yaml for Multiple Projects\nDESCRIPTION: This YAML configuration defines multiple code locations in a dagster_cloud.yaml file, with each location representing a separate project within the repository. The configuration specifies the package name for each project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/multi-tenancy.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster_cloud.yml\n\nlocations:\n  - location_name: project_a\n    code_source:\n      package_name: project_a\n    build:\n      # ...\n  - location_name: project_b\n    code_source:\n      package_name: project_b\n    build:\n      # ...\n```\n\n----------------------------------------\n\nTITLE: Generating GraphQL Types for Dagster UI\nDESCRIPTION: Command to generate new TypeScript types for GraphQL queries and fragments in the Dagster UI project. This should be run when making changes to GraphQL queries or fragments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/README.md#2025-04-22_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nmake generate-graphl\n```\n\n----------------------------------------\n\nTITLE: Creating and Altering Event Logs Table\nDESCRIPTION: Defines the 'event_logs' table in the PostgreSQL database, specifying columns for logging pipeline events. Alters the ownership of the table to user 'test'. This setup is crucial for event tracking and debugging within Dagster operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key character varying\n);\n\n\nALTER TABLE public.event_logs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Asset Materialization Event in Dagster Pipeline\nDESCRIPTION: Event record showing materialization of a traffic dashboard asset with URL metadata. The event captures step materialization in the longitudinal_pipeline with associated metadata and logging information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_59\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"dashboards\", \"traffic_dashboard\"]}, \"description\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"UrlMetadataEntryData\", \"url\": \"http://docs.dagster.io/traffic\"}, \"label\": \"build_traffic_dashboard\"}], \"partition\": \"2020-12-08\"}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_traffic_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_traffic_dashboard\"}, \"message\": \"Materialized value dashboards traffic_dashboard.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_traffic_dashboard\", \"parent\": null}, \"step_key\": \"build_traffic_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_traffic_dashboard - STEP_MATERIALIZATION - Materialized value dashboards traffic_dashboard.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_traffic_dashboard\", \"timestamp\": 1608666999.3111029, \"user_message\": \"Materialized value dashboards traffic_dashboard.\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Package in Development Mode using pip\nDESCRIPTION: Installs the current package in editable mode (-e flag) which allows for development changes to be immediately reflected without reinstallation. The '.' specifies the current directory as the package location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-f-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Running TypeScript Checks in Dagster UI\nDESCRIPTION: Command to run TypeScript checks across all packages in the Dagster UI project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmake ts\n```\n\n----------------------------------------\n\nTITLE: Defining Clickable UI Elements with WebDriverWait\nDESCRIPTION: This utility function creates a condition to check if UI elements are clickable, which is useful for handling cases when elements appear but are not yet ready for interaction.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/cli_test_scaffold.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef element_to_be_clickable(locator):\n    def condition(browser):\n        element = locator(browser)\n        if not element:\n            return False\n\n        return element.is_displayed() and element.is_enabled()\n\n    return condition\n```\n\n----------------------------------------\n\nTITLE: Alter normalized_cereals_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `normalized_cereals_id_seq` sequence to the `id` column of the `normalized_cereals` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.normalized_cereals_id_seq OWNED BY public.normalized_cereals.id;\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MDX Documentation Page with DocCardList\nDESCRIPTION: Sets up a documentation page with frontmatter configuration and renders a DocCardList component. The sidebar is hidden via a class name setting.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/index.md#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\n---\ntitle: 'Testing assets'\nsidebar_class_name: hidden\n---\n\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Jobs for Dagster Execution\nDESCRIPTION: This bash command lists the Kubernetes jobs created by Dagster, showing completions, duration, and age of step and run workers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/kubernetes-and-celery.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get jobs\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraint to daemon_heartbeats Table in SQL\nDESCRIPTION: Adds a unique constraint on the daemon_type column of the daemon_heartbeats table to ensure each daemon type is unique.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n```\n\n----------------------------------------\n\nTITLE: Creating Index on job_origin_id Column in job_ticks Table\nDESCRIPTION: SQL command to create an index on the job_origin_id column in the job_ticks table to optimize job origin lookups.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Logging Engine Event in Dagster Pipeline (JSON)\nDESCRIPTION: This snippet shows a log entry for an engine event in a Dagster pipeline execution. It includes metadata about the process ID and step keys being executed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"EngineEventData\",\n      \"error\": null,\n      \"marker_end\": null,\n      \"marker_start\": null,\n      \"metadata_entries\": [\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": null,\n          \"entry_data\": {\n            \"__class__\": \"TextMetadataEntryData\",\n            \"text\": \"1295\"\n          },\n          \"label\": \"pid\"\n        },\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": null,\n          \"entry_data\": {\n            \"__class__\": \"TextMetadataEntryData\",\n            \"text\": \"['materialization_solid']\"\n          },\n          \"label\": \"step_keys\"\n        }\n      ]\n    },\n    \"event_type_value\": \"ENGINE_EVENT\",\n    \"logging_tags\": {},\n    \"message\": \"Finished steps in process (pid: 1295) in 90ms\",\n    \"pid\": 1295,\n    \"pipeline_name\": \"model_pipeline\",\n    \"solid_handle\": null,\n    \"step_handle\": null,\n    \"step_key\": null,\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"model_pipeline - 1399fa66-f129-46ad-9cf9-2d528d0f87fa - 1295 - ENGINE_EVENT - Finished steps in process (pid: 1295) in 90ms\",\n  \"pipeline_name\": \"model_pipeline\",\n  \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\",\n  \"step_key\": null,\n  \"timestamp\": 1625760608.375371,\n  \"user_message\": \"Finished steps in process (pid: 1295) in 90ms\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring User Code Server TTL in YAML\nDESCRIPTION: This snippet demonstrates how to enable and configure time-to-live (TTL) for user code servers in Dagster+. It allows setting different TTL values for full and branch deployments, which can help save compute costs by spinning down idle servers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/settings/customizing-agent-settings.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster.yaml\ninstance_class:\n  module: dagster_cloud.instance\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token:\n    env: DAGSTER_CLOUD_AGENT_TOKEN\n  deployment: prod\n\nuser_code_launcher:\n  module: dagster_cloud.workspace.docker\n  class: DockerUserCodeLauncher\n  config:\n    server_ttl:\n      full_deployments:\n        enabled: true # Disabled by default for full deployments\n        ttl_seconds: 7200 # 2 hours\n      branch_deployments:\n        ttl_seconds: 3600 # 1 hour\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Success Event for Raw File Fans Step\nDESCRIPTION: JSON representation of a DagsterEventRecord for a STEP_SUCCESS event. This record shows the successful completion of the 'raw_file_fans.compute' step, which took 41ms to execute.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 41.13163199999992}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_fans\", \"solid_definition\": \"raw_file_fans\", \"step_key\": \"raw_file_fans.compute\"}, \"message\": \"Finished execution of step \\\"raw_file_fans.compute\\\" in 41ms.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_fans\", \"name\": \"raw_file_fans\", \"parent\": null}, \"step_key\": \"raw_file_fans.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_SUCCESS - Finished execution of step \\\"raw_file_fans.compute\\\" in 41ms.\\n event_specific_data = {\\\"duration_ms\\\": 41.13163199999992}\\n               solid = \\\"raw_file_fans\\\"\\n    solid_definition = \\\"raw_file_fans\\\"\\n            step_key = \\\"raw_file_fans.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_fans.compute\", \"timestamp\": 1576110682.961042, \"user_message\": \"Finished execution of step \\\"raw_file_fans.compute\\\" in 41ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Step Input Handling in Dagster with JSON\nDESCRIPTION: This snippet logs a STEP_INPUT event, recording the receipt and successful type-check of an input for a Dagster pipeline step. Useful for ensuring data integrity prior to computation, it requires the correct configuration of Dagster types and solids.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"_\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"_\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"train_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"train_model\"}, \"message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"train_model\", \"parent\": null}, \"step_key\": \"train_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - train_model - STEP_INPUT - Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"train_model\", \"timestamp\": 1608666999.36916, \"user_message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Including Data Files in setup.py for Dagster+ Serverless\nDESCRIPTION: Example setup.py configuration showing how to include data files in a Dagster+ Serverless deployment using the package_data parameter to specify which data files should be included.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"quickstart_etl\",\n    packages=find_packages(exclude=[\"quickstart_etl_tests\"]),\n    install_requires=[\n        \"dagster\",\n        \"dagster-aws\",\n        \"pandas\",\n        \"matplotlib\",\n    ],\n    extras_require={\"dev\": [\"dagit\", \"pytest\"]},\n    package_data={\n        \"quickstart_etl\": [\"data/*\"],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Explaining Root and Sink Asset Selection Functions in Markdown\nDESCRIPTION: This markdown table describes the usage of 'roots()' and 'sinks()' functions in Dagster for selecting root and sink assets. It provides examples of how these functions can be used with various asset selections.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/reference.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Example | Description |\n|--------|--------------||\n `roots(*)` | Select all assets without any upstream dependencies. |\n| `sinks(*)` | Select all assets without any downstream dependencies. |\n| `roots(group:\"public_data\")` | Selects root assets within the `public_data` group.|\n| `sinks(group:\"public_data\")` | Selects sink assets within the `public_data` group. |\n| `roots(+group:\"public_data\")` | Selects all root assets that feed into the `public_data` group. |\n| `sinks(group:\"public_data\"+)` | Selects all sink assets that depend on assets in the `public_data` group. |\n```\n\n----------------------------------------\n\nTITLE: Creating public.snapshots Table and Sequence\nDESCRIPTION: This snippet creates the 'snapshots' table in the 'public' schema for storing snapshots of Dagster objects.  It includes columns for ID, snapshot ID, snapshot body (as a byte array), and snapshot type. An associated sequence 'snapshots_id_seq' is also created to automatically generate unique IDs for each snapshot.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n\n\nALTER TABLE public.snapshots OWNER TO test;\n\n--\n-- Name: snapshots_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.snapshots_id_seq OWNER TO test;\n\n--\n-- Name: snapshots_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\"\n```\n\n----------------------------------------\n\nTITLE: Querying Popular Shelves Data in SQL\nDESCRIPTION: SQL query to display popular shelves data from the graphic_novels table, showing how users tag books in the Goodreads dataset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/feature-engineering.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nselect popular_shelves from graphic_novels limit 5;\n```\n\n----------------------------------------\n\nTITLE: Documentation Template for Jaffle Shop dbt Project\nDESCRIPTION: A documentation block that describes the Jaffle Shop fictional ecommerce store and explains that it's a dbt test project with a link to the source repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_asset_checks/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Creating Index on runs.status, update_timestamp, create_timestamp\nDESCRIPTION: This SQL statement creates a composite index on the `status`, `update_timestamp`, and `create_timestamp` columns of the `runs` table.  This is intended to optimize queries that involve filtering or sorting based on run status and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_run_range ON public.runs USING btree (status, update_timestamp, create_timestamp);\"\n```\n\n----------------------------------------\n\nTITLE: Installing DuckDB PySpark Integration\nDESCRIPTION: Command to install the DuckDB PySpark integration package.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/duckdb/reference.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-duckdb-pyspark\n```\n\n----------------------------------------\n\nTITLE: Visualizing Code Location Relationships in Dagster\nDESCRIPTION: This mermaid diagram shows the relationship between Code Locations and Definitions in Dagster. It illustrates how Definitions are contained within a Code Location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    CodeLocation(Code Location)\n\n    Definitions ==> CodeLocation\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Submit with Custom Properties\nDESCRIPTION: Shows how to launch a Spark application using spark-submit with custom configurations including executor options and Hadoop properties. Demonstrates runtime configuration modification.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_54\n\nLANGUAGE: bash\nCODE:\n```\n./bin/spark-submit \\\n--name \"My app\" \\\n--master local[4] \\\n --conf spark.eventLog.enabled=false \\\n--conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" \\\n--conf spark.hadoop.abc.def=xyz \\\nmyApp.jar\n```\n\n----------------------------------------\n\nTITLE: Pipeline Success Event in Dagster\nDESCRIPTION: JSON log event marking the successful completion of a Dagster pipeline execution. This event signifies that the longitudinal_pipeline has finished without errors.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of pipeline \\\"longitudinal_pipeline\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"longitudinal_pipeline\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608667064.387896, \"user_message\": \"Finished execution of pipeline \\\"longitudinal_pipeline\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Table: run_tags in PostgreSQL\nDESCRIPTION: Defines 'run_tags' table for associating tags with execution runs. Includes essential columns like run ID, tag key, and value, facilitating run categorization and filtering based on tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\nALTER TABLE public.run_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Shallow Clone DBT Repository\nDESCRIPTION: Command to perform a shallow clone of the jaffle-platform repository and remove its git history. Uses depth=1 for minimal git history and renames the target directory to 'dbt'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/17-jaffle-clone.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --depth=1 https://github.com/dagster-io/jaffle-platform.git dbt && rm -rf dbt/.git\n```\n\n----------------------------------------\n\nTITLE: Setting In-Memory Buffer Size for Shuffle Outputs - Shuffle Settings\nDESCRIPTION: This property sets the size of the in-memory buffer for each shuffle file output stream, reducing the number of disk accesses when creating intermediate files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_17\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.file.buffer\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Job Ticks by Timestamp in SQL\nDESCRIPTION: Creates a B-tree index on the job_ticks table to optimize queries filtering by job_origin_id and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n```\n\n----------------------------------------\n\nTITLE: Creating Repository Using Definitions Arguments\nDESCRIPTION: Function to create a repository using definition arguments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/definitions.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncreate_repository_using_definitions_args\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Test Images to ECR for K8s Tests\nDESCRIPTION: Shell commands to build, tag, and push Dagster test images to an AWS ECR repository for use with external Kubernetes clusters during testing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n./python_modules/dagster-test/dagster_test/test_project/build.sh 3.7.6\ndocker tag dagster-docker-buildkite:latest $AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/dagster-k8s-tests:2020-04-21T21-04-06\n\naws ecr get-login --no-include-email --region us-west-1 | sh\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.us-west-1.amazonaws.com/dagster-k8s-tests:2020-04-21T21-04-06\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with pip\nDESCRIPTION: This command installs the project dependencies using pip. The -e flag installs the package in editable mode, and the [dev] specifier includes development dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/google_drive_factory/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-airlift and related packages\nDESCRIPTION: Commands to activate a virtual environment and install the necessary Dagster packages, including dagster-airlift, for migrating from Airflow.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/peer.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsource .venv/bin/activate\nuv pip install 'dagster-airlift[core]' dagster-webserver dagster\n```\n\n----------------------------------------\n\nTITLE: Creating asset_keys Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'asset_keys' table used to store asset keys with an associated identifier and timestamp. The table consists of three columns: 'id', 'asset_key', and 'create_timestamp', with 'id' being a non-null integer.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: asset_keys; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.asset_keys OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with pip\nDESCRIPTION: Command to install the project dependencies including development packages using pip install with the -e flag for editable mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_llm_fine_tune/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Command-Line Completions for Dagster-Cloud CLI\nDESCRIPTION: Installs command-line completions to make using the dagster-cloud CLI easier.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-cloud-cli/installing-and-configuring.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud --install-completion\n```\n\n----------------------------------------\n\nTITLE: Importing Asset Mapping Functions in Python\nDESCRIPTION: Imports functions for mapping Dagster assets to Airflow tasks and DAGs. These functions allow for flexible integration between Dagster and Airflow workflows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-airlift.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_airlift.core import assets_with_task_mappings, assets_with_dag_mappings, assets_with_multiple_task_mappings\n```\n\n----------------------------------------\n\nTITLE: Viewing Dagster Project Structure\nDESCRIPTION: Output of the 'tree' command showing the file structure of a newly scaffolded Dagster project with components. This illustrates the standard Python project layout created by the scaffold command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/creating-a-project-with-components.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd jaffle-platform\n$ tree -a -I \"__pycache__|.git\"\n.\n├── .dg\n│   └── env\n├── pyproject.toml\n├── src\n│   └── jaffle_platform\n│       └── __init__.py\n├── tests\n│   └── __init__.py\n└── uv.lock\n\n5 directories, 4 files\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Evidence Integration Package via pip\nDESCRIPTION: Command to install the dagster-evidence Python package using pip. This package enables integration between Dagster workflows and the Evidence analytics platform.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/27-pip-add-evidence.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-evidence\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgresEventLogStorage in Dagster\nDESCRIPTION: Autoconfigurable class for PostgreSQL event log storage in Dagster. It provides configuration options for connecting to and using a PostgreSQL database for storing event logs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-postgres.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoconfigurable:: PostgresEventLogStorage\n```\n\n----------------------------------------\n\nTITLE: Creating a New Dagster Project from Example Template\nDESCRIPTION: Commands to install Dagster and create a new project based on the quickstart_etl example. This initializes a Dagster project locally that can be deployed to Dagster+.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/ci-cd-in-serverless.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster\ndagster project from-example \\\n  --name my-dagster-project \\\n  --example quickstart_etl\n```\n\n----------------------------------------\n\nTITLE: Waiting for Page Redirect with WebDriverWait\nDESCRIPTION: Demonstrates how to wait for a redirect to a specific URL pattern in a Selenium test, which is important for ensuring a page has fully navigated before proceeding with subsequent test steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/cli_test_scaffold.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nWebDriverWait(browser, timeout=10).until(\n    lambda browser: re.search(r\"/instance.*\", browser.current_url)\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Options in Dagster Documentation\nDESCRIPTION: This Jinja template block defines the possible status values for orders in a Dagster project. It contains a markdown table with five possible statuses (placed, shipped, completed, return_pending, returned) and provides detailed descriptions for each status value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/jaffle_shop/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in JSX\nDESCRIPTION: This code snippet imports the DocCardList component from the '@theme/DocCardList' module and renders it within JSX. The DocCardList component is likely used to display a list of documentation cards for Dagster libraries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/api/libraries/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster Development Server\nDESCRIPTION: Command to start the Dagster development webserver for monitoring and interacting with the pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/index.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Importing Errors Module in Python\nDESCRIPTION: Imports the errors module from Dagster's core package. This module contains functions for handling exceptions and errors in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.errors\n```\n\n----------------------------------------\n\nTITLE: Creating Asset Partition Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the asset_key and partition columns of the event_logs table to optimize queries that filter by these columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dagster-Cloud CLI Configuration\nDESCRIPTION: Initiates the setup process for the dagster-cloud CLI configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-cloud-cli/installing-and-configuring.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud config setup\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Run Tags Table\nDESCRIPTION: Sets up the 'run_tags' table to hold metadata associated with pipeline runs. The table is owned by user 'test', supporting flexibility and decentralized management of tags linked to different runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key character varying,\n    value character varying\n);\n\n\nALTER TABLE public.run_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Recording Asset Materialization in Dagster Pipeline (JSON)\nDESCRIPTION: JSON log entry for asset materialization in a Dagster pipeline. It includes details about the materialized asset, such as its key and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepMaterializationData\",\n      \"asset_lineage\": [],\n      \"materialization\": {\n        \"__class__\": \"AssetMaterialization\",\n        \"asset_key\": {\n          \"__class__\": \"AssetKey\",\n          \"path\": [\"model\"]\n        },\n        \"description\": null,\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"FloatMetadataEntryData\",\n              \"value\": 1625760608.337679\n            },\n            \"label\": \"timestamp\"\n          }\n        ],\n        \"partition\": null,\n        \"tags\": {}\n      }\n    },\n    \"event_type_value\": \"ASSET_MATERIALIZATION\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuf74gv30\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"model_pipeline\",\n      \"solid\": \"materialization_solid\",\n      \"step_key\": \"materialization_solid\"\n    },\n    \"message\": \"Materialized value model.\",\n    \"pid\": 1295,\n    \"pipeline_name\": \"model_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"materialization_solid\",\n      \"parent\": null\n    },\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"materialization_solid\",\n        \"parent\": null\n      }\n    },\n    \"step_key\": \"materialization_solid\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"model_pipeline - 1399fa66-f129-46ad-9cf9-2d528d0f87fa - 1295 - materialization_solid - ASSET_MATERIALIZATION - Materialized value model.\",\n  \"pipeline_name\": \"model_pipeline\",\n  \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\",\n  \"step_key\": \"materialization_solid\",\n  \"timestamp\": 1625760608.338121,\n  \"user_message\": \"Materialized value model.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Context with Dagstermill in Python\nDESCRIPTION: This snippet retrieves the execution context using the Dagstermill library's get_context method. The context is used to manage state and facilitate operations such as logging within a Dagster pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_logging.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncontext = dagstermill.get_context()\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment with pip\nDESCRIPTION: Commands to create and activate a virtual environment using Python's venv module, then install the project package as an editable installation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npip install --editable .\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project Dependencies\nDESCRIPTION: Command to install the Dagster project as a Python package in editable mode. This allows for local code changes to be automatically applied during development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_etl/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: SQL Pipeline and Execution Plan Storage\nDESCRIPTION: SQL code showing compressed binary data storage for pipeline and execution plan snapshots in the Dagster database. The data is stored in a hexadecimal encoded binary format with IDs for reference.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n1\t1deccee186d4c5a6c7e9d4859e07c550c2c2dd72\t\\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\n```\n\n----------------------------------------\n\nTITLE: Yielding Events in Dagstermill Notebook Python\nDESCRIPTION: This snippet demonstrates how to yield an 'AssetMaterialization' event from within a Dagstermill notebook using the Dagster library. It requires the 'dagstermill' module and the 'dagster' library to define and yield asset events. The purpose is to mark significant occurrences within the notebook, such as data processing completions or key transformations, using 'yield_event'. This snippet assumes that Dagster and Dagstermill are installed and configured.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/yield_materialization.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\\nfrom dagster import AssetMaterialization\\n\\ndagstermill.yield_event(\\n    AssetMaterialization(\\n        asset_key=\\\"notebook_yielded_asset\\\",\\n    )\\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Example Dependencies\nDESCRIPTION: Command to install the Python dependencies for the example project with development extras. This uses pip to install the project in editable mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Testing Dagster Sensor via CLI\nDESCRIPTION: Command to preview a sensor's evaluation results using the Dagster CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/testing-sensors.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster sensor preview my_sensor_name\n```\n\n----------------------------------------\n\nTITLE: Adding PRIMARY KEY Constraint for alembic_version Table in PostgreSQL\nDESCRIPTION: This command adds a primary key constraint to the column 'version_num' in the 'alembic_version' table to ensure data integrity and uniqueness.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.alembic_version ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n```\n\n----------------------------------------\n\nTITLE: Recording Object Store Operation in Dagster Pipeline\nDESCRIPTION: Logs an object store operation event for retrieving an intermediate object for the 'persist_model' step. The object is retrieved from the filesystem object store using pickle serialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"ObjectStoreOperationResultData\",\n      \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/train_model/result\",\n      \"metadata_entries\": [\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": null,\n          \"entry_data\": {\n            \"__class__\": \"PathMetadataEntryData\",\n            \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/train_model/result\"\n          },\n          \"label\": \"key\"\n        }\n      ],\n      \"op\": \"GET_OBJECT\",\n      \"value_name\": \"_\",\n      \"version\": null\n    },\n    \"event_type_value\": \"OBJECT_STORE_OPERATION\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"persist_model\",\n      \"solid_definition\": \"base_one_input\",\n      \"step_key\": \"persist_model\"\n    },\n    \"message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\",\n    \"pid\": 18688,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"persist_model\",\n      \"parent\": null\n    },\n    \"step_key\": \"persist_model\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_model - OBJECT_STORE_OPERATION - Retrieved intermediate object for input _ in filesystem object store using pickle.\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\",\n  \"step_key\": \"persist_model\",\n  \"timestamp\": 1608667063.829855,\n  \"user_message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Sling and Running Sync\nDESCRIPTION: Commands to install the latest version of Sling, run the sync process, and verify the results.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/sling_decorator/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade sling\nmake sync   # runs sling\nmake verify\n```\n\n----------------------------------------\n\nTITLE: Deploying to Dagster Cloud\nDESCRIPTION: Command to deploy code locations to Dagster+\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/configuring-ci-cd.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndagster-cloud ci deploy\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to job_ticks Table in SQL\nDESCRIPTION: Adds a primary key constraint on the 'id' column of the job_ticks table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Creating jobs Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the jobs table, which stores information about the different jobs processed in Dagster, including origins and statuses.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: jobs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.jobs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Migrating from SourceAsset to AssetSpec in Python\nDESCRIPTION: Shows how to migrate from the deprecated SourceAsset class to the new AssetSpec class. The key difference is that IO manager keys are now set through metadata entries with the 'dagster/io_manager_key' key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# before\nfrom dagster import SourceAsset\nmy_asset = SourceAsset(\"my_asset\", io_manager_key=\"abc\")\n\n# after\nfrom dagster import AssetSpec\nmy_asset = AssetSpec(\"my_asset\", metadata={\"dagster/io_manager_key\": \"abc\"})\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Key Constraint to Jobs in PostgreSQL\nDESCRIPTION: Adds a unique constraint on 'job_origin_id' in the 'jobs' table to guarantee there are no duplicate job origins tracked.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for schedule_ticks ID in PostgreSQL\nDESCRIPTION: Applies 'schedule_ticks_id_seq' for default ID generation in 'schedule_ticks', enabling orderly and automated tick entry identification while maintaining consistency.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.schedule_ticks ALTER COLUMN id SET DEFAULT nextval('public.schedule_ticks_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Listing Persistent Volume Claims in Kubernetes\nDESCRIPTION: Shell command to list persistent volume claims (PVCs) in a Kubernetes cluster, which is useful for managing storage resources used by Postgres and RabbitMQ.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pvc\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Shell command to start the Dagster development server with a specific configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/databricks-pipeline.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev -f dagster_databricks_pipes.py\n```\n\n----------------------------------------\n\nTITLE: Examples and Experimental Modules\nDESCRIPTION: Enumerates specific example modules and experimental features that are included as dependencies. Some entries are commented out, suggesting they are analyzed but not installed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/requirements.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n-e examples/assets_dynamic_partitions\n-e examples/assets_pandas_pyspark\n-e examples/development_to_production\n-e examples/docs_snippets[full]\n-e examples/feature_graph_backed_assets\n-e examples/tutorial_notebook_assets\n-e examples/with_great_expectations\n-e examples/with_openai\n-e examples/with_pyspark\n-e examples/with_wandb\n-e python_modules/libraries/dagster-airlift[mwaa,dbt,test]\n-e examples/experimental/dagster-dlift\n-e examples/starlift-demo\n-e python_modules/libraries/dagster-airlift/kitchen-sink\n-e python_modules/libraries/dagster-dbt/kitchen-sink\n-e examples/experimental/dagster-dlift/kitchen-sink\n-e python_modules/libraries/dagster-airlift/perf-harness\n-e examples/airlift-migration-tutorial\n-e examples/use_case_repository[dev]\n-e examples/docs_projects/project_atproto_dashboard\n-e examples/docs_projects/project_dagster_modal_pipes\n-e examples/docs_projects/project_prompt_eng\n-e examples/docs_projects/project_components_pdf_extraction\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Azure Key Vault\nDESCRIPTION: Commands to create an Azure Key Vault with RBAC authorization and assign administrator roles.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/key-vault.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naz keyvault create -n <vault-name> -g <resource-group> --enable-rbac-authorization\n```\n\nLANGUAGE: bash\nCODE:\n```\naz role assignment create --role 'Key Vault Administrator' --assignee '<your identity>' --scope '/subscriptions/<subscription_id>/resourceGroups/<resource_group>'\n```\n\n----------------------------------------\n\nTITLE: Sling Replication Configuration in YAML\nDESCRIPTION: YAML configuration file defining source, target, and stream specifications for Sling data replication\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/sling.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsource: MY_POSTGRES\ntarget: MY_SNOWFLAKE\n\ndefaults:\n  mode: full-refresh\n  object: \"{stream_schema}_{stream_table}\"\n\nstreams:\n  public.accounts:\n  public.users:\n  public.finance_departments:\n    object: \"departments\"\n```\n\n----------------------------------------\n\nTITLE: K-Means Clustering Analysis\nDESCRIPTION: Performs K-means clustering on the Iris dataset features and assigns cluster labels.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/tutorial_template/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport sklearn.cluster\n\nestimator = sklearn.cluster.KMeans(n_clusters=3)\nestimator.fit(\n    iris[[\"Sepal length (cm)\", \"Sepal width (cm)\", \"Petal length (cm)\", \"Petal width (cm)\"]]\n)\n\niris[\"K-means cluster assignment\"] = estimator.labels_\n```\n\n----------------------------------------\n\nTITLE: Configuring Scheduler in YAML\nDESCRIPTION: YAML configuration for setting up the SystemCronScheduler in the dagster.yaml file\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_38\n\nLANGUAGE: yaml\nCODE:\n```\nscheduler:\n    module: dagster_cron.cron_scheduler\n    class: SystemCronScheduler\n```\n\n----------------------------------------\n\nTITLE: Processing Event Log Entries for Dagster Pipeline Run\nDESCRIPTION: A series of JSON-formatted event log entries from a Dagster pipeline execution. The events show a sequence starting with logs capture, step execution, asset materialization, and ending with a failure due to a required database migration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ComputeLogsCaptureData\", \"log_key\": \"asset_solid\", \"step_keys\": [\"asset_solid\"]}, \"event_type_value\": \"LOGS_CAPTURED\", \"logging_tags\": {\"pipeline\": \"asset_pipeline\", \"solid\": \"asset_solid\", \"step_key\": \"asset_solid\"}, \"message\": \"Started capturing logs for solid: asset_solid.\", \"pid\": 3447, \"pipeline_name\": \"asset_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"asset_solid\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"asset_solid\", \"parent\": null}}, \"step_key\": \"asset_solid\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"asset_pipeline - 4a832042-13f5-4b77-ab3f-24b88827402e - 3447 - asset_solid - LOGS_CAPTURED - Started capturing logs for solid: asset_solid.\", \"pipeline_name\": \"asset_pipeline\", \"run_id\": \"4a832042-13f5-4b77-ab3f-24b88827402e\", \"step_key\": \"asset_solid\", \"timestamp\": 1625761044.465475, \"user_message\": \"Started capturing logs for solid: asset_solid.\"}\n```\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"asset_pipeline\", \"solid\": \"asset_solid\", \"step_key\": \"asset_solid\"}, \"message\": \"Started execution of step \\\"asset_solid\\\".\", \"pid\": 3447, \"pipeline_name\": \"asset_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"asset_solid\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"asset_solid\", \"parent\": null}}, \"step_key\": \"asset_solid\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"asset_pipeline - 4a832042-13f5-4b77-ab3f-24b88827402e - 3447 - asset_solid - STEP_START - Started execution of step \\\"asset_solid\\\".\", \"pipeline_name\": \"asset_pipeline\", \"run_id\": \"4a832042-13f5-4b77-ab3f-24b88827402e\", \"step_key\": \"asset_solid\", \"timestamp\": 1625761044.504541, \"user_message\": \"Started execution of step \\\"asset_solid\\\".\"}\n```\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"asset_lineage\": [], \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"a\"]}, \"description\": null, \"metadata_entries\": [], \"partition\": \"partition_1\", \"tags\": {\"foo\": \"FOO\"}}}, \"event_type_value\": \"ASSET_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"asset_pipeline\", \"solid\": \"asset_solid\", \"step_key\": \"asset_solid\"}, \"message\": \"Materialized value a.\", \"pid\": 3447, \"pipeline_name\": \"asset_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"asset_solid\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"asset_solid\", \"parent\": null}}, \"step_key\": \"asset_solid\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"asset_pipeline - 4a832042-13f5-4b77-ab3f-24b88827402e - 3447 - asset_solid - ASSET_MATERIALIZATION - Materialized value a.\", \"pipeline_name\": \"asset_pipeline\", \"run_id\": \"4a832042-13f5-4b77-ab3f-24b88827402e\", \"step_key\": \"asset_solid\", \"timestamp\": 1625761044.5114958, \"user_message\": \"Materialized value a.\"}\n```\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepFailureData\", \"error\": {\"__class__\": \"SerializableErrorInfo\", \"cause\": null, \"cls_name\": \"DagsterInstanceMigrationRequired\", \"message\": \"dagster.core.errors.DagsterInstanceMigrationRequired: Instance is out of date and must be migrated (Postgres event log storage requires migration). Database is at revision ddcc6d7244c6, head is 7b8304b4429d. Please run `dagster instance migrate`.\\n\\nOriginal exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/base.py\\\", line 1771, in _execute_context\\n    cursor, statement, parameters, context\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/default.py\\\", line 717, in do_execute\\n    cursor.execute(statement, parameters)\\npsycopg2.errors.UndefinedColumn: column \\\"last_materialization_timestamp\\\" of relation \\\"asset_keys\\\" does not exist\\nLINE 1: ...ys (asset_key, last_materialization, last_run_id, last_mater...\\n                                                             ^\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/storage/sql.py\\\", line 61, in handle_schema_errors\\n    yield\\n  File \\\"/Users/prha/code/dagster/python_modules/libraries/dagster-postgres/dagster_postgres/utils.py\\\", line 166, in create_pg_connection\\n    yield conn\\n  File \\\"/Users/prha/code/dagster/python_modules/libraries/dagster-postgres/dagster_postgres/event_log/event_log.py\\\", line 187, in store_asset\\n    else None,\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/base.py\\\", line 1262, in execute\\n    return meth(self, multiparams, params, _EMPTY_EXECUTION_OPTS)\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/sql/elements.py\\\", line 324, in _execute_on_connection\\n    self, multiparams, params, execution_options\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/base.py\\\", line 1461, in _execute_clauseelement\\n    cache_hit=cache_hit,\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/base.py\\\", line 1814, in _execute_context\\n    e, statement, parameters, cursor, context\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/base.py\\\", line 1995, in _handle_dbapi_exception\\n    sqlalchemy_exception, with_traceback=exc_info[2], from_=e\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/util/compat.py\\\", line 207, in raise_\\n    raise exception\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/base.py\\\", line 1771, in _execute_context\\n    cursor, statement, parameters, context\\n  File \\\"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/lib/python3.6/site-packages/sqlalchemy/engine/default.py\\\", line 717, in do_execute\\n    cursor.execute(statement, parameters)\\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column \\\"last_materialization_timestamp\\\" of relation \\\"asset_keys\\\" does not exist\\nLINE 1: ...ys (asset_key, last_materialization, last_run_id, last_mater...\\n                                                             ^\\n\\n[SQL: INSERT INTO asset_keys (asset_key, last_materialization, last_run_id, last_materialization_timestamp, tags) VALUES (%(asset_key)s, %(last_materialization)s, %(last_run_id)s, %(last_materialization_timestamp)s, %(tags)s) ON CONFLICT (asset_key) DO UPDATE SET last_materialization = %(param_1)s, last_run_id = %(param_2)s, last_materialization_timestamp = %(param_3)s, tags = %(param_4)s RETURNING asset_keys.id]\\n[parameters: {'asset_key': '[\\\"a\\\"]', 'last_materialization': '{\\\"__class__\\\": \\\"AssetMaterialization\\\", \\\"asset_key\\\": {\\\"__class__\\\": \\\"AssetKey\\\", \\\"path\\\": [\\\"a\\\"]}, \\\"description\\\": null, \\\"metadata_entries\\\": [], \\\"partition\\\": \\\"partition_1\\\", \\\"tags\\\": {\\\"foo\\\": \\\"FOO\\\"}}', 'last_run_id': '4a832042-13f5-4b77-ab3f-24b88827402e', 'last_materialization_timestamp': datetime.datetime(2021, 7, 8, 16, 17, 24, 511496, tzinfo=datetime.timezone.utc), 'tags': '{\\\"foo\\\": \\\"FOO\\\"}', 'param_1': '{\\\"__class__\\\": \\\"AssetMaterialization\\\", \\\"asset_key\\\": {\\\"__class__\\\": \\\"AssetKey\\\", \\\"path\\\": [\\\"a\\\"]}, \\\"description\\\": null, \\\"metadata_entries\\\": [], \\\"partition\\\": \\\"partition_1\\\", \\\"tags\\\": {\\\"foo\\\": \\\"FOO\\\"}}', 'param_2': '4a832042-13f5-4b77-ab3f-24b88827402e', 'param_3': datetime.datetime(2021, 7, 8, 16, 17, 24, 511496, tzinfo=datetime.timezone.utc), 'param_4': '{\\\"foo\\\": \\\"FOO\\\"}'}]\\n(Background on this error at: http://sqlalche.me/e/14/f405)\\n\\n\", \"stack\": [\"  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/execution/plan/execute_plan.py\\\", line 193, in _dagster_event_sequence_for_step\\n    for step_event in check.generator(step_events):\\n\", \"  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/execution/plan/execute_step.py\\\", line 331, in core_dagster_event_sequence_for_step\\n    yield DagsterEvent.asset_materialization(step_context, user_event, input_lineage)\\n\", \"  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/events/__init__.py\\\", line 701, in asset_materialization\\n    else \\\"\\\"\\n\", \"  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/events/__init__.py\\\", line 288, in from_step\\n    log_step_event(step_context, event)\\n\", \"  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/events/__init__.py\\\", line 200, in log_step_event\\n    pipeline_name=step_context.pipeline_name,\\n\", \"  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/log_manager.py\\\", line 234, in debug\\n    return self._log(logging.DEBUG, msg, kwargs)\\n\", \"  Fil\"]}}, \"error_info\": null, \"level\": 10, \"message\": null, \"pipeline_name\": \"asset_pipeline\", \"run_id\": \"4a832042-13f5-4b77-ab3f-24b88827402e\", \"step_key\": \"asset_solid\", \"timestamp\": 1625761044.511496, \"user_message\": null}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for run_tags id Column\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `run_tags` table to use the `public.run_tags_id_seq` sequence for generating IDs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Job Ticks in SQL\nDESCRIPTION: This SQL snippet modifies the 'job_ticks' table's 'id' column to automatically be assigned a unique value from the sequence 'public.job_ticks_id_seq' when a new record is created.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: job_ticks id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Viewing Project Directory Structure with tree Command\nDESCRIPTION: Using the tree command to view the directory structure showing the newly scaffolded asset file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/dagster-definitions.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntree\n\n├── defs\n│   └── assets\n│       └── my_asset.py\n└── package.json\n\n2 directories, 2 files\n```\n\n----------------------------------------\n\nTITLE: Run Launcher Configuration in YAML\nDESCRIPTION: Configuration for the default run launcher in Dagster that executes jobs in the same process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nrun_launcher:\n  module: dagster.core.launcher\n  class: DefaultRunLauncher\n```\n\n----------------------------------------\n\nTITLE: MDX Documentation Page Structure\nDESCRIPTION: Markdown documentation page frontmatter and JSX components for organizing deployment options documentation\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n---\ntitle: 'Deployment options'\nsidebar_position: 1\n---\n\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Recording Step Success in Dagster Pipeline\nDESCRIPTION: JSON representation of a step success event in a Dagster pipeline. The event records the successful completion of the 'return_two' step which took 58ms to execute.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 58.65532299503684}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmpq7rk3t2y\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"single_mode\", \"solid\": \"return_two\", \"step_key\": \"return_two\"}, \"message\": \"Finished execution of step \\\"return_two\\\" in 58ms.\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}}, \"step_key\": \"return_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - return_two - STEP_SUCCESS - Finished execution of step \\\"return_two\\\" in 58ms.\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": \"return_two\", \"timestamp\": 1625607811.61945, \"user_message\": \"Finished execution of step \\\"return_two\\\" in 58ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Dagster Project\nDESCRIPTION: Dockerfile that sets up a container with Dagster project dependencies including dagster, dagster-postgres, dagster-k8s, and pandas. It copies the project files and exposes port 80 for port-forwarding.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/deploying-to-kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: docker\nCODE:\n```\nFROM python:3.10-slim\n\nWORKDIR /iris_analysis\n\nCOPY . .\n\nRUN pip install \\\n    dagster \\\n    dagster-postgres \\\n    dagster-k8s \\\n    pandas\n\nEXPOSE 80\n```\n\n----------------------------------------\n\nTITLE: Cloning Jaffle Dashboard Repository with Shallow Copy in Shell\nDESCRIPTION: This command clones the Jaffle Dashboard repository from GitHub with a depth of 1 (shallow copy) and removes the .git directory to eliminate version history. It's useful for quickly obtaining the latest version of the project without its full history.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/29-jaffle-dashboard-clone.txt#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone --depth=1 https://github.com/dagster-io/jaffle-dashboard.git jaffle_dashboard && rm -rf jaffle_dashboard/.git\n```\n\n----------------------------------------\n\nTITLE: Enabling External Shuffle Service - Shuffle Settings\nDESCRIPTION: This property enables the external shuffle service, allowing executors to safely discard shuffle files. It must be used in conjunction with dynamic allocation settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_22\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.service.enabled\n```\n\n----------------------------------------\n\nTITLE: Generating ANTLR Files for Asset Selection\nDESCRIPTION: Generate ANTLR files from the AssetSelection.g4 grammar file using the make command. Produces multiple Python files for lexing, parsing, and visitor functionality.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/antlr_asset_selection/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake generate\n```\n\n----------------------------------------\n\nTITLE: Running Tests on External K8s Clusters\nDESCRIPTION: Shell commands showing how to run tests against an external Kubernetes cluster like EKS, by setting environment variables for the Docker image information and using the kubeconfig provider.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nexport DAGSTER_DOCKER_IMAGE_TAG=\"2020-04-21T21-04-06\"\nexport DAGSTER_DOCKER_REPOSITORY=\"$AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com\"\nexport DAGSTER_DOCKER_IMAGE=\"dagster-k8s-tests\"\n\n# First run with --no-cleanup to leave Helm chart in place\npytest --cluster-provider=\"kubeconfig\" --no-cleanup -s -vvv\n\n# Subsequent runs against existing Helm chart\npytest --cluster-provider=\"kubeconfig\" --existing-helm-namespace=\"dagster-test-<some id>\" -s -vvv\n```\n\n----------------------------------------\n\nTITLE: Setting Retry Wait Time for Shuffle IO - Shuffle Settings\nDESCRIPTION: This configuration defines the wait time between retries for fetch operations that fail. The total delay is calculated based on maximum retries and this wait time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_21\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.io.retryWait\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: This command installs the example and its Python dependencies. The '-e' flag installs the package in editable mode, and '[dev]' specifies the development dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_pyspark/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Recording Object Store Operation for Output - Python\nDESCRIPTION: This snippet logs an operation for storing an output in the memory object store after it has been computed. It provides context about the operation and the output being stored, verifying the success of the action.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_input.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_input.compute/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Stored intermediate object for output result in memory object store using pickle.\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_input.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466002.93098, \"user_message\": \"Stored intermediate object for output result in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: DuckDB IO Manager Configuration\nDESCRIPTION: Configuration for DuckDB IO Manager with IOManagerDefinition annotation\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-duckdb.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: DuckDBIOManager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Excluding Parentheses Group and Selecting Key Wildcard Assets in Dagster\nDESCRIPTION: Shows how to select assets whose keys contain 'raw' and are not of the kind 's3' or 'csv'. This syntax uses the 'not' operator, parentheses grouping, and key wildcard matching.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nnot (kind:\"s3\" or kind:\"csv\") and key:\"raw*\"\n```\n\nLANGUAGE: python\nCODE:\n```\nnot_s3_csv_raw_job = define_asset_job(\n    name=\"not_s3_csv_raw_job\", selection='not (kind:\"s3\" or kind:\"csv\") and key:\"raw*\"'\n)\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster asset list --select 'not (kind:\"s3\" or kind:\"csv\") and key:\"raw*\"'\ndagster asset materialize --select 'not (kind:\"s3\" or kind:\"csv\") and key:\"raw*\"'\n```\n\n----------------------------------------\n\nTITLE: Defining External Assets for Existing Snowflake Tables\nDESCRIPTION: Example of creating external assets to represent existing Snowflake tables in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/using-snowflake-with-dagster.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsource_assets = load_assets_from_db_table(\n    AssetSpec(\"iris_harvest_data\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring default queue for agent\nDESCRIPTION: YAML configuration for specifying which agent should serve the default queue in a multi-queue setup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloud:\n  agentQueues:\n    includeDefaultQueue: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Workspace-level Settings for dg in TOML\nDESCRIPTION: This snippet shows a comprehensive example of a dg.toml file for a workspace. It includes all supported settings in the 'workspace' section, such as project directories and excluded paths.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/configuring-dg.md#2025-04-22_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[workspace]\nproject_dirs = [\"projects/*\"]\nexcluded_dirs = [\"projects/deprecated\"]\n\n[cli]\ncolor = true\nlog_level = \"INFO\"\n\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Dagster and AWS\nDESCRIPTION: Shell commands for installing the necessary Python packages to use Dagster with AWS Lambda.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-lambda-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-webserver dagster-aws\n```\n\n----------------------------------------\n\nTITLE: GitHub Enterprise Configuration\nDESCRIPTION: Example showing how to configure the GithubResource for GitHub Enterprise by specifying a custom hostname.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-github.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nGithubResource(\n    github_app_id=os.getenv('GITHUB_APP_ID'),\n    github_app_private_rsa_key=os.getenv('GITHUB_PRIVATE_KEY'),\n    github_installation_id=os.getenv('GITHUB_INSTALLATION_ID'),\n    github_hostname=os.getenv('GITHUB_HOSTNAME'),\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating Run Coordinator with Custom Headers from Flask to Starlette\nDESCRIPTION: Example showing how to update a custom run coordinator that reads request headers when migrating from Flask to Starlette in Dagster 0.14.0.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom flask import has_request_context, request\n\ndef submit_run(self, context: SubmitRunContext) -> PipelineRun:\n    jwt_claims_header = (\n        request.headers.get(\"X-Amzn-Oidc-Data\", None) if has_request_context() else None\n    )\n```\n\nLANGUAGE: python\nCODE:\n```\ndef submit_run(self, context: SubmitRunContext) -> PipelineRun:\n    jwt_claims_header = context.get_request_header(\"X-Amzn-Oidc-Data\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster+ Serverless Deployment Project Structure\nDESCRIPTION: This snippet shows the file structure for a Dagster+ Serverless deployment. It includes the dagster_cloud.yaml file for defining code locations and the optional deployment_settings.yaml for full deployment settings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/dagster-project-file-reference.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n.\n├── README.md\n├── my_dagster_project\n│   ├── __init__.py\n│   ├──  assets.py\n│   └──  definitions.py\n├── my_dagster_project_tests\n├── dagster_cloud.yaml         ## defines code locations\n├── deployment_settings.yaml   ## optional, defines settings for full deployments\n├── pyproject.toml\n├── setup.cfg\n├── setup.py\n└── tox.ini\n```\n\n----------------------------------------\n\nTITLE: Importing Instance Module in Python\nDESCRIPTION: Imports the instance module from Dagster's core package. This module contains classes and functions for managing Dagster instances.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.instance\n```\n\n----------------------------------------\n\nTITLE: Cloning Repositories for Dagster Code Setup\nDESCRIPTION: Commands to clone the new repository and the Dagster+ hybrid quickstart repository, then copy contents to the new repo.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/acr-user-code.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone <your-repo-url> dagster-plus-code\ngit clone git@github.com:dagster-io/dagster-cloud-hybrid-quickstart.git\n```\n\nLANGUAGE: bash\nCODE:\n```\nrsync -av --exclude='.git' dagster-cloud-hybrid-quickstart/ dagster-plus-code/\ncd dagster-plus-code\ngit add .\ngit commit -m \"Initial commit\"\ngit push\n```\n\n----------------------------------------\n\nTITLE: Verifying Sync Results\nDESCRIPTION: Example output showing the synced data in both Postgres and DuckDB, demonstrating successful replication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/sling_decorator/README.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n                POSTGRES\n--------------------------------\n all_user_id |      name\n-------------+----------------\n           1 | Alice Johnson\n           2 | Bob Williams\n           3 | Charlie Miller\n(3 rows)\n\n                DUCKDB\n--------------------------------\n┌─────────────┬────────────────┬──────────────────┐\n│ all_user_id │      name      │ _sling_loaded_at │\n│    int32    │    varchar     │      int32       │\n├─────────────┼────────────────┼──────────────────┤\n│           1 │ Alice Johnson  │       1708665267 │\n│           2 │ Bob Williams   │       1708665267 │\n│           3 │ Charlie Miller │       1708665267 │\n└─────────────┴────────────────┴──────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Presenting Upstream and Downstream Layer Selection in Markdown\nDESCRIPTION: This markdown table showcases various syntax options for selecting upstream and downstream layers of assets in Dagster. It includes examples of selecting different levels of connections in the asset graph.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Asset selection | Syntax | Description |\n|------|--------|-------------|\n| **All layers upstream of an asset** | `+key:\"my_asset\"` | Selects all upstream assets that provide input into `my_asset`. |\n| **One layer upstream of an asset** | `1+key:\"my_asset\"` | Selects assets that directly provide input to `my_asset`. |\n| **Two layers upstream of an asset** | `2+key:\"my_asset\"` | Selects assets two steps upstream from `my_asset`. |\n| **All layers downstream of an asset** | `key:\"my_asset\"+` | Selects all downstream assets that depend on `my_asset`. |\n| **One layer downstream of an asset** | `key:\"my_asset\"+1` | Selects assets that directly receive input from `my_asset`. |\n| **Two layers downstream of an asset** | `key:\"my_asset\"+2` | Selects assets two steps downstream from `my_asset`. |\n| **One layer upstream and downstream of an asset** | `1+key:\"my_asset\"+1` | Selects one layer of assets providing input to and receiving input from `my_asset`. |\n| **Two layers upstream and downstream of an asset** | `2+key:\"my_asset\"+2` | Selects two layers of assets providing input to and receiving input from `my_asset`. |\n| **All layers upstream and downstream of an asset** | `+key:\"my_asset\"+` | Selects all assets upstream and downstream of `my_asset`. |\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster and Dagstermill\nDESCRIPTION: This snippet imports necessary modules from the Dagster and Dagstermill libraries. AssetMaterialization and ExpectationResult are imported from dagster for event creation, while dagstermill is imported to enable notebook execution within Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_explicit_yield.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetMaterialization, ExpectationResult\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Command to launch the Dagster development server to view and materialize assets\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/downstream-assets.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in JSX\nDESCRIPTION: Sets up a documentation page with a DocCardList component from Docusaurus theme to display migration-related documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/migration/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Configuring Instigators Table ID Sequence\nDESCRIPTION: Alter table to set default ID generation for instigators using a sequence\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.instigators ALTER COLUMN id SET DEFAULT nextval('public.instigators_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Installing UV on Linux using curl\nDESCRIPTION: This command uses curl to download and execute the UV installation script on Linux systems.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_InstallUv.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Creating Shell Command Component in Dagster\nDESCRIPTION: Command line instruction to scaffold a new shell command component type in a Dagster project using the dg scaffold utility. The command creates the necessary boilerplate files in the project's component library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/shell-script-component/1-dg-scaffold-shell-command.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold component-type ShellCommand\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Definitions Validate Command\nDESCRIPTION: Command for validating Dagster definitions with specified log level and workspace\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/34-dg-component-check-defs.txt#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster definitions validate --log-level warning --log-format colored --workspace /tmp/workspace\n```\n\n----------------------------------------\n\nTITLE: Updating Dependent Assets in Python\nDESCRIPTION: Modifies the upstream asset, changing both its return value and code version, to demonstrate how changes propagate to dependent assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset\n\n@asset(code_version=\"v3\")\ndef versioned_number():\n    return 2\n\n@asset\ndef multiplied_number(versioned_number):\n    return versioned_number * 5\n```\n\n----------------------------------------\n\nTITLE: Importing DagsterMill\nDESCRIPTION: This snippet imports the `dagstermill` library, which allows Dagster to execute and integrate with Jupyter notebooks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/error_notebook.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset with Partitions in Main Deployment - Python\nDESCRIPTION: Example of an asset definition with a WeeklyPartitionsDefinition starting from January 1, 2024 in the main deployment. Partitions definitions determine how asset data is divided.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset(partitions_def=WeeklyPartitionsDefinition(start_date=\"2024-01-01\"))\ndef weekly_orders(): ...\n```\n\n----------------------------------------\n\nTITLE: Installing the Tutorial Example Code from Dagster\nDESCRIPTION: Command to install the airlift-migration-tutorial example project, which contains the necessary files for following the migration tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/setup.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name airlift-migration-tutorial --example airlift-migration-tutorial\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Bulk Actions in SQL\nDESCRIPTION: Creates a B-tree index on the bulk_actions table to optimize queries filtering by key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Plugin Components List\nDESCRIPTION: Console output from 'dg list plugins' command showing a table of Dagster plugin components including symbols, summaries, and features. The output displays various components like assets, schedules, sensors, and specialized components for handling definitions and subprocess scripts.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/7-dg-list-plugins.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndg list plugins\n\nUsing /.../jaffle-platform/.venv/bin/dagster-components\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Plugin  ┃ Objects                                                                                                    ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ dagster │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓ │\n│         │ ┃ Symbol                                                      ┃ Summary            ┃ Features            ┃ │\n│         │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩ │\n│         │ │ dagster.asset                                               │ Create a           │ [scaffold-target]   │ │\n│         │ │                                                             │ definition for how │                     │ │\n│         │ │                                                             │ to compute an      │                     │ │\n│         │ │                                                             │ asset.             │                     │ │\n│         │ ├─────────────────────────────────────────────────────────────┼────────────────────┼─────────────────────┤ │\n│         │ │ dagster.components.DefinitionsComponent                     │ An arbitrary set   │ [component,         │ │\n│         │ │                                                             │ of dagster         │ scaffold-target]    │ │\n│         │ │                                                             │ definitions.       │                     │ │\n│         │ ├─────────────────────────────────────────────────────────────┼────────────────────┼─────────────────────┤ │\n│         │ │ dagster.components.DefsFolderComponent                      │ A folder which may │ [component,         │ │\n│         │ │                                                             │ contain multiple   │ scaffold-target]    │ │\n│         │ │                                                             │ submodules, each   │                     │ │\n│         │ │                                                             │ which define       │                     │ │\n│         │ │                                                             │ components.        │                     │ │\n│         │ ├─────────────────────────────────────────────────────────────┼────────────────────┼─────────────────────┤ │\n│         │ │ dagster.components.PipesSubprocessScriptCollectionComponent │ Assets that wrap   │ [component,         │ │\n│         │ │                                                             │ Python scripts     │ scaffold-target]    │ │\n│         │ │                                                             │ executed with      │                     │ │\n│         │ │                                                             │ Dagster's          │                     │ │\n│         │ │                                                             │ PipesSubprocessCl… │                     │ │\n│         │ ├─────────────────────────────────────────────────────────────┼────────────────────┼─────────────────────┤ │\n│         │ │ dagster.schedule                                            │ Creates a schedule │ [scaffold-target]   │ │\n│         │ │                                                             │ following the      │                     │ │\n│         │ │                                                             │ provided cron      │                     │ │\n│         │ │                                                             │ schedule and       │                     │ │\n│         │ │                                                             │ requests runs for  │                     │ │\n│         │ │                                                             │ the provided job.  │                     │ │\n│         │ ├─────────────────────────────────────────────────────────────┼────────────────────┼─────────────────────┤ │\n│         │ │ dagster.sensor                                              │ Creates a sensor   │ [scaffold-target]   │ │\n│         │ │                                                             │ where the          │                     │ │\n│         │ │                                                             │ decorated function │                     │ │\n│         │ │                                                             │ is used as the     │                     │ │\n│         │ │                                                             │ sensor's           │                     │ │\n│         │ │                                                             │ evaluation         │                     │ │\n│         │ │                                                             │ function.          │                     │ │\n│         │ └─────────────────────────────────────────────────────────────┴────────────────────┴─────────────────────┘ │\n└─────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Listing Partial Markdown Files in Bash\nDESCRIPTION: Shows how to list markdown partial files in the docs/partials/ directory using a bash command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -1 docs/partials/\n_Beta.md\n_Deprecated.md\n_InspirationList.md\n_KindsTags.md\n_Preview.md\n_Superseded.md\n```\n\n----------------------------------------\n\nTITLE: Recording Pipeline Success Event in Dagster\nDESCRIPTION: JSON representation of a pipeline success event in Dagster. The event signifies the successful completion of the 'single_mode' pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of pipeline \\\"single_mode\\\".\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"single_mode\\\".\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": null, \"timestamp\": 1625607811.678226, \"user_message\": \"Finished execution of pipeline \\\"single_mode\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster Project from YAML DSL Example\nDESCRIPTION: Creates a new Dagster project based on the asset_yaml_dsl example template. This command sets up the initial project structure and configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/assets_yaml_dsl/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example asset_yaml_dsl\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Index for Asset Daemon Evaluations in SQL\nDESCRIPTION: Creates a unique B-tree index on the asset_daemon_asset_evaluations table to ensure each asset key and evaluation ID combination appears only once.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nCREATE UNIQUE INDEX idx_asset_daemon_asset_evaluations_asset_key_evaluation_id ON public.asset_daemon_asset_evaluations USING btree (asset_key, evaluation_id);\n```\n\n----------------------------------------\n\nTITLE: Storing Dagster Intermediate Object in JSON\nDESCRIPTION: JSON record for an object store operation showing storage of the 'result' output from 'build_model' in the filesystem object store using pickle serialization. Contains storage path and operation metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/build_model/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/build_model/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_model - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_model\", \"timestamp\": 1609894320.050068, \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server\nDESCRIPTION: Command to start the Dagster development server and load the sample Sling pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/sling_decorator/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Formatting Full Path Links in Markdown\nDESCRIPTION: Demonstrates the correct way to format links in Docusaurus documentation using full paths instead of relative links to avoid 404 errors.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nFor more information, see \"[Defining assets](/guides/build/assets/defining-assets)\".\n```\n\n----------------------------------------\n\nTITLE: Starting Flower for Celery Monitoring (Bash)\nDESCRIPTION: Command to start Flower, a real-time web-based monitor for Celery tasks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncelery -A dagster_celery.app flower\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for run_tags.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `run_tags` table to be generated by the `run_tags_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_55\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Creating normalized_cereals Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the normalized_cereals table which stores attributes and nutritional information about different cereals, designed for analytics purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: normalized_cereals; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.normalized_cereals (\n    id integer NOT NULL,\n    name character varying,\n    mfr character varying,\n    type character varying,\n    calories double precision,\n    protein double precision,\n    fat double precision,\n    sodium double precision,\n    fiber double precision,\n    carbo double precision,\n    sugars double precision,\n    potass double precision,\n    vitamins double precision,\n    shelf double precision,\n    weight double precision,\n    cups double precision,\n    rating double precision\n);\nALTER TABLE public.normalized_cereals OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Loading Iris Dataset\nDESCRIPTION: Loads the Iris dataset from UCI Machine Learning Repository using pandas, with specified column names for features and species.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/tutorial_template/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\niris = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n    names=[\n        \"Sepal length (cm)\",\n        \"Sepal width (cm)\",\n        \"Petal length (cm)\",\n        \"Petal width (cm)\",\n        \"Species\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Table: asset_event_tags in PostgreSQL\nDESCRIPTION: Defines the 'asset_event_tags' table to store metadata tags for asset events in text format. Columns include event ID, asset key, and event timestamp. Essential for tagging assets with metadata during events.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.asset_event_tags (\n    id bigint NOT NULL,\n    event_id integer,\n    asset_key text NOT NULL,\n    key text NOT NULL,\n    value text,\n    event_timestamp timestamp without time zone\n);\nALTER TABLE public.asset_event_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Generating Python Dependencies with pip-compile\nDESCRIPTION: This code snippet shows the command used to generate the requirements file using pip-compile. It demonstrates how the dependencies were automatically compiled for Python 3.8.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/airlift-migration-tutorial/tutorial_example/shared/dbt/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#    pip-compile\n```\n\n----------------------------------------\n\nTITLE: Recording Step Input Event - Python\nDESCRIPTION: This snippet logs the reception of an input for a computation step, including successful type checking. It details the input name, type, and the context in which it was accepted within the pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"x\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"x\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Got input \\\"x\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_input.compute - STEP_INPUT - Got input \\\"x\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466002.914811, \"user_message\": \"Got input \\\"x\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Built-in IO Managers Module\nDESCRIPTION: This code snippet demonstrates how to import the Dagster module for built-in IO Managers. It sets the current module to 'dagster' for the subsequent class and function definitions related to built-in IO Managers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/io-managers.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Records in JSON Format\nDESCRIPTION: These records show the full event logs of a Dagster pipeline execution, containing structured data about step execution, inputs/outputs, and object store operations. Each record includes metadata such as event type, timestamp, pipeline name, run ID, and step key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"do_something.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_something\", \"solid_definition\": \"do_something\", \"step_key\": \"do_something.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_something\", \"parent\": null}, \"step_key\": \"do_something.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_something.compute - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_something.compute\", \"timestamp\": 1610466123.54216, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Proxying DAG execution to Dagster (YAML)\nDESCRIPTION: This YAML file defines the configuration for proxying the execution of a whole DAG to Dagster. It specifies the job name and the assets to be materialized.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/migrate.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\njob_name: rebuild_customers_list\nassets_to_materialize:\n  - customers_raw\n  - customers_clean\n  - rebuild_customers_list\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-pipes-typescript via npm\nDESCRIPTION: Command to install the '@dagster-io/dagster-pipes' package using npm. This package enables integration between TypeScript processes and the Dagster orchestrator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/typescript.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnpm install @dagster-io/dagster-pipes\n```\n\n----------------------------------------\n\nTITLE: Creating runs Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the runs table to keep track of each run within Dagster, detailing various attributes including pipeline name and execution status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: runs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name character varying,\n    status character varying(63),\n    run_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.runs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest for AWS Quickstart Project\nDESCRIPTION: Command to execute tests for the AWS quickstart project using pytest. The tests are located in the quickstart_aws_tests directory and can be run from the project root.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_aws/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npytest quickstart_aws_tests\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log Records in JSON Format\nDESCRIPTION: JSON-formatted log entries tracking the execution of a Dagster pipeline, including step execution events, input/output operations, and object store interactions. Each record contains detailed metadata about the pipeline execution state, timing, and operation results.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_54\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"STEP_START\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\",\n      \"dagster/schedule_name\": \"foo_schedule\",\n      \"pipeline\": \"foo\",\n      \"solid\": \"do_something\",\n      \"solid_definition\": \"do_something\",\n      \"step_key\": \"do_something.compute\"\n    },\n    \"message\": \"Started execution of step \\\"do_something.compute\\\".\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Snapshots Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'snapshots' table to store snapshots in Dagster. It includes columns for snapshot identification, binary data, and type information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.snapshots (\n    id bigint NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Defining an Asset with Metadata in Main Deployment - Python\nDESCRIPTION: Example of an asset definition with metadata specifying expected columns in the main deployment. Metadata provides additional information about the asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/change-tracking.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"expected_columns\": [\"sku\", \"price\", \"supplier\"]})\ndef products(): ...\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraint on runs.run_id\nDESCRIPTION: This SQL statement adds a unique constraint to the `runs` table on the `run_id` column.  This ensures that each run ID is unique within the table, preventing duplicate run entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\n\"ALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Compute Logs for Dagster Cloud\nDESCRIPTION: YAML configuration to disable compute logs for Dagster Cloud.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ncomputeLogs:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for Dagster-Cloud CLI\nDESCRIPTION: Shows the help options available in the dagster-cloud CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-cloud-cli/installing-and-configuring.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud --help\n```\n\n----------------------------------------\n\nTITLE: Importing React Documentation Components\nDESCRIPTION: Imports React components for documentation rendering, including a custom DgComponentsPreview component and theme's DocCardList component.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DgComponentsPreview from '@site/docs/partials/_DgComponentsPreview.md';\n\n<DgComponentsPreview />\n\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Creating Pending Steps Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'pending_steps' table to manage pending steps in Dagster pipelines. It includes columns for step identification, priority, and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.pending_steps (\n    id bigint NOT NULL,\n    concurrency_key text NOT NULL,\n    run_id text,\n    step_key text,\n    priority integer,\n    assigned_timestamp timestamp without time zone,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Mounting Cloudflare R2 Bucket for Data Storage\nDESCRIPTION: Configures a Cloudflare R2 bucket as a mounted filesystem within Modal. This allows seamless file access between Dagster and Modal, serving as a staging area for audio files and transcription results.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/modal-application.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nr2_bucket = modal.Mount.from_r2(\n    r2_bucket_name=\"podcast-ingestion\",\n    access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n    secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n    endpoint_url=\"https://xxxxxxxxxxxxxxxxxxxx.r2.cloudflarestorage.com\",\n)\n```\n\n----------------------------------------\n\nTITLE: Sample CSV Output from Custom Dagster Logger\nDESCRIPTION: Example output from the custom CSV logger implementation for Dagster. This demonstrates the format of logs produced by the custom CSV logger, showing timestamp, logger name, level, run ID, job name, operation name, and message.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/monitor/logging/custom-logging.md#2025-04-22_snippet_5\n\nLANGUAGE: csv\nCODE:\n```\n2024-09-04T09:29:33.643818,dagster,INFO,cc76a116-4c8f-400f-9c4d-c42b66cdee3a,topstory_ids_job,hackernews_topstory_ids,Compute Logger - Got 500 top stories.\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Asset Keys Table in SQL\nDESCRIPTION: Inserts multiple rows into the 'asset_keys' table, including asset keys and their creation timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.asset_keys (id, asset_key, create_timestamp) FROM stdin;\n1\t[\"cost_db_table\"]\t2020-12-22 19:55:24.427812\n2\t[\"traffic_db_table\"]\t2020-12-22 19:55:33.700015\n3\t[\"dashboards\", \"cost_dashboard\"]\t2020-12-22 19:55:34.269442\n4\t[\"dashboards\", \"traffic_dashboard\"]\t2020-12-22 19:56:39.317239\n5\t[\"model\"]\t2020-12-22 19:57:44.357557\n\\.\n```\n\n----------------------------------------\n\nTITLE: Setting Python Version in GitHub Workflow for Dagster+ Serverless\nDESCRIPTION: GitHub workflow configuration showing how to specify a custom Python version for Dagster+ Serverless deployment. This sets the PYTHON_VERSION environment variable for the deployment process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: Dagster Cloud Serverless Deploy\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  deploy:\n    name: Serverless Deploy\n    runs-on: ubuntu-latest\n    env:\n      PYTHON_VERSION: 3.11\n    # rest of your deploy.yml...\n```\n\n----------------------------------------\n\nTITLE: Bootstrapping a Dagster Project from Example\nDESCRIPTION: This snippet demonstrates how to create a new Dagster project from an example that includes integration with Weights & Biases. The command requires Dagster to be installed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_wandb/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example with_wandb\n```\n\n----------------------------------------\n\nTITLE: Running Airbyte Setup Script\nDESCRIPTION: Command to run a Python script that sets up data and creates an Airbyte connection between source and destination databases.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_modern_data_stack/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m assets_modern_data_stack.utils.setup_airbyte\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Key Constraint to Asset Keys in PostgreSQL\nDESCRIPTION: Sanctions a unique constraint for 'asset_key' in the 'asset_keys' table to prevent duplicate entries in the asset management process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Success Event in JSON\nDESCRIPTION: This snippet shows a Dagster event record for a successful step completion. It includes details about the step duration and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 203.34012400000034}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Finished execution of step \\\"many_materializations_and_passing_expectations.compute\\\" in 203ms.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_SUCCESS - Finished execution of step \\\"many_materializations_and_passing_expectations.compute\\\" in 203ms.\\n event_specific_data = {\\\"duration_ms\\\": 203.34012400000034}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Running Dagster in Development Mode\nDESCRIPTION: This command starts the Dagster development server, allowing users to interact with the Dagster UI and run assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/copy_csv_to_snowflake.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n\"dagster dev\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Plugin List Command Output\nDESCRIPTION: Terminal output showing the result of running 'dg list plugins' command, displaying available plugins and their components in a formatted table structure. Shows information about core dagster components, DBT integration, and Sling replication features.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/19-dg-list-plugins.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndg list plugins\n\nUsing /.../jaffle-platform/.venv/bin/dagster-components\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Plugin        ┃ Objects                                                                                              ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ dagster       │ [table of dagster core components]\n│ dagster_dbt   │ [table of dbt components]\n│ dagster_sling │ [table of sling components]\n└───────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Dependencies\nDESCRIPTION: Command to install Dagster and the required project dependencies using UV pip. The -e flag installs in development mode, and the [dev] specifier includes development dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster+ Hybrid Deployment Project Structure\nDESCRIPTION: This snippet demonstrates the file structure for a Dagster+ Hybrid deployment. It includes the dagster.yaml file for optional hybrid agent custom configuration, along with Dagster+ specific files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/dagster-project-file-reference.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n.\n├── README.md\n├── my_dagster_project\n│   ├── __init__.py\n│   ├──  assets.py\n│   └──  definitions.py\n├── my_dagster_project_tests\n├── dagster.yaml                ## optional, hybrid agent custom configuration\n├── dagster_cloud.yaml          ## defines code locations\n├── deployment_settings.yaml    ## optional, defines settings for full deployments\n├── pyproject.toml\n├── setup.cfg\n├── setup.py\n└── tox.ini\n```\n\n----------------------------------------\n\nTITLE: Importing Event Log Storage Module in Python\nDESCRIPTION: Imports the event log storage module from Dagster's core storage package. This module contains classes for managing event log storage, including SQL-based implementations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.event_log\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Verifying CLI\nDESCRIPTION: Commands to install Dagster using UV and verify that the Dagster CLI is working properly by checking its version.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/setup.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install dagster\ndagster --version\n```\n\n----------------------------------------\n\nTITLE: Running Airflow Server\nDESCRIPTION: Command to start the Airflow server in a separate shell as part of the migration testing environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-airlift/kitchen-sink/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake run_airflow\n```\n\n----------------------------------------\n\nTITLE: Creating Box and Swarm Plots\nDESCRIPTION: Creates a grid of box and swarm plots showing the distribution of measurements for each species.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/tutorial_template/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(12, 12))\n\nsns.boxplot(x=\"Species\", y=\"Sepal length (cm)\", data=iris, ax=axs[0, 0])\nsns.swarmplot(x=\"Species\", y=\"Sepal length (cm)\", data=iris, color=\".25\", ax=axs[0, 0])\n\nsns.boxplot(x=\"Species\", y=\"Sepal width (cm)\", data=iris, ax=axs[0, 1])\nsns.swarmplot(x=\"Species\", y=\"Sepal width (cm)\", data=iris, color=\".25\", ax=axs[0, 1])\n\nsns.boxplot(x=\"Species\", y=\"Petal length (cm)\", data=iris, ax=axs[1, 0])\nsns.swarmplot(x=\"Species\", y=\"Petal length (cm)\", data=iris, color=\".25\", ax=axs[1, 0])\n\nsns.boxplot(x=\"Species\", y=\"Petal width (cm)\", data=iris, ax=axs[1, 1])\nsns.swarmplot(x=\"Species\", y=\"Petal width (cm)\", data=iris, color=\".25\", ax=axs[1, 1])\n```\n\n----------------------------------------\n\nTITLE: Retrieving Operation Context in Dagstermill via Python\nDESCRIPTION: This code snippet retrieves the current operation context using dagstermill.get_context() and assigns it to the variable 'context'. This context is used to access operation-specific configurations and metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/print_dagstermill_context_op_config.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncontext = dagstermill.get_context()\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Pod Status\nDESCRIPTION: Command to check the status of Kubernetes pods after Dagster installation, showing the expected running components including Celery workers, daemon, webserver, and supporting services.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/kubernetes-and-celery.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods\nNAME                                                     READY   STATUS      RESTARTS   AGE\ndagster-celery-workers-74886cfbfb-m9cbc                  1/1     Running     1          3m42s\ndagster-daemon-68c4b8d68d-vvpls                          1/1     Running     1          3m42s\ndagster-webserver-69974dd75b-5m8gg                           1/1     Running     0          3m42s\ndagster-k8s-example-user-code-1-88764b4f4-25mbd          1/1     Running     0          3m42s\ndagster-postgresql-0                                     1/1     Running     0          3m42s\ndagster-rabbitmq-0                                       1/1     Running     0          3m42s\n```\n\n----------------------------------------\n\nTITLE: Labs Markdown Documentation Front Matter\nDESCRIPTION: Front matter configuration for the Labs documentation page that sets the title and hides the sidebar class.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: 'Labs'\nsidebar_class_name: hidden\n---\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Values in PostgreSQL for Dagster Database\nDESCRIPTION: Sets sequence values for various tables in the Dagster database schema, including asset-related tables, event logs, runs, and daemon heartbeats. These sequences control the auto-increment behavior for numeric IDs in each table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.asset_check_executions_id_seq', 1, false);\nSELECT pg_catalog.setval('public.asset_daemon_asset_evaluations_id_seq', 1, false);\nSELECT pg_catalog.setval('public.asset_event_tags_id_seq', 1, true);\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 2, true);\nSELECT pg_catalog.setval('public.bulk_actions_id_seq', 1, false);\nSELECT pg_catalog.setval('public.concurrency_limits_id_seq', 1, false);\nSELECT pg_catalog.setval('public.concurrency_slots_id_seq', 1, false);\nSELECT pg_catalog.setval('public.daemon_heartbeats_id_seq', 10, true);\nSELECT pg_catalog.setval('public.dynamic_partitions_id_seq', 1, false);\nSELECT pg_catalog.setval('public.event_logs_id_seq', 19, true);\nSELECT pg_catalog.setval('public.instance_info_id_seq', 1, true);\nSELECT pg_catalog.setval('public.instigators_id_seq', 1, false);\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\nSELECT pg_catalog.setval('public.kvs_id_seq', 1, false);\nSELECT pg_catalog.setval('public.pending_steps_id_seq', 1, false);\nSELECT pg_catalog.setval('public.run_tags_id_seq', 2, true);\nSELECT pg_catalog.setval('public.runs_id_seq', 1, true);\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 8, true);\nSELECT pg_catalog.setval('public.snapshots_id_seq', 3, true);\n```\n\n----------------------------------------\n\nTITLE: Running Backcompatibility Tests with Pytest in Bash\nDESCRIPTION: Commands to run backcompatibility tests using pytest for different scenarios: latest release and earliest release.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -m user-code-latest-release -xvv -ff tests/test_backcompat.py\npytest -m user-code-earliest-release -xvv -ff tests/test_backcompat.py\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment for Dagster\nDESCRIPTION: Command to create a new Python virtual environment named '.venv' in the current directory using the venv module. This isolates Dagster's dependencies from the system Python installation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-b-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Loading Data into runs Table in PostgreSQL for Dagster\nDESCRIPTION: SQL COPY command to insert data into the runs table, which stores information about Dagster pipeline runs. Each row contains details like run_id, snapshot_id, pipeline_name, status, and configuration data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.runs (id, run_id, snapshot_id, pipeline_name, status, run_body, partition, partition_set, create_timestamp, update_timestamp, mode) FROM stdin;\n4\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\tacb9cdc108355c862c469e72fe24307b1eda6b41\tmodel_pipeline\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"0c5e14d1ef58258500aab0cd3593f8a29ea0e55c\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": null, \"executable_path\": \"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/bin/python3.6\", \"module_name\": \"dagster_test.toys.repo\", \"package_name\": null, \"python_file\": null, \"working_directory\": null}, \"location_name\": \"dagster_test.toys.repo\"}, \"repository_name\": \"toys_repository\"}, \"pipeline_name\": \"model_pipeline\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_code_origin\": {\"__class__\": \"PipelinePythonOrigin\", \"pipeline_name\": \"model_pipeline\", \"repository_origin\": {\"__class__\": \"RepositoryPythonOrigin\", \"code_pointer\": {\"__class__\": \"ModuleCodePointer\", \"fn_name\": \"toys_repository\", \"module\": \"dagster_test.toys.repo\"}, \"container_image\": null, \"executable_path\": \"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/bin/python3.6\"}}, \"pipeline_name\": \"model_pipeline\", \"pipeline_snapshot_id\": \"acb9cdc108355c862c469e72fe24307b1eda6b41\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuf74gv30\\\"}\", \"dagster/solid_selection\": \"*\"}}\t\\N\t\\N\t2021-07-08 16:10:04.537024\t2021-07-08 16:10:56.331095\tdefault\n5\t4a832042-13f5-4b77-ab3f-24b88827402e\tab8bb23574e380cee398089291d713a2c07dd694\tasset_pipeline\tFAILURE\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"06f6cfdbae309e1cde1076f66b70e2bfa2d8e410\", \"external_pipeline_origin\": null, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_code_origin\": null, \"pipeline_name\": \"asset_pipeline\", \"pipeline_snapshot_id\": \"ab8bb23574e380cee398089291d713a2c07dd694\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"4a832042-13f5-4b77-ab3f-24b88827402e\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.FAILURE\"}, \"step_keys_to_execute\": null, \"tags\": {}}\t\\N\t\\N\t2021-07-08 16:17:24.371699\t2021-07-08 16:17:24.612054\tdefault\n6\t383f9411-4010-454a-9d31-d5a72ab57221\tab8bb23574e380cee398089291d713a2c07dd694\tasset_pipeline\tFAILURE\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"06f6cfdbae309e1cde1076f66b70e2bfa2d8e410\", \"external_pipeline_origin\": null, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_code_origin\": null, \"pipeline_name\": \"asset_pipeline\", \"pipeline_snapshot_id\": \"ab8bb23574e380cee398089291d713a2c07dd694\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"383f9411-4010-454a-9d31-d5a72ab57221\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.FAILURE\"}, \"step_keys_to_execute\": null, \"tags\": {}}\t\\N\t\\N\t2021-07-08 16:35:14.514784\t2021-07-08 16:35:14.709377\tdefault\n\\.\n```\n\n----------------------------------------\n\nTITLE: Validating Helm Charts for Dagster\nDESCRIPTION: Shell commands to validate and lint the Dagster Helm chart before deployment, checking for syntax errors and best practices compliance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nhelm install dagster --dry-run --debug helm/dagster\nhelm lint\n```\n\n----------------------------------------\n\nTITLE: Configuring Streaming Compute Logs in YAML\nDESCRIPTION: This configuration sets up streaming compute logs by specifying the log upload interval in seconds. It uses the CloudComputeLogManager class to manage the compute logs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/settings/customizing-agent-settings.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# dagster.yaml\ninstance_class:\n  module: dagster_cloud.instance\n  class: DagsterCloudAgentInstance\n\ndagster_cloud_api:\n  agent_token:\n    env: DAGSTER_CLOUD_AGENT_TOKEN\n  deployment: prod\n\nuser_code_launcher:\n  module: dagster_cloud.workspace.docker\n  class: DockerUserCodeLauncher\n\ncompute_logs:\n  module: dagster_cloud\n  class: CloudComputeLogManager\n  config:\n    upload_interval: 60\n```\n\n----------------------------------------\n\nTITLE: Importing DeltaLakePolarsIOManager in Python\nDESCRIPTION: This snippet shows how to import the DeltaLakePolarsIOManager class, which is an IOManagerDefinition for handling Delta Lake storage with Polars in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake-polars.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_deltalake_polars import DeltaLakePolarsIOManager\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Secondary Indexes in PostgreSQL\nDESCRIPTION: Inputting primary key constraint on the 'id' column of the 'secondary_indexes' table, assuring the integrity and uniqueness of entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project Structure (pip)\nDESCRIPTION: Command to show the directory structure of a Dagster project initialized with pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/scaffolding-a-project.md#2025-04-22_snippet_4\n\nLANGUAGE: txt\nCODE:\n```\n.\n├── pyproject.toml\n├── README.md\n├── src\n│   └── my_project\n│       ├── defs\n│       │   └── __init__.py\n│       ├── definitions.py\n│       ├── __init__.py\n│       └── lib\n│           └── __init__.py\n└── tests\n    └── __init__.py\n```\n\n----------------------------------------\n\nTITLE: Documentation Link in Markdown\nDESCRIPTION: Provides a markdown hyperlink to the official documentation for the dagster-aws package in the Dagster documentation website.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-aws\n\nThe docs for `dagster-aws` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-aws).\n```\n\n----------------------------------------\n\nTITLE: Initializing Dynamic Partitions Example Project in Dagster\nDESCRIPTION: Command to copy the dynamic partitions example project into the working directory using Dagster CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dynamic_partitions/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name assets_dynamic_partitions\n```\n\n----------------------------------------\n\nTITLE: Non-Root User Configuration in YAML\nDESCRIPTION: Configuration for running the Dagster agent as a non-root user with specific user ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/configuration.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloudAgent:\n  podSecurityContext:\n    runAsUser: 1001\n```\n\n----------------------------------------\n\nTITLE: Recording Dagster Step Start Event in JSON\nDESCRIPTION: JSON record for a step start event, indicating the beginning of execution for the 'persist_model' solid. Contains pipeline context and execution metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_model\"}, \"message\": \"Started execution of step \\\"persist_model\\\".\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_model\", \"parent\": null}, \"step_key\": \"persist_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - persist_model - STEP_START - Started execution of step \\\"persist_model\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"persist_model\", \"timestamp\": 1609894321.678578, \"user_message\": \"Started execution of step \\\"persist_model\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster UI Components\nDESCRIPTION: Example of importing common components from the Dagster UI library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/ui-components/README.md#2025-04-22_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\nimport {Box, Button, Color, Icon} from '@dagster-io/ui-components';\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for schedules ID in PostgreSQL\nDESCRIPTION: Sets automatic ID generation for 'schedules' table using 'schedules_id_seq', assuring unique and consistent identification of schedule entries within the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.schedules ALTER COLUMN id SET DEFAULT nextval('public.schedules_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Creating runs Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'runs' table that tracks execution runs of jobs with relevant details such as IDs, timestamps, and statuses. This table serves as a primary point for managing job executions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: runs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.runs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Querying Dagster Event Records from SQL Database\nDESCRIPTION: SQL table data containing Dagster event records for a completed pipeline execution. The records include metadata about step outputs, object store operations, and execution status in JSON format, showing the successful completion of the 'longitudinal_pipeline' with various processing steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.normalized_cereals (id, name, mfr, type, calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, shelf, weight, cups, rating) FROM stdin;\n1\t100% Bran\tN\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t393.93939393939394\t30.303030303030305\t15.151515151515152\t18.18181818181818\t848.4848484848485\t75.75757575757575\t3\t3.0303030303030303\t0.33\t68.402973\n2\t100% Natural Bran\tQ\tC\t120\t3\t5\t15\t2\t8\t8\t135\t0\t3\t1\t1\t33.983679\n3\tAll-Bran\tK\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t787.8787878787879\t27.272727272727273\t21.21212121212121\t15.151515151515152\t969.6969696969697\t75.75757575757575\t3\t3.0303030303030303\t0.33\t59.425505\n4\tAll-Bran with Extra Fiber\tK\tC\t100\t8\t0\t280\t28\t16\t0\t660\t50\t3\t2\t0.5\t93.704912\n5\tAlmond Delight\tR\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t266.66666666666663\t1.3333333333333333\t18.666666666666664\t10.666666666666666\t-1.3333333333333333\t33.33333333333333\t3\t1.3333333333333333\t0.75\t34.384843\n6\tApple Cinnamon Cheerios\tG\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t240\t2\t14\t13.333333333333332\t93.33333333333333\t33.33333333333333\t1\t1.3333333333333333\t0.75\t29.509541\n7\tApple Jacks\tK\tC\t110\t2\t0\t125\t1\t11\t14\t30\t25\t2\t1\t1\t33.174094\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Event Logs in SQL\nDESCRIPTION: This snippet alters the 'event_logs' table to set the default value for the 'id' column using the sequence 'public.event_logs_id_seq', which automates ID generation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: event_logs id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Executing a BasicTest with Dagstermill in Python\nDESCRIPTION: This code snippet demonstrates how to use Dagstermill to yield results from a BasicTest. The code imports necessary modules and executes a BasicTest instance, yielding its result through Dagstermill. Dependencies include the 'dagstermill' library and the 'BasicTest' repository from Dagster. The main purpose is to showcase an integration example of Dagstermill within a test scenario.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/yield_obj.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\nfrom dagstermill.examples.repository import BasicTest\n\ndagstermill.yield_result(BasicTest(3))\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server\nDESCRIPTION: This command starts the Dagster development server, which allows you to interact with and execute the pipeline through a web interface.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_openai/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Alter jobs_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `jobs_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.jobs_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Dependencies\nDESCRIPTION: Install Dagster and required dependencies using pip in editable mode with development extras.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Schedule in JSON\nDESCRIPTION: This JSON snippet defines the configuration for a Dagster schedule. It includes the schedule class, Python path, repository path, schedule definition data (cron schedule, environment variables, name), and status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"Schedule\", \"python_path\": \"/Users/alangenfeld/venvs/dagenv36/bin/python\", \"repository_path\": \"/Users/alangenfeld/dagster/examples/repository.yaml\", \"schedule_definition_data\": {\"__class__\": \"ScheduleDefinitionData\", \"cron_schedule\": \"10 11 10 * *\", \"environment_vars\": {\"POSTGRES_DB\": \"\", \"POSTGRES_HOST\": \"\", \"POSTGRES_PASSWORD\": \"\", \"POSTGRES_USERNAME\": \"\"}, \"name\": \"monthly_trip_ingest_schedule\"}, \"status\": {\"__enum__\": \"ScheduleStatus.STOPPED\"}}\n```\n\n----------------------------------------\n\nTITLE: Creating jobs Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'jobs' table to define various jobs within the system, capturing their IDs, statuses, types, and timestamps for creation and updates.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: jobs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.jobs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Running the Local Airflow Instance\nDESCRIPTION: Command to run the Airflow instance with the necessary environment variables, making the Airflow UI accessible at localhost:8080.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/setup.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_run\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Output Event in JSON\nDESCRIPTION: This snippet illustrates a Dagster event log entry for a step output event. It includes information about the output type, type check results, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepOutputData\",\n      \"intermediate_materialization\": null,\n      \"step_output_handle\": {\n        \"__class__\": \"StepOutputHandle\",\n        \"output_name\": \"result\",\n        \"step_key\": \"raw_file_event_admins.compute\"\n      },\n      \"type_check_data\": {\n        \"__class__\": \"TypeCheckData\",\n        \"description\": null,\n        \"label\": \"result\",\n        \"metadata_entries\": [],\n        \"success\": true\n      }\n    },\n    \"event_type_value\": \"STEP_OUTPUT\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"raw_file_event_admins\",\n      \"solid_definition\": \"raw_file_event_admins\",\n      \"step_key\": \"raw_file_event_admins.compute\"\n    },\n    \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"raw_file_event_admins\",\n      \"name\": \"raw_file_event_admins\",\n      \"parent\": null\n    },\n    \"step_key\": \"raw_file_event_admins.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"raw_file_event_admins.compute\\\", \\\"result\\\"], \\\"type_check_data\\\": [true, \\\"result\\\", null, []]}\\n               solid = \\\"raw_file_event_admins\\\"\\n    solid_definition = \\\"raw_file_event_admins\\\"\\n            step_key = \\\"raw_file_event_admins.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"raw_file_event_admins.compute\",\n  \"timestamp\": 1576110682.79508,\n  \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Step Initialization Event\nDESCRIPTION: Captures the start of step execution in a Dagster pipeline, including process ID, step key, and execution context\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_type_value\": \"STEP_START\"}}\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Secondary Indexes in SQL\nDESCRIPTION: This SQL snippet modifies the 'secondary_indexes' table's 'id' column to default to the next value from 'public.secondary_indexes_id_seq' sequence, automating unique ID assignment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: secondary_indexes id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Verifying CLI Availability\nDESCRIPTION: Installs the Dagster package and confirms that the dagster CLI is available by checking its version.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install dagster\ndagster --version\n```\n\n----------------------------------------\n\nTITLE: Installing Yarn Package Manager on macOS\nDESCRIPTION: Command to install Yarn package manager using Homebrew on macOS, which is required for Dagster's UI development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install yarn\n```\n\n----------------------------------------\n\nTITLE: Creating a View for Cold Cereals\nDESCRIPTION: This SQL statement creates a view named `sort_cold_cereals_by_calories` within the `test-schema`. The view selects all columns from the `sort_by_calories` table but filters the results to include only rows where the `type` column is equal to 'C', representing cold cereals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE VIEW \"test-schema\".sort_cold_cereals_by_calories AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n  WHERE (sort_by_calories.type = 'C'::text);\n```\n\n----------------------------------------\n\nTITLE: Processing Dagster Step Input Event in JSON\nDESCRIPTION: JSON record showing a Dagster step input event with type checking. This event indicates the successful receipt of input '_a' for the 'build_model' solid in the pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"_a\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"_a\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Got input \\\"_a\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_model - STEP_INPUT - Got input \\\"_a\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_model\", \"timestamp\": 1609894319.080182, \"user_message\": \"Got input \\\"_a\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sequence Values in PostgreSQL for Dagster Tables\nDESCRIPTION: Sets the current value for various sequence generators used by the tables event_logs, run_tags, runs, schedule_ticks, schedules and snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.event_logs_id_seq', 3, true);\nSELECT pg_catalog.setval('public.run_tags_id_seq', 3, true);\nSELECT pg_catalog.setval('public.runs_id_seq', 1, true);\nSELECT pg_catalog.setval('public.schedule_ticks_id_seq', 1, true);\nSELECT pg_catalog.setval('public.schedules_id_seq', 8, true);\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Creating Note Admonitions in Markdown\nDESCRIPTION: Demonstrates how to create a note admonition in Markdown documentation using the triple colon syntax.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n:::note\nThis guide is applicable to Dagster+\n:::\n```\n\n----------------------------------------\n\nTITLE: Logging Step Materialization - JSON\nDESCRIPTION: Logs the materialization of a value called 'table_info' by the 'raw_file_fans' step. It captures the metadata associated with the materialized value, providing useful context for data lineage tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/raw_file_fans.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_fans\", \"solid_definition\": \"raw_file_fans\", \"step_key\": \"raw_file_fans.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_fans\", \"name\": \"raw_file_fans\", \"parent\": null}, \"step_key\": \"raw_file_fans.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/raw_file_fans.raw\\\"]]]]}\\n               solid = \\\"raw_file_fans\\\"\\n    solid_definition = \\\"raw_file_fans\\\"\\n            step_key = \\\"raw_file_fans.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_fans.compute\", \"timestamp\": 1576110682.9209018, \"user_message\": \"Materialized value table_info.\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sequence Ownership in PostgreSQL for Dagster\nDESCRIPTION: SQL statements that establish ownership of sequence public.snapshots_id_seq and link it to the snapshots table. These commands ensure proper sequence management for auto-incrementing ID columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE public.snapshots_id_seq OWNER TO test;\n\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\n```\n\n----------------------------------------\n\nTITLE: Downloading Dagster Project Example for Jupyter Integration\nDESCRIPTION: This command downloads the example project for integrating Jupyter notebooks with Dagster. It creates a new project named 'my-dagster-project' based on the 'tutorial_notebook_assets' example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster project from-example --name my-dagster-project --example tutorial_notebook_assets\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Tests with pytest Command\nDESCRIPTION: Command for running Dagster unit tests using pytest from the project directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/test/unit-testing-assets-and-ops.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npytest my_project_tests\n```\n\n----------------------------------------\n\nTITLE: Using CliInvocationExample Component for CLI Commands\nDESCRIPTION: Demonstrates how to use the CliInvocationExample component to include CLI commands and their output in documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_12\n\nLANGUAGE: markdown\nCODE:\n```\n<CliInvocationExample path=\"path/to/file.txt\" />\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Dependencies via Shell\nDESCRIPTION: Shell commands for installing required Dagster packages including dagster, dagster-webserver, and dagster-aws.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-containers-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-webserver dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for snapshots_id_seq in PostgreSQL\nDESCRIPTION: Sets the sequence 'snapshots_id_seq' to 2 and marks it true, indicating that snapshots have been recorded.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Creating Run Tags Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the key and value columns of the run_tags table to optimize queries that filter by tag key-value pairs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_57\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n```\n\n----------------------------------------\n\nTITLE: BigQueryResource with Insights Tracking Implementation\nDESCRIPTION: Enhanced implementation using InsightsBigQueryResource to track BigQuery usage metrics in Dagster+ Insights. This is the configuration after adding Insights integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/google-bigquery.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\nfrom dagster_cloud.dagster_insights import InsightsBigQueryResource\n\n@asset\ndef my_asset(bigquery):\n    result = bigquery.query(\"SELECT * FROM my_dataset.my_table LIMIT 10\")\n    # ... process results\n\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"bigquery\": InsightsBigQueryResource(\n            project=\"my-gcp-project\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Dagster Asset with Cosmetic Refactor\nDESCRIPTION: This example demonstrates a cosmetic refactor of a Dagster asset, changing the code version and implementation without altering the output value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@asset(code_version=\"v5\")\ndef versioned_number():\n    result = 10 + 10\n    return Output(value=result, metadata={\"data_version\": DataVersion(str(result))})\n```\n\n----------------------------------------\n\nTITLE: Installing Private Python Packages in GitHub Workflow (YAML)\nDESCRIPTION: This snippet shows the steps to install a package from a private GitHub repository as a dependency in a GitHub workflow.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Checkout internal repository\n  uses: actions/checkout@v3\n  with:\n    token: ${{ secrets.GH_PAT }}\n    repository: my-org/private-repo\n    path: deps/private-repo\n    ref: some-branch # optional to check out a specific branch\n\n- name: Build a wheel\n  # adjust the `cd` command to cd into the directory with setup.py\n  run: >\n    cd deps/private-repo &&\n    python setup.py bdist_wheel &&\n    mkdir -p $GITHUB_WORKSPACE/deps &&\n    cp dist/*whl $GITHUB_WORKSPACE/deps\n\n# If you have multiple private packages, the above two steps should be repeated for each but the following step is only\n# needed once\n- name: Configure dependency resolution to use the wheel built above\n  run: >\n    echo \"[global]\" > $GITHUB_WORKSPACE/deps/pip.conf &&\n    echo \"find-links = \" >> $GITHUB_WORKSPACE/deps/pip.conf &&\n    echo \"    file://$GITHUB_WORKSPACE/deps/\" >> $GITHUB_WORKSPACE/deps/pip.conf &&\n    echo \"PIP_CONFIG_FILE=$GITHUB_WORKSPACE/deps/pip.conf\" > $GITHUB_ENV\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-dg package using pip\nDESCRIPTION: Command to install the dagster-dg package, which is a specialized extension for Dagster. This command uses pip, Python's package installer, to download and install the package from PyPI or another configured package repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-d-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-dg\n```\n\n----------------------------------------\n\nTITLE: Importing Airbyte Integration Module in Python\nDESCRIPTION: This snippet shows how to import the dagster_airbyte module, which is the main module for Airbyte integration in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-airbyte.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: dagster_airbyte\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Schema with Tables and Sequences for Dagster\nDESCRIPTION: SQL script that initializes the complete database schema for Dagster, including tables for asset tracking, event logging, concurrency management, and daemon operations. It sets up proper sequences, ownership permissions, and default values for timestamp fields.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n--\n-- PostgreSQL database dump\n--\n\n-- Dumped from database version 11.16 (Debian 11.16-1.pgdg90+1)\n-- Dumped by pg_dump version 15.8 (Homebrew)\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\n--\n-- Name: public; Type: SCHEMA; Schema: -; Owner: test\n--\n\n-- *not* creating schema, since initdb creates it\n\n\nALTER SCHEMA public OWNER TO test;\n\n--\n-- Name: SCHEMA public; Type: COMMENT; Schema: -; Owner: test\n--\n\nCOMMENT ON SCHEMA public IS '';\n\n\nSET default_tablespace = '';\n\n--\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\n\nALTER TABLE public.alembic_version OWNER TO test;\n\n--\n-- Name: asset_check_executions; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.asset_check_executions (\n    id bigint NOT NULL,\n    asset_key text,\n    check_name text,\n    partition text,\n    run_id character varying(255),\n    execution_status character varying(255),\n    evaluation_event text,\n    evaluation_event_timestamp timestamp without time zone,\n    evaluation_event_storage_id bigint,\n    materialization_event_storage_id bigint,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.asset_check_executions OWNER TO test;\n\n--\n-- Name: asset_check_executions_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.asset_check_executions_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_check_executions_id_seq OWNER TO test;\n\n--\n-- Name: asset_check_executions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.asset_check_executions_id_seq OWNED BY public.asset_check_executions.id;\n\n\n--\n-- Name: asset_daemon_asset_evaluations; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.asset_daemon_asset_evaluations (\n    id bigint NOT NULL,\n    evaluation_id bigint,\n    asset_key text,\n    asset_evaluation_body text,\n    num_requested integer,\n    num_skipped integer,\n    num_discarded integer,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.asset_daemon_asset_evaluations OWNER TO test;\n\n--\n-- Name: asset_daemon_asset_evaluations_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.asset_daemon_asset_evaluations_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_daemon_asset_evaluations_id_seq OWNER TO test;\n\n--\n-- Name: asset_daemon_asset_evaluations_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.asset_daemon_asset_evaluations_id_seq OWNED BY public.asset_daemon_asset_evaluations.id;\n\n\n--\n-- Name: asset_event_tags; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.asset_event_tags (\n    id bigint NOT NULL,\n    event_id integer,\n    asset_key text NOT NULL,\n    key text NOT NULL,\n    value text,\n    event_timestamp timestamp without time zone\n);\n\n\nALTER TABLE public.asset_event_tags OWNER TO test;\n\n--\n-- Name: asset_event_tags_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.asset_event_tags_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_event_tags_id_seq OWNER TO test;\n\n--\n-- Name: asset_event_tags_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.asset_event_tags_id_seq OWNED BY public.asset_event_tags.id;\n\n\n--\n-- Name: asset_keys; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    wipe_timestamp timestamp without time zone,\n    last_materialization_timestamp timestamp without time zone,\n    tags text,\n    cached_status_data text\n);\n\n\nALTER TABLE public.asset_keys OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n\n\n--\n-- Name: backfill_tags; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.backfill_tags (\n    id bigint NOT NULL,\n    backfill_id character varying(255),\n    key text,\n    value text\n);\n\n\nALTER TABLE public.backfill_tags OWNER TO test;\n\n--\n-- Name: backfill_tags_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.backfill_tags_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.backfill_tags_id_seq OWNER TO test;\n\n--\n-- Name: backfill_tags_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.backfill_tags_id_seq OWNED BY public.backfill_tags.id;\n\n\n--\n-- Name: bulk_actions; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.bulk_actions (\n    id bigint NOT NULL,\n    key character varying(32) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text,\n    action_type character varying(32),\n    selector_id text,\n    job_name text\n);\n\n\nALTER TABLE public.bulk_actions OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.bulk_actions_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.bulk_actions_id_seq OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.bulk_actions_id_seq OWNED BY public.bulk_actions.id;\n\n\n--\n-- Name: concurrency_limits; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.concurrency_limits (\n    id bigint NOT NULL,\n    concurrency_key character varying(512) NOT NULL,\n    \"limit\" integer NOT NULL,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.concurrency_limits OWNER TO test;\n\n--\n-- Name: concurrency_limits_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.concurrency_limits_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.concurrency_limits_id_seq OWNER TO test;\n\n--\n-- Name: concurrency_limits_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.concurrency_limits_id_seq OWNED BY public.concurrency_limits.id;\n\n\n--\n-- Name: concurrency_slots; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.concurrency_slots (\n    id bigint NOT NULL,\n    concurrency_key text NOT NULL,\n    run_id text,\n    step_key text,\n    deleted boolean NOT NULL,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.concurrency_slots OWNER TO test;\n\n--\n-- Name: concurrency_slots_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.concurrency_slots_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.concurrency_slots_id_seq OWNER TO test;\n\n--\n-- Name: concurrency_slots_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.concurrency_slots_id_seq OWNED BY public.concurrency_slots.id;\n\n\n--\n-- Name: daemon_heartbeats; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text,\n    id bigint NOT NULL\n);\n\n\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n\n--\n-- Name: daemon_heartbeats_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.daemon_heartbeats_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.daemon_heartbeats_id_seq OWNER TO test;\n\n--\n-- Name: daemon_heartbeats_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.daemon_heartbeats_id_seq OWNED BY public.daemon_heartbeats.id;\n\n\n--\n-- Name: dynamic_partitions; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.dynamic_partitions (\n    id bigint NOT NULL,\n    partitions_def_name text NOT NULL,\n    partition text NOT NULL,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.dynamic_partitions OWNER TO test;\n\n--\n-- Name: dynamic_partitions_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.dynamic_partitions_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.dynamic_partitions_id_seq OWNER TO test;\n\n--\n-- Name: dynamic_partitions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.dynamic_partitions_id_seq OWNED BY public.dynamic_partitions.id;\n\n\n--\n-- Name: event_logs; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n\n```\n\n----------------------------------------\n\nTITLE: Loading Definitions from Current Module\nDESCRIPTION: Function to load Dagster definitions from the current module context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/definitions.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nload_definitions_from_current_module\n```\n\n----------------------------------------\n\nTITLE: MySQL Migration Configuration in dagster.yaml\nDESCRIPTION: This snippet describes the MySQL storage configuration in 'dagster.yaml', detailing the modules and class settings necessary to execute migrations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/alembic/README.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nevent_log_storage:\n  module: dagster_mysql.event_log\n  class: MySQLEventLogStorage\n  config:\n    mysql_url: \"mysql+mysqlconnector://test:test@localhost:3306/test\"\nrun_storage:\n  module: dagster_mysql.run_storage\n  class: MySQLRunStorage\n  config:\n    mysql_url: \"mysql+mysqlconnector://test:test@localhost:3306/test\"\nschedule_storage:\n  module: dagster_mysql.schedule_storage\n  class: MySQLScheduleStorage\n  config:\n    mysql_url: \"mysql+mysqlconnector://test:test@localhost:3306/test\"\n```\n\n----------------------------------------\n\nTITLE: Installing Airlift Federation Tutorial Example Code\nDESCRIPTION: Uses the dagster CLI to install the example code for the Airlift Federation tutorial project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name airlift-federation-tutorial --example airlift-federation-tutorial\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment with uv\nDESCRIPTION: Commands to create and activate a virtual environment using uv, which synchronizes dependencies from pyproject.toml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuv sync\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Plotting Sample Bollinger Bands for Stocks in Python\nDESCRIPTION: This snippet visualizes sample Bollinger Bands for the analyzed stocks using a custom plotting function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/notebooks/bollinger.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbol.plot_sample_bollinger_bands(PRICES, BOLL)\n```\n\n----------------------------------------\n\nTITLE: Appending Data to Resource List\nDESCRIPTION: This snippet appends a string value to the 'list' resource within the Dagstermill context, demonstrating how to modify its contents dynamically.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource_with_exception.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncontext.resources.list.append(\"Hello, notebook!\")\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Docs Local Development Server\nDESCRIPTION: Command to start the local development server for the Dagster documentation site.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn start\n```\n\n----------------------------------------\n\nTITLE: Schedule Error Stack Trace\nDESCRIPTION: Example error stack trace from schedule execution issue\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nFile \".../dagster/python_modules/dagster/dagster/core/definitions/schedule.py\", line 171, in should_execute\n    return self._should_execute(context)\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Index for Asset Check Executions in SQL\nDESCRIPTION: Creates a unique B-tree index on the asset_check_executions table to enforce uniqueness of the combination of asset_key, check_name, run_id, and partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\nCREATE UNIQUE INDEX idx_asset_check_executions_unique ON public.asset_check_executions USING btree (asset_key, check_name, run_id, partition);\n```\n\n----------------------------------------\n\nTITLE: Enable Workload Identity for AKS Cluster\nDESCRIPTION: Azure CLI command to enable workload identity on an AKS cluster to allow the agent to use managed identity for Azure resource access.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/blob-compute-logs.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naz aks update --resource-group <resource-group> --name <cluster-name> --enable-workload-identity\n```\n\n----------------------------------------\n\nTITLE: Resuming Teradata Compute Cluster with dagster-teradata\nDESCRIPTION: This operation restores a previously suspended compute cluster in Teradata VantageCloud Lake. It allows workloads to resume execution within a Dagster pipeline, providing flexibility in resource allocation and cost management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresume_teradata_compute_cluster(\n    compute_profile_name: str,\n    compute_group_name: str,\n    timeout: int = constants.CC_OPR_TIME_OUT\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying Code to Branch Deployment with Dagster Cloud CLI in Shell\nDESCRIPTION: Uses the dagster-cloud CLI to deploy code to a branch deployment. Specifies location file, name, image, commit hash, and git URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/using-branch-deployments-with-the-cli.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud deployment add-location \\\n    --organization $ORGANIZATION_NAME \\\n    --deployment $BRANCH_DEPLOYMENT_NAME \\\n    --api-token $DAGSTER_CLOUD_API_TOKEN \\\n    --location-file $LOCATION_FILE \\\n    --location-name $LOCATION_NAME \\\n    --image \"${LOCATION_REGISTRY_URL}:${IMAGE_TAG}\" \\\n    --commit-hash \"${COMMIT_SHA}\" \\\n    --git-url \"${GIT_URL}\"\n```\n\n----------------------------------------\n\nTITLE: Normalized Cereals Data in PostgreSQL\nDESCRIPTION: Data for the normalized_cereals table containing nutritional information about various cereal products. Each row represents a cereal with information like name, manufacturer, type, calories, protein, fat, sodium, fiber, carbs, sugars, and other nutritional metrics.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_61\n\nLANGUAGE: SQL\nCODE:\n```\n63\tRice Krispies\tK\tC\t110\t2\t0\t290\t0\t22\t3\t35\t25\t1\t1\t1\t40.560159\n64\tShredded Wheat\tN\tC\t80\t2\t0\t0\t3\t16\t0\t95\t0\t1\t0.83\t1\t68.235885\n65\tShredded Wheat 'n'Bran\tN\tC\t134.32835820895522\t4.477611940298507\t0\t0\t5.970149253731343\t28.35820895522388\t0\t208.955223880597\t0\t1\t1.4925373134328357\t0.67\t74.472949\n66\tShredded Wheat spoon size\tN\tC\t134.32835820895522\t4.477611940298507\t0\t0\t4.477611940298507\t29.850746268656714\t0\t179.1044776119403\t0\t1\t1.4925373134328357\t0.67\t72.801787\n67\tSmacks\tK\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t93.33333333333333\t1.3333333333333333\t12\t20\t53.33333333333333\t33.33333333333333\t2\t1.3333333333333333\t0.75\t31.230054\n68\tSpecial K\tK\tC\t110\t6\t0\t230\t1\t16\t3\t55\t25\t1\t1\t1\t53.131324\n69\tStrawberry Fruit Wheats\tN\tC\t90\t2\t0\t15\t3\t15\t5\t90\t25\t2\t1\t1\t59.363993\n70\tTotal Corn Flakes\tG\tC\t110\t2\t1\t200\t0\t21\t3\t35\t100\t3\t1\t1\t38.839746\n71\tTotal Raisin Bran\tG\tC\t140\t3\t1\t190\t4\t15\t14\t230\t100\t3\t1.5\t1\t28.592785\n72\tTotal Whole Grain\tG\tC\t100\t3\t1\t200\t3\t16\t3\t110\t100\t3\t1\t1\t46.658844\n73\tTriples\tG\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t333.3333333333333\t0\t28\t4\t80\t33.33333333333333\t3\t1.3333333333333333\t0.75\t39.106174\n74\tTrix\tG\tC\t110\t1\t1\t140\t0\t13\t12\t25\t25\t2\t1\t1\t27.753301\n75\tWheat Chex\tR\tC\t149.25373134328356\t4.477611940298507\t1.4925373134328357\t343.2835820895522\t4.477611940298507\t25.373134328358205\t4.477611940298507\t171.6417910447761\t37.31343283582089\t1\t1.4925373134328357\t0.67\t49.787445\n76\tWheaties\tG\tC\t100\t3\t1\t200\t3\t17\t3\t110\t25\t1\t1\t1\t51.592193\n77\tWheaties Honey Gold\tG\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t266.66666666666663\t1.3333333333333333\t21.333333333333332\t10.666666666666666\t80\t33.33333333333333\t1\t1.3333333333333333\t0.75\t36.187559\n\\.\n```\n\n----------------------------------------\n\nTITLE: Installing Redis Chart without Password for Testing\nDESCRIPTION: Shell command to install Redis Helm chart with password authentication disabled, making it easier to use for testing environments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nhelm install dagredis stable/redis --set usePassword=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Agent for Branch Deployments in Dagster+\nDESCRIPTION: YAML configuration for a Docker agent that enables branch deployments in Dagster+. The configuration sets dagster_cloud_api.branch_deployments to true and removes any deployment field.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndagster_cloud_api:\n  agent_timeout_seconds: 30\n  branch_deployments: true\n  heartbeat_interval_seconds: 30\n  poll_interval_seconds: 3\n\n# Warning: Should not include any `deployment` field\n\n```\n\n----------------------------------------\n\nTITLE: Importing Census Resource and Output Classes in Python\nDESCRIPTION: This snippet shows the import statements for the CensusResource and CensusOutput classes from the dagster_census module. These classes are used to interact with Census within Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-census.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_census import CensusResource, CensusOutput\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Step Output\nDESCRIPTION: JSON record for a pipeline step output event showing successful type checking of 'result' output of type 'Any'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"raw_file_users.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_users\", \"solid_definition\": \"raw_file_users\", \"step_key\": \"raw_file_users.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Referencing Dagster Azure Documentation\nDESCRIPTION: This snippet provides a link to the official documentation for the dagster-azure library, which contains the API reference and usage instructions for integrating Dagster with Azure services.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-azure\n\nThe docs for `dagster-azure` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-azure).\n```\n\n----------------------------------------\n\nTITLE: Referencing Dagster GCP Pandas Documentation\nDESCRIPTION: This snippet provides a Markdown link to the official documentation for the dagster-gcp-pandas library. The documentation contains detailed information about using Dagster with Google Cloud Platform services and Pandas.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp-pandas/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-gcp-pandas\n\nThe docs for `dagster-gcp-pandas` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-gcp-pandas).\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Success Event - JSON\nDESCRIPTION: This snippet logs a pipeline success event, indicating the successful execution of the defined job. It captures the pipeline name and associated process ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of run for \\\"succeeds_job\\\".\", \"pid\": 46268, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"3a329e81-04ee-493a-b376-fb5d04d4068b\", \"step_key\": null, \"timestamp\": 1668643924.515601, \"user_message\": \"Finished execution of run for \\\"succeeds_job\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies for ATProto Dashboard\nDESCRIPTION: This command installs the project dependencies using pip. The -e flag installs the project in editable mode, and the [dev] specifier includes development dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_atproto_dashboard/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Step Output Monitoring with Dagster in JSON\nDESCRIPTION: This snippet logs a STEP_OUTPUT event in a Dagster pipeline, confirming the generation and type-checking of a pipeline step's output. It is critical for validating that step outputs meet expected criteria and formats. Dependency involves the use of Dagster's output and type-check functionality.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"train_model\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"train_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"train_model\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"train_model\", \"parent\": null}, \"step_key\": \"train_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - train_model - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"train_model\", \"timestamp\": 1608667063.781069, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Storing Intermediate Object in Dagster Memory Store (JSON)\nDESCRIPTION: JSON log entry showing a successful object store operation where an intermediate result was stored in the memory object store using pickle serialization for the 'do_input.compute' step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_input.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_input.compute/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Stored intermediate object for output result in memory object store using pickle.\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.695479, \"user_message\": \"Stored intermediate object for output result in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Starting Vercel Development Server\nDESCRIPTION: Command to start the Vercel development server for local testing and development of the marketplace API.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/marketplace/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nvercel dev\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence with Table Column in PostgreSQL\nDESCRIPTION: Associates the snapshots_id_seq sequence with the id column of the snapshots table. This makes the sequence automatically advance when new records are inserted into the table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Dagster Project in Bash\nDESCRIPTION: Command to install the project and its Python dependencies in editable mode. The '[dev]' flag includes development dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgreSQL Configuration for Dagster\nDESCRIPTION: Sets up initial PostgreSQL configuration parameters for database connection and behavior, including encoding, timeout settings, and security options\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n```\n\n----------------------------------------\n\nTITLE: Creating run_tags Table - SQL\nDESCRIPTION: This snippet creates the 'run_tags' table to store key-value pairs associated with specific runs, allowing for tagging and metadata storage for executions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: run_tags; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key character varying,\n    value character varying\n);\nALTER TABLE public.run_tags OWNER TO test;\n\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster Webserver\nDESCRIPTION: Command to start the Dagster development webserver.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/index.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Creating a PostgreSQL Schema\nDESCRIPTION: This snippet creates a new schema named 'test-schema' within the PostgreSQL database and assigns ownership to the user 'test'. Schemas provide a way to organize database objects like tables and views.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SCHEMA \\\"test-schema\\\";\n\n\nALTER SCHEMA \\\"test-schema\\\" OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Logging Engine Event for Process Exit - JSON\nDESCRIPTION: This snippet logs an engine event indicating that a process for a run has exited. The log captures the PID and contextual data around the exit.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Process for run exited (pid: 46268).\", \"pid\": null, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"3a329e81-04ee-493a-b376-fb5d04d4068b\", \"step_key\": null, \"timestamp\": 1668643924.571541, \"user_message\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster EventLogEntry JSON for Logs Capture Events\nDESCRIPTION: JSON representation of a Dagster EventLogEntry for a LOGS_CAPTURED event. This structure contains information about logs being captured for a specific step in the pipeline execution process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ComputeLogsCaptureData\", \"log_key\": \"add_four.emit_two_2.emit_one_2\", \"step_keys\": [\"add_four.emit_two_2.emit_one_2\"]}, \"event_type_value\": \"LOGS_CAPTURED\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"emit_one_2\", \"step_key\": \"add_four.emit_two_2.emit_one_2\"}, \"message\": \"Started capturing logs for step: add_four.emit_two_2.emit_one_2.\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two_2.emit_one_2\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two_2.emit_one_2\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two_2.emit_one_2 - LOGS_CAPTURED - Started capturing logs for step: add_four.emit_two_2.emit_one_2.\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two_2.emit_one_2\", \"timestamp\": 1640037521.750091, \"user_message\": \"Started capturing logs for step: add_four.emit_two_2.emit_one_2.\"}\n```\n\n----------------------------------------\n\nTITLE: Sample CSV Data\nDESCRIPTION: Sample CSV data to be used as the source for the Dagster pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/quickstart.md#2025-04-22_snippet_3\n\nLANGUAGE: csv\nCODE:\n```\nid,name,age,city\n1,Alice,28,New York\n2,Bob,35,San Francisco\n3,Charlie,42,Chicago\n4,Diana,31,Los Angeles\n```\n\n----------------------------------------\n\nTITLE: Hexadecimal Representation of Compressed Data in Dagster Project\nDESCRIPTION: This is a hexadecimal representation of compressed binary data. The data begins with '\\x789c' which indicates zlib compression. The content appears to be part of the Dagster project's pipeline definition or related infrastructure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: binary\nCODE:\n```\n\\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\n```\n\n----------------------------------------\n\nTITLE: Migrating from Dagit to Dagster-Webserver in Dockerfile\nDESCRIPTION: Example showing how to update Dockerfiles to use the new dagster-webserver package instead of the deprecated dagit package. This includes both the installation and entrypoint changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\n# no (deprecated)\nRUN pip install dagster dagit ...\n...\nENTRYPOINT [\"dagit\", \"-h\", \"0.0.0.0\", \"-p\", \"3000\"]\n\n# yes\nRUN pip install dagster dagster-webserver\n...\nENTRYPOINT [\"dagster-webserver\", \"-h\", \"0.0.0.0\", \"-p\", \"3000\"]\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Event Log Entries in Dagster\nDESCRIPTION: These JSON objects represent Dagster event log entries for a data pipeline execution. They track the execution lifecycle of a step called 'add_four.emit_two.add' in the 'composition' pipeline, showing events such as logs capture initialization, step execution start, input validation, output generation, and IO management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ComputeLogsCaptureData\", \"log_key\": \"add_four.emit_two.add\", \"step_keys\": [\"add_four.emit_two.add\"]}, \"event_type_value\": \"LOGS_CAPTURED\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{\\\".dagster/grpc_info\\\": '{\\\\\"host\\\\\": \\\\\"localhost\\\\\", \\\\\"socket\\\\\": \\\\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"add\", \"step_key\": \"add_four.emit_two.add\"}, \"message\": \"Started capturing logs for step: add_four.emit_two.add.\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"add\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two.add\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"add\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two.add\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two.add - LOGS_CAPTURED - Started capturing logs for step: add_four.emit_two.add.\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two.add\", \"timestamp\": 1640037521.90447, \"user_message\": \"Started capturing logs for step: add_four.emit_two.add.\"}\n```\n\n----------------------------------------\n\nTITLE: Viewing Dagster Cloud Agent Logs\nDESCRIPTION: Kubectl command to view logs for the Dagster Cloud agent pods.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl --namespace dagster-cloud logs -l deployment=agent\n```\n\n----------------------------------------\n\nTITLE: Dagster-Slack Integration Example\nDESCRIPTION: Example code snippet demonstrating how to use the Dagster-Slack integration. The actual code is not provided in the input, but referenced as an external file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/slack.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/integrations/slack.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for asset_keys.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `asset_keys` table to be generated by the `asset_keys_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Creating Index on event_logs.asset_key, partition\nDESCRIPTION: This SQL statement creates a composite index on the `asset_key` and `partition` columns of the `event_logs` table. This index optimizes queries that filter based on both asset key and partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\"\n```\n\n----------------------------------------\n\nTITLE: Creating Index on event_logs.asset_key\nDESCRIPTION: This SQL statement creates an index on the `asset_key` column of the `event_logs` table. This index can speed up queries that filter or sort by asset key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Dagster Project from the Smoke Test Example\nDESCRIPTION: Command to bootstrap a new Dagster project using the assets_smoke_test example as a template. This creates a new project with the specified name containing the smoke test example assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_smoke_test/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example assets_smoke_test\n```\n\n----------------------------------------\n\nTITLE: Helm Scheduler Configuration Example\nDESCRIPTION: YAML configuration showing the new structure for configuring DagsterDaemonScheduler in Helm values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_29\n\nLANGUAGE: yaml\nCODE:\n```\nscheduler:\n  type: DagsterDaemonScheduler\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Sequences and Ownership\nDESCRIPTION: SQL snippets to create sequences such as 'asset_keys_id_seq' and 'bulk_actions_id_seq' within the 'public' schema. Sequences are integral for generating unique identifiers for table entries. Ownership is assigned to 'test' and is associated with the respective table primary keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n```\n\n----------------------------------------\n\nTITLE: Examining Dagster ENGINE_EVENT Process Exit in JSON\nDESCRIPTION: This JSON snippet shows a Dagster ENGINE_EVENT recording a process exit for a run. It contains minimal metadata but captures the essential information about the process termination, including the process ID (3626) that was running the job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Process for run exited (pid: 3626).\", \"pid\": null, \"pipeline_name\": \"basic_assets_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"basic_assets_job\", \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"step_key\": null, \"timestamp\": 1731664852.299335, \"user_message\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Commands for installing the necessary Python and Node.js packages to run the examples.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/javascript-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-webserver\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @tensorflow/tfjs\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Ignore Patterns for Common Adjectives\nDESCRIPTION: This snippet defines patterns for Git to ignore files or content containing common adjectives often used in documentation or comments. It uses case-insensitive matching for flexibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/vale/styles/config/vocabularies/Dagster/reject.txt#2025-04-22_snippet_0\n\nLANGUAGE: gitignore\nCODE:\n```\n[Ee]xtremely\n[Rr]obust\n[Ss]imple\n[Ss]imply\n[Ss]traightforward\n[Vv]ery\n```\n\n----------------------------------------\n\nTITLE: Dagster Pipeline Step Output Event in JSON Format\nDESCRIPTION: JSON representation of a Dagster step output event record. This event indicates that the 'persist_model' step successfully yielded an output named 'result' that passed type checking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"persist_model\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_model\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_model\", \"parent\": null}, \"step_key\": \"persist_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_model - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_model\", \"timestamp\": 1608667064.359076, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Specifying Metrics and Time Granularity in Dagster+ GraphQL API\nDESCRIPTION: This GraphQL schema snippet defines the structure for specifying metrics and time granularity in the Dagster+ API. It includes the ReportingMetricsSelector input type, ReportingMetricsGranularity enum, and a list of valid metric names.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/export-metrics.md#2025-04-22_snippet_3\n\nLANGUAGE: graphql\nCODE:\n```\ninput ReportingMetricsSelector {\n  after: Float # timestamp\n  before: Float # timestamp\n  metricName: String # see below for valid values\n  granularity: ReportingMetricsGranularity\n}\n\nenum ReportingMetricsGranularity {\n  DAILY\n  WEEKLY\n  MONTHLY\n}\n\n# The valid metric names are:\n# \"__dagster_dagster_credits\"\n# \"__dagster_execution_time_ms\"\n# \"__dagster_materializations\"\n# \"__dagster_step_failures\"\n# \"__dagster_step_retries\"\n# \"__dagster_asset_check_errors\"\n# \"__dagster_asset_check_warnings\"\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Project from Example\nDESCRIPTION: This command bootstraps a new Dagster project using the assets_pandas_pyspark example. It creates a project named 'my-dagster-project' based on the example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_pyspark/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example assets_pandas_pyspark\n```\n\n----------------------------------------\n\nTITLE: Running Pytest Tests for Dagster ETL\nDESCRIPTION: Command to execute tests located in the quickstart_etl_tests directory using pytest.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_etl/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest quickstart_etl_tests\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring dagster-cloud CLI\nDESCRIPTION: Commands to install the dagster-cloud CLI and configure authentication with your Dagster+ organization. This establishes the connection needed for deployment operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/ci-cd-in-serverless.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-cloud\ndagster-cloud configure\n```\n\n----------------------------------------\n\nTITLE: Creating normalized_cereals Table in PostgreSQL\nDESCRIPTION: This SQL snippet sets up the 'normalized_cereals' table that aims to store data regarding different cereals, including various nutritional values and identifiers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: normalized_cereals; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.normalized_cereals (\n    id integer NOT NULL,\n    name character varying,\n    mfr character varying,\n    type character varying,\n    calories double precision,\n    protein double precision,\n    fat double precision,\n    sodium double precision,\n    fiber double precision,\n    carbo double precision,\n    sugars double precision,\n    potass double precision,\n    vitamins double precision,\n    shelf double precision,\n    weight double precision,\n    cups double precision,\n    rating double precision\n);\n\n\nALTER TABLE public.normalized_cereals OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Index on jobs.job_type\nDESCRIPTION: This SQL statement creates an index on the `job_type` column of the `jobs` table. This is for performance optimization when querying jobs based on their type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\"\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint from runs.snapshot_id to snapshots.snapshot_id\nDESCRIPTION: This SQL statement adds a foreign key constraint to the `runs` table, linking the `snapshot_id` column to the `snapshot_id` column in the `snapshots` table. This ensures referential integrity between runs and snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\n\"ALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\"\n```\n\n----------------------------------------\n\nTITLE: Moving Files Command\nDESCRIPTION: Command used to relocate the elt module into the defs directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmv elt defs/\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster dbt Integration Package\nDESCRIPTION: Command to install the dagster-dbt package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/dbt-core.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-dbt\n```\n\n----------------------------------------\n\nTITLE: Creating Foreign Key Constraints in PostgreSQL\nDESCRIPTION: Establishes referential integrity between related tables through foreign key constraints\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.asset_event_tags\n    ADD CONSTRAINT asset_event_tags_event_id_fkey FOREIGN KEY (event_id) REFERENCES public.event_logs(id) ON DELETE CASCADE;\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Importing File Manager Module in Python\nDESCRIPTION: Imports the file manager module from Dagster's core storage package. This module contains classes and functions for managing file operations within Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.file_manager\n```\n\n----------------------------------------\n\nTITLE: Formatting Image References in Markdown\nDESCRIPTION: Shows the correct syntax for including image references in Markdown documentation, specifying the path and alt text.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n![Highlighted Redeploy option in the dropdown menu next to a code location in Dagster+](/images/dagster-plus/deployment/code-locations/redeploy-code-location.png)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Snowflake Package\nDESCRIPTION: Command to install the dagster-snowflake Python package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/snowflake/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-snowflake\n```\n\n----------------------------------------\n\nTITLE: Adding a column in Alembic migration script for SQLite\nDESCRIPTION: This snippet shows how to create an Alembic migration script to add a new column 'foo' to the 'runs' table in SQLite. It outlines the necessary command to generate the migration and the syntax for defining the column addition operation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/alembic/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd python_modules/dagster/dagster/_core/storage/runs/sqlite/alembic; alembic revision -m 'add column foo'\n```\n\nLANGUAGE: python\nCODE:\n```\nop.add_column('runs', db.Column('foo', db.String))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: This snippet calculates the score of the fitted model on the training data and logs the output, which indicates the model's performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nscore = fit.score(X, y)\ncontext.log.info(\"Random forest model has score {}\".format(score))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster EventLogEntry for Loaded Input\nDESCRIPTION: This snippet is a Dagster EventLogEntry indicating that an input was loaded. The 'LOADED_INPUT' event shows that the input 'numbers' of step 'add_four.emit_two.add' was loaded using the input manager 'io_manager' from the output 'result' of step 'add_four.emit_two.emit_one'. It describes how the input was loaded into the step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"LoadedInputData\", \"input_name\": \"numbers\", \"manager_key\": \"io_manager\", \"upstream_output_name\": \"result\", \"upstream_step_key\": \"add_four.emit_two.emit_one\"}, \"event_type_value\": \"LOADED_INPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"add\", \"step_key\": \"add_four.emit_two.add\"}, \"message\": \"Loaded input \\\"numbers\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"add_four.emit_two.emit_one\\\"\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"add\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two.add\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"add\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two.add\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two.add - LOADED_INPUT - Loaded input \\\"numbers\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"add_four.emit_two.emit_one\\\"\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two.add\", \"timestamp\": 1640037521.9970748, \"user_message\": \"Loaded input \\\"numbers\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"add_four.emit_two.emit_one\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Server\nDESCRIPTION: Command to start the Dagster server in a second shell to complete the migration testing environment setup.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-airlift/kitchen-sink/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake run_dagster\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster Dependencies for CI Build Optimization\nDESCRIPTION: This code snippet enumerates various Dagster packages and modules that are used as dependencies in the project. The list is used to determine which parts of the CI build can be skipped when certain components are not affected by changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/daemon-test-suite/buildkite_deps.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# This file is used to declare dependencies so we can\n# conditionally skip unaffected parts of the CI build\ndagster\ndagster-graphql\ndagster-test\ndagster-aws\ndagster-pandas\ndagster-gcp\ndagster-celery\ndagster-celery-docker\ndagster-k8s\ndagster-celery-k8s\ndagster-postgres\ndagster-docker\n```\n\n----------------------------------------\n\nTITLE: Running Tests with pytest\nDESCRIPTION: Command to run tests using pytest for the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_modern_data_stack/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npytest assets_modern_data_stack_tests\n```\n\n----------------------------------------\n\nTITLE: Formatting Reference Tables in Markdown\nDESCRIPTION: Shows how to create reference tables in Markdown, including examples of complex content like line breaks and lists within table cells.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n| Key                                  | Value                                                                                                     |\n| ------------------------------------ | --------------------------------------------------------------------------------------------------------- |\n| `DAGSTER_CLOUD_DEPLOYMENT_NAME`      | The name of the Dagster+ deployment. <br/><br/> **Example:** `prod`.                                      |\n| `DAGSTER_CLOUD_IS_BRANCH_DEPLOYMENT` | `1` if the deployment is a [branch deployment](/dagster-plus/features/ci-cd/branch-deployments/index.md). |\n```\n\n----------------------------------------\n\nTITLE: Launching the Dagster Development Server\nDESCRIPTION: Command to start the Dagster development webserver, which provides a UI for interacting with the Dagster project, viewing assets, and monitoring jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/etl-pipeline-tutorial/index.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry for LOADED_INPUT Event\nDESCRIPTION: JSON representation of a Dagster event log entry for a LOADED_INPUT event. This entry documents loading the 'num' input for the 'div_two' step using the 'io_manager'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"LoadedInputData\", \"input_name\": \"num\", \"manager_key\": \"io_manager\", \"upstream_output_name\": \"result\", \"upstream_step_key\": \"int_to_float\"}, \"event_type_value\": \"LOADED_INPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"div_two\", \"step_key\": \"div_four.div_two\"}, \"message\": \"Loaded input \\\"num\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"int_to_float\\\"\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"div_four.div_two\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}}, \"step_key\": \"div_four.div_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - div_four.div_two - LOADED_INPUT - Loaded input \\\"num\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"int_to_float\\\"\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"div_four.div_two\", \"timestamp\": 1640037523.1103292, \"user_message\": \"Loaded input \\\"num\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"int_to_float\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Registry Secret in Kubernetes\nDESCRIPTION: Kubectl command to create a secret for Docker registry authentication.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create secret docker-registry regCred \\\n  --docker-server=DOCKER_REGISTRY_SERVER \\\n  --docker-username=DOCKER_USER \\\n  --docker-password=DOCKER_PASSWORD \\\n  --docker-email=DOCKER_EMAIL\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment with UV on MacOS\nDESCRIPTION: Command to create and activate a Python virtual environment using UV on MacOS. This isolates the project dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example source dagster_example/bin/activate\n```\n\n----------------------------------------\n\nTITLE: AWS SSM Parameter Store Integration Example\nDESCRIPTION: Reference to a Python example file demonstrating AWS SSM Parameter Store integration with Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/ssm.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndocs_snippets/docs_snippets/integrations/aws-ssm.py\n```\n\n----------------------------------------\n\nTITLE: Creating Hot Cereals View\nDESCRIPTION: Creates a view that filters and returns only hot cereals (type='H') from the sort_by_calories table with their nutritional information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW \"test-schema\".sort_hot_cereals_by_calories AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n  WHERE (sort_by_calories.type = 'H'::text);\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Sling Package via Pip\nDESCRIPTION: Command to install the dagster-sling Python package using pip package manager. This package provides integration between Dagster and Sling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/8-pip-add-sling.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-sling\n```\n\n----------------------------------------\n\nTITLE: DuckDB Module Import Definition\nDESCRIPTION: Module definition showing the current module context for DuckDB integration components\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-duckdb.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster_duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Dependencies with pip\nDESCRIPTION: Install the required Dagster packages using pip. This includes dagster, dagster-webserver, and dagster-aws.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-ecs-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-webserver dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Configure Kubernetes Service Account\nDESCRIPTION: YAML configuration for adding workload identity annotations and labels to the Kubernetes service account via Helm values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/blob-compute-logs.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nserviceAccount:\n  annotations:\n    azure.workload.identity/client-id: '<managed-identity-client-id>'\n\ndagsterCloudAgent:\n  labels:\n    azure.workload.identity/use: 'true'\n\nworkspace:\n  labels:\n    azure.workload.identity/use: 'true'\n```\n\n----------------------------------------\n\nTITLE: Process Completion Engine Event in Dagster (JSON)\nDESCRIPTION: JSON log entry showing the completion of steps in a Dagster pipeline process, including metadata about process ID and executed step keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80630\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['do_something.compute', 'do_input.compute']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 80630) in 168ms\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - ENGINE_EVENT - Finished steps in process (pid: 80630) in 168ms\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": null, \"timestamp\": 1610466063.712225, \"user_message\": \"Finished steps in process (pid: 80630) in 168ms\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Project with UV Environment\nDESCRIPTION: Command to scaffold a new Dagster project using the dg CLI tool. The command creates a new project in the specified directory and configures it to use a uv-managed Python environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/workspace/5-scaffold-project.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold project projects/project-2 --python-environment uv_managed\n```\n\n----------------------------------------\n\nTITLE: Logging Step Success Event in Dagster Pipeline (JSON)\nDESCRIPTION: This snippet represents a log entry for a successful step execution in a Dagster pipeline. It includes details such as execution duration, step key, and pipeline name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepSuccessData\",\n      \"duration_ms\": 26.379903996712528\n    },\n    \"event_type_value\": \"STEP_SUCCESS\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuf74gv30\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"model_pipeline\",\n      \"solid\": \"materialization_solid\",\n      \"step_key\": \"materialization_solid\"\n    },\n    \"message\": \"Finished execution of step \\\"materialization_solid\\\" in 26ms.\",\n    \"pid\": 1295,\n    \"pipeline_name\": \"model_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"materialization_solid\",\n      \"parent\": null\n    },\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"materialization_solid\",\n        \"parent\": null\n      }\n    },\n    \"step_key\": \"materialization_solid\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"model_pipeline - 1399fa66-f129-46ad-9cf9-2d528d0f87fa - 1295 - materialization_solid - STEP_SUCCESS - Finished execution of step \\\"materialization_solid\\\" in 26ms.\",\n  \"pipeline_name\": \"model_pipeline\",\n  \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\",\n  \"step_key\": \"materialization_solid\",\n  \"timestamp\": 1625760608.364062,\n  \"user_message\": \"Finished execution of step \\\"materialization_solid\\\" in 26ms.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Overview Documentation in dbt using Jinja and Markdown\nDESCRIPTION: This snippet demonstrates how to create a documentation block in dbt using the Jinja {% docs %} tag. The content is written in Markdown and provides an overview of the Jaffle Shop project, describing it as a fictional ecommerce store for testing dbt code.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_unit_tests/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Creating daemon_heartbeats Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'daemon_heartbeats' table, designed to track the heartbeats of various daemons. It includes fields for 'daemon_type', 'daemon_id', 'timestamp', and optional 'info', with specific constraints.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: daemon_heartbeats; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    info character varying\n);\n\n\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering Changelog in JSX\nDESCRIPTION: This code imports the CHANGES.md file contents and its table of contents, then renders the content. It also exports the table of contents for use in the documentation site navigation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/changelog.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Changes, {toc as ChangesTOC} from '@site/../CHANGES.md';\n\n<Changes />\n\nexport const toc = ChangesTOC;\n```\n\n----------------------------------------\n\nTITLE: Copying Data into pending_steps Table in PostgreSQL for Dagster\nDESCRIPTION: SQL statement for copying data into the pending_steps table that stores information about pending execution steps in Dagster's workflow system. The columns include step identifiers, run information, concurrency keys, and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.pending_steps (id, concurrency_key, run_id, step_key, priority, assigned_timestamp, create_timestamp) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating schedules Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'schedules' table to maintain scheduling information for jobs or tasks, incorporating various relevant identifiers and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: schedules; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.schedules (\n    id integer NOT NULL,\n    schedule_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    schedule_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.schedules OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Pipeline Engine Event Log\nDESCRIPTION: Event record for pipeline engine initialization showing process ID and step execution order. Documents the start of pipeline execution in a new process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"metadata_entries\": [{\"label\": \"pid\", \"entry_data\": {\"text\": \"18688\"}}, {\"label\": \"step_keys\", \"entry_data\": {\"text\": \"['ingest_costs', 'ingest_traffic', 'persist_costs', 'persist_traffic', 'build_cost_dashboard', 'build_model', 'build_traffic_dashboard', 'train_model', 'persist_model']\"}}]}, \"event_type_value\": \"ENGINE_EVENT\"}}\n```\n\n----------------------------------------\n\nTITLE: Documenting Jaffle Shop Overview in Markdown\nDESCRIPTION: This snippet provides an overview of the jaffle_shop dbt project, explaining its purpose as a fictional ecommerce store for testing code. It includes a link to the source code repository on GitHub.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/starlift-demo/dbt_example/shared/dbt/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Bulk Actions by Selector ID in SQL\nDESCRIPTION: Creates a B-tree index on the bulk_actions table to optimize queries filtering by selector_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions_selector_id ON public.bulk_actions USING btree (selector_id);\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Output Event in JSON\nDESCRIPTION: JSON structure of a Dagster event record showing a STEP_OUTPUT event for a 'build_cost_dashboard' step, documenting successful yielding of a result output with passing type check.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"build_cost_dashboard\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_cost_dashboard - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1609894319.013267, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Installing dg with uv\nDESCRIPTION: Command to install the dg tool globally using uv package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/scaffolding-a-project.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install dagster-dg\n```\n\n----------------------------------------\n\nTITLE: JSON Representation of Dagster Event Records with Materialization and Expectation Data\nDESCRIPTION: These JSON snippets represent Dagster event records tracking pipeline execution. Each record contains metadata about materializations (created data assets) or expectation validations (data quality checks) within a data pipeline named 'many_events'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/events.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/events.raw\\\"]]]]}}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.714868, \"user_message\": \"Materialized value table_info.\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Row count passed for events\", \"label\": \"events.row_count\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Row count passed for events\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Row count passed for events\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"events.row_count\\\", \\\"Row count passed for events\\\", []]}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.726065, \"user_message\": \"Row count passed for events\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/friends.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/friends.raw\\\"]]]]}}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.735657, \"user_message\": \"Materialized value table_info.\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Row count passed for friends\", \"label\": \"friends.row_count\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Row count passed for friends\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Row count passed for friends\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"friends.row_count\\\", \\\"Row count passed for friends\\\", []]}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.743922, \"user_message\": \"Row count passed for friends\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/pages.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/pages.raw\\\"]]]]}}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.752578, \"user_message\": \"Materialized value table_info.\"}\n```\n\n----------------------------------------\n\nTITLE: Updated Project Structure\nDESCRIPTION: Shows the project structure after moving the elt module.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmy_existing_project/\n├── definitions.py\n├── defs/\n│   └── elt/\n│       ├── __init__.py\n│       └── assets.py\n├── ml/\n│   ├── __init__.py\n│   └── assets.py\n└── viz/\n    ├── __init__.py\n    └── assets.py\n```\n\n----------------------------------------\n\nTITLE: Cleaning Airflow and Dagster run history\nDESCRIPTION: Command to clean the Airflow and Dagster run history, deleting runs from Airflow and asset materializations from Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/peer.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake clean\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests with No Cleanup Flag\nDESCRIPTION: Shell command to run integration tests with the --no-cleanup flag, which preserves the kind cluster after tests complete for faster subsequent test runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npytest --no-cleanup -s -vvv -m \"not integration\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster ENGINE_EVENT for Process Exit JSON in Python\nDESCRIPTION: JSON representation of a Dagster engine event for process exit. This event indicates that the process for executing the pipeline has exited. The record includes the process ID and pipeline context information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_60\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 80827).\", \"pid\": null, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Process for pipeline exited (pid: 80827).\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.705453, \"user_message\": \"Process for pipeline exited (pid: 80827).\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for secondary_indexes id Column\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `secondary_indexes` table, using the `public.secondary_indexes_id_seq` sequence to provide unique IDs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and dbt Dependencies\nDESCRIPTION: Command to install required packages including dagster-dbt, dagster-webserver, and dbt-duckdb using pip. These packages are necessary for running dbt with Dagster and using DuckDB as the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/index.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-dbt dagster-webserver dbt-duckdb\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Start Event in Dagster (JSON)\nDESCRIPTION: This snippet represents a log entry for the start of a Dagster pipeline execution. It includes the pipeline name, run ID, and process ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"PIPELINE_START\",\n    \"logging_tags\": {},\n    \"message\": \"Started execution of pipeline \\\"asset_pipeline\\\".\",\n    \"pid\": 3447,\n    \"pipeline_name\": \"asset_pipeline\",\n    \"solid_handle\": null,\n    \"step_handle\": null,\n    \"step_key\": null,\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"asset_pipeline - 4a832042-13f5-4b77-ab3f-24b88827402e - 3447 - PIPELINE_START - Started execution of pipeline \\\"asset_pipeline\\\".\",\n  \"pipeline_name\": \"asset_pipeline\",\n  \"run_id\": \"4a832042-13f5-4b77-ab3f-24b88827402e\",\n  \"step_key\": null,\n  \"timestamp\": 1625761044.385639,\n  \"user_message\": \"Started execution of pipeline \\\"asset_pipeline\\\".\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating daemon_heartbeats Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'daemon_heartbeats' table to track the status and timestamp of various daemons. This structure aids in monitoring the health and activity of background processes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: daemon_heartbeats; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with snapshots ID in PostgreSQL\nDESCRIPTION: Links 'snapshots_id_seq' with 'snapshots.id' for functionality that automatically handles assigning unique numeric identifiers to snapshot entries within the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\n```\n\n----------------------------------------\n\nTITLE: Step Output Event in Dagster Pipeline\nDESCRIPTION: JSON log event recording the output of a step in a Dagster pipeline. This event captures information about the yielded result from the build_cost_dashboard step, including type check information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"build_cost_dashboard\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_cost_dashboard - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1608666934.270924, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Setting PostgreSQL Configuration\nDESCRIPTION: This snippet includes various PostgreSQL SET commands to configure database behaviors such as timeouts and client encoding. These settings ensure that the subsequent SQL statements execute under controlled conditions. No inputs or outputs; the effects are on server configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n```\n\n----------------------------------------\n\nTITLE: Running Backcompatibility Tests with Tox in Bash\nDESCRIPTION: Commands to run backcompatibility tests using tox for different scenarios: latest release and earliest release.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntox user-code-latest-release\ntox user-code-earliest-release\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster SSH Integration\nDESCRIPTION: This command installs the Dagster SSH integration package, which is required for SFTP operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/sftp_file_upload.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-ssh\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Qdrant Dependencies\nDESCRIPTION: Command to install both Dagster and the Qdrant integration package via pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/qdrant.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-qdrant\n```\n\n----------------------------------------\n\nTITLE: Setting Kubernetes Config for Kind Cluster\nDESCRIPTION: Shell commands to generate and use a kubeconfig file for interacting with a kind cluster, allowing tools like kubectl and helm to work correctly with the kind cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkind get kubeconfig --name kind-test > /tmp/kubeconfig\nexport KUBECONFIG=/tmp/kubeconfig\n```\n\n----------------------------------------\n\nTITLE: Creating Index on event_logs.step_key\nDESCRIPTION: This SQL statement creates an index on the `step_key` column of the `event_logs` table. This index can speed up queries that filter or sort by step key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrent Non-Isolated Runs in Dagster+ Deployment Settings (YAML)\nDESCRIPTION: This YAML configuration enables non-isolated runs and sets the maximum number of concurrent non-isolated runs to 1. This helps prevent crashes and memory exhaustion.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/run-isolation.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nnon_isolated_runs:\n  enabled: True\n  max_concurrent_non_isolated_runs: 1\n```\n\n----------------------------------------\n\nTITLE: Declaring Entry Points for a dg Plugin in TOML\nDESCRIPTION: This code shows how to define the necessary entry point in a pyproject.toml file to register a Python package as a 'dg' plugin. The entry point maps the package to the module containing plugin objects.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-dg-plugin.md#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points]\n\"dagster_dg.plugin\" = { my_package = \"my_package\"}\n```\n\n----------------------------------------\n\nTITLE: Basic DBT Commands\nDESCRIPTION: Essential dbt CLI commands for running transformations and tests on your data models.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_modern_data_stack/dbt_project/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt run\ndbt test\n```\n\n----------------------------------------\n\nTITLE: GitLab CI Pipeline Configuration\nDESCRIPTION: Example GitLab CI configuration for container registry authentication\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nbuild-image:\n  ...\n  before_script:\n    # For GitLab Container Registry\n    - echo $CI_JOB_TOKEN | docker login --username $CI_REGISTRY_USER --password-stdin $REGISTRY_URL\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Scheduler Module in Python\nDESCRIPTION: This snippet demonstrates how to import the dagster._core.scheduler module, which contains the DagsterDaemonScheduler class.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/schedules-sensors.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.scheduler\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Dagster-Airbyte\nDESCRIPTION: Install the required Python packages for using Dagster with Airbyte Cloud.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-cloud.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-airbyte\n```\n\n----------------------------------------\n\nTITLE: Reviewing Dagster ENGINE_EVENT for Process Completion in JSON\nDESCRIPTION: This JSON snippet shows a Dagster ENGINE_EVENT that records the completion of multiple steps within a process. It includes metadata about the process ID (80827), the step keys that were executed, and the total execution time (160ms).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80827\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['do_something.compute', 'do_input.compute']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 80827) in 160ms\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - ENGINE_EVENT - Finished steps in process (pid: 80827) in 160ms\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.677631, \"user_message\": \"Finished steps in process (pid: 80827) in 160ms\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster with Celery Dependencies\nDESCRIPTION: Commands to install the required Dagster and Celery packages using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/celery.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster dagster-celery\n```\n\n----------------------------------------\n\nTITLE: Setting Page Visibility in Markdown Frontmatter\nDESCRIPTION: YAML frontmatter configuration that sets the unlisted property to true, indicating this page should not be listed in navigation or indices.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/todo.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nunlisted: true\n---\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Web Server\nDESCRIPTION: Command to start the Dagster web server for viewing and interacting with the project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dynamic_partitions/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Standard dagster-dbt Implementation with BigQuery\nDESCRIPTION: Example of a standard dbt asset configuration that targets BigQuery without Insights tracking. This shows the setup before enabling Insights integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/google-bigquery.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions\nfrom dagster_dbt import DbtCliResource, dbt_assets\n\n@dbt_assets\ndef my_dbt_assets(dbt):\n    return dbt.cli([\"build\"], manifest=dbt.get_manifest()).wait()\n\ndefs = Definitions(\n    assets=[my_dbt_assets],\n    resources={\n        \"dbt\": DbtCliResource(\n            project_dir=\"path/to/dbt_project\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Understanding Dagster Pipeline Start Event Format\nDESCRIPTION: This JSON structure represents a pipeline start event in Dagster. It marks the beginning of pipeline execution and contains information about the pipeline name, run ID, and process details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\"foo\\\".\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - PIPELINE_START - Started execution of pipeline \\\"foo\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.492956, \"user_message\": \"Started execution of pipeline \\\"foo\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Index on Asset Key in Event Logs for PostgreSQL\nDESCRIPTION: Creates an index named 'idx_asset_key' on the 'asset_key' column of 'event_logs' table to facilitate faster queries based on asset keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n```\n\n----------------------------------------\n\nTITLE: Displaying Local Development Project Structure with Single Code Location\nDESCRIPTION: This snippet demonstrates the file structure for a Dagster project with a single code location during local development. It includes optional configuration files like dagster.yaml and pyproject.toml.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/dagster-project-file-reference.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n.\n├── README.md\n├── my_dagster_project\n│   ├── __init__.py\n│   ├──  assets.py\n│   └──  definitions.py\n├── my_dagster_project_tests\n├── dagster.yaml      ## optional, used for instance settings\n├── pyproject.toml    ## optional, used to define the project as a module\n├── setup.cfg\n├── setup.py\n└── tox.ini\n```\n\n----------------------------------------\n\nTITLE: Dagster Step Execution Event Record\nDESCRIPTION: JSON event record showing step execution details including output handling and object store operations for a Dagster pipeline step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"do_something.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_something\", \"solid_definition\": \"do_something\", \"step_key\": \"do_something.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_something\", \"parent\": null}, \"step_key\": \"do_something.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_something.compute - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_something.compute\", \"timestamp\": 1610466002.801619, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Project from Example\nDESCRIPTION: Command to create a new Dagster project using the sling_decorator example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/sling_decorator/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example sling_decorator\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Template for dbt Project\nDESCRIPTION: A documentation block using dbt's documentation syntax (docs macro) that provides an overview of the Jaffle Shop project, including its purpose and source code location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_model_versions/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Uploading SAML Metadata to Dagster+ using CLI\nDESCRIPTION: This shell command uploads the SAML metadata file to Dagster+ using the dagster-cloud CLI. It requires specifying the path to the metadata file, a user token, and the Dagster+ organization URL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/pingone-sso.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud organization settings saml upload-identity-provider-metadata <path/to/metadata> \\\n  --api-token=<user_token> \\\n  --url https://<organization_name>.dagster.cloud\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster+ Serverless IP Addresses in Plain Text\nDESCRIPTION: This snippet contains a list of IP addresses used by Dagster+ Serverless for outgoing requests. These IPs may need to be whitelisted or allowlisted for services that the serverless code interacts with.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/dagster-ips.md#2025-04-22_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\n34.216.9.66\n35.162.181.243\n35.83.14.215\n44.230.239.14\n44.240.64.133\n52.34.41.163\n52.36.97.173\n52.37.188.218\n52.38.102.213\n52.39.253.102\n52.40.171.60\n52.89.191.177\n54.201.195.80\n54.68.25.27\n54.71.18.84\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster STEP_OUTPUT Event Structure in JSON\nDESCRIPTION: This JSON snippet represents a Dagster STEP_OUTPUT event which records when a solid step yields an output. It includes details about the output type, step key, pipeline name, and verification that the type check passed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"do_input.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_input.compute - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466123.653987, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx CLI Documentation for 'greet' Command\nDESCRIPTION: This reStructuredText directive configures Sphinx to generate full, nested documentation for a CLI command named 'greet' using the Click library. It specifies the command name, program name, and nesting level.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/_ext/sphinx-click/tests/roots/nested-full/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. click:: greet:greet\n   :prog: greet\n   :nested: full\n```\n\n----------------------------------------\n\nTITLE: Using PyObject Component for API Links\nDESCRIPTION: Demonstrates the updated usage of the PyObject component to create links to Python API documentation, including the new 'section' prop.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n<PyObject\n  section=\"assets\"\n  module=\"dagster\"\n  object=\"MaterializeResult\"\n/>\n```\n\n----------------------------------------\n\nTITLE: Defining In Recent Time Window Scheduling Condition in Dagster\nDESCRIPTION: This snippet outlines the in recent time window scheduling condition that checks if an asset falls into a predefined recent window of time, facilitating timely data updates and processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/declarative_automation/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nSchedulingCondition.in_recent_time_window()\n```\n\n----------------------------------------\n\nTITLE: Step Success Log for Build Model in Dagster\nDESCRIPTION: A JSON log entry recording the successful completion of the 'build_model' step in the Dagster pipeline. The log shows this step took over a minute to execute, completing in approximately 64.4 seconds.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_55\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 64417.477229988435}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Finished execution of step \\\"build_model\\\" in 1m4s.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_model - STEP_SUCCESS - Finished execution of step \\\"build_model\\\" in 1m4s.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_model\", \"timestamp\": 1608666998.762052, \"user_message\": \"Finished execution of step \\\"build_model\\\" in 1m4s.\"}\n```\n\n----------------------------------------\n\nTITLE: Activating Python Virtual Environment in jaffle-platform\nDESCRIPTION: Changes the current directory to jaffle-platform and activates its Python virtual environment using source command. This prepares the environment for running Python applications within the project's isolated environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-b-uv-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd jaffle-platform && source .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Adding Compute Kind to a Dagster Asset (Deprecated)\nDESCRIPTION: This snippet shows the deprecated method of adding a compute kind to a Dagster asset using the 'compute_kind' argument. It's recommended to use the 'kinds' argument instead.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/metadata-and-tags/kind-tags.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@asset(compute_kind=\"dbt\")\ndef my_asset():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing dg with pip\nDESCRIPTION: Command to install the dg command line tool into the project's virtual environment using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-dg\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-pipes in Dockerfile\nDESCRIPTION: Add the dagster-pipes module to the Docker image used for the ECS task. This Dockerfile installs dagster-pipes and copies the task script into the image.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-ecs-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.11-slim\n\nRUN python -m pip install dagster-pipes\n\n# copy the task script\nCOPY . .\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Asset Materialization\nDESCRIPTION: JSON event record showing the materialization of a cost_db_table asset with metadata including persist_costs value and partition information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"cost_db_table\"]}, \"description\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"FloatMetadataEntryData\", \"value\": 5315.783837199221}, \"label\": \"persist_costs\"}], \"partition\": \"2020-01-02\"}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_costs\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_costs\"}, \"message\": \"Materialized value cost_db_table.\"}}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values for Columns - SQL\nDESCRIPTION: These snippets set default values for the 'id' columns of the 'event_logs', 'run_tags', and 'runs' tables to use their respective sequences for generating new identifiers automatically.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: event_logs id; Type: DEFAULT; Schema: public; Owner: test\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\n-- Name: run_tags id; Type: DEFAULT; Schema: public; Owner: test\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\n-- Name: runs id; Type: DEFAULT; Schema: public; Owner: test\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\n```\n\n----------------------------------------\n\nTITLE: Highlighting Code in Python Examples\nDESCRIPTION: Shows how to use highlight-start and highlight-end comments to highlight specific parts of a Python code example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@dg.asset\n# highlight-start\ndef iris_dataset(iris_db: SnowflakeResource) -> None:\n    # highlight-end\n    iris_df = pd.read_csv(\"https://docs.dagster.io/assets/iris.csv\")\n```\n\n----------------------------------------\n\nTITLE: Create normalized_cereals Table\nDESCRIPTION: This SQL statement creates the `normalized_cereals` table for storing information about cereals. It includes fields for name, manufacturer, type, and various nutritional properties.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.normalized_cereals (\n    id integer NOT NULL,\n    name character varying,\n    mfr character varying,\n    type character varying,\n    calories double precision,\n    protein double precision,\n    fat double precision,\n    sodium double precision,\n    fiber double precision,\n    carbo double precision,\n    sugars double precision,\n    potass double precision,\n    vitamins double precision,\n    shelf double precision,\n    weight double precision,\n    cups double precision,\n    rating double precision\n);\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Asset Definitions Table\nDESCRIPTION: CLI output showing a table of Dagster asset definitions, including an asset named 'my_asset' in group 'my_group' with the description 'Asset that greets you.'\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/dagster-definitions/5-list-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list defs\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Section ┃ Definitions                                                     ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Assets  │ ┏━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓ │\n│         │ ┃ Key      ┃ Group    ┃ Deps ┃ Kinds ┃ Description            ┃ │\n│         │ ┡━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩ │\n│         │ │ my_asset │ my_group │      │       │ Asset that greets you. │ │\n│         │ └──────────┴──────────┴──────┴───────┴────────────────────────┘ │\n└─────────┴─────────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Yielding Computation Result with Dagstermill\nDESCRIPTION: Uses Dagstermill's yield_result method to return the computational result for further processing\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/mult_two_numbers.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndagstermill.yield_result(result)\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence secondary_indexes_id_seq in PostgreSQL\nDESCRIPTION: Defines the sequence 'secondary_indexes_id_seq' that auto-generates incremental IDs for the 'secondary_indexes' table, structuring index identities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.secondary_indexes_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Standard dbt_project.yml Configuration\nDESCRIPTION: Example of a standard dbt_project.yml configuration before adding Insights tracking capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/google-bigquery.md#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_dbt_project\nversion: 1.0.0\n\nmodels:\n  my_dbt_project:\n    materialized: view\n```\n\n----------------------------------------\n\nTITLE: Inserting Sample Data into Dagster Database Tables\nDESCRIPTION: This SQL snippet includes COPY statements for inserting data into various Dagster database tables. It's likely used for initializing the database with some default or sample data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\n5e139331e376\n\\.\n\nCOPY public.asset_keys (id, asset_key, last_materialization, last_run_id, asset_details, wipe_timestamp, last_materialization_timestamp, tags, create_timestamp) FROM stdin;\n\\.\n\nCOPY public.bulk_actions (id, key, status, \"timestamp\", body, action_type, selector_id) FROM stdin;\n\\.\n\nCOPY public.daemon_heartbeats (daemon_type, daemon_id, \"timestamp\", body) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for secondary_indexes ID in PostgreSQL\nDESCRIPTION: Uses 'secondary_indexes_id_seq' to assign default ID for 'secondary_indexes', ensuring seamless and unique ID allocation to manage database indexing efficiently.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Markdown Code Block in Metadata Entry\nDESCRIPTION: A markdown code block example embedded within Dagster's event metadata. The code block shows a simple commented code example with three lines of code, used to illustrate markdown formatting capabilities in Dagster's materialization records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: markdown\nCODE:\n```\n    // comments\n    line 1 of code\n    line 2 of code\n    line 3 of code\n```\n\n----------------------------------------\n\nTITLE: Scheduler Reset Command\nDESCRIPTION: Terminal command to reset and reinitialize the scheduler after upgrading\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\ndagster schedule wipe && dagster schedule up\n```\n\n----------------------------------------\n\nTITLE: Running the Dagster Web Server\nDESCRIPTION: Command to start the Dagster web interface for viewing and interacting with the example project. This launches the UI where users can explore the assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Cloning Dagster Repository and Navigating to Project Directory\nDESCRIPTION: Clone the Dagster repository and navigate to the specific project directory for the RAG example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/docs_projects/project_ask_ai_dagster\n```\n\n----------------------------------------\n\nTITLE: Running Tests for ATProto Dashboard\nDESCRIPTION: This command runs the project's unit tests using pytest. The tests are located in the project_atproto_dashboard_tests directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_atproto_dashboard/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest project_atproto_dashboard_tests\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint for Runs and Snapshots in PostgreSQL for Dagster\nDESCRIPTION: Adds a foreign key constraint between the runs and snapshots tables, ensuring that each run references a valid snapshot ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_61\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Running Airflow DAG backfill\nDESCRIPTION: Command to initiate a backfill run of the 'rebuild_customers_list' DAG in Airflow.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/peer.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags backfill rebuild_customers_list --start-date $(shell date +\"%Y-%m-%d\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Task Resources and Failure Handling\nDESCRIPTION: Parameters that control task resource allocation and how many times a task can fail before the job is aborted.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_43\n\nLANGUAGE: properties\nCODE:\n```\nspark.task.cpus\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.task.maxFailures\n```\n\n----------------------------------------\n\nTITLE: Creating runs_id_seq Sequence - SQL\nDESCRIPTION: This snippet creates a sequence 'runs_id_seq' to auto-generate unique IDs for the 'runs' table, ensuring each execution is uniquely identifiable.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: runs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.runs_id_seq OWNER TO test;\n\n```\n\n----------------------------------------\n\nTITLE: Postgres SQL Error in Asset Materialization\nDESCRIPTION: SQL query that failed due to a missing column 'last_materialization_timestamp' in the 'asset_keys' table of Postgres event log storage. This error triggered a DagsterInstanceMigrationRequired exception.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO asset_keys (asset_key, last_materialization, last_run_id, last_materialization_timestamp, tags) VALUES (%(asset_key)s, %(last_materialization)s, %(last_run_id)s, %(last_materialization_timestamp)s, %(tags)s) ON CONFLICT (asset_key) DO UPDATE SET last_materialization = %(param_1)s, last_run_id = %(param_2)s, last_materialization_timestamp = %(param_3)s, tags = %(param_4)s RETURNING asset_keys.id\n```\n\n----------------------------------------\n\nTITLE: Defining an Empty Component Type in Python\nDESCRIPTION: This code snippet creates a basic custom component type called 'EmptyComponent' using the ComponentType class. It includes a required build method that returns an empty dictionary.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-dg-plugin.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_dg.core.component_type import ComponentType\n\n\nclass EmptyComponent(ComponentType):\n    \"\"\"A component type that does nothing.\"\"\"\n\n    @classmethod\n    def build(cls, context):\n        \"\"\"Build an empty component instance.\"\"\"\n        return {}\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Event Logs Table in SQL\nDESCRIPTION: Inserts detailed event log entries into the 'event_logs' table, including run IDs, event types, timestamps, and JSON-formatted event data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key, asset_key) FROM stdin;\n1624\t15d2d9b4-8c36-4c00-a9c5-8b233d673161\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"cost_db_table\"]}, \"description\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"FloatMetadataEntryData\", \"value\": 7000.0}, \"label\": \"persist_costs\"}], \"partition\": \"2020-12-08\"}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_costs\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_costs\"}, \"message\": \"Materialized value cost_db_table.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_costs\", \"parent\": null}, \"step_key\": \"persist_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_costs - STEP_MATERIALIZATION - Materialized value cost_db_table.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_costs\", \"timestamp\": 1608666924.419132, \"user_message\": \"Materialized value cost_db_table.\"}\tSTEP_MATERIALIZATION\t2020-12-22 19:55:24.419132\tpersist_costs\t[\"cost_db_table\"]\n1656\t15d2d9b4-8c36-4c00-a9c5-8b233d673161\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 523.1808519747574}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_traffic_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_traffic_dashboard\"}, \"message\": \"Finished execution of step \\\"build_traffic_dashboard\\\" in 523ms.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_traffic_dashboard\", \"parent\": null}, \"step_key\": \"build_traffic_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_traffic_dashboard - STEP_SUCCESS - Finished execution of step \\\"build_traffic_dashboard\\\" in 523ms.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_traffic_dashboard\", \"timestamp\": 1608666999.33118, \"user_message\": \"Finished execution of step \\\"build_traffic_dashboard\\\" in 523ms.\"}\tSTEP_SUCCESS\t2020-12-22 19:56:39.33118\tbuild_traffic_dashboard\t\\N\n1625\t15d2d9b4-8c36-4c00-a9c5-8b233d673161\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"persist_costs\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_costs\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_costs\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_costs\", \"parent\": null}, \"step_key\": \"persist_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_costs - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_costs\", \"timestamp\": 1608666924.429594, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\tSTEP_OUTPUT\t2020-12-22 19:55:24.429594\tpersist_costs\t\\N\n1657\t15d2d9b4-8c36-4c00-a9c5-8b233d673161\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"train_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"train_model\"}, \"message\": \"Started execution of step \\\"train_model\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"train_model\", \"parent\": null}, \"step_key\": \"train_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - train_model - STEP_START - Started execution of step \\\"train_model\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"train_model\", \"timestamp\": 1608666999.3543732, \"user_message\": \"Started execution of step \\\"train_model\\\".\"}\tSTEP_START\t2020-12-22 19:56:39.354373\ttrain_model\t\\N\n\\.\n```\n\n----------------------------------------\n\nTITLE: Completing Resource Initialization in Dagster Pipeline\nDESCRIPTION: JSON log entry showing successful initialization of the InMemoryAssetStore resource for the 'object_manager', including initialization time metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"resources\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": \"Initialized in 0.01ms\", \"entry_data\": {\"__class__\": \"PythonArtifactMetadataEntryData\", \"module\": \"dagster.core.storage.asset_store\", \"name\": \"InMemoryAssetStore\"}, \"label\": \"object_manager\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished initialization of resources [object_manager].\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - ENGINE_EVENT - Finished initialization of resources [object_manager].\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608666901.049406, \"user_message\": \"Finished initialization of resources [object_manager].\"}\n```\n\n----------------------------------------\n\nTITLE: Creating alembic_version Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'alembic_version' table which is used to track database schema versions. It has a single column 'version_num' of type 'character varying'. The table is owned by the user 'test'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\n\nALTER TABLE public.alembic_version OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraints - PostgreSQL\nDESCRIPTION: SQL statements that add unique constraints to ensure uniqueness of specific columns like job_origin_id, run_id, and schedule_origin_id across various tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.schedules\n    ADD CONSTRAINT schedules_schedule_origin_id_key UNIQUE (schedule_origin_id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to run_tags Table in SQL\nDESCRIPTION: Adds a primary key constraint on the 'id' column of the run_tags table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Building Dagster API Documentation\nDESCRIPTION: Commands to build and serve the Dagster API documentation locally. Must be run from the docs directory after following the installation steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn build-api-docs\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn start\n```\n\n----------------------------------------\n\nTITLE: Referencing Dagster Pipes Documentation in Markdown\nDESCRIPTION: This snippet provides a markdown link to the official documentation for the dagster-pipes library. It directs users to the appropriate section of the Dagster documentation for detailed information about the library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster-pipes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-pipes\n\nThe docs for `dagster-pipes` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-pipes).\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Census Integration in Python\nDESCRIPTION: This command installs the dagster-census package using pip, which is required to use Census functionality within Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/census.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-census\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports essential Python libraries for data analysis and visualization including matplotlib, pandas, and seaborn.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/tutorial_template/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Dagster Cloud Authentication\nDESCRIPTION: Bash commands to set environment variables for authenticating with Dagster Cloud, including the organization name and API token.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport  DAGSTER_CLOUD_ORGANIZATION=\"organization-name\" # if your URL is https://acme.dagster.plus your organization name is \"acme\"\nexport  DAGSTER_CLOUD_API_TOKEN=\"your-token\"\n```\n\n----------------------------------------\n\nTITLE: Alter snapshots_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `snapshots_id_seq` sequence to the `id` column of the `snapshots` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\"\n```\n\n----------------------------------------\n\nTITLE: Creating Index on event_logs.dagster_event_type, id\nDESCRIPTION: This SQL statement creates a composite index on the `dagster_event_type` and `id` columns of the `event_logs` table.  This index improves query speed for queries that filter by event type and ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_event_type ON public.event_logs USING btree (dagster_event_type, id);\"\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Run Module in Python\nDESCRIPTION: Imports the Dagster run module from Dagster's core storage package. This module contains classes related to individual Dagster run records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.dagster_run\n```\n\n----------------------------------------\n\nTITLE: Loading Definitions from Package Name\nDESCRIPTION: Function to load Dagster definitions using a package name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/definitions.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nload_definitions_from_package_name\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for normalized_cereals.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `normalized_cereals` table to be generated by the `normalized_cereals_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_54\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.normalized_cereals ALTER COLUMN id SET DEFAULT nextval('public.normalized_cereals_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Command to start the Dagster UI web server for local development and monitoring of the ETL pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_snowflake/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Removed Configuration in dagster.yaml for Sensor Settings\nDESCRIPTION: Example of removed sensor daemon interval configuration in dagster.yaml. In Dagster 0.10.0, the sensor daemon now runs in a continuous loop, making this configuration obsolete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\n# removed, no longer a valid setting in dagster.yaml\n\nsensor_settings:\n  interval_seconds: 10\n```\n\n----------------------------------------\n\nTITLE: Running Local Airflow Instance\nDESCRIPTION: Command to start the Airflow Web UI with the necessary environment variables, allowing access to the UI at localhost:8080.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/setup.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_run\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Dagster Project\nDESCRIPTION: This snippet enumerates the required Python packages for the Dagster project. It includes LangChain components, Dagster core and webserver, and various utility libraries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_ask_ai_dagster/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nlangchain\nlangchain-core\ngql\nlangchain-community\nlangchain-openai\ndagster\ndagster-webserver\ntenacity\nbs4\nlxml\ndagster-openai\npinecone\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry in JSON\nDESCRIPTION: This snippet represents a single event log entry in the Dagster pipeline execution. It includes details such as the event type, timestamp, and related metadata for pipeline and step execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"STEP_START\",\n    \"logging_tags\": {\n      \"job_name\": \"basic_assets_job\",\n      \"op_name\": \"basic_asset_1\",\n      \"resource_fn_name\": \"None\",\n      \"resource_name\": \"None\",\n      \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\",\n      \"step_key\": \"basic_asset_1\"\n    },\n    \"message\": \"Started execution of step \\\"basic_asset_1\\\".\",\n    \"pid\": 3691,\n    \"pipeline_name\": \"basic_assets_job\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"basic_asset_1\",\n      \"parent\": null\n    },\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"key\": \"basic_asset_1\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"basic_asset_1\",\n        \"parent\": null\n      }\n    },\n    \"step_key\": \"basic_asset_1\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"\",\n  \"pipeline_name\": \"basic_assets_job\",\n  \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\",\n  \"step_key\": \"basic_asset_1\",\n  \"timestamp\": 1731664851.946173,\n  \"user_message\": \"Started execution of step \\\"basic_asset_1\\\".\"\n}\n```\n\n----------------------------------------\n\nTITLE: Foreign Key Constraints for Dagster Tables\nDESCRIPTION: SQL ALTER TABLE statements that establish foreign key relationships between Dagster tables, linking runs to snapshots and run tags to runs with cascade delete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster+ Agent IP Addresses\nDESCRIPTION: List of IP addresses that the Dagster+ agent uses for network communication. These addresses are used for interaction with AWS Cloudfront CDN services. Note that this list may be updated over time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-ips.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n34.215.239.157\n35.165.239.109\n35.83.161.124\n44.236.154.129\n44.236.31.202\n44.239.93.251\n54.185.29.42\n54.188.126.120\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with run_tags ID in PostgreSQL\nDESCRIPTION: Links 'run_tags_id_seq' sequence to the ID column of the 'run_tags' table, ensuring automatic assignment of unique IDs for each new run tag.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Locally with Make\nDESCRIPTION: This bash command starts the Dagster server in a local environment. It is dependent on the successful completion of the development setup commands. Running 'make run_dagster' launches the server, allowing local testing of migration scenarios.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/kitchen-sink/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake run_dagster\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Executable for PySpark Driver - Configuration\nDESCRIPTION: This property specifies the Python binary to use for the PySpark driver. By default, it uses the Python binary defined in 'spark.pyspark.python'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\nspark.pyspark.driver.python\n```\n\n----------------------------------------\n\nTITLE: Yielding Events in Dagstermill Notebook\nDESCRIPTION: Demonstrates how to yield Dagster events from a notebook using yield_event and AssetMaterialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/reference.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\nfrom dagster import AssetMaterialization\n\ndagstermill.yield_event(AssetMaterialization(asset_key=\"marketing_data_plotted\"))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster STEP_START Event\nDESCRIPTION: This JSON snippet represents a Dagster event log entry indicating the start of a step named 'ingest_costs' within the 'longitudinal_pipeline'. It includes details such as the pipeline name, run ID, step key, timestamp, and a user message. The logging_tags provides additional context, including the GRPC host and socket, partition, partition set, and solid selection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_costs\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_costs\"}, \"message\": \"Started execution of step \\\"ingest_costs\\\".\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_costs\", \"parent\": null}, \"step_key\": \"ingest_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ingest_costs - STEP_START - Started execution of step \\\"ingest_costs\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"ingest_costs\", \"timestamp\": 1609894307.2248409, \"user_message\": \"Started execution of step \\\"ingest_costs\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Selecting Root Assets Within a Group in Dagster\nDESCRIPTION: Illustrates how to select root assets within the 'public_data' group. This syntax uses the 'roots' function with a group selector.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nroots(group:\"public_data\")\n```\n\nLANGUAGE: python\nCODE:\n```\nroots_within_public_data_job = define_asset_job(\n    name=\"roots_within_public_data_job\", selection='roots(group:\"public_data\")'\n)\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster asset list --select 'roots(group:\"public_data\")'\ndagster asset materialize --select 'roots(group:\"public_data\")'\n```\n\n----------------------------------------\n\nTITLE: Installing UV on Mac using Homebrew\nDESCRIPTION: This command uses Homebrew to install the UV package manager on macOS systems.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_InstallUv.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbrew install uv\n```\n\n----------------------------------------\n\nTITLE: Navigating to Project Directory in Shell\nDESCRIPTION: Changes the current working directory to the newly created project folder using the cd command in the shell.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/set-up-dbt-project.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd tutorial-dbt-dagster\n```\n\n----------------------------------------\n\nTITLE: Object Store Operation Log for Ingest Costs in Dagster\nDESCRIPTION: A JSON log entry showing an object store operation in Dagster for the 'ingest_costs' step. The log indicates that the step output was stored in the filesystem object store using pickle serialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/ingest_costs/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/ingest_costs/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_costs\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_costs\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_costs\", \"parent\": null}, \"step_key\": \"ingest_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - ingest_costs - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"ingest_costs\", \"timestamp\": 1608666908.115123, \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Activating Python Virtual Environment with Shell Command\nDESCRIPTION: This command activates a Python virtual environment located in the .venv directory. It loads the virtual environment's settings and makes its binaries and packages available in the current shell session.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-c-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Bulk Actions in PostgreSQL\nDESCRIPTION: Establishes a primary key constraint on the 'id' column of the 'bulk_actions' table to enforce uniqueness within the records managed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Running Docker Agent with Registry Credentials\nDESCRIPTION: Docker run command that starts the Dagster agent with Docker registry credentials. It mounts the Docker config file containing credential helpers, allowing the agent to authenticate with private registries like ECR or GCR.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/docker/setup.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n  --network=dagster_cloud_agent \\\n  --volume $PWD/dagster.yaml:/opt/dagster/app/dagster.yaml:ro \\\n  --volume /var/run/docker.sock:/var/run/docker.sock \\\n  --volume ~/.docker/config.json:/root/.docker/config.json:ro \\\n  -it docker.io/dagster/dagster-cloud-agent:latest \\\n  dagster-cloud agent run /opt/dagster/app\n```\n\n----------------------------------------\n\nTITLE: Logging Step Output in Dagster Pipeline\nDESCRIPTION: Records a step output event for the 'persist_traffic' step in the longitudinal_pipeline. The output named 'result' of type 'Any' is yielded and passes the type check.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepOutputData\",\n      \"intermediate_materialization\": null,\n      \"step_output_handle\": {\n        \"__class__\": \"StepOutputHandle\",\n        \"mapping_key\": null,\n        \"output_name\": \"result\",\n        \"step_key\": \"persist_traffic\"\n      },\n      \"type_check_data\": {\n        \"__class__\": \"TypeCheckData\",\n        \"description\": null,\n        \"label\": \"result\",\n        \"metadata_entries\": [],\n        \"success\": true\n      },\n      \"version\": null\n    },\n    \"event_type_value\": \"STEP_OUTPUT\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"persist_traffic\",\n      \"solid_definition\": \"base_one_input\",\n      \"step_key\": \"persist_traffic\"\n    },\n    \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\",\n    \"pid\": 18688,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"persist_traffic\",\n      \"parent\": null\n    },\n    \"step_key\": \"persist_traffic\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_traffic - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\",\n  \"step_key\": \"persist_traffic\",\n  \"timestamp\": 1608666933.7016058,\n  \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"\n}\n```\n\n----------------------------------------\n\nTITLE: Run Sequence Management\nDESCRIPTION: Configures 'runs_id_seq' to generate IDs for each entry in 'runs'. It auto-increments and is essential for keeping track of individual pipeline run entries. The sequence is linked to the 'id' column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n\n\nALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\n```\n\n----------------------------------------\n\nTITLE: Initializing Python String Literal\nDESCRIPTION: A basic Python string that outputs the classic 'Hello, world!' greeting. Used for demonstration or testing purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/retroactive.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Creating Step Key Index on Event Logs\nDESCRIPTION: Creates a btree index on the step_key column of the event_logs table for faster lookups.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_66\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n```\n\n----------------------------------------\n\nTITLE: Pipeline Process Start Engine Event in Dagster (JSON)\nDESCRIPTION: JSON log entry showing the start of a new Dagster pipeline process with a unique PID, marking the beginning of pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80827\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 80827).\", \"pid\": null, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 80827).\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.454457, \"user_message\": \"Started process for pipeline (pid: 80827).\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for event_logs ID in PostgreSQL\nDESCRIPTION: Sets 'event_logs_id_seq' as the default for ID columns in 'event_logs', ensuring each log entry receives a unique identifier automatically, maintaining orderly logging.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Loading Definitions from Package Module\nDESCRIPTION: Function to load Dagster definitions from a package module.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/definitions.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nload_definitions_from_package_module\n```\n\n----------------------------------------\n\nTITLE: Cloning dbt Sample Project in Shell\nDESCRIPTION: Downloads the Jaffle Shop sample dbt project from GitHub using the git clone command in the shell.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/set-up-dbt-project.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/dbt-labs/jaffle_shop.git\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment - MacOS\nDESCRIPTION: Commands to create and activate a Python virtual environment using uv on MacOS.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example source dagster_example/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Creating Table schedules in PostgreSQL\nDESCRIPTION: Defines the 'schedules' table for storing details about scheduled jobs, identifies them by unique schedule IDs, and records their status and origins for efficient schedule monitoring in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.schedules (\n    id integer NOT NULL,\n    schedule_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    schedule_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.schedules OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Defining Ivy Settings for Spark Jars - Configuration\nDESCRIPTION: This configuration specifies the path to an Ivy settings file that customizes the resolution of Spark jars defined by 'spark.jars.packages'. It helps to resolve artifacts through additional repositories, useful in environments such as behind firewalls.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\nspark.jars.ivySettings\n```\n\n----------------------------------------\n\nTITLE: Starting Step Execution in Dagster Pipeline\nDESCRIPTION: JSON log entry recording the start of execution for the 'build_model' step, which uses the 'base_two_inputs' solid definition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Started execution of step \\\"build_model\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_model - STEP_START - Started execution of step \\\"build_model\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_model\", \"timestamp\": 1608666934.306417, \"user_message\": \"Started execution of step \\\"build_model\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Extending Multiple ESLint Configurations\nDESCRIPTION: This snippet shows how to extend multiple ESLint configurations, including @dagster-io/eslint-config. It demonstrates the correct order when using multiple configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/eslint-config/README.md#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// .eslintrc.js\nmodule.exports = {\n  extends: ['some-other-config', '@dagster-io/eslint-config'],\n};\n```\n\n----------------------------------------\n\nTITLE: Documenting Order Statuses in Markdown\nDESCRIPTION: This snippet provides a detailed explanation of different order statuses using a markdown table. It defines five statuses: placed, shipped, completed, return_pending, and returned, along with their descriptions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/starlift-demo/dbt_example/shared/dbt/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Registration Attempts for Shuffle Service - Shuffle Settings\nDESCRIPTION: This configuration limits the number of retry attempts for successfully registering with the external shuffle service.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_30\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.registration.maxAttempts\n```\n\n----------------------------------------\n\nTITLE: Creating Sample CSV Data\nDESCRIPTION: Sample CSV data to be used as input for the ETL pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nid,name,age,city\n1,Alice,28,New York\n2,Bob,35,San Francisco\n3,Charlie,42,Chicago\n4,Diana,31,Los Angeles\n```\n\n----------------------------------------\n\nTITLE: Creating jobs Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'jobs' table, which serves to hold information regarding scheduled jobs. It includes fields for job identifiers, status, and timestamps for creation and updating.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: jobs; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.jobs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraints to Dagster Database Tables\nDESCRIPTION: SQL commands to add foreign key constraints between Dagster tables. Creates relationships between runs and snapshots tables, and between run_tags and runs tables with cascade delete for the latter.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Dependencies\nDESCRIPTION: Command to install Dagster and its development dependencies using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/prompt-engineering/index.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log Entry - Asset Materialization\nDESCRIPTION: JSON log entry documenting the materialization of an asset from the 'basic_asset_1' step with versioning metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"asset_lineage\": [], \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"basic_asset_1\"]}, \"description\": null, \"metadata_entries\": [], \"partition\": null, \"tags\": {\"dagster/code_version\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"dagster/data_version\": \"6c9d21d97b078a4cedd1fdc1d1d3991dde75605279a2115ee2c1deea25d89d95\"}}}, \"event_type_value\": \"ASSET_MATERIALIZATION\"}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"basic_assets_job\", \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"step_key\": \"basic_asset_1\", \"timestamp\": 1731664851.966126, \"user_message\": \"Materialized value basic_asset_1.\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to runs Table in SQL\nDESCRIPTION: Adds a primary key constraint on the id column and a unique constraint on the run_id column of the runs table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Project with Great Expectations Example\nDESCRIPTION: This command creates a new Dagster project named `my-dagster-project` based on the `with_great_expectations` example. This provides a pre-configured setup for using Dagster with Great Expectations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_great_expectations/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster project from-example --name my-dagster-project --example with_great_expectations\n```\n\n----------------------------------------\n\nTITLE: SQL Table Creation with Serialized Binary Data\nDESCRIPTION: Contains SQL commands for inserting serialized binary data into PostgreSQL tables. The data includes encoded pipeline and execution plan information used by Dagster for workflow orchestration. The binary data is stored in a compressed format using PostgreSQL's native binary data support.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: sql\nCODE:\n```\n5\t080905473a698ce74ecaadcf7cc3c91a3db65cb3\t\\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\n```\n\n----------------------------------------\n\nTITLE: Setting PostgreSQL Configuration\nDESCRIPTION: Configures PostgreSQL session settings for this dump. Ensures proper environment setup by disabling timeouts and enabling UTF8 client encoding. These configurations are typical prerequisites for database operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\nSET default_tablespace = '';\n```\n\n----------------------------------------\n\nTITLE: Changing Directory and Activating Python Virtual Environment in Shell\nDESCRIPTION: This command changes the current directory to 'my-project' and then activates the Python virtual environment located in the '.venv' directory. It uses the 'source' command to execute the activation script.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-b-uv-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncd my-project && source .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Importing dagstermill in Python\nDESCRIPTION: This snippet imports the dagstermill library, which is required for interfacing between Dagster and Jupyter notebooks. No additional parameters are required, and this import is necessary for executing further dagstermill operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_output.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Using Makefile Commands for Snippet Management\nDESCRIPTION: Convenience commands available through the Makefile in the /docs directory for regenerating CLI snippets and running tests.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets_tests/snippet_checks/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Regenerate CLI snippets\nmake regenerate_cli_snippets\n\n# Regenerate CLI snippets and immediately run the tests\nmake regenerate_cli_snippets_and_test\n```\n\n----------------------------------------\n\nTITLE: Two-Column Numeric Data Table in Tab-Separated Format\nDESCRIPTION: A tab-separated data table containing two columns labeled num1 and num2, with two rows of integer values: 1,2 and 3,4.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-pandas/dagster_pandas_tests/num_table.txt#2025-04-22_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nnum1\tnum2\n1\t2\n3\t4\n```\n\n----------------------------------------\n\nTITLE: Creating DLT Sources Directory\nDESCRIPTION: Bash command to create a directory for DLT sources in the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd $DAGSTER_HOME && mkdir dlt_sources\n```\n\n----------------------------------------\n\nTITLE: Loading S&P 500 Price Data in Python\nDESCRIPTION: This snippet loads S&P 500 price data using a custom function from the bollinger library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/notebooks/bollinger.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nPRICES = bol.load_sp500_prices()\n```\n\n----------------------------------------\n\nTITLE: Starting Development UI with Yarn\nDESCRIPTION: This command sets the environment variable 'NEXT_PUBLIC_BACKEND_ORIGIN' to point to the local Dagster webserver and then starts the Yarn package manager to run the development interface. This allows for easier UI development and testing with live data from the Dagster backend.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster-webserver/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nNEXT_PUBLIC_BACKEND_ORIGIN=\"http://localhost:3333\" yarn start\n```\n\n----------------------------------------\n\nTITLE: Selecting Sink Assets Within a Group in Dagster\nDESCRIPTION: Shows how to select sink assets within the 'public_data' group. This syntax uses the 'sinks' function with a group selector.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nsinks(group:\"public_data\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsinks_within_public_data_job = define_asset_job(\n    name=\"sinks_within_public_data_job\", selection='sinks(group:\"public_data\")'\n)\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster asset list --select 'sinks(group:\"public_data\")'\ndagster asset materialize --select 'sinks(group:\"public_data\")'\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster definitions with PipesGlueClient\nDESCRIPTION: Add the PipesGlueClient resource to the Dagster Definitions object to enable launching AWS Glue jobs from Dagster assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-glue-pipeline.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[glue_pipes_asset],\n    resources={\n        \"glue_client\": PipesGlueClient(),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Suspending Teradata Compute Cluster with dagster-teradata\nDESCRIPTION: This operation temporarily suspends a compute cluster in Teradata VantageCloud Lake, reducing resource consumption while retaining the compute profile for future use. It can be integrated into Dagster workflows for automated resource management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsuspend_teradata_compute_cluster(\n    compute_profile_name: str,\n    compute_group_name: str,\n    timeout: int = constants.CC_OPR_TIME_OUT\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Root Storage Module in Python\nDESCRIPTION: Imports the root storage module from Dagster's core storage package. This module contains classes for managing local artifact storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.root\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Looker Dependencies\nDESCRIPTION: Command to install the required Dagster and Looker Python packages using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/looker.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-looker\n```\n\n----------------------------------------\n\nTITLE: Alter alembic_version Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `alembic_version` table to `test`.  This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.alembic_version OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for Airflow\nDESCRIPTION: Command to install the Python packages required for running the local Airflow instance in the tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/setup.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_install\n```\n\n----------------------------------------\n\nTITLE: Altering Table Owner for Cold Cereals View\nDESCRIPTION: This SQL statement changes the owner of the `sort_cold_cereals_by_calories` view within the `test-schema` to the `test` user. This is part of database administration for managing permissions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE \"test-schema\".sort_cold_cereals_by_calories OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Tree Structure\nDESCRIPTION: File system tree output showing the hierarchical structure of a Dagster project with analytics and ELT modules, including Python package files and configuration\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-definitions/1-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── README.md\n├── my_existing_project\n│   ├── __init__.py\n│   ├── analytics\n│   │   ├── __init__.py\n│   │   ├── assets.py\n│   │   └── jobs.py\n│   ├── definitions.py\n│   ├── defs\n│   │   └── __init__.py\n│   └── elt\n│       ├── __init__.py\n│       ├── assets.py\n│       └── jobs.py\n└── pyproject.toml\n\n5 directories, 11 files\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for GCP Operation References\nDESCRIPTION: ReStructuredText documentation defining BigQuery operations and data freshness functionality references.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-gcp.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\nBigQuery Ops\n^^^^^^^^^^^^^^^\n\n.. autofunction:: bq_create_dataset\n\n.. autofunction:: bq_delete_dataset\n\n.. autofunction:: bq_op_for_queries\n\n.. autofunction:: import_df_to_bq\n\n.. autofunction:: import_file_to_bq\n\n.. autofunction:: import_gcs_paths_to_bq\n\n\nData Freshness\n^^^^^^^^^^^^^^\n\n.. autofunction:: fetch_last_updated_timestamps\n\nOther\n^^^^^^^\n\n.. autoclass:: BigQueryError\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for Dagster Tables in SQL\nDESCRIPTION: Creates various indexes on different tables to optimize query performance for common operations in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_asset_event_tags ON public.asset_event_tags USING btree (asset_key, key, value);\n\nCREATE INDEX idx_asset_event_tags_event_id ON public.asset_event_tags USING btree (event_id);\n\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\n\nCREATE INDEX idx_bulk_actions_action_type ON public.bulk_actions USING btree (action_type);\n\nCREATE INDEX idx_bulk_actions_selector_id ON public.bulk_actions USING btree (selector_id);\n\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n\nCREATE INDEX idx_event_type ON public.event_logs USING btree (dagster_event_type, id);\n\nCREATE INDEX idx_events_by_asset ON public.event_logs USING btree (asset_key, dagster_event_type, id) WHERE (asset_key IS NOT NULL);\n\nCREATE INDEX idx_events_by_asset_partition ON public.event_logs USING btree (asset_key, dagster_event_type, partition, id) WHERE ((asset_key IS NOT NULL) AND (partition IS NOT NULL));\n\nCREATE INDEX idx_events_by_run_id ON public.event_logs USING btree (run_id, id);\n\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n\nCREATE UNIQUE INDEX idx_kvs_keys_unique ON public.kvs USING btree (key);\n\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\n\nCREATE INDEX idx_run_range ON public.runs USING btree (status, update_timestamp, create_timestamp);\n\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n\nCREATE INDEX idx_tick_selector_timestamp ON public.job_ticks USING btree (selector_id, \"timestamp\");\n\nCREATE INDEX ix_instigators_instigator_type ON public.instigators USING btree (instigator_type);\n\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Delta Lake IO Manager Configuration\nDESCRIPTION: Configuration definition for the DeltaLakeIOManager class which handles Delta Lake I/O operations in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: DeltaLakeIOManager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Creating Collapsible Prerequisites Section in HTML\nDESCRIPTION: Demonstrates how to create a collapsible prerequisites section using HTML details and summary tags in Markdown documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n<details>\n  <summary>Prerequisites</summary>\n\n  - Install this\n  - Configure that\n\n</details>\n```\n\n----------------------------------------\n\nTITLE: Creating Index on Asset Partition in PostgreSQL\nDESCRIPTION: Establishes an index for optimized querying on asset partition, thereby enhancing performance on the related data access within PostgreSQL.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_partition);\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-airlift with dbt support\nDESCRIPTION: Command to install the dbt extra of dagster-airlift using uv pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/observe.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install 'dagster-airlift[dbt]'\n```\n\n----------------------------------------\n\nTITLE: Importing Job Components in Python\nDESCRIPTION: Shows the module imports for working with Dagster Jobs and their associated components. These components are used to create and manage executable job definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/jobs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, JobDefinition, reconstructable, build_reconstructable_job\n```\n\n----------------------------------------\n\nTITLE: Defining an Airflow BashOperator Example\nDESCRIPTION: Example of defining a BashOperator in Airflow to execute a bash command as part of a data pipeline. This shows the standard pattern of creating a BashOperator with a task_id and a bash command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/bash-operator-general.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id=\"print_date\",\n    bash_command=\"echo \\\"{{ execution_date }}\\\" > /tmp/execution_date.txt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration Package\nDESCRIPTION: Command to install the dagster-aws package which provides AWS Systems Manager Parameter Store integration capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/ssm.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with asset_keys ID in PostgreSQL\nDESCRIPTION: Links the sequence 'asset_keys_id_seq' to the ID column of the 'asset_keys' table, ensuring each new row receives a unique ID. This constraint maintains database integrity for the asset_keys table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n```\n\n----------------------------------------\n\nTITLE: Commenting Dagster Documentation Component Library Purpose\nDESCRIPTION: This code snippet is a markdown comment explaining the purpose of the dg docs component library. It outlines that the library contains components for use in both the standalone 'dg docs' site and within the Dagster application's documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/dg-docs-components/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dg docs Component library\n\nThis library contains components that can be used for the `dg docs` standalone site or for documentation within the Dagster app.\n```\n\n----------------------------------------\n\nTITLE: Cloning Basic dbt Project Repository\nDESCRIPTION: This command clones a basic dbt project from GitHub, which includes models and a DuckDB backend. It's the starting point for the integration example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/transform-dbt.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/dagster-io/basic-dbt-project\n```\n\n----------------------------------------\n\nTITLE: Azure File Manager Documentation\nDESCRIPTION: ReStructuredText documentation for ADLS2 file management components including file manager and file handle implementations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-azure.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: dagster_azure.adls2.adls2_file_manager\n  :annotation: ResourceDefinition\n\n.. autoclass:: dagster_azure.adls2.ADLS2FileHandle\n  :members:\n```\n\n----------------------------------------\n\nTITLE: Repository File Reference\nDESCRIPTION: Reference to the repository configuration file containing daily job definition\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_aws/README.md#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nquickstart_aws/repository.py\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Examples Directory Content in Markdown\nDESCRIPTION: This Markdown snippet provides an overview of the Dagster examples directory, explaining its purpose and the nature of the projects contained within. It also mentions the maintenance status of different examples and suggests a larger project for reference.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Examples\n\nThis directory contains some example projects that showcase how Dagster is used in some small projects.\n\nSome of these are actively maintained as new features are added to Dagster, while those that are labeled with the `UNMAINTAINED` tag in their README are valid projects but may not be actively maintained.\n\nIf you'd like to see a larger scale Dagster project, check out our own analytics project: [Dagster Open Platform](https://github.com/dagster-io/dagster-open-platform)\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for asset_keys_id_seq in PostgreSQL\nDESCRIPTION: This snippet sets the current value of the 'asset_keys_id_seq' sequence to 1, indicating that the next value returned will be 2. It ensures that the sequence starts at the correct initial value to avoid conflicts.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Querying Table with Limit\nDESCRIPTION: This SQL snippet queries all fields from the `sort_by_calories` table within the `test-schema` schema. It limits the result set to only one row using the `LIMIT 1` clause, useful for inspecting the table's structure or a single sample row.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n LIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Generating Alembic Migration Script for SQLite in Dagster\nDESCRIPTION: This command creates a new Alembic migration script for SQLite in the Dagster project. It's used to initiate the process of creating a database migration for new features or schema changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/DEVELOPING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nalembic revision -m \"some new feature\"\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence event_logs_id_seq in PostgreSQL\nDESCRIPTION: Creates 'event_logs_id_seq', a sequence to automatically generate increasing integer IDs for the 'event_logs' table, managing the creation of unique IDs for event entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Loading Data into snapshots Table in PostgreSQL for Dagster\nDESCRIPTION: SQL COPY command to insert data into the snapshots table, which stores serialized snapshots of pipeline definitions. Each row contains an ID, snapshot_id, binary snapshot_body data, and snapshot_type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.snapshots (id, snapshot_id, snapshot_body, snapshot_type) FROM stdin;\n7\tacb9cdc108355c862c469e72fe24307b1eda6b41\t\\\\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\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster STEP_SUCCESS Event in JSON\nDESCRIPTION: This JSON snippet represents a Dagster STEP_SUCCESS event that occurs when a step completes successfully. It includes execution duration (15ms), step key information, and pipeline context details for tracking purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 15.877008438110352}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Finished execution of step \\\"do_input.compute\\\" in 15ms.\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_input.compute - STEP_SUCCESS - Finished execution of step \\\"do_input.compute\\\" in 15ms.\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466123.668594, \"user_message\": \"Finished execution of step \\\"do_input.compute\\\" in 15ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Recording Engine Event - Python\nDESCRIPTION: This snippet logs a generic engine event indicating that the process for a pipeline has started. It marks the process ID and provides context related to the pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80630\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 80630).\", \"pid\": null, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 80630).\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": null, \"timestamp\": 1610466063.469835, \"user_message\": \"Started process for pipeline (pid: 80630).\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Index on event_logs.run_id\nDESCRIPTION: This SQL statement creates an index on the `run_id` column of the `event_logs` table. This speeds up queries that filter or sort by run ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Dagster Installation\nDESCRIPTION: Command to check the installed version of Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndagster –version\n```\n\n----------------------------------------\n\nTITLE: Creating Dagster Project with dbt CLI\nDESCRIPTION: Command to scaffold a new Dagster project that wraps an existing dbt project using the dagster-dbt CLI tool.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/load-dbt-models.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster-dbt project scaffold --project-name jaffle_dagster\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Model Materialization\nDESCRIPTION: Event record showing model materialization with performance metrics including cost and traffic data\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"model\"]}, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"entry_data\": {\"__class__\": \"JsonMetadataEntryData\", \"data\": {\"cost\": 5315.783837199221, \"traffic\": 177.94356552275576}}, \"label\": \"persist_model\"}], \"partition\": \"2020-01-02\"}}}}\n```\n\n----------------------------------------\n\nTITLE: Creating Run ID Index on Event Logs\nDESCRIPTION: Creates a btree index on the run_id column of the event_logs table for improved query performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_65\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n```\n\n----------------------------------------\n\nTITLE: Configuring Registration Timeout for Shuffle Service - Shuffle Settings\nDESCRIPTION: This setting specifies the timeout duration for registering to the external shuffle service, ensuring timely connections.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_29\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.registration.timeout\n```\n\n----------------------------------------\n\nTITLE: Configuring the Shuffle Service Port - Shuffle Settings\nDESCRIPTION: This property specifies the port on which the external shuffle service will run, facilitating communication for managing shuffle files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_23\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.service.port\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Object Store Operation (SET) Event in JSON\nDESCRIPTION: JSON record of an OBJECT_STORE_OPERATION (SET_OBJECT) event in Dagster. This record shows the storage of an intermediate object in the memory store for the 'do_input.compute' step's output.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_input.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_input.compute/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Stored intermediate object for output result in memory object store using pickle.\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.695479, \"user_message\": \"Stored intermediate object for output result in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Buffers for Shuffle - Shuffle Settings\nDESCRIPTION: This setting indicates whether to prefer off-heap buffers during shuffle operations, aimed at reducing garbage collection. It can be adjusted based on memory constraints.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_20\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.io.preferDirectBufs\n```\n\n----------------------------------------\n\nTITLE: Setting Up Development Environment with Make\nDESCRIPTION: These bash commands are used to install development dependencies and set up the local environment for running Dagster. The commands require a Make utility as a dependency. The initial setup involves running 'make dev_install' followed by 'make setup_local_env'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/kitchen-sink/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\nmake setup_local_env\n```\n\n----------------------------------------\n\nTITLE: Initiating Airflow Backfill for Testing\nDESCRIPTION: Bash commands to initiate an Airflow backfill for the current day and clean the Airflow database for future testing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/task-level-migration/observe.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags backfill rebuild_customers_list --start-date $(date +\"%Y-%m-%d\")\n\nairflow db clean\n```\n\n----------------------------------------\n\nTITLE: Creating Pair Plot Grid\nDESCRIPTION: Generates a pair plot grid using seaborn to visualize relationships between features, colored by species.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/tutorial_template/notebooks/iris-kmeans.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ng = sns.PairGrid(iris, hue=\"Species\")\ng.map_diag(plt.hist, alpha=0.5)\ng.map_upper(plt.scatter, alpha=0.5, marker=\"x\")\ng.map_lower(sns.kdeplot, fill=True, thresh=0.05, alpha=0.3)\ng.add_legend()\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Runs by Job in SQL\nDESCRIPTION: Creates a B-tree index on the runs table to optimize queries filtering by pipeline_name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_runs_by_job ON public.runs USING btree (pipeline_name, id);\n```\n\n----------------------------------------\n\nTITLE: Populating Normalized Cereals Table with Sample Data\nDESCRIPTION: SQL COPY statement that inserts a dataset of cereal nutritional information into the normalized_cereals table. The data contains 41 records with various nutritional metrics for different cereal products.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.normalized_cereals (id, name, mfr, type, calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, shelf, weight, cups, rating) FROM stdin;\n1\t100% Bran\tN\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t393.93939393939394\t30.303030303030305\t15.151515151515152\t18.18181818181818\t848.4848484848485\t75.75757575757575\t3\t3.0303030303030303\t0.33\t68.402973\n2\t100% Natural Bran\tQ\tC\t120\t3\t5\t15\t2\t8\t8\t135\t0\t3\t1\t1\t33.983679\n3\tAll-Bran\tK\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t787.8787878787879\t27.272727272727273\t21.21212121212121\t15.151515151515152\t969.6969696969697\t75.75757575757575\t3\t3.0303030303030303\t0.33\t59.425505\n4\tAll-Bran with Extra Fiber\tK\tC\t100\t8\t0\t280\t28\t16\t0\t660\t50\t3\t2\t0.5\t93.704912\n5\tAlmond Delight\tR\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t266.66666666666663\t1.3333333333333333\t18.666666666666664\t10.666666666666666\t-1.3333333333333333\t33.33333333333333\t3\t1.3333333333333333\t0.75\t34.384843\n6\tApple Cinnamon Cheerios\tG\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t240\t2\t14\t13.333333333333332\t93.33333333333333\t33.33333333333333\t1\t1.3333333333333333\t0.75\t29.509541\n7\tApple Jacks\tK\tC\t110\t2\t0\t125\t1\t11\t14\t30\t25\t2\t1\t1\t33.174094\n8\tBasic 4\tG\tC\t173.33333333333331\t4\t2.6666666666666665\t280\t2.6666666666666665\t24\t10.666666666666666\t133.33333333333331\t33.33333333333333\t3\t1.7733333333333334\t0.75\t37.038562\n9\tBran Chex\tR\tC\t134.32835820895522\t2.9850746268656714\t1.4925373134328357\t298.5074626865671\t5.970149253731343\t22.388059701492537\t8.955223880597014\t186.56716417910445\t37.31343283582089\t1\t1.4925373134328357\t0.67\t49.120253\n10\tBran Flakes\tP\tC\t134.32835820895522\t4.477611940298507\t0\t313.4328358208955\t7.462686567164178\t19.402985074626862\t7.462686567164178\t283.58208955223876\t37.31343283582089\t3\t1.4925373134328357\t0.67\t53.313813\n11\tCap'n'Crunch\tQ\tC\t160\t1.3333333333333333\t2.6666666666666665\t293.3333333333333\t0\t16\t16\t46.666666666666664\t33.33333333333333\t2\t1.3333333333333333\t0.75\t18.042851\n12\tCheerios\tG\tC\t88\t4.800000000000001\t1.6\t232\t1.6\t13.600000000000001\t0.8\t84\t20\t1\t0.8\t1.25\t50.764999\n13\tCinnamon Toast Crunch\tG\tC\t160\t1.3333333333333333\t4\t280\t0\t17.333333333333332\t12\t60\t33.33333333333333\t2\t1.3333333333333333\t0.75\t19.823573\n14\tClusters\tG\tC\t220\t6\t4\t280\t4\t26\t14\t210\t50\t3\t2\t0.5\t40.400208\n15\tCocoa Puffs\tG\tC\t110\t1\t1\t180\t0\t12\t13\t55\t25\t2\t1\t1\t22.736446\n16\tCorn Chex\tR\tC\t110\t2\t0\t280\t0\t22\t3\t25\t25\t1\t1\t1\t41.445019\n17\tCorn Flakes\tK\tC\t100\t2\t0\t290\t1\t21\t2\t35\t25\t1\t1\t1\t45.863324\n18\tCorn Pops\tK\tC\t110\t1\t0\t90\t1\t13\t12\t20\t25\t2\t1\t1\t35.782791\n19\tCount Chocula\tG\tC\t110\t1\t1\t180\t0\t12\t13\t65\t25\t2\t1\t1\t22.396513\n20\tCracklin' Oat Bran\tK\tC\t220\t6\t6\t280\t8\t20\t14\t320\t50\t3\t2\t0.5\t40.448772\n21\tCream of Wheat (Quick)\tN\tH\t100\t3\t0\t80\t1\t21\t0\t-1\t0\t2\t1\t1\t64.533816\n22\tCrispix\tK\tC\t110\t2\t0\t220\t1\t21\t3\t30\t25\t3\t1\t1\t46.895644\n23\tCrispy Wheat & Raisins\tG\tC\t133.33333333333331\t2.6666666666666665\t1.3333333333333333\t186.66666666666666\t2.6666666666666665\t14.666666666666666\t13.333333333333332\t160\t33.33333333333333\t3\t1.3333333333333333\t0.75\t36.176196\n24\tDouble Chex\tR\tC\t133.33333333333331\t2.6666666666666665\t0\t253.33333333333331\t1.3333333333333333\t24\t6.666666666666666\t106.66666666666666\t33.33333333333333\t3\t1.3333333333333333\t0.75\t44.330856\n25\tFroot Loops\tK\tC\t110\t2\t1\t125\t1\t11\t13\t30\t25\t2\t1\t1\t32.207582\n26\tFrosted Flakes\tK\tC\t146.66666666666666\t1.3333333333333333\t0\t266.66666666666663\t1.3333333333333333\t18.666666666666664\t14.666666666666666\t33.33333333333333\t33.33333333333333\t1\t1.3333333333333333\t0.75\t31.435973\n27\tFrosted Mini-Wheats\tK\tC\t125\t3.75\t0\t0\t3.75\t17.5\t8.75\t125\t31.25\t2\t1.25\t0.8\t58.345141\n28\tFruit & Fibre Dates; Walnuts; and Oats\tP\tC\t179.1044776119403\t4.477611940298507\t2.9850746268656714\t238.8059701492537\t7.462686567164178\t17.91044776119403\t14.925373134328357\t298.5074626865671\t37.31343283582089\t3\t1.8656716417910446\t0.67\t40.917047\n29\tFruitful Bran\tK\tC\t179.1044776119403\t4.477611940298507\t0\t358.2089552238806\t7.462686567164178\t20.8955223880597\t17.91044776119403\t283.58208955223876\t37.31343283582089\t3\t1.9850746268656716\t0.67\t41.015492\n30\tFruity Pebbles\tP\tC\t146.66666666666666\t1.3333333333333333\t1.3333333333333333\t180\t0\t17.333333333333332\t16\t33.33333333333333\t33.33333333333333\t2\t1.3333333333333333\t0.75\t28.025765\n31\tGolden Crisp\tP\tC\t113.63636363636364\t2.272727272727273\t0\t51.13636363636364\t0\t12.500000000000002\t17.045454545454547\t45.45454545454546\t28.40909090909091\t1\t1.1363636363636365\t0.88\t35.252444\n32\tGolden Grahams\tG\tC\t146.66666666666666\t1.3333333333333333\t1.3333333333333333\t373.3333333333333\t0\t20\t12\t60\t33.33333333333333\t2\t1.3333333333333333\t0.75\t23.804043\n33\tGrape Nuts Flakes\tP\tC\t113.63636363636364\t3.409090909090909\t1.1363636363636365\t159.0909090909091\t3.409090909090909\t17.045454545454547\t5.6818181818181825\t96.5909090909091\t28.40909090909091\t3\t1.1363636363636365\t0.88\t52.076897\n34\tGrape-Nuts\tP\tC\t440\t12\t0\t680\t12\t68\t12\t360\t100\t3\t4\t0.25\t53.371007\n35\tGreat Grains Pecan\tP\tC\t363.6363636363636\t9.09090909090909\t9.09090909090909\t227.27272727272728\t9.09090909090909\t39.39393939393939\t12.121212121212121\t303.030303030303\t75.75757575757575\t3\t3.0303030303030303\t0.33\t45.811716\n36\tHoney Graham Ohs\tQ\tC\t120\t1\t2\t220\t1\t12\t11\t45\t25\t2\t1\t1\t21.871292\n37\tHoney Nut Cheerios\tG\tC\t146.66666666666666\t4\t1.3333333333333333\t333.3333333333333\t2\t15.333333333333332\t13.333333333333332\t120\t33.33333333333333\t1\t1.3333333333333333\t0.75\t31.072217\n38\tHoney-comb\tP\tC\t82.70676691729322\t0.7518796992481203\t0\t135.33834586466165\t0\t10.526315789473683\t8.270676691729323\t26.31578947368421\t18.796992481203006\t1\t0.7518796992481203\t1.33\t28.742414\n39\tJust Right Crunchy  Nuggets\tK\tC\t110\t2\t1\t170\t1\t17\t6\t60\t100\t3\t1\t1\t36.523683\n40\tJust Right Fruit & Nut\tK\tC\t186.66666666666666\t4\t1.3333333333333333\t226.66666666666666\t2.6666666666666665\t26.666666666666664\t12\t126.66666666666666\t133.33333333333331\t3\t1.7333333333333334\t0.75\t36.471512\n41\tKix\tG\tC\t73.33333333333333\t1.3333333333333333\t0.6666666666666666\t173.33333333333331\t0\t14\t2\t26.666666666666664\t16.666666666666664\t2\t0.6666666666666666\t1.5\t39.241114\n\\.\n```\n\n----------------------------------------\n\nTITLE: Initializing GitHub DLT Source\nDESCRIPTION: Commands to initialize a GitHub source using DLT's verified sources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd dlt_sources\n\ndlt init github snowflake\n```\n\n----------------------------------------\n\nTITLE: Create snapshots_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `snapshots_id_seq` to generate unique IDs for the `snapshots` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Alembic Version Primary Key Constraint\nDESCRIPTION: SQL constraint adding primary key to alembic_version table for version tracking\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n```\n\n----------------------------------------\n\nTITLE: Referencing Spark Application Properties in HTML Table Format\nDESCRIPTION: An HTML table documenting Spark application configuration properties, including property names, default values, and detailed explanations of their purposes and effects on Spark applications.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<table class=\"table\">\n<tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr>\n<tr>\n  <td><code>spark.app.name</code></td>\n  <td>(none)</td>\n  <td>\n    The name of your application. This will appear in the UI and in log data.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.driver.cores</code></td>\n  <td>1</td>\n  <td>\n    Number of cores to use for the driver process, only in cluster mode.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.driver.maxResultSize</code></td>\n  <td>1g</td>\n  <td>\n    Limit of total size of serialized results of all partitions for each Spark action (e.g. \n    collect) in bytes. Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total \n    size is above this limit.\n    Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory\n    and memory overhead of objects in JVM). Setting a proper limit can protect the driver from\n    out-of-memory errors.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.driver.memory</code></td>\n  <td>1g</td>\n  <td>\n    Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the\n    same format as JVM memory strings with a size unit suffix (\"k\", \"m\", \"g\" or \"t\")\n    (e.g. <code>512m</code>, <code>2g</code>).\n    <br />\n    <em>Note:</em> In client mode, this config must not be set through the <code>SparkConf</code>\n    directly in your application, because the driver JVM has already started at that point.\n    Instead, please set this through the <code>--driver-memory</code> command line option\n    or in your default properties file.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.driver.memoryOverhead</code></td>\n  <td>driverMemory * 0.10, with minimum of 384 </td>\n  <td>\n    The amount of off-heap memory to be allocated per driver in cluster mode, in MiB unless\n    otherwise specified. This is memory that accounts for things like VM overheads, interned strings, \n    other native overheads, etc. This tends to grow with the container size (typically 6-10%). \n    This option is currently supported on YARN and Kubernetes.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.executor.memory</code></td>\n  <td>1g</td>\n  <td>\n    Amount of memory to use per executor process, in the same format as JVM memory strings with\n    a size unit suffix (\"k\", \"m\", \"g\" or \"t\") (e.g. <code>512m</code>, <code>2g</code>).\n  </td>\n</tr>\n<tr>\n <td><code>spark.executor.pyspark.memory</code></td>\n  <td>Not set</td>\n  <td>\n    The amount of memory to be allocated to PySpark in each executor, in MiB\n    unless otherwise specified.  If set, PySpark memory for an executor will be\n    limited to this amount. If not set, Spark will not limit Python's memory use\n    and it is up to the application to avoid exceeding the overhead memory space\n    shared with other non-JVM processes. When PySpark is run in YARN or Kubernetes, this memory\n    is added to executor resource requests.\n  </td>\n</tr>\n<tr>\n <td><code>spark.executor.memoryOverhead</code></td>\n  <td>executorMemory * 0.10, with minimum of 384 </td>\n  <td>\n    The amount of off-heap memory to be allocated per executor, in MiB unless otherwise specified.\n    This is memory that accounts for things like VM overheads, interned strings, other native \n    overheads, etc. This tends to grow with the executor size (typically 6-10%).\n    This option is currently supported on YARN and Kubernetes.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.extraListeners</code></td>\n  <td>(none)</td>\n  <td>\n    A comma-separated list of classes that implement <code>SparkListener</code>; when initializing\n    SparkContext, instances of these classes will be created and registered with Spark's listener\n    bus.  If a class has a single-argument constructor that accepts a SparkConf, that constructor\n    will be called; otherwise, a zero-argument constructor will be called. If no valid constructor\n    can be found, the SparkContext creation will fail with an exception.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.local.dir</code></td>\n  <td>/tmp</td>\n  <td>\n    Directory to use for \"scratch\" space in Spark, including map output files and RDDs that get\n    stored on disk. This should be on a fast, local disk in your system. It can also be a\n    comma-separated list of multiple directories on different disks.\n\n    NOTE: In Spark 1.0 and later this will be overridden by SPARK_LOCAL_DIRS (Standalone), MESOS_SANDBOX (Mesos) or\n    LOCAL_DIRS (YARN) environment variables set by the cluster manager.\n\n  </td>\n</tr>\n<tr>\n  <td><code>spark.logConf</code></td>\n  <td>false</td>\n  <td>\n    Logs the effective SparkConf as INFO when a SparkContext is started.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.master</code></td>\n  <td>(none)</td>\n  <td>\n    The cluster manager to connect to. See the list of\n    <a href=\"submitting-applications.html#master-urls\"> allowed master URL's</a>.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.submit.deployMode</code></td>\n  <td>(none)</td>\n  <td>\n    The deploy mode of Spark driver program, either \"client\" or \"cluster\",\n    Which means to launch driver program locally (\"client\")\n    or remotely (\"cluster\") on one of the nodes inside the cluster.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.log.callerContext</code></td>\n  <td>(none)</td>\n  <td>\n    Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS.\n    Its length depends on the Hadoop configuration <code>hadoop.caller.context.max.size</code>. It should be concise,\n    and typically can have up to 50 characters.\n  </td>\n</tr>\n<tr>\n  <td><code>spark.driver.supervise</code></td>\n  <td>false</td>\n  <td>\n    If true, restarts the driver automatically if it fails with a non-zero exit status.\n    Only has effect in Spark standalone mode or Mesos cluster deploy mode.\n  </td>\n</tr>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Asset Materialization Event in JSON\nDESCRIPTION: JSON structure of a Dagster event record showing a STEP_MATERIALIZATION event for a 'build_cost_dashboard' step, documenting the creation of a dashboard asset with associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"dashboards\", \"cost_dashboard\"]}, \"description\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"UrlMetadataEntryData\", \"url\": \"http://docs.dagster.io/cost\"}, \"label\": \"build_cost_dashboard\"}], \"partition\": \"2020-01-02\"}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Materialized value dashboards cost_dashboard.\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_cost_dashboard - STEP_MATERIALIZATION - Materialized value dashboards cost_dashboard.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1609894319.005095, \"user_message\": \"Materialized value dashboards cost_dashboard.\"}\n```\n\n----------------------------------------\n\nTITLE: Understanding Dagster PIPELINE_SUCCESS Event in JSON\nDESCRIPTION: This JSON snippet represents a Dagster PIPELINE_SUCCESS event that occurs when an entire pipeline completes successfully. It contains the pipeline name, run ID, and timestamp information but minimal specific data as it's a terminal event.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of pipeline \\\"foo\\\".\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"foo\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.684907, \"user_message\": \"Finished execution of pipeline \\\"foo\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Scaffolding a New Component\nDESCRIPTION: Command to create a new component instance in your Dagster project. This creates a directory with a component.yaml file and any additional required files for the specified component type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/adding-components.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndg scaffold <component-type> <component-path>\n```\n\n----------------------------------------\n\nTITLE: Modifying a Dagster Resource\nDESCRIPTION: This snippet shows how to access and modify a defined resource within the Dagster context. It appends the string \"Hello, notebook!\" to the 'list' resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncontext.resources.list.append(\"Hello, notebook!\")\n```\n\n----------------------------------------\n\nTITLE: Installing UV on Windows using PowerShell\nDESCRIPTION: This PowerShell command downloads and executes the UV installation script on Windows systems.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_InstallUv.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npowershell -ExecutionPolicy ByPass -c 'irm https://astral.sh/uv/install.ps1 | iex'\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Success Event in JSON\nDESCRIPTION: JSON record of a STEP_SUCCESS event in the Dagster pipeline execution system. This record captures the completion of the 'do_something.compute' step, including execution duration and logging metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 17.81296730041504}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_something\", \"solid_definition\": \"do_something\", \"step_key\": \"do_something.compute\"}, \"message\": \"Finished execution of step \\\"do_something.compute\\\" in 17ms.\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_something\", \"parent\": null}, \"step_key\": \"do_something.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_something.compute - STEP_SUCCESS - Finished execution of step \\\"do_something.compute\\\" in 17ms.\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_something.compute\", \"timestamp\": 1610466063.587337, \"user_message\": \"Finished execution of step \\\"do_something.compute\\\" in 17ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Starting Next.js Development Server\nDESCRIPTION: Commands to start the Next.js development server using different package managers. Supports npm, yarn, pnpm, and bun.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/dg-docs-site/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\n----------------------------------------\n\nTITLE: Defining External Code in Python\nDESCRIPTION: This snippet shows the content of the external_code.py file, which contains a simple function that prints a message.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/create-subprocess-asset.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    print(\"Hello from external code!\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for asset_keys id Column\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `asset_keys` table of the `public` schema. It uses a sequence named `public.asset_keys_id_seq` to generate unique IDs for new rows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Capturing Compute Logs in Dagster\nDESCRIPTION: Tracks the initialization of log capture for a specific pipeline step, including step key and run information\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ComputeLogsCaptureData\", \"log_key\": \"add_four.emit_two.emit_one_2\"}}\n```\n\n----------------------------------------\n\nTITLE: Importing Schedules Storage Module in Python\nDESCRIPTION: Imports the schedules storage module from Dagster's core storage package. This module contains classes for managing schedule storage, including SQL-based implementations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.schedules\n```\n\n----------------------------------------\n\nTITLE: Linting Dagster Documentation\nDESCRIPTION: Command to check the documentation for formatting issues using Yarn.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn format\n```\n\n----------------------------------------\n\nTITLE: DagsterEventRecord JSON Structure\nDESCRIPTION: Example of a Dagster event record JSON structure showing step output data with type checking information and logging metadata\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"out_1\", \"step_key\": \"giver.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"out_1\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"sleepy_pipeline\", \"solid\": \"giver\", \"solid_definition\": \"giver\", \"step_key\": \"giver.compute\"}, \"message\": \"Yielded output \\\"out_1\\\" of type \\\"None\\\". (Type check passed).\", \"pipeline_name\": \"sleepy_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"giver\", \"name\": \"giver\", \"parent\": null}, \"step_key\": \"giver.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - STEP_OUTPUT - Yielded output \\\"out_1\\\" of type \\\"None\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"giver.compute\\\", \\\"out_1\\\"], \\\"type_check_data\\\": [true, \\\"out_1\\\", null, []]}\\n               solid = \\\"giver\\\"\\n    solid_definition = \\\"giver\\\"\\n            step_key = \\\"giver.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository\nDESCRIPTION: Updates the local Helm repository cache. This ensures that the Helm client has the latest information about available charts, including new releases of the Dagster Helm chart.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/helm/dagster/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Run ID in Event Logs in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the event_logs table for the run_id column. This index enhances query performance when searching for event logs associated with a specific run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_56\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n```\n\n----------------------------------------\n\nTITLE: Destroying ECS Deployment\nDESCRIPTION: Command to tear down the Dagster deployment and clean up AWS resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/deploy_ecs/README.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ndocker --context dagster-ecs compose --project-name dagster down\n```\n\n----------------------------------------\n\nTITLE: Create daemon_heartbeats Table\nDESCRIPTION: This SQL statement creates the `daemon_heartbeats` table to store heartbeat information from Dagster daemons. It includes the daemon type, ID, timestamp, and a body containing additional information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \\\"timestamp\\\" timestamp without time zone NOT NULL,\n    body text\n);\"\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Run Tags in PostgreSQL\nDESCRIPTION: Defines a primary key constraint on the 'id' column of the 'run_tags' table, preserving the uniqueness of run tag entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install the project and its dependencies in editable mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/creating-a-new-project.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Asset Definitions Table\nDESCRIPTION: ASCII table output showing a single Dagster asset named 'my_asset' in the default group without any dependencies, kinds or description specified.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/4-list-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndg list defs\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Section ┃ Definitions                                         ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Assets  │ ┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┓ │\n│         │ ┃ Key      ┃ Group   ┃ Deps ┃ Kinds ┃ Description ┃ │\n│         │ ┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━┩ │\n│         │ │ my_asset │ default │      │       │             │ │\n│         │ └──────────┴─────────┴──────┴───────┴─────────────┘ │\n└─────────┴─────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Event Records in JSON Format\nDESCRIPTION: These JSON objects represent event records from a Dagster pipeline execution. Each record contains detailed information about pipeline steps including events like STEP_SUCCESS, STEP_START, OBJECT_STORE_OPERATION, and STEP_MATERIALIZATION.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 960.8635610202327}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Finished execution of step \\\"build_model\\\" in 960ms.\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_model - STEP_SUCCESS - Finished execution of step \\\"build_model\\\" in 960ms.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_model\", \"timestamp\": 1609894320.056171, \"user_message\": \"Finished execution of step \\\"build_model\\\" in 960ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster GCP Integration\nDESCRIPTION: Command to install the Dagster GCP integration package via pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/dataproc.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-gcp\n```\n\n----------------------------------------\n\nTITLE: Alter asset_keys Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `asset_keys` table to `test`.  This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.asset_keys OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Advanced ETL Jobs Configuration with Jinja\nDESCRIPTION: Enhanced YAML configuration using Jinja templating for environment variables and more structured job definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/creating-asset-factories.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ns3:\n  bucket: \"{{ env['S3_BUCKET'] }}\"\n  aws_access_key_id: \"{{ env['AWS_ACCESS_KEY_ID'] }}\"\n  aws_secret_access_key: \"{{ env['AWS_SECRET_ACCESS_KEY'] }}\"\n\njobs:\n  - name: job1\n    source: path/to/source1.csv\n    destination: path/to/destination1.csv\n    query: SELECT * FROM source WHERE value > 100\n  - name: job2\n    source: path/to/source2.csv\n    destination: path/to/destination2.csv\n    query: SELECT * FROM source WHERE value < 0\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence asset_keys_id_seq in PostgreSQL\nDESCRIPTION: Initializes the sequence 'asset_keys_id_seq' used for generating unique integer IDs in the 'asset_keys' table, with ownership assigned to the table's owner. This sequence increments by 1 and has a starting value of 1.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Updating Schedule Definition in Python\nDESCRIPTION: Example showing how to update schedule definitions by removing the scheduler argument and using instance configuration instead\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n@schedules(scheduler=SystemCronScheduler)\ndef define_schedules():\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\n@schedules\ndef define_schedules():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Running Basic DBT Commands\nDESCRIPTION: Essential dbt CLI commands for executing models and running tests in a dbt project\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_analytics/dagster_pypi/dbt_project/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt run\n```\n\nLANGUAGE: shell\nCODE:\n```\ndbt test\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in JSX\nDESCRIPTION: This snippet imports the DocCardList component from the '@theme/DocCardList' module, which is likely used to render a list of documentation cards related to partitions and backfills in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n```\n\n----------------------------------------\n\nTITLE: Setting Up Airbyte Locally\nDESCRIPTION: Commands to clone the Airbyte repository and start it using Docker Compose.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_modern_data_stack/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/airbytehq/airbyte.git\n$ cd airbyte\n$ docker-compose up\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Earliest Tested Release in Bash\nDESCRIPTION: Sets the EARLIEST_TESTED_RELEASE environment variable to specify the earliest Dagster release to test for backcompatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport EARLIEST_TESTED_RELEASE=\"0.12.8\"\n```\n\n----------------------------------------\n\nTITLE: Database Constraints and Primary Keys in PostgreSQL\nDESCRIPTION: SQL statements defining primary key constraints for various tables in the database. These constraints ensure data integrity by enforcing uniqueness on specific columns like version_num, asset_key, id, and run_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_67\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.normalized_cereals\n    ADD CONSTRAINT normalized_cereals_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to Dagster Database Tables\nDESCRIPTION: Adds primary key and uniqueness constraints to tables in the Dagster database schema. These constraints ensure data integrity by preventing duplicate entries and establishing relationships between tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\nALTER TABLE ONLY public.asset_check_executions\n    ADD CONSTRAINT asset_check_executions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.asset_daemon_asset_evaluations\n    ADD CONSTRAINT asset_daemon_asset_evaluations_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.asset_event_tags\n    ADD CONSTRAINT asset_event_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.concurrency_limits\n    ADD CONSTRAINT concurrency_limits_concurrency_key_key UNIQUE (concurrency_key);\n\nALTER TABLE ONLY public.concurrency_limits\n    ADD CONSTRAINT concurrency_limits_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.concurrency_slots\n    ADD CONSTRAINT concurrency_slots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.dynamic_partitions\n    ADD CONSTRAINT dynamic_partitions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instance_info\n    ADD CONSTRAINT instance_info_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_selector_id_key UNIQUE (selector_id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.kvs\n    ADD CONSTRAINT kvs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.pending_steps\n    ADD CONSTRAINT pending_steps_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Using Pendulum with Standard Python Datetime Objects\nDESCRIPTION: Updated version of the previous asset example that explicitly converts standard datetime objects to Pendulum instances before using Pendulum-specific methods.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetExecutionContext\nimport pendulum\n\n@asset\ndef my_asset(context: AssetExecutionContext):\n  window_start, window_end = context.partition_time_window\n    window_start = pendulum.instance(window_start) # transform to a pendulum time\n\n  in_an_hour = window_start.add(hours=1) # will continue working\n```\n\n----------------------------------------\n\nTITLE: Creating asset_keys_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates a sequence named 'asset_keys_id_seq' starting from 1 to provide unique identifiers for the 'asset_keys' table. The sequence has no minimum or maximum values and caches one value at a time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Database Tables and Schema for Dagster in PostgreSQL\nDESCRIPTION: This SQL script defines the complete PostgreSQL database schema for Dagster, an open-source data orchestration framework. The schema includes tables for run storage, event logs, asset tracking, job scheduling, and other core functionality needed for Dagster operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n--\n-- PostgreSQL database dump\n--\n\n-- Dumped from database version 13.3\n-- Dumped by pg_dump version 13.3\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\nSET default_tablespace = '';\n\nSET default_table_access_method = heap;\n\n--\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\n\nALTER TABLE public.alembic_version OWNER TO test;\n\n--\n-- Name: asset_keys; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.asset_keys OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n\n\n--\n-- Name: backfills; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.backfills (\n    id integer NOT NULL,\n    backfill_id character varying(32) NOT NULL,\n    partition_set_origin_id character varying(255) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\n\nALTER TABLE public.backfills OWNER TO test;\n\n--\n-- Name: backfills_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.backfills_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.backfills_id_seq OWNER TO test;\n\n--\n-- Name: backfills_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.backfills_id_seq OWNED BY public.backfills.id;\n\n\n--\n-- Name: bulk_actions; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.bulk_actions (\n    id integer NOT NULL,\n    key character varying(32) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\n\nALTER TABLE public.bulk_actions OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.bulk_actions_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.bulk_actions_id_seq OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.bulk_actions_id_seq OWNED BY public.bulk_actions.id;\n\n\n--\n-- Name: daemon_heartbeats; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\n\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n\n--\n-- Name: event_logs; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\n\n\nALTER TABLE public.event_logs OWNER TO test;\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\n\n\n--\n-- Name: job_ticks; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.job_ticks OWNER TO test;\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.job_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.job_ticks_id_seq OWNER TO test;\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.job_ticks_id_seq OWNED BY public.job_ticks.id;\n\n\n--\n-- Name: jobs; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.jobs OWNER TO test;\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.jobs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.jobs_id_seq OWNER TO test;\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.jobs_id_seq OWNED BY public.jobs.id;\n\n\n--\n-- Name: normalized_cereals; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.normalized_cereals (\n    id integer NOT NULL,\n    name character varying,\n    mfr character varying,\n    type character varying,\n    calories double precision,\n    protein double precision,\n    fat double precision,\n    sodium double precision,\n    fiber double precision,\n    carbo double precision,\n    sugars double precision,\n    potass double precision,\n    vitamins double precision,\n    shelf double precision,\n    weight double precision,\n    cups double precision,\n    rating double precision\n);\n\n\nALTER TABLE public.normalized_cereals OWNER TO test;\n\n--\n-- Name: normalized_cereals_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.normalized_cereals_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.normalized_cereals_id_seq OWNER TO test;\n\n--\n-- Name: normalized_cereals_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.normalized_cereals_id_seq OWNED BY public.normalized_cereals.id;\n\n\n--\n-- Name: run_tags; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\n\n\nALTER TABLE public.run_tags OWNER TO test;\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\n\n\n--\n-- Name: runs; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    mode character varying\n);\n\n\nALTER TABLE public.runs OWNER TO test;\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\n\n\n--\n-- Name: secondary_indexes; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying(512),\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n\n\nALTER TABLE public.secondary_indexes OWNER TO test;\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.secondary_indexes_id_seq OWNER TO test;\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.secondary_indexes_id_seq OWNED BY public.secondary_indexes.id;\n\n\n--\n-- Name: snapshots; Type: TABLE; Schema: public; Owner: test\n--\n\nCREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n\n\nALTER TABLE public.snapshots OWNER TO test;\n\n--\n```\n\n----------------------------------------\n\nTITLE: Creating Index on job_ticks.job_origin_id, timestamp\nDESCRIPTION: This SQL statement creates a composite index on the `job_origin_id` and `timestamp` columns of the `job_ticks` table.  This optimizes queries filtering job ticks based on their origin and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \\\"timestamp\\\");\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Object Store Operation Event in JSON\nDESCRIPTION: JSON structure of a Dagster event record showing an OBJECT_STORE_OPERATION event for a 'build_cost_dashboard' step, documenting retrieval of an intermediate object from filesystem storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/persist_costs/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/persist_costs/result\"}, \"label\": \"key\"}], \"op\": \"GET_OBJECT\", \"value_name\": \"_\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_cost_dashboard - OBJECT_STORE_OPERATION - Retrieved intermediate object for input _ in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1609894318.485312, \"user_message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Asset Event Tags by Event ID in SQL\nDESCRIPTION: Creates a B-tree index on the asset_event_tags table to optimize queries filtering by event_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_event_tags_event_id ON public.asset_event_tags USING btree (event_id);\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dagster Partition Relationships\nDESCRIPTION: Mermaid diagram depicting Partition's relationship with Asset component, including style configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_11\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    Partition(Partition)\n\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Partition -.-> Asset\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster and Jupyter\nDESCRIPTION: Command to install Dagster and Jupyter using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster notebook\n```\n\n----------------------------------------\n\nTITLE: Running Jest Tests for Dagster UI\nDESCRIPTION: Command to run Jest tests across all packages in the Dagster UI project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/README.md#2025-04-22_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nmake jest\n```\n\n----------------------------------------\n\nTITLE: Frontmatter Configuration in Markdown\nDESCRIPTION: YAML frontmatter block defining metadata for a documentation page, including title, sidebar position, and visibility status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/migrating-to-declarative-automation.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Migrating to Declarative Automation\nsidebar_position: 100\nunlisted: true\n---\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dagster Graph Components and Relationships\nDESCRIPTION: Mermaid diagram depicting how Graph connects Ops and Config to form a Job, including style configurations for visual hierarchy.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_7\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Config fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Op fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Graph(Graph)\n\n    style Job fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Config -.-> Graph\n    Op ==> Graph\n    Graph ==> Job\n```\n\n----------------------------------------\n\nTITLE: Cloning Dagster Repository\nDESCRIPTION: Command to clone the Dagster repository and navigate to the project directory for LLM fine-tuning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/docs_projects/project-llm-fine-tune\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Cloud Agent Image Version\nDESCRIPTION: YAML configuration to set the Dagster Cloud agent image tag to 'latest'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloudAgent:\n  image:\n    tag: latest\n```\n\n----------------------------------------\n\nTITLE: Creating Job and Run Related Indexes in PostgreSQL\nDESCRIPTION: Creates indexes for job ticks, runs, and run tags tables to optimize job scheduling and run querying performance\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n\nCREATE INDEX idx_runs_by_job ON public.runs USING btree (pipeline_name, id);\n\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n```\n\n----------------------------------------\n\nTITLE: Creating backfills Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'backfills' table to manage backfill jobs in the system. It includes fields such as backfill ID, status, timestamp, and an optional body for additional details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: backfills; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.backfills (\n    id integer NOT NULL,\n    backfill_id character varying(32) NOT NULL,\n    partition_set_origin_id character varying(255) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\nALTER TABLE public.backfills OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Recording Object Store Operation - Python\nDESCRIPTION: This snippet logs an operation related to retrieving an intermediate object from the object store during a step execution. It reports the success of the action with detailed metadata and context about the object being accessed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_something.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_something.compute/result\"}, \"label\": \"key\"}], \"op\": \"GET_OBJECT\", \"value_name\": \"x\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Retrieved intermediate object for input x in memory object store using pickle.\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_input.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input x in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466002.858748, \"user_message\": \"Retrieved intermediate object for input x in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Index on run_id Column in event_logs Table\nDESCRIPTION: SQL command to create an index on the run_id column in the event_logs table to optimize event lookup by run_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n```\n\n----------------------------------------\n\nTITLE: Running UV Sync Command\nDESCRIPTION: A command to synchronize Python packages using the uv package manager, which is a modern alternative to pip offering improved performance\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/2-a-uv-venv.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv sync\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraint to snapshots Table\nDESCRIPTION: SQL command to add a unique constraint on the snapshot_id column in the snapshots table to ensure no duplicate snapshot IDs exist.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Setting Default Sequence for Snapshots Table in PostgreSQL\nDESCRIPTION: SQL statement that sets the default value for the id column in the snapshots table to use the next value from the snapshots_id_seq sequence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Process Exit in Dagster\nDESCRIPTION: JSON representation of an engine event recording process exit in a Dagster pipeline. The event logs that the process for the pipeline execution has exited, including the process ID information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 34000).\", \"pid\": null, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Process for pipeline exited (pid: 34000).\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": null, \"timestamp\": 1625607811.726439, \"user_message\": \"Process for pipeline exited (pid: 34000).\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint from run_tags.run_id to runs.run_id\nDESCRIPTION: This SQL statement adds a foreign key constraint to the `run_tags` table, linking the `run_id` column to the `run_id` column in the `runs` table. The `ON DELETE CASCADE` clause ensures that if a run is deleted, all associated run tags are also deleted.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\n\"ALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster STEP_OUTPUT Event JSON in Python\nDESCRIPTION: JSON representation of a Dagster step output event. This event indicates that a step yielded an output named 'result' that passed type checking. The record includes metadata about the pipeline, run, step, and event specifics.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_55\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"do_input.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_input.compute - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466123.653987, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in JSX/Markdown\nDESCRIPTION: This snippet imports the DocCardList component from the theme and renders it within the page. This component likely displays a list of related documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/index.md#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Modifying Sling Replication Component Configuration\nDESCRIPTION: YAML configuration for the Sling component that defines a replication from local files to a data source. The file specifies the component type and the path to the replication configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/components-etl-pipeline-tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncomponent_type: dagster_sling.SlingReplicationCollectionComponent\nreplications:\n  - path: replication.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Importing Local Compute Log Manager Module in Python\nDESCRIPTION: Imports the local compute log manager module from Dagster's core storage package. This module contains classes for managing local compute logs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.local_compute_log_manager\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Dagster Database Tables\nDESCRIPTION: These SQL statements alter multiple tables in the Dagster database schema to set default values for their ID columns. Each statement uses a sequence generator (nextval) to automatically assign incrementing IDs when new rows are inserted.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.concurrency_limits ALTER COLUMN id SET DEFAULT nextval('public.concurrency_limits_id_seq'::regclass);\n\nALTER TABLE ONLY public.concurrency_slots ALTER COLUMN id SET DEFAULT nextval('public.concurrency_slots_id_seq'::regclass);\n\nALTER TABLE ONLY public.daemon_heartbeats ALTER COLUMN id SET DEFAULT nextval('public.daemon_heartbeats_id_seq'::regclass);\n\nALTER TABLE ONLY public.dynamic_partitions ALTER COLUMN id SET DEFAULT nextval('public.dynamic_partitions_id_seq'::regclass);\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\nALTER TABLE ONLY public.instance_info ALTER COLUMN id SET DEFAULT nextval('public.instance_info_id_seq'::regclass);\n\nALTER TABLE ONLY public.instigators ALTER COLUMN id SET DEFAULT nextval('public.instigators_id_seq'::regclass);\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n\nALTER TABLE ONLY public.kvs ALTER COLUMN id SET DEFAULT nextval('public.kvs_id_seq'::regclass);\n\nALTER TABLE ONLY public.pending_steps ALTER COLUMN id SET DEFAULT nextval('public.pending_steps_id_seq'::regclass);\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Creating Table asset_keys in PostgreSQL\nDESCRIPTION: Defines the 'asset_keys' table structure with columns for ID, asset key, and creation timestamp. The primary key is set to NOT NULL, with a default timestamp, intended for tracking assets in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.asset_keys OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Scaffolded Component YAML Configuration\nDESCRIPTION: Generated YAML configuration file for a ShellCommandComponent instance, specifying the script path and asset specifications.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ncomponent_type: shell_command\n\nattributes:\n  script_path: script.sh\n  asset_specs:\n    - key: my_shell_asset\n```\n\n----------------------------------------\n\nTITLE: Copying Data into daemon_heartbeats Table\nDESCRIPTION: This SQL statement attempts to copy data into the `daemon_heartbeats` table in the `public` schema. It uses the `FROM stdin` clause to indicate that data will be provided inline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.daemon_heartbeats (daemon_type, daemon_id, \"timestamp\", body) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Pipes for Rust using Cargo\nDESCRIPTION: This command installs the Dagster Pipes implementation for Rust using Cargo, the Rust package manager. It adds the 'dagster_pipes_rust' package to the project dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/rust.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncargo add dagster_pipes_rust\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Cube Integration Package\nDESCRIPTION: Command to install the dagster-cube integration package using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/cube.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster_cube\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment\nDESCRIPTION: Uses Python's built-in venv module to create a new virtual environment in the .venv directory. This isolates project dependencies from the system Python installation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/2-a-pip-venv.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for secondary_indexes.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `secondary_indexes` table to be generated by the `secondary_indexes_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_57\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Mapping Tool Names to Icon Images in Markdown\nDESCRIPTION: This code snippet defines a markdown table that maps tool names to their corresponding icon images. Each row represents a tool, with the first column containing the tool name and the second column containing an HTML img tag for the icon.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_KindsTags.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| `googlesheets`        | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-googlesheets-color.svg\" width={20} height={20} />                   |\n| `grafana`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-grafana-color.svg\" width={20} height={20} />                        |\n| `graphql`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-graphql-color.svg\" width={20} height={20} />                        |\n| `greatexpectations`   | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-greatexpectations-color.svg\" width={20} height={20} />              |\n| `hackernews`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-hackernews-color.svg\" width={20} height={20} />                     |\n| `hackernewsapi`       | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-hackernews-color.svg\" width={20} height={20} />                     |\n| `hadoop`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-hadoop-color.svg\" width={20} height={20} />                         |\n| `hashicorp`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-hashicorp-color.svg\" width={20} height={20} />                      |\n| `hex`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-hex-color.svg\" width={20} height={20} />                            |\n| `hightouch`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-hightouch-color.svg\" width={20} height={20} />                      |\n| `hudi`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-hudi-color.svg\" width={20} height={20} />                           |\n| `huggingface`         | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-huggingface-color.svg\" width={20} height={20} />                    |\n| `huggingfaceapi`      | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-huggingface-color.svg\" width={20} height={20} />                    |\n| `iceberg`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-iceberg-color.svg\" width={20} height={20} />                        |\n| `icechunk`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-icechunk-color.svg\" width={20} height={20} />                       |\n| `impala`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-impala-color.svg\" width={20} height={20} />                         |\n| `instagram`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-instagram-color.svg\" width={20} height={20} />                      |\n| `ipynb`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-jupyter-color.svg\" width={20} height={20} />                        |\n| `java`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-java-color.svg\" width={20} height={20} />                           |\n| `javascript`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-javascript-color.svg\" width={20} height={20} />                     |\n| `json`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/json.svg\" width={20} height={20} />                                      |\n| `jupyter`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-jupyter-color.svg\" width={20} height={20} />                        |\n| `k8s`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-k8s-color.svg\" width={20} height={20} />                            |\n| `kafka`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-kafka-color.svg\" width={20} height={20} />                          |\n| `kubernetes`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-k8s-color.svg\" width={20} height={20} />                            |\n| `lakefs`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-lakefs-color.svg\" width={20} height={20} />                         |\n| `lightgbm`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-lightgbm-color.svg\" width={20} height={20} />                       |\n| `linear`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-linear-color.svg\" width={20} height={20} />                         |\n| `linkedin`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-linkedin-color.svg\" width={20} height={20} />                       |\n| `llama`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-llama-color.svg\" width={20} height={20} />                          |\n| `looker`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-looker-color.svg\" width={20} height={20} />                         |\n| `mariadb`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-mariadb-color.svg\" width={20} height={20} />                        |\n| `matplotlib`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-matplotlib-color.svg\" width={20} height={20} />                     |\n| `meltano`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-meltano-color.svg\" width={20} height={20} />                        |\n| `meta`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-meta-color.svg\" width={20} height={20} />                           |\n| `metabase`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-metabase-color.svg\" width={20} height={20} />                       |\n| `microsoft`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-microsoft-color.svg\" width={20} height={20} />                      |\n| `minio`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-minio-color.svg\" width={20} height={20} />                          |\n| `mistral`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-mistral-color.svg\" width={20} height={20} />                        |\n| `mlflow`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-mlflow-color.svg\" width={20} height={20} />                         |\n| `modal`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-modal-color.svg\" width={20} height={20} />                          |\n| `mongodb`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-mongodb-color.svg\" width={20} height={20} />                        |\n| `montecarlo`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-montecarlo-color.svg\" width={20} height={20} />                     |\n| `mysql`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-mysql-color.svg\" width={20} height={20} />                          |\n| `net`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-microsoft-color.svg\" width={20} height={20} />                      |\n| `notdiamond`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-notdiamond-color.svg\" width={20} height={20} />                     |\n| `noteable`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-noteable-color.svg\" width={20} height={20} />                       |\n| `notion`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-notion-color.svg\" width={20} height={20} />                         |\n| `numpy`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-numpy-color.svg\" width={20} height={20} />                          |\n| `omni`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-omni-color.svg\" width={20} height={20} />                           |\n| `openai`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-openai-color.svg\" width={20} height={20} />                         |\n| `openmetadata`        | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-openmetadata-color.svg\" width={20} height={20} />                   |\n| `optuna`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-optuna-color.svg\" width={20} height={20} />                         |\n| `oracle`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-oracle-color.svg\" width={20} height={20} />                         |\n| `pagerduty`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-pagerduty-color.svg\" width={20} height={20} />                      |\n| `pandas`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-pandas-color.svg\" width={20} height={20} />                         |\n| `pandera`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-pandera-color.svg\" width={20} height={20} />                        |\n| `papermill`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-papermill-color.svg\" width={20} height={20} />                      |\n| `papertrail`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-papertrail-color.svg\" width={20} height={20} />                     |\n| `parquet`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-parquet-color.svg\" width={20} height={20} />                        |\n```\n\n----------------------------------------\n\nTITLE: Dagster Log Message in JSON Format\nDESCRIPTION: A JSON representation of a log message from the Dagster pipeline execution. This log captures a sleep operation in the pipeline, indicating that the 'sleeper' step is sleeping for 1 second.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"LogMessageRecord\", \"dagster_event\": null, \"error_info\": null, \"level\": 20, \"message\": \"system - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - Sleeping for 1 seconds\\n               solid = \\\"sleeper\\\"\\n    solid_definition = \\\"sleeper\\\"\\n            step_key = \\\"sleeper.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Airflow Dependencies\nDESCRIPTION: Installs the Python packages required for running the Airflow instances in the tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_install\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for job_ticks_id_seq in PostgreSQL\nDESCRIPTION: This command initializes the 'job_ticks_id_seq' sequence to the value of 1, ensuring the correct sequencing for job ticks records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Creating Least Caloric View in SQL\nDESCRIPTION: This snippet creates a view named 'least_caloric' to select various attributes of cereals minimally, focusing on their calorie content. It expects data from another view 'sort_by_calories'. The output is a summarized view of the least caloric cereals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: least_caloric; Type: VIEW; Schema: test-schema; Owner: test\n\nCREATE VIEW \"test-schema\".least_caloric AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n LIMIT 1;\n\nALTER TABLE \"test-schema\".least_caloric OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Create runs_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `runs_id_seq` to generate unique IDs for the `runs` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Creating Run Partitions Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the partition_set and partition columns of the runs table to optimize queries that filter by these partitioning columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_54\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\n```\n\n----------------------------------------\n\nTITLE: Displaying SQL Configuration Settings in Java\nDESCRIPTION: This Java code snippet demonstrates how to display the entire list of SQL configuration settings using an existing SparkSession.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_46\n\nLANGUAGE: java\nCODE:\n```\n// spark is an existing SparkSession\nspark.sql(\"SET -v\").show(200, false);\n```\n\n----------------------------------------\n\nTITLE: Referencing Dagster Airbyte Documentation in Markdown\nDESCRIPTION: This snippet provides a markdown link to the official documentation for the dagster-airbyte library, directing users to the comprehensive API reference and usage guidelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-airbyte/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-airbyte\n\nThe docs for `dagster-airbyte` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-airbyte).\n```\n\n----------------------------------------\n\nTITLE: Copying Pipeline Run Data into runs Table in PostgreSQL for Dagster\nDESCRIPTION: SQL statement for populating the runs table with Dagster pipeline execution records. This includes a complete pipeline run record with job configuration, status, timestamps, and serialized run metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.runs (id, run_id, snapshot_id, pipeline_name, mode, status, run_body, partition, partition_set, create_timestamp, update_timestamp, start_time, end_time) FROM stdin;\n1\t2464fea5-b2f4-437b-8461-5cd6d6572ff4\tdd6c6dd475cd4d91c23a9dbaf55eb56b2b1d0d25\t__ASSET_JOB\t\\N\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"asset_check_selection\": {\"__frozenset__\": []}, \"asset_selection\": {\"__frozenset__\": [{\"__class__\": \"AssetKey\", \"path\": [\"not_partitioned\"]}]}, \"execution_plan_snapshot_id\": \"93b84fd7cd172f878122450c2dae6901be99048f\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": null, \"executable_path\": null, \"module_name\": null, \"package_name\": null, \"python_file\": \"../jamie_examples/backfill_multiple_iterations.py\", \"working_directory\": \"/Users/jamie/dev/dagster\"}, \"location_name\": \"backfill_multiple_iterations.py\"}, \"repository_name\": \"__repository__\"}, \"pipeline_name\": \"__ASSET_JOB\"}, \"has_repository_load_data\": false, \"mode\": null, \"parent_run_id\": null, \"pipeline_code_origin\": {\"__class__\": \"PipelinePythonOrigin\", \"pipeline_name\": \"__ASSET_JOB\", \"repository_origin\": {\"__class__\": \"RepositoryPythonOrigin\", \"code_pointer\": {\"__class__\": \"FileCodePointer\", \"fn_name\": \"defs\", \"python_file\": \"../jamie_examples/backfill_multiple_iterations.py\", \"working_directory\": \"/Users/jamie/dev/dagster\"}, \"container_context\": {}, \"container_image\": null, \"entry_point\": [\"dagster\"], \"executable_path\": \"/Users/jamie/.pyenv/versions/3.11.1/envs/dagster-3.11/bin/python3\"}}, \"pipeline_name\": \"__ASSET_JOB\", \"pipeline_snapshot_id\": \"dd6c6dd475cd4d91c23a9dbaf55eb56b2b1d0d25\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"2464fea5-b2f4-437b-8461-5cd6d6572ff4\", \"run_op_concurrency\": null, \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": [\"not_partitioned\"], \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ns/r7rp0cg558zdj1yjm3p66qn80000gn/T/tmpf7rfg43p\\\"}\"}}}\t\\N\t\\N\t2024-10-21 12:38:37.374203\t2024-10-21 12:38:41.509902\t1729528719.559257\t1729528721.509902\n\\.\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster AWS Integration\nDESCRIPTION: Command to install the dagster-aws package using pip package manager. This package is required to use AWS Glue integration with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/glue.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-aws\n```\n\n----------------------------------------\n\nTITLE: Alter runs Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `runs` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.runs OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Cloud Python Client\nDESCRIPTION: Command to install the dagster-cloud Python client, which is required for managing code locations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/code-locations/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-cloud\n```\n\n----------------------------------------\n\nTITLE: SQL INSERT Statement Failing Due to Missing Column in PostgreSQL\nDESCRIPTION: SQL query that's failing because the 'last_materialization_timestamp' column doesn't exist in the 'asset_keys' table. The query attempts to insert asset key data with materialization information, but encounters a schema mismatch error.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT INTO asset_keys (asset_key, last_materialization, last_run_id, last_materialization_timestamp, tags) VALUES (%(asset_key)s, %(last_materialization)s, %(last_run_id)s, %(last_materialization_timestamp)s, %(tags)s) ON CONFLICT (asset_key) DO UPDATE SET last_materialization = %(param_1)s, last_run_id = %(param_2)s, last_materialization_timestamp = %(param_3)s, tags = %(param_4)s RETURNING asset_keys.id\n```\n\n----------------------------------------\n\nTITLE: Creating and Altering Event Logs ID Sequence\nDESCRIPTION: Creates and alters a sequence 'event_logs_id_seq' for generating unique identifiers for entries in the 'event_logs' table. It establishes a relationship between the sequence and the table by owning it to the 'id' column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n\n\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster EventLogEntry JSON for Step Start Events\nDESCRIPTION: JSON representation of a Dagster EventLogEntry for a STEP_START event. This structure contains information about the initialization of a pipeline step execution, including solid handle hierarchies and execution context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"emit_one_2\", \"step_key\": \"add_four.emit_two_2.emit_one_2\"}, \"message\": \"Started execution of step \\\"add_four.emit_two_2.emit_one_2\\\".\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two_2.emit_one_2\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two_2.emit_one_2\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two_2.emit_one_2 - STEP_START - Started execution of step \\\"add_four.emit_two_2.emit_one_2\\\".\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two_2.emit_one_2\", \"timestamp\": 1640037521.772129, \"user_message\": \"Started execution of step \\\"add_four.emit_two_2.emit_one_2\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoding GCP Credentials in Shell\nDESCRIPTION: Command to base64 encode a GCP key file for use with Dagster's BigQuery I/O manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncat ~/.gcp/key.json | base64\n```\n\n----------------------------------------\n\nTITLE: Accessing Dagster Definitions Class Members\nDESCRIPTION: Core Definitions class with methods for retrieving job definitions, sensor definitions, schedule definitions, and asset value loading capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/definitions.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDefinitions\n    .get_job_def\n    .get_sensor_def\n    .get_schedule_def\n    .load_asset_value\n    .get_asset_value_loader\n```\n\n----------------------------------------\n\nTITLE: Setting Up Airflow Instances for Federation Tutorial\nDESCRIPTION: Scaffolds the two Airflow instances required for the federation tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_setup\n```\n\n----------------------------------------\n\nTITLE: Listing Unchecked Dagster Examples and Components\nDESCRIPTION: This section enumerates the Dagster examples and components that are not type checked. These may include experimental features, documentation snippets, or components that don't require strict type checking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/exclude.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nexamples/assets_dbt_python\nexamples/assets_modern_data_stack\nexamples/assets_smoke_test\nexamples/docs_snippets\nexamples/project_analytics\nexamples/project_dagster_university_start\nexamples/project_du_dbt_starter\nexamples/tutorial\nhelm\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence snapshots_id_seq in PostgreSQL\nDESCRIPTION: 'snapshots_id_seq' allocates unique IDs to 'snapshots' table entries, ensuring each snapshot is distinguishable in the system and facilitating data management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.snapshots_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Raising Retry Requests in Dagster with Python\nDESCRIPTION: This code snippet raises a RetryRequested exception in Dagster with a limit of maximum retries. It is used when retrying tasks in a Dagster pipeline to control the number of retry attempts. The snippet imports the RetryRequested class from the Dagster framework and raises it with a parameter specifying the maximum retries. There are no specific inputs or outputs apart from the exception mechanism.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/raise_retry.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import RetryRequested\n\nraise RetryRequested(max_retries=1)\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraints in SQL\nDESCRIPTION: Adds foreign key constraints to establish relationships between tables in the Dagster database schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.asset_event_tags\n    ADD CONSTRAINT asset_event_tags_event_id_fkey FOREIGN KEY (event_id) REFERENCES public.event_logs(id) ON DELETE CASCADE;\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Creating event_logs Table in PostgreSQL\nDESCRIPTION: This SQL snippet defines the 'event_logs' table for storing event logs related to data processing tasks. It includes various fields such as 'id', 'run_id', 'event', 'dagster_event_type', and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: event_logs; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key character varying,\n    asset_key character varying,\n    partition character varying\n);\n\n\nALTER TABLE public.event_logs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for asset_keys.id in PostgreSQL\nDESCRIPTION: Sets the default value for the id column in the asset_keys table to use the next value from the asset_keys_id_seq sequence. This enables automatic ID generation for new records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Processing Second Dagster Step Input Event in JSON\nDESCRIPTION: JSON record for another step input event, showing successful processing of input '_b' for the 'build_model' solid. Contains type check data and pipeline execution context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"_b\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"_b\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Got input \\\"_b\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_model - STEP_INPUT - Got input \\\"_b\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_model\", \"timestamp\": 1609894319.087975, \"user_message\": \"Got input \\\"_b\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Logging Step Output - JSON\nDESCRIPTION: Logs the output of the 'raw_file_events' step, indicating that it successfully yielded output of type 'Any'. This event includes details about the type check results and the corresponding output handle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"raw_file_events.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_events\", \"solid_definition\": \"raw_file_events\", \"step_key\": \"raw_file_events.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_events\", \"name\": \"raw_file_events\", \"parent\": null}, \"step_key\": \"raw_file_events.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"raw_file_events.compute\\\", \\\"result\\\"], \\\"type_check_data\\\": [true, \\\"result\\\", null, []]}\\n               solid = \\\"raw_file_events\\\"\\n    solid_definition = \\\"raw_file_events\\\"\\n            step_key = \\\"raw_file_events.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_events.compute\", \"timestamp\": 1576110682.858183, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint Between Runs and Snapshots in SQL\nDESCRIPTION: Adds a foreign key constraint to the runs table referencing the snapshots table by snapshot_id to maintain referential integrity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_56\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Importing Normalized Cereal Data into PostgreSQL\nDESCRIPTION: This SQL statement uses the COPY command to bulk import normalized cereal data into the 'public.normalized_cereals' table. The data includes fields such as id, name, manufacturer, type, and various nutritional values for 76 cereal products.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.normalized_cereals (id, name, mfr, type, calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, shelf, weight, cups, rating) FROM stdin;\n1\t100% Bran\tN\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t393.93939393939394\t30.303030303030305\t15.151515151515152\t18.18181818181818\t848.4848484848485\t75.75757575757575\t3\t3.0303030303030303\t0.33\t68.402973\n2\t100% Natural Bran\tQ\tC\t120\t3\t5\t15\t2\t8\t8\t135\t0\t3\t1\t1\t33.983679\n3\tAll-Bran\tK\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t787.8787878787879\t27.272727272727273\t21.21212121212121\t15.151515151515152\t969.6969696969697\t75.75757575757575\t3\t3.0303030303030303\t0.33\t59.425505\n4\tAll-Bran with Extra Fiber\tK\tC\t100\t8\t0\t280\t28\t16\t0\t660\t50\t3\t2\t0.5\t93.704912\n5\tAlmond Delight\tR\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t266.66666666666663\t1.3333333333333333\t18.666666666666664\t10.666666666666666\t-1.3333333333333333\t33.33333333333333\t3\t1.3333333333333333\t0.75\t34.384843\n6\tApple Cinnamon Cheerios\tG\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t240\t2\t14\t13.333333333333332\t93.33333333333333\t33.33333333333333\t1\t1.3333333333333333\t0.75\t29.509541\n7\tApple Jacks\tK\tC\t110\t2\t0\t125\t1\t11\t14\t30\t25\t2\t1\t1\t33.174094\n8\tBasic 4\tG\tC\t173.33333333333331\t4\t2.6666666666666665\t280\t2.6666666666666665\t24\t10.666666666666666\t133.33333333333331\t33.33333333333333\t3\t1.7733333333333334\t0.75\t37.038562\n9\tBran Chex\tR\tC\t134.32835820895522\t2.9850746268656714\t1.4925373134328357\t298.5074626865671\t5.970149253731343\t22.388059701492537\t8.955223880597014\t186.56716417910445\t37.31343283582089\t1\t1.4925373134328357\t0.67\t49.120253\n10\tBran Flakes\tP\tC\t134.32835820895522\t4.477611940298507\t0\t313.4328358208955\t7.462686567164178\t19.402985074626862\t7.462686567164178\t283.58208955223876\t37.31343283582089\t3\t1.4925373134328357\t0.67\t53.313813\n11\tCap'n'Crunch\tQ\tC\t160\t1.3333333333333333\t2.6666666666666665\t293.3333333333333\t0\t16\t16\t46.666666666666664\t33.33333333333333\t2\t1.3333333333333333\t0.75\t18.042851\n12\tCheerios\tG\tC\t88\t4.800000000000001\t1.6\t232\t1.6\t13.600000000000001\t0.8\t84\t20\t1\t0.8\t1.25\t50.764999\n13\tCinnamon Toast Crunch\tG\tC\t160\t1.3333333333333333\t4\t280\t0\t17.333333333333332\t12\t60\t33.33333333333333\t2\t1.3333333333333333\t0.75\t19.823573\n14\tClusters\tG\tC\t220\t6\t4\t280\t4\t26\t14\t210\t50\t3\t2\t0.5\t40.400208\n15\tCocoa Puffs\tG\tC\t110\t1\t1\t180\t0\t12\t13\t55\t25\t2\t1\t1\t22.736446\n16\tCorn Chex\tR\tC\t110\t2\t0\t280\t0\t22\t3\t25\t25\t1\t1\t1\t41.445019\n17\tCorn Flakes\tK\tC\t100\t2\t0\t290\t1\t21\t2\t35\t25\t1\t1\t1\t45.863324\n18\tCorn Pops\tK\tC\t110\t1\t0\t90\t1\t13\t12\t20\t25\t2\t1\t1\t35.782791\n19\tCount Chocula\tG\tC\t110\t1\t1\t180\t0\t12\t13\t65\t25\t2\t1\t1\t22.396513\n20\tCracklin' Oat Bran\tK\tC\t220\t6\t6\t280\t8\t20\t14\t320\t50\t3\t2\t0.5\t40.448772\n21\tCream of Wheat (Quick)\tN\tH\t100\t3\t0\t80\t1\t21\t0\t-1\t0\t2\t1\t1\t64.533816\n22\tCrispix\tK\tC\t110\t2\t0\t220\t1\t21\t3\t30\t25\t3\t1\t1\t46.895644\n23\tCrispy Wheat & Raisins\tG\tC\t133.33333333333331\t2.6666666666666665\t1.3333333333333333\t186.66666666666666\t2.6666666666666665\t14.666666666666666\t13.333333333333332\t160\t33.33333333333333\t3\t1.3333333333333333\t0.75\t36.176196\n24\tDouble Chex\tR\tC\t133.33333333333331\t2.6666666666666665\t0\t253.33333333333331\t1.3333333333333333\t24\t6.666666666666666\t106.66666666666666\t33.33333333333333\t3\t1.3333333333333333\t0.75\t44.330856\n25\tFroot Loops\tK\tC\t110\t2\t1\t125\t1\t11\t13\t30\t25\t2\t1\t1\t32.207582\n26\tFrosted Flakes\tK\tC\t146.66666666666666\t1.3333333333333333\t0\t266.66666666666663\t1.3333333333333333\t18.666666666666664\t14.666666666666666\t33.33333333333333\t33.33333333333333\t1\t1.3333333333333333\t0.75\t31.435973\n27\tFrosted Mini-Wheats\tK\tC\t125\t3.75\t0\t0\t3.75\t17.5\t8.75\t125\t31.25\t2\t1.25\t0.8\t58.345141\n28\tFruit & Fibre Dates; Walnuts; and Oats\tP\tC\t179.1044776119403\t4.477611940298507\t2.9850746268656714\t238.8059701492537\t7.462686567164178\t17.91044776119403\t14.925373134328357\t298.5074626865671\t37.31343283582089\t3\t1.8656716417910446\t0.67\t40.917047\n29\tFruitful Bran\tK\tC\t179.1044776119403\t4.477611940298507\t0\t358.2089552238806\t7.462686567164178\t20.8955223880597\t17.91044776119403\t283.58208955223876\t37.31343283582089\t3\t1.9850746268656716\t0.67\t41.015492\n30\tFruity Pebbles\tP\tC\t146.66666666666666\t1.3333333333333333\t1.3333333333333333\t180\t0\t17.333333333333332\t16\t33.33333333333333\t33.33333333333333\t2\t1.3333333333333333\t0.75\t28.025765\n31\tGolden Crisp\tP\tC\t113.63636363636364\t2.272727272727273\t0\t51.13636363636364\t0\t12.500000000000002\t17.045454545454547\t45.45454545454546\t28.40909090909091\t1\t1.1363636363636365\t0.88\t35.252444\n32\tGolden Grahams\tG\tC\t146.66666666666666\t1.3333333333333333\t1.3333333333333333\t373.3333333333333\t0\t20\t12\t60\t33.33333333333333\t2\t1.3333333333333333\t0.75\t23.804043\n33\tGrape Nuts Flakes\tP\tC\t113.63636363636364\t3.409090909090909\t1.1363636363636365\t159.0909090909091\t3.409090909090909\t17.045454545454547\t5.6818181818181825\t96.5909090909091\t28.40909090909091\t3\t1.1363636363636365\t0.88\t52.076897\n34\tGrape-Nuts\tP\tC\t440\t12\t0\t680\t12\t68\t12\t360\t100\t3\t4\t0.25\t53.371007\n35\tGreat Grains Pecan\tP\tC\t363.6363636363636\t9.09090909090909\t9.09090909090909\t227.27272727272728\t9.09090909090909\t39.39393939393939\t12.121212121212121\t303.030303030303\t75.75757575757575\t3\t3.0303030303030303\t0.33\t45.811716\n36\tHoney Graham Ohs\tQ\tC\t120\t1\t2\t220\t1\t12\t11\t45\t25\t2\t1\t1\t21.871292\n37\tHoney Nut Cheerios\tG\tC\t146.66666666666666\t4\t1.3333333333333333\t333.3333333333333\t2\t15.333333333333332\t13.333333333333332\t120\t33.33333333333333\t1\t1.3333333333333333\t0.75\t31.072217\n38\tHoney-comb\tP\tC\t82.70676691729322\t0.7518796992481203\t0\t135.33834586466165\t0\t10.526315789473683\t8.270676691729323\t26.31578947368421\t18.796992481203006\t1\t0.7518796992481203\t1.33\t28.742414\n39\tJust Right Crunchy  Nuggets\tK\tC\t110\t2\t1\t170\t1\t17\t6\t60\t100\t3\t1\t1\t36.523683\n40\tJust Right Fruit & Nut\tK\tC\t186.66666666666666\t4\t1.3333333333333333\t226.66666666666666\t2.6666666666666665\t26.666666666666664\t12\t126.66666666666666\t133.33333333333331\t3\t1.7333333333333334\t0.75\t36.471512\n41\tKix\tG\tC\t73.33333333333333\t1.3333333333333333\t0.6666666666666666\t173.33333333333331\t0\t14\t2\t26.666666666666664\t16.666666666666664\t2\t0.6666666666666666\t1.5\t39.241114\n42\tLife\tQ\tC\t149.25373134328356\t5.970149253731343\t2.9850746268656714\t223.88059701492534\t2.9850746268656714\t17.91044776119403\t8.955223880597014\t141.79104477611938\t37.31343283582089\t2\t1.4925373134328357\t0.67\t45.328074\n43\tLucky Charms\tG\tC\t110\t2\t1\t180\t0\t12\t12\t55\t25\t2\t1\t1\t26.734515\n44\tMaypo\tA\tH\t100\t4\t1\t0\t0\t16\t3\t95\t25\t2\t1\t1\t54.850917\n45\tMuesli Raisins; Dates; & Almonds\tR\tC\t150\t4\t3\t95\t3\t16\t11\t170\t25\t3\t1\t1\t37.136863\n46\tMuesli Raisins; Peaches; & Pecans\tR\tC\t150\t4\t3\t150\t3\t16\t11\t170\t25\t3\t1\t1\t34.139765\n47\tMueslix Crispy Blend\tK\tC\t238.8059701492537\t4.477611940298507\t2.9850746268656714\t223.88059701492534\t4.477611940298507\t25.373134328358205\t19.402985074626862\t238.8059701492537\t37.31343283582089\t3\t2.2388059701492535\t0.67\t30.313351\n48\tMulti-Grain Cheerios\tG\tC\t100\t2\t1\t220\t2\t15\t6\t90\t25\t1\t1\t1\t40.105965\n49\tNut&Honey Crunch\tK\tC\t179.1044776119403\t2.9850746268656714\t1.4925373134328357\t283.58208955223876\t0\t22.388059701492537\t13.432835820895521\t59.70149253731343\t37.31343283582089\t2\t1.4925373134328357\t0.67\t29.924285\n50\tNutri-Grain Almond-Raisin\tK\tC\t208.955223880597\t4.477611940298507\t2.9850746268656714\t328.35820895522386\t4.477611940298507\t31.343283582089548\t10.44776119402985\t194.02985074626864\t37.31343283582089\t3\t1.9850746268656716\t0.67\t40.69232\n51\tNutri-grain Wheat\tK\tC\t90\t3\t0\t170\t3\t18\t2\t90\t25\t3\t1\t1\t59.642837\n52\tOatmeal Raisin Crisp\tG\tC\t260\t6\t4\t340\t3\t27\t20\t240\t50\t3\t2.5\t0.5\t30.450843\n53\tPost Nat. Raisin Bran\tP\tC\t179.1044776119403\t4.477611940298507\t1.4925373134328357\t298.5074626865671\t8.955223880597014\t16.417910447761194\t20.8955223880597\t388.0597014925373\t37.31343283582089\t3\t1.9850746268656716\t0.67\t37.840594\n54\tProduct 19\tK\tC\t100\t3\t0\t320\t1\t20\t3\t45\t100\t3\t1\t1\t41.50354\n55\tPuffed Rice\tQ\tC\t50\t1\t0\t0\t0\t13\t0\t15\t0\t3\t0.5\t1\t60.756112\n56\tPuffed Wheat\tQ\tC\t50\t2\t0\t0\t1\t10\t0\t50\t0\t3\t0.5\t1\t63.005645\n57\tQuaker Oat Squares\tQ\tC\t200\t8\t2\t270\t4\t28\t12\t220\t50\t3\t2\t0.5\t49.511874\n58\tQuaker Oatmeal\tQ\tH\t149.25373134328356\t7.462686567164178\t2.9850746268656714\t0\t4.029850746268656\t-1.4925373134328357\t-1.4925373134328357\t164.17910447761193\t0\t1\t1.4925373134328357\t0.67\t50.828392\n59\tRaisin Bran\tK\tC\t160\t4\t1.3333333333333333\t280\t6.666666666666666\t18.666666666666664\t16\t320\t33.33333333333333\t2\t1.7733333333333334\t0.75\t39.259197\n60\tRaisin Nut Bran\tG\tC\t200\t6\t4\t280\t5\t21\t16\t280\t50\t3\t2\t0.5\t39.7034\n61\tRaisin Squares\tK\tC\t180\t4\t0\t0\t4\t30\t12\t220\t50\t3\t2\t0.5\t55.333142\n62\tRice Chex\tR\tC\t97.34513274336284\t0.8849557522123894\t0\t212.38938053097345\t0\t20.353982300884958\t1.7699115044247788\t26.548672566371682\t22.123893805309734\t1\t0.8849557522123894\t1.13\t41.998933\n63\tRice Krispies\tK\tC\t110\t2\t0\t290\t0\t22\t3\t35\t25\t1\t1\t1\t40.560159\n64\tShredded Wheat\tN\tC\t80\t2\t0\t0\t3\t16\t0\t95\t0\t1\t0.83\t1\t68.235885\n65\tShredded Wheat 'n'Bran\tN\tC\t134.32835820895522\t4.477611940298507\t0\t0\t5.970149253731343\t28.35820895522388\t0\t208.955223880597\t0\t1\t1.4925373134328357\t0.67\t74.472949\n66\tShredded Wheat spoon size\tN\tC\t134.32835820895522\t4.477611940298507\t0\t0\t4.477611940298507\t29.850746268656714\t0\t179.1044776119403\t0\t1\t1.4925373134328357\t0.67\t72.801787\n67\tSmacks\tK\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t93.33333333333333\t1.3333333333333333\t12\t20\t53.33333333333333\t33.33333333333333\t2\t1.3333333333333333\t0.75\t31.230054\n68\tSpecial K\tK\tC\t110\t6\t0\t230\t1\t16\t3\t55\t25\t1\t1\t1\t53.131324\n69\tStrawberry Fruit Wheats\tN\tC\t90\t2\t0\t15\t3\t15\t5\t90\t25\t2\t1\t1\t59.363993\n70\tTotal Corn Flakes\tG\tC\t110\t2\t1\t200\t0\t21\t3\t35\t100\t3\t1\t1\t38.839746\n71\tTotal Raisin Bran\tG\tC\t140\t3\t1\t190\t4\t15\t14\t230\t100\t3\t1.5\t1\t28.592785\n72\tTotal Whole Grain\tG\tC\t100\t3\t1\t200\t3\t16\t3\t110\t100\t3\t1\t1\t46.658844\n73\tTriples\tG\tC\t146.66666666666666\t2.6666666666666665\t1.3333333333333333\t333.3333333333333\t0\t28\t4\t80\t33.33333333333333\t3\t1.3333333333333333\t0.75\t39.106174\n74\tTrix\tG\tC\t110\t1\t1\t140\t0\t13\t12\t25\t25\t2\t1\t1\t27.753301\n75\tWheat Chex\tR\tC\t149.25373134328356\t4.477611940298507\t1.4925373134328357\t343.2835820895522\t4.477611940298507\t25.373134328358205\t4.477611940298507\t171.6417910447761\t37.31343283582089\t1\t1.4925373134328357\t0.67\t49.787445\n76\tWheaties\tG\tC\t100\t3\t1\t200\t3\t17\t3\t110\t25\t1\t1\t1\t51.592193\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster-Airbyte Integration\nDESCRIPTION: Command to install the dagster-airbyte Python package via pip\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/airbyte/airbyte-oss.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-airbyte\n```\n\n----------------------------------------\n\nTITLE: Create event_logs_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `event_logs_id_seq` to generate unique IDs for the `event_logs` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Inserting Event Logs into Dagster Database\nDESCRIPTION: This SQL snippet inserts event log entries into the event_logs table. These entries contain detailed information about pipeline executions, including timestamps, event types, and JSON-formatted event data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key, asset_key, partition) FROM stdin;\n1\t26c370d7-64b4-40d6-93ba-1f821f1af0e3\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_STARTING\", \"logging_tags\": {}, \"message\": null, \"pid\": null, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"step_key\": null, \"timestamp\": 1665619791.7340128, \"user_message\": \"\"}\tPIPELINE_STARTING\t2022-10-13 00:09:51.734013\t\\N\t\\N\t\\N\n2\t26c370d7-64b4-40d6-93ba-1f821f1af0e3\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"64856\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for run (pid: 64856).\", \"pid\": null, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"step_key\": null, \"timestamp\": 1665619793.4486232, \"user_message\": \"\"}\tENGINE_EVENT\t2022-10-13 00:09:53.448623\t\\N\t\\N\t\\N\n3\t26c370d7-64b4-40d6-93ba-1f821f1af0e3\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of run for \\\"succeeds_job\\\".\", \"pid\": 64856, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"step_key\": null, \"timestamp\": 1665619796.019179, \"user_message\": \"Started execution of run for \\\"succeeds_job\\\".\"}\tPIPELINE_START\t2022-10-13 00:09:56.019179\t\\N\t\\N\t\\N\n4\t26c370d7-64b4-40d6-93ba-1f821f1af0e3\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"64856\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['succeeds']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps using multiprocess executor: parent process (pid: 64856)\", \"pid\": 64856, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"step_key\": null, \"timestamp\": 1665619796.093468, \"user_message\": \"Executing steps using multiprocess executor: parent process (pid: 64856)\"}\tENGINE_EVENT\t2022-10-13 00:09:56.093468\t\\N\t\\N\t\\N\n5\t26c370d7-64b4-40d6-93ba-1f821f1af0e3\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"step_process_start\", \"metadata_entries\": []}, \"event_type_value\": \"STEP_WORKER_STARTING\", \"logging_tags\": {\"pipeline_name\": \"succeeds_job\", \"pipeline_tags\": \"{'./dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/lr/mcmhlx2177953tcj5m7v8l3h0000gn/T/tmplz8aiibf\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"solid_name\": \"succeeds\", \"step_key\": \"succeeds\"}, \"message\": \"Launching subprocess for \\\"succeeds\\\".\", \"pid\": 64856, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"succeeds\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"succeeds\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"succeeds\", \"parent\": null}}, \"step_key\": \"succeeds\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"step_key\": \"succeeds\", \"timestamp\": 1665619796.104007, \"user_message\": \"Launching subprocess for \\\"succeeds\\\".\"}\tSTEP_WORKER_STARTING\t2022-10-13 00:09:56.104007\tsucceeds\t\\N\t\\N\n6\t26c370d7-64b4-40d6-93ba-1f821f1af0e3\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"step_process_start\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"64882\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"STEP_WORKER_STARTED\", \"logging_tags\": {}, \"message\": \"Executing step \\\"succeeds\\\" in subprocess.\", \"pid\": 64882, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": \"succeeds\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"step_key\": \"succeeds\", \"timestamp\": 1665619799.623569, \"user_message\": \"Executing step \\\"succeeds\\\" in subprocess.\"}\tSTEP_WORKER_STARTED\t2022-10-13 00:09:59.623569\tsucceeds\t\\N\t\\N\n7\t26c370d7-64b4-40d6-93ba-1f821f1af0e3\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"RESOURCE_INIT_STARTED\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [io_manager].\", \"pid\": 64882, \"pipeline_name\": \"succeeds_job\", \"solid_handle\": null, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"succeeds\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"succeeds\", \"parent\": null}}, \"step_key\": \"succeeds\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"succeeds_job\", \"run_id\": \"26c370d7-64b4-40d6-93ba-1f821f1af0e3\", \"step_key\": \"succeeds\", \"timestamp\": 1665619799.6432981, \"user_message\": \"Starting initialization of resources [io_manager].\"}\tRESOURCE_INIT_STARTED\t2022-10-13 00:09:59.643298\tsucceeds\t\\N\t\\N\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating Event Log Table in PostgreSQL for Dagster\nDESCRIPTION: SQL code to create the event_log table which stores detailed event records for Dagster pipeline runs. It includes columns for event ID, run ID, and a JSON payload containing event details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_54\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.event_log (\n    id integer NOT NULL,\n    run_id text,\n    event text,\n    CONSTRAINT event_log_pkey PRIMARY KEY (id)\n);\n```\n\n----------------------------------------\n\nTITLE: Creating public.bulk_actions Table and Sequence\nDESCRIPTION: This snippet creates the 'bulk_actions' table in the 'public' schema to store information about bulk actions. The table includes columns for ID, key, status, timestamp, and the action's body. An associated sequence 'bulk_actions_id_seq' is also defined to auto-generate unique IDs for each bulk action.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.bulk_actions (\n    id integer NOT NULL,\n    key character varying(32) NOT NULL,\n    status character varying(255) NOT NULL,\n    \\\"timestamp\\\" timestamp without time zone NOT NULL,\n    body text\n);\n\n\nALTER TABLE public.bulk_actions OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.bulk_actions_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.bulk_actions_id_seq OWNER TO test;\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.bulk_actions_id_seq OWNED BY public.bulk_actions.id;\"\n```\n\n----------------------------------------\n\nTITLE: Examining Dagster Pipeline Success Event Structure\nDESCRIPTION: This JSON object represents a pipeline success event in Dagster. It indicates the successful completion of a pipeline execution with relevant metadata such as pipeline name and run ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of pipeline \\\"foo\\\".\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"foo\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": null, \"timestamp\": 1610466063.7197, \"user_message\": \"Finished execution of pipeline \\\"foo\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Rendering DocCardList Component in JSX\nDESCRIPTION: This snippet renders the DocCardList component in JSX. It is used to display a list of related documentation cards in the Kubernetes agent documentation page.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/index.md#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Index on job_origin_id and status Columns in job_ticks Table\nDESCRIPTION: SQL command to create a composite index on job_origin_id and status columns in the job_ticks table to optimize job status lookups.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n```\n\n----------------------------------------\n\nTITLE: Creating Index on key Column in bulk_actions Table\nDESCRIPTION: SQL command to create an index on the key column in the bulk_actions table to optimize lookups by key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\n```\n\n----------------------------------------\n\nTITLE: Alter snapshots_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `snapshots_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.snapshots_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Database Clone in GitLab CI/CD Pipeline\nDESCRIPTION: GitLab CI/CD pipeline step for triggering the deletion of database clones when a branch is closed or merged.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndeploy:\\n  script:\\n    - dagster-cloud job launch drop_prod_clone\n```\n\n----------------------------------------\n\nTITLE: Creating Unique and Foreign Key Constraints in PostgreSQL for Dagster\nDESCRIPTION: SQL commands that establish a unique constraint on the run_id column in the runs table and a foreign key relationship between run_tags and runs tables. The foreign key ensures referential integrity with cascade deletion.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Alter runs_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `runs_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.runs_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Creating normalized_cereals_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates a sequence 'normalized_cereals_id_seq' utilized for generating unique identifiers for entries in the 'normalized_cereals' table. This ensures every cereal record receives a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: normalized_cereals_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.normalized_cereals_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.normalized_cereals_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Job Origin ID in Job Ticks in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the job_ticks table for the job_origin_id column. This index improves query performance when searching for job ticks by their origin.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_61\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for runs.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `runs` table to be generated by the `runs_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_56\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Variable Assignment and Numerical Computation\nDESCRIPTION: Assigns integer values to variables and performs basic multiplication operation\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/mult_two_numbers.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\na = 2\nb = 3\n\nresult = a * b\nresult\n```\n\n----------------------------------------\n\nTITLE: Scheduling Daily Asset Materialization in Python\nDESCRIPTION: Python code defining a daily schedule for materializing all assets in the Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/components-etl-pipeline-tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster import AssetSelection, ScheduleDefinition, define_asset_job\n\ndaily_jaffle_job = define_asset_job(\"daily_jaffle_job\", selection=\"*\")\n\ndaily_jaffle = ScheduleDefinition(\n    job=daily_jaffle_job,\n    cron_schedule=\"@daily\",\n)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using DocCardList Component in Dagster Documentation\nDESCRIPTION: This snippet imports the DocCardList component from the Docusaurus theme and renders it to display a list of documentation cards for child pages in the Settings section.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/settings/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraints in SQL\nDESCRIPTION: Adds foreign key constraints to the runs and run_tags tables, referencing the snapshots and runs tables respectively, to maintain referential integrity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Importing Run Launcher Module in Python\nDESCRIPTION: Imports the run launcher module from Dagster's core package. This module contains classes for launching Dagster runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.launcher\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents in Sphinx for Dagster Documentation\nDESCRIPTION: This snippet configures the table of contents for the Dagster documentation using Sphinx. It hides the toctree, includes all files matching the specified glob patterns, and sets the document title.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. title:: Home\n\n.. toctree::\n   :hidden:\n   :glob:\n\n   sections/api/apidocs/dagster/*\n   sections/api/apidocs/libraries/*\n   sections/api/apidocs/dagster-dg-cli\n```\n\n----------------------------------------\n\nTITLE: SQL Error on Missing Column in PostgreSQL Asset Materialization Query\nDESCRIPTION: SQL statement that failed due to a missing column 'last_materialization_timestamp' in the 'asset_keys' table. This error occurs when the Dagster instance database schema is out of date and requires migration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT INTO asset_keys (asset_key, last_materialization, last_run_id, last_materialization_timestamp, tags) VALUES (%(asset_key)s, %(last_materialization)s, %(last_run_id)s, %(last_materialization_timestamp)s, %(tags)s) ON CONFLICT (asset_key) DO UPDATE SET last_materialization = %(param_1)s, last_run_id = %(param_2)s, last_materialization_timestamp = %(param_3)s, tags = %(param_4)s RETURNING asset_keys.id\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry for HANDLED_OUTPUT Event\nDESCRIPTION: JSON representation of a Dagster event log entry for a HANDLED_OUTPUT event. This entry confirms the 'result' output from the 'div_two' step was processed by the IO manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"HandledOutputData\", \"manager_key\": \"io_manager\", \"metadata_entries\": [], \"output_name\": \"result\"}, \"event_type_value\": \"HANDLED_OUTPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"div_two\", \"step_key\": \"div_four.div_two\"}, \"message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"div_four.div_two\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}}, \"step_key\": \"div_four.div_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - div_four.div_two - HANDLED_OUTPUT - Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"div_four.div_two\", \"timestamp\": 1640037523.194361, \"user_message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Libraries Dependencies\nDESCRIPTION: This section details a list of dependencies for various Dagster library components, inclusive of automation modules, various integrations, and individual library packages. Each entry is an editable dependency, specified with the `-e` flag, which might include special config like [test], [stubs], or [dev].\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/requirements.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n-e docs/sphinx/_ext/dagster-sphinx\n-e python_modules/automation\n-e python_modules/dagster[pyright,test]\n-e python_modules/dagit\n-e python_modules/dagster-graphql\n-e python_modules/dagster-test\n-e python_modules/dagster-pipes[stubs]\n-e python_modules/dagster-webserver\n-e python_modules/libraries/dagster-airbyte/\n-e python_modules/libraries/dagster-airflow[test_airflow_2]\n-e python_modules/libraries/dagster-aws[stubs,test]\n-e python_modules/libraries/dagster-azure/\n-e python_modules/libraries/dagster-celery/\n-e python_modules/libraries/dagster-celery-docker/\n-e python_modules/libraries/dagster-celery-k8s/\n-e python_modules/libraries/dagster-census/\n-e python_modules/libraries/dagster-dask[yarn,pbs,kube,test]\n-e python_modules/libraries/dagster-databricks/\n-e python_modules/libraries/dagster-datadog/\n-e python_modules/libraries/dagster-datahub/\n-e python_modules/libraries/dagster-dbt/\n-e python_modules/libraries/dagster-deltalake/\n-e python_modules/libraries/dagster-deltalake-pandas/\n-e python_modules/libraries/dagster-deltalake-polars/\n-e python_modules/libraries/dagster-dlt/\n-e python_modules/libraries/dagster-dg[test]\n-e python_modules/libraries/dagster-cloud-cli[test]\n-e python_modules/libraries/dagster-docker/\n-e python_modules/libraries/dagster-duckdb/\n-e python_modules/libraries/dagster-duckdb-pandas/\n-e python_modules/libraries/dagster-duckdb-pyspark/\n-e python_modules/libraries/dagster-duckdb-polars/\n-e python_modules/libraries/dagster-embedded-elt\n-e python_modules/libraries/dagster-fivetran/\n-e python_modules/libraries/dagster-gcp[test,dataproc]\n-e python_modules/libraries/dagster-gcp-pandas[test]\n-e python_modules/libraries/dagster-gcp-pyspark/\n-e python_modules/libraries/dagster-ge/\n-e python_modules/libraries/dagster-github/\n-e python_modules/libraries/dagster-k8s/\n-e python_modules/libraries/dagster-managed-elements/\n-e python_modules/libraries/dagster-mlflow/\n-e python_modules/libraries/dagster-msteams/\n-e python_modules/libraries/dagster-mysql/\n-e python_modules/libraries/dagster-looker/\n-e python_modules/libraries/dagster-openai/\n-e python_modules/libraries/dagster-pagerduty/\n-e python_modules/libraries/dagster-pandas/\n-e python_modules/libraries/dagster-pandera/\n-e python_modules/libraries/dagster-papertrail/\n-e python_modules/libraries/dagster-postgres/\n-e python_modules/libraries/dagster-powerbi\n-e python_modules/libraries/dagster-prometheus/\n-e python_modules/libraries/dagster-pyspark/\n-e python_modules/libraries/dagster-shared/\n-e python_modules/libraries/dagster-sigma[test]\n-e python_modules/libraries/dagster-slack/\n-e python_modules/libraries/dagster-sling/\n-e python_modules/libraries/dagster-snowflake/\n-e python_modules/libraries/dagster-snowflake-pandas/\n-e python_modules/libraries/dagster-snowflake-pyspark/\n-e python_modules/libraries/dagster-spark/\n-e python_modules/libraries/dagster-ssh/\n-e python_modules/libraries/dagster-tableau\n-e python_modules/libraries/dagster-twilio/\n-e python_modules/libraries/dagster-wandb[dev]\n-e python_modules/libraries/dagstermill/\n```\n\n----------------------------------------\n\nTITLE: Defining Run Tags ID Sequence\nDESCRIPTION: Initializes a sequence for the 'run_tags' table's 'id' column to generate sequential integer values starting from 1.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating public.event_logs Table and Sequence\nDESCRIPTION: This snippet creates the 'event_logs' table in the 'public' schema for storing event logs generated during Dagster runs.  It includes columns for ID, run ID, event data, event type, timestamp, step key, asset key, and partition.  An associated sequence 'event_logs_id_seq' is created to automatically generate unique IDs for each event log.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \\\"timestamp\\\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\n\n\nALTER TABLE public.event_logs OWNER TO test;\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\"\n```\n\n----------------------------------------\n\nTITLE: Alter daemon_heartbeats Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `daemon_heartbeats` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.daemon_heartbeats OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Finishing Dagster Pipeline Process Steps\nDESCRIPTION: JSON log event indicating the completion of multiple steps within a Dagster pipeline process. The event lists all the completed step keys and includes process metadata such as process ID and execution time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"18688\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['ingest_costs', 'ingest_traffic', 'persist_costs', 'persist_traffic', 'build_cost_dashboard', 'build_model', 'build_traffic_dashboard', 'train_model', 'persist_model']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 18688) in 2m43s\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - ENGINE_EVENT - Finished steps in process (pid: 18688) in 2m43s\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608667064.381969, \"user_message\": \"Finished steps in process (pid: 18688) in 2m43s\"}\n```\n\n----------------------------------------\n\nTITLE: Creating public.runs Table and Sequence\nDESCRIPTION: This snippet creates the 'runs' table in the 'public' schema to store information about Dagster runs.  It includes columns for ID, run ID, snapshot ID, pipeline name, mode, status, run body, partition, partition set, creation timestamp, update timestamp, start time, and end time. An associated sequence 'runs_id_seq' is also created to automatically generate unique IDs for each run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    mode text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    start_time double precision,\n    end_time double precision\n);\n\n\nALTER TABLE public.runs OWNER TO test;\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\"\n```\n\n----------------------------------------\n\nTITLE: Creating Step Key Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the step_key column of the event_logs table to optimize queries that filter by step key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_58\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n```\n\n----------------------------------------\n\nTITLE: Defining Overview Documentation Block in dbt using Jinja\nDESCRIPTION: This snippet defines a dbt documentation block using Jinja templating syntax. It provides an overview of the 'jaffle_shop' fictional ecommerce store project, mentioning that it's used for testing dbt code and including a link to the source code repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/pre_packaged_jaffle_shop/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Pandas IO Manager Configuration\nDESCRIPTION: ReStructuredText directive for documenting the DeltaLakePandasIOManager with IOManagerDefinition annotation\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake-pandas.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: DeltaLakePandasIOManager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Inserting Cereal Product Data into PostgreSQL Table\nDESCRIPTION: This SQL command inserts a large dataset of cereal product information into the 'normalized_cereals' table. It includes various attributes such as name, manufacturer, nutritional information, and ratings.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_60\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.normalized_cereals (id, name, mfr, type, calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, shelf, weight, cups, rating) FROM stdin;\n1\t100% Bran\tN\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t393.93939393939394\t30.303030303030305\t15.151515151515152\t18.18181818181818\t848.4848484848485\t75.75757575757575\t3\t3.0303030303030303\t0.33\t68.402973\n2\t100% Natural Bran\tQ\tC\t120\t3\t5\t15\t2\t8\t8\t135\t0\t3\t1\t1\t33.983679\n3\tAll-Bran\tK\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t787.8787878787879\t27.272727272727273\t21.21212121212121\t15.151515151515152\t969.6969696969697\t75.75757575757575\t3\t3.0303030303030303\t0.33\t59.425505\n4\tAll-Bran with Extra Fiber\tK\tC\t100\t8\t0\t280\t28\t16\t0\t660\t50\t3\t2\t0.5\t93.704912\n5\tAlmond Delight\tR\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t266.66666666666663\t1.3333333333333333\t18.666666666666664\t10.666666666666666\t-1.3333333333333333\t33.33333333333333\t3\t1.3333333333333333\t0.75\t34.384843\n6\tApple Cinnamon Cheerios\tG\tC\t146.66666666666666\t2.6666666666666665\t2.6666666666666665\t240\t2\t14\t13.333333333333332\t93.33333333333333\t33.33333333333333\t1\t1.3333333333333333\t0.75\t29.509541\n7\tApple Jacks\tK\tC\t110\t2\t0\t125\t1\t11\t14\t30\t25\t2\t1\t1\t33.174094\n8\tBasic 4\tG\tC\t173.33333333333331\t4\t2.6666666666666665\t280\t2.6666666666666665\t24\t10.666666666666666\t133.33333333333331\t33.33333333333333\t3\t1.7733333333333334\t0.75\t37.038562\n9\tBran Chex\tR\tC\t134.32835820895522\t2.9850746268656714\t1.4925373134328357\t298.5074626865671\t5.970149253731343\t22.388059701492537\t8.955223880597014\t186.56716417910445\t37.31343283582089\t1\t1.4925373134328357\t0.67\t49.120253\n10\tBran Flakes\tP\tC\t134.32835820895522\t4.477611940298507\t0\t313.4328358208955\t7.462686567164178\t19.402985074626862\t7.462686567164178\t283.58208955223876\t37.31343283582089\t3\t1.4925373134328357\t0.67\t53.313813\n11\tCap'n'Crunch\tQ\tC\t160\t1.3333333333333333\t2.6666666666666665\t293.3333333333333\t0\t16\t16\t46.666666666666664\t33.33333333333333\t2\t1.3333333333333333\t0.75\t18.042851\n12\tCheerios\tG\tC\t88\t4.800000000000001\t1.6\t232\t1.6\t13.600000000000001\t0.8\t84\t20\t1\t0.8\t1.25\t50.764999\n13\tCinnamon Toast Crunch\tG\tC\t160\t1.3333333333333333\t4\t280\t0\t17.333333333333332\t12\t60\t33.33333333333333\t2\t1.3333333333333333\t0.75\t19.823573\n14\tClusters\tG\tC\t220\t6\t4\t280\t4\t26\t14\t210\t50\t3\t2\t0.5\t40.400208\n15\tCocoa Puffs\tG\tC\t110\t1\t1\t180\t0\t12\t13\t55\t25\t2\t1\t1\t22.736446\n16\tCorn Chex\tR\tC\t110\t2\t0\t280\t0\t22\t3\t25\t25\t1\t1\t1\t41.445019\n17\tCorn Flakes\tK\tC\t100\t2\t0\t290\t1\t21\t2\t35\t25\t1\t1\t1\t45.863324\n18\tCorn Pops\tK\tC\t110\t1\t0\t90\t1\t13\t12\t20\t25\t2\t1\t1\t35.782791\n19\tCount Chocula\tG\tC\t110\t1\t1\t180\t0\t12\t13\t65\t25\t2\t1\t1\t22.396513\n20\tCracklin' Oat Bran\tK\tC\t220\t6\t6\t280\t8\t20\t14\t320\t50\t3\t2\t0.5\t40.448772\n21\tCream of Wheat (Quick)\tN\tH\t100\t3\t0\t80\t1\t21\t0\t-1\t0\t2\t1\t1\t64.533816\n22\tCrispix\tK\tC\t110\t2\t0\t220\t1\t21\t3\t30\t25\t3\t1\t1\t46.895644\n23\tCrispy Wheat & Raisins\tG\tC\t133.33333333333331\t2.6666666666666665\t1.3333333333333333\t186.66666666666666\t2.6666666666666665\t14.666666666666666\t13.333333333333332\t160\t33.33333333333333\t3\t1.3333333333333333\t0.75\t36.176196\n24\tDouble Chex\tR\tC\t133.33333333333331\t2.6666666666666665\t0\t253.33333333333331\t1.3333333333333333\t24\t6.666666666666666\t106.66666666666666\t33.33333333333333\t3\t1.3333333333333333\t0.75\t44.330856\n25\tFroot Loops\tK\tC\t110\t2\t1\t125\t1\t11\t13\t30\t25\t2\t1\t1\t32.207582\n26\tFrosted Flakes\tK\tC\t146.66666666666666\t1.3333333333333333\t0\t266.66666666666663\t1.3333333333333333\t18.666666666666664\t14.666666666666666\t33.33333333333333\t33.33333333333333\t1\t1.3333333333333333\t0.75\t31.435973\n27\tFrosted Mini-Wheats\tK\tC\t125\t3.75\t0\t0\t3.75\t17.5\t8.75\t125\t31.25\t2\t1.25\t0.8\t58.345141\n28\tFruit & Fibre Dates; Walnuts; and Oats\tP\tC\t179.1044776119403\t4.477611940298507\t2.9850746268656714\t238.8059701492537\t7.462686567164178\t17.91044776119403\t14.925373134328357\t298.5074626865671\t37.31343283582089\t3\t1.8656716417910446\t0.67\t40.917047\n29\tFruitful Bran\tK\tC\t179.1044776119403\t4.477611940298507\t0\t358.2089552238806\t7.462686567164178\t20.8955223880597\t17.91044776119403\t283.58208955223876\t37.31343283582089\t3\t1.9850746268656716\t0.67\t41.015492\n30\tFruity Pebbles\tP\tC\t146.66666666666666\t1.3333333333333333\t1.3333333333333333\t180\t0\t17.333333333333332\t16\t33.33333333333333\t33.33333333333333\t2\t1.3333333333333333\t0.75\t28.025765\n31\tGolden Crisp\tP\tC\t113.63636363636364\t2.272727272727273\t0\t51.13636363636364\t0\t12.500000000000002\t17.045454545454547\t45.45454545454546\t28.40909090909091\t1\t1.1363636363636365\t0.88\t35.252444\n32\tGolden Grahams\tG\tC\t146.66666666666666\t1.3333333333333333\t1.3333333333333333\t373.3333333333333\t0\t20\t12\t60\t33.33333333333333\t2\t1.3333333333333333\t0.75\t23.804043\n33\tGrape Nuts Flakes\tP\tC\t113.63636363636364\t3.409090909090909\t1.1363636363636365\t159.0909090909091\t3.409090909090909\t17.045454545454547\t5.6818181818181825\t96.5909090909091\t28.40909090909091\t3\t1.1363636363636365\t0.88\t52.076897\n34\tGrape-Nuts\tP\tC\t440\t12\t0\t680\t12\t68\t12\t360\t100\t3\t4\t0.25\t53.371007\n35\tGreat Grains Pecan\tP\tC\t363.6363636363636\t9.09090909090909\t9.09090909090909\t227.27272727272728\t9.09090909090909\t39.39393939393939\t12.121212121212121\t303.030303030303\t75.75757575757575\t3\t3.0303030303030303\t0.33\t45.811716\n36\tHoney Graham Ohs\tQ\tC\t120\t1\t2\t220\t1\t12\t11\t45\t25\t2\t1\t1\t21.871292\n37\tHoney Nut Cheerios\tG\tC\t146.66666666666666\t4\t1.3333333333333333\t333.3333333333333\t2\t15.333333333333332\t13.333333333333332\t120\t33.33333333333333\t1\t1.3333333333333333\t0.75\t31.072217\n38\tHoney-comb\tP\tC\t82.70676691729322\t0.7518796992481203\t0\t135.33834586466165\t0\t10.526315789473683\t8.270676691729323\t26.31578947368421\t18.796992481203006\t1\t0.7518796992481203\t1.33\t28.742414\n39\tJust Right Crunchy  Nuggets\tK\tC\t110\t2\t1\t170\t1\t17\t6\t60\t100\t3\t1\t1\t36.523683\n40\tJust Right Fruit & Nut\tK\tC\t186.66666666666666\t4\t1.3333333333333333\t226.66666666666666\t2.6666666666666665\t26.666666666666664\t12\t126.66666666666666\t133.33333333333331\t3\t1.7333333333333334\t0.75\t36.471512\n41\tKix\tG\tC\t73.33333333333333\t1.3333333333333333\t0.6666666666666666\t173.33333333333331\t0\t14\t2\t26.666666666666664\t16.666666666666664\t2\t0.6666666666666666\t1.5\t39.241114\n42\tLife\tQ\tC\t149.25373134328356\t5.970149253731343\t2.9850746268656714\t223.88059701492534\t2.9850746268656714\t17.91044776119403\t8.955223880597014\t141.79104477611938\t37.31343283582089\t2\t1.4925373134328357\t0.67\t45.328074\n43\tLucky Charms\tG\tC\t110\t2\t1\t180\t0\t12\t12\t55\t25\t2\t1\t1\t26.734515\n44\tMaypo\tA\tH\t100\t4\t1\t0\t0\t16\t3\t95\t25\t2\t1\t1\t54.850917\n45\tMuesli Raisins; Dates; & Almonds\tR\tC\t150\t4\t3\t95\t3\t16\t11\t170\t25\t3\t1\t1\t37.136863\n46\tMuesli Raisins; Peaches; & Pecans\tR\tC\t150\t4\t3\t150\t3\t16\t11\t170\t25\t3\t1\t1\t34.139765\n47\tMueslix Crispy Blend\tK\tC\t238.8059701492537\t4.477611940298507\t2.9850746268656714\t223.88059701492534\t4.477611940298507\t25.373134328358205\t19.402985074626862\t238.8059701492537\t37.31343283582089\t3\t2.2388059701492535\t0.67\t30.313351\n48\tMulti-Grain Cheerios\tG\tC\t100\t2\t1\t220\t2\t15\t6\t90\t25\t1\t1\t1\t40.105965\n49\tNut&Honey Crunch\tK\tC\t179.1044776119403\t2.9850746268656714\t1.4925373134328357\t283.58208955223876\t0\t22.388059701492537\t13.432835820895521\t59.70149253731343\t37.31343283582089\t2\t1.4925373134328357\t0.67\t29.924285\n50\tNutri-Grain Almond-Raisin\tK\tC\t208.955223880597\t4.477611940298507\t2.9850746268656714\t328.35820895522386\t4.477611940298507\t31.343283582089548\t10.44776119402985\t194.02985074626864\t37.31343283582089\t3\t1.9850746268656716\t0.67\t40.69232\n51\tNutri-grain Wheat\tK\tC\t90\t3\t0\t170\t3\t18\t2\t90\t25\t3\t1\t1\t59.642837\n52\tOatmeal Raisin Crisp\tG\tC\t260\t6\t4\t340\t3\t27\t20\t240\t50\t3\t2.5\t0.5\t30.450843\n53\tPost Nat. Raisin Bran\tP\tC\t179.1044776119403\t4.477611940298507\t1.4925373134328357\t298.5074626865671\t8.955223880597014\t16.417910447761194\t20.8955223880597\t388.0597014925373\t37.31343283582089\t3\t1.9850746268656716\t0.67\t37.840594\n54\tProduct 19\tK\tC\t100\t3\t0\t320\t1\t20\t3\t45\t100\t3\t1\t1\t41.50354\n55\tPuffed Rice\tQ\tC\t50\t1\t0\t0\t0\t13\t0\t15\t0\t3\t0.5\t1\t60.756112\n56\tPuffed Wheat\tQ\tC\t50\t2\t0\t0\t1\t10\t0\t50\t0\t3\t0.5\t1\t63.005645\n57\tQuaker Oat Squares\tQ\tC\t200\t8\t2\t270\t4\t28\t12\t220\t50\t3\t2\t0.5\t49.511874\n58\tQuaker Oatmeal\tQ\tH\t149.25373134328356\t7.462686567164178\t2.9850746268656714\t0\t4.029850746268656\t-1.4925373134328357\t-1.4925373134328357\t164.17910447761193\t0\t1\t1.4925373134328357\t0.67\t50.828392\n59\tRaisin Bran\tK\tC\t160\t4\t1.3333333333333333\t280\t6.666666666666666\t18.666666666666664\t16\t320\t33.33333333333333\t2\t1.7733333333333334\t0.75\t39.259197\n60\tRaisin Nut Bran\tG\tC\t200\t6\t4\t280\t5\t21\t16\t280\t50\t3\t2\t0.5\t39.7034\n61\tRaisin Squares\tK\tC\t180\t4\t0\t0\t4\t30\t12\t220\t50\t3\t2\t0.5\t55.333142\n62\tRice Chex\tR\tC\t97.34513274336284\t0.8849557522123894\t0\t212.38938053097345\t0\t20.353982300884958\t1.7699115044247788\t26.548672566371682\t22.123893805309734\t1\t0.8849557522123894\t1.13\t41.998933\n\\.\n```\n\n----------------------------------------\n\nTITLE: Markdown Header for Dagster Managed Elements\nDESCRIPTION: Markdown header defining the module name for Dagster managed elements functionality\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-managed-elements/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-managed-elements\n```\n\n----------------------------------------\n\nTITLE: Creating public.alembic_version Table\nDESCRIPTION: This snippet creates the 'alembic_version' table in the 'public' schema, used for tracking database migrations. The table contains a single column, 'version_num', which stores the version number of the current database schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\"\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Process Start - JSON\nDESCRIPTION: This snippet logs the initiation of the 'error_monster' pipeline process, capturing the run ID and relevant event information. It includes details such as the message to be logged and associated tags. Necessary for tracking when pipeline processing begins.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster_tests/general_tests/compat_tests/dead_events.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"event_specific_data\": {\"pipeline_name\": \"error_monster\", \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\"}, \"event_type_value\": \"PIPELINE_PROCESS_START\", \"logging_tags\": {}, \"message\": \"About to start process for pipeline \\\"error_monster\\\" (run_id: 53909416-1450-409f-a360-79649d7f589f).\", \"pipeline_name\": \"error_monster\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"dagster_event\": {\"event_specific_data\": {\"pipeline_name\": \"error_monster\", \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\"}, \"event_type_value\": \"PIPELINE_PROCESS_START\", \"logging_tags\": {}, \"message\": \"About to start process for pipeline \\\"error_monster\\\" (run_id: 53909416-1450-409f-a360-79649d7f589f).\", \"pipeline_name\": \"error_monster\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"About to start process for pipeline \\\"error_monster\\\" (run_id: 53909416-1450-409f-a360-79649d7f589f).\", \"pipeline_name\": \"error_monster\", \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\", \"step_key\": null, \"timestamp\": 1582148401.946458, \"user_message\": \"About to start process for pipeline \\\"error_monster\\\" (run_id: 53909416-1450-409f-a360-79649d7f589f).\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Run Tags in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the run_tags table for the key and value columns. This index improves query performance when searching for runs based on their tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_59\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Key Constraint to Secondary Indexes in PostgreSQL\nDESCRIPTION: Creates a unique constraint on the 'name' column of the 'secondary_indexes' table, ensuring no duplicate index names can be created.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n```\n\n----------------------------------------\n\nTITLE: Installing dg with uv\nDESCRIPTION: Command to install the dg command line tool globally using uv, placing it in the user's PATH independently of any virtual environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install dagster-dg\n```\n\n----------------------------------------\n\nTITLE: Creating test-schema.cereals Table\nDESCRIPTION: This snippet creates the 'cereals' table within the 'test-schema' schema. This table stores information about different types of cereals and their nutritional values.  The table includes columns for name, manufacturer, type, calories, protein, fat, sodium, fiber, carbohydrates, sugars, potassium, vitamins, shelf location, weight, cups, and rating.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE \\\"test-schema\\\".cereals (\n    name text,\n    mfr text,\n    type text,\n    calories integer,\n    protein integer,\n    fat integer,\n    sodium integer,\n    fiber double precision,\n    carbo double precision,\n    sugars integer,\n    potass integer,\n    vitamins integer,\n    shelf integer,\n    weight double precision,\n    cups double precision,\n    rating double precision\n);\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Pipeline Results\nDESCRIPTION: Command to view the contents of the processed CSV file after running the pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/quickstart.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncat data/processed_data.csv\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Record in JSON\nDESCRIPTION: This snippet shows the structure of a DagsterEventRecord object serialized as JSON. It includes details about the pipeline execution, such as event type, timestamp, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"PIPELINE_SUCCESS\",\n    \"logging_tags\": {},\n    \"message\": \"Finished execution of pipeline \\\"foo\\\".\",\n    \"pid\": 80538,\n    \"pipeline_name\": \"foo\",\n    \"solid_handle\": null,\n    \"step_key\": null,\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"foo\\\".\",\n  \"pipeline_name\": \"foo\",\n  \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\",\n  \"step_key\": null,\n  \"timestamp\": 1610466002.955528,\n  \"user_message\": \"Finished execution of pipeline \\\"foo\\\".\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to run_tags Table in SQL\nDESCRIPTION: Adds a primary key constraint on the id column of the run_tags table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to secondary_indexes Table in SQL\nDESCRIPTION: Adds a unique constraint on the 'name' column and a primary key constraint on the 'id' column of the secondary_indexes table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for jobs_id_seq in PostgreSQL\nDESCRIPTION: Initializes the sequence 'jobs_id_seq' at 1 for new job entries in the database, preventing ID collisions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Creating Runs Table in PostgreSQL\nDESCRIPTION: Defines a table to track pipeline runs with metadata including run ID, pipeline name, status, and timestamps for creation and update\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name character varying,\n    status character varying(63),\n    run_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Run Status in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the runs table for the status column. This index enhances query performance when filtering runs by their status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_58\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Dagster Pipeline Step Success Event in JSON Format\nDESCRIPTION: JSON representation of a Dagster step success event record. This event marks the successful completion of the 'persist_model' step, which executed in 524 milliseconds.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 524.0851900016423}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_model\"}, \"message\": \"Finished execution of step \\\"persist_model\\\" in 524ms.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_model\", \"parent\": null}, \"step_key\": \"persist_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_model - STEP_SUCCESS - Finished execution of step \\\"persist_model\\\" in 524ms.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_model\", \"timestamp\": 1608667064.370995, \"user_message\": \"Finished execution of step \\\"persist_model\\\" in 524ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Success Event in JSON\nDESCRIPTION: This snippet demonstrates a Dagster event log entry for a step success event. It includes information about the step execution duration and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepSuccessData\",\n      \"duration_ms\": 23.992793999999762\n    },\n    \"event_type_value\": \"STEP_SUCCESS\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"raw_file_event_admins\",\n      \"solid_definition\": \"raw_file_event_admins\",\n      \"step_key\": \"raw_file_event_admins.compute\"\n    },\n    \"message\": \"Finished execution of step \\\"raw_file_event_admins.compute\\\" in 23ms.\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"raw_file_event_admins\",\n      \"name\": \"raw_file_event_admins\",\n      \"parent\": null\n    },\n    \"step_key\": \"raw_file_event_admins.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_SUCCESS - Finished execution of step \\\"raw_file_event_admins.compute\\\" in 23ms.\\n event_specific_data = {\\\"duration_ms\\\": 23.992793999999762}\\n               solid = \\\"raw_file_event_admins\\\"\\n    solid_definition = \\\"raw_file_event_admins\\\"\\n            step_key = \\\"raw_file_event_admins.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"raw_file_event_admins.compute\",\n  \"timestamp\": 1576110682.8025632,\n  \"user_message\": \"Finished execution of step \\\"raw_file_event_admins.compute\\\" in 23ms.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for Dagster Database Tables in PostgreSQL\nDESCRIPTION: These SQL commands create indexes on various Dagster-related tables in the PostgreSQL database. Indexes improve query performance for frequently accessed data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_asset_check_executions ON public.asset_check_executions USING btree (asset_key, check_name, materialization_event_storage_id, partition);\n\nCREATE UNIQUE INDEX idx_asset_check_executions_unique ON public.asset_check_executions USING btree (asset_key, check_name, run_id, partition);\n\nCREATE UNIQUE INDEX idx_asset_daemon_asset_evaluations_asset_key_evaluation_id ON public.asset_daemon_asset_evaluations USING btree (asset_key, evaluation_id);\n\nCREATE INDEX idx_asset_event_tags ON public.asset_event_tags USING btree (asset_key, key, value);\n\nCREATE INDEX idx_asset_event_tags_event_id ON public.asset_event_tags USING btree (event_id);\n\nCREATE INDEX idx_backfill_tags_backfill_id ON public.backfill_tags USING btree (backfill_id, id);\n\nCREATE INDEX idx_bulk_actions_action_type ON public.bulk_actions USING btree (action_type);\n\nCREATE INDEX idx_bulk_actions_key ON public.bulk_actions USING btree (key);\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Asset Keys Table in SQL\nDESCRIPTION: This snippet inserts multiple rows of data into the 'asset_keys' table, including asset keys and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.asset_keys (id, asset_key, create_timestamp) FROM stdin;\n6\t[\"cost_db_table\"]\t2021-01-06 00:51:58.181361\n7\t[\"traffic_db_table\"]\t2021-01-06 00:51:58.434805\n8\t[\"dashboards\", \"cost_dashboard\"]\t2021-01-06 00:51:59.011595\n9\t[\"dashboards\", \"traffic_dashboard\"]\t2021-01-06 00:52:00.61416\n10\t[\"model\"]\t2021-01-06 00:52:02.215038\n\\.\n```\n\n----------------------------------------\n\nTITLE: Scaffolding Dagster Sling Replication Collection Component via CLI\nDESCRIPTION: This command uses the Dagster CLI to scaffold a SlingReplicationCollectionComponent named 'ingest_files' within the dagster_sling module. It generates the necessary boilerplate code for a new component in a Dagster project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/10-dg-scaffold-sling-replication.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold 'dagster_sling.SlingReplicationCollectionComponent' ingest_files\n```\n\n----------------------------------------\n\nTITLE: Using DocCardList Component in JSX\nDESCRIPTION: This snippet demonstrates the usage of the DocCardList component in JSX. It's a self-closing tag, suggesting that the component doesn't require any children or props to render the list of documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/partitions-and-backfills/index.md#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values for Table Columns in SQL\nDESCRIPTION: This snippet sets default values for the 'id' column in multiple tables using the nextval function with their respective sequences.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n\nALTER TABLE ONLY public.normalized_cereals ALTER COLUMN id SET DEFAULT nextval('public.normalized_cereals_id_seq'::regclass);\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\nALTER TABLE ONLY public.schedule_ticks ALTER COLUMN id SET DEFAULT nextval('public.schedule_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.schedules ALTER COLUMN id SET DEFAULT nextval('public.schedules_id_seq'::regclass);\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for assets_pandas_type_metadata Example in Dagster\nDESCRIPTION: This snippet specifies the dependencies for the assets_pandas_type_metadata example in the Dagster project. It includes core Dagster modules and the dagster-pandera library.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/alt-1/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-e examples/assets_pandas_type_metadata\n  -e python_modules/dagster[pyright,test]\n  -e python_modules/dagster-webserver\n  -e python_modules/dagster-graphql\n  -e python_modules/libraries/dagster-pandera/\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dagster Op Relationships\nDESCRIPTION: Mermaid diagram showing Op's relationships with Type and Graph components, including style configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_10\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Type fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Op(Op)\n\n    style Graph fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Type -.-> Op\n    Op ==> Graph\n```\n\n----------------------------------------\n\nTITLE: Creating Run ID Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the run_id column of the event_logs table to optimize queries that filter by run identifier.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_53\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n```\n\n----------------------------------------\n\nTITLE: Creating Index for job_ticks Table in SQL\nDESCRIPTION: Creates an index on the selector_id and timestamp columns of the job_ticks table to optimize queries based on selector and time range.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_tick_selector_timestamp ON public.job_ticks USING btree (selector_id, \"timestamp\");\n```\n\n----------------------------------------\n\nTITLE: Create snapshots Table\nDESCRIPTION: This SQL statement creates the `snapshots` table for storing Dagster snapshots. It includes the snapshot ID, the snapshot body as a byte array, and the snapshot type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\"\n```\n\n----------------------------------------\n\nTITLE: Themed Image Component Implementation\nDESCRIPTION: React component implementation for displaying a themed image with light/dark mode support showing Dagster lineage visualization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/intro.md#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<ThemedImage\n  alt=\"Docusaurus themed image\"\n  style={{width: '100%', height: 'auto'}}\n  sources={{\n    light: './img/getting-started/lineage-light.jpg',\n    dark: './img/getting-started/lineage-dark.jpg',\n  }}\n/>\n```\n\n----------------------------------------\n\nTITLE: Pipeline Start Event in Dagster (JSON)\nDESCRIPTION: JSON log entry marking the start of execution for a Dagster pipeline named 'foo', including run ID and process ID information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\"foo\\\".\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - PIPELINE_START - Started execution of pipeline \\\"foo\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.492956, \"user_message\": \"Started execution of pipeline \\\"foo\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for event_logs_id_seq in PostgreSQL\nDESCRIPTION: This command sets the 'event_logs_id_seq' sequence to start at 1 for the event_logs table, which aids in generating unique identifiers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.event_logs_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraint on snapshots.snapshot_id\nDESCRIPTION: This SQL statement adds a unique constraint to the `snapshots` table on the `snapshot_id` column. This guarantees that each snapshot ID is unique within the table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\n\"ALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\"\n```\n\n----------------------------------------\n\nTITLE: Altering Table Owner for Hot Cereals View\nDESCRIPTION: This SQL statement changes the owner of the `sort_hot_cereals_by_calories` view within the `test-schema` to the `test` user. Similar to the other owner alterations, this manages access control.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE \"test-schema\".sort_hot_cereals_by_calories OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Input Event for Cost Dashboard in JSON\nDESCRIPTION: JSON structure of a Dagster event record showing a STEP_INPUT event for a 'build_cost_dashboard' step, indicating successful type checking of an input named '_'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"_\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"_\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_cost_dashboard - STEP_INPUT - Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1609894318.492403, \"user_message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Debug Logging to Component Execution\nDESCRIPTION: Example of extending SlingReplicationCollectionComponent to add debug logging during execution by overriding the execute method.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/customizing-components.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/guides/components/custom-subclass/debug-mode.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: GraphX Configuration Properties\nDESCRIPTION: GraphX-specific configuration for Pregel checkpoint intervals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_51\n\nLANGUAGE: properties\nCODE:\n```\nspark.graphx.pregel.checkpointInterval=-1\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Repository in Editable Mode\nDESCRIPTION: Command to install the Dagster repository as a Python package in editable mode, allowing local code changes to apply automatically during development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_aws/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: CloudFormation Template Reference for ECS Agent VPC Setup\nDESCRIPTION: URL reference to the CloudFormation template that sets up an ECS agent from scratch, creating a new VPC and ECS cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/amazon-ecs/new-vpc.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhttps://s3.amazonaws.com/dagster.cloud/cloudformation/ecs-agent-vpc.yaml\n```\n\n----------------------------------------\n\nTITLE: Materializing table_info with raw_groups metadata\nDESCRIPTION: Dagster event record documenting the materialization of a table_info value with metadata about raw_groups including table name, path, data and markdown documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n// comments\nline 1 of code\nline 2 of code\nline 3 of code\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster SSH Integration\nDESCRIPTION: Command to install the dagster-ssh package using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/ssh-sftp.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-ssh\n```\n\n----------------------------------------\n\nTITLE: Disabling PEX Deploys in GitLab CI/CD (YAML)\nDESCRIPTION: This snippet shows how to disable PEX-based deploys in a GitLab CI/CD configuration by setting the DISABLE_FAST_DEPLOYS variable to true.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/serverless/runtime-environment.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nvariables:\n  DISABLE_FAST_DEPLOYS: \"true\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Dagster CLI Help\nDESCRIPTION: Shows how to display the help menu for the Dagster CLI using the 'dg' command with the --help flag\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_DgReference.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg --help\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Job Type in Jobs in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the jobs table for the job_type column. This index enhances query performance when filtering jobs by their type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_62\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Start Event in Dagster (JSON)\nDESCRIPTION: JSON log entry for the start of the 'asset_pipeline' execution. It includes details such as the pipeline name, run ID, and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"PIPELINE_START\",\n    \"logging_tags\": {},\n    \"message\": \"Started execution of pipeline \\\"asset_pipeline\\\".\",\n    \"pid\": 5200,\n    \"pipeline_name\": \"asset_pipeline\",\n    \"solid_handle\": null,\n    \"step_handle\": null,\n    \"step_key\": null,\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"asset_pipeline - 383f9411-4010-454a-9d31-d5a72ab57221 - 5200 - PIPELINE_START - Started execution of pipeline \\\"asset_pipeline\\\".\",\n  \"pipeline_name\": \"asset_pipeline\",\n  \"run_id\": \"383f9411-4010-454a-9d31-d5a72ab57221\",\n  \"step_key\": null,\n  \"timestamp\": 1625762114.5281448,\n  \"user_message\": \"Started execution of pipeline \\\"asset_pipeline\\\".\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint Between Run Tags and Runs in SQL\nDESCRIPTION: Adds a foreign key constraint to the run_tags table referencing the runs table by run_id with cascade delete to maintain referential integrity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_57\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Creating runs Table - SQL\nDESCRIPTION: This snippet defines the 'runs' table, which is central to tracking pipeline executions with fields such as pipeline name, status, and timestamps for creation and updates.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: runs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    pipeline_name character varying,\n    status character varying(63),\n    run_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.runs OWNER TO test;\n\n```\n\n----------------------------------------\n\nTITLE: Alter run_tags_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `run_tags_id_seq` sequence to the `id` column of the `run_tags` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\"\n```\n\n----------------------------------------\n\nTITLE: Sequence Configuration in PostgreSQL\nDESCRIPTION: This SQL snippet configures sequences in the PostgreSQL database to generate unique identifiers for table records. The sequences are defined for tables such as 'asset_keys', 'bulk_actions', and 'job_ticks', with each sequence being owned by the respective table's ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n\n-- Name: asset_keys_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n\n-- Further sequence configurations follow\n```\n\n----------------------------------------\n\nTITLE: Creating Least Caloric Cereals View\nDESCRIPTION: Creates a view that returns the cereal with the lowest calories from the sort_by_calories table, limiting to 1 result.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW \"test-schema\".least_caloric AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n LIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Creating Index on bulk_actions.key\nDESCRIPTION: This SQL statement creates an index on the `key` column of the `bulk_actions` table. This speeds up queries that filter or sort by the bulk action key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\"\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for job_ticks.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `job_ticks` table to be generated by the `job_ticks_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Creating public.jobs Table and Sequence\nDESCRIPTION: This snippet creates the 'jobs' table in the 'public' schema for storing information about Dagster jobs.  It includes columns for ID, job origin ID, repository origin ID, status, job type, job body, creation timestamp, and update timestamp. An associated sequence 'jobs_id_seq' is also created to automatically generate unique IDs for each job.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.jobs OWNER TO test;\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.jobs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.jobs_id_seq OWNER TO test;\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.jobs_id_seq OWNED BY public.jobs.id;\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster STEP_SUCCESS Event JSON in Python\nDESCRIPTION: JSON representation of a Dagster step success event. This event indicates that the 'do_input.compute' step completed successfully in 15ms. The record includes execution duration and step context information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_57\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 15.877008438110352}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Finished execution of step \\\"do_input.compute\\\" in 15ms.\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_input.compute - STEP_SUCCESS - Finished execution of step \\\"do_input.compute\\\" in 15ms.\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466123.668594, \"user_message\": \"Finished execution of step \\\"do_input.compute\\\" in 15ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill Library\nDESCRIPTION: Imports the Dagstermill library for data processing and integration with Jupyter notebooks\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/mult_two_numbers.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for job_ticks id Column\nDESCRIPTION: This SQL statement configures the `id` column in the `job_ticks` table to automatically generate unique IDs using the `public.job_ticks_id_seq` sequence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project Directory Tree Structure in Shell\nDESCRIPTION: A shell command and its output showing the directory structure of a Dagster project. The structure reveals an organization pattern with team-specific subdirectories and Python definition files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/adding-attributes-to-assets/1-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntree my_project/defs\n\nmy_project/defs\n├── __init__.py\n├── team_a\n│   ├── b.py\n│   └── subproject\n│       └── a.py\n└── team_b\n    └── c.py\n\n4 directories, 4 files\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Index for kvs Table in SQL\nDESCRIPTION: Creates a unique index on the key column of the kvs table to ensure key uniqueness and improve lookup performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE UNIQUE INDEX idx_kvs_keys_unique ON public.kvs USING btree (key);\n```\n\n----------------------------------------\n\nTITLE: Creating Job Tick Timestamp Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the job_origin_id and timestamp columns of the job_ticks table to optimize time-based queries for jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Not Diamond Package\nDESCRIPTION: Command to install the Dagster Not Diamond integration package via pip\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/notdiamond.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-notdiamond\n```\n\n----------------------------------------\n\nTITLE: Create job_ticks Table\nDESCRIPTION: This SQL statement creates the `job_ticks` table for storing information about job ticks. It includes fields for job origin ID, status, type, timestamp, tick body, creation timestamp, and update timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \\\"timestamp\\\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Dagster to ECS\nDESCRIPTION: Docker compose command to deploy the Dagster stack to AWS ECS using the created context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/deploy_ecs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndocker --context dagster-ecs compose --project-name dagster up\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log Entry - Pipeline Success\nDESCRIPTION: JSON log entry indicating successful completion of the 'basic_assets_job' pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of run for \\\"basic_assets_job\\\".\", \"pid\": 3626, \"pipeline_name\": \"basic_assets_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"basic_assets_job\", \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"step_key\": null, \"timestamp\": 1731664852.27652, \"user_message\": \"Finished execution of run for \\\"basic_assets_job\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log Entry - Step Success\nDESCRIPTION: Event log entry recording successful completion of the 'add_four.add' step with execution duration\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 53.64487200000134}, \"event_type_value\": \"STEP_SUCCESS\"}}\n```\n\n----------------------------------------\n\nTITLE: Yielding an AssetMaterialization Event\nDESCRIPTION: This snippet shows how to yield an AssetMaterialization event from a Dagstermill-executed notebook.  This event indicates that an asset has been materialized to a specified file path. `dagstermill.yield_event` is used to send the event to Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_explicit_yield.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndagstermill.yield_event(AssetMaterialization.file(\"/path/to/file\"))\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to event_logs Table in SQL\nDESCRIPTION: Adds a primary key constraint on the id column of the event_logs table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Inserting Snapshot Data in SQL\nDESCRIPTION: SQL COPY statement to insert data into the 'public.snapshots' table. This statement is empty, indicating no snapshot data is being inserted in this example.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.snapshots (id, snapshot_id, snapshot_body, snapshot_type) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating normalized_cereals Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'normalized_cereals' table to store various cereal types along with their nutritional information and ratings. This table assists in data management for food-related entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: normalized_cereals; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.normalized_cereals (\n    id integer NOT NULL,\n    name character varying,\n    mfr character varying,\n    type character varying,\n    calories double precision,\n    protein double precision,\n    fat double precision,\n    sodium double precision,\n    fiber double precision,\n    carbo double precision,\n    sugars double precision,\n    potass double precision,\n    vitamins double precision,\n    shelf double precision,\n    weight double precision,\n    cups double precision,\n    rating double precision\n);\n\nALTER TABLE public.normalized_cereals OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Event Logs by Asset in SQL\nDESCRIPTION: Creates a conditional B-tree index on the event_logs table for records with non-null asset_key values to optimize asset-related queries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_events_by_asset ON public.event_logs USING btree (asset_key, dagster_event_type, id) WHERE (asset_key IS NOT NULL);\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Step Start\nDESCRIPTION: Log entry for the start of build_traffic_dashboard step execution in the longitudinal pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_57\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_traffic_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_traffic_dashboard\"}, \"message\": \"Started execution of step \\\"build_traffic_dashboard\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_traffic_dashboard\", \"parent\": null}, \"step_key\": \"build_traffic_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_traffic_dashboard - STEP_START - Started execution of step \\\"build_traffic_dashboard\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_traffic_dashboard\", \"timestamp\": 1608666998.786407, \"user_message\": \"Started execution of step \\\"build_traffic_dashboard\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Listing Dagster Package Dependencies\nDESCRIPTION: A comprehensive list of Dagster packages and integrations used for determining CI build dependencies. Includes core packages like dagster and dagster-graphql, as well as integration packages for various platforms like Kubernetes, Celery, AWS, and GCP.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/k8s-test-suite/buildkite_deps.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndagster\ndagster-graphql\ndagster-test\ndagster-pandas\ndagster-k8s\ndagster-celery\ndagster-celery-k8s\ndagster-celery-docker\ndagster-postgres\ndagster-airflow\ndagster-docker\ndagster-aws\ndagster-gcp\n```\n\n----------------------------------------\n\nTITLE: Inserting Secondary Index Data in SQL\nDESCRIPTION: SQL COPY statement to insert data into the 'public.secondary_indexes' table. It includes information about various secondary indexes and their migration status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.secondary_indexes (id, name, create_timestamp, migration_completed) FROM stdin;\n1\trun_partitions\t2024-11-15 17:59:56.304765\t2024-11-15 10:59:56.689595\n2\trun_repo_label_tags\t2024-11-15 17:59:56.321786\t2024-11-15 10:59:56.707363\n3\tbulk_action_types\t2024-11-15 17:59:56.333672\t2024-11-15 10:59:56.719637\n4\trun_backfill_id\t2024-11-15 17:59:56.348944\t2024-11-15 10:59:56.734508\n5\tbackfill_job_name_and_tags\t2024-11-15 17:59:56.363713\t2024-11-15 10:59:56.749431\n6\tasset_key_index_columns\t2024-11-15 17:59:56.391099\t2024-11-15 10:59:56.776723\n7\tschedule_jobs_selector_id\t2024-11-15 17:59:56.42125\t2024-11-15 10:59:56.806893\n\\.\n```\n\n----------------------------------------\n\nTITLE: Checking Browser Element Availability with Optional Default Value\nDESCRIPTION: Utility function that finds an element with a CSS selector and returns a default value if the element is not found, which helps handle conditionally present elements in tests.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/cli_test_scaffold.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef find_element(browser, selector=None, xpath=None, default=None):\n    try:\n        if selector:\n            return browser.find_element_by_css_selector(selector)\n        else:\n            return browser.find_element_by_xpath(xpath)\n    except NoSuchElementException:\n        return default\n```\n\n----------------------------------------\n\nTITLE: Initialize Dagster Workspace with UV Environment\nDESCRIPTION: Command for initializing a new Dagster workspace using UV for Python environment management. Creates scaffold files and prompts for project name to set up the workspace structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/workspace/1-dg-init.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg init --workspace dagster-workspace --python-environment uv_managed\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Module Import Reference\nDESCRIPTION: Module reference for the dagster-deltalake package showing the current module context for documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: dagster_deltalake\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for bulk_actions_id_seq in PostgreSQL\nDESCRIPTION: This snippet sets the current value of the 'bulk_actions_id_seq' sequence to 1, ensuring the sequence starts at the specified point for generating IDs in bulk_actions table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.bulk_actions_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to event_logs Table in SQL\nDESCRIPTION: Adds a primary key constraint on the 'id' column of the event_logs table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Component Types and Summaries\nDESCRIPTION: This snippet shows the output of the 'dg list component-type' command, presenting a table of Dagster component types and their brief summaries. It includes information about various components such as DefinitionsComponent, DefsFolderComponent, and integration components for DBT, Evidence.dev, and Sling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/27-dg-list-component-types.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Component Type                                              ┃ Summary                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ dagster.components.DefinitionsComponent                     │ An arbitrary set of      │\n│                                                             │ dagster definitions.     │\n│ dagster.components.DefsFolderComponent                      │ A folder which may       │\n│                                                             │ contain multiple         │\n│                                                             │ submodules, each         │\n│                                                             │ which define components. │\n│ dagster.components.PipesSubprocessScriptCollectionComponent │ Assets that wrap Python  │\n│                                                             │ scripts executed with    │\n│                                                             │ Dagster's                │\n│                                                             │ PipesSubprocessClient.   │\n│ dagster_dbt.DbtProjectComponent                             │ Expose a DBT project to  │\n│                                                             │ Dagster as a set of      │\n│                                                             │ assets.                  │\n│ dagster_evidence.EvidenceProject                            │ Expose an Evidence.dev   │\n│                                                             │ dashboard as a Dagster   │\n│                                                             │ asset.                   │\n│ dagster_sling.SlingReplicationCollectionComponent           │ Expose one or more Sling │\n│                                                             │ replications to Dagster  │\n│                                                             │ as assets.               │\n└─────────────────────────────────────────────────────────────┴──────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Defining Dagster Integration Icons in Markdown\nDESCRIPTION: A markdown table that maps integration tags to icon images. Each row contains a tag and an HTML img element referencing the corresponding icon SVG file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_KindsTags.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Tag | Image |\n|-----|-------|\n| `airbyte`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-airbyte-color.svg\" width={20} height={20} />                        |\n| `airflow`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-airflow-color.svg\" width={20} height={20} />                        |\n| `airliftmapped`       | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-airflow-color.svg\" width={20} height={20} />                        |\n| `airtable`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-airtable-color.svg\" width={20} height={20} />                       |\n| `athena`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-aws-color.svg\" width={20} height={20} />                            |\n| `atlan`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-atlan-color.svg\" width={20} height={20} />                          |\n| `aws`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-aws-color.svg\" width={20} height={20} />                            |\n| `awsstepfunction`     | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-stepfunctions-color.svg\" width={20} height={20} />                  |\n| `awsstepfunctions`    | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-stepfunctions-color.svg\" width={20} height={20} />                  |\n| `axioma`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-axioma-color.svg\" width={20} height={20} />                         |\n| `azure`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-azure-color.svg\" width={20} height={20} />                          |\n| `azureml`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-azureml-color.svg\" width={20} height={20} />                        |\n| `bigquery`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-bigquery-color.svg\" width={20} height={20} />                       |\n| `cassandra`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-cassandra-color.svg\" width={20} height={20} />                      |\n| `catboost`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-catboost-color.svg\" width={20} height={20} />                       |\n| `celery`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-celery-color.svg\" width={20} height={20} />                         |\n| `census`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-census-color.svg\" width={20} height={20} />                         |\n| `chalk`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-chalk-color.svg\" width={20} height={20} />                          |\n| `claude`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-claude-color.svg\" width={20} height={20} />                         |\n| `clickhouse`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-clickhouse-color.svg\" width={20} height={20} />                     |\n| `cockroachdb`         | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-cockroachdb-color.svg\" width={20} height={20} />                    |\n| `collibra`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-collibra-color.svg\" width={20} height={20} />                       |\n| `cplus`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-cplus-color.svg\" width={20} height={20} />                          |\n| `cplusplus`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-cplus-color.svg\" width={20} height={20} />                          |\n| `csharp`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-csharp-color.svg\" width={20} height={20} />                         |\n| `cube`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-cube-color.svg\" width={20} height={20} />                           |\n| `dask`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-dask-color.svg\" width={20} height={20} />                           |\n| `databricks`          | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-databricks-color.svg\" width={20} height={20} />                     |\n| `datadog`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-datadog-color.svg\" width={20} height={20} />                        |\n| `datahub`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-datahub-color.svg\" width={20} height={20} />                        |\n| `db2`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-db2-color.svg\" width={20} height={20} />                            |\n| `dbt`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-dbt-color.svg\" width={20} height={20} />                            |\n| `dbtcloud`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-dbt-color.svg\" width={20} height={20} />                            |\n| `deepseek`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-deepseek-color.svg\" width={20} height={20} />                       |\n| `deltalake`           | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-deltalake-color.svg\" width={20} height={20} />                      |\n| `denodo`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-denodo-color.svg\" width={20} height={20} />                         |\n| `dify`                | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-dify-color.svg\" width={20} height={20} />                           |\n| `dingtalk`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-dingtalk-color.svg\" width={20} height={20} />                       |\n| `discord`             | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-discord-color.svg\" width={20} height={20} />                        |\n| `dlt`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-dlthub-color.svg\" width={20} height={20} />                         |\n| `dlthub`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-dlthub-color.svg\" width={20} height={20} />                         |\n| `docker`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-docker-color.svg\" width={20} height={20} />                         |\n| `doris`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-doris-color.svg\" width={20} height={20} />                          |\n| `doubao`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-doubao-color.svg\" width={20} height={20} />                         |\n| `druid`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-druid-color.svg\" width={20} height={20} />                          |\n| `duckdb`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-duckdb-color.svg\" width={20} height={20} />                         |\n| `elasticsearch`       | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-elasticsearch-color.svg\" width={20} height={20} />                  |\n| `excel`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-excel-color.svg\" width={20} height={20} />                          |\n| `facebook`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-facebook-color.svg\" width={20} height={20} />                       |\n| `fivetran`            | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-fivetran-color.svg\" width={20} height={20} />                       |\n| `flink`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-flink-color.svg\" width={20} height={20} />                          |\n| `gcp`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-googlecloud-color.svg\" width={20} height={20} />                    |\n| `gcs`                 | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-gcs-color.svg\" width={20} height={20} />                            |\n| `gemini`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-gemini-color.svg\" width={20} height={20} />                         |\n| `github`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-github-color.svg\" width={20} height={20} />                         |\n| `gitlab`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-gitlab-color.svg\" width={20} height={20} />                         |\n| `go`                  | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-go-color.svg\" width={20} height={20} />                             |\n| `google`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-google-color.svg\" width={20} height={20} />                         |\n| `googlecloud`         | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-googlecloud-color.svg\" width={20} height={20} />                    |\n| `googledrive`         | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-googledrive-color.svg\" width={20} height={20} />                    |\n```\n\n----------------------------------------\n\nTITLE: Displaying SQL Configuration Settings in Scala\nDESCRIPTION: This Scala code snippet shows how to display the entire list of SQL configuration settings using an existing SparkSession.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_45\n\nLANGUAGE: scala\nCODE:\n```\n// spark is an existing SparkSession\nspark.sql(\"SET -v\").show(numRows = 200, truncate = false)\n```\n\n----------------------------------------\n\nTITLE: Step Success Log for Ingest Costs in Dagster\nDESCRIPTION: A JSON log entry recording the successful completion of the 'ingest_costs' step in the Dagster pipeline. The log includes execution time information showing the step completed in approximately 7.02 seconds.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_54\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 7018.427522009006}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_costs\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_costs\"}, \"message\": \"Finished execution of step \\\"ingest_costs\\\" in 7.02s.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_costs\", \"parent\": null}, \"step_key\": \"ingest_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - ingest_costs - STEP_SUCCESS - Finished execution of step \\\"ingest_costs\\\" in 7.02s.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"ingest_costs\", \"timestamp\": 1608666908.121127, \"user_message\": \"Finished execution of step \\\"ingest_costs\\\" in 7.02s.\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Index on run_tags.key, value\nDESCRIPTION: This SQL statement creates a composite index on the `key` and `value` columns of the `run_tags` table. This index improves query speed for filtering or sorting run tags by key and value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\"\n```\n\n----------------------------------------\n\nTITLE: Creating Foreign Key Constraints for Dagster Tables\nDESCRIPTION: Establishes referential integrity between runs and snapshots tables, and between run_tags and runs tables with cascade delete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: SQL Data Loading for Job Ticks\nDESCRIPTION: Empty data load statement for job_ticks table tracking scheduled job execution history.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_61\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.job_ticks (id, job_origin_id, status, type, \"timestamp\", tick_body, create_timestamp, update_timestamp) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Reading Dataset using Pandas in Python\nDESCRIPTION: This snippet reads the Iris dataset from a specified URL into a pandas DataFrame called 'iris'. It utilizes 'pandas.read_csv' to fetch and store the data locally for further processing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/clean_data.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\niris = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into event_logs Table Using COPY in PostgreSQL\nDESCRIPTION: This statement initiates the copying of CSV-like entries into 'event_logs', importing event data with columns matching the table's structure for bulk insertion, streamlining data population.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key, asset_key) FROM stdin;\n1\\t8c59d1b7-1841-4c60-8346-89e298dbf743\\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80538\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 80538).\", \"pid\": null, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 80538).\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": null, \"timestamp\": 1610466002.695335, \"user_message\": \"Started process for pipeline (pid: 80538).\"}\tENGINE_EVENT\t2021-01-12 10:40:02.695335\t\\N\t\\N\n```\n\n----------------------------------------\n\nTITLE: Inserting Event Log Data for Dagster Pipeline Run\nDESCRIPTION: SQL code to insert event log data for a Dagster pipeline run. This includes detailed JSON payloads for various pipeline events such as step successes and engine events.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_57\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.event_log (id, run_id, event) FROM stdin;\n86\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 24.6336420000004}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"check_users_and_groups_one_fails_one_succeeds\", \"solid_definition\": \"check_users_and_groups_one_fails_one_succeeds\", \"step_key\": \"check_users_and_groups_one_fails_one_succeeds.compute\"}, \"message\": \"Finished execution of step \\\"check_users_and_groups_one_fails_one_succeeds.compute\\\" in 24ms.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"check_users_and_groups_one_fails_one_succeeds\", \"name\": \"check_users_and_groups_one_fails_one_succeeds\", \"parent\": null}, \"step_key\": \"check_users_and_groups_one_fails_one_succeeds.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_SUCCESS - Finished execution of step \\\"check_users_and_groups_one_fails_one_succeeds.compute\\\" in 24ms.\\n event_specific_data = {\\\"duration_ms\\\": 24.6336420000004}\\n               solid = \\\"check_users_and_groups_one_fails_one_succeeds\\\"\\n    solid_definition = \\\"check_users_and_groups_one_fails_one_succeeds\\\"\\n            step_key = \\\"check_users_and_groups_one_fails_one_succeeds.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"check_users_and_groups_one_fails_one_succeeds.compute\", \"timestamp\": 1576110683.9869819, \"user_message\": \"Finished execution of step \\\"check_users_and_groups_one_fails_one_succeeds.compute\\\" in 24ms.\"}\n87\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"22892\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"{'many_materializations_and_passing_expectations.compute', 'raw_file_friends.compute', 'raw_file_fans.compute', 'check_admins_both_succeed.compute', 'raw_file_pages.compute', 'many_table_materializations.compute', 'raw_file_users.compute', 'raw_file_group_admins.compute', 'raw_file_event_admins.compute', 'check_users_and_groups_one_fails_one_succeeds.compute', 'raw_file_events.compute', 'raw_file_groups.compute'}\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 22892) in 1.24s\", \"pipeline_name\": \"many_events\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - ENGINE_EVENT - Finished steps in process (pid: 22892) in 1.24s\\n event_specific_data = {\\\"metadata_entries\\\": [[\\\"pid\\\", null, [\\\"22892\\\"]], [\\\"step_keys\\\", null, [\\\"{'many_materializations_and_passing_expectations.compute', 'raw_file_friends.compute', 'raw_file_fans.compute', 'check_admins_both_succeed.compute', 'raw_file_pages.compute', 'many_table_materializations.compute', 'raw_file_users.compute', 'raw_file_group_admins.compute', 'raw_file_event_admins.compute', 'check_users_and_groups_one_fails_one_succeeds.compute', 'raw_file_events.compute', 'raw_file_groups.compute'}\\\"]]]\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": null, \"timestamp\": 1576110684.043597, \"user_message\": \"Finished steps in process (pid: 22892) in 1.24s\"}\n88\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of pipeline \\\"many_events\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - PIPELINE_SUCCESS - Finished execution of pipeline \\\"many_events\\\".\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": null, \"timestamp\": 1576110684.053497, \"user_message\": \"Finished execution of pipeline \\\"many_events\\\".\"}\n89\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"PipelineProcessExitedData\", \"pipeline_name\": \"many_events\", \"process_id\": 22892, \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\"}, \"event_type_value\": \"PIPELINE_PROCESS_EXITED\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 22892).\", \"pipeline_name\": \"many_events\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Process for pipeline exited (pid: 22892).\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": null, \"timestamp\": 1576110684.072887, \"user_message\": \"Process for pipeline exited (pid: 22892).\"}\n\\.\n```\n\n----------------------------------------\n\nTITLE: Copying Environment File for ATProto Dashboard Setup\nDESCRIPTION: This command copies the example environment file to create a new .env file for configuration. Users are instructed to populate the fields in the newly created file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_atproto_dashboard/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Creating Custom View Definitions in PostgreSQL\nDESCRIPTION: Provides SQL commands for defining views such as 'sort_by_calories' and 'least_caloric' in the 'test-schema'. These views allow for customized data presentation, particularly as a sorted list of cereal data. Views depend on the structure of underlying tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE VIEW \"test-schema\".sort_by_calories AS\n SELECT cereals.name,\n    cereals.mfr,\n    cereals.type,\n    cereals.calories,\n    cereals.protein,\n    cereals.fat,\n    cereals.sodium,\n    cereals.fiber,\n    cereals.carbo,\n    cereals.sugars,\n    cereals.potass,\n    cereals.vitamins,\n    cereals.shelf,\n    cereals.weight,\n    cereals.cups,\n    cereals.rating\n   FROM \"test-schema\".cereals\n  ORDER BY cereals.calories;\n\nALTER TABLE \"test-schema\".sort_by_calories OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Index on status Column in runs Table\nDESCRIPTION: SQL command to create an index on the status column in the runs table to optimize queries filtering by run status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Using Airflow KubernetesPodOperator\nDESCRIPTION: Example of using the KubernetesPodOperator in Apache Airflow to execute containerized tasks within Kubernetes pods as part of data pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/airflow-operator-migration/kubernetes-pod-operator.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# The snippet shows how to use KubernetesPodOperator in Airflow\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Project Directory Structure\nDESCRIPTION: This tree structure shows the layout of a Dagster project. It includes a main project directory 'my_existing_project' containing Python files for assets, definitions, and type hints, along with a setup.py file at the root level.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/1-pip-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── my_existing_project\n│   ├── __init__.py\n│   ├── assets.py\n│   ├── definitions.py\n│   └── py.typed\n└── setup.py\n\n2 directories, 5 files\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for runs_id_seq in PostgreSQL\nDESCRIPTION: This snippet initializes the 'runs_id_seq' sequence at 1, ensuring sequential ID generation for runs table entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.runs_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Illustrating Changed_in_branch Filter in Markdown\nDESCRIPTION: This markdown table demonstrates the usage of the 'changed_in_branch' filter in Dagster+ branch deployments. It shows different reasons for asset changes and their corresponding syntax.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/reference.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Reason | Syntax | Description |\n|------------|--------|-------------|\n| `ANY`      | `changed_in_branch: \"ANY\"` | Selects any assets changed in the branch. |\n| `CODE_VERSION` | `changed_in_branch: \"CODE_VERSION\"` | Selects assets whose code version changed in the branch. |\n| `DEPENDENCIES` | `changed_in_branch: \"DEPENDENCIES\"` | Selects assets whose dependencies changed in the branch. |\n| `METADATA` | `changed_in_branch: \"METADATA\"`  | Selects assets whose metadata has changed in the branch. |\n| `NEW` | `changed_in_branch: \"NEW\"` | Selects assets that are new in the branch. |\n| `PARTITIONS_DEFINITION` | `changed_in_branch: \"PARTITIONS_DEFINITION\"` | Selects assets whose partitions definition has changed in the branch. |\n| `TAGS` | `changed_in_branch: \"TAGS\"` | Selects assets whose tags have changed in the branch. |\n```\n\n----------------------------------------\n\nTITLE: List Service Accounts in Kubernetes\nDESCRIPTION: Kubernetes command to retrieve service accounts in the Dagster agent namespace.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/blob-compute-logs.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get serviceaccount -n <dagster-agent-namespace>\n```\n\n----------------------------------------\n\nTITLE: Use Case Fetching Script Reference\nDESCRIPTION: JavaScript script for retrieving use cases from the repository for Dagster.io website integration, located in dagster-website/scripts/fetchUseCases.js\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/README.md#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Script fetches use cases from master branch\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Index on partition_set and partition Columns in runs Table\nDESCRIPTION: SQL command to create a composite index on partition_set and partition columns in the runs table to optimize partition-based queries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values for Multiple Table Columns in PostgreSQL\nDESCRIPTION: Sets default values for id columns in various tables using their respective sequences. These statements configure automatic ID generation for new records in each of the tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.backfills ALTER COLUMN id SET DEFAULT nextval('public.backfills_id_seq'::regclass);\n\nALTER TABLE ONLY public.bulk_actions ALTER COLUMN id SET DEFAULT nextval('public.bulk_actions_id_seq'::regclass);\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n\nALTER TABLE ONLY public.normalized_cereals ALTER COLUMN id SET DEFAULT nextval('public.normalized_cereals_id_seq'::regclass);\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Creating a View for Hot Cereals\nDESCRIPTION: This SQL statement creates a view named `sort_hot_cereals_by_calories` within the `test-schema`. The view selects all columns from the `sort_by_calories` table but filters the results to include only rows where the `type` column is equal to 'H', representing hot cereals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE VIEW \"test-schema\".sort_hot_cereals_by_calories AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n  WHERE (sort_by_calories.type = 'H'::text);\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for runs id Column\nDESCRIPTION: This SQL statement configures the `id` column in the `runs` table to automatically generate IDs using the `public.runs_id_seq` sequence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Creating Table: jobs in PostgreSQL\nDESCRIPTION: Defines 'jobs' table to store metadata and configuration for job executions, including job origin and type. Features timestamps for tracking job creation and updates, essential for job lifecycle management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    selector_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.jobs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Initializing Resources in Dagster Pipeline\nDESCRIPTION: JSON log entry showing the start of resource initialization for the 'object_manager' resource in the pipeline execution process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [object_manager].\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - ENGINE_EVENT - Starting initialization of resources [object_manager].\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608666901.041621, \"user_message\": \"Starting initialization of resources [object_manager].\"}\n```\n\n----------------------------------------\n\nTITLE: Runs Table Data Load in PostgreSQL\nDESCRIPTION: SQL statement to load data into the runs table in the public schema. The table appears to be empty as shown by the lack of records after the 'FROM stdin;' line. This table likely stores information about pipeline executions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_63\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.runs (id, run_id, snapshot_id, pipeline_name, status, run_body, partition, partition_set, create_timestamp, update_timestamp) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating bulk_actions Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'bulk_actions' table for tracking bulk operation actions with associated statuses and timestamps. This helps manage bulk processing tasks efficiently.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: bulk_actions; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.bulk_actions (\n    id integer NOT NULL,\n    key character varying(32) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\nALTER TABLE public.bulk_actions OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to secondary_indexes Table\nDESCRIPTION: SQL command to add a primary key constraint to the secondary_indexes table on the id column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\nADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for jobs_id_seq in PostgreSQL\nDESCRIPTION: This SQL snippet sets the current value of 'jobs_id_seq' to 1, ensuring that the next identifier generated is 2 for jobs table records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Creating event_logs Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the event_logs table to log various events associated with Dagster executions, providing a historical record of events and state changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: event_logs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key character varying,\n    asset_key character varying\n);\nALTER TABLE public.event_logs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Importing Legacy Datadog Resource in Python\nDESCRIPTION: Demonstrates how to import the legacy datadog_resource from the dagster_datadog module. This is an older version of the Datadog resource definition that is still supported.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-datadog.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_datadog import datadog_resource\n```\n\n----------------------------------------\n\nTITLE: Copying Data into event_logs Table\nDESCRIPTION: This SQL statement inserts data into the `event_logs` table in the `public` schema. It includes various fields like `run_id`, `event`, `dagster_event_type`, and `timestamp`. The data represents log entries related to a Dagster pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key, asset_key, partition) FROM stdin;\n1\tf1e0df44-6395-4e6e-aaf3-d867a26e3662\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_STARTING\", \"logging_tags\": {}, \"message\": null, \"pid\": null, \"pipeline_name\": \"composition\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": null, \"timestamp\": 1640037514.010169, \"user_message\": \"\"}\tPIPELINE_STARTING\t2021-12-20 21:58:34.010169\t\\N\t\\N\t\\N\n2\tf1e0df44-6395-4e6e-aaf3-d867a26e3662\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"58212\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for run (pid: 58212).\", \"pid\": null, \"pipeline_name\": \"composition\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for run (pid: 58212).\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": null, \"timestamp\": 1640037516.3287091, \"user_message\": \"Started process for run (pid: 58212).\"}\tENGINE_EVENT\t2021-12-20 21:58:36.328709\t\\N\t\\N\t\\N\n3\tf1e0df44-6395-4e6e-aaf3-d867a26e3662\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of run for \\\"composition\\\".\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - RUN_START - Started execution of run for \\\"composition\\\".\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": null, \"timestamp\": 1640037520.9545128, \"user_message\": \"Started execution of run for \\\"composition\\\".\"}\tPIPELINE_START\t2021-12-20 21:58:40.954513\t\\N\t\\N\t\\N\n4\tf1e0df44-6395-4e6e-aaf3-d867a26e3662\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"58212\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['add_four.emit_two.emit_one', 'add_four.emit_two.emit_one_2', 'add_four.emit_two.add', 'add_four.emit_two_2.emit_one', 'add_four.emit_two_2.emit_one_2', 'add_four.emit_two_2.add', 'add_four.add', 'int_to_float', 'div_four.div_two', 'div_four.div_two_2']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps in process (pid: 58212)\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - ENGINE_EVENT - Executing steps in process (pid: 58212)\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": null, \"timestamp\": 1640037520.9899762, \"user_message\": \"Executing steps in process (pid: 58212)\"}\tENGINE_EVENT\t2021-12-20 21:58:40.989976\t\\N\t\\N\t\\N\n5\tf1e0df44-6395-4e6e-aaf3-d867a26e3662\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [io_manager].\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - ENGINE_EVENT - Starting initialization of resources [io_manager].\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": null, \"timestamp\": 1640037521.097007, \"user_message\": \"Starting initialization of resources [io_manager].\"}\tENGINE_EVENT\t2021-12-20 21:58:41.097007\t\\N\t\\N\t\\N\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Expectation Result Event in JSON\nDESCRIPTION: This snippet shows a Dagster event log entry for a step expectation result. It includes details about the expectation check, its success status, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepExpectationResultData\",\n      \"expectation_result\": {\n        \"__class__\": \"ExpectationResult\",\n        \"description\": \"Checked raw_file_event_admins exists\",\n        \"label\": \"output_table_exists\",\n        \"metadata_entries\": [],\n        \"success\": true\n      }\n    },\n    \"event_type_value\": \"STEP_EXPECTATION_RESULT\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"raw_file_event_admins\",\n      \"solid_definition\": \"raw_file_event_admins\",\n      \"step_key\": \"raw_file_event_admins.compute\"\n    },\n    \"message\": \"Checked raw_file_event_admins exists\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"raw_file_event_admins\",\n      \"name\": \"raw_file_event_admins\",\n      \"parent\": null\n    },\n    \"step_key\": \"raw_file_event_admins.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Checked raw_file_event_admins exists\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"output_table_exists\\\", \\\"Checked raw_file_event_admins exists\\\", []]}\\n               solid = \\\"raw_file_event_admins\\\"\\n    solid_definition = \\\"raw_file_event_admins\\\"\\n            step_key = \\\"raw_file_event_admins.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"raw_file_event_admins.compute\",\n  \"timestamp\": 1576110682.7872021,\n  \"user_message\": \"Checked raw_file_event_admins exists\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Instigators by Type in SQL\nDESCRIPTION: Creates a B-tree index on the instigators table to optimize queries filtering by instigator_type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_53\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_instigators_instigator_type ON public.instigators USING btree (instigator_type);\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Key Constraint to Daemon Heartbeats in PostgreSQL\nDESCRIPTION: Defines a unique constraint on the 'daemon_type' field in the 'daemon_heartbeats' table to prevent multiple heartbeats of the same type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n```\n\n----------------------------------------\n\nTITLE: Deploying to Vercel\nDESCRIPTION: Command to deploy the marketplace API project to Vercel's hosting platform.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/marketplace/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nvercel\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster OBJECT_STORE_OPERATION Event\nDESCRIPTION: This JSON snippet represents a Dagster event log entry indicating an object store operation related to the step 'ingest_costs' within the 'longitudinal_pipeline'. The operation is 'SET_OBJECT', indicating that an intermediate object for the output 'result' has been stored in the filesystem object store using pickle. The event includes the address of the stored object, metadata entries, and logging tags for context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/ingest_costs/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/ingest_costs/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_costs\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_costs\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_costs\", \"parent\": null}, \"step_key\": \"ingest_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ingest_costs - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"ingest_costs\", \"timestamp\": 1609894312.562929, \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Building Dagster Docker Images\nDESCRIPTION: Command to build Docker images for Dagster using specified parameters. Ensure that Python versions are listed in 'versions.yaml'. The '--platform=linux/amd64' flag might be necessary for certain architectures.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation/docker/images/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster-image build-all --name <IMAGE NAME> --dagster-version <DAGSTER VERSION> --platform=linux/amd64\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Runs by Status in SQL\nDESCRIPTION: Creates a B-tree index on the runs table to optimize queries filtering by status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Sample Event Log Entry in Dagster\nDESCRIPTION: Contains a detailed JSON-formatted event log entry representing a pipeline start event with metadata and execution details\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"PIPELINE_STARTING\",\n    \"logging_tags\": {},\n    \"message\": null,\n    \"pipeline_name\": \"succeeds_job\"\n  },\n  \"run_id\": \"3a329e81-04ee-493a-b376-fb5d04d4068b\",\n  \"timestamp\": 1668643915.4717178\n}\n```\n\n----------------------------------------\n\nTITLE: Adding UNIQUE Constraint for asset_keys Table in PostgreSQL\nDESCRIPTION: Adds a unique constraint on the column 'asset_key' in the 'asset_keys' table, ensuring that all values in this column are distinct.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n```\n\n----------------------------------------\n\nTITLE: Initial Project Structure Display\nDESCRIPTION: Shows the initial directory structure of the project before migration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmy_existing_project/\n├── definitions.py\n├── defs/\n├── elt/\n│   ├── __init__.py\n│   └── assets.py\n├── ml/\n│   ├── __init__.py\n│   └── assets.py\n└── viz/\n    ├── __init__.py\n    └── assets.py\n```\n\n----------------------------------------\n\nTITLE: Create jobs Table\nDESCRIPTION: This SQL statement creates the `jobs` table for storing information about Dagster jobs. It includes fields for job origin ID, repository origin ID, status, job type, job body, creation timestamp, and update timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\"\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to snapshots Table in SQL\nDESCRIPTION: Adds a primary key constraint on the id column and a unique constraint on the snapshot_id column of the snapshots table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key and Unique Constraints to instigators Table in SQL\nDESCRIPTION: Adds a primary key constraint on the id column and a unique constraint on the selector_id column of the instigators table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_selector_id_key UNIQUE (selector_id);\n```\n\n----------------------------------------\n\nTITLE: Updated DagsterType Configuration\nDESCRIPTION: Example showing the new loader and materializer configuration for DagsterType definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import DagsterType, dagster_type_loader, dagster_type_materializer\n\n\n@dagster_type_loader(config_schema=my_config_schema)\ndef my_loader(_context, config):\n    '''some implementation'''\n\n\n@dagster_type_materializer(config_schema=my_config_schema)\ndef my_materializer(_context, config):\n    '''some implementation'''\n\n\nMyType = DagsterType(\n    loader=my_loader,\n    materializer=my_materializer,\n    type_check_fn=my_type_check,\n)\n```\n\n----------------------------------------\n\nTITLE: Table Constraints Configuration\nDESCRIPTION: SQL statements defining primary key, unique constraints, and foreign key relationships for the database tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.schedule_ticks\n    ADD CONSTRAINT schedule_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.schedules\n    ADD CONSTRAINT schedules_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.schedules\n    ADD CONSTRAINT schedules_repository_name_schedule_name_key UNIQUE (repository_name, schedule_name);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Materialization Event in JSON\nDESCRIPTION: This snippet represents a Dagster event record for a step materialization. It includes metadata about a materialized table named 'raw_fans', such as its path, name, and a markdown description.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepMaterializationData\",\n      \"materialization\": {\n        \"__class__\": \"Materialization\",\n        \"description\": null,\n        \"label\": \"table_info\",\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"TextMetadataEntryData\",\n              \"text\": \"raw_fans\"\n            },\n            \"label\": \"table_name\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"PathMetadataEntryData\",\n              \"path\": \"/path/to/raw_fans\"\n            },\n            \"label\": \"table_path\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"JsonMetadataEntryData\",\n              \"data\": {\"name\": \"raw_fans\"}\n            },\n            \"label\": \"table_data\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"UrlMetadataEntryData\",\n              \"url\": \"https://bigty.pe/raw_fans\"\n            },\n            \"label\": \"table_name_big\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"MarkdownMetadataEntryData\",\n              \"md_str\": \"# h1 Heading :)\\n\\n## h2 Heading\\n\\n### h3 Heading\\n\\n#### h4 Heading\\n\\n##### h5 Heading\\n\\n###### h6 Heading\\n\\n## Horizontal Rules\\n\\n---\\n\\n## Blockquotes\\n\\n> Blockquote can be nested\\n>\\n> > indentation by arrow level\\n\\n## Unordered lists\\n\\n- One\\n- Two\\n  - Indented\\n- Three\\n\\n## Ordered lists\\n\\n1. One\\n1. Two\\n1. Three\\n\\n## Code\\n\\nInline `code`\\n\\n    // comments\\n    line 1 of code\\n    line 2 of code\\n    line 3 of code\\n\\n## Links\\n\\n[rick roll](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\\n\"\n            },\n            \"label\": \"table_blurb\"\n          }\n        ]\n      }\n    },\n    \"event_type_value\": \"STEP_MATERIALIZATION\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"many_table_materializations\",\n      \"solid_definition\": \"many_table_materializations\",\n      \"step_key\": \"many_table_materializations.compute\"\n    },\n    \"message\": \"Materialized value table_info.\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"many_table_materializations\",\n      \"name\": \"many_table_materializations\",\n      \"parent\": null\n    },\n    \"step_key\": \"many_table_materializations.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_name\\\", null, [\\\"raw_fans\\\"]], [\\\"table_path\\\", null, [\\\"/path/to/raw_fans\\\"]], [\\\"table_data\\\", null, [{\\\"name\\\": \\\"raw_fans\\\"}]], [\\\"table_name_big\\\", null, [\\\"https://bigty.pe/raw_fans\\\"]], [\\\"table_blurb\\\", null, [\\\"# h1 Heading :)\\\\n\\\\n## h2 Heading\\\\n\\\\n### h3 Heading\\\\n\\\\n#### h4 Heading\\\\n\\\\n##### h5 Heading\\\\n\\\\n###### h6 Heading\\\\n\\\\n## Horizontal Rules\\\\n\\\\n---\\\\n\\\\n## Blockquotes\\\\n\\\\n> Blockquote can be nested\\\\n>\\\\n> > indentation by arrow level\\\\n\\\\n## Unordered lists\\\\n\\\\n- One\\\\n- Two\\\\n  - Indented\\\\n- Three\\\\n\\\\n## Ordered lists\\\\n\\\\n1. One\\\\n1. Two\\\\n1. Three\\\\n\\\\n## Code\\\\n\\\\nInline `code`\\\\n\\\\n    // comments\\\\n    line 1 of code\\\\n    line 2 of code\\\\n    line 3 of code\\\\n\\\\n## Links\\\\n\\\\n[rick roll](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\\\\n\\\"]]]]}\n               solid = \\\"many_table_materializations\\\"\n    solid_definition = \\\"many_table_materializations\\\"\n            step_key = \\\"many_table_materializations.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"many_table_materializations.compute\",\n  \"timestamp\": 1576110683.52045,\n  \"user_message\": \"Materialized value table_info.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Plugin List in Command Line\nDESCRIPTION: This snippet shows the output of running 'dg list plugins' command, which displays a table of available Dagster plugins, their objects, symbols, summaries, and features.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/9-dg-list-plugins.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list plugins\n\nUsing /.../jaffle-platform/.venv/bin/dagster-components\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Plugin        ┃ Objects                                                                                              ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ dagster       │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓ │\n│               │ ┃ Symbol                                                      ┃ Summary          ┃ Features        ┃ │\n│               │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩ │\n│               │ │ dagster.asset                                               │ Create a         │ [scaffold-targ… │ │\n│               │ │                                                             │ definition for   │                 │ │\n│               │ │                                                             │ how to compute   │                 │ │\n│               │ │                                                             │ an asset.        │                 │ │\n│               │ ├─────────────────────────────────────────────────────────────┼──────────────────┼─────────────────┤ │\n│               │ │ dagster.components.DefinitionsComponent                     │ An arbitrary set │ [component,     │ │\n│               │ │                                                             │ of dagster       │ scaffold-targe… │ │\n│               │ │                                                             │ definitions.     │                 │ │\n│               │ ├─────────────────────────────────────────────────────────────┼──────────────────┼─────────────────┤ │\n│               │ │ dagster.components.DefsFolderComponent                      │ A folder which   │ [component,     │ │\n│               │ │                                                             │ may contain      │ scaffold-targe… │ │\n│               │ │                                                             │ multiple         │                 │ │\n│               │ │                                                             │ submodules, each │                 │ │\n│               │ │                                                             │ which define     │                 │ │\n│               │ │                                                             │ components.      │                 │ │\n│               │ ├─────────────────────────────────────────────────────────────┼──────────────────┼─────────────────┤ │\n│               │ │ dagster.components.PipesSubprocessScriptCollectionComponent │ Assets that wrap │ [component,     │ │\n│               │ │                                                             │ Python scripts   │ scaffold-targe… │ │\n│               │ │                                                             │ executed with    │                 │ │\n│               │ │                                                             │ Dagster's        │                 │ │\n│               │ │                                                             │ PipesSubprocess… │                 │ │\n│               │ ├─────────────────────────────────────────────────────────────┼──────────────────┼─────────────────┤ │\n│               │ │ dagster.schedule                                            │ Creates a        │ [scaffold-targ… │ │\n│               │ │                                                             │ schedule         │                 │ │\n│               │ │                                                             │ following the    │                 │ │\n│               │ │                                                             │ provided cron    │                 │ │\n│               │ │                                                             │ schedule and     │                 │ │\n│               │ │                                                             │ requests runs    │                 │ │\n│               │ │                                                             │ for the provided │                 │ │\n│               │ │                                                             │ job.             │                 │ │\n│               │ ├─────────────────────────────────────────────────────────────┼──────────────────┼─────────────────┤ │\n│               │ │ dagster.sensor                                              │ Creates a sensor │ [scaffold-targ… │ │\n│               │ │                                                             │ where the        │                 │ │\n│               │ │                                                             │ decorated        │                 │ │\n│               │ │                                                             │ function is used │                 │ │\n│               │ │                                                             │ as the sensor's  │                 │ │\n│               │ │                                                             │ evaluation       │                 │ │\n│               │ │                                                             │ function.        │                 │ │\n│               │ └─────────────────────────────────────────────────────────────┴──────────────────┴─────────────────┘ │\n│ dagster_sling │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓ │\n│               │ ┃ Symbol                                            ┃ Summary              ┃ Features              ┃ │\n│               │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩ │\n│               │ │ dagster_sling.SlingReplicationCollectionComponent │ Expose one or more   │ [component,           │ │\n│               │ │                                                   │ Sling replications   │ scaffold-target]      │ │\n│               │ │                                                   │ to Dagster as        │                       │ │\n│               │ │                                                   │ assets.              │                       │ │\n│               │ └───────────────────────────────────────────────────┴──────────────────────┴───────────────────────┘ │\n└───────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Alter run_tags Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `run_tags` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.run_tags OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to alembic_version Table in PostgreSQL\nDESCRIPTION: This snippet adds a primary key constraint on the 'version_num' column of the 'alembic_version' table to ensure unique version tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry for STEP_SUCCESS Event\nDESCRIPTION: JSON representation of a Dagster event log entry for a STEP_SUCCESS event. This entry records the successful completion of the 'div_four.div_two' step with execution time of 46ms.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 46.083018999999226}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"div_two\", \"step_key\": \"div_four.div_two\"}, \"message\": \"Finished execution of step \\\"div_four.div_two\\\" in 46ms.\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"div_four.div_two\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}}, \"step_key\": \"div_four.div_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - div_four.div_two - STEP_SUCCESS - Finished execution of step \\\"div_four.div_two\\\" in 46ms.\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"div_four.div_two\", \"timestamp\": 1640037523.2137961, \"user_message\": \"Finished execution of step \\\"div_four.div_two\\\" in 46ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Auto-generated pip requirements file listing package dependencies with pinned versions and their dependency relationships. Created using pip-compile for Python 3.8.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/starlift-demo/dbt_example/shared/dbt/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nagate==1.7.1\nannotated-types==0.6.0\nappdirs==1.4.4\nattrs==23.2.0\nbabel==2.14.0\ncertifi==2024.2.2\ncffi==1.16.0\nchardet==5.2.0\ncharset-normalizer==3.3.2\ncli-helpers[styles]==2.3.1\nclick==8.1.7\ncolorama==0.4.6\nconfigobj==5.0.8\ndbt-core==1.7.9\ndbt-duckdb==1.7.3\ndbt-extractor==0.5.1\ndbt-semantic-interfaces==0.4.4\ndiff-cover==8.0.3\nduckcli==0.2.1\nduckdb==0.10.0\nexceptiongroup==1.2.0\nidna==3.6\nimportlib-metadata==6.11.0\nimportlib-resources==6.1.3\niniconfig==2.0.0\nisodate==0.6.1\njinja2==3.1.3\njsonschema==4.21.1\njsonschema-specifications==2023.12.1\nleather==0.4.0\nlogbook==1.5.3\nmarkupsafe==2.1.5\nmashumaro[msgpack]==3.12\nminimal-snowplow-tracker==0.0.2\nmore-itertools==10.2.0\nmsgpack==1.0.8\nnetworkx==3.1\npackaging==24.0\nparsedatetime==2.6\npathspec==0.11.2\npkgutil-resolve-name==1.3.10\npluggy==1.4.0\nprompt-toolkit==3.0.43\nprotobuf==4.25.3\npycparser==2.21\npydantic==2.6.3\npydantic-core==2.16.3\npygments==2.17.2\npytest==8.1.1\npython-dateutil==2.9.0.post0\npython-slugify==8.0.4\npytimeparse==1.1.8\npytz==2024.1\npyyaml==6.0.1\nreferencing==0.33.0\nregex==2023.12.25\nrequests==2.31.0\nrpds-py==0.18.0\nsix==1.16.0\nsqlfluff==2.3.5\nsqlparse==0.4.4\ntabulate[widechars]==0.9.0\ntblib==3.0.0\ntext-unidecode==1.3\ntoml==0.10.2\ntomli==2.0.1\ntqdm==4.66.2\ntyping-extensions==4.10.0\nurllib3==1.26.18\nwcwidth==0.2.13\nzipp==3.17.0\n```\n\n----------------------------------------\n\nTITLE: Declaring Sphinx Documentation Dependencies\nDESCRIPTION: Specifies required Sphinx packages with version constraints. Pins Sphinx to version 8.x and includes sphinx_toolbox without version restriction.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx>=8,<9\nsphinx_toolbox\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in JSX\nDESCRIPTION: Imports and renders the DocCardList component for displaying GCP documentation navigation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing PostgreSQL Schema\nDESCRIPTION: Defines the 'test-schema' schema in the PostgreSQL database and assigns ownership to the user 'test'. It is necessary to configure schema-specific organization and permissions for the database. The schema name is 'test-schema' and it does not include any data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SCHEMA \"test-schema\";\n\nALTER SCHEMA \"test-schema\" OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Asset Metadata Icons\nDESCRIPTION: A markdown table mapping tag names to their corresponding SVG icon images. Each icon has consistent dimensions of 20x20 pixels and includes tools like Teams, Tecton, TensorFlow, as well as data quality indicators like bronze/silver/gold and asset types like tables, views, and dashboards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_KindsTags.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| `teams`               | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-teams-color.svg\" width={20} height={20} /> |\n| `tecton`              | <img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-tecton-color.svg\" width={20} height={20} /> |\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for event_logs Table in SQL\nDESCRIPTION: Creates indexes on asset_key, asset_partition, dagster_event_type, run_id, and step_key columns of the event_logs table to improve query performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\n\nCREATE INDEX idx_event_type ON public.event_logs USING btree (dagster_event_type, id);\n\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n```\n\n----------------------------------------\n\nTITLE: Creating Schedules Table in PostgreSQL\nDESCRIPTION: Defines a table to track scheduled pipeline executions with repository name, schedule details, status, and timestamps\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.schedules (\n    id integer NOT NULL,\n    repository_name character varying(255),\n    schedule_name character varying,\n    status character varying(63),\n    schedule_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Connections per Shuffle Peer - Shuffle Settings\nDESCRIPTION: This property defines the number of connections to reuse between hosts during shuffle operations. Adjusting this value can optimize performance based on cluster architecture.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_19\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.io.numConnectionsPerPeer\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Pipeline Completion Event in JSON\nDESCRIPTION: JSON log entry for a pipeline exit event, recording when the process for the 'longitudinal_pipeline' exited with its process ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 18688).\", \"pid\": null, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Process for pipeline exited (pid: 18688).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608667064.406528, \"user_message\": \"Process for pipeline exited (pid: 18688).\"}\n```\n\n----------------------------------------\n\nTITLE: Alter job_ticks_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `job_ticks_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.job_ticks_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Yielding Result with Dagstermill\nDESCRIPTION: Demonstrates how to use dagstermill.yield_result() to return a value in a Dagster notebook or pipeline context\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/yield_3.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\\n\\ndagstermill.yield_result(3)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Run Data Insert\nDESCRIPTION: SQL data insertion for pipeline run record showing a successful execution of pandas_hello_world_pipeline with associated configuration and timing metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.runs (id, run_id, pipeline_name, status, run_body, create_timestamp, update_timestamp) FROM stdin;\n1\td5f89349-7477-4fab-913e-0925cef0a959\tpandas_hello_world_pipeline\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"environment_dict\": {\"solids\": {\"mult_solid\": {\"inputs\": {\"num_df\": {\"csv\": {\"path\": \"data/num_prod.csv\"}}}}, \"sum_solid\": {\"inputs\": {\"num_df\": {\"csv\": {\"path\": \"data/num_prod.csv\"}}}}}}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"pandas_hello_world_pipeline\", \"pipeline_snapshot_id\": null, \"previous_run_id\": null, \"root_run_id\": null, \"run_id\": \"d5f89349-7477-4fab-913e-0925cef0a959\", \"selector\": {\"__class__\": \"ExecutionSelector\", \"name\": \"pandas_hello_world_pipeline\", \"solid_subset\": null}, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {}}\t2020-04-11 13:33:35.168311\t2020-04-11 06:33:38.142666\n\\.\n```\n\n----------------------------------------\n\nTITLE: Alter snapshots Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `snapshots` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.snapshots OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraint to daemon_heartbeats Table in SQL\nDESCRIPTION: Adds a unique constraint on the 'daemon_type' column of the daemon_heartbeats table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n```\n\n----------------------------------------\n\nTITLE: Actor Feed Snapshot Asset Implementation\nDESCRIPTION: Asset implementation for processing individual member feeds with dynamic partitioning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/ingestion.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstart_actor_feed_snapshot\n@asset(\n    partitions_def=member_partitions,\n    auto_materialize_condition=dg.AutomationCondition.eager(),\n    kinds=[\"Python\"],\n    group_name=\"ingestion\",\n)\ndef actor_feed_snapshot(\n    context,\n    atproto: ATProtoResource,\n    s3: S3Resource,\n) -> None:\n    client = atproto.get_client()\n    feed_items = get_all_feed_items(client, context.partition_key)\n    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    object_path = f\"feeds/{context.partition_key}/{date_str}.json\"\n    s3.put_object(\n        Bucket=BUCKET,\n        Key=object_path,\n        Body=json.dumps(feed_items).encode(),\n    )\n\n    return dg.MetadataValue.json({\"file\": object_path, \"count\": len(feed_items)})\nend_actor_feed_snapshot\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Sequence and Constraint Settings for Database Tables\nDESCRIPTION: SQL statements showing sequence values and database constraints for various tables in the Dagster PostgreSQL database. Includes primary key constraints, unique constraints, and sequence values for IDs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 23, true);\n\n\n--\n-- Name: backfills_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.backfills_id_seq', 1, false);\n\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.bulk_actions_id_seq', 1, false);\n\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.event_logs_id_seq', 3054, true);\n\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\n\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\n\n\n--\n-- Name: normalized_cereals_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.normalized_cereals_id_seq', 77, true);\n\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.run_tags_id_seq', 60, true);\n\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.runs_id_seq', 12, true);\n\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 1, true);\n\n\n--\n-- Name: snapshots_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Installing the Smoke Test Example with Development Dependencies\nDESCRIPTION: Command to install the example project along with its development dependencies. The -e flag ensures the package is installed in development mode, allowing for changes to be immediately reflected.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_smoke_test/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Dagster Cloud\nDESCRIPTION: Creates a new namespace in Kubernetes for Dagster Cloud resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create namespace dagster-cloud\n```\n\n----------------------------------------\n\nTITLE: Importing and Using DocCardList Component in JSX\nDESCRIPTION: Imports the DocCardList component from the theme and renders it to display a list of documentation cards related to deployment management. This is commonly used in documentation sites to create navigation or topic indexes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dagster Definitions and Components Relationships\nDESCRIPTION: Mermaid diagram showing how Definitions object relates to other Dagster components like Assets, Jobs, Schedules, and Code Location. Shows hierarchical relationships with style configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_6\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Job fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Schedule fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Sensor fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style IOManager fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Resource fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style AssetCheck fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style CodeLocation fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Asset -.-> Definitions\n    AssetCheck -.-> Definitions\n    IOManager -.-> Definitions\n    Job -.-> Definitions\n    Resource -.-> Definitions\n    Schedule -.-> Definitions\n    Sensor -.-> Definitions\n\n    Definitions(Definitions)\n\n    Definitions ==> CodeLocation\n```\n\n----------------------------------------\n\nTITLE: Creating DynamoDB Lock Table for S3\nDESCRIPTION: AWS CLI command to create a DynamoDB table for implementing locking mechanism with Delta Lake S3 operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/deltalake/reference.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naws dynamodb create-table --table-name delta_rs_lock_table \\\n    --attribute-definitions \\\n        AttributeName=key,AttributeType=S \\\n    --key-schema \\\n        AttributeName=key,KeyType=HASH \\\n    --provisioned-throughput \\\n        ReadCapacityUnits=10,WriteCapacityUnits=10\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Plugins List Command Output\nDESCRIPTION: Terminal output from the 'dg list plugins' command showing available Dagster plugins and their components. The output is formatted in tables showing plugins, their symbols, summaries, and supported features.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/28-dg-list-plugins.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndg list plugins\n\nUsing /.../jaffle-platform/.venv/bin/dagster-components\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Plugin           ┃ Objects                                                                                           ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ dagster          │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓ │\n│                  │ ┃ Symbol                                                      ┃ Summary        ┃ Features       ┃ │\n│                  │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩ │\n│                  │ │ dagster.asset                                               │ Create a       │ [scaffold-tar… │ │\n│                  │ │                                                             │ definition for │                │ │\n│                  │ │                                                             │ how to compute │                │ │\n│                  │ │                                                             │ an asset.      │                │ │\n│                  │ ├─────────────────────────────────────────────────────────────┼────────────────┼────────────────┤ │\n│                  │ │ dagster.components.DefinitionsComponent                     │ An arbitrary   │ [component,    │ │\n│                  │ │                                                             │ set of dagster │ scaffold-targ… │ │\n│                  │ │                                                             │ definitions.   │                │ │\n│                  │ ├─────────────────────────────────────────────────────────────┼────────────────┼────────────────┤ │\n│                  │ │ dagster.components.DefsFolderComponent                      │ A folder which │ [component,    │ │\n│                  │ │                                                             │ may contain    │ scaffold-targ… │ │\n│                  │ │                                                             │ multiple       │                │ │\n│                  │ │                                                             │ submodules,    │                │ │\n│                  │ │                                                             │ each           │                │ │\n│                  │ │                                                             │ which define   │                │ │\n│                  │ │                                                             │ components.    │                │ │\n│                  │ ├─────────────────────────────────────────────────────────────┼────────────────┼────────────────┤ │\n│                  │ │ dagster.components.PipesSubprocessScriptCollectionComponent │ Assets that    │ [component,    │ │\n│                  │ │                                                             │ wrap Python    │ scaffold-targ… │ │\n│                  │ │                                                             │ scripts        │                │ │\n│                  │ │                                                             │ executed with  │                │ │\n│                  │ │                                                             │ Dagster's      │                │ │\n│                  │ │                                                             │ PipesSubproce… │                │ │\n│                  │ ├─────────────────────────────────────────────────────────────┼────────────────┼────────────────┤ │\n│                  │ │ dagster.schedule                                            │ Creates a      │ [scaffold-tar… │ │\n│                  │ │                                                             │ schedule       │                │ │\n│                  │ │                                                             │ following the  │                │ │\n│                  │ │                                                             │ provided cron  │                │ │\n│                  │ │                                                             │ schedule and   │                │ │\n│                  │ │                                                             │ requests runs  │                │ │\n│                  │ │                                                             │ for the        │                │ │\n│                  │ │                                                             │ provided job.  │                │ │\n│                  │ ├─────────────────────────────────────────────────────────────┼────────────────┼────────────────┤ │\n│                  │ │ dagster.sensor                                              │ Creates a      │ [scaffold-tar… │ │\n│                  │ │                                                             │ sensor where   │                │ │\n│                  │ │                                                             │ the decorated  │                │ │\n│                  │ │                                                             │ function is    │                │ │\n│                  │ │                                                             │ used as the    │                │ │\n│                  │ │                                                             │ sensor's       │                │ │\n│                  │ │                                                             │ evaluation     │                │ │\n│                  │ │                                                             │ function.      │                │ │\n│                  │ └─────────────────────────────────────────────────────────────┴────────────────┴────────────────┘ │\n│ dagster_dbt      │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │\n│                  │ ┃ Symbol                          ┃ Summary                      ┃ Features                     ┃ │\n│                  │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │\n│                  │ │ dagster_dbt.DbtProjectComponent │ Expose a DBT project to      │ [component, scaffold-target] │ │\n│                  │ │                                 │ Dagster as a set of assets.  │                              │ │\n│                  │ └─────────────────────────────────┴──────────────────────────────┴──────────────────────────────┘ │\n│ dagster_evidence │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │\n│                  │ ┃ Symbol                           ┃ Summary                      ┃ Features                    ┃ │\n│                  │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩ │\n│                  │ │ dagster_evidence.EvidenceProject │ Expose an Evidence.dev       │ [component,                 │ │\n│                  │ │                                  │ dashboard as a Dagster       │ scaffold-target]            │ │\n│                  │ │                                  │ asset.                       │                             │ │\n│                  │ └──────────────────────────────────┴──────────────────────────────┴─────────────────────────────┘ │\n│ dagster_sling    │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓ │\n│                  │ ┃ Symbol                                            ┃ Summary             ┃ Features            ┃ │\n│                  │ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩ │\n│                  │ │ dagster_sling.SlingReplicationCollectionComponent │ Expose one or more  │ [component,         │ │\n│                  │ │                                                   │ Sling replications  │ scaffold-target]    │ │\n│                  │ │                                                   │ to Dagster as       │                     │ │\n│                  │ │                                                   │ assets.             │                     │ │\n│                  │ └───────────────────────────────────────────────────┴─────────────────────┴─────────────────────┘ │\n└──────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Creating bulk_actions_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates a sequence named 'bulk_actions_id_seq' for generating unique IDs for the 'bulk_actions' table. This allows for the automatic increment of IDs for new records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: bulk_actions_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.bulk_actions_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.bulk_actions_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster OBJECT_STORE_OPERATION Event in JSON\nDESCRIPTION: This JSON snippet details a Dagster OBJECT_STORE_OPERATION event that occurs when an intermediate output object is stored. It includes the storage location, operation type (SET_OBJECT), and metadata about the storage mechanism (memory object store using pickle).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_input.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_input.compute/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Stored intermediate object for output result in memory object store using pickle.\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_input.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466123.661306, \"user_message\": \"Stored intermediate object for output result in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Values and Descriptions using Dagster docs\nDESCRIPTION: A documentation block that defines the various possible status values for orders in the system. It includes a markdown table that maps each status (placed, shipped, completed, return_pending, and returned) to its corresponding business description.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_duplicate_source_asset_key/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Populating Alembic Version Table in PostgreSQL\nDESCRIPTION: SQL COPY statement that inserts Alembic version data into the alembic_version table. This table is used by the Alembic migration system to track the current database schema version.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\n3e71cf573ba6\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating runs Table in PostgreSQL\nDESCRIPTION: This SQL snippet establishes the 'runs' table to track execution runs of pipelines, capturing their status and associated metadata. It includes various important identifiers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: runs; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name character varying,\n    status character varying(63),\n    run_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.runs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Jobs in PostgreSQL\nDESCRIPTION: Applies a primary key constraint on the 'id' column of 'jobs' table, ensuring the unique assimilation of each job record.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Defining and Managing Schedules Table\nDESCRIPTION: Creates the 'schedules' table to maintain scheduling information including repository identifiers and status. Default timestamps track creation and updates, bridging into Dagster's scheduling mechanism.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.schedules (\n    id integer NOT NULL,\n    repository_name character varying(255),\n    schedule_name character varying,\n    status character varying(63),\n    schedule_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.schedules OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating jobs_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'jobs_id_seq' sequence which is used to generate unique IDs for new rows in the 'jobs' table. It automates the process of ID assignment, ensuring uniqueness.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: jobs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.jobs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.jobs_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Alembic Version Table in SQL\nDESCRIPTION: Inserts a version number into the 'alembic_version' table. This is likely used for database migration tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\nc9159e740d7e\n\\.\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to jobs Table in SQL\nDESCRIPTION: Adds a unique constraint on the 'job_origin_id' column and a primary key constraint on the 'id' column of the jobs table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for snapshots_id_seq in PostgreSQL\nDESCRIPTION: Sets the current value of the 'snapshots_id_seq' sequence to 1, ensuring unique ID generation for snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.snapshots_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependency for HELM\nDESCRIPTION: This entry defines the dependency for the HELM schema in the Dagster project's pyright environment. It is specified using the relative path to the `dagster/schema` directory. The dependency is marked with the `-e` flag indicating an editable install.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n-e helm/dagster/schema\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster ASSET_MATERIALIZATION_PLANNED Event in JSON\nDESCRIPTION: This JSON snippet represents a Dagster ASSET_MATERIALIZATION_PLANNED event indicating a job's intention to materialize an asset. It includes the asset key ('basic_asset_1') and is part of the plan phase before actual execution begins.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"AssetMaterializationPlannedData\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"basic_asset_1\"]}, \"partition\": null, \"partitions_subset\": null}, \"event_type_value\": \"ASSET_MATERIALIZATION_PLANNED\", \"logging_tags\": {}, \"message\": \"basic_assets_job intends to materialize asset [\\\"basic_asset_1\\\"]\", \"pid\": null, \"pipeline_name\": \"basic_assets_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": \"basic_asset_1\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"basic_assets_job\", \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"step_key\": \"basic_asset_1\", \"timestamp\": 1731664846.554184, \"user_message\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: DuckDB Resource Configuration\nDESCRIPTION: Configuration for DuckDB Resource with ResourceDefinition annotation\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-duckdb.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: DuckDBResource\n  :annotation: ResourceDefinition\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Materialization Event in JSON\nDESCRIPTION: This snippet represents a Dagster event log entry for a step materialization event. It includes details about the materialized value, its path, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepMaterializationData\",\n      \"materialization\": {\n        \"__class__\": \"Materialization\",\n        \"description\": null,\n        \"label\": \"table_info\",\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"PathMetadataEntryData\",\n              \"path\": \"/path/to/raw_file_event_admins.raw\"\n            },\n            \"label\": \"table_path\"\n          }\n        ]\n      }\n    },\n    \"event_type_value\": \"STEP_MATERIALIZATION\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"raw_file_event_admins\",\n      \"solid_definition\": \"raw_file_event_admins\",\n      \"step_key\": \"raw_file_event_admins.compute\"\n    },\n    \"message\": \"Materialized value table_info.\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"raw_file_event_admins\",\n      \"name\": \"raw_file_event_admins\",\n      \"parent\": null\n    },\n    \"step_key\": \"raw_file_event_admins.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/raw_file_event_admins.raw\\\"]]]]\\n               solid = \\\"raw_file_event_admins\\\"\\n    solid_definition = \\\"raw_file_event_admins\\\"\\n            step_key = \\\"raw_file_event_admins.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"raw_file_event_admins.compute\",\n  \"timestamp\": 1576110682.7796,\n  \"user_message\": \"Materialized value table_info.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Secondary Index Data in PostgreSQL for Dagster\nDESCRIPTION: SQL statement for populating the secondary_indexes table which tracks database migration state for Dagster's metadata indexes. Each entry includes an index name, creation timestamp, and migration completion timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.secondary_indexes (id, name, create_timestamp, migration_completed) FROM stdin;\n1\trun_partitions\t2024-10-21 12:37:40.354141\t2024-10-21 12:37:40.342366\n2\trun_repo_label_tags\t2024-10-21 12:37:40.402754\t2024-10-21 12:37:40.385744\n3\tbulk_action_types\t2024-10-21 12:37:40.455787\t2024-10-21 12:37:40.439019\n4\trun_start_end_overwritten\t2024-10-21 12:37:40.526003\t2024-10-21 12:37:40.513725\n5\tasset_key_table\t2024-10-21 12:37:40.648814\t2024-10-21 12:37:40.638386\n6\tasset_key_index_columns\t2024-10-21 12:37:40.683779\t2024-10-21 12:37:40.671983\n7\tschedule_jobs_selector_id\t2024-10-21 12:37:40.812837\t2024-10-21 12:37:40.8024\n8\tschedule_ticks_selector_id\t2024-10-21 12:37:40.904544\t2024-10-21 12:37:40.890472\n\\.\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for jobs id Column\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `jobs` table, ensuring new rows receive unique IDs from the `public.jobs_id_seq` sequence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Documenting Order Status Types in Markdown\nDESCRIPTION: A markdown table that defines the different possible statuses for orders in a system, with descriptions of what each status represents in the order fulfillment process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/components/code_locations/dependency_on_dbt_project_location/defs/jaffle_shop_dbt/jaffle_shop/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Failure in Dagster\nDESCRIPTION: This code logs a pipeline failure event in Dagster, including details such as pipeline name, run ID, and error message. It demonstrates Dagster's structured logging capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n{\n    \"dagster_event\": {\n        \"event_specific_data\": {\n            \"error\": {\n                \"cause\": {\n                    \"message\": \"Instance is out of date and must be migrated (Postgres event log storage requires migration). Database is at revision ddcc6d7244c6, head is 7b8304b4429d. Please run `dagster instance migrate`.\",\n                    \"stack\": [\"  File \\\"/Users/prha/code/dagster/python_modules/libraries/dagster-postgres/dagster_postgres/event_log/event_log.py\\\", line 187, in store_asset\\n    else None,\\n\", \"  File \\\"/Users/prha/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py\\\", line 99, in __exit__\\n    self.gen.throw(type, value, traceback)\\n\", \"  File \\\"/Users/prha/code/dagster/python_modules/libraries/dagster-postgres/dagster_postgres/utils.py\\\", line 166, in create_pg_connection\\n    yield conn\\n\", \"  File \\\"/Users/prha/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py\\\", line 99, in __exit__\\n    self.gen.throw(type, value, traceback)\\n\", \"  File \\\"/Users/prha/code/dagster/python_modules/dagster/dagster/core/storage/sql.py\\\", line 82, in handle_schema_errors\\n    ) from None\\n\"]\n                }\n            }\n        },\n        \"event_type_value\": \"PIPELINE_FAILURE\",\n        \"logging_tags\": {},\n        \"message\": \"Execution of pipeline \\\"asset_pipeline\\\" failed. An exception was thrown during execution.\",\n        \"pid\": 3447,\n        \"pipeline_name\": \"asset_pipeline\",\n        \"solid_handle\": null,\n        \"step_handle\": null,\n        \"step_key\": null,\n        \"step_kind_value\": null\n    },\n    \"error_info\": null,\n    \"level\": 40,\n    \"message\": \"asset_pipeline - 4a832042-13f5-4b77-ab3f-24b88827402e - 3447 - PIPELINE_FAILURE - Execution of pipeline \\\"asset_pipeline\\\" failed. An exception was thrown during execution.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Airlift Performance Harness\nDESCRIPTION: Commands to install the performance harness utility using uv package manager from the Dagster directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-airlift/perf-harness/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd python_modules/libraries/dagster-airlift/perf-harness\npip install uv\nuv pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Index for Key-Value Store in SQL\nDESCRIPTION: Creates a unique B-tree index on the kvs table to ensure uniqueness of keys in the key-value store.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: sql\nCODE:\n```\nCREATE UNIQUE INDEX idx_kvs_keys_unique ON public.kvs USING btree (key);\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster ENGINE_EVENT with Process Completion JSON in Python\nDESCRIPTION: JSON representation of a Dagster engine event showing process completion. This event indicates that pipeline steps have finished execution in a specific process. The record includes metadata about the process ID and executed step keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_58\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80827\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['do_something.compute', 'do_input.compute']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 80827) in 160ms\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - ENGINE_EVENT - Finished steps in process (pid: 80827) in 160ms\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.677631, \"user_message\": \"Finished steps in process (pid: 80827) in 160ms\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with PEX using Dockerfile\nDESCRIPTION: Dockerfile configuration to package Python dependencies including dagster-pipes into a PEX file for EMR environment. Uses Amazon Linux 2 as base and creates a virtual environment with required packages.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/aws/aws-emr-pipeline.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\n# this Dockerfile can be used to create a venv archive for PySpark on AWS EMR\n\nFROM amazonlinux:2 AS builder\n\nRUN yum install -y python3\n\nWORKDIR /build\n\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\n\nENV VIRTUAL_ENV=/build/.venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\nRUN uv python install --python-preference only-managed 3.9.16 && uv python pin 3.9.16\n\nRUN uv venv .venv\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install pex dagster-pipes boto3 pyspark\n\nRUN pex dagster-pipes boto3 pyspark -o /output/venv.pex && chmod +x /output/venv.pex\n\n# test imports\nRUN /output/venv.pex -c \"import dagster_pipes, pyspark, boto3;\"\n\nFROM scratch AS export\n\nCOPY --from=builder /output/venv.pex /venv.pex\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Key Constraint to Bulk Actions in PostgreSQL\nDESCRIPTION: This snippet defines a unique constraint on the 'key' column within the 'bulk_actions' table, ensuring no duplicates exist while handling bulk actions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n```\n\n----------------------------------------\n\nTITLE: Verification Command Output\nDESCRIPTION: Shows the output of running dg list defs to verify all definitions are loaded.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-definitions.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nFound 3 definitions:\n\n- defs.elt\n- defs.ml\n- defs.viz\n```\n\n----------------------------------------\n\nTITLE: Copying Data into run_tags Table in PostgreSQL for Dagster\nDESCRIPTION: SQL statement for populating the run_tags table which stores metadata associated with Dagster pipeline runs. This example includes repository and gRPC connection information tags for a specific run ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.run_tags (id, run_id, key, value) FROM stdin;\n1\t2464fea5-b2f4-437b-8461-5cd6d6572ff4\t.dagster/repository\t__repository__@backfill_multiple_iterations.py\n2\t2464fea5-b2f4-437b-8461-5cd6d6572ff4\t.dagster/grpc_info\t{\"host\": \"localhost\", \"socket\": \"/var/folders/ns/r7rp0cg558zdj1yjm3p66qn80000gn/T/tmpf7rfg43p\"}\n\\.\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster EventLogEntry for Step Output\nDESCRIPTION: This snippet represents a Dagster EventLogEntry for a step output. It shows that the step 'add_four.emit_two.emit_one_2' yielded the output 'result' of type 'Int', and the type check passed. It provides information about the event, including event-specific data, logging tags, and solid handles.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"metadata_entries\": [], \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"add_four.emit_two.emit_one_2\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"emit_one_2\", \"step_key\": \"add_four.emit_two.emit_one_2\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Int\\\". (Type check passed).\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two.emit_one_2\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two.emit_one_2\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two.emit_one_2 - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Int\\\". (Type check passed).\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two.emit_one_2\", \"timestamp\": 1640037521.439292, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Int\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster EventLogEntry JSON for Step Success Events\nDESCRIPTION: JSON representation of a Dagster EventLogEntry object for a STEP_SUCCESS event. This structure contains execution details, logging information, and timing data for a completed step in a Dagster pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 55.83948700000008}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"emit_one\", \"step_key\": \"add_four.emit_two_2.emit_one\"}, \"message\": \"Finished execution of step \\\"add_four.emit_two_2.emit_one\\\" in 55ms.\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two_2.emit_one\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two_2.emit_one\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two_2.emit_one - STEP_SUCCESS - Finished execution of step \\\"add_four.emit_two_2.emit_one\\\" in 55ms.\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two_2.emit_one\", \"timestamp\": 1640037521.6807811, \"user_message\": \"Finished execution of step \\\"add_four.emit_two_2.emit_one\\\" in 55ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI with Shell Command\nDESCRIPTION: Command to start the Dagster UI from the /tutorial_template directory, which will serve the webserver on a local port.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/jupyter/using-notebooks-with-dagster.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Additional SQL Binary Data Record\nDESCRIPTION: Second record of serialized binary data in PostgreSQL tables. This appears to be an execution plan record that complements the pipeline data from the previous record. Both records use PostgreSQL's binary data format to store compressed Dagster workflow information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: sql\nCODE:\n```\n6\t7293e986c3d0c85ea67e6ddfb211bfa85633fe67\t\\x789ce5575d6b1b3110fc2b41cfa1b83889e3be851068292581344fc1883d691d8be874e224414df07faff6acf37df8d26028c4871f6f35d2eeccac57f21be35c68708e73f6ed8cddfd4111bc2acc8306f368c0ba55e1d9f91983d2ab2508efb8c5d229e75146bc2f03c645ab2c6a6590bbb4832b5a6593ebc97c7279319bc2d5fc5ae0ec02058014cb99105331ff0a53995d5d8a6c4a09e28996bfe2da715f70accac078c63353e6059de7a270de11307dfb12964b2528922a6a2075a085c982d2b24270096e951550ca269e171275f399f6759131a84c83ac736c038bc4c051c96fef68fa1801a4e996850dfe43f40f42d55b24bcc41425f76b8ba414a16fcc9a960ce42416e3f411acf32542ce8be0e376be0223350ee5a214f715e87b85a1dd39581b254e094cd03a06d349759a125dd0beed1a45bb366c161b1225adbd27ffab32d42954179a903765fd8c0b5f6eef7f3d3cfdbe631baa0b3d48f0c095c7bc22b3d815f6a18e5b920708991856145ca1954c2aa6c61ea413abfc44e7e1a8acffb7140793cb8e815c3d4d863a7b371646ddd05b169fdbc7c7eef4e0dd306ad7f7191ddc012d95fad7f548a5e9d0f80f7ab49e22e35664f7db38d929d16d8dce88d87b898ed4ebde7d7eb256b7dffcc34e8ffde2eff2385da7fbd36dd0ecf10ff1bd17cec91ade79f176dceefdd11fa9d3bdd1f51772141c7c\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment (MacOS)\nDESCRIPTION: Commands to create and activate a virtual environment on MacOS using uv.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/llm-fine-tuning/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example source dagster_example/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Workspace Directory Tree Structure\nDESCRIPTION: Command to display the hierarchical directory structure of a Dagster workspace, showing the layout of configuration files, source code, and test directories.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/workspace/2-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd dagster-workspace && tree\n\n.\n├── dg.toml\n└── projects\n    └── project-1\n        ├── pyproject.toml\n        ├── src\n        │   └── project_1\n        │       ├── __init__.py\n        │       ├── definitions.py\n        │       ├── defs\n        │       │   └── __init__.py\n        │       └── lib\n        │           └── __init__.py\n        ├── tests\n        │   └── __init__.py\n        └── uv.lock\n```\n\n----------------------------------------\n\nTITLE: Logging Information to Dagster\nDESCRIPTION: This snippet demonstrates how to log information using the Dagster context's `log` attribute. The `info()` method is used to log a string message to Dagster's logging system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontext.log.info(\"Test logging information\")\n```\n\n----------------------------------------\n\nTITLE: Object Store Operation Log for Build Model in Dagster\nDESCRIPTION: A JSON log entry showing an object store operation in Dagster for the 'build_model' step. This log records the successful storage of the 'result' output from this step in the filesystem using pickle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_53\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/build_model/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/build_model/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_model - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_model\", \"timestamp\": 1608666998.756136, \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Formatting Tabs in Markdown Documentation\nDESCRIPTION: Demonstrates how to create tabbed content in Markdown documentation using the Tabs and TabItem components from Docusaurus.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n<Tabs>\n  <TabItem value=\"github\" label=\"GitHub\" default>\n    This is AWS-specific content.\n  </TabItem>\n  <TabItem value=\"gitlab\" label=\"GitLab\">\n    This is GCP-specific content.\n  </TabItem>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-pagerduty Package\nDESCRIPTION: Command to install the dagster-pagerduty library using pip. This is required to use the PagerDuty integration in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-pagerduty.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-pagerduty\n```\n\n----------------------------------------\n\nTITLE: Dagster Pipeline Event JSON Structure\nDESCRIPTION: JSON structure of Dagster event records showing the format of execution event data. Each record contains detailed information about pipeline steps, including event type, timestamp, process ID, and metadata about outputs and operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"persist_model\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_model\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_model\", \"parent\": null}, \"step_key\": \"persist_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - persist_model - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"persist_model\", \"timestamp\": 1609894322.21651, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Records JSON Structure\nDESCRIPTION: JSON structure representing Dagster pipeline event logs, including step inputs/outputs, success events, and engine events. Contains metadata about DataFrame processing steps and pipeline execution details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepInputData\",\n      \"input_name\": \"mult_df\",\n      \"type_check_data\": {\n        \"__class__\": \"TypeCheckData\",\n        \"description\": null,\n        \"label\": \"mult_df\",\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": \"Number of rows in DataFrame\",\n            \"entry_data\": {\n              \"__class__\": \"TextMetadataEntryData\",\n              \"text\": \"2\"\n            },\n            \"label\": \"row_count\"\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Alter secondary_indexes_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `secondary_indexes_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.secondary_indexes_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Tracking Step Output and Type Checking\nDESCRIPTION: Logs details of step output generation, including type checking results and output metadata in a Dagster pipeline execution\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"success\": true}}}\n```\n\n----------------------------------------\n\nTITLE: Printing Greeting in Python\nDESCRIPTION: This code snippet prints the string \"hello\" to the console, serving as a basic demonstration of output in Python.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprint(\"hello\")\n```\n\n----------------------------------------\n\nTITLE: Altering Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `least_caloric` table in the `test-schema` schema to the `test` user.  This is typically done for access control and permission management in a database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE \"test-schema\".least_caloric OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Alter asset_keys_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `asset_keys_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.asset_keys_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Logging Step Start - JSON\nDESCRIPTION: Logs the start of execution for the 'raw_file_fans' step, providing details about the step execution context. This helps in tracking the pipeline's current state and understanding the flow of execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_fans\", \"solid_definition\": \"raw_file_fans\", \"step_key\": \"raw_file_fans.compute\"}, \"message\": \"Started execution of step \\\"raw_file_fans.compute\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_fans\", \"name\": \"raw_file_fans\", \"parent\": null}, \"step_key\": \"raw_file_fans.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"raw_file_fans.compute\\\".\\n               solid = \\\"raw_file_fans\\\"\\n    solid_definition = \\\"raw_file_fans\\\"\\n            step_key = \\\"raw_file_fans.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_fans.compute\", \"timestamp\": 1576110682.899371, \"user_message\": \"Started execution of step \\\"raw_file_fans.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: TODO Comment About Document Purpose\nDESCRIPTION: A comment noting uncertainty about the purpose of this page, questioning if the content is already covered by another document called \"Customizing agent settings\".\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/settings/hybrid-agent-settings.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{/* TODO not sure why this page exists, is this covered by \"Customizing agent settings\"? */}\n```\n\n----------------------------------------\n\nTITLE: Creating Teradata Authorization for Azure Blob Storage\nDESCRIPTION: SQL statement to create a Teradata Authorization object with Azure Blob Storage account credentials. This is required for secure access to private Azure Blob containers when transferring data to Teradata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nCREATE AUTHORIZATION azure_authorization USER 'azuretestquickstart' PASSWORD 'AZURE_BLOB_ACCOUNT_SECRET_KEY'\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Runs in SQL\nDESCRIPTION: This snippet sets the default value for the 'id' column in the 'runs' table using the sequence 'public.runs_id_seq' to ensure unique identification of run records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: runs id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Records in JSON Format\nDESCRIPTION: These JSON objects represent Dagster event records that track pipeline execution logs. Each record contains metadata about pipeline operations including timestamps, event types, messages, and execution context such as process IDs and step keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80538\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['do_something.compute', 'do_input.compute']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 80538) in 174ms\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - ENGINE_EVENT - Finished steps in process (pid: 80538) in 174ms\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": null, \"timestamp\": 1610466002.947824, \"user_message\": \"Finished steps in process (pid: 80538) in 174ms\"}\n```\n\n----------------------------------------\n\nTITLE: Installing and Setting Up Dagster Environment\nDESCRIPTION: Initial commands to install development dependencies and set up the local environment for the Kitchen Sink testbed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-airlift/kitchen-sink/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\nmake setup_local_env\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Webserver\nDESCRIPTION: Command to install the Dagster webserver component using pip package manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/deploying-dagster-as-a-service.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Requesting Retry in Dagstermill Notebook\nDESCRIPTION: This snippet shows how to raise a RetryRequested exception in a Dagstermill notebook, instructing Dagster to retry the execution of the node. It sets `max_retries=1`, meaning the node will be retried once if it fails.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/yield_retry.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster import RetryRequested\nimport dagstermill\n\ndagstermill.yield_event(RetryRequested(max_retries=1))\n```\n\n----------------------------------------\n\nTITLE: Creating backfills_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'backfills_id_seq' sequence for generating unique identifiers for the 'backfills' table. This sequence allows for automatic ID assignment in a concurrent environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: backfills_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.backfills_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.backfills_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Pipeline Success Event in Dagster (JSON)\nDESCRIPTION: JSON log entry recording the successful completion of a Dagster pipeline named 'foo', including run ID and timestamp information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_SUCCESS\", \"logging_tags\": {}, \"message\": \"Finished execution of pipeline \\\"foo\\\".\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"foo\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": null, \"timestamp\": 1610466063.7197, \"user_message\": \"Finished execution of pipeline \\\"foo\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Order Status Documentation Template in Markdown\nDESCRIPTION: A markdown table documenting the possible order statuses in a commerce system. The table includes status codes and detailed descriptions of what each status represents in the order lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_model_versions/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Snapshots Table Data SQL\nDESCRIPTION: SQL INSERT data for snapshots table containing serialized pipeline and execution plan snapshot data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.snapshots (id, snapshot_id, snapshot_body, snapshot_type) FROM stdin;\n1\tea3ba6e82e89be10e74a953e070fd5f27cc058b6\t\\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\tPIPELINE\n```\n\n----------------------------------------\n\nTITLE: Creating and Altering Snapshots Table in SQL\nDESCRIPTION: Creates a table named 'snapshots' with columns for id, snapshot_id, snapshot_body, and snapshot_type. It also sets up a sequence for the id column and assigns ownership to the 'test' user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.snapshots_id_seq OWNER TO test;\n\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\n```\n\n----------------------------------------\n\nTITLE: Creating public.job_ticks Table and Sequence\nDESCRIPTION: This snippet creates the 'job_ticks' table in the 'public' schema to store information about job ticks, which are used for scheduling and monitoring recurring jobs.  It includes columns for ID, job origin ID, status, type, timestamp, tick body, creation timestamp, and update timestamp. An associated sequence 'job_ticks_id_seq' is also created to automatically generate unique IDs for each job tick.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \\\"timestamp\\\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.job_ticks OWNER TO test;\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.job_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.job_ticks_id_seq OWNER TO test;\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.job_ticks_id_seq OWNED BY public.job_ticks.id;\"\n```\n\n----------------------------------------\n\nTITLE: Creating job_ticks Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the job_ticks table to record the state of jobs over time, enabling tracking and management of job executions in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: job_ticks; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\nALTER TABLE public.job_ticks OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for asset_keys ID in PostgreSQL\nDESCRIPTION: Configures the 'asset_keys' table to use 'asset_keys_id_seq' to automatically assign default ID values, critical for ensuring automated key generation and unique asset key identification.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Dagster Object Store SET Operation Event in JSON Format\nDESCRIPTION: JSON representation of a Dagster object store operation record. This event documents storing an intermediate object for the 'persist_model' step's 'result' output in the filesystem object store using pickle serialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_model/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_model/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_model\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_model\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_model\", \"parent\": null}, \"step_key\": \"persist_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_model - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_model\", \"timestamp\": 1608667064.365171, \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Creating job_ticks_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'job_ticks_id_seq' sequence for generating unique IDs for the 'job_ticks' table's entries. This ensures the efficient tracking of job tick records with unique identifiers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: job_ticks_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.job_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.job_ticks_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Copying Data to Daemon Heartbeats Table in PostgreSQL\nDESCRIPTION: SQL statement to copy daemon heartbeat data into the daemon_heartbeats table. This appears to be empty in the sample data as indicated by the absence of rows between the COPY statement and terminator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.daemon_heartbeats (daemon_type, daemon_id, \"timestamp\", body) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Dagster Pipeline Event Record - Object Store Operation\nDESCRIPTION: JSON log entry capturing object retrieval from memory store using pickle serialization within a Dagster pipeline step\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_something.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_something.compute/result\"}, \"label\": \"key\"}], \"op\": \"GET_OBJECT\", \"value_name\": \"x\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Retrieved intermediate object for input x in memory object store using pickle.\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_input.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input x in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466002.858748, \"user_message\": \"Retrieved intermediate object for input x in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Values in PostgreSQL for Dagster Tables\nDESCRIPTION: These SQL commands set the current value for various sequence generators used in Dagster tables. This ensures proper ID generation for new records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.concurrency_limits_id_seq', 1, false);\n\nSELECT pg_catalog.setval('public.concurrency_slots_id_seq', 1, false);\n\nSELECT pg_catalog.setval('public.daemon_heartbeats_id_seq', 39, true);\n\nSELECT pg_catalog.setval('public.dynamic_partitions_id_seq', 1, false);\n\nSELECT pg_catalog.setval('public.event_logs_id_seq', 73, true);\n\nSELECT pg_catalog.setval('public.instance_info_id_seq', 1, true);\n\nSELECT pg_catalog.setval('public.instigators_id_seq', 1, false);\n\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\n\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\n\nSELECT pg_catalog.setval('public.kvs_id_seq', 2, true);\n\nSELECT pg_catalog.setval('public.pending_steps_id_seq', 1, false);\n\nSELECT pg_catalog.setval('public.run_tags_id_seq', 11, true);\n\nSELECT pg_catalog.setval('public.runs_id_seq', 4, true);\n\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 7, true);\n\nSELECT pg_catalog.setval('public.snapshots_id_seq', 5, true);\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to Runs in PostgreSQL\nDESCRIPTION: Establishes a primary key on the 'id' column in the 'runs' table that maintains unique identifiers for all runs associated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: event_logs_id_seq in PostgreSQL\nDESCRIPTION: Sets up 'event_logs_id_seq' sequence to ensure unique ID generation for 'event_logs' identifier column. Begins at 1, incrementing by 1, for maintaining uniqueness of log records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.event_logs_id_seq OWNER TO test;\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Job Ticks by Status in SQL\nDESCRIPTION: Creates a B-tree index on the job_ticks table to optimize queries filtering by job_origin_id and status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n```\n\n----------------------------------------\n\nTITLE: Defining Data Documentation Overview in Markdown\nDESCRIPTION: This snippet defines the overview documentation for the Jaffle Shop project using Markdown syntax within a Jinja2 template block. It includes a brief description of the project, its purpose, and a link to the source code repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_time_partition_freshness/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Database Indexes for Dagster Tables\nDESCRIPTION: SQL CREATE INDEX statements that create btree indexes on various columns for performance optimization, including indexes on asset keys, run status, job ticks, and tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\n\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\n\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n```\n\n----------------------------------------\n\nTITLE: Defining integration.yaml Structure for Dagster Libraries\nDESCRIPTION: This YAML snippet shows the required structure for the integration.yaml file that must be included in each Dagster library subfolder. The file defines the library name, maintenance status, owner information, maintainer type, and contact email.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: The name of the library, which may be different than the package name\nsuspended: bool, if true, this integration is excluded from tests and is a candidate for removal in the future.\nowner: Name of the person or company who supports this package. Dagster-supported packages will have 'Dagster' as the owner.\nmaintainer_type: Either dagster or community.\nmaintainer_email: The email address of who maintains this package.\n```\n\n----------------------------------------\n\nTITLE: Creating Hot Cereals View by Calories in SQL\nDESCRIPTION: This snippet creates a view named 'sort_hot_cereals_by_calories', filtering the cereal data to include only those of type 'H'. It derives its data from the 'sort_by_calories' view.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: sort_hot_cereals_by_calories; Type: VIEW; Schema: test-schema; Owner: test\n\nCREATE VIEW \"test-schema\".sort_hot_cereals_by_calories AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n  WHERE (sort_by_calories.type = 'H'::text);\n\nALTER TABLE \"test-schema\".sort_hot_cereals_by_calories OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Dagster Pipeline Event Record Structure\nDESCRIPTION: JSON structure for Dagster pipeline event records containing execution details, logging info, and event-specific data. Shows format used for tracking pipeline steps, object store operations, and resource initialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"ObjectStoreOperationResultData\",\n      \"address\": \"/intermediates/do_something.compute/result\",\n      \"metadata_entries\": [\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": null,\n          \"entry_data\": {\n            \"__class__\": \"PathMetadataEntryData\",\n            \"path\": \"/intermediates/do_something.compute/result\"\n          },\n          \"label\": \"key\"\n        }\n      ],\n      \"op\": \"SET_OBJECT\",\n      \"value_name\": \"result\",\n      \"version\": null\n    },\n    \"event_type_value\": \"OBJECT_STORE_OPERATION\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\",\n      \"dagster/schedule_name\": \"foo_schedule\",\n      \"pipeline\": \"foo\",\n      \"solid\": \"do_something\",\n      \"solid_definition\": \"do_something\",\n      \"step_key\": \"do_something.compute\"\n    },\n    \"message\": \"Stored intermediate object for output result in memory object store using pickle.\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster EventLogEntry JSON for Handled Output Events\nDESCRIPTION: JSON representation of a Dagster EventLogEntry for a HANDLED_OUTPUT event. This structure contains information about pipeline outputs being processed by an IO manager, including output names and manager keys.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"HandledOutputData\", \"manager_key\": \"io_manager\", \"metadata_entries\": [], \"output_name\": \"result\"}, \"event_type_value\": \"HANDLED_OUTPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"emit_one_2\", \"step_key\": \"add_four.emit_two_2.emit_one_2\"}, \"message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two_2.emit_one_2\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two_2.emit_one_2\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two_2.emit_one_2 - HANDLED_OUTPUT - Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two_2.emit_one_2\", \"timestamp\": 1640037521.817625, \"user_message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Documentation Template for dbt Project Overview in Markdown\nDESCRIPTION: This snippet defines a documentation block using Jinja templating for a dbt project. It provides an overview of the 'jaffle_shop' fictional ecommerce store project, including a link to the dbt website and the project's GitHub repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_last_update_freshness/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint for Runs and Snapshots\nDESCRIPTION: Creates a foreign key constraint linking the snapshot_id column in the runs table to the snapshots table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_68\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Bulk Actions in SQL\nDESCRIPTION: This snippet sets the default value for the 'id' column of the 'bulk_actions' table to be generated from the sequence 'public.bulk_actions_id_seq', ensuring unique identifiers for new records.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: bulk_actions id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.bulk_actions ALTER COLUMN id SET DEFAULT nextval('public.bulk_actions_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry in JSON\nDESCRIPTION: This snippet represents a single event log entry in JSON format for a Dagster pipeline execution. It includes details about the event type, step information, timestamps, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepSuccessData\",\n      \"duration_ms\": 80.36702999999967\n    },\n    \"event_type_value\": \"STEP_SUCCESS\",\n    \"logging_tags\": {\n      \"pipeline_name\": \"composition\",\n      \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}'}\",\n      \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n      \"solid_name\": \"add\",\n      \"step_key\": \"add_four.emit_two.add\"\n    },\n    \"message\": \"Finished execution of step \\\"add_four.emit_two.add\\\" in 80ms.\",\n    \"pipeline_name\": \"composition\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"add\",\n      \"parent\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"emit_two\",\n        \"parent\": {\n          \"__class__\": \"SolidHandle\",\n          \"name\": \"add_four\",\n          \"parent\": null\n        }\n      }\n    },\n    \"step_key\": \"add_four.emit_two.add\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two.add - STEP_SUCCESS - Finished execution of step \\\"add_four.emit_two.add\\\" in 80ms.\",\n  \"pipeline_name\": \"composition\",\n  \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n  \"step_key\": \"add_four.emit_two.add\",\n  \"timestamp\": 1640037522.161348,\n  \"user_message\": \"Finished execution of step \\\"add_four.emit_two.add\\\" in 80ms.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory in Shell\nDESCRIPTION: Creates a new directory for the Dagster and dbt project using the mkdir command in the shell.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/set-up-dbt-project.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmkdir tutorial-dbt-dagster\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Output Event in JSON\nDESCRIPTION: This snippet represents a Dagster event record for a step output. It includes details about the output type, type check data, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Nothing\\\". (Type check passed).\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Nothing\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"many_materializations_and_passing_expectations.compute\\\", \\\"result\\\"], \\\"type_check_data\\\": [true, \\\"result\\\", null, []]}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry in JSON\nDESCRIPTION: This snippet represents a typical event log entry in the Dagster execution pipeline. It includes details such as the event type, timestamp, step information, and other relevant metadata for tracking job execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"EngineEventData\",\n      \"error\": null,\n      \"marker_end\": null,\n      \"marker_start\": \"resources\",\n      \"metadata_entries\": []\n    },\n    \"event_type_value\": \"RESOURCE_INIT_STARTED\",\n    \"logging_tags\": {},\n    \"message\": \"Starting initialization of resources [io_manager].\",\n    \"pid\": 46291,\n    \"pipeline_name\": \"succeeds_job\",\n    \"solid_handle\": null,\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"key\": \"succeeds\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"succeeds\",\n        \"parent\": null\n      }\n    },\n    \"step_key\": \"succeeds\",\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"\",\n  \"pipeline_name\": \"succeeds_job\",\n  \"run_id\": \"3a329e81-04ee-493a-b376-fb5d04d4068b\",\n  \"step_key\": \"succeeds\",\n  \"timestamp\": 1668643923.92491,\n  \"user_message\": \"Starting initialization of resources [io_manager].\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project Dependencies\nDESCRIPTION: Commands to navigate to the project directory and install its dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/sling_decorator/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd my-dagster-project\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Pipeline Event Record in JSON\nDESCRIPTION: JSON record of a Dagster pipeline event showing object retrieval from filesystem store. The record contains event metadata, logging information, and details about the pipeline execution step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/persist_traffic/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/persist_traffic/result\"}, \"label\": \"key\"}], \"op\": \"GET_OBJECT\", \"value_name\": \"_b\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Retrieved intermediate object for input _b in filesystem object store using pickle.\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_model - OBJECT_STORE_OPERATION - Retrieved intermediate object for input _b in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_model\", \"timestamp\": 1609894319.070787, \"user_message\": \"Retrieved intermediate object for input _b in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Processing Step Output Event in Dagster Pipeline\nDESCRIPTION: JSON representation of a step output event in a Dagster pipeline. The event indicates that the 'return_two' step yielded an output of type 'Any' which passed type checking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"metadata_entries\": [], \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"return_two\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmpq7rk3t2y\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"single_mode\", \"solid\": \"return_two\", \"step_key\": \"return_two\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}}, \"step_key\": \"return_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - return_two - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": \"return_two\", \"timestamp\": 1625607811.567116, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Pipeline Event Record - Log Message\nDESCRIPTION: Log message record indicating a sleep operation being performed during pipeline execution, with relevant step context and duration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"LogMessageRecord\", \"dagster_event\": null, \"error_info\": null, \"level\": 20, \"message\": \"system - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - Sleeping for 1 seconds\", \"pipeline_name\": null, \"run_id\": \"ca7f1e33-526d-4f75-9bc5-3e98da41ab97\", \"step_key\": \"sleeper_4.compute\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Page with JSX Components\nDESCRIPTION: React/JSX code that sets up a documentation page with a DocCardList component for displaying ML pipeline documentation. Includes frontmatter configuration for title and sidebar positioning.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ml-pipelines/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n---\ntitle: \"ML pipelines\"\nsidebar_position: 70\n---\n\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Dagster Database Sequence Settings\nDESCRIPTION: SQL commands setting sequence values for various Dagster database tables that handle runs, events, assets, and other components. These settings show initial values for auto-incrementing primary keys in the database schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.asset_event_tags_id_seq', 1, false);\n\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 1, false);\n\n\n--\n-- Name: bulk_actions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.bulk_actions_id_seq', 1, false);\n\n\n--\n-- Name: event_logs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.event_logs_id_seq', 17, true);\n\n\n--\n-- Name: instigators_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.instigators_id_seq', 1, false);\n\n\n--\n-- Name: job_ticks_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\n\n\n--\n-- Name: jobs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\n\n\n--\n-- Name: run_tags_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.run_tags_id_seq', 3, true);\n\n\n--\n-- Name: runs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.runs_id_seq', 1, true);\n\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 8, true);\n\n\n--\n-- Name: snapshots_id_seq; Type: SEQUENCE SET; Schema: public; Owner: test\n--\n\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Run Partitions in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the runs table for the partition_set and partition columns. This index improves query performance when filtering runs by their partition information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_57\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\n```\n\n----------------------------------------\n\nTITLE: Creating Run Tags Table in PostgreSQL for Dagster\nDESCRIPTION: SQL code to create the run_tags table which stores tags associated with Dagster pipeline runs. It includes columns for tag ID, run ID, key, and value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_55\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id text,\n    key text,\n    value text,\n    CONSTRAINT run_tags_pkey PRIMARY KEY (id),\n    CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id)\n);\n```\n\n----------------------------------------\n\nTITLE: Snapshots Table Data Load in PostgreSQL\nDESCRIPTION: SQL statement to load data into the snapshots table in the public schema. The table appears to be empty as shown by the lack of records after the 'FROM stdin;' line. This table likely stores pipeline configuration snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_65\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.snapshots (id, snapshot_id, snapshot_body, snapshot_type) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Logging Step Materialization in Dagster Pipeline (JSON)\nDESCRIPTION: Log entry for a step materialization event in a Dagster pipeline. It records the materialization of a 'traffic_db_table' asset with associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepMaterializationData\",\n      \"materialization\": {\n        \"__class__\": \"AssetMaterialization\",\n        \"asset_key\": {\n          \"__class__\": \"AssetKey\",\n          \"path\": [\"traffic_db_table\"]\n        },\n        \"description\": null,\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"FloatMetadataEntryData\",\n              \"value\": 9200.0\n            },\n            \"label\": \"persist_traffic\"\n          }\n        ],\n        \"partition\": \"2020-12-08\"\n      }\n    },\n    \"event_type_value\": \"STEP_MATERIALIZATION\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"persist_traffic\",\n      \"solid_definition\": \"base_one_input\",\n      \"step_key\": \"persist_traffic\"\n    },\n    \"message\": \"Materialized value traffic_db_table.\",\n    \"pid\": 18688,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"persist_traffic\",\n      \"parent\": null\n    },\n    \"step_key\": \"persist_traffic\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_traffic - STEP_MATERIALIZATION - Materialized value traffic_db_table.\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\",\n  \"step_key\": \"persist_traffic\",\n  \"timestamp\": 1608666933.6923678,\n  \"user_message\": \"Materialized value traffic_db_table.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Step Key in Event Logs in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the event_logs table for the step_key column. This index enhances query performance when searching for event logs associated with a specific step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_60\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in MDX\nDESCRIPTION: MDX code snippet that imports and renders a DocCardList component from a theme package, used to display a list of documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/index.md#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Importing Legacy prometheus_resource in Python\nDESCRIPTION: This snippet shows how to import the legacy prometheus_resource, which is a ResourceDefinition for Prometheus integration. It's marked as legacy, suggesting there might be a newer replacement available.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-prometheus.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_prometheus import prometheus_resource\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: instigators_id_seq in PostgreSQL\nDESCRIPTION: Creates 'instigators_id_seq', a sequence for generating unique IDs for 'instigators'. The sequence starts at 1 and increments by 1, ensuring a unique identifier for each instigator record.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.instigators_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.instigators_id_seq OWNER TO test;\nALTER SEQUENCE public.instigators_id_seq OWNED BY public.instigators.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Bulk Actions Status Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the status column of the bulk_actions table to optimize queries that filter by bulk action status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Creating Table run_tags in PostgreSQL\nDESCRIPTION: The 'run_tags' table stores mapping between run IDs and tag key-value pairs, essential for categorizing and filtering DAG runs efficiently in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key character varying,\n    value character varying\n);\n\nALTER TABLE public.run_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for run_tags_id_seq in PostgreSQL\nDESCRIPTION: Sets the sequence 'run_tags_id_seq' to 3 and marks it as true, indicating that IDs have been previously generated.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.run_tags_id_seq', 3, true);\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Pipeline Step Execution in JSON\nDESCRIPTION: JSON-formatted log entries for Dagster pipeline step execution. Each entry includes details such as event type, timestamp, step key, and execution metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepSuccessData\",\n      \"duration_ms\": 1047.2058140003355\n    },\n    \"event_type_value\": \"STEP_SUCCESS\",\n    \"logging_tags\": {\n      \"pipeline\": \"sleepy_pipeline\",\n      \"solid\": \"sleeper_2\",\n      \"solid_definition\": \"sleeper\",\n      \"step_key\": \"sleeper_2.compute\"\n    },\n    \"message\": \"Finished execution of step \\\"sleeper_2.compute\\\" in 1.05s.\",\n    \"pipeline_name\": \"sleepy_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"sleeper\",\n      \"name\": \"sleeper_2\",\n      \"parent\": null\n    },\n    \"step_key\": \"sleeper_2.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - STEP_SUCCESS - Finished execution of step \\\"sleeper_2.compute\\\" in 1.05s.\\n event_specific_data = {\\\"duration_ms\\\": 1047.2058140003355}\\n               solid = \\\"sleeper_2\\\"\\n    solid_definition = \\\"sleeper\\\"\\n            step_key = \\\"sleeper_2.compute\\\"\",\n  \"pipeline_name\": \"sleepy_pipeline\",\n  \"run_id\": \"ca7f1e33-526d-4f75-9bc5-3e98da41ab97\",\n  \"step_key\": \"sleeper_2.compute\",\n  \"timestamp\": 1586206654.447701,\n  \"user_message\": \"Finished execution of step \\\"sleeper_2.compute\\\" in 1.05s.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing Helm Chart Values in Shell\nDESCRIPTION: Commands to add the Dagster+ Helm repository and view the available configuration values for the Kubernetes agent.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/configuration.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add dagster-plus https://dagster-io.github.io/helm-user-cloud\nhelm repo update\nhelm show values dagster-plus/dagster-plus-agent\n```\n\n----------------------------------------\n\nTITLE: Activating Python Virtual Environment\nDESCRIPTION: Activates a Python virtual environment using the source command. This makes the virtual environment's Python interpreter and installed packages available in the current shell session.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-c-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Configuring ACS URL and Entity ID for Dagster+ in Google Workspace\nDESCRIPTION: This URL format is used to set up the ACS URL and Entity ID in Google Workspace's Service Provider details. It should be customized with your organization's name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/google-workspace-sso.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://<organization_name>.dagster.cloud/auth/saml/consume\n```\n\n----------------------------------------\n\nTITLE: Setting Automation Conditions for dbt Nodes in Dagster\nDESCRIPTION: This example shows how to create a custom DagsterDbtTranslator to override the AutomationCondition for each dbt node in a project. It sets the condition to AutomationCondition.eager for all dbt nodes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/reference.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass CustomDbtTranslator(DagsterDbtTranslator):\n    def get_automation_condition(self, dbt_resource_props: Mapping[str, Any]) -> AutomationCondition:\n        return AutomationCondition.eager()\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies in Dagster-Anthropic Integration Demo\nDESCRIPTION: Command to install the project and its development dependencies using pip. This is the first step to set up the Dagster project that integrates with Anthropic for prompt engineering.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_prompt_eng/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ESLint with @dagster-io/eslint-config\nDESCRIPTION: This snippet demonstrates how to extend the @dagster-io/eslint-config in a project's ESLint configuration file. It exports a module that extends the shared configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/eslint-config/README.md#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// .eslintrc.js\nmodule.exports = {\n  extends: ['@dagster-io/eslint-config'],\n};\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Table Constraints for Dagster Database Schema\nDESCRIPTION: SQL constraint definitions for Dagster database tables, including primary keys, unique constraints, and other database constraints ensuring data integrity in the schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\n--\n-- Name: alembic_version alembic_version_pkc; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\n\n--\n-- Name: asset_keys asset_keys_asset_key_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\n\n--\n-- Name: asset_keys asset_keys_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: backfills backfills_backfill_id_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.backfills\n    ADD CONSTRAINT backfills_backfill_id_key UNIQUE (backfill_id);\n\n\n--\n-- Name: backfills backfills_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.backfills\n    ADD CONSTRAINT backfills_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: bulk_actions bulk_actions_key_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n\n\n--\n-- Name: bulk_actions bulk_actions_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: daemon_heartbeats daemon_heartbeats_daemon_type_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n\n\n--\n-- Name: event_logs event_logs_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: job_ticks job_ticks_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: jobs jobs_job_origin_id_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\n\n--\n-- Name: jobs jobs_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: normalized_cereals normalized_cereals_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.normalized_cereals\n    ADD CONSTRAINT normalized_cereals_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: run_tags run_tags_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: runs runs_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: runs runs_run_id_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\n\n--\n-- Name: secondary_indexes secondary_indexes_name_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\n\n--\n-- Name: secondary_indexes secondary_indexes_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.secondary_indexes\n```\n\n----------------------------------------\n\nTITLE: Creating Event Logs Table in Dagster Schema\nDESCRIPTION: Establishes a table for storing event logs with details about run events, including run ID, timestamp, event type, and associated asset or step information\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\n```\n\n----------------------------------------\n\nTITLE: Writing Documentation Block in dbt Using Jinja\nDESCRIPTION: A documentation block written in Jinja templating format for dbt (data build tool). This defines the overview section for the Jaffle Shop project documentation, describing it as a fictional ecommerce store used for testing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_semantic_models/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry in JSON\nDESCRIPTION: This snippet represents a single event log entry from a Dagster pipeline execution. It contains detailed information about a specific step in the pipeline, including event type, timestamps, and execution details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"LoadedInputData\",\n      \"input_name\": \"numbers\",\n      \"manager_key\": \"io_manager\",\n      \"upstream_output_name\": \"result\",\n      \"upstream_step_key\": \"add_four.emit_two_2.emit_one_2\"\n    },\n    \"event_type_value\": \"LOADED_INPUT\",\n    \"logging_tags\": {\n      \"pipeline_name\": \"composition\",\n      \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\",\n      \"resource_fn_name\": \"None\",\n      \"resource_name\": \"None\",\n      \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n      \"solid_name\": \"add\",\n      \"step_key\": \"add_four.emit_two_2.add\"\n    },\n    \"message\": \"Loaded input \\\"numbers\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"add_four.emit_two_2.emit_one_2\\\"\",\n    \"pid\": 58212,\n    \"pipeline_name\": \"composition\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"add\",\n      \"parent\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"emit_two_2\",\n        \"parent\": {\n          \"__class__\": \"SolidHandle\",\n          \"name\": \"add_four\",\n          \"parent\": null\n        }\n      }\n    },\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"key\": \"add_four.emit_two_2.add\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"add\",\n        \"parent\": {\n          \"__class__\": \"SolidHandle\",\n          \"name\": \"emit_two_2\",\n          \"parent\": {\n            \"__class__\": \"SolidHandle\",\n            \"name\": \"add_four\",\n            \"parent\": null\n          }\n        }\n      }\n    },\n    \"step_key\": \"add_four.emit_two_2.add\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two_2.add - LOADED_INPUT - Loaded input \\\"numbers\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"add_four.emit_two_2.emit_one_2\\\"\",\n  \"pipeline_name\": \"composition\",\n  \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n  \"step_key\": \"add_four.emit_two_2.add\",\n  \"timestamp\": 1640037522.344652,\n  \"user_message\": \"Loaded input \\\"numbers\\\" using input manager \\\"io_manager\\\", from output \\\"result\\\" of step \\\"add_four.emit_two_2.emit_one_2\\\"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Schedules ID Sequence Setup\nDESCRIPTION: Sets up 'schedules_id_seq' enabling auto-incrementing IDs for schedule entries, crucial for uniquely tracking each record. The sequence is owned by the ID column of the 'schedules' table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.schedules_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.schedules_id_seq OWNER TO test;\n\n\nALTER SEQUENCE public.schedules_id_seq OWNED BY public.schedules.id;\n```\n\n----------------------------------------\n\nTITLE: Step Start Logging with Dagster in JSON\nDESCRIPTION: This JSON snippet logs a STEP_START event in the Dagster pipeline, indicating the commencement of a computation step. It is pivotal for monitoring the flow and order of actions in the pipeline. It requires a properly defined Dagster pipeline with the appropriate solid configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_traffic\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_traffic\"}, \"message\": \"Started execution of step \\\"persist_traffic\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_traffic\", \"parent\": null}, \"step_key\": \"persist_traffic\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_traffic - STEP_START - Started execution of step \\\"persist_traffic\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_traffic\", \"timestamp\": 1608666924.466048, \"user_message\": \"Started execution of step \\\"persist_traffic\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Dagster Step Output Event Log in JSON Format\nDESCRIPTION: A JSON representation of a Dagster event record for step output. This log shows the successful output generation named 'total' of type 'Int' from the 'sleeper' step in the pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"total\", \"step_key\": \"sleeper.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"total\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"sleepy_pipeline\", \"solid\": \"sleeper\", \"solid_definition\": \"sleeper\", \"step_key\": \"sleeper.compute\"}, \"message\": \"Yielded output \\\"total\\\" of type \\\"Int\\\". (Type check passed).\", \"pipeline_name\": \"sleepy_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"sleeper\", \"name\": \"sleeper\", \"parent\": null}, \"step_key\": \"sleeper.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - STEP_OUTPUT - Yielded output \\\"total\\\" of type \\\"Int\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"sleeper.compute\\\", \\\"total\\\"], \\\"type_check_data\\\": [true, \\\"total\\\", null, []]}\\n               solid = \\\"sleeper\\\"\\n    solid_definition = \\\"sleeper\\\"\\n            step_key = \\\"sleeper.compute\\\"\", \"pipeline_name\": \"sleepy_pipeline\", \"run_id\": \"ca7f1e33-526d-4f75-9bc5-3e98da41ab97\", \"step_key\": \"sleeper.compute\", \"timestamp\": 1586206653.076192, \"user_message\": \"Yielded output \\\"total\\\" of type \\\"Int\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraints - SQL\nDESCRIPTION: SQL commands that add foreign key constraints to establish relationships between tables. Links run_tags to runs and runs to snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_68\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Tables and Their Ownership\nDESCRIPTION: This series of snippets provide SQL commands to create tables such as 'alembic_version', 'asset_keys', 'bulk_actions', and others within the 'public' schema, setting ownership to 'test'. These tables facilitate recording versioning, asset key data, bulk actions, and more, essential for data management. Tables have fixed structures as per definition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\nALTER TABLE public.alembic_version OWNER TO test;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    wipe_timestamp timestamp without time zone,\n    last_materialization_timestamp timestamp without time zone,\n    tags text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.asset_keys OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating asset_keys_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates a sequence named 'asset_keys_id_seq' which is linked to the 'asset_keys' table for generating unique identifiers for new entries. This ensures that each asset key has a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Table schedule_ticks in PostgreSQL\nDESCRIPTION: The table 'schedule_ticks' is structured to track individual scheduling events or 'ticks', including status, timestamps, and origin IDs, which are critical for managing automated schedules in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.schedule_ticks (\n    id integer NOT NULL,\n    schedule_origin_id character varying(255),\n    status character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.schedule_ticks OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Parsing DagsterEventRecord JSON for Step Materialization Event (Groups Table)\nDESCRIPTION: JSON structure representing a DagsterEventRecord for a STEP_MATERIALIZATION event. This captures the materialization of a table_info value with metadata about the groups table path in a Dagster pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/groups.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/groups.raw\\\"]]]]}}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.6908958, \"user_message\": \"Materialized value table_info.\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Job Ticks Origin ID Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the job_origin_id column of the job_ticks table to optimize queries that filter by job origin identifier.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_59\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Command to start the Dagster development server which will be accessible at http://localhost:3000.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_dagster_modal_pipes/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Creating alembic_version Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the alembic_version table in a PostgreSQL database. The table is designed to store the version number of database migrations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\nALTER TABLE public.alembic_version OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Setting PostgreSQL Session Parameters\nDESCRIPTION: Sets various session parameters for a PostgreSQL database connection intended for dumping or restoring database schema. This includes encoding, statement timeouts, and message settings. These configurations are prerequisites for maintaining consistent and error-free database operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\nSET default_tablespace = '';\nSET default_table_access_method = heap;\n```\n\n----------------------------------------\n\nTITLE: Asset Keys Table Constraints\nDESCRIPTION: SQL constraints for asset_keys table defining unique and primary key constraints\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to snapshots Table in SQL\nDESCRIPTION: Adds a primary key constraint on the 'id' column and a unique constraint on the 'snapshot_id' column of the snapshots table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: SQL Table Update Error in Postgres\nDESCRIPTION: Failed SQL insert/update statement attempting to modify the asset_keys table, failing due to a missing column 'last_materialization_timestamp'. Indicates a database schema version mismatch requiring migration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO asset_keys (asset_key, last_materialization, last_run_id, last_materialization_timestamp, tags) VALUES (%(asset_key)s, %(last_materialization)s, %(last_run_id)s, %(last_materialization_timestamp)s, %(tags)s) ON CONFLICT (asset_key) DO UPDATE SET last_materialization = %(param_1)s, last_run_id = %(param_2)s, last_materialization_timestamp = %(param_3)s, tags = %(param_4)s RETURNING asset_keys.id\n```\n\n----------------------------------------\n\nTITLE: Installing OpenJDK with Homebrew\nDESCRIPTION: Install Java using Homebrew package manager on M1 MacBook. Verifies Java installation and hardware compatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/antlr_asset_selection/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install openjdk\n```\n\nLANGUAGE: bash\nCODE:\n```\n$(brew --prefix openjdk)/bin/java --version\n```\n\nLANGUAGE: bash\nCODE:\n```\nfile $(brew --prefix openjdk)/bin/java\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo ln -sfn /opt/homebrew/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk\njava -version\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Asset Keys in SQL\nDESCRIPTION: This snippet alters the 'id' column of the 'asset_keys' table to have a default value generated from the sequence 'public.asset_keys_id_seq'. It ensures that new entries automatically receive a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: asset_keys id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Recording Step Start Event - Python\nDESCRIPTION: This snippet logs the start of a computation step within a Dagster pipeline. It captures essential information such as the event type, logging tags, and the step's execution context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Started execution of step \\\"do_input.compute\\\".\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_input.compute - STEP_START - Started execution of step \\\"do_input.compute\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466002.849921, \"user_message\": \"Started execution of step \\\"do_input.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dagster Repository for the Bluesky Project\nDESCRIPTION: Command to clone the Dagster repository and navigate to the Bluesky project directory. This is the first step in setting up the analytics pipeline environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/bluesky/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/docs_project/project_atproto_dashboard\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry for STEP_OUTPUT Event\nDESCRIPTION: JSON representation of a Dagster event log entry for a STEP_OUTPUT event. This entry documents successful output of 'result' from the 'div_two' step with Float type validation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"metadata_entries\": [], \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"div_four.div_two\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"div_two\", \"step_key\": \"div_four.div_two\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Float\\\". (Type check passed).\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"div_four.div_two\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}}, \"step_key\": \"div_four.div_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - div_four.div_two - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Float\\\". (Type check passed).\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"div_four.div_two\", \"timestamp\": 1640037523.17328, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Float\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Options in Markdown\nDESCRIPTION: A markdown table that documents the five possible order statuses (placed, shipped, completed, return_pending, returned) with detailed descriptions of what each status represents in the order lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_python_interleaving/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Event Logs by Run ID in SQL\nDESCRIPTION: Creates a B-tree index on the event_logs table to optimize queries filtering by run_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_events_by_run_id ON public.event_logs USING btree (run_id, id);\n```\n\n----------------------------------------\n\nTITLE: Logging Step Expectation Result - JSON\nDESCRIPTION: Logs a step expectation result indicating whether the output table for the 'raw_file_fans' solid exists, providing evidence of the solid's output validation. This event includes details on the successful check for existence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Checked raw_file_fans exists\", \"label\": \"output_table_exists\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_fans\", \"solid_definition\": \"raw_file_fans\", \"step_key\": \"raw_file_fans.compute\"}, \"message\": \"Checked raw_file_fans exists\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_fans\", \"name\": \"raw_file_fans\", \"parent\": null}, \"step_key\": \"raw_file_fans.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Checked raw_file_fans exists\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"output_table_exists\\\", \\\"Checked raw_file_fans exists\\\", []]}\\n               solid = \\\"raw_file_fans\\\"\\n    solid_definition = \\\"raw_file_fans\\\"\\n            step_key = \\\"raw_file_fans.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_fans.compute\", \"timestamp\": 1576110682.931911, \"user_message\": \"Checked raw_file_fans exists\"}\n```\n\n----------------------------------------\n\nTITLE: Secondary Indexes SQL Data\nDESCRIPTION: SQL INSERT data for secondary index metadata tracking index creation and migration status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.secondary_indexes (id, name, create_timestamp, migration_completed) FROM stdin;\n1\trun_partitions\t2021-07-06 17:41:12.196811\t2021-07-06 17:41:12.19195\n2\tasset_key_table\t2021-07-06 17:41:12.27291\t2021-07-06 17:41:12.268737\n\\.\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dagster Repository\nDESCRIPTION: Git command to clone the Dagster repository from GitHub to your local machine, which is the first step in contributing to the codebase.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:dagster-io/dagster.git\n```\n\n----------------------------------------\n\nTITLE: Logging Object Store Operation in Dagster Pipeline (JSON)\nDESCRIPTION: Log entry for an object store operation in a Dagster pipeline. It records retrieving an intermediate object from the filesystem object store using pickle serialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"ObjectStoreOperationResultData\",\n      \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/ingest_traffic/result\",\n      \"metadata_entries\": [\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": null,\n          \"entry_data\": {\n            \"__class__\": \"PathMetadataEntryData\",\n            \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/ingest_traffic/result\"\n          },\n          \"label\": \"key\"\n        }\n      ],\n      \"op\": \"GET_OBJECT\",\n      \"value_name\": \"_\",\n      \"version\": null\n    },\n    \"event_type_value\": \"OBJECT_STORE_OPERATION\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"persist_traffic\",\n      \"solid_definition\": \"base_one_input\",\n      \"step_key\": \"persist_traffic\"\n    },\n    \"message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\",\n    \"pid\": 18688,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"persist_traffic\",\n      \"parent\": null\n    },\n    \"step_key\": \"persist_traffic\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_traffic - OBJECT_STORE_OPERATION - Retrieved intermediate object for input _ in filesystem object store using pickle.\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\",\n  \"step_key\": \"persist_traffic\",\n  \"timestamp\": 1608666924.474242,\n  \"user_message\": \"Retrieved intermediate object for input _ in filesystem object store using pickle.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Scaffolded Shell Script Template\nDESCRIPTION: Template shell script file generated when scaffolding a new ShellCommandComponent instance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\necho \"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Documentation Link in Markdown\nDESCRIPTION: A markdown link pointing to the official dagster-deltalake documentation on the Dagster documentation website.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-deltalake/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-deltalake\n\nThe docs for `dagster-deltalake` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-deltalake).\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment on MacOS\nDESCRIPTION: Create a new virtual environment named 'dagster_example' and activate it on MacOS.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/rag/index.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv venv dagster_example source dagster_example/bin/activate\n```\n\n----------------------------------------\n\nTITLE: LogMessageRecord JSON Structure\nDESCRIPTION: Example of a Dagster log message record showing system message with step execution details\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"LogMessageRecord\", \"dagster_event\": null, \"error_info\": null, \"level\": 20, \"message\": \"system - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - Sleeping for 1 seconds\\n               solid = \\\"sleeper_2\\\"\\n    solid_definition = \\\"sleeper\\\"\\n            step_key = \\\"sleeper_2.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Setting PostgreSQL Sequence Values for Dagster Database Tables\nDESCRIPTION: A series of SQL commands that set the current value for various sequence objects in the Dagster database schema. These sequences are used for auto-incrementing primary key columns in tables like asset_keys, bulk_actions, event_logs, etc.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 1, false);\nSELECT pg_catalog.setval('public.bulk_actions_id_seq', 1, false);\nSELECT pg_catalog.setval('public.event_logs_id_seq', 74, true);\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\nSELECT pg_catalog.setval('public.run_tags_id_seq', 2, true);\nSELECT pg_catalog.setval('public.runs_id_seq', 1, true);\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 3, true);\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Defining Runs ID Sequence\nDESCRIPTION: Creates a sequence for the 'id' attribute in the 'runs' table, intended for auto-incrementing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Installing @dagster-io/eslint-config Package\nDESCRIPTION: This snippet shows how to install the @dagster-io/eslint-config package using Yarn. The -D flag adds it as a dev dependency.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/eslint-config/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn -D add @dagster-io/eslint-config\n```\n\n----------------------------------------\n\nTITLE: Basic Arithmetic Operations in Python\nDESCRIPTION: This snippet demonstrates simple variable assignment and arithmetic operations in Python. It sets two variables, 'a' and 'b', and calculates their sum, which can serve as a foundational example for further development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/add_two_numbers_no_yield.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\na = 3\nb = 4\n```\n\n----------------------------------------\n\nTITLE: Defining Event Log ID Sequence\nDESCRIPTION: Initializes a sequence for generating unique identifiers for the 'event_log' table. This sequence starts from 1 and increments by 1.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.event_log_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.event_log_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Logging Step Input in Dagster Pipeline\nDESCRIPTION: Records a step input event for the 'persist_model' step in the longitudinal_pipeline. The input named '_' of type 'Any' is received and passes the type check.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepInputData\",\n      \"input_name\": \"_\",\n      \"type_check_data\": {\n        \"__class__\": \"TypeCheckData\",\n        \"description\": null,\n        \"label\": \"_\",\n        \"metadata_entries\": [],\n        \"success\": true\n      }\n    },\n    \"event_type_value\": \"STEP_INPUT\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"persist_model\",\n      \"solid_definition\": \"base_one_input\",\n      \"step_key\": \"persist_model\"\n    },\n    \"message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\",\n    \"pid\": 18688,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"persist_model\",\n      \"parent\": null\n    },\n    \"step_key\": \"persist_model\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_model - STEP_INPUT - Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\",\n  \"step_key\": \"persist_model\",\n  \"timestamp\": 1608667063.837975,\n  \"user_message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\"\n}\n```\n\n----------------------------------------\n\nTITLE: Pipeline Starting Event in Dagster\nDESCRIPTION: JSON log event indicating the start of a Dagster pipeline execution. This event marks the beginning of the longitudinal_pipeline run with minimal metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_STARTING\", \"logging_tags\": {}, \"message\": null, \"pid\": null, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608666898.986248, \"user_message\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: Copying Data to Asset Keys Table in PostgreSQL\nDESCRIPTION: SQL statement to copy asset keys data into the asset_keys table. This appears to be empty in the sample data as indicated by the absence of rows between the COPY statement and terminator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.asset_keys (id, asset_key, last_materialization, last_run_id, asset_details, create_timestamp) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Type Relationship Diagram in Mermaid\nDESCRIPTION: Mermaid flowchart showing how Types connect to Ops, with custom theme configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_13\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    Type(Type)\n\n    style Op fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Type -.-> Op\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Jobs in SQL\nDESCRIPTION: This snippet sets a default value for the 'id' column in the 'jobs' table to be generated from 'public.jobs_id_seq', ensuring that new job records maintain unique identifiers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: jobs id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID for Snapshots Table\nDESCRIPTION: Configures the snapshots table to use the auto-generated ID sequence as its default ID column value\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for secondary_indexes_id_seq in PostgreSQL\nDESCRIPTION: This command sets the 'secondary_indexes_id_seq' sequence to a starting value of 5, allowing corresponding table entries to continue generating sequences correctly.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 5, true);\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Index on job_origin_id and timestamp Columns in job_ticks Table\nDESCRIPTION: SQL command to create a composite index on job_origin_id and timestamp columns in the job_ticks table to optimize time-based job queries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_38\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log Entry - Handled Output\nDESCRIPTION: Event log entry showing successful output handling for the 'add_four.add' step using the 'io_manager'\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"HandledOutputData\", \"manager_key\": \"io_manager\", \"metadata_entries\": [], \"output_name\": \"result\"}, \"event_type_value\": \"HANDLED_OUTPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}'}\"}}\n```\n\n----------------------------------------\n\nTITLE: Parsing DagsterEventRecord JSON for Step Expectation Result Event (Users)\nDESCRIPTION: JSON structure representing a DagsterEventRecord for a STEP_EXPECTATION_RESULT event. This captures a successful row count expectation check for the users table during a Dagster pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Row count passed for users\", \"label\": \"users.row_count\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Row count passed for users\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Row count passed for users\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"users.row_count\\\", \\\"Row count passed for users\\\", []]}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.675036, \"user_message\": \"Row count passed for users\"}\n```\n\n----------------------------------------\n\nTITLE: Yielding Dagster Result\nDESCRIPTION: This snippet demonstrates how to yield a result from the notebook back to Dagster.  It accesses the 'greeting' configuration value from the Dagster op's configuration and yields it as a result using `dagstermill.yield_result()`. The context is required to access the op_config.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_config_struct.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndagstermill.yield_result(context.op_config[\"greeting\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Run Status Sensor to Dagster Definitions\nDESCRIPTION: This snippet shows how to add a run status sensor to a Dagster Definitions object, allowing it to be enabled and used like other sensors in the Dagster ecosystem.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/sensors/run-status-sensors.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    jobs=[job],\n    sensors=[job_success_sensor],\n)\n```\n\n----------------------------------------\n\nTITLE: Importing the UUID module in Python\nDESCRIPTION: This snippet imports the `uuid` module, which is necessary for generating UUIDs.  The `uuid` module provides functions for generating UUIDs in various forms.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster-webserver/dagster_webserver_tests/render_uuid_notebook.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport uuid\n```\n\n----------------------------------------\n\nTITLE: Loading Data into run_tags Table in PostgreSQL for Dagster\nDESCRIPTION: SQL COPY command to insert data into the run_tags table, which stores tags associated with Dagster pipeline runs. Each row contains an ID, run_id, key, and value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.run_tags (id, run_id, key, value) FROM stdin;\n6\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\tdagster/solid_selection\t*\n7\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\t.dagster/grpc_info\t{\"host\": \"localhost\", \"socket\": \"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuf74gv30\"}\n\\.\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for runs ID in PostgreSQL\nDESCRIPTION: Configures ID assignment for the 'runs' table to use 'runs_id_seq', facilitating seamless identifier management with default numeric ID generation for runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Enabling Compression for Shuffle Output Files - Shuffle Settings\nDESCRIPTION: This setting specifies whether to compress map output files, typically guided by the compression codec defined in 'spark.io.compression.codec'. Compression generally improves performance and reduces disk usage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_16\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.compress\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record JSON Structure\nDESCRIPTION: JSON structure representing a Dagster pipeline event record with materialization and expectation results. Shows events for processing tables including pages, fans, and event_admins with row count validations and file path materializations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepExpectationResultData\",\n      \"expectation_result\": {\n        \"__class__\": \"ExpectationResult\",\n        \"description\": \"Row count passed for pages\",\n        \"label\": \"pages.row_count\",\n        \"metadata_entries\": [],\n        \"success\": true\n      }\n    },\n    \"event_type_value\": \"STEP_EXPECTATION_RESULT\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"many_materializations_and_passing_expectations\",\n      \"solid_definition\": \"many_materializations_and_passing_expectations\",\n      \"step_key\": \"many_materializations_and_passing_expectations.compute\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Step Input Event in Dagster Pipeline\nDESCRIPTION: Records a step input event for the 'total' solid in the 'sleepy_pipeline'. It logs the receipt of input 'in_3' of type 'Int' and confirms that the type check passed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepInputData\",\n      \"input_name\": \"in_3\",\n      \"type_check_data\": {\n        \"__class__\": \"TypeCheckData\",\n        \"description\": null,\n        \"label\": \"in_3\",\n        \"metadata_entries\": [],\n        \"success\": true\n      }\n    },\n    \"event_type_value\": \"STEP_INPUT\",\n    \"logging_tags\": {\n      \"pipeline\": \"sleepy_pipeline\",\n      \"solid\": \"total\",\n      \"solid_definition\": \"total\",\n      \"step_key\": \"total.compute\"\n    },\n    \"message\": \"Got input \\\"in_3\\\" of type \\\"Int\\\". (Type check passed).\",\n    \"pipeline_name\": \"sleepy_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"total\",\n      \"name\": \"total\",\n      \"parent\": null\n    },\n    \"step_key\": \"total.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - STEP_INPUT - Got input \\\"in_3\\\" of type \\\"Int\\\". (Type check passed).\\n event_specific_data = {\\\"input_name\\\": \\\"in_3\\\", \\\"type_check_data\\\": [true, \\\"in_3\\\", null, []]}\\n               solid = \\\"total\\\"\\n    solid_definition = \\\"total\\\"\\n            step_key = \\\"total.compute\\\"\",\n  \"pipeline_name\": \"sleepy_pipeline\",\n  \"run_id\": \"ca7f1e33-526d-4f75-9bc5-3e98da41ab97\",\n  \"step_key\": \"total.compute\",\n  \"timestamp\": 1586206657.194197,\n  \"user_message\": \"Got input \\\"in_3\\\" of type \\\"Int\\\". (Type check passed).\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Step Output\nDESCRIPTION: JSON event record showing the output of a step execution with type checking information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"persist_costs\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\"}}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Object Store Operation (GET) Event in JSON\nDESCRIPTION: JSON record of an OBJECT_STORE_OPERATION (GET_OBJECT) event in Dagster. This record shows the retrieval of an intermediate object from the memory store for the 'do_input.compute' step's input.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_something.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_something.compute/result\"}, \"label\": \"key\"}], \"op\": \"GET_OBJECT\", \"value_name\": \"x\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Retrieved intermediate object for input x in memory object store using pickle.\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input x in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.620257, \"user_message\": \"Retrieved intermediate object for input x in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Copying Data into event_logs Table - SQL\nDESCRIPTION: This snippet is used to insert existing event logs into the 'event_logs' table, preserving historical logging information necessary for audits and debugging.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\n-- Data for Name: event_logs; Type: TABLE DATA; Schema: public; Owner: test\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key) FROM stdin;\n129\tca7f1e33-526d-4f75-9bc5-3e98da41ab97\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"dagit_subprocess_init\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"[SubprocessExecutionManager] About to start process for pipeline \\\"sleepy_pipeline\\\" (run_id: ca7f1e33-526d-4f75-9bc5-3e98da41ab97).\", \"pipeline_name\": \"sleepy_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"[SubprocessExecutionManager] About to start process for pipeline \\\"sleepy_pipeline\\\" (run_id: ca7f1e33-526d-4f75-9bc5-3e98da41ab97).\", \"pipeline_name\": \"sleepy_pipeline\", \"run_id\": \"ca7f1e33-526d-4f75-9bc5-3e98da41ab97\", \"step_key\": null, \"timestamp\": 1586206645.187435, \"user_message\": \"[SubprocessExecutionManager] About to start process for pipeline \\\"sleepy_pipeline\\\" (run_id: ca7f1e33-526d-4f75-9bc5-3e98da41ab97).\"}\tENGINE_EVENT\t2020-04-06 13:57:25.187435\t\\N\n...\n\n```\n\n----------------------------------------\n\nTITLE: Importing AirflowInstance in Python\nDESCRIPTION: Imports the AirflowInstance class from the dagster_airlift.core module. This class is likely used to represent and interact with an Airflow instance within Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-airlift.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_airlift.core import AirflowInstance\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: asset_keys_id_seq in PostgreSQL\nDESCRIPTION: Creates a sequence 'asset_keys_id_seq' for generating unique IDs for 'asset_keys' table's 'id' column. The sequence starts at 1, with an increment of 1, ensuring unique asset key identifiers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n```\n\n----------------------------------------\n\nTITLE: Inserting Initial Data into alembic_version Table in PostgreSQL\nDESCRIPTION: Inserts a version number into the alembic_version table to track the database schema version. This is used for database migration management with Alembic.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\nddcc6d7244c6\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating runs_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'runs_id_seq' sequence for generating unique IDs for entries in the 'runs' table, ensuring that each run is distinctly identified.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: runs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster EventLogEntry for Step Success\nDESCRIPTION: This snippet is a Dagster EventLogEntry indicating the successful completion of a step. The 'STEP_SUCCESS' event shows that the step 'add_four.emit_two.emit_one_2' finished execution in 58ms. It provides information about the execution time and step details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 58.821823000000606}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"emit_one_2\", \"step_key\": \"add_four.emit_two.emit_one_2\"}, \"message\": \"Finished execution of step \\\"add_four.emit_two.emit_one_2\\\" in 58ms.\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two.emit_one_2\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two.emit_one_2\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two.emit_one_2 - STEP_SUCCESS - Finished execution of step \\\"add_four.emit_two.emit_one_2\\\" in 58ms.\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two.emit_one_2\", \"timestamp\": 1640037521.494874, \"user_message\": \"Finished execution of step \\\"add_four.emit_two.emit_one_2\\\" in 58ms.\"\n```\n\n----------------------------------------\n\nTITLE: Run Tags Table Data Load in PostgreSQL\nDESCRIPTION: SQL statement to load data into the run_tags table in the public schema. The table appears to be empty as shown by the lack of records after the 'FROM stdin;' line. This table likely stores tags associated with pipeline runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_62\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.run_tags (id, run_id, key, value) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Multi-Asset Materialization in External Process\nDESCRIPTION: Demonstrates how to report multiple asset materializations from a single external process execution\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/using-dagster-pipes/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncontext.report_asset_materialization(\n    asset_key=[\"table_1\"],\n    description=\"First table materialized\"\n)\ncontext.report_asset_materialization(\n    asset_key=[\"table_2\"],\n    description=\"Second table materialized\"\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Database Schema Migration Error in Dagster Pipeline Execution\nDESCRIPTION: This snippet shows the error handling process when a Dagster pipeline fails due to an outdated database schema. It captures and logs detailed error information, including SQL queries and stack traces.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nFile \"/Users/prha/code/dagster/python_modules/dagster/dagster/core/storage/sql.py\", line 82, in handle_schema_errors\n    ) from None\n\n```\n\n----------------------------------------\n\nTITLE: Adding UNIQUE Constraint for bulk_actions Table in PostgreSQL\nDESCRIPTION: This command adds a unique constraint to the 'key' column in the 'bulk_actions' table, ensuring that all values are unique.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.bulk_actions ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n```\n\n----------------------------------------\n\nTITLE: Create normalized_cereals_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `normalized_cereals_id_seq` to generate unique IDs for the `normalized_cereals` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.normalized_cereals_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Starting the Dagster Webserver for GraphQL API Access\nDESCRIPTION: Command to start the Dagster development server which serves the GraphQL API at the /graphql endpoint.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/index.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for bulk_actions id Column\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `bulk_actions` table of the `public` schema, using a sequence for automatic ID generation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.bulk_actions ALTER COLUMN id SET DEFAULT nextval('public.bulk_actions_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Inserting Run Data for Dagster Pipeline\nDESCRIPTION: SQL code to insert run data for a completed Dagster pipeline execution. This includes the run ID, pipeline name, status, and a JSON payload with run details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_58\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.runs (id, run_id, pipeline_name, status, run_body, create_timestamp, update_timestamp) FROM stdin;\n1\t089287c5-964d-44c0-b727-357eb7ba522e\tmany_events\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"environment_dict\": {}, \"mode\": \"default\", \"pipeline_name\": \"many_events\", \"previous_run_id\": null, \"reexecution_config\": null, \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"selector\": {\"__class__\": \"ExecutionSelector\", \"name\": \"many_events\", \"solid_subset\": null}, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {}}\t2019-12-12 00:31:19.310745\t2019-12-11 16:31:24.068813\n\\.\n```\n\n----------------------------------------\n\nTITLE: Alter event_logs_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `event_logs_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.event_logs_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Dagster CLI Commands using reStructuredText\nDESCRIPTION: This snippet demonstrates how to document Dagster CLI commands using reStructuredText directives. It includes various commands such as asset, debug, definitions, dev, instance, job, run, schedule, sensor, project, graphql, webserver, and daemon commands.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/cli.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: dagster\n\n.. click:: dagster._cli.asset:asset_cli\n   :prog: dagster asset\n   :nested:\n\n.. click:: dagster._cli.debug:debug_cli\n   :prog: dagster debug\n   :nested:\n\n.. click:: dagster._cli.definitions:definitions_validate_command\n   :prog: dagster definitions validate\n   :nested:\n\n.. click:: dagster._cli.dev:dev_command\n   :prog: dagster dev\n   :nested:\n\n.. click:: dagster._cli.instance:instance_cli\n   :prog: dagster instance\n   :nested:\n\n.. click:: dagster._cli.job:job_cli\n   :prog: dagster job\n   :nested:\n\n.. click:: dagster._cli.run:run_cli\n   :prog: dagster run\n   :nested:\n\n.. click:: dagster._cli.schedule:schedule_cli\n   :prog: dagster schedule\n   :nested:\n\n.. click:: dagster._cli.sensor:sensor_cli\n   :prog: dagster sensor\n   :nested:\n\n.. click:: dagster._cli.project:project_cli\n   :prog: dagster project\n   :nested:\n\n.. currentmodule:: dagster_graphql\n\n.. click:: dagster_graphql.cli:cli\n   :prog: dagster-graphql\n\n.. currentmodule:: dagster_webserver\n\n.. click:: dagster_webserver.cli:cli\n   :prog: dagster-webserver\n\n.. currentmodule:: dagster\n\n.. click:: dagster._daemon.cli:run_command\n   :prog: dagster-daemon run\n   :nested:\n\n.. click:: dagster._daemon.cli:wipe_command\n   :prog: dagster-daemon wipe\n\n.. click:: dagster._daemon.cli:debug_heartbeat_dump_command\n   :prog: dagster-daemon debug heartbeat-dump\n\n.. click:: dagster._cli.api:grpc_command\n   :prog: dagster api grpc\n```\n\n----------------------------------------\n\nTITLE: Creating alembic_version Table - SQL\nDESCRIPTION: This snippet creates the 'alembic_version' table in the 'public' schema to track version numbers of database migrations. It uses a character varying type for the version number and sets it to not allow null values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: alembic_version; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\nALTER TABLE public.alembic_version OWNER TO test;\n\n```\n\n----------------------------------------\n\nTITLE: Creating Jobs Type Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the job_type column of the jobs table to optimize queries that filter by job type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_60\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint Between run_tags and runs Tables with Cascade Delete\nDESCRIPTION: SQL command to add a foreign key constraint to the run_tags table referencing the runs table with CASCADE DELETE behavior for automatic deletion of tags when a run is deleted.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install Dagster and DLT dependencies using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dlt.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-dlt\n```\n\n----------------------------------------\n\nTITLE: Step Success Logging with Dagster in JSON\nDESCRIPTION: This JSON snippet represents a STEP_SUCCESS event within the Dagster pipeline, including details on the successful execution duration of a particular computation step. Essential for audit trails and performance monitoring of pipeline executions, it requires a running Dagster pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 7028.4497769898735}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_costs\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_costs\"}, \"message\": \"Finished execution of step \\\"persist_costs\\\" in 7.03s.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_costs\", \"parent\": null}, \"step_key\": \"persist_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_costs - STEP_SUCCESS - Finished execution of step \\\"persist_costs\\\" in 7.03s.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_costs\", \"timestamp\": 1608666924.4421299, \"user_message\": \"Finished execution of step \\\"persist_costs\\\" in 7.03s.\"}\n```\n\n----------------------------------------\n\nTITLE: Using CLI to Scaffold a New Component Type in Dagster\nDESCRIPTION: Command-line example showing how to use the 'dg' utility to scaffold a new component type called shell_command.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/creating-new-component-types/creating-and-registering-a-component-type.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ dg scaffold component_type shell_command\nCreated my_component_library/lib/shell_command.py\n```\n\n----------------------------------------\n\nTITLE: Defining Order Statuses in dbt Documentation\nDESCRIPTION: A markdown table that documents the possible order statuses in a system. The table includes five statuses (placed, shipped, completed, return_pending, and returned) with descriptions explaining what each status represents in the order fulfillment process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_alias/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Event Logs Table in SQL\nDESCRIPTION: This snippet inserts multiple rows of data into the 'event_logs' table, including detailed event information and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key, asset_key, partition) FROM stdin;\n1673\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_STARTING\", \"logging_tags\": {}, \"message\": null, \"pid\": null, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": null, \"timestamp\": 1609894305.155692, \"user_message\": \"\"}\tPIPELINE_STARTING\t2021-01-06 00:51:45.155692\t\\N\t\\N\t\\N\n1674\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"66816\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 66816).\", \"pid\": null, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 66816).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": null, \"timestamp\": 1609894306.295252, \"user_message\": \"Started process for pipeline (pid: 66816).\"}\tENGINE_EVENT\t2021-01-06 00:51:46.295252\t\\N\t\\N\t\\N\n1675\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [object_manager].\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ENGINE_EVENT - Starting initialization of resources [object_manager].\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": null, \"timestamp\": 1609894307.16489, \"user_message\": \"Starting initialization of resources [object_manager].\"}\tENGINE_EVENT\t2021-01-06 00:51:47.16489\t\\N\t\\N\t\\N\n1676\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"resources\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": \"Initialized in 0.01ms\", \"entry_data\": {\"__class__\": \"PythonArtifactMetadataEntryData\", \"module\": \"dagster.core.storage.asset_store\", \"name\": \"InMemoryAssetStore\"}, \"label\": \"object_manager\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished initialization of resources [object_manager].\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ENGINE_EVENT - Finished initialization of resources [object_manager].\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": null, \"timestamp\": 1609894307.171674, \"user_message\": \"Finished initialization of resources [object_manager].\"}\tENGINE_EVENT\t2021-01-06 00:51:47.171674\t\\N\t\\N\t\\N\n1677\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\"longitudinal_pipeline\\\".\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - PIPELINE_START - Started execution of pipeline \\\"longitudinal_pipeline\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": null, \"timestamp\": 1609894307.178921, \"user_message\": \"Started execution of pipeline \\\"longitudinal_pipeline\\\".\"}\tPIPELINE_START\t2021-01-06 00:51:47.178921\t\\N\t\\N\t\\N\n1678\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"66816\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['ingest_costs', 'ingest_traffic', 'persist_costs', 'persist_traffic', 'build_cost_dashboard', 'build_model', 'build_traffic_dashboard', 'train_model', 'persist_model']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps in process (pid: 66816)\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ENGINE_EVENT - Executing steps in process (pid: 66816)\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": null, \"timestamp\": 1609894307.1997461, \"user_message\": \"Executing steps in process (pid: 66816)\"}\tENGINE_EVENT\t2021-01-06 00:51:47.199746\t\\N\t\\N\t\\N\n\\.\n```\n\n----------------------------------------\n\nTITLE: Alter event_logs Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `event_logs` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.event_logs OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering Dagster Components Preview in JSX/Markdown\nDESCRIPTION: This snippet imports a partial markdown file called _DgComponentsPreview.md and renders it within the page. The import and render pattern is using JSX syntax within a markdown document.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DgComponentsPreview from '@site/docs/partials/\\_DgComponentsPreview.md';\n\n<DgComponentsPreview />\n```\n\n----------------------------------------\n\nTITLE: Displaying SQL Configuration Settings in R\nDESCRIPTION: This R code snippet shows how to display the entire list of SQL configuration settings using SparkR session and SQL function.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_48\n\nLANGUAGE: r\nCODE:\n```\nsparkR.session()\nproperties <- sql(\"SET -v\")\nshowDF(properties, numRows = 200, truncate = FALSE)\n```\n\n----------------------------------------\n\nTITLE: Creating Event Log Table\nDESCRIPTION: Defines the 'event_log' table with an 'id', 'run_id', and 'event_body' as columns. It sets ownership and specifies 'id' as a non-null field without a default value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.event_log (\n    id bigint NOT NULL,\n    run_id character varying(255) NOT NULL,\n    event_body character varying NOT NULL\n);\n\nALTER TABLE public.event_log OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Create asset_keys_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `asset_keys_id_seq` to generate unique IDs for the `asset_keys` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Logging Asset Materialization Event in Dagster (JSON)\nDESCRIPTION: JSON log entry for the materialization of an asset in the 'asset_pipeline'. It includes details about the materialized asset, such as its key, partition, and associated tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepMaterializationData\",\n      \"asset_lineage\": [],\n      \"materialization\": {\n        \"__class__\": \"AssetMaterialization\",\n        \"asset_key\": {\n          \"__class__\": \"AssetKey\",\n          \"path\": [\"a\"]\n        },\n        \"description\": null,\n        \"metadata_entries\": [],\n        \"partition\": \"partition_1\",\n        \"tags\": {\"foo\": \"FOO\"}\n      }\n    },\n    \"event_type_value\": \"ASSET_MATERIALIZATION\",\n    \"logging_tags\": {\n      \"pipeline\": \"asset_pipeline\",\n      \"solid\": \"asset_solid\",\n      \"step_key\": \"asset_solid\"\n    },\n    \"message\": \"Materialized value a.\",\n    \"pid\": 5200,\n    \"pipeline_name\": \"asset_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"asset_solid\",\n      \"parent\": null\n    },\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"asset_solid\",\n        \"parent\": null\n      }\n    },\n    \"step_key\": \"asset_solid\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"asset_pipeline - 383f9411-4010-454a-9d31-d5a72ab57221 - 5200 - asset_solid - ASSET_MATERIALIZATION - Materialized value a.\",\n  \"pipeline_name\": \"asset_pipeline\",\n  \"run_id\": \"383f9411-4010-454a-9d31-d5a72ab57221\",\n  \"step_key\": \"asset_solid\",\n  \"timestamp\": 1625762114.606975,\n  \"user_message\": \"Materialized value a.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dagster Step Materialization Event Record\nDESCRIPTION: JSON event record showing materialization of 'raw_file_pages' step output with file path metadata\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/raw_file_pages.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_pages\", \"solid_definition\": \"raw_file_pages\", \"step_key\": \"raw_file_pages.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_pages\", \"name\": \"raw_file_pages\", \"parent\": null}, \"step_key\": \"raw_file_pages.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/raw_file_pages.raw\\\"]]]]}\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_pages.compute\", \"timestamp\": 1576110683.2661078, \"user_message\": \"Materialized value table_info.\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Package in Development Mode\nDESCRIPTION: Installs the current package in editable mode (-e flag) which creates a link to the source code instead of copying files. This allows for development changes to be immediately reflected without reinstallation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-f-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Copying Event Logs Data to PostgreSQL\nDESCRIPTION: SQL statement copying execution event logs into the event_logs table. Contains 8 event log entries for a Dagster pipeline execution, including pipeline starting, process initialization, step execution, and resource management events.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key, asset_key, partition) FROM stdin;\n1\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_STARTING\", \"logging_tags\": {}, \"message\": null, \"pid\": null, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": null, \"timestamp\": 1625607805.930971, \"user_message\": \"\"}\tPIPELINE_STARTING\t2021-07-06 21:43:25.930971\t\\N\t\\N\t\\N\n2\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"34000\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 34000).\", \"pid\": null, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 34000).\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": null, \"timestamp\": 1625607810.80894, \"user_message\": \"Started process for pipeline (pid: 34000).\"}\tENGINE_EVENT\t2021-07-06 21:43:30.80894\t\\N\t\\N\t\\N\n3\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\"single_mode\\\".\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - PIPELINE_START - Started execution of pipeline \\\"single_mode\\\".\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": null, \"timestamp\": 1625607811.299105, \"user_message\": \"Started execution of pipeline \\\"single_mode\\\".\"}\tPIPELINE_START\t2021-07-06 21:43:31.299105\t\\N\t\\N\t\\N\n4\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"34000\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['return_two']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps in process (pid: 34000)\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - ENGINE_EVENT - Executing steps in process (pid: 34000)\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": null, \"timestamp\": 1625607811.365226, \"user_message\": \"Executing steps in process (pid: 34000)\"}\tENGINE_EVENT\t2021-07-06 21:43:31.365226\t\\N\t\\N\t\\N\n5\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [io_manager].\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}}, \"step_key\": \"return_two\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - return_two - ENGINE_EVENT - Starting initialization of resources [io_manager].\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": \"return_two\", \"timestamp\": 1625607811.4210088, \"user_message\": \"Starting initialization of resources [io_manager].\"}\tENGINE_EVENT\t2021-07-06 21:43:31.421009\treturn_two\t\\N\t\\N\n6\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"resources\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": \"Initialized in 0.2ms\", \"entry_data\": {\"__class__\": \"PythonArtifactMetadataEntryData\", \"module\": \"dagster.core.storage.mem_io_manager\", \"name\": \"InMemoryIOManager\"}, \"label\": \"io_manager\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished initialization of resources [io_manager].\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}}, \"step_key\": \"return_two\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - return_two - ENGINE_EVENT - Finished initialization of resources [io_manager].\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": \"return_two\", \"timestamp\": 1625607811.4485571, \"user_message\": \"Finished initialization of resources [io_manager].\"}\tENGINE_EVENT\t2021-07-06 21:43:31.448557\treturn_two\t\\N\t\\N\n7\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ComputeLogsCaptureData\", \"log_key\": \"return_two\", \"step_keys\": [\"return_two\"]}, \"event_type_value\": \"LOGS_CAPTURED\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmpq7rk3t2y\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"single_mode\", \"solid\": \"return_two\", \"step_key\": \"return_two\"}, \"message\": \"Started capturing logs for solid: return_two.\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}}, \"step_key\": \"return_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - return_two - LOGS_CAPTURED - Started capturing logs for solid: return_two.\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": \"return_two\", \"timestamp\": 1625607811.526817, \"user_message\": \"Started capturing logs for solid: return_two.\"}\tLOGS_CAPTURED\t2021-07-06 21:43:31.526817\treturn_two\t\\N\t\\N\n8\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmpq7rk3t2y\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"single_mode\", \"solid\": \"return_two\", \"step_key\": \"return_two\"}, \"message\": \"Started execution of step \\\"return_two\\\".\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}}, \"step_key\": \"return_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - return_two - STEP_START - Started execution of step \\\"return_two\\\".\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": \"return_two\", \"timestamp\": 1625607811.543516, \"user_message\": \"Started execution of step \\\"return_two\\\".\"}\tSTEP_START\t2021-07-06 21:43:31.543516\treturn_two\t\\N\t\\N\n```\n\n----------------------------------------\n\nTITLE: Creating Snapshots ID Sequence in PostgreSQL\nDESCRIPTION: Defines an auto-incrementing sequence for the snapshots table's ID column with default start and increment values\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n```\n\n----------------------------------------\n\nTITLE: Step Start Log for Ingest Traffic in Dagster\nDESCRIPTION: A JSON log entry indicating the start of execution for the 'ingest_traffic' step in the Dagster pipeline. This log marks the beginning of the step's compute process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_56\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_traffic\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_traffic\"}, \"message\": \"Started execution of step \\\"ingest_traffic\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_traffic\", \"parent\": null}, \"step_key\": \"ingest_traffic\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - ingest_traffic - STEP_START - Started execution of step \\\"ingest_traffic\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"ingest_traffic\", \"timestamp\": 1608666908.144228, \"user_message\": \"Started execution of step \\\"ingest_traffic\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for instigators_id_seq in PostgreSQL\nDESCRIPTION: Resets the 'instigators_id_seq' to 1 for assigning new IDs, ensuring that the sequence starts with this value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.instigators_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log Entry - Logs Captured\nDESCRIPTION: Event log entry indicating start of log capture for the 'int_to_float' step\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ComputeLogsCaptureData\", \"log_key\": \"int_to_float\", \"step_keys\": [\"int_to_float\"]}, \"event_type_value\": \"LOGS_CAPTURED\"}}\n```\n\n----------------------------------------\n\nTITLE: Configuring IntSource in Dagster Python\nDESCRIPTION: Shows how to use IntSource for configuring an operation with either an integer literal or an environment variable selector for integer values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/config.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op, IntSource\n\n@op(config_schema=IntSource)\ndef secret_int_op(context) -> int:\n    return context.op_config\n\n@job\ndef secret_job():\n    secret_int_op()\n\nsecret_job.execute_in_process(\n    run_config={\n        'ops': {'secret_int_op': {'config': 1234}}\n    }\n)\n\nsecret_job.execute_in_process(\n    run_config={\n        'ops': {'secret_int_op': {'config': {'env': 'VERY_SECRET_ENV_VARIABLE_INT'}}}\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Airflow Configuration Constants in Python\nDESCRIPTION: Sets three numerical constants that define the number of DAGs, tasks, and assets per task for an Airflow configuration. These values are likely used elsewhere in the codebase to generate or configure Airflow DAG structures.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-airlift/perf-harness/perf_harness/shared/constants.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nNUM_DAGS 1\nNUM_TASKS 1\nNUM_ASSETS_PER_TASK 1\n```\n\n----------------------------------------\n\nTITLE: Steps Execution Engine Event in Dagster (JSON)\nDESCRIPTION: JSON log entry indicating that Dagster is executing pipeline steps in a process, including metadata about which step keys are being executed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_53\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"80827\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['do_something.compute', 'do_input.compute']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps in process (pid: 80827)\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - ENGINE_EVENT - Executing steps in process (pid: 80827)\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.511117, \"user_message\": \"Executing steps in process (pid: 80827)\"}\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DgComponentsPreview in Markdown\nDESCRIPTION: This snippet imports a custom React component called DgComponentsPreview and renders it within the Markdown document. It's likely used to display a preview of Dagster components for troubleshooting purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/troubleshooting-components.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport DgComponentsPreview from '@site/docs/partials/\\_DgComponentsPreview.md';\n\n<DgComponentsPreview />\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Snowflake Insights Integration\nDESCRIPTION: Command to install the necessary Python packages for integrating Dagster with Snowflake and enabling Insights tracking. These packages provide the core Dagster functionality, Dagster+ cloud features, and Snowflake connectivity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/snowflake.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-cloud dagster-snowflake\n```\n\n----------------------------------------\n\nTITLE: Importing and Using Markdown Partials in Docusaurus\nDESCRIPTION: Demonstrates how to import and include a markdown partial file as an HTML tag in Docusaurus documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nimport Deprecated from '@site/docs/partials/_Deprecated.md';\n\n<Deprecated />\n```\n\n----------------------------------------\n\nTITLE: Running the Dagster Documentation Website Locally\nDESCRIPTION: Commands to install dependencies and start the Dagster documentation website locally for development and testing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncd docs\nyarn install && yarn start\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for secondary_indexes_id_seq in PostgreSQL\nDESCRIPTION: Resets the sequence 'secondary_indexes_id_seq' to 8 for consistent indexing as new entries are added.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 8, true);\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring snapshots_id_seq Sequence in PostgreSQL\nDESCRIPTION: Creates a sequence named snapshots_id_seq that starts at 1 and increments by 1. The sequence is then altered to be owned by the test user and associated with the snapshots.id column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in Markdown\nDESCRIPTION: Imports the DocCardList component from the theme and renders it in an MDX documentation page. This is used to display a list of documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/index.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Code locations\nsidebar_position: 30\n---\n```\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Cereal Nutrition Dataset in CSV Format\nDESCRIPTION: A comprehensive dataset containing nutritional information for various breakfast cereals. The data includes metrics like calories, protein, fat, sodium content, carbohydrates, sugars, and other nutritional values for different cereal brands from manufacturers denoted by single-letter codes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_54\n\nLANGUAGE: CSV\nCODE:\n```\nBran Chex\tR\tC\t90\t2\t1\t200\t4\t15\t6\t125\t25\t1\t1\t0.67\t49.120253\nBran Flakes\tP\tC\t90\t3\t0\t210\t5\t13\t5\t190\t25\t3\t1\t0.67\t53.313813\nCap'n'Crunch\tQ\tC\t120\t1\t2\t220\t0\t12\t12\t35\t25\t2\t1\t0.75\t18.042851\nCheerios\tG\tC\t110\t6\t2\t290\t2\t17\t1\t105\t25\t1\t1\t1.25\t50.764999\nCinnamon Toast Crunch\tG\tC\t120\t1\t3\t210\t0\t13\t9\t45\t25\t2\t1\t0.75\t19.823573\nClusters\tG\tC\t110\t3\t2\t140\t2\t13\t7\t105\t25\t3\t1\t0.5\t40.400208\nCocoa Puffs\tG\tC\t110\t1\t1\t180\t0\t12\t13\t55\t25\t2\t1\t1\t22.736446\nCorn Chex\tR\tC\t110\t2\t0\t280\t0\t22\t3\t25\t25\t1\t1\t1\t41.445019\nCorn Flakes\tK\tC\t100\t2\t0\t290\t1\t21\t2\t35\t25\t1\t1\t1\t45.863324\nCorn Pops\tK\tC\t110\t1\t0\t90\t1\t13\t12\t20\t25\t2\t1\t1\t35.782791\nCount Chocula\tG\tC\t110\t1\t1\t180\t0\t12\t13\t65\t25\t2\t1\t1\t22.396513\nCracklin' Oat Bran\tK\tC\t110\t3\t3\t140\t4\t10\t7\t160\t25\t3\t1\t0.5\t40.448772\nCream of Wheat (Quick)\tN\tH\t100\t3\t0\t80\t1\t21\t0\t-1\t0\t2\t1\t1\t64.533816\nCrispix\tK\tC\t110\t2\t0\t220\t1\t21\t3\t30\t25\t3\t1\t1\t46.895644\nCrispy Wheat & Raisins\tG\tC\t100\t2\t1\t140\t2\t11\t10\t120\t25\t3\t1\t0.75\t36.176196\nDouble Chex\tR\tC\t100\t2\t0\t190\t1\t18\t5\t80\t25\t3\t1\t0.75\t44.330856\nFroot Loops\tK\tC\t110\t2\t1\t125\t1\t11\t13\t30\t25\t2\t1\t1\t32.207582\nFrosted Flakes\tK\tC\t110\t1\t0\t200\t1\t14\t11\t25\t25\t1\t1\t0.75\t31.435973\nFrosted Mini-Wheats\tK\tC\t100\t3\t0\t0\t3\t14\t7\t100\t25\t2\t1\t0.8\t58.345141\nFruit & Fibre Dates; Walnuts; and Oats\tP\tC\t120\t3\t2\t160\t5\t12\t10\t200\t25\t3\t1.25\t0.67\t40.917047\nFruitful Bran\tK\tC\t120\t3\t0\t240\t5\t14\t12\t190\t25\t3\t1.33\t0.67\t41.015492\nFruity Pebbles\tP\tC\t110\t1\t1\t135\t0\t13\t12\t25\t25\t2\t1\t0.75\t28.025765\nGolden Crisp\tP\tC\t100\t2\t0\t45\t0\t11\t15\t40\t25\t1\t1\t0.88\t35.252444\nGolden Grahams\tG\tC\t110\t1\t1\t280\t0\t15\t9\t45\t25\t2\t1\t0.75\t23.804043\nGrape Nuts Flakes\tP\tC\t100\t3\t1\t140\t3\t15\t5\t85\t25\t3\t1\t0.88\t52.076897\nGrape-Nuts\tP\tC\t110\t3\t0\t170\t3\t17\t3\t90\t25\t3\t1\t0.25\t53.371007\nGreat Grains Pecan\tP\tC\t120\t3\t3\t75\t3\t13\t4\t100\t25\t3\t1\t0.33\t45.811716\nHoney Graham Ohs\tQ\tC\t120\t1\t2\t220\t1\t12\t11\t45\t25\t2\t1\t1\t21.871292\nHoney Nut Cheerios\tG\tC\t110\t3\t1\t250\t1.5\t11.5\t10\t90\t25\t1\t1\t0.75\t31.072217\nHoney-comb\tP\tC\t110\t1\t0\t180\t0\t14\t11\t35\t25\t1\t1\t1.33\t28.742414\nJust Right Crunchy  Nuggets\tK\tC\t110\t2\t1\t170\t1\t17\t6\t60\t100\t3\t1\t1\t36.523683\nJust Right Fruit & Nut\tK\tC\t140\t3\t1\t170\t2\t20\t9\t95\t100\t3\t1.3\t0.75\t36.471512\nKix\tG\tC\t110\t2\t1\t260\t0\t21\t3\t40\t25\t2\t1\t1.5\t39.241114\nLife\tQ\tC\t100\t4\t2\t150\t2\t12\t6\t95\t25\t2\t1\t0.67\t45.328074\nLucky Charms\tG\tC\t110\t2\t1\t180\t0\t12\t12\t55\t25\t2\t1\t1\t26.734515\nMaypo\tA\tH\t100\t4\t1\t0\t0\t16\t3\t95\t25\t2\t1\t1\t54.850917\nMuesli Raisins; Dates; & Almonds\tR\tC\t150\t4\t3\t95\t3\t16\t11\t170\t25\t3\t1\t1\t37.136863\nMuesli Raisins; Peaches; & Pecans\tR\tC\t150\t4\t3\t150\t3\t16\t11\t170\t25\t3\t1\t1\t34.139765\nMueslix Crispy Blend\tK\tC\t160\t3\t2\t150\t3\t17\t13\t160\t25\t3\t1.5\t0.67\t30.313351\nMulti-Grain Cheerios\tG\tC\t100\t2\t1\t220\t2\t15\t6\t90\t25\t1\t1\t1\t40.105965\nNut&Honey Crunch\tK\tC\t120\t2\t1\t190\t0\t15\t9\t40\t25\t2\t1\t0.67\t29.924285\nNutri-Grain Almond-Raisin\tK\tC\t140\t3\t2\t220\t3\t21\t7\t130\t25\t3\t1.33\t0.67\t40.69232\nNutri-grain Wheat\tK\tC\t90\t3\t0\t170\t3\t18\t2\t90\t25\t3\t1\t1\t59.642837\nOatmeal Raisin Crisp\tG\tC\t130\t3\t2\t170\t1.5\t13.5\t10\t120\t25\t3\t1.25\t0.5\t30.450843\nPost Nat. Raisin Bran\tP\tC\t120\t3\t1\t200\t6\t11\t14\t260\t25\t3\t1.33\t0.67\t37.840594\nProduct 19\tK\tC\t100\t3\t0\t320\t1\t20\t3\t45\t100\t3\t1\t1\t41.50354\nPuffed Rice\tQ\tC\t50\t1\t0\t0\t0\t13\t0\t15\t0\t3\t0.5\t1\t60.756112\nPuffed Wheat\tQ\tC\t50\t2\t0\t0\t1\t10\t0\t50\t0\t3\t0.5\t1\t63.005645\nQuaker Oat Squares\tQ\tC\t100\t4\t1\t135\t2\t14\t6\t110\t25\t3\t1\t0.5\t49.511874\nQuaker Oatmeal\tQ\tH\t100\t5\t2\t0\t2.7\t-1\t-1\t110\t0\t1\t1\t0.67\t50.828392\nRaisin Bran\tK\tC\t120\t3\t1\t210\t5\t14\t12\t240\t25\t2\t1.33\t0.75\t39.259197\nRaisin Nut Bran\tG\tC\t100\t3\t2\t140\t2.5\t10.5\t8\t140\t25\t3\t1\t0.5\t39.7034\nRaisin Squares\tK\tC\t90\t2\t0\t0\t2\t15\t6\t110\t25\t3\t1\t0.5\t55.333142\nRice Chex\tR\tC\t110\t1\t0\t240\t0\t23\t2\t30\t25\t1\t1\t1.13\t41.998933\nRice Krispies\tK\tC\t110\t2\t0\t290\t0\t22\t3\t35\t25\t1\t1\t1\t40.560159\nShredded Wheat\tN\tC\t80\t2\t0\t0\t3\t16\t0\t95\t0\t1\t0.83\t1\t68.235885\nShredded Wheat 'n'Bran\tN\tC\t90\t3\t0\t0\t4\t19\t0\t140\t0\t1\t1\t0.67\t74.472949\nShredded Wheat spoon size\tN\tC\t90\t3\t0\t0\t3\t20\t0\t120\t0\t1\t1\t0.67\t72.801787\nSmacks\tK\tC\t110\t2\t1\t70\t1\t9\t15\t40\t25\t2\t1\t0.75\t31.230054\nSpecial K\tK\tC\t110\t6\t0\t230\t1\t16\t3\t55\t25\t1\t1\t1\t53.131324\nStrawberry Fruit Wheats\tN\tC\t90\t2\t0\t15\t3\t15\t5\t90\t25\t2\t1\t1\t59.363993\nTotal Corn Flakes\tG\tC\t110\t2\t1\t200\t0\t21\t3\t35\t100\t3\t1\t1\t38.839746\nTotal Raisin Bran\tG\tC\t140\t3\t1\t190\t4\t15\t14\t230\t100\t3\t1.5\t1\t28.592785\nTotal Whole Grain\tG\tC\t100\t3\t1\t200\t3\t16\t3\t110\t100\t3\t1\t1\t46.658844\nTriples\tG\tC\t110\t2\t1\t250\t0\t21\t3\t60\t25\t3\t1\t0.75\t39.106174\nTrix\tG\tC\t110\t1\t1\t140\t0\t13\t12\t25\t25\t2\t1\t1\t27.753301\nWheat Chex\tR\tC\t100\t3\t1\t230\t3\t17\t3\t115\t25\t1\t1\t0.67\t49.787445\nWheaties\tG\tC\t100\t3\t1\t200\t3\t17\t3\t110\t25\t1\t1\t1\t51.592193\nWheaties Honey Gold\tG\tC\t110\t2\t1\t200\t1\t16\t8\t60\t25\t1\t1\t0.75\t36.187559\n\\.\n```\n\n----------------------------------------\n\nTITLE: Examining Dagster Pipeline Process Exit Event in JSON\nDESCRIPTION: This JSON snippet shows a Dagster ENGINE_EVENT recording a process exit for a pipeline. It signals the termination of the process (PID: 80827) that was executing the pipeline after its completion.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 80827).\", \"pid\": null, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Process for pipeline exited (pid: 80827).\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.705453, \"user_message\": \"Process for pipeline exited (pid: 80827).\"}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster EventLogEntry in JSON\nDESCRIPTION: This snippet shows the structure of a Dagster EventLogEntry object serialized as JSON. It includes details about a handled output event for a step in the 'composition' pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"HandledOutputData\",\n      \"manager_key\": \"io_manager\",\n      \"metadata_entries\": [],\n      \"output_name\": \"result\"\n    },\n    \"event_type_value\": \"HANDLED_OUTPUT\",\n    \"logging_tags\": {\n      \"pipeline_name\": \"composition\",\n      \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\",\n      \"resource_fn_name\": \"None\",\n      \"resource_name\": \"None\",\n      \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n      \"solid_name\": \"div_two_2\",\n      \"step_key\": \"div_four.div_two_2\"\n    },\n    \"message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\",\n    \"pid\": 58212,\n    \"pipeline_name\": \"composition\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"div_two_2\",\n      \"parent\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"div_four\",\n        \"parent\": null\n      }\n    },\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"key\": \"div_four.div_two_2\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"div_two_2\",\n        \"parent\": {\n          \"__class__\": \"SolidHandle\",\n          \"name\": \"div_four\",\n          \"parent\": null\n        }\n      }\n    },\n    \"step_key\": \"div_four.div_two_2\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - div_four.div_two_2 - HANDLED_OUTPUT - Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\",\n  \"pipeline_name\": \"composition\",\n  \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n  \"step_key\": \"div_four.div_two_2\",\n  \"timestamp\": 1640037523.387864,\n  \"user_message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing DagsterEventRecord JSON for Step Start Event\nDESCRIPTION: JSON structure representing a DagsterEventRecord for a STEP_START event. This indicates the beginning of a computation step in a Dagster pipeline, containing metadata about the pipeline, solid, and step being executed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Started execution of step \\\"many_materializations_and_passing_expectations.compute\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"many_materializations_and_passing_expectations.compute\\\".\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.639885, \"user_message\": \"Started execution of step \\\"many_materializations_and_passing_expectations.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Code Block Syntax in Plain Text\nDESCRIPTION: This code snippet demonstrates how to format inline code and multi-line code blocks in plain text documentation. There are no dependencies as the snippet is written in plain text. Key parameters or lines include inline code within backticks and multi-line code encapsulated within indentation. Expected outcome is properly formatted code sections in documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster-test/dagster_test/toys/markdown_example.md#2025-04-22_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\n    // comments\n    line 1 of code\n    line 2 of code\n    line 3 of code\n```\n\n----------------------------------------\n\nTITLE: Defining Operations with Nothing Type in Python\nDESCRIPTION: Demonstrates how to use the 'Nothing' type in Dagster to establish execution dependencies without passing values. It shows examples of defining ops with 'Nothing' inputs and outputs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/types.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@op\ndef wait(_) -> Nothing:\n    time.sleep(1)\n    return\n\n@op(\n    ins={\"ready\": In(dagster_type=Nothing)},\n)\ndef done(_) -> str:\n    return 'done'\n\n@job\ndef nothing_job():\n    done(wait())\n\n# Any value will pass the type check for Nothing\n@op\ndef wait_int(_) -> Int:\n    time.sleep(1)\n    return 1\n\n@job\ndef nothing_int_job():\n    done(wait_int())\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Event Logs by Event Type in SQL\nDESCRIPTION: Creates a B-tree index on the event_logs table to optimize queries filtering by dagster_event_type and id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_event_type ON public.event_logs USING btree (dagster_event_type, id);\n```\n\n----------------------------------------\n\nTITLE: Creating Index for run_tags Table in SQL\nDESCRIPTION: Creates an index on the key and value columns of the run_tags table to improve query performance for tag-based searches.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n```\n\n----------------------------------------\n\nTITLE: SQL Data Loading for Dagster Event Records\nDESCRIPTION: Database insert statements containing event record data for a Dagster pipeline execution, including step input validation and logging information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_60\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.jobs (id, job_origin_id, repository_origin_id, status, job_type, job_body, create_timestamp, update_timestamp) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating Table: event_logs in PostgreSQL\nDESCRIPTION: Defines 'event_logs' table to record events related to runs and steps, including timestamps and event details. Useful for auditing and monitoring purposes within the system, with columns for event type and step information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\nALTER TABLE public.event_logs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Serializer in Configuration\nDESCRIPTION: Example of how to set a custom serializer in Spark configuration, specifically the KryoSerializer for improved performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_33\n\nLANGUAGE: java\nCODE:\n```\norg.apache.spark.serializer.KryoSerializer\n```\n\n----------------------------------------\n\nTITLE: Creating Run Tags Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'run_tags' table to store tags associated with Dagster runs. It includes columns for run identification and key-value pairs for tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.run_tags (\n    id bigint NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\n```\n\n----------------------------------------\n\nTITLE: Materializing table_info with raw_events metadata\nDESCRIPTION: Dagster event record documenting the materialization of a table_info value with metadata about raw_events including table name, path, data and markdown documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n// comments\nline 1 of code\nline 2 of code\nline 3 of code\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to snapshots Table\nDESCRIPTION: SQL command to add a primary key constraint to the snapshots table on the id column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Adding PRIMARY KEY Constraint for asset_keys Table in PostgreSQL\nDESCRIPTION: This SQL command establishes a primary key constraint on the 'id' column of 'asset_keys,' ensuring that it contains unique values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Create job_ticks_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `job_ticks_id_seq` to generate unique IDs for the `job_ticks` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.job_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Helm Chart for Local Development\nDESCRIPTION: A shell command to install the Dagster Helm chart on a local Kubernetes environment like kind or minikube. This command configures image repositories, tags, and pull policies for the Dagster webserver and job runner components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhelm install \\\n    --set dagsterWebserver.image.repository=\"dagster.io/buildkite-test-image\" \\\n    --set dagsterWebserver.image.tag=\"py310-latest\" \\\n    --set job_runner.image.repository=\"dagster.io/buildkite-test-image\" \\\n    --set job_runner.image.tag=\"py310-latest\" \\\n    --set imagePullPolicy=\"IfNotPresent\" \\\n    dagster \\\n    helm/dagster/\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint for Runs and Snapshots in PostgreSQL\nDESCRIPTION: Adds a foreign key constraint to the runs table, referencing the snapshot_id column in the snapshots table. This ensures referential integrity between runs and their associated snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_63\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Process Exited - JSON\nDESCRIPTION: This snippet logs when the 'error_monster' pipeline process has exited. It captures the process ID and provides a message confirming the process has concluded, which is essential for state monitoring.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster_tests/general_tests/compat_tests/dead_events.txt#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"event_specific_data\": {\"pipeline_name\": \"error_monster\", \"process_id\": 17633, \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\"}, \"event_type_value\": \"PIPELINE_PROCESS_EXITED\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 17633).\", \"pipeline_name\": \"error_monster\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"dagster_event\": {\"event_specific_data\": {\"pipeline_name\": \"error_monster\", \"process_id\": 17633, \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\"}, \"event_type_value\": \"PIPELINE_PROCESS_EXITED\", \"logging_tags\": {}, \"message\": \"Process for pipeline exited (pid: 17633).\", \"pipeline_name\": \"error_monster\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Process for pipeline exited (pid: 17633).\", \"pipeline_name\": \"error_monster\", \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\", \"step_key\": null, \"timestamp\": 1582148410.880915, \"user_message\": \"Process for pipeline exited (pid: 17633).\"}\n```\n\n----------------------------------------\n\nTITLE: Scaffolding a Dagster Project with Components\nDESCRIPTION: Command to scaffold a new Dagster project named 'jaffle-platform' with components support. This creates a new project structure and initializes a Python virtual environment using uv.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/creating-a-project-with-components.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ dg scaffold project jaffle-platform\nCreating project 'jaffle-platform' with a uv-managed environment.\nCreating project structure...\nSetting up uv environment...\nInstalling dependencies in uv environment...\n\nProject created! You can work with it using:\n\ncd jaffle-platform\ndg --help\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Step Output\nDESCRIPTION: Event record showing successful output from train_model step with type checking\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"train_model\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\"}}\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Module in Python\nDESCRIPTION: This snippet shows how to import the Dagster module, which is required for using Ops and related functionality.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/ops.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: dagster\n```\n\n----------------------------------------\n\nTITLE: Alter asset_keys_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `asset_keys_id_seq` sequence to the `id` column of the `asset_keys` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\"\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with schedule_ticks ID in PostgreSQL\nDESCRIPTION: Links 'schedule_ticks_id_seq' with the 'schedule_ticks.id', ensuring each new scheduling tick gets a unique ID automatically, maintaining database consistency.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.schedule_ticks_id_seq OWNED BY public.schedule_ticks.id;\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Snapshots in SQL\nDESCRIPTION: This snippet sets the default value for the ID column in the 'snapshots' table to be generated by the sequence 'public.snapshots_id_seq', ensuring new records receive a unique identifier.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: snapshots id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-datadog package\nDESCRIPTION: Command to install the dagster-datadog integration package using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/datadog.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-datadog\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Pipeline Run Configuration JSON in Python\nDESCRIPTION: JSON representation of a Dagster pipeline run configuration. This record contains metadata about the pipeline execution including snapshot IDs, origin information, run status, and configuration details for a successfully completed run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_61\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"53f52f323e0e20f0b8ec27a3d698335b3e0918fb\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": \"bar\", \"executable_path\": \"/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\", \"module_name\": null, \"package_name\": null, \"python_file\": \"/Users/dgibson/dagster/python_modules/dagit/dagit_tests/toy/bar_repo.py\", \"working_directory\": \"/Users/dgibson/dagster-home\"}, \"location_name\": \"bar_repo.py:bar\"}, \"repository_name\": \"bar\"}, \"pipeline_name\": \"foo\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"foo\", \"pipeline_snapshot_id\": \"47992fd61f2aecc862f741e1355e2b910d5850de\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\"}}\n```\n\n----------------------------------------\n\nTITLE: Materializing Cost Dashboard in Dagster Pipeline\nDESCRIPTION: JSON log event for a step materialization in a Dagster pipeline. The event captures the materialization of a cost dashboard asset with associated metadata including a URL reference to documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"dashboards\", \"cost_dashboard\"]}, \"description\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"UrlMetadataEntryData\", \"url\": \"http://docs.dagster.io/cost\"}, \"label\": \"build_cost_dashboard\"}], \"partition\": \"2020-12-08\"}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Materialized value dashboards cost_dashboard.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_cost_dashboard - STEP_MATERIALIZATION - Materialized value dashboards cost_dashboard.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1608666934.26312, \"user_message\": \"Materialized value dashboards cost_dashboard.\"}\n```\n\n----------------------------------------\n\nTITLE: Performing Numeric Summation\nDESCRIPTION: Iterates through the list and computes the cumulative sum of its elements. This snippet demonstrates a basic iterative addition operation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/reimport.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = 0\nfor i in l:\n    result = result + i\n```\n\n----------------------------------------\n\nTITLE: Inserting Initial Asset Data into asset_keys Table in PostgreSQL\nDESCRIPTION: Inserts asset key data into the asset_keys table, including JSON-structured metadata about materialization details, run ID, and timestamps for a model asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.asset_keys (id, asset_key, last_materialization, last_run_id, asset_details, create_timestamp) FROM stdin;\n27\t[\"model\"]\t{\"__class__\": \"AssetMaterialization\", \"asset_key\": {\"__class__\": \"AssetKey\", \"path\": [\"model\"]}, \"description\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"FloatMetadataEntryData\", \"value\": 1625760608.337679}, \"label\": \"timestamp\"}], \"partition\": null, \"tags\": {}}\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\t\\N\t2021-07-08 16:10:08.348777\n\\.\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record JSON Structure\nDESCRIPTION: JSON structure representing a Dagster pipeline event record containing metadata about step execution, object store operations, and logging information. Shows the standard format for pipeline execution events.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/build_cost_dashboard/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/build_cost_dashboard/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}}\n```\n\n----------------------------------------\n\nTITLE: Logging Resource Initialization in Dagster Pipeline (JSON)\nDESCRIPTION: JSON log entry for finishing the initialization of resources in a Dagster pipeline. It includes details about the IO manager resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"EngineEventData\",\n      \"error\": null,\n      \"marker_end\": \"resources\",\n      \"marker_start\": null,\n      \"metadata_entries\": [\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": \"Initialized in 0.01ms\",\n          \"entry_data\": {\n            \"__class__\": \"PythonArtifactMetadataEntryData\",\n            \"module\": \"dagster.core.storage.mem_io_manager\",\n            \"name\": \"InMemoryIOManager\"\n          },\n          \"label\": \"io_manager\"\n        }\n      ]\n    },\n    \"event_type_value\": \"ENGINE_EVENT\",\n    \"logging_tags\": {},\n    \"message\": \"Finished initialization of resources [io_manager].\",\n    \"pid\": 1295,\n    \"pipeline_name\": \"model_pipeline\",\n    \"solid_handle\": null,\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"materialization_solid\",\n        \"parent\": null\n      }\n    },\n    \"step_key\": \"materialization_solid\",\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"model_pipeline - 1399fa66-f129-46ad-9cf9-2d528d0f87fa - 1295 - materialization_solid - ENGINE_EVENT - Finished initialization of resources [io_manager].\",\n  \"pipeline_name\": \"model_pipeline\",\n  \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\",\n  \"step_key\": \"materialization_solid\",\n  \"timestamp\": 1625760608.296083,\n  \"user_message\": \"Finished initialization of resources [io_manager].\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Documentation Snapshot Tests with Tox\nDESCRIPTION: Command to execute the docs_snapshot_test environment in tox to validate documentation snippets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets_tests/snippet_checks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntox -e docs_snapshot_test\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Values in PostgreSQL\nDESCRIPTION: Sets sequence values for various tables including asset_keys, event_logs, job_ticks, jobs and others. Uses pg_catalog.setval to configure sequence starting points.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_63\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 5, true);\nSELECT pg_catalog.setval('public.event_logs_id_seq', 1672, true);\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\nSELECT pg_catalog.setval('public.jobs_id_seq', 1, false);\nSELECT pg_catalog.setval('public.normalized_cereals_id_seq', 77, true);\nSELECT pg_catalog.setval('public.run_tags_id_seq', 9, true);\nSELECT pg_catalog.setval('public.runs_id_seq', 4, true);\nSELECT pg_catalog.setval('public.schedule_ticks_id_seq', 1, false);\nSELECT pg_catalog.setval('public.schedules_id_seq', 1, false);\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 1, false);\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Secondary Indexes Table Data Load in PostgreSQL\nDESCRIPTION: SQL statement loading data into the secondary_indexes table in the public schema. This table contains one record for a run_partitions index created on February 12, 2021, with a completed migration timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_64\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.secondary_indexes (id, name, create_timestamp, migration_completed) FROM stdin;\n1\trun_partitions\t2021-02-12 01:00:31.236069\t2021-02-11 17:00:31.232932\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: dynamic_partitions_id_seq in PostgreSQL\nDESCRIPTION: Creates a sequence 'dynamic_partitions_id_seq' to auto-increment the 'id' field in 'dynamic_partitions'. Starts at 1 with an increment of 1, ensuring unique identification for each partition record.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.dynamic_partitions_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.dynamic_partitions_id_seq OWNER TO test;\nALTER SEQUENCE public.dynamic_partitions_id_seq OWNED BY public.dynamic_partitions.id;\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Materialization Event for Another Table in JSON\nDESCRIPTION: This snippet represents another Dagster event record for a step materialization, similar to the first one but for a table named 'raw_event_admins'. It includes metadata about the table such as its path, name, and a markdown description.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepMaterializationData\",\n      \"materialization\": {\n        \"__class__\": \"Materialization\",\n        \"description\": null,\n        \"label\": \"table_info\",\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"TextMetadataEntryData\",\n              \"text\": \"raw_event_admins\"\n            },\n            \"label\": \"table_name\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"PathMetadataEntryData\",\n              \"path\": \"/path/to/raw_event_admins\"\n            },\n            \"label\": \"table_path\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"JsonMetadataEntryData\",\n              \"data\": {\"name\": \"raw_event_admins\"}\n            },\n            \"label\": \"table_data\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"UrlMetadataEntryData\",\n              \"url\": \"https://bigty.pe/raw_event_admins\"\n            },\n            \"label\": \"table_name_big\"\n          },\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"MarkdownMetadataEntryData\",\n              \"md_str\": \"# h1 Heading :)\\n\\n## h2 Heading\\n\\n### h3 Heading\\n\\n#### h4 Heading\\n\\n##### h5 Heading\\n\\n###### h6 Heading\\n\\n## Horizontal Rules\\n\\n---\\n\\n## Blockquotes\\n\\n> Blockquote can be nested\\n>\\n> > indentation by arrow level\\n\\n## Unordered lists\\n\\n- One\\n- Two\\n  - Indented\\n- Three\\n\\n## Ordered lists\\n\\n1. One\\n1. Two\\n1. Three\\n\\n## Code\\n\\nInline `code`\\n\\n    // comments\\n    line 1 of code\\n    line 2 of code\\n    line 3 of code\\n\\n## Links\\n\\n[rick roll](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\\n\"\n            },\n            \"label\": \"table_blurb\"\n          }\n        ]\n      }\n    },\n    \"event_type_value\": \"STEP_MATERIALIZATION\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"many_table_materializations\",\n      \"solid_definition\": \"many_table_materializations\",\n      \"step_key\": \"many_table_materializations.compute\"\n    },\n    \"message\": \"Materialized value table_info.\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"many_table_materializations\",\n      \"name\": \"many_table_materializations\",\n      \"parent\": null\n    },\n    \"step_key\": \"many_table_materializations.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_name\\\", null, [\\\"raw_event_admins\\\"]], [\\\"table_path\\\", null, [\\\"/path/to/raw_event_admins\\\"]], [\\\"table_data\\\", null, [{\\\"name\\\": \\\"raw_event_admins\\\"}]], [\\\"table_name_big\\\", null, [\\\"https://bigty.pe/raw_event_admins\\\"]], [\\\"table_blurb\\\", null, [\\\"# h1 Heading :)\\\\n\\\\n## h2 Heading\\\\n\\\\n### h3 Heading\\\\n\\\\n#### h4 Heading\\\\n\\\\n##### h5 Heading\\\\n\\\\n###### h6 Heading\\\\n\\\\n## Horizontal Rules\\\\n\\\\n---\\\\n\\\\n## Blockquotes\\\\n\\\\n> Blockquote can be nested\\\\n>\\\\n> > indentation by arrow level\\\\n\\\\n## Unordered lists\\\\n\\\\n- One\\\\n- Two\\\\n  - Indented\\\\n- Three\\\\n\\\\n## Ordered lists\\\\n\\\\n1. One\\\\n1. Two\\\\n1. Three\\\\n\\\\n## Code\\\\n\\\\nInline `code`\\\\n\\\\n    // comments\\\\n    line 1 of code\\\\n    line 2 of code\\\\n    line 3 of code\\\\n\\\\n## Links\\\\n\\\\n[rick roll](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\\\\n\\\"]]]]}\n               solid = \\\"many_table_materializations\\\"\n    solid_definition = \\\"many_table_materializations\\\"\n            step_key = \\\"many_table_materializations.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"many_table_materializations.compute\",\n  \"timestamp\": 1576110683.5378811,\n  \"user_message\": \"Materialized value table_info.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Recording Step Success Event - Python\nDESCRIPTION: This snippet logs the successful completion of a computation step, detailing the duration of the execution. It provides feedback confirming the step's successful execution to the pipeline system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 16.441822052001953}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Finished execution of step \\\"do_input.compute\\\" in 16ms.\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_input.compute - STEP_SUCCESS - Finished execution of step \\\"do_input.compute\\\" in 16ms.\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466002.938373, \"user_message\": \"Finished execution of step \\\"do_input.compute\\\" in 16ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Upstream Asset PySpark Script\nDESCRIPTION: External PySpark script for the upstream asset implementation using Dagster Pipes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/migrating-from-step-launchers-to-pipes.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import create_client\nfrom pyspark.sql import SparkSession\n\nwith create_client() as client:\n    spark = SparkSession.builder.getOrCreate()\n    df = spark.createDataFrame([{\"a\": 1}, {\"a\": 2}])\n    path = \"/path/to/tmp\"\n    df.write.parquet(path)\n    client.log_event(\"path\", path)\n```\n\n----------------------------------------\n\nTITLE: Reviewing Dagster PIPELINE_STARTING Event in JSON\nDESCRIPTION: This JSON snippet shows a Dagster PIPELINE_STARTING event that signals the beginning of a pipeline execution. It contains minimal data as it's an initiating event but includes the pipeline name and run ID for tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_STARTING\", \"logging_tags\": {}, \"message\": null, \"pid\": null, \"pipeline_name\": \"basic_assets_job\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"basic_assets_job\", \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"step_key\": null, \"timestamp\": 1731664846.564075, \"user_message\": \"\"}\n```\n\n----------------------------------------\n\nTITLE: Creating and Modifying Snapshots Table in SQL\nDESCRIPTION: This snippet creates the 'snapshots' table with columns for id, snapshot_id, snapshot_body, and snapshot_type. It also sets the table owner and creates a sequence for the id column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE public.snapshots OWNER TO test;\n\nCREATE SEQUENCE public.snapshots_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.snapshots_id_seq OWNER TO test;\n\nALTER SEQUENCE public.snapshots_id_seq OWNED BY public.snapshots.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Alembic Version Table\nDESCRIPTION: Database schema table for tracking Alembic database migration versions\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence runs_id_seq in PostgreSQL\nDESCRIPTION: This sequence, 'runs_id_seq', auto-generates unique numeric IDs for the 'runs' table and manages key generation for each pipeline's record entry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Encoded Pipeline Configuration Data\nDESCRIPTION: Hex-encoded binary data representing pipeline configuration. The data appears to be compressed or encoded in a specific format, starting with patterns suggesting a binary/compressed structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: hex\nCODE:\n```\n9aa22d93bd6a8af62b3f76a4295ad06933b957e8b4cb218764643b2fdcca4dbc3d9b727577d1ead99336fcfb528a4d5fbd3f7850d4cdf5e2dbc65befd45f580bd41dee86ef1c97309dffed81d78edfdbc5f564ef335d66642d74b6f4f258ed77047eb9bb937d350b6791222f8705693de52b3e5cd2acdf49eecfa7cd973f60b24054ef33262db22b85c4e2324ff064f92325e02a882cccc4e93cb8b79672fa24d1ea75e1cb256ae1b0987ec5e531670b4730732a629cc4ff5ba6d70c6869201d625ef1a1894d519bbe537d056c2b3f2eb18c5bcbc0bba1dbca7be6b5b19cf2de0766cf172b364573f606f41570aefe4ac563f07cc48be1b5019d71df8768e7271f36c5b37969f90a34577ddce131586efc16776d244fd39538c2a475d819d937c5724a70359ead565a989e5f5cfd348dcc5da036cc68a3d2eabf0f99be94675360e6b173052e0fa4398f51b5c7bc925e1bd739ff9db0d61900cf652e3ea6d5b85819f879ce53262b1f97d3d6edb80fb978cca238bdcd2896c8d7943ae60e97b2e1e9acdd5468719a8fab4831d3f3e0a96cd1e262e1c155274140c06c020d621ad12c9178b7a613e31c06cf01966c5c49427dbd5fa89f7fc2294b668ad63cd25a1a8469ce1c8c5e9eeebea869478baf11cef385594e4d01664e332a7d07ce16fbea3d9606aa3a50058c7bc54d7d81accff4d2e9b1455e978f963d0d8b1d27c016d9ec3a22b60d565fd5bb2d868e536af7bb593ec6f6341ad3258407205960f6cb5d5f90ba29703e5ce82b8a8e2f2f4faea2bf5fbc9344e4e726f1cd829b9ec6da8c71f5649f84ff090d3e70b13248d65b72569239534d5a84eaca66a426caeaa9fba1e3f54078f781f08d995bae81297b9b7d5ba37e5eb69b6e565929d1cf4da32793e9d39e5bb82f841d6cf1e8d8cca1645664105c1ab437dbf8b320fa256f31735d71aa8257b922bd530ef3c3bc693bb7a3d928cfcab802bf3373860bfeb3e3760ff9ad6bd893ed0eee25d884987e2c9a483f77c5ed72d962d5aab3871581b7f56b5f26d844f5550cd6c9c023389cc7d316a62b0f9c34c9456d144dab68e6be6653168da0ff4755b2b6ec9732eb5791bf7c9177cc525ec5fe32c5fe24e1a463faf5aa5f2f53bf96c5fe72330835139f5ffe1f6255d519\n```\n\n----------------------------------------\n\nTITLE: Copying Data to Bulk Actions Table in PostgreSQL\nDESCRIPTION: SQL statement to copy bulk actions data into the bulk_actions table. This appears to be empty in the sample data as indicated by the absence of rows between the COPY statement and terminator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.bulk_actions (id, key, status, \"timestamp\", body) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for instigators and jobs Tables in SQL\nDESCRIPTION: Creates indexes on the instigator_type column of the instigators table and the job_type column of the jobs table to improve query performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_37\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX ix_instigators_instigator_type ON public.instigators USING btree (instigator_type);\n\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Configuring Celery Queue for Dagster Op\nDESCRIPTION: This Python code snippet demonstrates how to set a specific Celery queue for a Dagster op using tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/kubernetes-and-celery.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@op(\n  tags = {\n    'dagster-celery/queue': 'snowflake_queue',\n  }\n)\ndef my_op(context):\n  context.log.info('running')\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in MDX\nDESCRIPTION: Imports the DocCardList component from the theme to display a list of related documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/rbac/index.md#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Creating event_logs_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'event_logs_id_seq' sequence for generating unique identifiers for the 'event_logs' table. It ensures that each event log entry has a distinct ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: event_logs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Running Dagster-K8s Unit Tests\nDESCRIPTION: A simple shell command to run unit tests for the dagster-k8s library, excluding integration tests that require a Kubernetes cluster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npytest -m \"not integration\"\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: job_ticks_id_seq in PostgreSQL\nDESCRIPTION: Sets up 'job_ticks_id_seq' to auto-increment IDs for the 'job_ticks' table, ensuring unique identifiers for each job tick. It starts at 1 and increments by 1 for each new record.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.job_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.job_ticks_id_seq OWNER TO test;\nALTER SEQUENCE public.job_ticks_id_seq OWNED BY public.job_ticks.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Runs by Partition in SQL\nDESCRIPTION: Creates a B-tree index on the runs table to optimize queries filtering by partition_set and partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\n```\n\n----------------------------------------\n\nTITLE: Importing MySQL Resource and Storage Classes in Python\nDESCRIPTION: This snippet shows the import statements for MySQL-related classes in the dagster-mysql module. It includes a resource definition and storage classes for event logs, runs, and schedules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-mysql.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_mysql import MySQLResource, MySQLEventLogStorage, MySQLRunStorage, MySQLScheduleStorage\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint for Run Tags and Runs in PostgreSQL\nDESCRIPTION: Adds a foreign key constraint to the run_tags table, referencing the run_id column in the runs table. This ensures referential integrity between run tags and their associated runs, with cascading delete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_64\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Error: Missing Column in Asset Keys Table\nDESCRIPTION: SQL error showing a failed INSERT operation due to a missing column 'last_materialization_timestamp' in the 'asset_keys' table. This error indicates that the database schema is outdated and requires migration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO asset_keys (asset_key, last_materialization, last_run_id, last_materialization_timestamp, tags) VALUES (%(asset_key)s, %(last_materialization)s, %(last_run_id)s, %(last_materialization_timestamp)s, %(tags)s) ON CONFLICT (asset_key) DO UPDATE SET last_materialization = %(param_1)s, last_run_id = %(param_2)s, last_materialization_timestamp = %(param_3)s, tags = %(param_4)s RETURNING asset_keys.id\n```\n\n----------------------------------------\n\nTITLE: Altering Table Schema and Setting Default Values in PostgreSQL\nDESCRIPTION: These SQL commands modify table schemas by setting default values for ID columns using sequences. They target the 'secondary_indexes' and 'snapshots' tables in the public schema.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_58\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Create secondary_indexes_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `secondary_indexes_id_seq` to generate unique IDs for the `secondary_indexes` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_42\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Listing Separately Checked Dagster Examples\nDESCRIPTION: This section lists the Dagster examples that are checked separately from the main project. These examples likely have their own testing or validation processes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/exclude.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nexamples/assets_pandas_type_metadata\nexamples/project_fully_featured\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Run Tags by Run ID in SQL\nDESCRIPTION: Creates a B-tree index on the run_tags table to optimize queries filtering by run_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_tags_run_idx ON public.run_tags USING btree (run_id, id);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Intermediate Objects in Dagster Pipeline\nDESCRIPTION: JSON log entry documenting an object store operation where an intermediate object for input '_a' was retrieved from the filesystem using pickle for the 'build_model' step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_costs/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_costs/result\"}, \"label\": \"key\"}], \"op\": \"GET_OBJECT\", \"value_name\": \"_a\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Retrieved intermediate object for input _a in filesystem object store using pickle.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_model - OBJECT_STORE_OPERATION - Retrieved intermediate object for input _a in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_model\", \"timestamp\": 1608666934.3161442, \"user_message\": \"Retrieved intermediate object for input _a in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Table: kvs in PostgreSQL\nDESCRIPTION: Defines 'kvs', a simple key-value storage table, storing generic text keys and associated values. This table is useful for storing miscellaneous settings and configuration data within the system.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.kvs (\n    key text NOT NULL,\n    value text\n);\nALTER TABLE public.kvs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Setting Default Values for Multiple Tables in SQL\nDESCRIPTION: Sets default values for the 'id' column in various tables including asset_keys, event_logs, job_ticks, jobs, and others. The default values are set to use the next value from their respective sequences.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.asset_keys ALTER COLUMN id SET DEFAULT nextval('public.asset_keys_id_seq'::regclass);\n\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.jobs ALTER COLUMN id SET DEFAULT nextval('public.jobs_id_seq'::regclass);\n\nALTER TABLE ONLY public.normalized_cereals ALTER COLUMN id SET DEFAULT nextval('public.normalized_cereals_id_seq'::regclass);\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\nALTER TABLE ONLY public.schedule_ticks ALTER COLUMN id SET DEFAULT nextval('public.schedule_ticks_id_seq'::regclass);\n\nALTER TABLE ONLY public.schedules ALTER COLUMN id SET DEFAULT nextval('public.schedules_id_seq'::regclass);\n\nALTER TABLE ONLY public.secondary_indexes ALTER COLUMN id SET DEFAULT nextval('public.secondary_indexes_id_seq'::regclass);\n\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Local Webserver Preview Command\nDESCRIPTION: Bash command to start a local webserver for previewing Dagster use case changes before committing\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake webserver\n```\n\n----------------------------------------\n\nTITLE: Complete Dagster Kubernetes Pipes Integration Code\nDESCRIPTION: This is the complete Python file that defines the Dagster asset, PipesK8sClient resource, and Definitions for the Kubernetes Pipes integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/kubernetes-pipeline.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# dagster_k8s_pipes.py\n\nfrom dagster import AssetExecutionContext, Definitions, asset\nfrom dagster_k8s import PipesK8sClient\n\n\n@asset\ndef k8s_pipes_asset(context: AssetExecutionContext, k8s_pipes_client: PipesK8sClient):\n  return k8s_pipes_client.run(\n      context=context,\n      image=\"pipes-example:v1\",\n      extras={\n            \"some_parameter\": 1\n      }\n  ).get_materialize_result()\n\n\ndefs = Definitions(\n  assets=[k8s_pipes_asset],\n  resources={\n    \"k8s_pipes_client\": PipesK8sClient(),\n  },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Table: instance_info in PostgreSQL\nDESCRIPTION: Defines 'instance_info' table containing a single column to store instance information such as run storage IDs. This table is used for simple key-value storage of configuration or instance-related data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.instance_info (\n    run_storage_id text\n);\nALTER TABLE public.instance_info OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Cold Cereals View\nDESCRIPTION: Creates a view that filters and returns only cold cereals (type='C') from the sort_by_calories table with their nutritional information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nCREATE VIEW \"test-schema\".sort_cold_cereals_by_calories AS\n SELECT sort_by_calories.name,\n    sort_by_calories.mfr,\n    sort_by_calories.type,\n    sort_by_calories.calories,\n    sort_by_calories.protein,\n    sort_by_calories.fat,\n    sort_by_calories.sodium,\n    sort_by_calories.fiber,\n    sort_by_calories.carbo,\n    sort_by_calories.sugars,\n    sort_by_calories.potass,\n    sort_by_calories.vitamins,\n    sort_by_calories.shelf,\n    sort_by_calories.weight,\n    sort_by_calories.cups,\n    sort_by_calories.rating\n   FROM \"test-schema\".sort_by_calories\n  WHERE (sort_by_calories.type = 'C'::text);\n```\n\n----------------------------------------\n\nTITLE: Creating Index on bulk_actions.status\nDESCRIPTION: This SQL statement creates an index on the `status` column of the `bulk_actions` table.  This enhances query performance when filtering or sorting by bulk action status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\"\n```\n\n----------------------------------------\n\nTITLE: Legacy Azure Components Documentation\nDESCRIPTION: ReStructuredText documentation for legacy Azure components including ConfigurablePickledObjectADLS2IOManager and related resources.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-azure.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable::  dagster_azure.adls2.ConfigurablePickledObjectADLS2IOManager\n  :annotation: IOManagerDefinition\n\n.. autoconfigurable:: dagster_azure.adls2.adls2_resource\n  :annotation: ResourceDefinition\n\n.. autoconfigurable:: dagster_azure.adls2.adls2_pickle_io_manager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Values in PostgreSQL for Dagster Database\nDESCRIPTION: SQL commands to set sequence values for secondary_indexes_id_seq and snapshots_id_seq tables. Both sequences are set to start from value 2.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 2, true);\n\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Logging Pipeline Process Started - JSON\nDESCRIPTION: This snippet logs the event indicating that the 'error_monster' pipeline process has started successfully. It captures the process ID and relevant metadata useful for real-time monitoring of the running pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster_tests/general_tests/compat_tests/dead_events.txt#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"event_specific_data\": {\"pipeline_name\": \"error_monster\", \"process_id\": 17633, \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\"}, \"event_type_value\": \"PIPELINE_PROCESS_STARTED\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 17633).\", \"pipeline_name\": \"error_monster\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"dagster_event\": {\"event_specific_data\": {\"pipeline_name\": \"error_monster\", \"process_id\": 17633, \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\"}, \"event_type_value\": \"PIPELINE_PROCESS_STARTED\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 17633).\", \"pipeline_name\": \"error_monster\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 17633).\", \"pipeline_name\": \"error_monster\", \"run_id\": \"53909416-1450-409f-a360-79649d7f589f\", \"step_key\": null, \"timestamp\": 1582148405.9197319, \"user_message\": \"Started process for pipeline (pid: 17633).\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Event Logs Table in PostgreSQL\nDESCRIPTION: Defines a table to store event logs for Dagster pipeline execution with columns for run ID, event details, timestamp, and associated step/asset keys\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key character varying,\n    asset_key character varying\n);\n```\n\n----------------------------------------\n\nTITLE: Viewing Dagster-Cloud CLI Configuration\nDESCRIPTION: Displays the contents of the dagster-cloud CLI configuration file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-cloud-cli/installing-and-configuring.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ dagster-cloud config view\n\ndefault_deployment: prod\norganization: hooli\nuser_token: '*******************************8214fe'\n```\n\n----------------------------------------\n\nTITLE: Alter normalized_cereals Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `normalized_cereals` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.normalized_cereals OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Parsing DagsterEventRecord JSON for Step Expectation Result Event (Groups)\nDESCRIPTION: JSON structure representing a DagsterEventRecord for a STEP_EXPECTATION_RESULT event. This captures a successful row count expectation check for the groups table during a Dagster pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Row count passed for groups\", \"label\": \"groups.row_count\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Row count passed for groups\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Row count passed for groups\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"groups.row_count\\\", \\\"Row count passed for groups\\\", []]}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"timestamp\": 1576110683.700545, \"user_message\": \"Row count passed for groups\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Constraints for Dagster Tables\nDESCRIPTION: Adds unique constraints for run_id in runs table, repository_name/schedule_name pair in schedules table, and snapshot_id in snapshots table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.schedules\n    ADD CONSTRAINT schedules_repository_name_schedule_name_key UNIQUE (repository_name, schedule_name);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Installing LakeFS Client in Python Environment\nDESCRIPTION: Command to install the LakeFS Python client library via pip package manager\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/lakefs.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lakefs-client\n```\n\n----------------------------------------\n\nTITLE: Recording Step Output Event - Python\nDESCRIPTION: This snippet logs the successful yielding of an output from a computation step within the pipeline. It includes details about the output value and its type while confirming a successful type check.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"do_input.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - do_input.compute - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466002.923249, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Index on job_type Column in jobs Table\nDESCRIPTION: SQL command to create an index on the job_type column in the jobs table to optimize queries filtering by job type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster EventLogEntry for Handled Output\nDESCRIPTION: This snippet is a Dagster EventLogEntry indicating that an output was handled. The 'HANDLED_OUTPUT' event shows that the output 'result' from step 'add_four.emit_two.emit_one_2' was handled using the IO manager 'io_manager'. It provides details about the event and the IO manager used.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"HandledOutputData\", \"manager_key\": \"io_manager\", \"metadata_entries\": [], \"output_name\": \"result\"}, \"event_type_value\": \"HANDLED_OUTPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"emit_one_2\", \"step_key\": \"add_four.emit_two.emit_one_2\"}, \"message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"add_four.emit_two.emit_one_2\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_one_2\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"emit_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"add_four\", \"parent\": null}}}}, \"step_key\": \"add_four.emit_two.emit_one_2\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - add_four.emit_two.emit_one_2 - HANDLED_OUTPUT - Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"add_four.emit_two.emit_one_2\", \"timestamp\": 1640037521.463843, \"user_message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Project Dependencies\nDESCRIPTION: These commands navigate to the project directory and install the project dependencies, including development dependencies. The '-e' flag installs the project in editable mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/tutorial_notebook_assets/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd my-dagster-project\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Set Default Value for event_logs.id\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `event_logs` table to be generated by the `event_logs_id_seq` sequence. This ensures that new rows automatically get a unique ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\"\n```\n\n----------------------------------------\n\nTITLE: Creating Bulk Actions Indexes in PostgreSQL\nDESCRIPTION: Creates unique and non-unique indexes on the bulk_actions table for key, selector_id and status columns\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nCREATE UNIQUE INDEX idx_bulk_actions_key ON public.bulk_actions USING btree (key);\n\nCREATE INDEX idx_bulk_actions_selector_id ON public.bulk_actions USING btree (selector_id);\n\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js LTS Version with NVM\nDESCRIPTION: Command to install the long-term-support version of Node.js using NVM (Node Version Manager).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnvm install --lts\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Materialization Event for Raw File Friends Step\nDESCRIPTION: JSON representation of a DagsterEventRecord for a STEP_MATERIALIZATION event. This record shows the materialization of a 'table_info' value with a path metadata entry pointing to '/path/to/raw_file_friends.raw'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/raw_file_friends.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_friends\", \"solid_definition\": \"raw_file_friends\", \"step_key\": \"raw_file_friends.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_friends\", \"name\": \"raw_file_friends\", \"parent\": null}, \"step_key\": \"raw_file_friends.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/raw_file_friends.raw\\\"]]]]}}\\n               solid = \\\"raw_file_friends\\\"\\n    solid_definition = \\\"raw_file_friends\\\"\\n            step_key = \\\"raw_file_friends.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_friends.compute\", \"timestamp\": 1576110683.024043, \"user_message\": \"Materialized value table_info.\"}\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill in Python\nDESCRIPTION: This snippet imports the Dagstermill library, which is essential for working with Jupyter notebooks within a Dagster pipeline. It enables the use of Dagstermill functions in the script.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_config.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dagstermill\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for Dagster Database Tables\nDESCRIPTION: SQL commands to create indexes on various Dagster tables to improve query performance. Indexes are created for event_logs, bulk_actions, job_ticks, runs, and run_tags tables on frequently queried columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\n\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\n\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\n\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\n\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Creating Key-Value Store Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'kvs' table to implement a simple key-value store for Dagster. It includes columns for an ID, key, and value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.kvs (\n    id bigint NOT NULL,\n    key text NOT NULL,\n    value text\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Event Logs by Step Key in SQL\nDESCRIPTION: Creates a B-tree index on the event_logs table to optimize queries filtering by step_key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n```\n\n----------------------------------------\n\nTITLE: Recording Pipeline Success in Dagster\nDESCRIPTION: Logs the successful completion of the 'sleepy_pipeline' execution. This event marks the end of the pipeline run and indicates that all steps were completed without errors.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"PIPELINE_SUCCESS\",\n    \"logging_tags\": {},\n    \"message\": \"Finished execution of pipeline \\\"sleepy_pipeline\\\".\",\n    \"pipeline_name\": \"sleepy_pipeline\",\n    \"solid_handle\": null,\n    \"step_key\": null,\n    \"step_kind_value\": null\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - PIPELINE_SUCCESS - Finished execution of pipeline \\\"sleepy_pipeline\\\".\",\n  \"pipeline_name\": \"sleepy_pipeline\",\n  \"run_id\": \"ca7f1e33-526d-4f75-9bc5-3e98da41ab97\",\n  \"step_key\": null,\n  \"timestamp\": 1586206657.363752,\n  \"user_message\": \"Finished execution of pipeline \\\"sleepy_pipeline\\\".\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Types in Markdown\nDESCRIPTION: A markdown table that defines five possible order statuses (placed, shipped, completed, return_pending, returned) with detailed descriptions for each status state. This documentation is wrapped in a template tag structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_time_partition_freshness/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Dagster Project\nDESCRIPTION: Command to install the project and its Python dependencies in editable mode with development extras.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_modern_data_stack/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Dev Server\nDESCRIPTION: Command to start the Dagster development server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Existing Kind Cluster\nDESCRIPTION: Shell command to run pytest with a specified existing kind cluster, avoiding the creation of a new cluster for each test run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npytest --kind-cluster=kind-test\n```\n\n----------------------------------------\n\nTITLE: Standard BigQueryResource Implementation\nDESCRIPTION: Example of a standard BigQueryResource implementation without Insights tracking. This represents the configuration before adding Insights integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/insights/google-bigquery.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, asset\nfrom dagster_gcp import BigQueryResource\n\n@asset\ndef my_asset(bigquery):\n    result = bigquery.query(\"SELECT * FROM my_dataset.my_table LIMIT 10\")\n    # ... process results\n\ndefs = Definitions(\n    assets=[my_asset],\n    resources={\n        \"bigquery\": BigQueryResource(\n            project=\"my-gcp-project\",\n        )\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Dagster Asset Definitions Table\nDESCRIPTION: ASCII table output from Dagster CLI showing two assets: 'autoloaded_asset' and 'my_asset', both in the default group with no specified dependencies, kinds or descriptions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-project/10-list-defs.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg list defs\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Section ┃ Definitions                                                 ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ Assets  │ ┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┓ │\n│         │ ┃ Key              ┃ Group   ┃ Deps ┃ Kinds ┃ Description ┃ │\n│         │ ┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━┩ │\n│         │ │ autoloaded_asset │ default │      │       │             │ │\n│         │ ├──────────────────┼─────────┼──────┼───────┼─────────────┤ │\n│         │ │ my_asset         │ default │      │       │             │ │\n│         │ └──────────────────┴─────────┴──────┴───────┴─────────────┘ │\n└─────────┴─────────────────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Creating Secondary Indexes Table in PostgreSQL for Dagster\nDESCRIPTION: Creates a table named 'secondary_indexes' to store information about secondary indexes. The table includes columns for ID, name, creation timestamp, and migration completion timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying(512),\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster longitudinal_pipeline Run with SUCCESS Status\nDESCRIPTION: A JSON configuration for a successfully completed Dagster pipeline run. It includes the same solid configurations as the previous run but with a SUCCESS status and additional GRPC information in the tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_46\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"7293e986c3d0c85ea67e6ddfb211bfa85633fe67\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": null, \"executable_path\": \"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/bin/python3.6\", \"module_name\": \"dagster_test.toys.repo\", \"package_name\": null, \"python_file\": null, \"working_directory\": null}, \"location_name\": \"dagster_test.toys.repo\"}, \"repository_name\": \"toys_repository\"}, \"pipeline_name\": \"longitudinal_pipeline\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"longitudinal_pipeline\", \"pipeline_snapshot_id\": \"080905473a698ce74ecaadcf7cc3c91a3db65cb3\", \"root_run_id\": null, \"run_config\": {\"intermediate_storage\": {\"filesystem\": {}}, \"solids\": {\"build_cost_dashboard\": {\"config\": {\"materialization_key_list\": [\"dashboards\", \"cost_dashboard\"], \"materialization_url\": \"http://docs.dagster.io/cost\", \"partition\": \"2020-01-02\"}}, \"build_model\": {\"config\": {\"partition\": \"2020-01-02\", \"sleep\": 0.9459095295394656}}, \"build_traffic_dashboard\": {\"config\": {\"materialization_key_list\": [\"dashboards\", \"traffic_dashboard\"], \"materialization_url\": \"http://docs.dagster.io/traffic\", \"partition\": \"2020-01-02\"}}, \"ingest_costs\": {\"config\": {\"error_rate\": 0.11758427684728616, \"partition\": \"2020-01-02\", \"sleep\": 5.315783837199222}}, \"ingest_traffic\": {\"config\": {\"error_rate\": 0.0714843164718762, \"partition\": \"2020-01-02\", \"sleep\": 0.17794356552275575}}, \"persist_costs\": {\"config\": {\"error_rate\": 0.0007343012569762941, \"materialization_key\": \"cost_db_table\", \"materialization_value\": 5315.783837199221, \"partition\": \"2020-01-02\", \"sleep\": 5.315783837199222}}, \"persist_model\": {\"config\": {\"materialization_json\": {\"cost\": 5315.783837199221, \"traffic\": 177.94356552275576}, \"materialization_key\": \"model\", \"partition\": \"2020-01-02\"}}, \"persist_traffic\": {\"config\": {\"error_rate\": 0.009586646737069058, \"materialization_key\": \"traffic_db_table\", \"materialization_value\": 177.94356552275576, \"partition\": \"2020-01-02\", \"sleep\": 0.17794356552275575}}, \"train_model\": {\"config\": {\"partition\": \"2020-01-02\", \"sleep\": 0.9459095295394656}}}}, \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\"}}\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Graph Components in Python\nDESCRIPTION: This snippet shows the import statements for various Dagster graph-related classes and decorators. It includes GraphDefinition, GraphIn, GraphOut, and dependency-related classes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/graphs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import graph, GraphDefinition, GraphIn, GraphOut, DependencyDefinition, MultiDependencyDefinition, NodeInvocation, OutputMapping, InputMapping\n```\n\n----------------------------------------\n\nTITLE: Logging Engine Completion Event in Dagster Pipeline\nDESCRIPTION: JSON representation of an engine event in a Dagster pipeline. The event indicates that the process completed all steps in 275ms with information about the process ID and step keys executed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"34000\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['return_two']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished steps in process (pid: 34000) in 275ms\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - ENGINE_EVENT - Finished steps in process (pid: 34000) in 275ms\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": null, \"timestamp\": 1625607811.66014, \"user_message\": \"Finished steps in process (pid: 34000) in 275ms\"}\n```\n\n----------------------------------------\n\nTITLE: Dagster Raw File Users Step Event\nDESCRIPTION: JSON event record for the 'raw_file_users' processing step showing materialization with file path metadata\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/raw_file_users.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_users\", \"solid_definition\": \"raw_file_users\", \"step_key\": \"raw_file_users.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_users\", \"name\": \"raw_file_users\", \"parent\": null}, \"step_key\": \"raw_file_users.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/raw_file_users.raw\\\"]]]]}\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_users.compute\", \"timestamp\": 1576110683.353704, \"user_message\": \"Materialized value table_info.\"}\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in JSX\nDESCRIPTION: This snippet imports the DocCardList component from the theme and renders it within the page. It's likely used to generate a list of documentation cards related to operating Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Printing a Hello Message in Python\nDESCRIPTION: This snippet utilizes the print function to output 'hello' to the console. It serves as a basic example of generating output in Python. No dependencies are required for this operation, and the expected output is the string displayed in the console.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster_tests/api_tests/foo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"hello\")\n```\n\n----------------------------------------\n\nTITLE: Inserting Dagster Repository Data in SQL\nDESCRIPTION: SQL COPY statement to insert data into the 'public.dagster_repositories' table. It includes repository information and GRPC connection details.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.dagster_repositories (id, run_id, repository_name, repository_data) FROM stdin;\n10\t4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\t.dagster/repository\tbasic_assets_repository@dagster_test.toys.repo\n11\t4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\t.dagster/grpc_info\t{\"host\": \"localhost\", \"socket\": \"/var/folders/zt/9txyngt92fxgpd954jkfz6zm0000gn/T/tmpmc6skfzp\"}\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating Tables and Sequences in PostgreSQL\nDESCRIPTION: This SQL snippet provides commands for creating tables and sequences within a PostgreSQL database schema. It includes table creation for `alembic_version`, `asset_event_tags`, `asset_keys`, `bulk_actions`, `daemon_heartbeats`, `event_logs`, `instance_info`, `instigators`, `job_ticks`, `jobs`, `kvs`, `run_tags`, `runs`, and `secondary_indexes`. Each table is followed by a sequence setup to support auto-incrementing primary keys. The tables are set with appropriate data types and constraints to enforce data integrity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\n\nALTER TABLE public.alembic_version OWNER TO test;\n\nCREATE TABLE public.asset_event_tags (\n    id bigint NOT NULL,\n    event_id integer,\n    asset_key text NOT NULL,\n    key text NOT NULL,\n    value text,\n    event_timestamp timestamp without time zone\n);\n\nALTER TABLE public.asset_event_tags OWNER TO test;\n\nCREATE SEQUENCE public.asset_event_tags_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.asset_event_tags_id_seq OWNER TO test;\n\nALTER SEQUENCE public.asset_event_tags_id_seq OWNED BY public.asset_event_tags.id;\n\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    wipe_timestamp timestamp without time zone,\n    last_materialization_timestamp timestamp without time zone,\n    tags text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.asset_keys OWNER TO test;\n\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\n\nCREATE TABLE public.bulk_actions (\n    id integer NOT NULL,\n    key character varying(32) NOT NULL,\n    status character varying(255) NOT NULL,\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text,\n    action_type character varying(32),\n    selector_id text\n);\n\nALTER TABLE public.bulk_actions OWNER TO test;\n\nCREATE SEQUENCE public.bulk_actions_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.bulk_actions_id_seq OWNER TO test;\n\nALTER SEQUENCE public.bulk_actions_id_seq OWNED BY public.bulk_actions.id;\n\nCREATE TABLE public.daemon_heartbeats (\n    daemon_type character varying(255) NOT NULL,\n    daemon_id character varying(255),\n    \"timestamp\" timestamp without time zone NOT NULL,\n    body text\n);\n\nALTER TABLE public.daemon_heartbeats OWNER TO test;\n\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\n\nALTER TABLE public.event_logs OWNER TO test;\n\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\n\nCREATE TABLE public.instance_info (\n    run_storage_id text\n);\n\nALTER TABLE public.instance_info OWNER TO test;\n\nCREATE TABLE public.instigators (\n    id integer NOT NULL,\n    selector_id character varying(255),\n    repository_selector_id character varying(255),\n    status character varying(63),\n    instigator_type character varying(63),\n    instigator_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.instigators OWNER TO test;\n\nCREATE SEQUENCE public.instigators_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.instigators_id_seq OWNER TO test;\n\nALTER SEQUENCE public.instigators_id_seq OWNED BY public.instigators.id;\n\nCREATE TABLE public.job_ticks (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    selector_id character varying(255),\n    status character varying(63),\n    type character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.job_ticks OWNER TO test;\n\nCREATE SEQUENCE public.job_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.job_ticks_id_seq OWNER TO test;\n\nALTER SEQUENCE public.job_ticks_id_seq OWNED BY public.job_ticks.id;\n\nCREATE TABLE public.jobs (\n    id integer NOT NULL,\n    job_origin_id character varying(255),\n    selector_id character varying(255),\n    repository_origin_id character varying(255),\n    status character varying(63),\n    job_type character varying(63),\n    job_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.jobs OWNER TO test;\n\nCREATE SEQUENCE public.jobs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.jobs_id_seq OWNER TO test;\n\nALTER SEQUENCE public.jobs_id_seq OWNED BY public.jobs.id;\n\nCREATE TABLE public.kvs (\n    key text NOT NULL,\n    value text\n);\n\nALTER TABLE public.kvs OWNER TO test;\n\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\n\nALTER TABLE public.run_tags OWNER TO test;\n\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\n\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    snapshot_id character varying(255),\n    pipeline_name text,\n    mode text,\n    status character varying(63),\n    run_body text,\n    partition text,\n    partition_set text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    start_time double precision,\n    end_time double precision\n);\n\nALTER TABLE public.runs OWNER TO test;\n\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.runs_id_seq OWNER TO test;\n\nALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\n\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying(512),\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n\nALTER TABLE public.secondary_indexes OWNER TO test;\n\nCREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.secondary_indexes_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for event_logs id Column\nDESCRIPTION: This SQL statement sets the default value for the `id` column in the `event_logs` table of the `public` schema, configuring the column to automatically generate IDs using a sequence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Creating event_logs_id_seq Sequence - SQL\nDESCRIPTION: This snippet creates a sequence 'event_logs_id_seq' used to generate unique IDs for entries in the 'event_logs' table, starting from 1 and incrementing by 1.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: event_logs_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Size of Map Outputs for Fetching - Shuffle Settings\nDESCRIPTION: This configuration controls the maximum size of output files that can be fetched from each reduce task, measured in MiB. It is critical to manage memory overhead for reduce tasks effectively.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\nspark.reducer.maxSizeInFlight\n```\n\n----------------------------------------\n\nTITLE: Running Development UI with Yarn\nDESCRIPTION: This snippet demonstrates starting the development UI application using Yarn with a specific backend origin URL. The command sets the environment variable 'NEXT_PUBLIC_BACKEND_ORIGIN' to the provided local host address and port before launching the UI. This requires Yarn and Node.js to be installed and configured.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagit/README.rst#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nNEXT_PUBLIC_BACKEND_ORIGIN=\\\"http://localhost:3333\\\" yarn start\n```\n\n----------------------------------------\n\nTITLE: Using Basic Data Structures in Dagster Config\nDESCRIPTION: Shows how to use Python data structures like List, Dict, and Mapping in config schemas. This allows for more complex configuration patterns like lists of values or mappings between keys and values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/configuration/advanced-config-types.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyAssetConfig(Config):\n    users: List[str]\n    scores: Dict[str, int]\n\n@asset(config_schema=MyAssetConfig)\ndef hello_asset(context):\n    config = MyAssetConfig(users=[\"Alice\", \"Bob\"], scores={\"Alice\": 100, \"Bob\": 90})\n    assert config.users == [\"Alice\", \"Bob\"]\n    assert config.scores == {\"Alice\": 100, \"Bob\": 90}\n```\n\n----------------------------------------\n\nTITLE: Database Constraint Definitions for Dagster Schema\nDESCRIPTION: SQL code defining primary key and unique constraints for Dagster database tables. These constraints ensure data integrity for critical tables like asset_keys, alembic_version, and asset_event_tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\n\n--\n-- Name: asset_event_tags asset_event_tags_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.asset_event_tags\n    ADD CONSTRAINT asset_event_tags_pkey PRIMARY KEY (id);\n\n\n--\n-- Name: asset_keys asset_keys_asset_key_key; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\n\n--\n-- Name: asset_keys asset_keys_pkey; Type: CONSTRAINT; Schema: public; Owner: test\n--\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Creating Secondary Indexes Table in PostgreSQL for Dagster\nDESCRIPTION: This SQL snippet creates the 'secondary_indexes' table to manage secondary indexes in Dagster. It includes columns for index identification and migration timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.secondary_indexes (\n    id bigint NOT NULL,\n    name character varying(512),\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Table Constraints\nDESCRIPTION: Adds primary key and unique constraints to alembic_version, asset_keys and daemon_heartbeats tables. Configures table-level integrity rules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_64\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for GCS Components\nDESCRIPTION: ReStructuredText documentation defining Google Cloud Storage (GCS) resources, I/O managers, sensors, and file management components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-gcp.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\nGCS\n---\n\nGCS Resource\n^^^^^^^^^^^^^\n\n.. autoconfigurable:: GCSResource\n  :annotation: ResourceDefinition\n\n\nGCS I/O Manager\n^^^^^^^^^^^^^^^^^^\n\n.. autoconfigurable:: GCSPickleIOManager\n  :annotation: IOManagerDefinition\n\n\nGCS Sensor\n^^^^^^^^^^\n\n.. autofunction:: dagster_gcp.gcs.sensor.get_gcs_keys\n\n\nFile Manager\n^^^^^^^^^^^^\n\n.. autoclass:: GCSFileHandle\n  :members:\n\n.. autoconfigurable:: GCSFileManagerResource\n  :annotation: ResourceDefinition\n\nGCS Compute Log Manager\n^^^^^^^^^^^^^^^^^^^^^^^^\n.. autoclass:: dagster_gcp.gcs.GCSComputeLogManager\n```\n\n----------------------------------------\n\nTITLE: Pipeline Run Data Insert SQL\nDESCRIPTION: SQL INSERT statement containing pipeline run metadata including execution plan ID, pipeline origin, configuration and status information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.runs (id, run_id, pipeline_snapshot_id, execution_plan_snapshot_id, parent_run_id, root_run_id, status, start_time, end_time) FROM stdin;\n1\tab4b26d6-58cc-4172-bf6f-d9b99bedec79\tea3ba6e82e89be10e74a953e070fd5f27cc058b6\tsingle_mode\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"cfa77af064b2961a3a593580d097f5b795bf25ea\",...}\t\\N\t\\N\t2021-07-06 17:43:25.919394\t2021-07-06 17:43:31.716787\n\\.\n```\n\n----------------------------------------\n\nTITLE: Visualizing Anonymous Anomalous Events in Stock Prices using Python\nDESCRIPTION: This snippet creates a plot of anonymous anomalous events detected in the stock prices.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_pandas_type_metadata/notebooks/bollinger.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbol.plot_sample_anonymous_events(ANOMALIES)\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Index on asset_key and partition Columns in event_logs Table\nDESCRIPTION: SQL command to create a composite index on asset_key and partition columns in the event_logs table to optimize queries filtering by both fields.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\n```\n\n----------------------------------------\n\nTITLE: Declaring a String Literal in Python\nDESCRIPTION: A Python string literal declaration using double quotes to create the classic \"Hello, world!\" message. This represents one of the most basic examples of string declaration in Python.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster-test/dagster_test/toys/hello_world.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in React\nDESCRIPTION: React component import statement for rendering a list of documentation cards in the Dagster+ documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Alter run_tags_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `run_tags_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.run_tags_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster Development Server\nDESCRIPTION: Command to start the Dagster development server which will be accessible at http://localhost:3000.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_llm_fine_tune/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Copying Snapshot Data to snapshots Table in PostgreSQL for Dagster\nDESCRIPTION: SQL statement for populating the snapshots table with serialized binary data (hex-encoded) representing Dagster pipeline snapshots. These snapshots contain the complete definition of a pipeline at a point in time.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.snapshots (id, snapshot_id, snapshot_body, snapshot_type) FROM stdin;\n1\t653890b7bc24ce4415c9f2704bf15f80c2bb8f65\t\\\\x789ced5d5b73dbb815fe2b1cf5a1edccc6e19dc4be3989d375ebda99d8db4e679d6170b5d8a5482e49395177fcdf7b005212259332244bb2ecf5ce6c1252e0c1c177ae3800c8df075144135c965134f8d1187c8a739ec429bf4c715e0eb36af08431a0592ae29ba8a4433ec25139fde547e3f7c567dfab7697aa59fb799c24d19486bc1d9149f42b9f2802c7e9a48fd0d524576c48128c97b488f32ace5268908e93046ef2743c8a6e7132e6e5fca68879c25ad737f12d4fa3148fb8a42cbb839b75efb3ab38650d138ae4120fff809f8f8ecfff33b883b625c5092ea2e691a68f0a5a45392ef0488e6bdab96c7e5c1478727439c4393f32a9c5698884136217f92c4082993e13260f7c4a7c8b21ea9100876c7d3c0667715919993036ea6e23249b7b532037ec570bf9cf9f8fd7c2fe97c19a8c7c6949aa2ae2f4e6f112a8c96c0fd919bd5d215677a090789765c906086c36dabab7f97867971ae3bc7c7f7c76fcb963a05d4f5daa16eab977171767eaa97ebbfd9864b8d7c36d1f84babb390af3eb1dc2f0f1ece2f8ea011c4ed33da2203b9b6330bdda2102a7e70f8dff3c4b3926093fda08888de3d4a20f5860421392f38bf393e3776727ebb901d981f20135483fa7308e2369926f2e79c269951547847bccb342e2201efad875a8276cce29169e13524bf834082c8b78e8c9d0da02eb6ba95df4f3f9e9c5f97a384fbddcfaacdd938e7215f33132d3c44ec09dd00c981350247c44a96f231450177bc2c556e0738f3a07219e4d79dfb97c66fe777ddeee09086c6a3e448c0284080a2c66d998f2c0c4360b43d3a16108fffb8e6b05cc0c6cef20c4b319e73b174ee305d7e7ec9e68eaac673e466eba81ed50c41092b2c6c4b66cd3771d8fd8b6ed0654809986b67f18be6d63e6772ea079b6ba3e73bd325229bded322642866de2baa14b31a2c834b96d236e0576e843c20fbe928787613e9b71be4fe9acc7592d9aa9442d2288e50798636406a189b11f200fc1bf4c1a588485d4f7383389bd2351fcd249f4a3fc794e55e07152457991ddc68c4b58aa62cc5bbf28ea112ea3ff96591a95552129fd7ed7cb525c4605ff6d1c178a98c04929a94dd3571697324b53135b05fc542714cc0c3b083b9c7bdc272e3199637b1e115c20cf37ad10995830882d2692c23cbcb1f1742b43fbf280f5acad5cbad6727276f2feeaa26b8af0600967c694ed059669216161cf8671790eb32cd3b12dd7c3d4e19ee39a7e20e0f2a0347e2ac8556ad1f4a4a1168d8ecdb5e2f69e46d4de455bd2daa0ee53d2be1001e188078140810bded021823aa68779e8e010b9a0833641c23c28496bd9fff5a0e020215e5e4bdeafa776adaeeeee3abcc3e0e43ba7e38a1b38498cb2e27969c4a9818d12a49c7003faa7bc2c8f060f7a90388d9ac6dd4ec47585ed62ea79a68718b6206de3a18b61e603be20e026678e63db9e4577eb1faf0723fc5d16abe9b828785a5dcfc4b339721cd3a1824e2197c29f2c06a6c6385903be11f01caf04901098237206e6842c4bb8e0763d24ed88387ee85b81872d4c2ce6afe185b5cd609fb6193a56283c2c403398e9a3107b34109c098208a38127438de933123e3fdbecd2a19afcb8e04635e4465b090caed42b2b8c2a03edc245659463d2fcc84b632c2dd4f82ab2e2d79217b7bcf8aaa165f3d63d464a608a0be013db64c2b16cdfe7f05fc0436a99be292c026ac74ccfdd7b12f378a0ca1c7f4b7530520df79508696bfb3e4df0898b18079808491abd16e390c0016b716d27b498340fea09c7b74287518428417ee0737b638bd9e9b8f298fe9af0973832456c7960b2b2a46d9a5b2f886dc3349fb83aff6a9aafa6b91bd3542b26dab6b9f5959e6dd8e6132fcdbcdae6ab6deec636ebd5326de3dcfa32df368cf389977e5e8df3d53877639ceb1664b7be8cb80deb14b6e03e138cfa26b6901be050843ca0e03e026c5a367103d3f708fd0395de5fadf3b98dacd33ae50e646dd3d4b6827d9866bdc536b41ccff57d8720e6925010c12c4a42ee7861e03bc8b291077f3ae2a0ec52a7c6783d383dff78713de8a8345e0db991643737bcf8736954c38297c32c611a854378284af82d4ffa5cf40e6ba6d703866fca8a17bd43925cca0dcc936c5c34e3d31893fa7bc388b39e06e9eaf4d5e7d3f757d1e54fc79fd6dafbb8a4d73bdbaaff5ce38d14da76d576a7ec26f128ae3acb7b07c1edb21975676e8fde4ba469825b3eb0b12513143c743c87f9c4736d4ca9c00c114e09239813640bdf459c10840fca0435d798679140ae1e4f438d5c629652aaefcddc75d76297f4d74d1f06cd920c940afe4ecb2ce1faaebb3e91d59d69697b654d0dd395e55e35cc0a84408e13123b2438c0b6474d1305264c6f4ce6301b01439613facf50c336dde585d3ac1af2221a420e904c806cc9ef39d11abbc075b81b20b963c7a3c4763d982232e63b0eb7ad80084a5c00cfc2c161ee664bb2f4262ac669dab7cefdbc870752947a5fc5f2a1be4d7bcf7b88a0a611c3f14b565239c46d1aa296a7d6f689fbf5d4c08fc058585c30cff49cc01581b03d21d313ce3c44984720dfb10eca53ef366324b8040b88effb2f95285e82de502e87ad2b775d84f72af727deee7f08725f9ad7e03c4f26677272f389cba9c16f63feafcd974b777332612ba2d7de0efa22443ff8e5c3c9a7cf27ef8faf4e3e7cd1d80c8a8b5f214dab3260312b7b0b30bbdd28bbb801b66396f2ef2197d9a4d1ec9b3570c18de621232b0cc8528e8c7793e944e687ae761ad398e6a97b18ac7d9440cf5ab4f572efd68203cbf103cf0c386316ccd57c70e10eb26d9bba8e6513c7471639286bd969f6b46a76ab1decf455420bfcfdaa84ee62c921a9c46e979670357c54ad5a1bd27d0b5a6f4ff68b10f4e0e47b9ec434ae928951e69cc66252eff8ced83881e05165465ef024c34c9eb690bfccf7b41f191732207d8b65d7f25f5cc59aea5b66502cb78243d3595dad1eb0118b1969d9168256d36d0cd1c93815aa8b0f75b1cef86f468c6fb83464f710e444918d0cdc3caf7a344ab9ca52df30bec54962103ee5b7a1073dd44dbf3625c0afd3f671d96afa70606cda460dfbf7561edb2f96d1d67f3d4ddbabfe6bcf805f84fe3f22f869ad35af07e95e051d7a21735d1c1048d73c393931e11ab916a2340888e959b61014b1c3da3fa9994cd7126b8e96b54faf4c6f6d762aad332f9f1f951966df9a337d2ae7aecfab31704cd5501df22bc6a986a3a99f8bfbf6a3503bb06d1f52638e435780e402db31434e0321b04f1d147836253cd87f156e09887ae1a434384cae8d11408f6fc0e9af894543a41b091e228f218f739f995c80bf0c61be1df80294dac6243043244cea22b2eb995b5799bfd618a950ade2f8fce6524979fec3522176f187fb1dac1603f45ac510209513e09594860cc9192828fca5c8e89c9584f63d89bf6e7573c70288b3688453d0afa231db96f96bd9ec10cb25bf8297aace579b6f3cca133e02efb0be05cf0875c386b86912c7112c14dc425e881d9b3930d3713cd7a436dc0a439813fbba53686d3fbed7e8a23dc8e71d5d3ab5ebdd384eaa37a03022864c71022161649c5e188d8e42368a21ebadb2426a5aca9a42cd2d5c3549727d84526ddd837fe81c069f1940cf4e3fdd29b59ec669cb76af1aa77d60fbb9699ceca96f97d9784440a332313dfa2ea76c52bd4678229d95314f7292c96279103c5a29e7401004e42c8fd4277d4117c7455a2ba224fbb59d3c4965a4f9183227082c7ff9abcee9dec5446b593b17dec4f75a5f5dafbebaef9d570d8f75c46c9ff89600d082e34a4eb9db083587c06b2d93cfca06971c826e55e5e58f6fdfb28c9647f9a41a66e95156dcbc75de263129703179bbac76c36a94fc0914a9e2dfabf20d78cd37eae8f91b486b8619d3c96254fba86edf8bbef6f9f0fda37fac2c154c52edc56bcc5c422fd7b062d051751c5f4e3e64b662a8ec928ec18d1a15be29552d0437765d5b7d5d1651d4e49d16992c4d26357dc0e74dfd4c8ee3a229a8e850c1b4c8ca52bde5a4096a9998919c157ada8464c7d8603168981ac0d7beb5b9dae2bfb63b56a51fd55e52a9161ce3583d39e542e6bf7336ceb38ad797f04cc9a7d04a5473e909c6e90faa4475936404271a4a0650cfdd1d9d4435c19e5ad17abb05f542b37610dc6b68d69eb33eb7d0dc555de80a2e8f596359fbf5317a\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Snowflake Integration\nDESCRIPTION: This command installs the `dagster-snowflake` package, which provides the necessary integration between Dagster and Snowflake.  This package provides the `SnowflakeResource` used to connect to and interact with Snowflake.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/copy_csv_to_snowflake.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"pip install dagster-snowflake\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Engine Event in JSON\nDESCRIPTION: JSON record of an ENGINE_EVENT in Dagster. This record shows the initialization of resources, specifically the 'asset_store' resource, with timing information for a new pipeline run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"resources\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": \"Initialized in 0.03ms\", \"entry_data\": {\"__class__\": \"PythonArtifactMetadataEntryData\", \"module\": \"dagster.core.storage.asset_store\", \"name\": \"InMemoryAssetStore\"}, \"label\": \"asset_store\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Finished initialization of resources [asset_store].\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - ENGINE_EVENT - Finished initialization of resources [asset_store].\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.484303, \"user_message\": \"Finished initialization of resources [asset_store].\"}\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements Configuration\nDESCRIPTION: Comprehensive requirements.txt file listing Python package dependencies with specific version numbers. Includes core packages like pandas, numpy, boto3 as well as development dependencies like pytest and mypy. Contains local package installations in editable mode for dagster modules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/alt-1/requirements-pinned.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nagate==1.9.1\naiobotocore==2.21.1\naiofile==3.9.0\naiohappyeyeballs==2.6.1\naiohttp==3.11.16\naioitertools==0.12.0\naiosignal==1.3.2\nalembic==1.15.2\nannotated-types==0.7.0\nantlr4-python3-runtime==4.13.2\nanyio==4.9.0\nappdirs==1.4.4\nappnope==0.1.4\nargon2-cffi==23.1.0\nargon2-cffi-bindings==21.2.0\narrow==1.3.0\nasn1crypto==1.5.1\n-e examples/assets_pandas_type_metadata\nastroid==3.3.9\nasttokens==3.0.0\nasync-lru==2.0.5\nattrs==25.3.0\nbabel==2.17.0\nbackoff==2.2.1\nbackports-tarfile==1.2.0\nbeautifulsoup4==4.13.4\nbleach==6.2.0\nboto3==1.37.1\nboto3-stubs-lite==1.37.36\nbotocore==1.37.1\nbotocore-stubs==1.37.29\nbuildkite-test-collector==1.0.2\ncachetools==5.5.2\ncaio==0.9.22\ncertifi==2025.1.31\ncffi==1.17.1\nchardet==5.2.0\ncharset-normalizer==3.4.1\nclick==8.1.8\ncolorama==0.4.6\ncoloredlogs==14.0\ncomm==0.2.2\ncontourpy==1.3.2\ncoverage==7.8.0\ncryptography==44.0.2\ncycler==0.12.1\ndaff==1.3.46\n```\n\n----------------------------------------\n\nTITLE: Logging Step Expectation Result - JSON\nDESCRIPTION: Logs a step expectation result indicating whether the output table for the 'raw_file_events' solid exists. This event includes various details about the execution context and logging tags for traceability.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Checked raw_file_events exists\", \"label\": \"output_table_exists\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_events\", \"solid_definition\": \"raw_file_events\", \"step_key\": \"raw_file_events.compute\"}, \"message\": \"Checked raw_file_events exists\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_events\", \"name\": \"raw_file_events\", \"parent\": null}, \"step_key\": \"raw_file_events.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Checked raw_file_events exists\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"output_table_exists\\\", \\\"Checked raw_file_events exists\\\", []]}\\n               solid = \\\"raw_file_events\\\"\\n    solid_definition = \\\"raw_file_events\\\"\\n            step_key = \\\"raw_file_events.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_events.compute\", \"timestamp\": 1576110682.846945, \"user_message\": \"Checked raw_file_events exists\"}\n```\n\n----------------------------------------\n\nTITLE: Documenting Dagster Error Classes using reStructuredText\nDESCRIPTION: This snippet uses reStructuredText directives to automatically generate documentation for Dagster error classes. It specifies the module to document and lists individual exceptions to be included in the documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/errors.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: dagster._core.errors\n   :no-members:\n\n.. currentmodule:: dagster\n\n.. autoexception:: DagsterError\n\n.. autoexception:: DagsterConfigMappingFunctionError\n\n.. autoexception:: DagsterEventLogInvalidForRun\n\n.. autoexception:: DagsterExecutionStepExecutionError\n\n.. autoexception:: DagsterExecutionStepNotFoundError\n\n.. autoexception:: DagsterInvalidConfigError\n\n.. autoexception:: DagsterInvalidConfigDefinitionError\n\n.. autoexception:: DagsterInvalidDefinitionError\n\n.. autoexception:: DagsterInvalidSubsetError\n\n.. autoexception:: DagsterInvariantViolationError\n\n.. autoexception:: DagsterResourceFunctionError\n\n.. autoexception:: DagsterRunNotFoundError\n\n.. autoexception:: DagsterStepOutputNotFoundError\n\n.. autoexception:: DagsterSubprocessError\n\n.. autoexception:: DagsterTypeCheckDidNotPass\n\n.. autoexception:: DagsterTypeCheckError\n\n.. autoexception:: DagsterUnknownResourceError\n\n.. autoexception:: DagsterUnmetExecutorRequirementsError\n\n.. autoexception:: DagsterUserCodeExecutionError\n```\n\n----------------------------------------\n\nTITLE: Alter job_ticks_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `job_ticks_id_seq` sequence to the `id` column of the `job_ticks` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.job_ticks_id_seq OWNED BY public.job_ticks.id;\"\n```\n\n----------------------------------------\n\nTITLE: Alter secondary_indexes Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `secondary_indexes` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.secondary_indexes OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Defining Spark Executor Extra Java Options in HTML Table\nDESCRIPTION: HTML table row defining the spark.executor.extraJavaOptions property, which allows passing extra JVM options to executors. It includes usage notes and examples of symbol interpolation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_6\n\nLANGUAGE: html\nCODE:\n```\n<tr>\n  <td><code>spark.executor.extraJavaOptions</code></td>\n  <td>(none)</td>\n  <td>\n    A string of extra JVM options to pass to executors. For instance, GC settings or other logging.\n    Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this\n    option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file\n    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.\n\n    The following symbols, if present will be interpolated: {{APP_ID}} will be replaced by\n    application ID and {{EXECUTOR_ID}} will be replaced by executor ID. For example, to enable\n    verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of:\n    <code>-verbose:gc -Xloggc:/tmp/{{APP_ID}}-{{EXECUTOR_ID}}.gc</code>\n\n  </td>\n</tr>\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Repository Package\nDESCRIPTION: Command to install the Dagster repository as a Python package in editable mode, allowing local code changes to automatically apply during development.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_snowflake/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to secondary_indexes Table in SQL\nDESCRIPTION: Adds a unique constraint on the name column and a primary key constraint on the id column of the secondary_indexes table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Creating Base Dagster Tables and Sequences in PostgreSQL\nDESCRIPTION: A collection of SQL statements that create the core database schema for Dagster, including tables for event logs, jobs, runs, assets and other infrastructure components. Includes sequence definitions, ownership assignments, and column specifications with appropriate data types.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE public.event_logs OWNER TO test;\n\nCREATE SEQUENCE public.event_logs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.event_logs_id_seq OWNER TO test;\n\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\n\nCREATE TABLE public.instance_info (\n    id bigint NOT NULL,\n    run_storage_id text\n);\n\nALTER TABLE public.instance_info OWNER TO test;\n\nCREATE SEQUENCE public.instance_info_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.instance_info_id_seq OWNER TO test;\n\nALTER SEQUENCE public.instance_info_id_seq OWNED BY public.instance_info.id;\n\nCREATE TABLE public.instigators (\n    id bigint NOT NULL,\n    selector_id character varying(255),\n    repository_selector_id character varying(255),\n    status character varying(63),\n    instigator_type character varying(63),\n    instigator_body text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Up PostgreSQL Configuration\nDESCRIPTION: This snippet configures the PostgreSQL environment by setting various parameters like statement timeout, lock timeout, client encoding, and search path. It ensures the database operates with specific settings related to data handling, security, and compatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n\"SET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\"\n```\n\n----------------------------------------\n\nTITLE: Exploring Dagster Step Start Event Structure\nDESCRIPTION: This JSON structure represents a step start event in Dagster. It marks the beginning of a specific computation step within a pipeline and includes details about the solid being executed and process information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_something\", \"solid_definition\": \"do_something\", \"step_key\": \"do_something.compute\"}, \"message\": \"Started execution of step \\\"do_something.compute\\\".\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_something\", \"parent\": null}, \"step_key\": \"do_something.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_something.compute - STEP_START - Started execution of step \\\"do_something.compute\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_something.compute\", \"timestamp\": 1610466123.532808, \"user_message\": \"Started execution of step \\\"do_something.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with event_logs ID in PostgreSQL\nDESCRIPTION: Associates 'event_logs_id_seq' with the ID column of the 'event_logs' table, enabling automatic ID assignment for new rows to maintain data integrity.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\n```\n\n----------------------------------------\n\nTITLE: Alter event_logs_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `event_logs_id_seq` sequence to the `id` column of the `event_logs` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.event_logs_id_seq OWNED BY public.event_logs.id;\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install required Python packages for running the asset factory examples.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/creating-asset-factories.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-aws duckdb pyyaml pydantic\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Command to install the project and its Python dependencies in development mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/assets_dbt_python/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Defining a simple string literal in Python\nDESCRIPTION: Creates a basic string literal with the text 'Hello, world!'. This is commonly used as an introductory example in programming.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/foo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"Hello, world!\"\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Materialization Event in JSON\nDESCRIPTION: This snippet represents a Dagster event record for a step materialization. It includes details about the materialized value, its path, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/group_admins.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/group_admins.raw\\\"]]]]\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Inserting Dagster Schedule State Data in SQL\nDESCRIPTION: SQL insert statement for Dagster schedule state data. Includes schedule ID, origin ID, status, and serialized schedule configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_64\n\nLANGUAGE: SQL\nCODE:\n```\n1\t3c69c0fa6cf35ab1c72bf2ec46dd1b85f0582618\tf08361dc16dea651cea80148e7ce436ff5f253a9\tRUNNING\t{\"__class__\": \"ScheduleState\", \"cron_schedule\": \"* * * * *\", \"origin\": {\"__class__\": \"SchedulePythonOrigin\", \"repository_origin\": {\"__class__\": \"RepositoryPythonOrigin\", \"code_pointer\": {\"__class__\": \"FileCodePointer\", \"fn_name\": \"bar\", \"python_file\": \"/Users/dgibson/dagster-home/../dagster/python_modules/dagit/dagit_tests/toy/bar_repo.py\", \"working_directory\": \"/Users/dgibson/dagster-home/\"}, \"executable_path\": \"python\"}, \"schedule_name\": \"foo_schedule\"}, \"start_timestamp\": 1610465951.557492, \"status\": {\"__enum__\": \"ScheduleStatus.RUNNING\"}}\t2021-01-12 10:39:10.167914\t2021-01-12 10:39:10.167914\n\\.\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for Legacy Components\nDESCRIPTION: ReStructuredText documentation defining legacy components and resources for backward compatibility.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-gcp.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\nLegacy\n------\n\n.. autoconfigurable:: ConfigurablePickledObjectGCSIOManager\n  :annotation: IOManagerDefinition\n\n.. autoconfigurable:: bigquery_resource\n  :annotation: ResourceDefinition\n\n.. autoconfigurable:: build_bigquery_io_manager\n  :annotation: IOManagerDefinition\n\n.. autoconfigurable:: gcs_resource\n  :annotation: ResourceDefinition\n\n.. autoconfigurable:: gcs_pickle_io_manager\n  :annotation: IOManagerDefinition\n\n.. autodata:: gcs_file_manager\n  :annotation: ResourceDefinition\n\n.. autoconfigurable:: dataproc_resource\n  :annotation: ResourceDefinition\n```\n\n----------------------------------------\n\nTITLE: Constrained Package Versions\nDESCRIPTION: This section outlines dependencies that are version-constrained beyond runtime needs due to issues like type annotation bugs. These constraints are necessary to ensure correct static type checking performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/requirements.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\napache-airflow>2.7\npendulum<3\ntypes-sqlalchemy==1.4.53.34\nfastapi>=0.115.6\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: runs_id_seq in PostgreSQL\nDESCRIPTION: Creates 'runs_id_seq' to auto-increment run IDs in the 'runs' table, starting at 1 with an increment of 1. This ensures unique identifiers for execution runs, supporting efficient tracking and referencing.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.runs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n```\n\n----------------------------------------\n\nTITLE: MLflow Package Reference in Markdown\nDESCRIPTION: Package reference using inline code formatting to highlight the dagster-mlflow package name\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-mlflow/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-mlflow\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster STEP_OUTPUT Event (ingest_traffic)\nDESCRIPTION: This JSON snippet represents a Dagster event log entry indicating the output of a step named 'ingest_traffic' within the 'longitudinal_pipeline'. It includes details about the output named 'result', its type ('Any'), and a type check indicating success.  The event provides information about intermediate materialization, step output handle, and type check data, along with logging tags for context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"ingest_traffic\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_traffic\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_traffic\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_traffic\", \"parent\": null}, \"step_key\": \"ingest_traffic\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ingest_traffic - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"ingest_traffic\", \"timestamp\": 1609894312.7837, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for run_tags ID in PostgreSQL\nDESCRIPTION: Links 'run_tags_id_seq' with the 'run_tags' ID column, enabling automatically assigned default ID numbers and streamlining tag management and consistency.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Job Ticks Timestamp in PostgreSQL\nDESCRIPTION: Creates a B-tree index on the job_ticks table for the job_origin_id and timestamp columns. This index improves query performance when searching for job ticks by their origin and timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_55\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Output Event in JSON\nDESCRIPTION: JSON record of a step output event showing successful yield of 'result' output from the 'build_model' solid. Contains step output handle and type checking information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"build_model\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_model\", \"solid_definition\": \"base_two_inputs\", \"step_key\": \"build_model\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_model\", \"parent\": null}, \"step_key\": \"build_model\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_model - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_model\", \"timestamp\": 1609894320.042903, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Scaffolding a New Dagster Project\nDESCRIPTION: Command to create a new Dagster project named dagster-quickstart.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/teradata.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster project scaffold --name dagster-quickstart\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Docs with Preview Environment\nDESCRIPTION: Command to start the local development server with the preview environment flag for validating the version dropdown menu.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nVERCEL_ENV=preview yarn start\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster STEP_START Event (ingest_traffic)\nDESCRIPTION: This JSON snippet represents a Dagster event log entry indicating the start of a step named 'ingest_traffic' within the 'longitudinal_pipeline'. It includes details such as the pipeline name, run ID, step key, timestamp, and a user message. The logging_tags provides additional context, including the GRPC host and socket, partition, partition set, and solid selection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_traffic\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_traffic\"}, \"message\": \"Started execution of step \\\"ingest_traffic\\\".\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_traffic\", \"parent\": null}, \"step_key\": \"ingest_traffic\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ingest_traffic - STEP_START - Started execution of step \\\"ingest_traffic\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"ingest_traffic\", \"timestamp\": 1609894312.594458, \"user_message\": \"Started execution of step \\\"ingest_traffic\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Table Data - Runs and Run Tags\nDESCRIPTION: SQL data dump showing database records for run_tags and runs tables containing Dagster pipeline execution metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.run_tags (id, run_id, key, value) FROM stdin;\n10\tac6813ac-1860-4feb-a2b9-79c245f813ff\tdagster/partition\t2020-01-02\n11\tac6813ac-1860-4feb-a2b9-79c245f813ff\tdagster/partition_set\tingest_and_train\n12\tac6813ac-1860-4feb-a2b9-79c245f813ff\tdagster/solid_selection\t*\n13\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\tdagster/partition\t2020-01-02\n14\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\tdagster/partition_set\tingest_and_train\n15\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\tdagster/solid_selection\t*\n16\tbc7168d1-3d66-4d5d-93e0-0df885ee30f2\t.dagster/grpc_info\t{\"host\": \"localhost\", \"socket\": \"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\"}\n\\.\n\nCOPY public.runs (id, run_id, snapshot_id, pipeline_name, status, run_body, create_timestamp, update_timestamp) FROM stdin;\n\\.\n\n```\n\n----------------------------------------\n\nTITLE: Sequence Value Configuration\nDESCRIPTION: SQL commands setting sequence values for various database tables including event_logs, run_tags, runs, schedule_ticks, and schedules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.event_logs_id_seq', 20, true);\nSELECT pg_catalog.setval('public.run_tags_id_seq', 1, false);\nSELECT pg_catalog.setval('public.runs_id_seq', 1, true);\nSELECT pg_catalog.setval('public.schedule_ticks_id_seq', 1, false);\nSELECT pg_catalog.setval('public.schedules_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Creating Index on asset_key Column in event_logs Table\nDESCRIPTION: SQL command to create an index on the asset_key column in the event_logs table to optimize queries filtering by asset_key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n```\n\n----------------------------------------\n\nTITLE: Python Pass Statement\nDESCRIPTION: The 'pass' statement is used as a null operation; nothing happens when it executes. It is useful as a placeholder when a statement is required syntactically, but no code needs to be executed, for example, in an empty function or class definition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/bad_kernel.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"pass\"\n```\n\n----------------------------------------\n\nTITLE: Specifying AWS CLI Version in Plain Text\nDESCRIPTION: This snippet defines the exact version of the AWS Command Line Interface (CLI) to be used in the project. It ensures that all environments or deployments use the same version of the AWS CLI, maintaining consistency in AWS interactions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation/docker/images/buildkite-build-test-project-image/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nawscli==1.32.22\n```\n\n----------------------------------------\n\nTITLE: Initializing Documentation Template in dbt\nDESCRIPTION: Sets up the overview documentation block for the Jaffle Shop dbt project using Jinja templating. Contains project description and source code reference links.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_mixed_freshness/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Ignoring Dependencies in Cron-based Automation\nDESCRIPTION: Example of how to ignore specific upstream dependencies when using AutomationCondition.on_cron() by specifying AssetSelection to ignore.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/declarative-automation/customizing-automation-conditions/example-customizations.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@asset(auto_materialize_policy=AutoMaterializePolicy(AutomationCondition.on_cron(\"0 0 * * *\", ignore_deps=AssetSelection.groups(\"upstream_group\"))))\n```\n\n----------------------------------------\n\nTITLE: Dagster Cloud CLI Branch Deployment\nDESCRIPTION: Shell command to create or update a branch deployment using the dagster-cloud CLI\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/setting-up-branch-deployments.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nBRANCH_DEPLOYMENT_NAME=$(\n    dagster-cloud branch-deployment create-or-update \\\n        --organization $ORGANIZATION_NAME \\\n        --api-token $DAGSTER_CLOUD_API_TOKEN \\ # Agent token from Step 1\n        --git-repo-name $REPOSITORY_NAME \\ # Git repository name\n        --branch-name $BRANCH_NAME \\ # Git branch name\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Start Event in JSON\nDESCRIPTION: JSON record of a STEP_START event in the Dagster pipeline execution system. This record captures the beginning of the 'do_input.compute' step with associated logging tags and pipeline metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Started execution of step \\\"do_input.compute\\\".\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - STEP_START - Started execution of step \\\"do_input.compute\\\".\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.610631, \"user_message\": \"Started execution of step \\\"do_input.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Creating public.secondary_indexes Table and Sequence\nDESCRIPTION: This snippet creates the 'secondary_indexes' table in the 'public' schema to store metadata about secondary indexes. It includes columns for ID, index name, creation timestamp, and migration completed timestamp. An associated sequence 'secondary_indexes_id_seq' is also created to automatically generate unique IDs for each index.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying(512),\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n\n\nALTER TABLE public.secondary_indexes OWNER TO test;\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.secondary_indexes_id_seq OWNER TO test;\n\n--\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.secondary_indexes_id_seq OWNED BY public.secondary_indexes.id;\"\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record JSON Structure\nDESCRIPTION: JSON structure representing log entries for Dagster pipeline execution events, including pipeline initialization, step execution, and output handling. Contains nested event data with metadata about pipeline steps, resources, and execution status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"EngineEventData\",\n      \"error\": null,\n      \"marker_end\": \"resources\",\n      \"marker_start\": null,\n      \"metadata_entries\": [{\n        \"__class__\": \"EventMetadataEntry\",\n        \"description\": \"Initialized in 0.04ms\",\n        \"entry_data\": {\n          \"__class__\": \"PythonArtifactMetadataEntryData\",\n          \"module\": \"dagster.core.storage.asset_store\",\n          \"name\": \"InMemoryAssetStore\"\n        },\n        \"label\": \"asset_store\"\n      }]\n    },\n    \"event_type_value\": \"ENGINE_EVENT\",\n    \"logging_tags\": {},\n    \"message\": \"Finished initialization of resources [asset_store].\",\n    \"pid\": 80538,\n    \"pipeline_name\": \"foo\",\n    \"solid_handle\": null,\n    \"step_key\": null,\n    \"step_kind_value\": null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting Headings in reStructuredText\nDESCRIPTION: Shows the correct syntax for formatting headings at different levels in reStructuredText files used for API documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nH1 heading\n==========\n\nH2 heading\n----------\n\nH3 heading\n^^^^^^^^^^\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Start Event in JSON\nDESCRIPTION: This snippet shows a Dagster event log entry for a step start event. It includes information about the starting step and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"STEP_START\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"raw_file_events\",\n      \"solid_definition\": \"raw_file_events\",\n      \"step_key\": \"raw_file_events.compute\"\n    },\n    \"message\": \"Started execution of step \\\"raw_file_events.compute\\\".\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"raw_file_events\",\n      \"name\": \"raw_file_events\",\n      \"parent\": null\n    },\n    \"step_key\": \"raw_file_events.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"raw_file_events.compute\\\".\\n               solid = \\\"raw_file_events\\\"\\n    solid_definition = \\\"raw_file_events\\\"\\n            step_key = \\\"raw_file_events.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"raw_file_events.compute\",\n  \"timestamp\": 1576110682.827326,\n  \"user_message\": \"Started execution of step \\\"raw_file_events.compute\\\".\"\n}\n```\n\n----------------------------------------\n\nTITLE: Create alembic_version Table\nDESCRIPTION: This SQL statement creates the `alembic_version` table in the `public` schema. It is used by Alembic for database migrations and stores the current version number.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\"\n\n```\n\n----------------------------------------\n\nTITLE: Accessing HackerNews Assets Path\nDESCRIPTION: Reference to file path where HackerNews assets are defined in the project structure\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_aws/README.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nquickstart_aws/assets/hackernews.py\n```\n\n----------------------------------------\n\nTITLE: Pipeline Event Record - Step Input\nDESCRIPTION: JSON log record showing validation of pipeline step input named 'units'. Includes type checking and event metadata for the 'sleeper_4' component.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"units\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"units\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\"pipeline\": \"sleepy_pipeline\", \"solid\": \"sleeper_4\", \"solid_definition\": \"sleeper\", \"step_key\": \"sleeper_4.compute\"}, \"message\": \"Got input \\\"units\\\" of type \\\"None\\\". (Type check passed).\"}, \"error_info\": null}\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to runs Table in SQL\nDESCRIPTION: Adds a primary key constraint on the 'id' column and a unique constraint on the 'run_id' column of the runs table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence with Event Log Table\nDESCRIPTION: Sets the 'event_log_id_seq' sequence to provide default values for the 'id' column in the 'event_log' table, linking them directly.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.event_log_id_seq OWNED BY public.event_log.id;\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Output Event for Raw File Friends Step\nDESCRIPTION: JSON representation of a DagsterEventRecord for a STEP_OUTPUT event. This record shows that the 'raw_file_friends' step successfully yielded a 'result' output of type 'Any'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"raw_file_friends.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_friends\", \"solid_definition\": \"raw_file_friends\", \"step_key\": \"raw_file_friends.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_friends\", \"name\": \"raw_file_friends\", \"parent\": null}, \"step_key\": \"raw_file_friends.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"raw_file_friends.compute\\\", \\\"result\\\"], \\\"type_check_data\\\": [true, \\\"result\\\", null, []]}\\n               solid = \\\"raw_file_friends\\\"\\n    solid_definition = \\\"raw_file_friends\\\"\\n            step_key = \\\"raw_file_friends.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_friends.compute\", \"timestamp\": 1576110683.054624, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence run_tags_id_seq in PostgreSQL\nDESCRIPTION: Creates 'run_tags_id_seq', a sequence used to generate IDs for the 'run_tags' table entries, ensuring each tag entry has a unique identifier in the Dagster database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Integrating Gemini with Dagster Pipeline\nDESCRIPTION: Example code demonstrating how to use the dagster-gemini library to incorporate Gemini AI functionality into a Dagster pipeline. It includes resource configuration, asset definition, and Gemini API calls.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gemini.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"docs_snippets/docs_snippets/integrations/gemini.py\" language=\"python\" />\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Airlift Performance Test\nDESCRIPTION: Command to execute a sanity check run of the performance harness with 1 DAG and 1 task.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-airlift/perf-harness/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nperf-harness 1 1\n```\n\n----------------------------------------\n\nTITLE: Opening a Browser Window for Testing\nDESCRIPTION: Example showing how to manage browser windows during testing by closing them after tests complete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill_tests/notebooks/cli_test_scaffold.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbrowser = webdriver.Chrome(options=chrome_options)\ntry:\n    # perform operations with browser\nfinally:\n    browser.quit()\n```\n\n----------------------------------------\n\nTITLE: Scaffolding Dagster Asset File using CLI\nDESCRIPTION: Command to scaffold a new Dagster asset file using the dagster-components CLI tool. Creates the file at assets/my_asset.py with basic asset boilerplate.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/dagster-definitions/1-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg scaffold dagster.asset assets/my_asset.py\n```\n\n----------------------------------------\n\nTITLE: Importing React Components for Documentation Page\nDESCRIPTION: Import statements for React components used to build the documentation page layout, including Card components and themed image handling.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/intro.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport {Card, CardGroup} from '@site/src/components/Cards';\nimport ThemedImage from '@theme/ThemedImage';\n```\n\n----------------------------------------\n\nTITLE: Defining Spark Python Profile Configuration in HTML Table\nDESCRIPTION: HTML table row defining the spark.python.profile property, which enables profiling in Python workers. It includes usage instructions for displaying and dumping profile results.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_7\n\nLANGUAGE: html\nCODE:\n```\n<tr>\n  <td><code>spark.python.profile</code></td>\n  <td>false</td>\n  <td>\n    Enable profiling in Python worker, the profile result will show up by <code>sc.show_profiles()</code>,\n    or it will be displayed before the driver exits. It also can be dumped into disk by\n    <code>sc.dump_profiles(path)</code>. If some of the profile results had been displayed manually,\n    they will not be displayed automatically before driver exiting.\n\n    By default the <code>pyspark.profiler.BasicProfiler</code> will be used, but this can be overridden by\n    passing a profiler class in as a parameter to the <code>SparkContext</code> constructor.\n\n  </td>\n</tr>\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Step Expectation Result Event in JSON\nDESCRIPTION: This snippet shows a Dagster event record for a step expectation result. It includes details about the expectation, its success status, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_50\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Row count passed for group_admins\", \"label\": \"group_admins.row_count\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"many_materializations_and_passing_expectations\", \"solid_definition\": \"many_materializations_and_passing_expectations\", \"step_key\": \"many_materializations_and_passing_expectations.compute\"}, \"message\": \"Row count passed for group_admins\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"many_materializations_and_passing_expectations\", \"name\": \"many_materializations_and_passing_expectations\", \"parent\": null}, \"step_key\": \"many_materializations_and_passing_expectations.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Row count passed for group_admins\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"group_admins.row_count\\\", \\\"Row count passed for group_admins\\\", []]}\\n               solid = \\\"many_materializations_and_passing_expectations\\\"\\n    solid_definition = \\\"many_materializations_and_passing_expectations\\\"\\n            step_key = \\\"many_materializations_and_passing_expectations.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence Ownership with schedules ID in PostgreSQL\nDESCRIPTION: Associates 'schedules_id_seq' with 'schedules.id', orchestrating automatic ID allocation, safeguarding data coherence and enabling orderly schedule tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.schedules_id_seq OWNED BY public.schedules.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: jobs_id_seq in PostgreSQL\nDESCRIPTION: Creates 'jobs_id_seq' sequence for auto-generating IDs in 'jobs' table, starting at 1 and incrementing by 1. Ensures unique identifiers for each job record, facilitating tracking and referencing jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.jobs_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.jobs_id_seq OWNER TO test;\nALTER SEQUENCE public.jobs_id_seq OWNED BY public.jobs.id;\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Success Event\nDESCRIPTION: Captures detailed information about a successful step execution in a Dagster pipeline, including duration, run ID, and step metadata\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 53.146590999999965}, \"event_type_value\": \"STEP_SUCCESS\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint to job_ticks Table in SQL\nDESCRIPTION: Adds a primary key constraint on the id column of the job_ticks table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Setting Accurate Block Threshold for Shuffle - Shuffle Settings\nDESCRIPTION: This property defines the size threshold for accurately recording shuffle block sizes in the HighlyCompressedMapStatus, helping to prevent out-of-memory issues during fetch operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_28\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.accurateBlockThreshold\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Docker Image\nDESCRIPTION: Commands to build and push a Docker image to a container registry\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/configuring-ci-cd.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -t ghcr.io/org/dagster-cloud-image:$IMAGE_TAG\ndocker push ghcr.io/org/dagster-cloud-image:$IMAGE_TAG\n```\n\n----------------------------------------\n\nTITLE: Using CodeExample Component for Code Snippets\nDESCRIPTION: Shows how to use the CodeExample component to include code snippets in documentation, with optional properties for customization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n<CodeExample path=\"path/to/file.py\" language=\"python\" startAfter=\"start-after-comment\" endBefore=\"end-before-comment\" title=\"My example\" />\n```\n\n----------------------------------------\n\nTITLE: Dagster Pipeline Step Input Event in JSON Format\nDESCRIPTION: JSON representation of a Dagster step input event record. This event indicates that the 'build_cost_dashboard' step successfully received an input named '_' that passed type checking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"_\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"_\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_cost_dashboard - STEP_INPUT - Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1608666933.754719, \"user_message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster longitudinal_pipeline Run with NOT_STARTED Status\nDESCRIPTION: A JSON configuration for a Dagster pipeline run in NOT_STARTED state. It contains detailed settings for various solids including ingest operations, model building and dashboard creation with partitioning for date '2020-01-02'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"7293e986c3d0c85ea67e6ddfb211bfa85633fe67\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": null, \"executable_path\": \"/Users/prha/.pyenv/versions/3.6.8/envs/dagster-3.6.8/bin/python3.6\", \"module_name\": \"dagster_test.toys.repo\", \"package_name\": null, \"python_file\": null, \"working_directory\": null}, \"location_name\": \"dagster_test.toys.repo\"}, \"repository_name\": \"toys_repository\"}, \"pipeline_name\": \"longitudinal_pipeline\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"longitudinal_pipeline\", \"pipeline_snapshot_id\": \"080905473a698ce74ecaadcf7cc3c91a3db65cb3\", \"root_run_id\": null, \"run_config\": {\"intermediate_storage\": {\"filesystem\": {}}, \"solids\": {\"build_cost_dashboard\": {\"config\": {\"materialization_key_list\": [\"dashboards\", \"cost_dashboard\"], \"materialization_url\": \"http://docs.dagster.io/cost\", \"partition\": \"2020-01-02\"}}, \"build_model\": {\"config\": {\"partition\": \"2020-01-02\", \"sleep\": 0.9459095295394656}}, \"build_traffic_dashboard\": {\"config\": {\"materialization_key_list\": [\"dashboards\", \"traffic_dashboard\"], \"materialization_url\": \"http://docs.dagster.io/traffic\", \"partition\": \"2020-01-02\"}}, \"ingest_costs\": {\"config\": {\"error_rate\": 0.11758427684728616, \"partition\": \"2020-01-02\", \"sleep\": 5.315783837199222}}, \"ingest_traffic\": {\"config\": {\"error_rate\": 0.0714843164718762, \"partition\": \"2020-01-02\", \"sleep\": 0.17794356552275575}}, \"persist_costs\": {\"config\": {\"error_rate\": 0.0007343012569762941, \"materialization_key\": \"cost_db_table\", \"materialization_value\": 5315.783837199221, \"partition\": \"2020-01-02\", \"sleep\": 5.315783837199222}}, \"persist_model\": {\"config\": {\"materialization_json\": {\"cost\": 5315.783837199221, \"traffic\": 177.94356552275576}, \"materialization_key\": \"model\", \"partition\": \"2020-01-02\"}}, \"persist_traffic\": {\"config\": {\"error_rate\": 0.009586646737069058, \"materialization_key\": \"traffic_db_table\", \"materialization_value\": 177.94356552275576, \"partition\": \"2020-01-02\", \"sleep\": 0.17794356552275575}}, \"train_model\": {\"config\": {\"partition\": \"2020-01-02\", \"sleep\": 0.9459095295394656}}}}, \"run_id\": \"ac6813ac-1860-4feb-a2b9-79c245f813ff\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.NOT_STARTED\"}, \"step_keys_to_execute\": null, \"tags\": {\"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\"}}\n```\n\n----------------------------------------\n\nTITLE: Setting SQL Defaults and Configurations\nDESCRIPTION: This snippet sets session-level configuration parameters for a PostgreSQL dump process. It ensures proper encoding with UTF-8 and disables function body checks and row-level security.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET client_min_messages = warning;\nSET row_security = off;\n\nSET default_tablespace = '';\n\nSET default_with_oids = false;\n```\n\n----------------------------------------\n\nTITLE: SQL Data Loading for Normalized Cereals\nDESCRIPTION: Data load statement containing normalized nutritional information and ratings for various cereal products.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_62\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.normalized_cereals (id, name, mfr, type, calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, shelf, weight, cups, rating) FROM stdin;\n1\t100% Bran\tN\tC\t212.12121212121212\t12.121212121212121\t3.0303030303030303\t393.93939393939394\t30.303030303030305\t15.151515151515152\t18.18181818181818\t848.4848484848485\t75.75757575757575\t3\t3.0303030303030303\t0.33\t68.402973\n[...additional data rows...]\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for Dataproc and Pipes Components\nDESCRIPTION: ReStructuredText documentation defining Dataproc resources, operations, and Pipes-related components including clients, context injectors, and message readers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-gcp.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\nDataproc\n--------\n\nDataproc Resource\n^^^^^^^^^^^^^^^^^^\n\n.. autoconfigurable:: DataprocResource\n  :annotation: ResourceDefinition\n\nDataproc Ops\n^^^^^^^^^^^^^^\n\n.. autoconfigurable:: dataproc_op\n\n.. currentmodule:: dagster_gcp.pipes\n\nPipes\n--------------\n\nClients\n^^^^^^^\n\n.. autoclass:: dagster_gcp.pipes.PipesDataprocJobClient\n\nContext Injectors\n^^^^^^^^^^^^^^^^^\n\n.. autoclass:: dagster_gcp.pipes.PipesGCSContextInjector\n\nMessage Readers\n^^^^^^^^^^^^^^^\n\n.. autoclass:: dagster_gcp.pipes.PipesGCSMessageReader\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local Environment for Airflow and Dagster\nDESCRIPTION: This command sets up a local Airflow instance, Dagster instance, and dbt project for the demo environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/starlift-demo/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake setup_local_env\n```\n\n----------------------------------------\n\nTITLE: Navigating to dbt Project Directory in Shell\nDESCRIPTION: Changes the current working directory to the cloned dbt project folder using the cd command in the shell.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/set-up-dbt-project.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd jaffle_shop\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for runs Table in SQL\nDESCRIPTION: Creates indexes on partition_set, partition, status, update_timestamp, and create_timestamp columns of the runs table to optimize query performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\n\nCREATE INDEX idx_run_range ON public.runs USING btree (status, update_timestamp, create_timestamp);\n\nCREATE INDEX idx_run_status ON public.runs USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering Dagster Migration Guide\nDESCRIPTION: This snippet imports migration guide content from a MIGRATION.md file and renders it within the page. It also exports the table of contents from the imported document.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/version-migration.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport VersionMigration, {toc as VersionMigrationTOC} from '@site/../MIGRATION.md';\n\n<VersionMigration />\n\nexport const toc = VersionMigrationTOC;\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry in JSON\nDESCRIPTION: This JSON structure represents a Dagster event log entry. It includes detailed information about pipeline execution, such as event type, timestamps, and step-specific data. The structure is consistent across multiple log entries, capturing different stages of pipeline execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"__class__\": \"EventLogEntry\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"ComputeLogsCaptureData\",\n      \"log_key\": \"div_four.div_two_2\",\n      \"step_keys\": [\"div_four.div_two_2\"]\n    },\n    \"event_type_value\": \"LOGS_CAPTURED\",\n    \"logging_tags\": {\n      \"pipeline_name\": \"composition\",\n      \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\",\n      \"resource_fn_name\": \"None\",\n      \"resource_name\": \"None\",\n      \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n      \"solid_name\": \"div_two_2\",\n      \"step_key\": \"div_four.div_two_2\"\n    },\n    \"message\": \"Started capturing logs for step: div_four.div_two_2.\",\n    \"pid\": 58212,\n    \"pipeline_name\": \"composition\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"div_two_2\",\n      \"parent\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"div_four\",\n        \"parent\": null\n      }\n    },\n    \"step_handle\": {\n      \"__class__\": \"StepHandle\",\n      \"key\": \"div_four.div_two_2\",\n      \"solid_handle\": {\n        \"__class__\": \"SolidHandle\",\n        \"name\": \"div_two_2\",\n        \"parent\": {\n          \"__class__\": \"SolidHandle\",\n          \"name\": \"div_four\",\n          \"parent\": null\n        }\n      }\n    },\n    \"step_key\": \"div_four.div_two_2\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - div_four.div_two_2 - LOGS_CAPTURED - Started capturing logs for step: div_four.div_two_2.\",\n  \"pipeline_name\": \"composition\",\n  \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\",\n  \"step_key\": \"div_four.div_two_2\",\n  \"timestamp\": 1640037523.273063,\n  \"user_message\": \"Started capturing logs for step: div_four.div_two_2.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dagster Pipeline Step Start Event in JSON Format\nDESCRIPTION: JSON representation of a Dagster step start event record. This event marks the beginning of execution for the 'build_cost_dashboard' step in the longitudinal_pipeline.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Started execution of step \\\"build_cost_dashboard\\\".\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_cost_dashboard - STEP_START - Started execution of step \\\"build_cost_dashboard\\\".\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1608666933.739378, \"user_message\": \"Started execution of step \\\"build_cost_dashboard\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Alembic Version Table in SQL\nDESCRIPTION: This snippet inserts a version number into the 'alembic_version' table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\n3e0770016702\n\\.\n```\n\n----------------------------------------\n\nTITLE: Copying Data into alembic_version Table\nDESCRIPTION: This SQL statement copies data into the `alembic_version` table in the `public` schema. It inserts a single row with the version number 'f4b6a4885876', likely representing a database migration version.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\nf4b6a4885876\n\\.\n```\n\n----------------------------------------\n\nTITLE: Copying Data into bulk_actions Table\nDESCRIPTION: This SQL statement attempts to copy data into the `bulk_actions` table within the `public` schema. The `FROM stdin` clause means the data is provided directly within the statement.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.bulk_actions (id, key, status, \"timestamp\", body) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating snapshots Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'snapshots' table designed to store snapshots of data in bytea format along with relevant metadata. This serves archival and retrieval purposes by ensuring each snapshot can be tracked.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: snapshots; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n\nALTER TABLE public.snapshots OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Pipeline Execution Events in JSON Format\nDESCRIPTION: These JSON objects represent log entries for various events during the execution of a Dagster pipeline. Each entry contains detailed information about the event, including event type, timestamp, step key, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"ObjectStoreOperationResultData\",\n      \"address\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/ingest_traffic/result\",\n      \"metadata_entries\": [\n        {\n          \"__class__\": \"EventMetadataEntry\",\n          \"description\": null,\n          \"entry_data\": {\n            \"__class__\": \"PathMetadataEntryData\",\n            \"path\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/ingest_traffic/result\"\n          },\n          \"label\": \"key\"\n        }\n      ],\n      \"op\": \"SET_OBJECT\",\n      \"value_name\": \"result\",\n      \"version\": null\n    },\n    \"event_type_value\": \"OBJECT_STORE_OPERATION\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\",\n      \"dagster/partition\": \"2020-01-02\",\n      \"dagster/partition_set\": \"ingest_and_train\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"ingest_traffic\",\n      \"solid_definition\": \"base_no_input\",\n      \"step_key\": \"ingest_traffic\"\n    },\n    \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\",\n    \"pid\": 66816,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"ingest_traffic\",\n      \"parent\": null\n    },\n    \"step_key\": \"ingest_traffic\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ingest_traffic - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\",\n  \"step_key\": \"ingest_traffic\",\n  \"timestamp\": 1609894312.7912698,\n  \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Alter normalized_cereals_id_seq Sequence Owner\nDESCRIPTION: This SQL statement changes the owner of the `normalized_cereals_id_seq` sequence to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.normalized_cereals_id_seq OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Default Executor for Dagster Code Location\nDESCRIPTION: This snippet shows how to specify a default executor for all jobs and assets in a Dagster code location using the 'executor' argument of the Definitions object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/run-executors.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n    assets=[asset_1, asset_2],\n    jobs=[job_1, job_2],\n    executor=in_process_executor,\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Database Clone in GitHub Actions Workflow\nDESCRIPTION: GitHub Actions workflow step that triggers the deletion of database clones when a branch is closed or merged.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/branch-deployments/testing.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n    - name: Queue drop_prod_clone job after deployment\\n      env:\\n        DAGSTER_CLOUD_API_TOKEN: ${{ secrets.DAGSTER_CLOUD_API_TOKEN }}\\n      run: |\\n        dagster-cloud job launch drop_prod_clone\n```\n\n----------------------------------------\n\nTITLE: Logging into Vercel CLI\nDESCRIPTION: Command to log into the Vercel CLI for development and deployment purposes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/marketplace/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nvercel login\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Input Event in JSON\nDESCRIPTION: JSON structure of a Dagster event record showing a STEP_INPUT event for a 'build_traffic_dashboard' step, indicating successful type checking of an input named '_'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"_\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"_\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_traffic_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_traffic_dashboard\"}, \"message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_traffic_dashboard\", \"parent\": null}, \"step_key\": \"build_traffic_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - build_traffic_dashboard - STEP_INPUT - Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"build_traffic_dashboard\", \"timestamp\": 1609894320.097025, \"user_message\": \"Got input \\\"_\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Setting PostgreSQL Environment Configuration\nDESCRIPTION: This snippet sets various session-level configuration settings such as statement timeout, lock timeout, client encoding, and message levels to configure the PostgreSQL environment for subsequent SQL commands. The settings are prerequisites for the database dump actions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n```\n\n----------------------------------------\n\nTITLE: Integration Tests Python Dependencies\nDESCRIPTION: This code snippet lists the integration test environments included in the pyright configuration, specifically dagster-k8s-test-infra. Others are commented out, indicating no current requirements documented for their respective test suites.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/pyright/master/requirements.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n-e integration_tests/python_modules/dagster-k8s-test-infra\n# -r integration_tests/test_suites/backcompat-test-suite/requirements.txt  # (no reqs)\n# -r integration_tests/test_suites/celery-k8s-test-suite/requirements.txt  # (no reqs)\n# -r integration_tests/test_suites/daemon-test-suite/requirements.txt  # (no reqs)\n# -r integration_tests/test_suites/k8s-test-suite/requirements.txt  # (no reqs)\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation Snapshots with Tox\nDESCRIPTION: Command to execute the docs_snapshot_update environment in tox to update documentation snippets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets_tests/snippet_checks/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntox -e docs_snapshot_update\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for Event Logs Table\nDESCRIPTION: Creates indexes on asset_key and step_key columns in the event_logs table to improve query performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\n\nCREATE INDEX idx_step_key ON public.event_logs USING btree (step_key);\n```\n\n----------------------------------------\n\nTITLE: Object Store Operation in Dagster Pipeline\nDESCRIPTION: JSON log event recording an object store operation in a Dagster pipeline. The event shows the storage of an intermediate object output to the filesystem using pickle serialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/build_cost_dashboard/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/build_cost_dashboard/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_cost_dashboard - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1608666934.277407, \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Output Event in JSON\nDESCRIPTION: JSON record of a STEP_OUTPUT event in Dagster. This record shows the output 'result' being yielded by the 'do_input.compute' step with a successful type check for type 'Any'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"do_input.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.688006, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Sequence Values for IDs\nDESCRIPTION: Alters tables to use sequences to auto-generate default values for the primary key 'id' field. This ensures ease of inserting new records without manually specifying IDs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.event_logs ALTER COLUMN id SET DEFAULT nextval('public.event_logs_id_seq'::regclass);\n\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n\n\nALTER TABLE ONLY public.schedule_ticks ALTER COLUMN id SET DEFAULT nextval('public.schedule_ticks_id_seq'::regclass);\n\n\nALTER TABLE ONLY public.schedules ALTER COLUMN id SET DEFAULT nextval('public.schedules_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster UI from Command Line\nDESCRIPTION: Command to start the Dagster UI for interacting with and visualizing assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-versioning-and-caching.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Implementing Synced Tabs with Docusaurus\nDESCRIPTION: Shows how to create two sets of synchronized tabs for displaying OS-specific keyboard shortcuts. Uses the groupId parameter to link tab selections between different tab groups, ensuring coordinated tab switching.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/CONTRIBUTING.md#2025-04-22_snippet_15\n\nLANGUAGE: html\nCODE:\n```\n<Tabs groupId=\"operating-systems\">\n  <TabItem value=\"win\" label=\"Windows\">Use Ctrl + C to copy.</TabItem>\n  <TabItem value=\"mac\" label=\"macOS\">Use Command + C to copy.</TabItem>\n</Tabs>\n\n<Tabs groupId=\"operating-systems\">\n  <TabItem value=\"win\" label=\"Windows\">Use Ctrl + V to paste.</TabItem>\n  <TabItem value=\"mac\" label=\"macOS\">Use Command + V to paste.</TabItem>\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Documentation Block for Jaffle Shop Overview in Dagster\nDESCRIPTION: This is a documentation block using the dbt docs syntax to define an overview section for the Jaffle Shop project. It describes the project as a fictional ecommerce store used for testing dbt code and provides a link to the source repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_asset_key_exceptions/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Configuring Job Ticks Table ID Sequence\nDESCRIPTION: Alter table to set default ID generation for job ticks using a sequence\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.job_ticks ALTER COLUMN id SET DEFAULT nextval('public.job_ticks_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to bulk_actions Table in SQL\nDESCRIPTION: Adds a unique constraint on the 'key' column and a primary key constraint on the 'id' column of the bulk_actions table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Creating Table: alembic_version in PostgreSQL\nDESCRIPTION: Defines a table named 'alembic_version' for tracking database schema versioning using Alembic. The 'version_num' column stores the version number as a non-null, variable character string. It is essential for Alembic migrations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.alembic_version (\n    version_num character varying(32) NOT NULL\n);\nALTER TABLE public.alembic_version OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Primary Key Constraints for Dagster Tables\nDESCRIPTION: Adds primary key constraints to the event_logs, run_tags, runs, schedule_ticks, schedules and snapshots tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_8_0_scheduler_update/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.schedule_ticks\n    ADD CONSTRAINT schedule_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.schedules\n    ADD CONSTRAINT schedules_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Documenting Order Status Types in Markdown\nDESCRIPTION: A markdown table that defines the possible status values for orders and their corresponding descriptions. The table covers the complete lifecycle of an order from initial placement through shipping, completion, and potential returns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_asset_checks/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Materializing Looker PDTs from Dagster\nDESCRIPTION: Python code demonstrating how to model Looker PDTs as assets and orchestrate their materialization using Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/looker.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import Definitions, EnvVar\nfrom dagster_looker import LookerResource, RequestStartPdtBuild, build_looker_pdt_assets_definitions\n\nlooker_resource = LookerResource(\n    base_url=EnvVar(\"LOOKER_BASE_URL\"),\n    client_id=EnvVar(\"LOOKER_CLIENT_ID\"),\n    client_secret=EnvVar(\"LOOKER_CLIENT_SECRET\"),\n)\n\npdt_builds = [\n    RequestStartPdtBuild(\"my_model\", \"my_pdt\"),\n    RequestStartPdtBuild(\"another_model\", \"another_pdt\"),\n]\n\npdt_asset_definitions = build_looker_pdt_assets_definitions(pdt_builds)\n\ndefs = Definitions(\n    assets=pdt_asset_definitions,\n    resources={\"looker\": looker_resource},\n)\n```\n\n----------------------------------------\n\nTITLE: Structured Event Log for Dagster Pipeline Execution in JSON\nDESCRIPTION: A series of JSON records representing Dagster pipeline execution events. Each record contains detailed information about step execution status, materializations, expectation results, and outputs for the 'many_events' pipeline with run ID '089287c5-964d-44c0-b727-357eb7ba522e'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_groups\", \"solid_definition\": \"raw_file_groups\", \"step_key\": \"raw_file_groups.compute\"}, \"message\": \"Started execution of step \\\"raw_file_groups.compute\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_groups\", \"name\": \"raw_file_groups\", \"parent\": null}, \"step_key\": \"raw_file_groups.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"raw_file_groups.compute\\\".\\n               solid = \\\"raw_file_groups\\\"\\n    solid_definition = \\\"raw_file_groups\\\"\\n            step_key = \\\"raw_file_groups.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_groups.compute\", \"timestamp\": 1576110683.1815948, \"user_message\": \"Started execution of step \\\"raw_file_groups.compute\\\".\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/raw_file_groups.raw\"}, \"label\": \"table_path\"}]}}, \"event_type_value\": \"STEP_MATERIALIZATION\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_groups\", \"solid_definition\": \"raw_file_groups\", \"step_key\": \"raw_file_groups.compute\"}, \"message\": \"Materialized value table_info.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_groups\", \"name\": \"raw_file_groups\", \"parent\": null}, \"step_key\": \"raw_file_groups.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_MATERIALIZATION - Materialized value table_info.\\n event_specific_data = {\\\"materialization\\\": [\\\"table_info\\\", null, [[\\\"table_path\\\", null, [\\\"/path/to/raw_file_groups.raw\\\"]]]]}}\\n               solid = \\\"raw_file_groups\\\"\\n    solid_definition = \\\"raw_file_groups\\\"\\n            step_key = \\\"raw_file_groups.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_groups.compute\", \"timestamp\": 1576110683.196021, \"user_message\": \"Materialized value table_info.\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Checked raw_file_groups exists\", \"label\": \"output_table_exists\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_groups\", \"solid_definition\": \"raw_file_groups\", \"step_key\": \"raw_file_groups.compute\"}, \"message\": \"Checked raw_file_groups exists\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_groups\", \"name\": \"raw_file_groups\", \"parent\": null}, \"step_key\": \"raw_file_groups.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Checked raw_file_groups exists\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"output_table_exists\\\", \\\"Checked raw_file_groups exists\\\", []]}\\n               solid = \\\"raw_file_groups\\\"\\n    solid_definition = \\\"raw_file_groups\\\"\\n            step_key = \\\"raw_file_groups.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_groups.compute\", \"timestamp\": 1576110683.206023, \"user_message\": \"Checked raw_file_groups exists\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"raw_file_groups.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_groups\", \"solid_definition\": \"raw_file_groups\", \"step_key\": \"raw_file_groups.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_groups\", \"name\": \"raw_file_groups\", \"parent\": null}, \"step_key\": \"raw_file_groups.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"raw_file_groups.compute\\\", \\\"result\\\"], \\\"type_check_data\\\": [true, \\\"result\\\", null, []]}\\n               solid = \\\"raw_file_groups\\\"\\n    solid_definition = \\\"raw_file_groups\\\"\\n            step_key = \\\"raw_file_groups.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_groups.compute\", \"timestamp\": 1576110683.217262, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 30.617904000000085}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_groups\", \"solid_definition\": \"raw_file_groups\", \"step_key\": \"raw_file_groups.compute\"}, \"message\": \"Finished execution of step \\\"raw_file_groups.compute\\\" in 30ms.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_groups\", \"name\": \"raw_file_groups\", \"parent\": null}, \"step_key\": \"raw_file_groups.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_SUCCESS - Finished execution of step \\\"raw_file_groups.compute\\\" in 30ms.\\n event_specific_data = {\\\"duration_ms\\\": 30.617904000000085}\\n               solid = \\\"raw_file_groups\\\"\\n    solid_definition = \\\"raw_file_groups\\\"\\n            step_key = \\\"raw_file_groups.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_groups.compute\", \"timestamp\": 1576110683.225554, \"user_message\": \"Finished execution of step \\\"raw_file_groups.compute\\\" in 30ms.\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_pages\", \"solid_definition\": \"raw_file_pages\", \"step_key\": \"raw_file_pages.compute\"}, \"message\": \"Started execution of step \\\"raw_file_pages.compute\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_pages\", \"name\": \"raw_file_pages\", \"parent\": null}, \"step_key\": \"raw_file_pages.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"raw_file_pages.compute\\\".\\n               solid = \\\"raw_file_pages\\\"\\n    solid_definition = \\\"raw_file_pages\\\"\\n            step_key = \\\"raw_file_pages.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_pages.compute\", \"timestamp\": 1576110683.252584, \"user_message\": \"Started execution of step \\\"raw_file_pages.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Dagster Example\nDESCRIPTION: This snippet installs the necessary Python dependencies for the Dagster example project. It uses pip with the editable flag to install the development dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/with_wandb/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \\\".[dev]\\\"\n```\n\n----------------------------------------\n\nTITLE: Alter runs_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `runs_id_seq` sequence to the `id` column of the `runs` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_39\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Airflow and DBT for the Tutorial\nDESCRIPTION: Command to scaffold the Airflow instance and initialize the dbt project required for the tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/setup.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake airflow_setup\n```\n\n----------------------------------------\n\nTITLE: Initializing Event Logs with Initial Data\nDESCRIPTION: Demonstrates the insertion of initial data into the 'event_logs' table using the COPY command. The data reflects Dagster event records, pre-filled with example entries for testing and validation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key) FROM stdin;\n1\\td5f89349-7477-4fab-913e-0925cef0a959\\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"dagit_subprocess_init\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"[SubprocessExecutionManager] About to start process for pipeline \\\\\"pandas_hello_world_pipeline\\\\\" (run_id: d5f89349-7477-4fab-913e-0925cef0a959).\", \"pipeline_name\": \"pandas_hello_world_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"[SubprocessExecutionManager] About to start process for pipeline \\\\\"pandas_hello_world_pipeline\\\\\" (run_id: d5f89349-7477-4fab-913e-0925cef0a959).\", \"pipeline_name\": \"pandas_hello_world_pipeline\", \"run_id\": \"d5f89349-7477-4fab-913e-0925cef0a959\", \"step_key\": null, \"timestamp\": 1586612015.159553, \"user_message\": \"[SubprocessExecutionManager] About to start process for pipeline \\\\\"pandas_hello_world_pipeline\\\\\" (run_id: d5f89349-7477-4fab-913e-0925cef0a959).\"}\\tENGINE_EVENT\\t2020-04-11 06:33:35.159553\\t\\N\n2\\td5f89349-7477-4fab-913e-0925cef0a959\\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"dagit_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"85555\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"[SubprocessExecutionManager] Started process for pipeline (pid: 85555).\", \"pipeline_name\": \"pandas_hello_world_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"[SubprocessExecutionManager] Started process for pipeline (pid: 85555).\", \"pipeline_name\": \"pandas_hello_world_pipeline\", \"run_id\": \"d5f89349-7477-4fab-913e-0925cef0a959\", \"step_key\": null, \"timestamp\": 1586612016.278537, \"user_message\": \"[SubprocessExecutionManager] Started process for pipeline (pid: 85555).\"}\\tENGINE_EVENT\\t2020-04-11 06:33:36.278537\\t\\N\n3\\td5f89349-7477-4fab-913e-0925cef0a959\\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\\\"pandas_hello_world_pipeline\\\\\".\", \"pipeline_name\": \"pandas_hello_world_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"pandas_hello_world_pipeline - d5f89349-7477-4fab-913e-0925cef0a959 - PIPELINE_START - Started execution of pipeline \\\\\"pandas_hello_world_pipeline\\\\\".\", \"pipeline_name\": \"pandas_hello_world_pipeline\", \"run_id\": \"d5f89349-7477-4fab-913e-0925cef0a959\", \"step_key\": null, \"timestamp\": 1586612017.504948, \"user_message\": \"Started execution of pipeline \\\\\"pandas_hello_world_pipeline\\\\\".\"}\\tPIPELINE_START\\t2020-04-11 06:33:37.504948\\t\\N\n```\n\n----------------------------------------\n\nTITLE: Create Federated Identity Credential\nDESCRIPTION: Azure CLI command to create a federated credential linking the managed identity with the Kubernetes service account.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/blob-compute-logs.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naz identity federated-credential create \\\n  --name dagster-agent-federated-id \\\n  --identity-name agent-identity \\\n  --resource-group <resource-group> \\\n  --issuer $(az aks show -g <resource-group> -n <aks-cluster-name> --query \"oidcIssuerProfile.issuerUrl\" -otsv) \\\n  --subject system:serviceaccount:<dagster-agent-namespace>:<dagster-agent-service-account>\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Cloud Agent Helm Chart\nDESCRIPTION: Installs the Dagster Cloud agent using Helm in the specified namespace.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nhelm --namespace dagster-cloud upgrade --install agent dagster-cloud/dagster-cloud-agent\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Params Loader for Dagster Pipes\nDESCRIPTION: Example of creating a custom params loader that reads from a cloud service metadata store.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/dagster-pipes-details-and-customization.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom cloud_service import METADATA\nfrom dagster_pipes import PipesParams, PipesParamsLoader\n\nclass CloudServiceParamsLoader(PipesParamsLoader):\n    def load_params(self) -> PipesParams:\n        params_dict = {\n            \"context_path\": METADATA[\"context_path\"],\n            \"message_path\": METADATA[\"message_path\"],\n        }\n        return PipesParams.from_dict(params_dict)\n```\n\n----------------------------------------\n\nTITLE: Populating Event Log Table with Data\nDESCRIPTION: Inserts sample data into the 'event_log' table, demonstrating typical entries including serialized JSON for event data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.event_log (id, run_id, event_body) FROM stdin;\n1\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"PipelineProcessStartData\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\"}, \"event_type_value\": \"PIPELINE_PROCESS_START\", \"logging_tags\": {}, \"message\": \"About to start process for pipeline \\\"many_events\\\" (run_id: 089287c5-964d-44c0-b727-357eb7ba522e).\", \"pipeline_name\": \"many_events\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"About to start process for pipeline \\\"many_events\\\" (run_id: 089287c5-964d-44c0-b727-357eb7ba522e).\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": null, \"timestamp\": 1576110679.302774, \"user_message\": \"About to start process for pipeline \\\"many_events\\\" (run_id: 089287c5-964d-44c0-b727-357eb7ba522e).\"}\n2\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"PipelineProcessStartedData\", \"pipeline_name\": \"many_events\", \"process_id\": 22892, \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\"}, \"event_type_value\": \"PIPELINE_PROCESS_STARTED\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 22892).\", \"pipeline_name\": \"many_events\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 22892).\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": null, \"timestamp\": 1576110681.292107, \"user_message\": \"Started process for pipeline (pid: 22892).\"}\n3\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\"many_events\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - PIPELINE_START - Started execution of pipeline \\\"many_events\\\".\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": null, \"timestamp\": 1576110682.734879, \"user_message\": \"Started execution of pipeline \\\"many_events\\\".\"}\n4\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"22892\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"{'many_materializations_and_passing_expectations.compute', 'raw_file_friends.compute', 'raw_file_fans.compute', 'check_admins_both_succeed.compute', 'raw_file_pages.compute', 'many_table_materializations.compute', 'raw_file_users.compute', 'raw_file_group_admins.compute', 'raw_file_event_admins.compute', 'check_users_and_groups_one_fails_one_succeeds.compute', 'raw_file_events.compute', 'raw_file_groups.compute'}\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps in process (pid: 22892)\", \"pipeline_name\": \"many_events\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - ENGINE_EVENT - Executing steps in process (pid: 22892)\\n event_specific_data = {\\\"metadata_entries\\\": [[\\\"pid\\\", null, [\\\"22892\\\"]], [\\\"step_keys\\\", null, [\\\"{'many_materializations_and_passing_expectations.compute', 'raw_file_friends.compute', 'raw_file_fans.compute', 'check_admins_both_succeed.compute', 'raw_file_pages.compute', 'many_table_materializations.compute', 'raw_file_users.compute', 'raw_file_group_admins.compute', 'raw_file_event_admins.compute', 'check_users_and_groups_one_fails_one_succeeds.compute', 'raw_file_events.compute', 'raw_file_groups.compute'}\\\"]]]}\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": null, \"timestamp\": 1576110682.751248, \"user_message\": \"Executing steps in process (pid: 22892)\"}\n5\t089287c5-964d-44c0-b727-357eb7ba522e\t{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_event_admins\", \"solid_definition\": \"raw_file_event_admins\", \"step_key\": \"raw_file_event_admins.compute\"}, \"message\": \"Started execution of step \\\"raw_file_event_admins.compute\\\".\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_event_admins\", \"name\": \"raw_file_event_admins\", \"parent\": null}, \"step_key\": \"raw_file_event_admins.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"raw_file_event_admins.compute\\\".\\n               solid = \\\"raw_file_event_admins\\\"\\n    solid_definition = \\\"raw_file_event_admins\\\"\\n            step_key = \\\"raw_file_event_admins.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_event_admins.compute\", \"timestamp\": 1576110682.76989, \"user_message\": \"Started execution of step \\\"raw_file_event_admins.compute\\\".\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring CI/CD Workflow for Multiple Projects\nDESCRIPTION: GitHub Actions workflow configuration for deploying multiple code locations in Dagster+ Hybrid. Shows how to set up build outputs for different projects within the same repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/managing-multiple-projects-and-teams.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# .github/workflows/dagster-cloud-deploy.yml\n\njobs:\n  dagster-cloud-deploy:\n    # ...\n    steps:\n      - name: Update build session with image tag for \"project_a\" code location\n        id: ci-set-build-output-project-a\n        if: steps.prerun.outputs.result != 'skip'\n        uses: dagster-io/dagster-cloud-action/actions/utils/dagster-cloud-cli@v0.1\n        with:\n          command: 'ci set-build-output --location-name=project_a --image-tag=$IMAGE_TAG'\n\n      - name: Update build session with image tag for \"project_b\" code location\n        id: ci-set-build-output-project-b\n        if: steps.prerun.outputs.result != 'skip'\n        uses: dagster-io/dagster-cloud-action/actions/utils/dagster-cloud-cli@v0.1\n        with:\n          command: 'ci set-build-output --location-name=project_b --image-tag=$IMAGE_TAG'\n      # ...\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to jobs Table in SQL\nDESCRIPTION: Adds a unique constraint on the job_origin_id column and a primary key constraint on the id column of the jobs table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Docker Agents\nDESCRIPTION: Docker Compose configuration to set up multiple agent replicas in the same environment using replicated deployment mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/multiple.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  dagster-cloud-agent:\n    ...\n    deploy:\n      mode: replicated\n      replicas: 2\n```\n\n----------------------------------------\n\nTITLE: Importing MwaaSessionAuthBackend in Python\nDESCRIPTION: Imports the MwaaSessionAuthBackend class from the dagster_airlift.mwaa module. This class is likely used for authentication with Amazon MWAA (Managed Workflows for Apache Airflow).\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-airlift.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_airlift.mwaa import MwaaSessionAuthBackend\n```\n\n----------------------------------------\n\nTITLE: Run Coordinator Configuration in YAML\nDESCRIPTION: Configuration for the queued run coordinator with concurrency limits in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-yaml.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrun_coordinator:\n  module: dagster.core.run_coordinator\n  class: QueuedRunCoordinator\n  config:\n    max_concurrent_runs: 25\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Version Constraints for Dagster Project\nDESCRIPTION: This snippet defines version constraints for multiple Python packages used in the Dagster project. It includes pins for packages like markupsafe, nbformat, protobuf, grpcio, sqlalchemy, and pendulum to maintain compatibility and avoid known issues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/webserver_service/pins.txt#2025-04-22_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# pins for recently released packages that cause problems in older dagster versions\nmarkupsafe<=2.0.1\n\n# 5.2+ stops pulling in `ipython_genutils`, on which the old version of `nbconvert` we use\n# implicitly depends. Can remove this pin when/if dagit cap on nbconvert is lifted.\nnbformat<=5.1.3\n\n# protobuf 4 retroactively breaks old versions of dagster\nprotobuf>=3.13.0,<4\n\n# Deadlock / hang issue in new version of grpc\ngrpcio<1.48.1; python_version < '3.10'\n\n# Added sqlalchemy pins in later versions\nsqlalchemy<2.0.0\n\n# Added pendulum pin in later versions\npendulum<3\n```\n\n----------------------------------------\n\nTITLE: Defining Run Config Schema for Dagster Jobs\nDESCRIPTION: This code snippet outlines the schema for the run_config used in Dagster jobs. It includes sections for execution, loggers, resources, and ops configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/execution.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n  # configuration for execution, required if executors require config\n  execution: {\n    # the name of one, and only one available executor, typically 'in_process' or 'multiprocess'\n    __executor_name__: {\n      # executor-specific config, if required or permitted\n      config: {\n        ...\n      }\n    }\n  },\n\n  # configuration for loggers, required if loggers require config\n  loggers: {\n    # the name of an available logger\n    __logger_name__: {\n      # logger-specific config, if required or permitted\n      config: {\n        ...\n      }\n    },\n    ...\n  },\n\n  # configuration for resources, required if resources require config\n  resources: {\n    # the name of a resource\n    __resource_name__: {\n      # resource-specific config, if required or permitted\n      config: {\n        ...\n      }\n    },\n    ...\n  },\n\n  # configuration for underlying ops, required if ops require config\n  ops: {\n\n    # these keys align with the names of the ops, or their alias in this job\n    __op_name__: {\n\n      # pass any data that was defined via config_field\n      config: ...,\n\n      # configurably specify input values, keyed by input name\n      inputs: {\n        __input_name__: {\n          # if an dagster_type_loader is specified, that schema must be satisfied here;\n          # scalar, built-in types will generally allow their values to be specified directly:\n          value: ...\n        }\n      },\n\n    }\n  },\n\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Default Value for snapshots id Column\nDESCRIPTION: This SQL statement configures the `id` column in the `snapshots` table to automatically generate unique IDs based on the `public.snapshots_id_seq` sequence.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.snapshots ALTER COLUMN id SET DEFAULT nextval('public.snapshots_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Copying Data to Alembic Version Table in PostgreSQL\nDESCRIPTION: SQL statement to copy migration version data into the alembic_version table. It includes a migration version identifier used for tracking database schema changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\n7cba9eeaaf1d\n\\.\n```\n\n----------------------------------------\n\nTITLE: Creating Bulk Actions Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the key column of the bulk_actions table to optimize queries that filter by this column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_48\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\n```\n\n----------------------------------------\n\nTITLE: Azure Resource Definitions Documentation\nDESCRIPTION: ReStructuredText documentation for Azure resources including ADLS2Resource, FakeADLS2Resource, and AzureBlobStorageResource implementations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-azure.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: dagster_azure.adls2.ADLS2Resource\n  :annotation: ResourceDefinition\n\n.. autoconfigurable:: dagster_azure.fakes.FakeADLS2Resource\n    :annotation: ResourceDefinition\n\n.. autoconfigurable:: dagster_azure.blob.AzureBlobStorageResource\n    :annotation: ResourceDefinition\n\n.. autoclass:: dagster_azure.blob.AzureBlobComputeLogManager\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Model Fitting\nDESCRIPTION: This snippet extracts features and target variables from the DataFrame for machine learning: 'sepal_length' and 'sepal_width' are features; 'petal_width' is the target.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nX = df[[\"sepal_length\", \"sepal_width\"]]\ny = df[\"petal_width\"]\n```\n\n----------------------------------------\n\nTITLE: Rendering Document Card List Component in JSX\nDESCRIPTION: Imports and renders a DocCardList component, likely used to display a list of related documentation pages about tokens.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/tokens/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Starting Dagster for ATProto Dashboard\nDESCRIPTION: This command starts the Dagster development server. It sets the DAGSTER_HOME environment variable to the current directory before running dagster dev.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_atproto_dashboard/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDAGSTER_HOME=$(pwd) dagster dev\n```\n\n----------------------------------------\n\nTITLE: Running Dagster for Kitchen Sink Project in Bash\nDESCRIPTION: This command starts the Dagster instance for the Kitchen Sink project. It should be run in another shell, separate from the Airflow instance, after the installation and setup steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/dagster-dlift/kitchen-sink/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake run_dagster\n```\n\n----------------------------------------\n\nTITLE: Displaying Directory Tree Structure\nDESCRIPTION: Shows the hierarchical file and folder structure of a Dagster project, including the main project directory, analytics module, and ELT module with their respective assets and jobs files.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-definitions/5-tree-after.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── README.md\n├── my_existing_project\n│   ├── __init__.py\n│   ├── analytics\n│   │   ├── __init__.py\n│   │   ├── assets.py\n│   │   └── jobs.py\n│   ├── definitions.py\n│   └── defs\n│       ├── __init__.py\n│       └── elt\n│           ├── __init__.py\n│           ├── assets.py\n│           └── jobs.py\n├── pyproject.toml\n└── uv.lock\n\n5 directories, 12 files\n```\n\n----------------------------------------\n\nTITLE: Running Warehouse Airflow Instance\nDESCRIPTION: Starts the \"upstream\" Airflow instance with required environment variables, accessible on port 8081.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake warehouse_airflow_run\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry for STEP_INPUT Event\nDESCRIPTION: JSON representation of a Dagster event log entry for a STEP_INPUT event. This entry confirms the 'num' input for the 'div_two' step passed type checking as a Float value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_45\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"num\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"num\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\"pipeline_name\": \"composition\", \"pipeline_tags\": \"{'.dagster/grpc_info': '{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmp9y0m3f2y\\\"}', 'dagster/solid_selection': '*'}\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"solid_name\": \"div_two\", \"step_key\": \"div_four.div_two\"}, \"message\": \"Got input \\\"num\\\" of type \\\"Float\\\". (Type check passed).\", \"pid\": 58212, \"pipeline_name\": \"composition\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}, \"step_handle\": {\"__class__\": \"StepHandle\", \"key\": \"div_four.div_two\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"div_two\", \"parent\": {\"__class__\": \"SolidHandle\", \"name\": \"div_four\", \"parent\": null}}}, \"step_key\": \"div_four.div_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"composition - f1e0df44-6395-4e6e-aaf3-d867a26e3662 - 58212 - div_four.div_two - STEP_INPUT - Got input \\\"num\\\" of type \\\"Float\\\". (Type check passed).\", \"pipeline_name\": \"composition\", \"run_id\": \"f1e0df44-6395-4e6e-aaf3-d867a26e3662\", \"step_key\": \"div_four.div_two\", \"timestamp\": 1640037523.140536, \"user_message\": \"Got input \\\"num\\\" of type \\\"Float\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Primary Key Constraint on secondary_indexes.id\nDESCRIPTION: This SQL statement defines the primary key for the `secondary_indexes` table, using the `id` column. A primary key ensures uniqueness and serves as a unique identifier for each row in the table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\n\"ALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\"\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: asset_event_tags_id_seq in PostgreSQL\nDESCRIPTION: Creates a sequence 'asset_event_tags_id_seq' to auto-generate unique identifiers for the 'id' column in the 'asset_event_tags' table. Starts at 1, increments by 1 with a cache of 1. Used for ensuring unique IDs for asset event tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.asset_event_tags_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.asset_event_tags_id_seq OWNER TO test;\nALTER SEQUENCE public.asset_event_tags_id_seq OWNED BY public.asset_event_tags.id;\n```\n\n----------------------------------------\n\nTITLE: Importing Dagstermill and Pandas Libraries\nDESCRIPTION: Imports necessary libraries for data processing and Dagster integration. Provides essential dependencies for data manipulation and workflow management.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-pandas/dagster_pandas/examples/notebooks/papermill_pandas_hello_world.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dagstermill\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Running Tests with pytest for Snowflake Quickstart\nDESCRIPTION: Command to run tests for the Snowflake quickstart project using pytest. The tests are located in the quickstart_snowflake_tests directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/quickstart_snowflake/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npytest quickstart_snowflake_tests\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Job Ticks by Job Origin ID in SQL\nDESCRIPTION: Creates a B-tree index on the job_ticks table to optimize queries filtering by job_origin_id.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_54\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Scaling Dagster Components Back Up\nDESCRIPTION: These commands scale the Dagster webserver and daemon deployments back up to their original replica counts after the migration is complete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/migrating-while-upgrading.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl scale deploy $WEBSERVER_DEPLOYMENT_NAME --replicas=$WEBSERVER_DEPLOYMENT_REPLICA_COUNT\nkubectl scale deploy $DAEMON_DEPLOYMENT_NAME --replicas=$DAEMON_DEPLOYMENT_REPLICA_COUNT\n```\n\n----------------------------------------\n\nTITLE: Importing and Using DocCardList Component in JSX\nDESCRIPTION: This code snippet imports the DocCardList component from the theme and renders it in JSX. This component likely displays a list of documentation cards related to single sign-on functionality.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/authentication-and-access-control/sso/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Order Status Definition Table in Markdown\nDESCRIPTION: A markdown table defining the various order statuses and their descriptions. The table includes five possible statuses from initial placement to eventual return, with detailed explanations of what each status represents in the order lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_mixed_freshness/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n```\n\n----------------------------------------\n\nTITLE: Requesting resources for a code location\nDESCRIPTION: YAML configuration in dagster_cloud.yaml for requesting specific CPU, memory, and GPU resources for both server and run pods in a code location.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/kubernetes/setup.md#2025-04-22_snippet_23\n\nLANGUAGE: yaml\nCODE:\n```\nlocations:\n  - location_name: cloud-examples\n    image: dagster/dagster-cloud-examples:latest\n    code_source:\n      package_name: dagster_cloud_examples\n    container_context:\n      k8s:\n        server_k8s_config:\n          container_config:\n            resources:\n              limits:\n                cpu: 500m\n                memory: 2560Mi\n        run_k8s_config:\n          container_config:\n            resources:\n              limits:\n                cpu: 500m\n                memory: 2560Mi\n                nvidia.com/gpu: 1\n```\n\n----------------------------------------\n\nTITLE: Referencing Dagster-Dask Documentation\nDESCRIPTION: This snippet provides a link to the official documentation for the dagster-dask integration library. The documentation contains comprehensive information on how to use Dask's distributed computing capabilities with Dagster workflows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dask/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-dask\n\nThe docs for `dagster-dask` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-dask).\n```\n\n----------------------------------------\n\nTITLE: Launch Run Mutation in GraphQL\nDESCRIPTION: GraphQL mutation to initiate a new Dagster run, requiring repository information, job name, and run configuration data as parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/index.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmutation LaunchRunMutation(\n  $repositoryLocationName: String!\n  $repositoryName: String!\n  $jobName: String!\n  $runConfigData: RunConfigData!\n) {\n  launchRun(\n    executionParams: {\n      selector: {\n        repositoryLocationName: $repositoryLocationName\n        repositoryName: $repositoryName\n        jobName: $jobName\n      }\n      runConfigData: $runConfigData\n    }\n  ) {\n    __typename\n    ... on LaunchRunSuccess {\n      run {\n        runId\n      }\n    }\n    ... on RunConfigValidationInvalid {\n      errors {\n        message\n        reason\n      }\n    }\n    ... on PythonError {\n      message\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Required AWS Environment Variables\nDESCRIPTION: Environment variables that must be set to run integration tests locally with AWS credentials\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/python_modules/dagster-k8s-test-infra/README.md#2025-04-22_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nAWS_ACCOUNT_ID\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\n```\n\n----------------------------------------\n\nTITLE: Inserting Pipeline Run Data in SQL\nDESCRIPTION: SQL COPY statement to insert data into the 'public.runs' table. It includes detailed information about pipeline runs, including run configurations, statuses, and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.runs (id, run_id, snapshot_id, pipeline_name, status, run_body, create_timestamp, update_timestamp, partition, partition_set, mode, start_time, end_time, backfill_id) FROM stdin;\n1\t8c59d1b7-1841-4c60-8346-89e298dbf743\t47992fd61f2aecc862f741e1355e2b910d5850de\tfoo\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"53f52f323e0e20f0b8ec27a3d698335b3e0918fb\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": \"bar\", \"executable_path\": \"/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\", \"module_name\": null, \"package_name\": null, \"python_file\": \"/Users/dgibson/dagster/python_modules/dagit/dagit_tests/toy/bar_repo.py\", \"working_directory\": \"/Users/dgibson/dagster-home\"}, \"location_name\": \"bar_repo.py:bar\"}, \"repository_name\": \"bar\"}, \"pipeline_name\": \"foo\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"foo\", \"pipeline_snapshot_id\": \"47992fd61f2aecc862f741e1355e2b910d5850de\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpOhX8MX\\\"}\", \"dagster/schedule_name\": \"foo_schedule\"}}\t2021-01-12 10:40:02.568045\t2021-01-12 09:40:02.973559\t\\N\t\\N\t\\N\t\\N\t\\N\t\\N\n2\td45fa10b-121d-4051-9358-cfd3dfcbc878\t47992fd61f2aecc862f741e1355e2b910d5850de\tfoo\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"53f52f323e0e20f0b8ec27a3d698335b3e0918fb\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": \"bar\", \"executable_path\": \"/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\", \"module_name\": null, \"package_name\": null, \"python_file\": \"/Users/dgibson/dagster/python_modules/dagit/dagit_tests/toy/bar_repo.py\", \"working_directory\": \"/Users/dgibson/dagster-home\"}, \"location_name\": \"bar_repo.py:bar\"}, \"repository_name\": \"bar\"}, \"pipeline_name\": \"foo\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"foo\", \"pipeline_snapshot_id\": \"47992fd61f2aecc862f741e1355e2b910d5850de\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\"}}\t2021-01-12 10:41:03.336991\t2021-01-12 09:41:03.737958\t\\N\t\\N\t\\N\t\\N\t\\N\t\\N\n3\t9f130936-1409-4bd0-b15d-67cf6e7a67ec\t47992fd61f2aecc862f741e1355e2b910d5850de\tfoo\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"execution_plan_snapshot_id\": \"53f52f323e0e20f0b8ec27a3d698335b3e0918fb\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": \"bar\", \"executable_path\": \"/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\", \"module_name\": null, \"package_name\": null, \"python_file\": \"/Users/dgibson/dagster/python_modules/dagit/dagit_tests/toy/bar_repo.py\", \"working_directory\": \"/Users/dgibson/dagster-home\"}, \"location_name\": \"bar_repo.py:bar\"}, \"repository_name\": \"bar\"}, \"pipeline_name\": \"foo\"}, \"mode\": \"default\", \"parent_run_id\": null, \"pipeline_name\": \"foo\", \"pipeline_snapshot_id\": \"47992fd61f2aecc862f741e1355e2b910d5850de\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": null, \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\"}}\t2021-01-12 10:42:03.334236\t2021-01-12 09:42:03.702535\t\\N\t\\N\t\\N\t\\N\t\\N\t\\N\n4\t4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\t75080c9f16819e52f57f50a6fad451ba486ee568\tbasic_assets_job\tSUCCESS\t{\"__class__\": \"PipelineRun\", \"asset_check_selection\": {\"__frozenset__\": []}, \"asset_selection\": {\"__frozenset__\": [{\"__class__\": \"AssetKey\", \"path\": [\"basic_asset_1\"]}]}, \"execution_plan_snapshot_id\": \"03002c9d8db5df9d5ce603d57fa7fdda258054b1\", \"external_pipeline_origin\": {\"__class__\": \"ExternalPipelineOrigin\", \"external_repository_origin\": {\"__class__\": \"ExternalRepositoryOrigin\", \"repository_location_origin\": {\"__class__\": \"ManagedGrpcPythonEnvRepositoryLocationOrigin\", \"loadable_target_origin\": {\"__class__\": \"LoadableTargetOrigin\", \"attribute\": null, \"executable_path\": null, \"module_name\": \"dagster_test.toys.repo\", \"package_name\": null, \"python_file\": null, \"working_directory\": \"/Users/hynekblaha/dagster\"}, \"location_name\": \"dagster_test.toys.repo\"}, \"repository_name\": \"basic_assets_repository\"}, \"pipeline_name\": \"basic_assets_job\"}, \"has_repository_load_data\": false, \"mode\": null, \"parent_run_id\": null, \"pipeline_code_origin\": {\"__class__\": \"PipelinePythonOrigin\", \"pipeline_name\": \"basic_assets_job\", \"repository_origin\": {\"__class__\": \"RepositoryPythonOrigin\", \"code_pointer\": {\"__class__\": \"ModuleCodePointer\", \"fn_name\": \"basic_assets_repository\", \"module\": \"dagster_test.toys.repo\", \"working_directory\": \"/Users/hynekblaha/dagster\"}, \"container_context\": {}, \"container_image\": null, \"entry_point\": [\"dagster\"], \"executable_path\": \"/Users/hynekblaha/dagster/venv/bin/python3\"}}, \"pipeline_name\": \"basic_assets_job\", \"pipeline_snapshot_id\": \"75080c9f16819e52f57f50a6fad451ba486ee568\", \"root_run_id\": null, \"run_config\": {}, \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"run_op_concurrency\": null, \"solid_selection\": null, \"solids_to_execute\": null, \"status\": {\"__enum__\": \"PipelineRunStatus.SUCCESS\"}, \"step_keys_to_execute\": [\"basic_asset_1\"], \"tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/zt/9txyngt92fxgpd954jkfz6zm0000gn/T/tmpmc6skfzp\\\"}\"}}}\t2024-11-15 18:00:46.162863\t2024-11-15 18:00:52.293929\t\\N\t\\N\t\\N\t1731664849.31304693\t1731664852.2939291\t\\N\n\\.\n```\n\n----------------------------------------\n\nTITLE: Referring to Dagster-Celery Documentation in Markdown\nDESCRIPTION: A markdown link pointing to the official documentation for the dagster-celery package, which is part of the Dagster Python API libraries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-celery/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# dagster-celery\n\nThe docs for `dagster-celery` can be found\n[here](https://docs.dagster.io/api/python-api/libraries/dagster-celery).\n```\n\n----------------------------------------\n\nTITLE: Configuring Robot Access for All User Agents in robots.txt\nDESCRIPTION: This snippet defines the robot access rules for the website. It allows all user agents (crawlers) to access all parts of the site without any restrictions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/app-oss/public/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: robotstxt\nCODE:\n```\n# https://www.robotstxt.org/robotstxt.html\nUser-agent: *\nDisallow:\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for event_logs_id_seq in PostgreSQL\nDESCRIPTION: This command initializes the 'event_logs_id_seq' to 16 and marks it as true, indicating that IDs have already been used to avoid conflicts in the future.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.event_logs_id_seq', 16, true);\n```\n\n----------------------------------------\n\nTITLE: Setting Default ID Values for Run Tags in SQL\nDESCRIPTION: This snippet alters the 'run_tags' table to set its 'id' column's default value to be the next in sequence from 'public.run_tags_id_seq', automating the ID assignment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: run_tags id; Type: DEFAULT; Schema: public; Owner: test\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Markdown Page Frontmatter for Hybrid Agent Settings\nDESCRIPTION: YAML frontmatter defining the page title, sidebar position, and visibility status for the hybrid agent settings documentation page. The page is currently marked as unlisted.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/settings/hybrid-agent-settings.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: 'Hybrid agent settings'\nsidebar_position: 400\nunlisted: true\n---\n```\n\n----------------------------------------\n\nTITLE: Creating run_tags_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'run_tags_id_seq' sequence for generating unique identifiers for the 'run_tags' table entries, enabling automated ID assignments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: run_tags_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Displaying Beta Feature Warning in Markdown\nDESCRIPTION: This code snippet shows how to display a warning message for a beta feature using custom markdown syntax. It highlights that the feature is still being tested and may change.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_Beta.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::warning\nThis feature is considered in a beta stage. It is still being tested and may change.\n:::\n```\n\n----------------------------------------\n\nTITLE: Creating ECR Repositories\nDESCRIPTION: AWS CLI commands to create ECR repositories for webserver, daemon, and user code images.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/deploy_ecs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\naws ecr create-repository --repository-name deploy_ecs/webserver\naws ecr create-repository --repository-name deploy_ecs/daemon\naws ecr create-repository --repository-name deploy_ecs/user_code\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for bulk_actions_id_seq in PostgreSQL\nDESCRIPTION: Sets the sequence 'bulk_actions_id_seq' to start at 1 for future inserts. This is necessary to maintain order and prevent ID collisions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.bulk_actions_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: Defines the page title and sidebar position using YAML frontmatter in markdown.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/index.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: 'Incrementally adopting dg'\nsidebar_position: 400\n---\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Log Entry - Step Output\nDESCRIPTION: JSON log entry showing successful output from a Dagster pipeline step 'basic_asset_1' with type checking and metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"metadata_entries\": [], \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"basic_asset_1\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"job_name\": \"basic_assets_job\", \"op_name\": \"basic_asset_1\", \"resource_fn_name\": \"None\", \"resource_name\": \"None\", \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"step_key\": \"basic_asset_1\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}, \"error_info\": null, \"level\": 10, \"message\": \"\", \"pipeline_name\": \"basic_assets_job\", \"run_id\": \"4e37f0ab-1cd4-4af7-9c5a-76bb1f526d54\", \"step_key\": \"basic_asset_1\", \"timestamp\": 1731664851.952908, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark UI Filters Example\nDESCRIPTION: Example showing how to configure Spark UI filters using configuration properties. Demonstrates setting up a custom filter class with parameters.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_31\n\nLANGUAGE: properties\nCODE:\n```\nspark.ui.filters=com.test.filter1\nspark.com.test.filter1.param.name1=foo\nspark.com.test.filter1.param.name2=bar\n```\n\n----------------------------------------\n\nTITLE: Engine Subprocess Initialization Event in Dagster\nDESCRIPTION: JSON log event for the initialization of a subprocess engine in a Dagster pipeline. The event captures information about the started process including its process ID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"18688\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 18688).\", \"pid\": null, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 18688).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": null, \"timestamp\": 1608666900.123899, \"user_message\": \"Started process for pipeline (pid: 18688).\"}\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Output Event for Raw File Fans Step\nDESCRIPTION: JSON representation of a DagsterEventRecord for a STEP_OUTPUT event in the 'many_events' pipeline. This record indicates successful output generation from the 'raw_file_fans' step with an 'Any' type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"result\", \"step_key\": \"raw_file_fans.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_fans\", \"solid_definition\": \"raw_file_fans\", \"step_key\": \"raw_file_fans.compute\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_fans\", \"name\": \"raw_file_fans\", \"parent\": null}, \"step_key\": \"raw_file_fans.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\\n event_specific_data = {\\\"intermediate_materialization\\\": null, \\\"step_output_handle\\\": [\\\"raw_file_fans.compute\\\", \\\"result\\\"], \\\"type_check_data\\\": [true, \\\"result\\\", null, []]}\\n               solid = \\\"raw_file_fans\\\"\\n    solid_definition = \\\"raw_file_fans\\\"\\n            step_key = \\\"raw_file_fans.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_fans.compute\", \"timestamp\": 1576110682.947354, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Documenting 'greet' Command with Sphinx Click Directive\nDESCRIPTION: This snippet uses Sphinx's click directive to generate documentation for the 'greet' command. It specifies the command module, program name, and available subcommands.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/_ext/sphinx-click/tests/roots/commands/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: greet:greet\n   :prog: greet\n   :commands: world\n```\n\n----------------------------------------\n\nTITLE: Create run_tags_id_seq Sequence\nDESCRIPTION: This SQL statement creates a sequence named `run_tags_id_seq` to generate unique IDs for the `run_tags` table. It configures the sequence to start at 1, increment by 1, and have no minimum or maximum value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\"\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster Module in Python\nDESCRIPTION: Imports the Dagster module to access its internal APIs and components. This is typically used at the beginning of scripts or modules that work with Dagster internals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Webserver\nDESCRIPTION: This command starts the Dagster webserver, allowing you to view and interact with the example project through Dagster's user interface.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/feature_graph_backed_assets/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Alter jobs_id_seq Sequence Owned By\nDESCRIPTION: This SQL statement links the `jobs_id_seq` sequence to the `id` column of the `jobs` table. This ensures that the sequence is used to generate default values for the ID column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER SEQUENCE public.jobs_id_seq OWNED BY public.jobs.id;\"\n```\n\n----------------------------------------\n\nTITLE: Importing Dagster IO Manager Module\nDESCRIPTION: This code snippet shows how to import the Dagster module for IO Managers. It sets the current module to 'dagster' for the subsequent class and function definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/io-managers.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster\n```\n\n----------------------------------------\n\nTITLE: Creating public.asset_keys Table and Sequence\nDESCRIPTION: This snippet creates the 'asset_keys' table in the 'public' schema for storing information about Dagster assets.  It includes columns for asset key, last materialization, run ID, details, timestamps, and tags. An associated sequence 'asset_keys_id_seq' is also created to automatically generate unique IDs for each asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    wipe_timestamp timestamp without time zone,\n    last_materialization_timestamp timestamp without time zone,\n    tags text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.asset_keys OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\n--\n\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n\n--\n-- Name: asset_keys_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: test\n--\n\nALTER SEQUENCE public.asset_keys_id_seq OWNED BY public.asset_keys.id;\"\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Development Server\nDESCRIPTION: Launches the Dagster development server to view and interact with the project through the web UI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/assets_yaml_dsl/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev\n```\n\n----------------------------------------\n\nTITLE: Defining Content Security Policy Directives\nDESCRIPTION: This snippet defines a Content Security Policy with specific directives for base-uri, object-src, script-src, and style-src. It allows scripts from various sources and inline styles.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/js_modules/dagster-ui/packages/app-oss/csp-header-dev.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbase-uri 'none'; object-src 'none'; script-src 'nonce-NONCE-PLACEHOLDER' 'unsafe-eval' 'unsafe-inline' https: http: 'strict-dynamic'; style-src 'unsafe-inline' 'self' 'unsafe-eval';\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in JSX\nDESCRIPTION: React/JSX code that imports and renders a DocCardList component for displaying documentation cards in a sidebar layout. The component is imported from a theme package and used to organize related documentation content.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/features/ci-cd/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Delta Table Resource Configuration\nDESCRIPTION: Configuration definition for the DeltaTableResource class which provides Delta Lake table resources in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoconfigurable:: DeltaTableResource\n  :annotation: ResourceDefinition\n```\n\n----------------------------------------\n\nTITLE: Alter job_ticks Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `job_ticks` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.job_ticks OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Importing Serdes Module in Python\nDESCRIPTION: Imports the serdes module from Dagster. This module likely contains serialization and deserialization utilities for Dagster objects.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._serdes\n```\n\n----------------------------------------\n\nTITLE: Creating secondary_indexes Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'secondary_indexes' table used for managing secondary indexing strategies with timestamps for creation and migration completion. This assists in optimizing data retrieval processes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: secondary_indexes; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n\nALTER TABLE public.secondary_indexes OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access with robots.txt\nDESCRIPTION: This robots.txt file instructs all web crawlers to access any part of the website and provides the sitemap location at docs.dagster.io/sitemap.xml. It uses standard robots.txt syntax for search engine optimization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/static/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nUser-agent: *\nAllow: /\nSitemap: https://docs.dagster.io/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: Defining Order Statuses using Markdown Table\nDESCRIPTION: This snippet uses a markdown table to define and describe the possible statuses for orders in the system. It includes five statuses: placed, shipped, completed, return_pending, and returned, each with a detailed description of what that status means in the order lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_semantic_models/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n```\n\n----------------------------------------\n\nTITLE: Inserting Event Log Data into event_logs Table in PostgreSQL\nDESCRIPTION: Inserts event log entries into the event_logs table including pipeline execution events with JSON-structured metadata, timestamps, and event types for tracking Dagster workflow execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.event_logs (id, run_id, event, dagster_event_type, \"timestamp\", step_key, asset_key, partition) FROM stdin;\n144\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_STARTING\", \"logging_tags\": {}, \"message\": null, \"pid\": null, \"pipeline_name\": \"model_pipeline\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"\", \"pipeline_name\": \"model_pipeline\", \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\", \"step_key\": null, \"timestamp\": 1625760604.543912, \"user_message\": \"\"}\tPIPELINE_STARTING\t2021-07-08 16:10:04.543912\t\\N\t\\N\t\\N\n145\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": \"cli_api_subprocess_init\", \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"1295\"}, \"label\": \"pid\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Started process for pipeline (pid: 1295).\", \"pid\": null, \"pipeline_name\": \"model_pipeline\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 20, \"message\": \"Started process for pipeline (pid: 1295).\", \"pipeline_name\": \"model_pipeline\", \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\", \"step_key\": null, \"timestamp\": 1625760606.400359, \"user_message\": \"Started process for pipeline (pid: 1295).\"}\tENGINE_EVENT\t2021-07-08 16:10:06.400359\t\\N\t\\N\t\\N\n146\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_START\", \"logging_tags\": {}, \"message\": \"Started execution of pipeline \\\"model_pipeline\\\".\", \"pid\": 1295, \"pipeline_name\": \"model_pipeline\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"model_pipeline - 1399fa66-f129-46ad-9cf9-2d528d0f87fa - 1295 - PIPELINE_START - Started execution of pipeline \\\"model_pipeline\\\".\", \"pipeline_name\": \"model_pipeline\", \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\", \"step_key\": null, \"timestamp\": 1625760608.24893, \"user_message\": \"Started execution of pipeline \\\"model_pipeline\\\".\"}\tPIPELINE_START\t2021-07-08 16:10:08.24893\t\\N\t\\N\t\\N\n147\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": null, \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"1295\"}, \"label\": \"pid\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"['materialization_solid']\"}, \"label\": \"step_keys\"}]}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Executing steps in process (pid: 1295)\", \"pid\": 1295, \"pipeline_name\": \"model_pipeline\", \"solid_handle\": null, \"step_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"model_pipeline - 1399fa66-f129-46ad-9cf9-2d528d0f87fa - 1295 - ENGINE_EVENT - Executing steps in process (pid: 1295)\", \"pipeline_name\": \"model_pipeline\", \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\", \"step_key\": null, \"timestamp\": 1625760608.275929, \"user_message\": \"Executing steps in process (pid: 1295)\"}\tENGINE_EVENT\t2021-07-08 16:10:08.275929\t\\N\t\\N\t\\N\n148\t1399fa66-f129-46ad-9cf9-2d528d0f87fa\t{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [io_manager].\", \"pid\": 1295, \"pipeline_name\": \"model_pipeline\", \"solid_handle\": null, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"materialization_solid\", \"parent\": null}}, \"step_key\": \"materialization_solid\", \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"model_pipeline - 1399fa66-f129-46ad-9cf9-2d528d0f87fa - 1295 - materialization_solid - ENGINE_EVENT - Starting initialization of resources [io_manager].\", \"pipeline_name\": \"model_pipeline\", \"run_id\": \"1399fa66-f129-46ad-9cf9-2d528d0f87fa\", \"step_key\": \"materialization_solid\", \"timestamp\": 1625760608.289093, \"user_message\": \"Starting initialization of resources [io_manager].\"}\tENGINE_EVENT\t2021-07-08 16:10:08.289093\tmaterialization_solid\t\\N\t\\N\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-polars package\nDESCRIPTION: Command to install the dagster-polars package using pip. Some IOManagers may require additional dependencies which can be installed using extras.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-polars.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install dagster-polars\n```\n\n----------------------------------------\n\nTITLE: Copying Data into asset_keys Table\nDESCRIPTION: This SQL statement attempts to copy data into the `asset_keys` table in the `public` schema. The `FROM stdin` clause indicates that the data is provided inline. The presence of `\\.` signifies the end of the data input.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.asset_keys (id, asset_key, last_materialization, last_run_id, asset_details, wipe_timestamp, last_materialization_timestamp, tags, create_timestamp) FROM stdin;\n\\.\n```\n\n----------------------------------------\n\nTITLE: Importing Scheduler Module in Python\nDESCRIPTION: Imports the scheduler module from Dagster's core package. This module contains classes for scheduling Dagster jobs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.scheduler\n```\n\n----------------------------------------\n\nTITLE: Backfills Table Constraints\nDESCRIPTION: SQL constraints for backfills table defining unique and primary key constraints\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.backfills\n    ADD CONSTRAINT backfills_backfill_id_key UNIQUE (backfill_id);\n\nALTER TABLE ONLY public.backfills\n    ADD CONSTRAINT backfills_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Retry Attempts for Shuffle IO - Shuffle Settings\nDESCRIPTION: This property specifies the maximum number of automated retries for IO-related exceptions during shuffle fetches. This helps maintain stability in large shuffle operations amidst transient issues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_18\n\nLANGUAGE: properties\nCODE:\n```\nspark.shuffle.io.maxRetries\n```\n\n----------------------------------------\n\nTITLE: Loading Data into secondary_indexes Table in PostgreSQL for Dagster\nDESCRIPTION: SQL COPY command to insert data into the secondary_indexes table, which tracks database migrations and indexes in the Dagster system. Each row contains an ID, name, creation timestamp, and migration completion timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\nCOPY public.secondary_indexes (id, name, create_timestamp, migration_completed) FROM stdin;\n1\tasset_key_table\t2021-07-07 05:43:47.528264\t2021-07-06 22:43:47.524712\n3\trun_partitions\t2021-07-08 16:10:56.313889\t2021-07-08 09:10:56.309887\n4\tadd_mode_column\t2021-07-08 16:10:56.338122\t2021-07-08 09:10:56.334199\n\\.\n```\n\n----------------------------------------\n\nTITLE: Dagster Step Start Event Log in JSON Format\nDESCRIPTION: A JSON representation of a Dagster event record for step start. This log indicates the beginning of execution for the 'sleeper_2' step in the 'sleepy_pipeline'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"STEP_START\", \"logging_tags\": {\"pipeline\": \"sleepy_pipeline\", \"solid\": \"sleeper_2\", \"solid_definition\": \"sleeper\", \"step_key\": \"sleeper_2.compute\"}, \"message\": \"Started execution of step \\\"sleeper_2.compute\\\".\", \"pipeline_name\": \"sleepy_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"sleeper\", \"name\": \"sleeper_2\", \"parent\": null}, \"step_key\": \"sleeper_2.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - STEP_START - Started execution of step \\\"sleeper_2.compute\\\".\\n               solid = \\\"sleeper_2\\\"\\n    solid_definition = \\\"sleeper\\\"\\n            step_key = \\\"sleeper_2.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Displaying File Tree Structure for Dagster Project\nDESCRIPTION: This code snippet shows the directory structure of a Dagster project using the 'tree' command. It includes the main project folder, subdirectories for analytics and ELT operations, and configuration files like pyproject.toml and uv.lock.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/migrating-definitions/6-tree-after-all.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── README.md\n├── my_existing_project\n│   ├── __init__.py\n│   ├── definitions.py\n│   └── defs\n│       ├── __init__.py\n│       ├── analytics\n│       │   ├── __init__.py\n│       │   ├── assets.py\n│       │   └── jobs.py\n│       └── elt\n│           ├── __init__.py\n│           ├── assets.py\n│           └── jobs.py\n├── pyproject.toml\n└── uv.lock\n\n5 directories, 12 files\n```\n\n----------------------------------------\n\nTITLE: Defining and Altering Runs Table\nDESCRIPTION: Creates the 'runs' table to track pipeline executions, including metadata such as pipeline name and status. The table provides automatic timestamping with defaults, and ownership is assigned to user 'test'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    pipeline_name character varying,\n    status character varying(63),\n    run_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.runs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Documentation Overview with dbt and Jinja Templating\nDESCRIPTION: A documentation block using dbt's documentation syntax with Jinja templating to create an overview section for the Jaffle Shop project. The block defines the project as a fictional e-commerce store and provides context about it being a dbt test project with a link to the source code repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/jaffle_shop/models/overview.md#2025-04-22_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{% docs __overview__ %}\n\n## Data Documentation for Jaffle Shop\n\n`jaffle_shop` is a fictional ecommerce store.\n\nThis [dbt](https://www.getdbt.com/) project is for testing out code.\n\nThe source code can be found [here](https://github.com/clrcrl/jaffle_shop).\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Module Reference Path Definition\nDESCRIPTION: ReStructuredText directive specifying the current module path for documentation generation\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-deltalake-pandas.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: dagster_deltalake_pandas\n```\n\n----------------------------------------\n\nTITLE: Analyzing Dagster STEP_OUTPUT Event\nDESCRIPTION: This JSON snippet represents a Dagster event log entry indicating the output of a step named 'ingest_costs' within the 'longitudinal_pipeline'. It includes details about the output named 'result', its type ('Any'), and a type check indicating success.  The event provides information about intermediate materialization, step output handle, and type check data, along with logging tags for context.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"ingest_costs\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmpuqko9vtg\\\"}\", \"dagster/partition\": \"2020-01-02\", \"dagster/partition_set\": \"ingest_and_train\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_costs\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_costs\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 66816, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_costs\", \"parent\": null}, \"step_key\": \"ingest_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - bc7168d1-3d66-4d5d-93e0-0df885ee30f2 - 66816 - ingest_costs - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"bc7168d1-3d66-4d5d-93e0-0df885ee30f2\", \"step_key\": \"ingest_costs\", \"timestamp\": 1609894312.55449, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Module References\nDESCRIPTION: A series of reStructuredText directives that specify the documentation structure for various Dagster CLI commands, using the click directive to document command groups and nested subcommands.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster-dg-cli.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: dagster_dg\n\n.. click:: dagster_dg.cli.check:check_group\n    :prog: dg check\n    :nested:\n\n.. click:: dagster_dg.cli.dev:dev_command\n    :prog: dg dev\n    :nested:\n\n.. click:: dagster_dg.cli.docs:docs_group\n    :prog: dg docs\n    :nested:\n\n.. click:: dagster_dg.cli.launch:launch_command\n    :prog: dg launch\n    :nested:\n\n.. click:: dagster_dg.cli.list:list_group\n    :prog: dg list\n    :nested:\n\n.. click:: dagster_dg.cli.plus:plus_group\n    :prog: dg plus\n    :nested:\n\n.. click:: dagster_dg.cli.scaffold:scaffold_group\n    :prog: dg scaffold\n    :nested:\n```\n\n----------------------------------------\n\nTITLE: Logging Dagster Event Records in JSON\nDESCRIPTION: These JSON objects represent Dagster event records for various pipeline execution steps. They include information about step starts, expectation results, and step successes, with details such as timestamps, run IDs, and event-specific data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_53\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": null,\n    \"event_type_value\": \"STEP_START\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"check_admins_both_succeed\",\n      \"solid_definition\": \"check_admins_both_succeed\",\n      \"step_key\": \"check_admins_both_succeed.compute\"\n    },\n    \"message\": \"Started execution of step \\\"check_admins_both_succeed.compute\\\".\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"check_admins_both_succeed\",\n      \"name\": \"check_admins_both_succeed\",\n      \"parent\": null\n    },\n    \"step_key\": \"check_admins_both_succeed.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_START - Started execution of step \\\"check_admins_both_succeed.compute\\\".\\n               solid = \\\"check_admins_both_succeed\\\"\\n    solid_definition = \\\"check_admins_both_succeed\\\"\\n            step_key = \\\"check_admins_both_succeed.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"check_admins_both_succeed.compute\",\n  \"timestamp\": 1576110683.8889349,\n  \"user_message\": \"Started execution of step \\\"check_admins_both_succeed.compute\\\".\"\n}\n```\n\n----------------------------------------\n\nTITLE: Detecting Dev Environment\nDESCRIPTION: Python code to detect whether Dagster is running in development mode by checking environment variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/running-dagster-locally.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nif os.getenv(\"DAGSTER_IS_DEV_CLI\"):\n    print(\"Running in local dev environment\")\n```\n\n----------------------------------------\n\nTITLE: Schedule Relationship Diagram in Mermaid\nDESCRIPTION: Mermaid flowchart showing how Schedules connect to Assets, Config, Jobs and Definitions, with custom theme configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/getting-started/concepts.md#2025-04-22_snippet_14\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#4F43DD',\n      'primaryTextColor': '#FFFFFF',\n      'primaryBorderColor': '#231F1B',\n      'lineColor': '#DEDDFF',\n      'secondaryColor': '#BDBAB7',\n      'tertiaryColor': '#FFFFFF'\n    }\n  }\n}%%\n  graph LR\n    style Asset fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Config fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Definitions fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n    style Job fill:#BDBAB7,stroke:#BDBAB7,stroke-width:2px\n\n    Schedule(Schedule)\n\n    Asset -.-> Schedule\n    Config -.-> Schedule\n    Job -.-> Schedule\n    Schedule ==> Definitions\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Step Expectation Result\nDESCRIPTION: JSON record showing a Dagster pipeline step that checks for existence of 'raw_file_users' table with success result.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepExpectationResultData\", \"expectation_result\": {\"__class__\": \"ExpectationResult\", \"description\": \"Checked raw_file_users exists\", \"label\": \"output_table_exists\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_EXPECTATION_RESULT\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_users\", \"solid_definition\": \"raw_file_users\", \"step_key\": \"raw_file_users.compute\"}, \"message\": \"Checked raw_file_users exists\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_users\", \"name\": \"raw_file_users\", \"parent\": null}, \"step_key\": \"raw_file_users.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_EXPECTATION_RESULT - Checked raw_file_users exists\\n event_specific_data = {\\\"expectation_result\\\": [true, \\\"output_table_exists\\\", \\\"Checked raw_file_users exists\\\", []]}\\n               solid = \\\"raw_file_users\\\"\\n    solid_definition = \\\"raw_file_users\\\"\\n            step_key = \\\"raw_file_users.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Object Store Operation Event Log\nDESCRIPTION: Event record for retrieving an intermediate object from filesystem storage using pickle serialization. Contains metadata about the pipeline step and object store operation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_49\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_traffic/result\"}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"message\": \"Retrieved intermediate object for input _b in filesystem object store using pickle.\"}, \"pipeline_name\": \"longitudinal_pipeline\"}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Test Image for Dagster K8s Tests\nDESCRIPTION: Shell command to build a Docker image for testing dagster-k8s with a specific Python version, used for loading into a kind cluster for integration tests.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-k8s.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./python_modules/dagster-test/dagster_test/test_project/build.sh 3.7.6 \\\n    dagster.io.priv/buildkite-test-image:py310-latest\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence with Runs Table\nDESCRIPTION: Allows the 'runs_id_seq' sequence to automatically assign IDs to the 'runs' table's 'id' column.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.runs_id_seq OWNED BY public.runs.id;\n```\n\n----------------------------------------\n\nTITLE: Create asset_keys Table\nDESCRIPTION: This SQL statement creates the `asset_keys` table, which stores asset keys and their creation timestamps. It includes an auto-incrementing ID and a timestamp that defaults to the current timestamp.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n\"CREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\"\n```\n\n----------------------------------------\n\nTITLE: Dagster Table Materialization Event JSON\nDESCRIPTION: Event record showing materialization of a table_info value with metadata including table name, path, data and markdown documentation. Contains detailed logging information about the pipeline step execution.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepMaterializationData\", \"materialization\": {\"__class__\": \"Materialization\", \"description\": null, \"label\": \"table_info\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"TextMetadataEntryData\", \"text\": \"raw_group_admins\"}, \"label\": \"table_name\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/path/to/raw_group_admins\"}, \"label\": \"table_path\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"JsonMetadataEntryData\", \"data\": {\"name\": \"raw_group_admins\"}}, \"label\": \"table_data\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"UrlMetadataEntryData\", \"url\": \"https://bigty.pe/raw_group_admins\"}, \"label\": \"table_name_big\"}, {\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"MarkdownMetadataEntryData\", \"md_str\": \"# h1 Heading :)\\n\\n## h2 Heading\\n\\n### h3 Heading\\n\\n#### h4 Heading\\n\\n##### h5 Heading\\n\\n###### h6 Heading\\n\\n## Horizontal Rules\\n\\n---\\n\\n## Blockquotes\\n\\n> Blockquote can be nested\\n>\\n> > indentation by arrow level\\n\\n## Unordered lists\\n\\n- One\\n- Two\\n  - Indented\\n- Three\\n\\n## Ordered lists\\n\\n1. One\\n1. Two\\n1. Three\\n\\n## Code\\n\\nInline `code`\\n\\n    // comments\\n    line 1 of code\\n    line 2 of code\\n    line 3 of code\\n\\n## Links\\n\\n[rick roll](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\\n\"}, \"label\": \"table_blurb\"}]}}}\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Network for Dagster Cloud Agent\nDESCRIPTION: Creates a Docker network named 'dagster_cloud_agent' that will be used by the Docker agent to launch containers.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/docker/setup.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker network create dagster_cloud_agent\n```\n\n----------------------------------------\n\nTITLE: Creating run_tags_id_seq Sequence - SQL\nDESCRIPTION: This snippet establishes the 'run_tags_id_seq' sequence for generating unique identifiers for entries in the 'run_tags' table, similar to the event_logs sequence but for tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: run_tags_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.run_tags_id_seq OWNER TO test;\n\n```\n\n----------------------------------------\n\nTITLE: Importing and Using DocCardList Component in JSX\nDESCRIPTION: Imports and renders a DocCardList component, likely used to display a list of documentation cards related to Dagster execution concepts.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for runs_id_seq in PostgreSQL\nDESCRIPTION: Resets the 'runs_id_seq' to start at 1. This command ensures that the sequence is properly initialized.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.runs_id_seq', 1, true);\n```\n\n----------------------------------------\n\nTITLE: Accessing Dagster Resource Value\nDESCRIPTION: This snippet accesses the value of the defined 'list' resource through the `context.resources` object.  The code retrieves the current state of the list after it has been modified.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_resource.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncontext.resources.list\n```\n\n----------------------------------------\n\nTITLE: Setting Default Sequences on ID Columns\nDESCRIPTION: Assigns sequence default values for ID columns in 'event_log', 'run_tags', and 'runs' tables using respective sequences.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.event_log ALTER COLUMN id SET DEFAULT nextval('public.event_log_id_seq'::regclass);\n\nALTER TABLE ONLY public.run_tags ALTER COLUMN id SET DEFAULT nextval('public.run_tags_id_seq'::regclass);\n\nALTER TABLE ONLY public.runs ALTER COLUMN id SET DEFAULT nextval('public.runs_id_seq'::regclass);\n```\n\n----------------------------------------\n\nTITLE: Event Logs Table Creation\nDESCRIPTION: Table for storing detailed event logs for Dagster runs, including run ID, event details, timestamps, and associated asset/step information\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.event_logs (\n    id bigint NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrame Values\nDESCRIPTION: Performs element-wise addition of 1 to all values in the DataFrame. Demonstrates basic data transformation operation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-pandas/dagster_pandas/examples/notebooks/papermill_pandas_hello_world.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = df + 1\n\ndf\n```\n\n----------------------------------------\n\nTITLE: Installing Dagster Cloud CLI in Bash\nDESCRIPTION: Command to install the dagster-cloud CLI, which is required to run a local agent. This should be executed in the same environment where the agent will run, preferably in a Python virtual environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/local.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-cloud\n```\n\n----------------------------------------\n\nTITLE: Configuring Schedule Ticks Table\nDESCRIPTION: Defines 'schedule_ticks', responsible for documenting each scheduling event, including timestamps and status. Ownership is maintained by 'test', supporting Dagster's handling of recurring tasks.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.schedule_ticks (\n    id integer NOT NULL,\n    repository_name character varying(255),\n    schedule_name character varying,\n    status character varying(63),\n    \"timestamp\" timestamp without time zone,\n    tick_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\n\nALTER TABLE public.schedule_ticks OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Adding PRIMARY KEY Constraint for bulk_actions Table in PostgreSQL\nDESCRIPTION: Establishes a primary key on the 'id' column of 'bulk_actions', ensuring the uniqueness of its values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.bulk_actions ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Values in Markdown Table\nDESCRIPTION: A markdown table that documents the possible statuses of orders in the system, including placed, shipped, completed, return_pending, and returned statuses along with their descriptions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_exceptions/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n```\n\n----------------------------------------\n\nTITLE: Creating run_tags Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'run_tags' table which is used to store metadata tags associated with different runs at various stages. This structure aids in organizing and retrieving run-related information efficiently.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: run_tags; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key text,\n    value text\n);\n\nALTER TABLE public.run_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Sequence Values Configuration in PostgreSQL\nDESCRIPTION: A series of SQL statements setting the current values for various sequence objects in the database. These sequences are used to auto-generate primary key values for tables such as asset_keys, event_logs, job_ticks, jobs, and normalized_cereals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_66\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 10, true);\nSELECT pg_catalog.setval('public.event_logs_id_seq', 1788, true);\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, true);\nSELECT pg_catalog.setval('public.jobs_id_seq', 19, true);\nSELECT pg_catalog.setval('public.normalized_cereals_id_seq', 77, true);\nSELECT pg_catalog.setval('public.run_tags_id_seq', 2, true);\nSELECT pg_catalog.setval('public.runs_id_seq', 1, true);\nSELECT pg_catalog.setval('public.secondary_indexes_id_seq', 1, true);\nSELECT pg_catalog.setval('public.snapshots_id_seq', 2, true);\n```\n\n----------------------------------------\n\nTITLE: Yielding an ExpectationResult Event\nDESCRIPTION: This snippet demonstrates how to yield an ExpectationResult event from a Dagstermill-executed notebook. This event signals whether a specific expectation has been met. `dagstermill.yield_event` is used to send the event to Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/hello_world_explicit_yield.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndagstermill.yield_event(ExpectationResult(True))\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster OBJECT_STORE_OPERATION Event JSON in Python\nDESCRIPTION: JSON representation of a Dagster object store operation event. This event indicates that an intermediate object for the 'result' output was stored in memory using pickle. The record includes storage location and operation metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_56\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/intermediates/do_input.compute/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/intermediates/do_input.compute/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpbmtnrh\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Stored intermediate object for output result in memory object store using pickle.\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - do_input.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466123.661306, \"user_message\": \"Stored intermediate object for output result in memory object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Inserting Alembic Version Data\nDESCRIPTION: Copy operation to insert version number for database migration tracking\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\n284a732df317\n\\.\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Step Input Event in JSON\nDESCRIPTION: JSON record of a STEP_INPUT event in Dagster. This record shows the input 'x' being received by the 'do_input.compute' step with a successful type check for type 'Any'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_9_3_add_run_tags_run_id_idx/postgres/pg_dump.txt#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"x\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"x\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/tmp/tmpRbaiCU\\\"}\", \"dagster/schedule_name\": \"foo_schedule\", \"pipeline\": \"foo\", \"solid\": \"do_input\", \"solid_definition\": \"do_input\", \"step_key\": \"do_input.compute\"}, \"message\": \"Got input \\\"x\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 80630, \"pipeline_name\": \"foo\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"do_input\", \"parent\": null}, \"step_key\": \"do_input.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"foo - d45fa10b-121d-4051-9358-cfd3dfcbc878 - 80630 - do_input.compute - STEP_INPUT - Got input \\\"x\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"foo\", \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"step_key\": \"do_input.compute\", \"timestamp\": 1610466063.67839, \"user_message\": \"Got input \\\"x\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Configuring ANTLR\nDESCRIPTION: Download ANTLR JAR file, configure CLASSPATH, and create an alias for easy ANTLR tool usage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/antlr_asset_selection/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/local/lib\ncurl -O https://www.antlr.org/download/antlr-4.13.2-complete.jar\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport CLASSPATH=\".:/usr/local/lib/antlr-4.13.2-complete.jar:$CLASSPATH\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nalias antlr4='java -Xmx500M -cp \"/usr/local/lib/antlr-4.13.2-complete.jar:$CLASSPATH\" org.antlr.v4.Tool'\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install antlr4-python3-runtime\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Values in Markdown\nDESCRIPTION: A markdown table that documents the possible status values for orders in the system. Each status is paired with a detailed description explaining what that status means in the order lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/components/code_locations/templated_custom_keys_dbt_project_location/defs/jaffle_shop_dbt/jaffle_shop/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n```\n\n----------------------------------------\n\nTITLE: Enhanced External Code with Dagster Pipes Context\nDESCRIPTION: Extended version of external code that uses PipesContext to send logs and metadata back to Dagster, demonstrating bi-directional communication capabilities.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_pipes import PipesContext\n\ndef main():\n    # Create pipes context\n    with PipesContext() as context:\n        context.log.info(\"Running external computation...\")\n        \n        # Perform computation\n        result = 42\n        \n        context.log.info(f\"Computation complete. Result: {result}\")\n        context.add_output_metadata({\n            \"result\": result,\n            \"computation_type\": \"example\"\n        })\n        \n        return result\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Codes in Markdown Documentation\nDESCRIPTION: A markdown table documenting the five possible statuses for orders in the system, including descriptions of what each status represents in the order lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_last_update_freshness/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Creating run_tags Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'run_tags' table, which is utilized to associate key-value pairs with runs. This table comprises columns for identifiers, run ID, and the key-value structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: run_tags; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key character varying,\n    value character varying\n);\n\n\nALTER TABLE public.run_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Table snapshots in PostgreSQL\nDESCRIPTION: Sets up the 'snapshots' table to store snapshot data including their IDs, bodies, and types in the form of a byte array, all crucial for versioning and recoverability in Dagster pipelines.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n\nALTER TABLE public.snapshots OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraint on secondary_indexes.name\nDESCRIPTION: This SQL statement adds a unique constraint to the `secondary_indexes` table on the `name` column. This constraint ensures that each secondary index name is unique within the table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\n\"ALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\"\n```\n\n----------------------------------------\n\nTITLE: Building Dagster API Documentation\nDESCRIPTION: Commands to build API documentation and the full documentation site for error checking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# build and copy API markdown files; build and copy the sphinx `objects.inv` to static/\nyarn build-api-docs\n\n# build the static site\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Selecting Parentheses Group or Named Group Assets in Dagster\nDESCRIPTION: Demonstrates how to select assets that are either owned by the sales team and of kind 'csv', or belong to the 'public_data' group. This syntax uses parentheses grouping and the 'or' operator.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/asset-selection-syntax/examples.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n(owner:\"team:sales\" and kind:\"csv\") or group:\"public_data\"\n```\n\nLANGUAGE: python\nCODE:\n```\nsales_csv_public_job = define_asset_job(\n    name=\"sales_csv_public_job\", selection='(owner:\"team:sales\" and kind:\"csv\") or group:\"public_data\"'\n)\n```\n\nLANGUAGE: shell\nCODE:\n```\ndagster asset list --select '(owner:\"team:sales\" and kind:\"csv\") or group:\"public_data\"'\ndagster asset materialize --select '(owner:\"team:sales\" and kind:\"csv\") or group:\"public_data\"'\n```\n\n----------------------------------------\n\nTITLE: Creating secondary_indexes_id_seq Sequence in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'secondary_indexes_id_seq' sequence to handle the generation of unique IDs for the 'secondary_indexes' table entries, facilitating proper record tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: secondary_indexes_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nALTER TABLE public.secondary_indexes_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Index for Pending Steps in SQL\nDESCRIPTION: Creates a unique B-tree index on the pending_steps table to ensure uniqueness of the combination of concurrency_key, run_id, and step_key.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\nCREATE UNIQUE INDEX idx_pending_steps ON public.pending_steps USING btree (concurrency_key, run_id, step_key);\n```\n\n----------------------------------------\n\nTITLE: Dagster Engine Event Record - Resource Initialization Start\nDESCRIPTION: JSON event record for starting resource initialization in a Dagster pipeline, specifically for the asset_store resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [asset_store].\", \"pid\": 80538, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 8c59d1b7-1841-4c60-8346-89e298dbf743 - 80538 - ENGINE_EVENT - Starting initialization of resources [asset_store].\", \"pipeline_name\": \"foo\", \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"step_key\": null, \"timestamp\": 1610466002.725283, \"user_message\": \"Starting initialization of resources [asset_store].\"}\n```\n\n----------------------------------------\n\nTITLE: Database Indexes Creation in PostgreSQL\nDESCRIPTION: SQL statements creating various indexes on tables to improve query performance. These indexes target columns like asset_key, run_id, job_origin_id, and partition to optimize lookups and joins when filtering by these values.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_68\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_asset_key ON public.event_logs USING btree (asset_key);\nCREATE INDEX idx_asset_partition ON public.event_logs USING btree (asset_key, partition);\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\nCREATE INDEX idx_run_id ON public.event_logs USING btree (run_id);\nCREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\nCREATE INDEX ix_jobs_job_type ON public.jobs USING btree (job_type);\n```\n\n----------------------------------------\n\nTITLE: Importing celery_docker_executor from dagster_celery_docker\nDESCRIPTION: This snippet shows how to import the celery_docker_executor, which is an ExecutorDefinition, from the dagster_celery_docker module. It's used for configuring Celery + Docker execution in Dagster workflows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery-docker.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dagster_celery_docker import celery_docker_executor\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster Module in pyproject.toml\nDESCRIPTION: TOML configuration for specifying the module name in pyproject.toml to load Dagster definitions without command line arguments.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/running-dagster-locally.md#2025-04-22_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\npyproject.toml\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in React JSX\nDESCRIPTION: This code snippet imports the DocCardList component from the @theme/DocCardList module. It's then used to render a list of documentation cards, likely containing various Kubernetes-related topics or configurations for Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/deployment-options/kubernetes/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence: run_tags_id_seq in PostgreSQL\nDESCRIPTION: Establishes 'run_tags_id_seq' sequence to auto-generate IDs for 'run_tags' table's identifier column. Sequence starts at 1 and increments by 1, ensuring unique tag identifiers for each run.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_22\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.run_tags_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.run_tags_id_seq OWNER TO test;\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Sequence for Secondary Indexes ID in PostgreSQL for Dagster\nDESCRIPTION: Creates a sequence named 'secondary_indexes_id_seq' for generating unique IDs for the secondary_indexes table. The sequence starts at 1 and increments by 1 for each new entry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SEQUENCE public.secondary_indexes_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Op Job with Dependencies in Python\nDESCRIPTION: Demonstrates how to create an op job using the @job decorator, defining dependencies between ops through function calls within the decorated function body.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/op-jobs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import job, op\n\n@op\ndef return_five():\n    return 5\n\n@op\ndef add_one(num):\n    return num + 1\n\n@job\ndef my_job():\n    add_one(return_five())\n```\n\n----------------------------------------\n\nTITLE: Creating secondary_indexes Table in PostgreSQL\nDESCRIPTION: This SQL snippet sets up the 'secondary_indexes' table to track secondary indexing information as part of the database schema, including identifiers and timestamps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: secondary_indexes; Type: TABLE; Schema: public; Owner: test\n\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n\n\nALTER TABLE public.secondary_indexes OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Bulk Actions by Status in SQL\nDESCRIPTION: Creates a B-tree index on the bulk_actions table to optimize queries filtering by status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Component Type in YAML\nDESCRIPTION: Shows how to reference a custom component subclass in the component.yaml configuration file using the fully qualified type name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/components/building-pipelines-with-components/customizing-components.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntype: my_project.defs.my_def.CustomSubclass\n\nattributes: ...\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Alembic Version Table in PostgreSQL\nDESCRIPTION: This SQL command inserts a version number into the 'alembic_version' table, which is likely used for database migration tracking.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_59\n\nLANGUAGE: SQL\nCODE:\n```\nCOPY public.alembic_version (version_num) FROM stdin;\n4ea2b1f6f67b\n\\.\n```\n\n----------------------------------------\n\nTITLE: Importing Run Coordinator Module in Python\nDESCRIPTION: Imports the run coordinator module from Dagster's core package. This module contains classes for coordinating Dagster runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.run_coordinator\n```\n\n----------------------------------------\n\nTITLE: Displaying Metadata Tag Icon in HTML\nDESCRIPTION: This snippet demonstrates the HTML structure used to display a metadata tag icon. It includes an img element with the SVG file path, width, and height attributes. This pattern is repeated for each tool or technology in the list.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/partials/_KindsTags.md#2025-04-22_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n<img src=\"/images/guides/build/assets/metadata-tags/kinds/icons/tool-pinot-color.svg\" width={20} height={20} />\n```\n\n----------------------------------------\n\nTITLE: Creating Snapshots Table in PostgreSQL for Dagster\nDESCRIPTION: Creates a table named 'snapshots' to store snapshot data. The table includes columns for ID, snapshot ID, snapshot body (stored as bytea), and snapshot type.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_27\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE public.snapshots (\n    id integer NOT NULL,\n    snapshot_id character varying(255) NOT NULL,\n    snapshot_body bytea NOT NULL,\n    snapshot_type character varying(63) NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns in Downstream Asset with BigQuery I/O Manager\nDESCRIPTION: Demonstrates how to select specific columns when loading data from a BigQuery table into a downstream Dagster asset.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import asset, AssetIn\nimport pandas as pd\n\n@asset(\n    ins={\"iris_data\": AssetIn(metadata={\"columns\": [\"sepal_length_cm\", \"sepal_width_cm\"]})}\n)\ndef sepal_data(iris_data: pd.DataFrame) -> pd.DataFrame:\n    return iris_data\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing BigQuery Data in Downstream Assets\nDESCRIPTION: Example of loading data from a BigQuery table into a downstream asset, filtering for specific data, and storing results back in BigQuery.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/using-bigquery-with-dagster.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@asset\ndef iris_setosa(iris_data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Data for only Iris-Setosa species\"\"\"\n    return iris_data[iris_data[\"species\"] == \"setosa\"]\n```\n\n----------------------------------------\n\nTITLE: Shutting Down a Repository Location Server in Dagster\nDESCRIPTION: This code shows how to shut down a repository location server using the shutdown_repository_location method, which can be useful when you want to restart the server to reload repository definitions.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/graphql-client.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Shutdown a repository location\nclient.shutdown_repository_location(\"my_repository_location\")\n```\n\n----------------------------------------\n\nTITLE: Creating Index on runs.partition_set, partition\nDESCRIPTION: This SQL statement creates a composite index on the `partition_set` and `partition` columns of the `runs` table. This index will improve query performance when filtering by partition set and partition.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: sql\nCODE:\n```\n\"CREATE INDEX idx_run_partitions ON public.runs USING btree (partition_set, partition);\"\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Schedule with ScheduleDefinition in Dagster\nDESCRIPTION: This snippet demonstrates how to define a schedule using ScheduleDefinition that runs a job every day at midnight. It uses a cron expression to specify the schedule.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/defining-schedules.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbasic_schedule = ScheduleDefinition(\n    job=my_job,\n    cron_schedule=\"0 0 * * *\",  # Daily at midnight\n)\n```\n\n----------------------------------------\n\nTITLE: Creating run_tags Table - PostgreSQL SQL\nDESCRIPTION: This snippet creates the run_tags table for attaching metadata tags to run executions, facilitating better management and categorization of runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: run_tags; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.run_tags (\n    id integer NOT NULL,\n    run_id character varying(255),\n    key character varying,\n    value character varying\n);\nALTER TABLE public.run_tags OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Reloading Repository Location in Dagster\nDESCRIPTION: This example shows how to reload all repositories in a specified repository location using the reload_repository_location method, which is useful for refreshing the Dagster UI without restarting the server.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/operate/graphql/graphql-client.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Reload a repository location\nclient.reload_repository_location(\"my_repository_location\")\n```\n\n----------------------------------------\n\nTITLE: Defining Assets in Python for Dagster Asset Jobs\nDESCRIPTION: This snippet demonstrates the definition of assets that will be used in subsequent examples of asset jobs. It includes assets for cereals, healthy cereals, and sugary cereals.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/asset-jobs.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncereals = define_asset()\nhealthy_cereals = define_asset(deps=[cereals])\nsugary_cereals = define_asset(deps=[cereals])\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Directory Tree Structure\nDESCRIPTION: Shows the directory structure of a Python project using tree command output. The project follows a src-layout pattern with type hints enabled via py.typed file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/creating-dg-plugin/1-tree.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── pyproject.toml\n└── src\n    └── my_library\n        ├── __init__.py\n        ├── py.typed\n        └── utils.py\n\n3 directories, 4 files\n```\n\n----------------------------------------\n\nTITLE: Modifying Python Test Cleanup for Debugging Docker Containers\nDESCRIPTION: Python code modification in the test suite to prevent removal of Docker containers after tests, allowing for log inspection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    yield\nfinally:\n    subprocess.check_output([\"docker-compose\", \"-f\", docker_compose_file, \"stop\"])\n  #  subprocess.check_output([\"docker-compose\", \"-f\", docker_compose_file, \"rm\", \"-f\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Unique Constraint to Snapshots Table in SQL\nDESCRIPTION: Adds a unique constraint to the snapshot_id column in the snapshots table to ensure that snapshot IDs are unique across the database.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Running Basic DBT Commands\nDESCRIPTION: Essential dbt CLI commands for running transformations and tests in a dbt project.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/project_du_dbt_starter/analytics/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndbt run\ndbt test\n```\n\n----------------------------------------\n\nTITLE: Alter jobs Table Owner\nDESCRIPTION: This SQL statement changes the owner of the `jobs` table to `test`. This ensures proper permissions for the specified user.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_6_add_bulk_actions_table/postgres/pg_dump.txt#2025-04-22_snippet_21\n\nLANGUAGE: SQL\nCODE:\n```\n\"ALTER TABLE public.jobs OWNER TO test;\"\n```\n\n----------------------------------------\n\nTITLE: Adding Constraints to instigators Table in SQL\nDESCRIPTION: Adds a primary key constraint on the 'id' column and a unique constraint on the 'selector_id' column of the instigators table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_17_pre_add_cached_status_data_column/postgres/pg_dump.txt#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.instigators\n    ADD CONSTRAINT instigators_selector_id_key UNIQUE (selector_id);\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgresRunStorage in Dagster\nDESCRIPTION: Autoconfigurable class for PostgreSQL run storage in Dagster. It allows configuration of PostgreSQL database connection and usage for storing pipeline run information.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-postgres.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autoconfigurable:: PostgresRunStorage\n```\n\n----------------------------------------\n\nTITLE: Importing and Rendering DocCardList Component in JSX\nDESCRIPTION: This snippet imports the DocCardList component from the theme and renders it within the page. The DocCardList component is likely used to display a list of documentation cards related to Dagster projects.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/projects/index.md#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Installing and Setting Up Kitchen Sink Environment in Bash\nDESCRIPTION: These commands install the necessary dependencies and set up the local environment for the Kitchen Sink project. They should be run first before proceeding to the execution steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/dagster-dlift/kitchen-sink/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\nmake setup_local_env\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Dagster and Sling\nDESCRIPTION: This command installs the necessary Python packages including Dagster, Dagster UI (dagster-webserver), and dagster-sling using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/use_case_repository/use_case_repository/guides/snowflake_to_s3_sling.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster dagster-sling dagster-webserver\n```\n\n----------------------------------------\n\nTITLE: Configuring S3ComputeLogManager in dagster.yaml\nDESCRIPTION: Configuration for storing compute logs in your own S3 bucket using the S3ComputeLogManager in the dagster.yaml file.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/managing-compute-logs-and-error-messages.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster_aws.s3.compute_log_manager\n  class: S3ComputeLogManager\n  config:\n    show_url_only: true\n    bucket: your-compute-log-storage-bucket\n    region: your-bucket-region\n```\n\n----------------------------------------\n\nTITLE: Initializing Dagster project in current directory\nDESCRIPTION: Command to initialize a new Dagster project in the current directory using the Dagster CLI. This creates all necessary configuration files and directory structure needed to start developing with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/2-e-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndg init .\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraint for Run Tags\nDESCRIPTION: Creates a foreign key constraint linking the run_id column in the run_tags table to the runs table with cascade delete.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_69\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Specifying BigQuery Dataset as Metadata in Dagster\nDESCRIPTION: Demonstrates how to specify the BigQuery dataset as metadata for an asset in Dagster. This allows storing assets in different datasets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/gcp/bigquery/reference.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@asset(metadata={\"bigquery_dataset\": \"IRIS\"})\ndef iris_data():\n    return pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Navigating to Project Directory - Shell Commands\nDESCRIPTION: Creates a new directory called 'my-project' and changes the current working directory to it. Uses shell command chaining with && operator to execute commands sequentially.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/dg/scaffolding-project/2-a-pip-scaffold.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmkdir my-project && cd my-project\n```\n\n----------------------------------------\n\nTITLE: Defining External Assets in Python using AssetSpec\nDESCRIPTION: Demonstrates how to define an external asset representing raw transaction data from an SFTP source using the AssetSpec class.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/assets/external-assets.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster import AssetSpec\n\nraw_transactions = AssetSpec(\n    \"raw_transactions\",\n    description=\"Raw transaction data from our partner\",\n    metadata={\"source\": \"partner_sftp\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for asset_keys_id_seq in PostgreSQL\nDESCRIPTION: This SQL command sets the current value of the sequence 'asset_keys_id_seq' to 1. This is typically used when initializing the database to ensure that the next generated ID starts at 1.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.asset_keys_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Associating Sequence with Run Tags Table\nDESCRIPTION: Links 'run_tags_id_seq' sequence with the 'id' column in the 'run_tags' table, automating ID generation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nALTER SEQUENCE public.run_tags_id_seq OWNED BY public.run_tags.id;\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Object Store Operation\nDESCRIPTION: Event record showing storage of model output in filesystem using pickle serialization\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_40\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/train_model/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/bc7168d1-3d66-4d5d-93e0-0df885ee30f2/intermediates/train_model/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\"}}\n```\n\n----------------------------------------\n\nTITLE: Defining Primary Key Constraints in PostgreSQL for Dagster\nDESCRIPTION: SQL commands that add primary key constraints to alembic_version, event_logs, run_tags, and runs tables. These constraints ensure uniqueness and data integrity for critical columns.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Importing No-Op Compute Log Manager Module in Python\nDESCRIPTION: Imports the no-op compute log manager module from Dagster's core storage package. This module contains a class for a no-operation compute log manager.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/dagster/internals.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: dagster._core.storage.noop_compute_log_manager\n```\n\n----------------------------------------\n\nTITLE: Executing Dagster YAML Validation Check\nDESCRIPTION: Output from running the 'dg check yaml' command that validates all YAML configuration files in a Dagster project. The output indicates successful validation of all components.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/33-dg-component-check-yaml.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndg check yaml\n\nAll components validated successfully.\n```\n\n----------------------------------------\n\nTITLE: Importing DockerRunLauncher for Docker Deployment\nDESCRIPTION: This snippet imports the DockerRunLauncher, which is used for launching runs in a Docker container when deploying Dagster using Docker Compose.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/execution/run-launchers.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_docker import DockerRunLauncher\n```\n\n----------------------------------------\n\nTITLE: Customizing Celery Configuration (Python)\nDESCRIPTION: Python code example for customizing Celery broker URL and other configurations for use with Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-celery.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom celery import Celery\n\nfrom dagster_celery.tasks import create_task\n\napp = Celery('dagster', broker_url='some://custom@value', ...)\n\nexecute_plan = create_task(app)\n\nif __name__ == '__main__':\n    app.worker_main()\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Structure in YAML for Dagster\nDESCRIPTION: This YAML snippet defines the structure of a graph in Dagster, specifying ops and their dependencies.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/graphs.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nname: my_graph\nops:\n  - return_one\n  - add_one\n  - adder\ndependencies:\n  add_one:\n    num: {op: return_one}\n  adder:\n    num1: {op: return_one}\n    num2: {op: add_one}\n```\n\n----------------------------------------\n\nTITLE: AWS Glue Integration Example\nDESCRIPTION: Reference to a Python example file showing AWS Glue integration with Dagster. The example demonstrates how to create and manage AWS Glue jobs using Dagster's integration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/aws/glue.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndocs_snippets/docs_snippets/integrations/aws-glue.py\n```\n\n----------------------------------------\n\nTITLE: Invalid YAML Configuration with Unclosed Quote\nDESCRIPTION: YAML configuration snippet from component.yaml showing a syntax error where a quoted string is not properly closed.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_snippets/docs_snippets/guides/components/index/23-dg-component-check-error.txt#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntranslation:\n  key: \"target/main/{{ node.name }}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Starlift Demo\nDESCRIPTION: This command installs the necessary Python dependencies for the starlift-demo project from the root directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/starlift-demo/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Importing DocCardList Component in MDX\nDESCRIPTION: Imports and renders a DocCardList component from the theme package to display a list of documentation cards.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/index.md#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\n---\ntitle: \"Build pipelines\"\nsidebar_class_name: hidden\n---\n\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Recording Successful Step Completion in Dagster Pipeline\nDESCRIPTION: JSON log entry for a STEP_SUCCESS event, indicating that the 'build_cost_dashboard' step was successfully completed in 523ms.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_43\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 523.1132539920509}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"build_cost_dashboard\", \"solid_definition\": \"base_one_input\", \"step_key\": \"build_cost_dashboard\"}, \"message\": \"Finished execution of step \\\"build_cost_dashboard\\\" in 523ms.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"build_cost_dashboard\", \"parent\": null}, \"step_key\": \"build_cost_dashboard\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - build_cost_dashboard - STEP_SUCCESS - Finished execution of step \\\"build_cost_dashboard\\\" in 523ms.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"build_cost_dashboard\", \"timestamp\": 1608666934.2837, \"user_message\": \"Finished execution of step \\\"build_cost_dashboard\\\" in 523ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Rebuilding Kinds Tags for Dagster Docs\nDESCRIPTION: Command to generate and update the kinds tags programmatically for the documentation.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn rebuild-kinds-tags\n```\n\n----------------------------------------\n\nTITLE: Logging Step Success - JSON\nDESCRIPTION: Logs the successful completion of the 'raw_file_events' step, including the duration of execution. This event is crucial for tracking the performance of the pipeline execution steps.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepSuccessData\", \"duration_ms\": 31.54361700000008}, \"event_type_value\": \"STEP_SUCCESS\", \"logging_tags\": {\"pipeline\": \"many_events\", \"solid\": \"raw_file_events\", \"solid_definition\": \"raw_file_events\", \"step_key\": \"raw_file_events.compute\"}, \"message\": \"Finished execution of step \\\"raw_file_events.compute\\\" in 31ms.\", \"pipeline_name\": \"many_events\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"raw_file_events\", \"name\": \"raw_file_events\", \"parent\": null}, \"step_key\": \"raw_file_events.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_SUCCESS - Finished execution of step \\\"raw_file_events.compute\\\" in 31ms.\\n event_specific_data = {\\\"duration_ms\\\": 31.54361700000008}\\n               solid = \\\"raw_file_events\\\"\\n    solid_definition = \\\"raw_file_events\\\"\\n            step_key = \\\"raw_file_events.compute\\\"\", \"pipeline_name\": \"many_events\", \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\", \"step_key\": \"raw_file_events.compute\", \"timestamp\": 1576110682.8682199, \"user_message\": \"Finished execution of step \\\"raw_file_events.compute\\\" in 31ms.\"}\n```\n\n----------------------------------------\n\nTITLE: Defining an Op with Multiple Outputs Using Tuple Annotation in Python\nDESCRIPTION: Demonstrates how to use a tuple annotation to specify multiple outputs for an op.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/ops/index.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@op(out={\"int_output\": Out(), \"str_output\": Out()})\ndef my_op() -> Tuple[int, str]:\n    return 1, \"foo\"\n```\n\n----------------------------------------\n\nTITLE: Importing Data Analysis Libraries\nDESCRIPTION: This snippet imports essential libraries for data manipulation (pandas), machine learning (sklearn), and plotting (matplotlib) in Python.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagstermill/dagstermill/examples/notebooks/tutorial_RF.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport sklearn.ensemble\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Generating a UUID as a string in Python\nDESCRIPTION: This snippet generates a UUID using `uuid.uuid4()` and converts it to a string using `str()`.  `uuid.uuid4()` creates a random UUID, and `str()` ensures that the result is a human-readable string representation of the UUID.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/dagster-webserver/dagster_webserver_tests/render_uuid_notebook.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstr(uuid.uuid4())\n```\n\n----------------------------------------\n\nTITLE: Running Dagster Webserver in Bash\nDESCRIPTION: Command to run the Dagster webserver with a specific workspace configuration, used for debugging backcompatibility issues.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/integration_tests/test_suites/backcompat-test-suite/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndagster-webserver -w integration_tests/test_suites/backcompat-test-suite/webserver_service/workspace.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with Pip\nDESCRIPTION: Command to install the project and its development dependencies using pip install with the -e flag for editable mode.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/docs_projects/project_dagster_modal_pipes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence for job_ticks_id_seq in PostgreSQL\nDESCRIPTION: Sets the sequence 'job_ticks_id_seq' to start at 1, ensuring the sequence is reset for future entries.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT pg_catalog.setval('public.job_ticks_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Installing Virtual Environment with UV for Dagster-Airflow Tutorial\nDESCRIPTION: Creates a fresh virtual environment using the UV tool and activates it for the Dagster-Airflow federation tutorial.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/federation/setup.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install uv\nuv venv\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Creating Index on status Column in bulk_actions Table\nDESCRIPTION: SQL command to create an index on the status column in the bulk_actions table to optimize queries filtering by status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Creating Schedule Origin ID Index on Schedule Ticks\nDESCRIPTION: Creates a btree index on the schedule_origin_id column of the schedule_ticks table.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_67\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ix_schedule_ticks_schedule_origin_id ON public.schedule_ticks USING btree (schedule_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Recording Asset Materialization in Dagster Pipeline\nDESCRIPTION: Logs an asset materialization event for the 'persist_model' step. The materialized asset is named 'model' and includes metadata about cost and traffic.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_28\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepMaterializationData\",\n      \"materialization\": {\n        \"__class__\": \"AssetMaterialization\",\n        \"asset_key\": {\n          \"__class__\": \"AssetKey\",\n          \"path\": [\"model\"]\n        },\n        \"description\": null,\n        \"metadata_entries\": [\n          {\n            \"__class__\": \"EventMetadataEntry\",\n            \"description\": null,\n            \"entry_data\": {\n              \"__class__\": \"JsonMetadataEntryData\",\n              \"data\": {\"cost\": 7000, \"traffic\": 9200}\n            },\n            \"label\": \"persist_model\"\n          }\n        ],\n        \"partition\": \"2020-12-08\"\n      }\n    },\n    \"event_type_value\": \"STEP_MATERIALIZATION\",\n    \"logging_tags\": {\n      \".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\",\n      \"dagster/solid_selection\": \"*\",\n      \"pipeline\": \"longitudinal_pipeline\",\n      \"solid\": \"persist_model\",\n      \"solid_definition\": \"base_one_input\",\n      \"step_key\": \"persist_model\"\n    },\n    \"message\": \"Materialized value model.\",\n    \"pid\": 18688,\n    \"pipeline_name\": \"longitudinal_pipeline\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"name\": \"persist_model\",\n      \"parent\": null\n    },\n    \"step_key\": \"persist_model\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_model - STEP_MATERIALIZATION - Materialized value model.\",\n  \"pipeline_name\": \"longitudinal_pipeline\",\n  \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\",\n  \"step_key\": \"persist_model\",\n  \"timestamp\": 1608667064.350898,\n  \"user_message\": \"Materialized value model.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Package Version Constraints\nDESCRIPTION: Defines specific version requirements for Python packages to maintain compatibility with Dagster example projects. Includes pins for responses, pydantic, and pyOpenSSL to avoid breaking changes.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/temp_pins.txt#2025-04-22_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nresponses==0.23.1\n\n# pydantic 2.9.0 release briefly broke dagster\npydantic<2.9.0\n\n# https://github.com/snowflakedb/snowflake-connector-python/issues/2109\npyOpenSSL>=22.1.0\n```\n\n----------------------------------------\n\nTITLE: Parsing Dagster Event Log Entry in JSON\nDESCRIPTION: This snippet demonstrates the structure of a Dagster event log entry in JSON format. It includes details about the event type, step execution, and associated metadata.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__class__\": \"DagsterEventRecord\",\n  \"dagster_event\": {\n    \"__class__\": \"DagsterEvent\",\n    \"event_specific_data\": {\n      \"__class__\": \"StepSuccessData\",\n      \"duration_ms\": 50.41671499999989\n    },\n    \"event_type_value\": \"STEP_SUCCESS\",\n    \"logging_tags\": {\n      \"pipeline\": \"many_events\",\n      \"solid\": \"raw_file_friends\",\n      \"solid_definition\": \"raw_file_friends\",\n      \"step_key\": \"raw_file_friends.compute\"\n    },\n    \"message\": \"Finished execution of step \\\"raw_file_friends.compute\\\" in 50ms.\",\n    \"pipeline_name\": \"many_events\",\n    \"solid_handle\": {\n      \"__class__\": \"SolidHandle\",\n      \"definition_name\": \"raw_file_friends\",\n      \"name\": \"raw_file_friends\",\n      \"parent\": null\n    },\n    \"step_key\": \"raw_file_friends.compute\",\n    \"step_kind_value\": \"COMPUTE\"\n  },\n  \"error_info\": null,\n  \"level\": 10,\n  \"message\": \"many_events - 089287c5-964d-44c0-b727-357eb7ba522e - STEP_SUCCESS - Finished execution of step \\\"raw_file_friends.compute\\\" in 50ms.\\n event_specific_data = {\\\"duration_ms\\\": 50.41671499999989}\\n               solid = \\\"raw_file_friends\\\"\\n    solid_definition = \\\"raw_file_friends\\\"\\n            step_key = \\\"raw_file_friends.compute\\\"\",\n  \"pipeline_name\": \"many_events\",\n  \"run_id\": \"089287c5-964d-44c0-b727-357eb7ba522e\",\n  \"step_key\": \"raw_file_friends.compute\",\n  \"timestamp\": 1576110683.07207,\n  \"user_message\": \"Finished execution of step \\\"raw_file_friends.compute\\\" in 50ms.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 File Input in YAML\nDESCRIPTION: Example of how to configure an S3 file input using the S3Coordinate DagsterType in YAML format. This snippet demonstrates setting the bucket and key for an S3 file input.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-aws.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninputs:\n  s3_file:\n    value:\n      bucket: my-bucket\n      key: my-key\n```\n\n----------------------------------------\n\nTITLE: Creating Index for Run Tags in SQL\nDESCRIPTION: Creates a B-tree index on the run_tags table to optimize queries filtering by key and value.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_8_12_pre_add_backfill_id_column_to_runs_table/postgres/pg_dump.txt#2025-04-22_snippet_47\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_run_tags ON public.run_tags USING btree (key, value);\n```\n\n----------------------------------------\n\nTITLE: Displaying Command-Line Completions for Dagster-Cloud CLI\nDESCRIPTION: Prints out the completion for copying or manual installation of the dagster-cloud CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/dagster-cloud-cli/installing-and-configuring.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud --show-completion\n```\n\n----------------------------------------\n\nTITLE: Initial Changelog Entry in Markdown\nDESCRIPTION: Documents the initial 0.0.1 release of the package with basic version information and changelog structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/dagster-dlift/CHANGES.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Changelog\n\n## 0.0.1\n\n### New\n\n- Introduced package\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataproc Package Installation\nDESCRIPTION: YAML configuration to install required packages in the Dataproc environment.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/external-pipelines/gcp-dataproc-pipeline.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndataproc:pip.packages: \"dagster-pipes,google-cloud-storage\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Agent Replicas via Helm\nDESCRIPTION: Helm command and values.yaml configuration for setting up multiple Kubernetes agent replicas.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/multiple.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhelm upgrade \\\n    ...\n    --set replicas=2\n```\n\nLANGUAGE: yaml\nCODE:\n```\ndagsterCloudAgent:\n  ...\n  replicas: 2\n```\n\n----------------------------------------\n\nTITLE: Creating Runs Table\nDESCRIPTION: Defines the 'runs' table with columns for an 'id', 'run_id', 'pipeline_name', 'status', and timestamp columns. It sets default timestamps and ownership.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_6_6/postgres/pg_dump.txt#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.runs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    pipeline_name character varying,\n    status character varying(63),\n    run_body character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    update_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP\n);\n\nALTER TABLE public.runs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Configuring Telemetry Settings\nDESCRIPTION: Configuration for enabling or disabling anonymous usage statistics collection.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ntelemetry:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Dagster Step Input Event Log in JSON Format\nDESCRIPTION: A JSON representation of a Dagster event record capturing the step input validation. It shows that the input named 'units' passed type checking for the 'sleeper' step in the 'sleepy_pipeline'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepInputData\", \"input_name\": \"units\", \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"units\", \"metadata_entries\": [], \"success\": true}}, \"event_type_value\": \"STEP_INPUT\", \"logging_tags\": {\"pipeline\": \"sleepy_pipeline\", \"solid\": \"sleeper\", \"solid_definition\": \"sleeper\", \"step_key\": \"sleeper.compute\"}, \"message\": \"Got input \\\"units\\\" of type \\\"None\\\". (Type check passed).\", \"pipeline_name\": \"sleepy_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"definition_name\": \"sleeper\", \"name\": \"sleeper\", \"parent\": null}, \"step_key\": \"sleeper.compute\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"sleepy_pipeline - ca7f1e33-526d-4f75-9bc5-3e98da41ab97 - STEP_INPUT - Got input \\\"units\\\" of type \\\"None\\\". (Type check passed).\\n event_specific_data = {\\\"input_name\\\": \\\"units\\\", \\\"type_check_data\\\": [true, \\\"units\\\", null, []]}\\n               solid = \\\"sleeper\\\"\\n    solid_definition = \\\"sleeper\\\"\\n            step_key = \\\"sleeper.compute\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Schedule Ticks Sequence Control\nDESCRIPTION: Establishes 'schedule_ticks_id_seq' for sequential ID generation specific to 'schedule_ticks'. This sequence backs the integrity of data within the 'schedule_ticks' by maintaining a unique identifier for each log entry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_add_pipeline_snapshot/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE SEQUENCE public.schedule_ticks_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\nALTER TABLE public.schedule_ticks_id_seq OWNER TO test;\n\n\nALTER SEQUENCE public.schedule_ticks_id_seq OWNED BY public.schedule_ticks.id;\n```\n\n----------------------------------------\n\nTITLE: Importing DatadogResource in Python\nDESCRIPTION: Shows how to import the DatadogResource from the dagster_datadog module. This resource is used to configure and manage the Datadog integration in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-datadog.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dagster_datadog import DatadogResource\n```\n\n----------------------------------------\n\nTITLE: Launching Dagster UI Server\nDESCRIPTION: Command to start the Dagster development server and view jobs in the UI\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/build/jobs/job-execution.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndagster dev -f my_job.py\n```\n\n----------------------------------------\n\nTITLE: View Creation for Sorted Data in PostgreSQL\nDESCRIPTION: This SQL snippet illustrates the creation of a view named 'sort_by_calories' within the 'test-schema'. The view provides a sorted display of rows from the 'cereals' table, ordered by the calories column. It is owned by the 'test' user and facilitates simplified queries on sorted cereal data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_schedule_migration_table/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: sort_by_calories; Type: VIEW; Schema: test-schema; Owner: test\nCREATE VIEW \"test-schema\".sort_by_calories AS\n SELECT cereals.name,\n    cereals.mfr,\n    cereals.type,\n    cereals.calories,\n    cereals.protein,\n    cereals.fat,\n    cereals.sodium,\n    cereals.fiber,\n    cereals.carbo,\n    cereals.sugars,\n    cereals.potass,\n    cereals.vitamins,\n    cereals.shelf,\n    cereals.weight,\n    cereals.cups,\n    cereals.rating\n   FROM \"test-schema\".cereals\n  ORDER BY cereals.calories;\n\nALTER TABLE \"test-schema\".sort_by_calories OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Resource Registration Parameters\nDESCRIPTION: Parameters that control how long Spark will wait for resources to register before scheduling begins, and what ratio of resources must be available.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_39\n\nLANGUAGE: properties\nCODE:\n```\nspark.scheduler.maxRegisteredResourcesWaitingTime\n```\n\nLANGUAGE: properties\nCODE:\n```\nspark.scheduler.minRegisteredResourcesRatio\n```\n\n----------------------------------------\n\nTITLE: Pipeline Event Record - Step Output\nDESCRIPTION: Log record capturing successful output generation of type 'Int' named 'total' from a pipeline computation step.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_7_6_pre_event_log_migration/postgres/pg_dump.txt#2025-04-22_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"output_name\": \"total\", \"step_key\": \"sleeper_4.compute\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"success\": true}}, \"event_type_value\": \"STEP_OUTPUT\"}, \"error_info\": null}\n```\n\n----------------------------------------\n\nTITLE: Defining Primary Key and Unique Constraints for Dagster Database Tables\nDESCRIPTION: SQL ALTER TABLE commands that add primary key and unique constraints to various tables in the Dagster database schema. These constraints ensure data integrity by preventing duplicate entries and establishing relationships between tables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_13_12_pre_start_time_end_time/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: SQL\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: Installing dagster-k8s Package\nDESCRIPTION: Command to install the dagster-k8s package using pip.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/kubernetes.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dagster-k8s\n```\n\n----------------------------------------\n\nTITLE: Creating Job Tick Status Index in PostgreSQL for Dagster\nDESCRIPTION: Creates a btree index on the job_origin_id and status columns of the job_ticks table to optimize queries that filter by job status.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n```\n\n----------------------------------------\n\nTITLE: Threshold for Fetching Remote Blocks to Disk - Shuffle Settings\nDESCRIPTION: This configuration defines the size limit above which blocks will be fetched to disk instead of memory, preventing excessive memory usage during shuffle operations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/automation/automation_tests/__snapshots__/spark_confs.md#2025-04-22_snippet_15\n\nLANGUAGE: properties\nCODE:\n```\nspark.maxRemoteBlockSizeFetchToMem\n```\n\n----------------------------------------\n\nTITLE: Dagster Event Record - Step Output\nDESCRIPTION: Log entry for the output generation of ingest_traffic step, showing successful type checking of the 'result' output.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_58\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"StepOutputData\", \"intermediate_materialization\": null, \"step_output_handle\": {\"__class__\": \"StepOutputHandle\", \"mapping_key\": null, \"output_name\": \"result\", \"step_key\": \"ingest_traffic\"}, \"type_check_data\": {\"__class__\": \"TypeCheckData\", \"description\": null, \"label\": \"result\", \"metadata_entries\": [], \"success\": true}, \"version\": null}, \"event_type_value\": \"STEP_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"ingest_traffic\", \"solid_definition\": \"base_no_input\", \"step_key\": \"ingest_traffic\"}, \"message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"ingest_traffic\", \"parent\": null}, \"step_key\": \"ingest_traffic\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - ingest_traffic - STEP_OUTPUT - Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"ingest_traffic\", \"timestamp\": 1608666917.356471, \"user_message\": \"Yielded output \\\"result\\\" of type \\\"Any\\\". (Type check passed).\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Server as Code Location in YAML\nDESCRIPTION: Sets up a gRPC server as a code location with host, port, and custom name.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/code-locations/workspace-yaml.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nload_from:\n  - grpc_server:\n      host: localhost\n      port: 4266\n      location_name: 'my_grpc_server'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Current Deployment Settings with dagster-cloud CLI\nDESCRIPTION: Shell command showing how to use the dagster-cloud CLI to retrieve and display the current deployment settings, including default values. This is useful for verifying the applied configuration.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/management/deployments/managing-deployments.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndagster-cloud deployment settings get\n```\n\n----------------------------------------\n\nTITLE: Primary Key and Unique Constraints for Dagster Tables\nDESCRIPTION: SQL ALTER TABLE statements that add primary key and unique constraints to various Dagster tables including bulk_actions, daemon_heartbeats, event_logs, jobs, runs and snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n```\n\n----------------------------------------\n\nTITLE: Defining Imports for Downstream Assets\nDESCRIPTION: Import statements for plotly and required Dagster modules including get_asset_key_for_model and MetadataValue\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/integrations/libraries/dbt/using-dbt-with-dagster/downstream-assets.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstart_imports\\nend_imports\n```\n\n----------------------------------------\n\nTITLE: Adding Foreign Key Constraints - PostgreSQL\nDESCRIPTION: SQL statements that establish referential integrity between tables by adding foreign key constraints, linking runs to snapshots and run_tags to runs.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_run_partition/postgres/pg_dump.txt#2025-04-22_snippet_52\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT fk_runs_snapshot_id_snapshots_snapshot_id FOREIGN KEY (snapshot_id) REFERENCES public.snapshots(snapshot_id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_run_id_fkey FOREIGN KEY (run_id) REFERENCES public.runs(run_id) ON DELETE CASCADE;\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto Materialize Sensor Behavior in YAML\nDESCRIPTION: YAML configuration for controlling the behavior of auto materialize sensors. This setting can be added to dagster.yaml to retain previous behavior where auto materialize evaluations aren't handled by sensors.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/MIGRATION.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nauto_materialize:\n  use_sensors: true\n```\n\n----------------------------------------\n\nTITLE: Running Airflow DAG Backfill\nDESCRIPTION: Executes a backfill operation for the rebuild_customers_list DAG in Airflow, which will be detected by Dagster and registered as an asset materialization.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/dag-level-migration/peer.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nairflow dags backfill rebuild_customers_list --start-date $(shell date +\"%Y-%m-%d\")\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Ownership in PostgreSQL\nDESCRIPTION: Changes the owner of the snapshots_id_seq sequence to the test user. This establishes proper permissions for the sequence object.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_12_0_pre_asset_index_cols/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE public.snapshots_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Configuring Schedule Definitions in Dagster\nDESCRIPTION: Shows how to include a schedule in a Dagster Definitions object, which is necessary for the schedule to be detectable and loadable by Dagster tools like the UI and CLI.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/automate/schedules/troubleshooting-schedules.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndefs = Definitions(\n   assets=[asset_1, asset_2],\n   jobs=[job_1],\n   schedules=[all_assets_job_schedule],\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Output with IO Manager in Dagster Pipeline\nDESCRIPTION: JSON representation of a handled output event in a Dagster pipeline. The event shows that the 'result' output from the 'return_two' step was handled by the 'io_manager'.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"HandledOutputData\", \"manager_key\": \"io_manager\", \"metadata_entries\": [], \"output_name\": \"result\"}, \"event_type_value\": \"HANDLED_OUTPUT\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/hy/3g1v320524152x2v4bv8924r0000gn/T/tmpq7rk3t2y\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"single_mode\", \"solid\": \"return_two\", \"step_key\": \"return_two\"}, \"message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pid\": 34000, \"pipeline_name\": \"single_mode\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}, \"step_handle\": {\"__class__\": \"StepHandle\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"return_two\", \"parent\": null}}, \"step_key\": \"return_two\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"single_mode - ab4b26d6-58cc-4172-bf6f-d9b99bedec79 - 34000 - return_two - HANDLED_OUTPUT - Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\", \"pipeline_name\": \"single_mode\", \"run_id\": \"ab4b26d6-58cc-4172-bf6f-d9b99bedec79\", \"step_key\": \"return_two\", \"timestamp\": 1625607811.59834, \"user_message\": \"Handled output \\\"result\\\" using IO manager \\\"io_manager\\\"\"}\n```\n\n----------------------------------------\n\nTITLE: Creating asset_keys_id_seq Sequence - PostgreSQL SQL\nDESCRIPTION: This snippet creates a sequence called asset_keys_id_seq, associated with the asset_keys table to generate unique identifiers for new rows.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\n-- Name: asset_keys_id_seq; Type: SEQUENCE; Schema: public; Owner: test\nCREATE SEQUENCE public.asset_keys_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\nALTER TABLE public.asset_keys_id_seq OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Handling Object Storage Operations with Dagster in JSON\nDESCRIPTION: This snippet captures an OBJECT_STORE_OPERATION event from Dagster, detailing the storage and retrieval processes of intermediate objects using the filesystem. It includes metadata on the operation type, the path to the object, and the specific pipeline state at the time of the event. Dependencies include having Dagster set up and running a pipeline that reads/writes intermediates to a filesystem store via Pickle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_9_22_pre_asset_partition/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: JSON\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"ObjectStoreOperationResultData\", \"address\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_costs/result\", \"metadata_entries\": [{\"__class__\": \"EventMetadataEntry\", \"description\": null, \"entry_data\": {\"__class__\": \"PathMetadataEntryData\", \"path\": \"/Users/prha/dagster/storage/15d2d9b4-8c36-4c00-a9c5-8b233d673161/intermediates/persist_costs/result\"}, \"label\": \"key\"}], \"op\": \"SET_OBJECT\", \"value_name\": \"result\", \"version\": null}, \"event_type_value\": \"OBJECT_STORE_OPERATION\", \"logging_tags\": {\".dagster/grpc_info\": \"{\\\"host\\\": \\\"localhost\\\", \\\"socket\\\": \\\"/var/folders/ls/p35278613w573s8j1fn7_c4m0000gn/T/tmppw8twsmt\\\"}\", \"dagster/solid_selection\": \"*\", \"pipeline\": \"longitudinal_pipeline\", \"solid\": \"persist_costs\", \"solid_definition\": \"base_one_input\", \"step_key\": \"persist_costs\"}, \"message\": \"Stored intermediate object for output result in filesystem object store using pickle.\", \"pid\": 18688, \"pipeline_name\": \"longitudinal_pipeline\", \"solid_handle\": {\"__class__\": \"SolidHandle\", \"name\": \"persist_costs\", \"parent\": null}, \"step_key\": \"persist_costs\", \"step_kind_value\": \"COMPUTE\"}, \"error_info\": null, \"level\": 10, \"message\": \"longitudinal_pipeline - 15d2d9b4-8c36-4c00-a9c5-8b233d673161 - 18688 - persist_costs - OBJECT_STORE_OPERATION - Stored intermediate object for output result in filesystem object store using pickle.\", \"pipeline_name\": \"longitudinal_pipeline\", \"run_id\": \"15d2d9b4-8c36-4c00-a9c5-8b233d673161\", \"step_key\": \"persist_costs\", \"timestamp\": 1608666924.435771, \"user_message\": \"Stored intermediate object for output result in filesystem object store using pickle.\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Primary and Unique Constraints to Dagster Database Tables\nDESCRIPTION: SQL ALTER TABLE commands that add primary key and unique constraints to various Dagster tables including alembic_version, asset_keys, bulk_actions, daemon_heartbeats, event_logs, job_ticks, jobs, run_tags, runs, secondary_indexes, and snapshots.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_16_pre_add_mode_column/postgres/pg_dump.txt#2025-04-22_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE ONLY public.alembic_version\n    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_asset_key_key UNIQUE (asset_key);\n\nALTER TABLE ONLY public.asset_keys\n    ADD CONSTRAINT asset_keys_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_key_key UNIQUE (key);\n\nALTER TABLE ONLY public.bulk_actions\n    ADD CONSTRAINT bulk_actions_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.daemon_heartbeats\n    ADD CONSTRAINT daemon_heartbeats_daemon_type_key UNIQUE (daemon_type);\n\nALTER TABLE ONLY public.event_logs\n    ADD CONSTRAINT event_logs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.job_ticks\n    ADD CONSTRAINT job_ticks_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_job_origin_id_key UNIQUE (job_origin_id);\n\nALTER TABLE ONLY public.jobs\n    ADD CONSTRAINT jobs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.run_tags\n    ADD CONSTRAINT run_tags_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.runs\n    ADD CONSTRAINT runs_run_id_key UNIQUE (run_id);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_name_key UNIQUE (name);\n\nALTER TABLE ONLY public.secondary_indexes\n    ADD CONSTRAINT secondary_indexes_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_pkey PRIMARY KEY (id);\n\nALTER TABLE ONLY public.snapshots\n    ADD CONSTRAINT snapshots_snapshot_id_key UNIQUE (snapshot_id);\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for GCP Integration\nDESCRIPTION: ReStructuredText (RST) documentation layout defining the structure and organization of Dagster's GCP integration components including BigQuery, GCS, Dataproc, and Pipes modules.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/sphinx/sections/api/apidocs/libraries/dagster-gcp.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nGCP (dagster-gcp)\n=================\n\n.. currentmodule:: dagster_gcp\n\nBigQuery\n--------\n\nRelated Guides:\n\n* `Using Dagster with BigQuery <https://docs.dagster.io/integrations/libraries/gcp/bigquery/>`_\n* `BigQuery I/O manager reference <https://docs.dagster.io/integrations/libraries/gcp/bigquery/reference>`_\n\n\nBigQuery Resource\n^^^^^^^^^^^^^^^^^^\n\n.. autoconfigurable:: BigQueryResource\n  :annotation: ResourceDefinition\n\n\nBigQuery I/O Manager\n^^^^^^^^^^^^^^^^^^^^^\n\n.. autoconfigurable:: BigQueryIOManager\n  :annotation: IOManagerDefinition\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for bulk_actions Table in SQL\nDESCRIPTION: Creates indexes on key, action_type, selector_id, and status columns of the bulk_actions table to optimize query performance.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_31\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_bulk_actions ON public.bulk_actions USING btree (key);\n\nCREATE INDEX idx_bulk_actions_action_type ON public.bulk_actions USING btree (action_type);\n\nCREATE INDEX idx_bulk_actions_selector_id ON public.bulk_actions USING btree (selector_id);\n\nCREATE INDEX idx_bulk_actions_status ON public.bulk_actions USING btree (status);\n```\n\n----------------------------------------\n\nTITLE: Documenting Order Status Values in Markdown\nDESCRIPTION: A markdown table that defines the possible status values for orders and provides descriptions of what each status means in the order lifecycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_dbt_source_freshness/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Setting Sequence Value for run_tags_id_seq in PostgreSQL\nDESCRIPTION: This command sets the run_tags sequence to a starting value of 1, facilitating proper ID generation for run tags.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_14_6_instigators_table/postgres/pg_dump.txt#2025-04-22_snippet_24\n\nLANGUAGE: sql\nCODE:\n```\nSELECT pg_catalog.setval('public.run_tags_id_seq', 1, false);\n```\n\n----------------------------------------\n\nTITLE: Creating event_logs Table in PostgreSQL\nDESCRIPTION: This SQL snippet creates the 'event_logs' table for logging events triggered in the system. It contains fields for event details, including timestamps and identifiers for related tasks and assets.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_11_0_pre_asset_details/postgres/pg_dump.txt#2025-04-22_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\n-- Name: event_logs; Type: TABLE; Schema: public; Owner: test\nCREATE TABLE public.event_logs (\n    id integer NOT NULL,\n    run_id character varying(255),\n    event text NOT NULL,\n    dagster_event_type text,\n    \"timestamp\" timestamp without time zone,\n    step_key text,\n    asset_key text,\n    partition text\n);\n\nALTER TABLE public.event_logs OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Configuring Dagster+ Authorization with Custom Proxy Operator\nDESCRIPTION: Example showing how to establish a connection to a Dagster+ deployment using a custom proxy operator that retrieves organization name, deployment name, and user token from Airflow Variables.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/migrate/airflow-to-dagster/migration-reference.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n<CodeExample path=\"airlift-migration-tutorial/tutorial_example/snippets/custom_operator_examples/plus_proxy_operator.py\" />\n```\n\n----------------------------------------\n\nTITLE: Creating Table: asset_keys in PostgreSQL\nDESCRIPTION: Defines the 'asset_keys' table to manage asset key information, including materialization details and timestamps. Essential columns include asset key, last materialization, and creation timestamp to track asset life cycle.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_1_22_pre_primary_key/postgres/pg_dump.txt#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.asset_keys (\n    id integer NOT NULL,\n    asset_key character varying(512),\n    last_materialization text,\n    last_run_id character varying(255),\n    asset_details text,\n    wipe_timestamp timestamp without time zone,\n    last_materialization_timestamp timestamp without time zone,\n    tags text,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    cached_status_data text\n);\nALTER TABLE public.asset_keys OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Creating Indexes for job_ticks Table in SQL\nDESCRIPTION: Creates indexes on job_origin_id, status, and timestamp columns of the job_ticks table to improve query efficiency.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_1_0_12_pre_add_asset_event_tags_table/postgres/pg_dump.txt#2025-04-22_snippet_32\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE INDEX idx_job_tick_status ON public.job_ticks USING btree (job_origin_id, status);\n\nCREATE INDEX idx_job_tick_timestamp ON public.job_ticks USING btree (job_origin_id, \"timestamp\");\n\nCREATE INDEX ix_job_ticks_job_origin_id ON public.job_ticks USING btree (job_origin_id);\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Image for Postgres\nDESCRIPTION: Commands to build and run a Docker image containing a Postgres instance with sample data.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/examples/experimental/sling_decorator/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake build  # builds the docker image\nmake run    # runs the docker image\n```\n\n----------------------------------------\n\nTITLE: Resource Initialization Engine Event in Dagster (JSON)\nDESCRIPTION: JSON log entry indicating the start of resource initialization in a Dagster pipeline, specifically initializing the asset_store resource.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_51\n\nLANGUAGE: json\nCODE:\n```\n{\"__class__\": \"DagsterEventRecord\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": {\"__class__\": \"EngineEventData\", \"error\": null, \"marker_end\": null, \"marker_start\": \"resources\", \"metadata_entries\": []}, \"event_type_value\": \"ENGINE_EVENT\", \"logging_tags\": {}, \"message\": \"Starting initialization of resources [asset_store].\", \"pid\": 80827, \"pipeline_name\": \"foo\", \"solid_handle\": null, \"step_key\": null, \"step_kind_value\": null}, \"error_info\": null, \"level\": 10, \"message\": \"foo - 9f130936-1409-4bd0-b15d-67cf6e7a67ec - 80827 - ENGINE_EVENT - Starting initialization of resources [asset_store].\", \"pipeline_name\": \"foo\", \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"step_key\": null, \"timestamp\": 1610466123.476954, \"user_message\": \"Starting initialization of resources [asset_store].\"}\n```\n\n----------------------------------------\n\nTITLE: Inserting Dagster Schedule Tick Data in SQL\nDESCRIPTION: SQL insert statements for Dagster schedule tick data. Includes tick IDs, statuses, timestamps, and serialized tick configurations.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_63\n\nLANGUAGE: SQL\nCODE:\n```\n1\tacea3d9d526d4aa5e549e9eb0f60d9ef8b15eec7\tSUCCESS\t2021-01-12 10:40:01.192489\t{\"__class__\": \"ScheduleTickData\", \"cron_schedule\": \"* * * * *\", \"error\": null, \"run_id\": \"8c59d1b7-1841-4c60-8346-89e298dbf743\", \"schedule_name\": \"foo_schedule\", \"schedule_origin_id\": \"acea3d9d526d4aa5e549e9eb0f60d9ef8b15eec7\", \"status\": {\"__enum__\": \"ScheduleTickStatus.SUCCESS\"}, \"timestamp\": 1610466001.192489}\t2021-01-12 10:40:01.20459\t2021-01-12 10:40:01.20459\n2\tacea3d9d526d4aa5e549e9eb0f60d9ef8b15eec7\tSUCCESS\t2021-01-12 10:41:02.0638\t{\"__class__\": \"ScheduleTickData\", \"cron_schedule\": \"* * * * *\", \"error\": null, \"run_id\": \"d45fa10b-121d-4051-9358-cfd3dfcbc878\", \"schedule_name\": \"foo_schedule\", \"schedule_origin_id\": \"acea3d9d526d4aa5e549e9eb0f60d9ef8b15eec7\", \"status\": {\"__enum__\": \"ScheduleTickStatus.SUCCESS\"}, \"timestamp\": 1610466062.0638}\t2021-01-12 10:41:02.069839\t2021-01-12 10:41:02.069839\n3\tacea3d9d526d4aa5e549e9eb0f60d9ef8b15eec7\tSUCCESS\t2021-01-12 10:42:01.999318\t{\"__class__\": \"ScheduleTickData\", \"cron_schedule\": \"* * * * *\", \"error\": null, \"run_id\": \"9f130936-1409-4bd0-b15d-67cf6e7a67ec\", \"schedule_name\": \"foo_schedule\", \"schedule_origin_id\": \"acea3d9d526d4aa5e549e9eb0f60d9ef8b15eec7\", \"status\": {\"__enum__\": \"ScheduleTickStatus.SUCCESS\"}, \"timestamp\": 1610466121.999318}\t2021-01-12 10:42:02.005972\t2021-01-12 10:42:02.005972\n\\.\n```\n\n----------------------------------------\n\nTITLE: Defining Order Status Types in Markdown Table Format\nDESCRIPTION: Documentation block that defines different order statuses and their descriptions in a markdown table format. The table outlines five possible statuses (placed, shipped, completed, return_pending, returned) and explains what each status means in the order fulfillment process.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt_tests/dbt_projects/test_dagster_asset_key_exceptions/models/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{% docs orders_status %}\n\nOrders can be one of the following statuses:\n\n| status         | description                                                                                                            |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| placed         | The order has been placed but has not yet left the warehouse                                                           |\n| shipped        | The order has ben shipped to the customer and is currently in transit                                                  |\n| completed      | The order has been received by the customer                                                                            |\n| return_pending | The customer has indicated that they would like to return the order, but it has not yet been received at the warehouse |\n| returned       | The order has been returned by the customer and received at the warehouse                                              |\n\n{% enddocs %}\n```\n\n----------------------------------------\n\nTITLE: Creating Table secondary_indexes in PostgreSQL\nDESCRIPTION: This snippet creates a table to store secondary index information, including names and timestamps for creation and migration completion — vital for maintaining database indexing in Dagster.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-postgres/dagster_postgres_tests/compat_tests/snapshot_0_10_0_wipe_schedules/postgres/pg_dump.txt#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE public.secondary_indexes (\n    id integer NOT NULL,\n    name character varying,\n    create_timestamp timestamp without time zone DEFAULT CURRENT_TIMESTAMP,\n    migration_completed timestamp without time zone\n);\n\nALTER TABLE public.secondary_indexes OWNER TO test;\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Project Directory in Bash\nDESCRIPTION: Command to change directory to the example project location after cloning the Dagster repository.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/examples/modal/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/docs_projects/project_dagster_modal_pipes\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Storage for Compute Logs\nDESCRIPTION: Configuration for AzureBlobComputeLogManager which writes logs to Azure Blob Storage.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/deploy/dagster-instance-configuration.md#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ncompute_logs:\n  module: dagster_azure.blob\n  class: AzureBlobComputeLogManager\n  config:\n    storage_account: \"my-storage-account\"\n    container: \"my-container\"\n    secret_key: \"my-key\"\n```\n\n----------------------------------------\n\nTITLE: Generating Azure Service Principal for GitHub Actions\nDESCRIPTION: Azure CLI command to create a service principal for GitHub Actions to authenticate with Azure Container Registry.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/dagster-plus/deployment/deployment-types/hybrid/azure/acr-user-code.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naz ad sp create-for-rbac --name \"github-actions-acr\" --role contributor --scopes /subscriptions/<your_azure_subscription_id>/resourceGroups/<your_resource_group>/providers/Microsoft.ContainerRegistry/registries/<your_acr_name>\n```\n\n----------------------------------------\n\nTITLE: Building API Documentation for Dagster\nDESCRIPTION: Command to build the API documentation using Sphinx when changes are made to RST files in the docs directory.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/about/contributing.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyarn build-api-docs\n```\n\n----------------------------------------\n\nTITLE: Creating defs Directory for Autoloading\nDESCRIPTION: Command to create a new directory for autoloaded definitions within the project structure.\nSOURCE: https://github.com/dagster-io/dagster/blob/master/docs/docs/guides/labs/dg/incrementally-adopting-dg/migrating-project.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p my_existing_project/defs\ntouch my_existing_project/defs/__init__.py\n```"
  }
]
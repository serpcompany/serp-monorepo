[
  {
    "owner": "tecton-ai",
    "repo": "tecton-sample-repo",
    "content": "TITLE: Main Documentation Processing Script\nDESCRIPTION: Core implementation including code declaration extraction, markdown processing, and documentation generation. Contains multiple utility functions for handling files, parsing code, and organizing documentation content.\nSOURCE: https://github.com/tecton-ai/tecton-sample-repo/blob/main/genai/notebooks/ingestion.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom unstructured.partition.md import partition_md\nfrom unstructured.chunking.dispatch import chunk\nfrom tecton_gen_ai.api import Agent\nimport pandas as pd\nimport os\nimport tqdm\nfrom pydantic import BaseModel, Field\nfrom tecton_gen_ai.testing import set_dev_mode\n\nset_dev_mode()\n\nclass Declarations(BaseModel):\n    declarations: list[list[str,str]] = Field(..., description=\"\"\"List of tuples of declarations.\nEach tuple contains the object/function name and the description.\n\nYou should only extract:\n- Tecton classes and decorated functions\n- Tecton objects embedded in other objects (e.g. SnowflakeConfig in BatchSource, Attribute and Aggregate)\n- Unit tests (set the first value in the tuple as \"test\")\n                                              \nPay attention to the import statements at the beginning that tells you which objects and functions are imported from Tecton.\n\nDon't extract declarations that are commented out\n\nThe description should be under 150 words\n\n\nFor example, with this code:\n\n```from tecton import Entity, FeatureTable, Attribute\nfrom tecton.types import String, Timestamp, Int64, Field\nfrom fraud.entities import user\nfrom datetime import timedelta\n\n\nfeatures = [\n    Attribute('user_login_count_7d', Int64),\n    Attribute('user_login_count_30d', Int64),\n]\n\nuser_login_counts = FeatureTable(\n    name='user_login_counts',\n    entities=[user],\n    features=features,\n    online=True,\n    offline=True,\n    ttl=timedelta(days=7),\n    owner='demo-user@tecton.ai',\n    tags={'release': 'production'},\n    description='User login counts over time.',\n    timestamp_field='timestamp'\n)\n```\n\nThe declarations would be:\n\n[(\"FeatureTable\", \"User login counts over time.\")]\n\nIn this code\n\n```python\nfraud_detection_feature_service = FeatureService(\n    name='fraud_detection_feature_service',\n    prevent_destroy=False,  # Set to True for production services to prevent accidental destructive changes or downtime.\n    features=[\n        transaction_amount_is_higher_than_average,\n        user_transaction_amount_metrics,\n        user_transaction_counts,\n        user_distinct_merchant_transaction_count_30d,\n        merchant_fraud_rate\n    ]\n)\n\nminimal_fs = FeatureService(\n     name='minimal_fs',\n     features=[\n         transaction_amount_is_high\n     ]\n)\n```\n\nThe declarations would be:\n\n[\n    (\"FeatureService\", \"Fraud detection feature service\"),\n    (\"FeatureService\", \"Whether transaction amount is higher\")\n]\n\nIn this code:\n\n```\nimport math\n\nfrom ads.features.on_demand_feature_views.user_query_embedding_similarity import user_query_embedding_similarity\n\n\n# Testing the 'user_query_embedding_similarity' feature which takes in request data ('query_embedding')\n# and a precomputed feature ('user_embedding') as inputs\ndef test_user_query_embedding_similarity():\n    request = {'query_embedding': [1.0, 1.0, 0.0]}\n    user_embedding = {'user_embedding': [0.0, 1.0, 1.0]}\n\n    actual = user_query_embedding_similarity.test_run(request=request, user_embedding=user_embedding)\n\n    # Float comparison.\n    expected = 0.5\n    assert math.isclose(actual['cosine_similarity'], expected)\n```\n\nThe declarations would be:\n\n[(\"test\", \"Testing the 'user_query_embedding_similarity' feature which takes in request data ('query_embedding') and a precomputed feature ('user_embedding') as inputs\")]\n\nIn this code\n                                              \n```python\nfrom tecton import BatchSource, SnowflakeConfig\nfrom tecton.types import Field, Int64, String, Timestamp, Array\n\ngaming_user_batch = BatchSource(\n    name=\"gaming_users\",\n    batch_config=SnowflakeConfig(\n      database=\"VINCE_DEMO_DB\",\n      schema=\"PUBLIC\",\n      table=\"ONLINE_GAMING_USERS\",\n      url=\"https://<your-cluster>.<your-snowflake-region>.snowflakecomputing.com/\",\n      warehouse=\"COMPUTE_WH\",\n      timestamp_field='TIMESTAMP',\n    ),\n)\n```\n\n(Pay attention that SnowflakeConfig is a configuration object embedded in the BatchSource object, we also need to extract that)\n\nThe declarations would be:\n\n[(\"BatchSource\", \"Gaming users batch source\"), (\"SnowflakeConfig\", \"Gaming users batch source configuration\")]  \n\nIn this code:\n```\n# The following defines several sliding time window aggregations over a user's transaction amounts\n@stream_feature_view(\n    source=transactions_stream,\n    entities=[user],\n    mode='pandas',\n    batch_schedule=timedelta(days=1), # Defines how frequently batch jobs are scheduled to ingest into the offline store\n    features=[\n        Aggregate(input_column=Field('amt', Float64), function='sum', time_window=timedelta(hours=1)),\n        Aggregate(input_column=Field('amt', Float64), function='max', time_window=timedelta(days=1)),\n        Aggregate(input_column=Field('amt', Float64), function='min', time_window=timedelta(days=3)),\n        Aggregate(input_column=Field('amt', Float64), function=approx_percentile(percentile=0.5, precision=100), time_window=timedelta(hours=1))\n    ],\n    timestamp_field='timestamp',\n    online=True,\n    offline=True,\n    feature_start_time=datetime(2022, 5, 1),\n    tags={'release': 'production'},\n    owner='demo-user@tecton.ai',\n    description='Transaction amount statistics and total over a series of time windows, updated every 10 minutes.',\n    aggregation_leading_edge=AggregationLeadingEdge.LATEST_EVENT_TIME\n)\ndef user_transaction_amount_metrics(transactions):\n    return transactions[['user_id', 'amt', 'timestamp']]\n```\n                                              \n[(\"Aggregate\", \"sum of transaction amounts over the past hour\"), (\"Aggregate\", \"max of transaction amounts over the past day\"), (\"Aggregate\", \"min of transaction amounts over the past 3 days\"), (\"Aggregate\", \"50th percentile of transaction amounts over the past hour\")]\n\"\"\")\n\nagent = Agent(\n    name=\"code_parser\",\n    prompt=\"Extract the function, class or test declarations from the code.\",\n    output_schema=Declarations,\n    llm = {\n        \"model\": \"openai/gpt-4o-2024-11-20\",\n        \"temperature\": 0,\n        \"timeout\": 30,\n        \"max_tokens\": 2000,\n    }\n)\n\ndef get_py_files(directory):\n    files = []\n    for root, dirs, filenames in os.walk(directory):\n        for filename in filenames:\n            if filename.endswith('.py'):\n                files.append(os.path.join(root, filename))\n    return files\n\ndef extract_declarations(folders):\n    files = []\n    for folder in folders:\n        files+= get_py_files(folder)\n    \n    res = []\n    for i in tqdm.tqdm(range(len(files))):\n        with open(files[i], 'r') as f:\n            code = f.read()\n        for d in agent.invoke(code)[\"declarations\"]:\n            res.append({\"text\": f\"Example of {d[0]}. {d[1]}\", \"code\": code}) \n    return res\n\n\ndef partition(path, **kwargs):\n    with open(path, 'r') as f:\n        markdown_text = f.read()\n    replaced_text, code_snippets = extract_and_replace(markdown_text)\n    replacer = CodeReplacer(code_snippets)\n    elements = partition_md(text=replaced_text, languages=[\"eng\"], **kwargs)\n    for element in elements:\n        element.apply(replacer.apply)\n        yield element\n\ndef chunk_md(path, chunking_strategy, max_characters, **kwargs):\n    elements = partition(path, **kwargs)\n    chunks = chunk(elements, chunking_strategy=chunking_strategy, max_characters=max_characters)\n    for ck in chunks:\n        yield {'text': ck.text, \"id\": ck.id}\n\ndef clean_source_id(source: str) -> str:\n    if source.endswith(\".md\"):\n        src = source[:-3]\n    else:\n        src = source\n    if src.endswith(\"/changelog\"):\n        return \"changelog\"\n    parts = src.split(\"/\")\n    # there can be duplications in the last n parts, dedup\n    while(len(parts) > 1 and parts[-1] == parts[-2]):\n        parts = parts[:-1]\n    return \"/\".join(parts)\n\ndef generate_docs(version, base_path, chunking_strategy, max_characters, concurrency, url_func, **kwargs):\n    files = get_md_files(base_path)\n    files += [\"../../changelog.md\"]\n\n    def process_file(file):\n        source_id = clean_source_id(os.path.relpath(file, base_path))\n        if any(x.startswith(\"_\") for x in source_id.split(\"/\")):\n            return None\n        with open(file, 'r') as f:\n            markdown_text = f.read()\n        df = pd.DataFrame(chunk_md(file, chunking_strategy=chunking_strategy, max_characters=max_characters, **kwargs))        \n        df[\"source\"] = source_id\n        return pd.DataFrame([{\"source\":source_id, \"version\":version, \"url\": url_func(source_id), \"text\":markdown_text}]), df\n\n    #with Pool(concurrency) as pool:\n    #    raw = pool.map(process_file, files)\n    raw = []\n    for i in tqdm.tqdm(range(len(files))):\n        res = process_file(files[i])\n        if res is not None:\n            raw.append(process_file(files[i]))\n    # Flatten the list of lists\n    chunks = pd.concat([sublist for _, sublist in raw])\n    texts = pd.concat([md for md, _ in raw])\n    return texts, chunks\n\ndef to_url(source_id, prefix) -> str:\n    if source_id == \"changelog\":\n        return \"https://docs.tecton.ai/changelog\"\n    return prefix + source_id\n```\n\n----------------------------------------\n\nTITLE: Installing Tecton CLI and Setting Up Workspace\nDESCRIPTION: Commands for installing Tecton, logging into a cluster, creating a workspace, and applying features. These steps are necessary to set up and use the Tecton examples in this repository.\nSOURCE: https://github.com/tecton-ai/tecton-sample-repo/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install tecton>=1.1\ntecton login <cluster_name>\ntecton workspace create <workspace_name>\ncd spark  # or 'cd rift' depending on compute preference\ntecton init\ntecton apply\n```\n\n----------------------------------------\n\nTITLE: Extracting Declarations and Saving to Parquet\nDESCRIPTION: Script to extract declarations from specified folders and save results to a parquet file\nSOURCE: https://github.com/tecton-ai/tecton-sample-repo/blob/main/genai/notebooks/ingestion.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nres = extract_declarations([\"../../rift\", \"../../spark\", \"../../../examples\"])\ndf=pd.DataFrame(res)\ndf.to_parquet(\"/tmp/examples.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Installing Tecton Dependencies with Extensions\nDESCRIPTION: Specifies Tecton package installation requirements with pyspark and rift extensions, both targeting version 1.1.0\nSOURCE: https://github.com/tecton-ai/tecton-sample-repo/blob/main/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntecton[pyspark]~=1.1.0\ntecton[rift]~=1.1.0\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Results\nDESCRIPTION: Script to configure pandas display options and show the first 400 rows of the DataFrame\nSOURCE: https://github.com/tecton-ai/tecton-sample-repo/blob/main/genai/notebooks/ingestion.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# Set Pandas to display all rows\npd.set_option('display.max_rows', 400)\npd.set_option('display.max_colwidth', 150)\ndisplay(df.head(400))\n```\n\n----------------------------------------\n\nTITLE: Getting DataFrame Length\nDESCRIPTION: Simple snippet to get the length of the declarations DataFrame\nSOURCE: https://github.com/tecton-ai/tecton-sample-repo/blob/main/genai/notebooks/ingestion.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlen(df)\n```\n\n----------------------------------------\n\nTITLE: Installing multiprocess Package\nDESCRIPTION: Simple pip install command for the multiprocess package dependency\nSOURCE: https://github.com/tecton-ai/tecton-sample-repo/blob/main/genai/notebooks/ingestion.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install multiprocess\n```"
  }
]
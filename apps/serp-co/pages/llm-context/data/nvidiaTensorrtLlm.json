[
  {
    "owner": "nvidia",
    "repo": "tensorrt-llm",
    "content": "TITLE: Building Llama 3.1 405B Engine with TensorRT-LLM\nDESCRIPTION: Bash script for building a TensorRT-LLM engine for Llama 3.1 405B model using tensor parallelism across 16 GPUs on 2 nodes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nfolder_trt_llm=../TensorRT-LLM\nmodel_dir=Llama-3.1-405B\nckpt_dir=ckpt_llama_3.1_405b\nengine_dir=engine_llama_3.1_405b\ndtype=bfloat16\ntp_size=16\npp_size=1\nkv_cache_type=paged\nmax_input_len=128\nmax_output_len=128\nmax_batch_size=4\nworkers=8\n\npython ${folder_trt_llm}/examples/llama/convert_checkpoint.py \\\n    --output_dir ${ckpt_dir} \\\n    --model_dir ${model_dir} \\\n    --dtype ${dtype} \\\n    --tp_size ${tp_size} \\\n    --pp_size ${pp_size} \\\n    --workers ${workers} \\\n    --use_parallel_embedding\n\ntrtllm-build \\\n    --output_dir ${engine_dir} \\\n    --checkpoint_dir ${ckpt_dir} \\\n    --gemm_plugin ${dtype} \\\n    --gpt_attention_plugin ${dtype} \\\n    --kv_cache_type ${kv_cache_type} \\\n    --max_input_len ${max_input_len} \\\n    --max_seq_len $(( max_input_len + max_output_len )) \\\n    --max_batch_size ${max_batch_size} \\\n    --workers ${workers}\n```\n\n----------------------------------------\n\nTITLE: Enabling Multiple Profiles in TensorRT-LLM\nDESCRIPTION: This code demonstrates how to enable multiple optimization profiles in TensorRT-LLM through the BuildConfig object. Multiple profiles allow the engine to handle varying request loads with different tensor shapes more efficiently, which is recommended for production builds.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm import LLM, BuildConfig\n\ndef main():\n    build_config = BuildConfig()\n    build_config.plugin_config.multiple_profiles = True\n\n    llm = LLM(\n        model=\"/scratch/Llama-3.3-70B-Instruct\",\n        tensor_parallel_size=4,\n        build_config=build_config\n    )\n\n    llm.save(\"build_flags_multiple_profiles\")\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Running Basic TensorRT-LLM Quickstart with PyTorch\nDESCRIPTION: Simple command to run the TensorRT-LLM quickstart script with default settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/pytorch/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 quickstart.py\n```\n\n----------------------------------------\n\nTITLE: Running Qwen Model with INT4-GPTQ Quantization\nDESCRIPTION: Command to run inference on a TensorRT-LLM Qwen model with INT4-GPTQ quantization. It uses the quantized model to generate text in response to Chinese input.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --input_text \"你好，请问你叫什么？\" \\\n                  --max_output_len=50 \\\n                  --tokenizer_dir ./tmp/Qwen-7B-Chat-Int4 \\\n                  --engine_dir=./tmp/Qwen/7B/trt_engines/int4_GPTQ/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building and Running SantaCoder Model with Multi-Query Attention\nDESCRIPTION: Complete pipeline for downloading, converting, building, and running the SantaCoder model with 4-way tensor parallelism. SantaCoder extends GPT with multi-query attention mechanism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n# Download hf santacoder model\ngit clone https://huggingface.co/bigcode/santacoder\n\n# Convert to TensorRT-LLM checkpoint\npython3 convert_checkpoint.py --model_dir santacoder \\\n        --dtype float16 \\\n        --tp_size 4 \\\n        --output_dir santacoder/trt_ckpt/fp16/4-gpu\n\n# Build TensorRT-LLM engines\ntrtllm-build --checkpoint_dir santacoder/trt_ckpt/fp16/4-gpu \\\n        --gemm_plugin auto \\\n        --output_dir santacoder/trt_engines/fp16/4-gpu\n\n# Run inference\nmpirun -np 4 \\\n    python3 ../run.py --engine_dir santacoder/trt_engines/fp16/4-gpu \\\n        --tokenizer_dir santacoder \\\n        --input_text \"def print_hello_world():\" \\\n        --max_output_len 20\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines from TensorRT-LLM Checkpoints\nDESCRIPTION: Uses the trtllm-build command to create TensorRT engines from TensorRT-LLM checkpoints for various GPU configurations, enabling optimizations like attention plugins and paged KV cache.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/fp16/1-gpu \\\n        --output_dir gpt2/trt_engines/fp16/1-gpu\n\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/fp16/2-gpu \\\n        --output_dir gpt2/trt_engines/fp16/2-gpu\n\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/fp16/4-gpu \\\n        --output_dir gpt2/trt_engines/fp16/4-gpu\n```\n\n----------------------------------------\n\nTITLE: Extracting Tokenizer Information for Guided Decoding in Python\nDESCRIPTION: This snippet demonstrates how to extract the necessary tokenizer information to enable guided decoding with TensorRT-LLM. It shows how to convert a Hugging Face tokenizer into the required format for GuidedDecodingConfig.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/executor.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nencoded_vocab = tokenizer.get_vocab()\nencoded_vocab = [token for token, _ in sorted(encoded_vocab.items(), key=lambda x: x[1])]\ntokenizer_str = tokenizer.backend_tokenizer.to_str()\nstop_token_ids = [tokenizer.eos_token_id]\n```\n\n----------------------------------------\n\nTITLE: Implementing a Plugin-Based Graph Rewriting Pattern in Python\nDESCRIPTION: Advanced example of a pattern rewriter that adds remove-padding functionality to the GPTAttention plugin. This example demonstrates how to use FLayerInfo to access and modify the inputs of a TensorRT plugin, which would otherwise be opaque in the Python API.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/graph-rewriting.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass GPTAttentionPluginRemovePaddingRewritePass(PatternRewriter):\n\n    def __init__(self):\n        super().__init__('gpt_attention_plugin_remove_padding',\n                         root_layer={trt.LayerType.PLUGIN_V2})\n\n    def match_and_rewrite(self, layer: Layer) -> bool:\n        if layer.as_layer().type != trt.LayerType.PLUGIN_V2 or \\\n                layer.as_layer().plugin.plugin_namespace != 'tensorrt_llm' or \\\n                layer.as_layer().plugin.plugin_type != 'GPTAttention':\n            return False\n\n        # Retrieve the FLayerInfo\n        flayer = FLayerInfoMemo.instance().get(layer.name)\n        assert flayer\n        # Although the layer is a plugin, which is a black box, we get some high-level input information from the FLayerInfo.\n        tensor_input: Tensor = flayer.get_input('qkv')\n        if tensor_input.shape[0] == 1:  # Already in remove-padding mode\n            return False\n\n        # Some information could be passed in from external\n        assert self.args is not None, \"args should be passed in from RewritePatternManager.rewrite()\"\n        batch_size, in_len, hidden_size = self.args['batch_size'], self.args['in_len'], self.args['hidden_size']\n\n        with net_guard(layer.network):\n            new_inputs = flayer.clone_inputs()\n\n            # Step 1: Create new inputs and replace the original arglist.\n            input = Tensor(\n                name='qkv',\n                dtype=trt.float16,\n                shape=(1, batch_size * in_len, hidden_size),\n            )\n            new_inputs['qkv'] = input\n\n            # Step 2: Create a new plugin instance.\n            new_outs = gpt_attention(**new_inputs)\n\n            # Step 3: Deprive all the users of the old plugin instance.\n            flayer.replace_outputs_uses_with(layer.network, new_outs)\n\n            # Step 4: Remove the old plugin instance.\n            layer.mark_as_removed()\n\n        return True\n```\n\n----------------------------------------\n\nTITLE: Converting Falcon Weights from Hugging Face to TensorRT-LLM Format\nDESCRIPTION: Various commands for converting Hugging Face model weights to TensorRT-LLM checkpoint format. Includes examples for different model sizes with options for tensor parallelism, pipeline parallelism, and different data types.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# falcon-rw-1b: single gpu, dtype float16\npython3 convert_checkpoint.py --model_dir ./falcon/rw-1b \\\n                --dtype float16 \\\n                --output_dir ./falcon/rw-1b/trt_ckpt/fp16/1-gpu/\n\n# falcon-7b-instruct: single gpu, dtype bfloat16\npython3 convert_checkpoint.py --model_dir ./falcon/7b-instruct \\\n                --dtype bfloat16 \\\n                --output_dir ./falcon/7b-instruct/trt_ckpt/bf16/1-gpu/\n\n# falcon-40b-instruct: 2-way tensor parallelism\npython3 convert_checkpoint.py --model_dir ./falcon/40b-instruct \\\n                --dtype bfloat16 \\\n                --output_dir ./falcon/40b-instruct/trt_ckpt/bf16/tp2-pp1/ \\\n                --tp_size 2\n\n# falcon-40b-instruct: 2-way tensor parallelism and 2-way pipeline parallelism\npython3 convert_checkpoint.py --model_dir ./falcon/40b-instruct \\\n                --dtype bfloat16 \\\n                --output_dir ./falcon/40b-instruct/trt_ckpt/bf16/tp2-pp2/ \\\n                --tp_size 2 \\\n                --pp_size 2\n\n# falcon-180b: 8-way tensor parallelism, loading weights shard-by-shard\npython3 convert_checkpoint.py --model_dir ./falcon/180b \\\n                --dtype bfloat16 \\\n                --output_dir ./falcon/180b/trt_ckpt/bf16/tp8-pp1/ \\\n                --tp_size 8 \\\n                --load_by_shard \\\n                --workers 8\n\n# falcon-180b: 4-way tensor parallelism and 2-way pipeline parallelism, loading weights shard-by-shard\npython3 convert_checkpoint.py --model_dir ./falcon/180b \\\n                --dtype bfloat16 \\\n                --output_dir ./falcon/180b/trt_ckpt/bf16/tp4-pp2/ \\\n                --tp_size 4 \\\n                --pp_size 2 \\\n                --load_by_shard \\\n                --workers 8\n\n# falcon-11b (Falcon 2): single gpu, dtype bfloat16\npython3 convert_checkpoint.py --model_dir ./falcon/11b \\\n                --dtype bfloat16 \\\n                --output_dir ./falcon/11b/trt_ckpt/bf16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Implementing SiLU Activation Function in TensorRT-LLM\nDESCRIPTION: Shows how more complex activation functions like SiLU (Swish) can be implemented by composing simpler activation functions in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# In tensorrt_llm.functional:\n\ndef silu(input: Tensor) -> Tensor:\n    return input * sigmoid(input)\n```\n\n----------------------------------------\n\nTITLE: Integrating AutoDeploy with TensorRT-LLM's LLM API in Python\nDESCRIPTION: This code snippet illustrates how to integrate AutoDeploy into custom workflows using TensorRT-LLM's high-level API. It includes setting up build configuration, AutoDeploy configuration, and constructing the LLM object with AutoDeploy as the backend.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm import LLM\nfrom tensorrt_llm.builder import BuildConfig\nfrom tensorrt_llm._torch.auto_deploy.shim import AutoDeployConfig\n\n# 1. Set up the build configuration\nbuild_config = BuildConfig(\n    max_seq_len=<MAX_SEQ_LEN>,\n    max_batch_size=<MAX_BS>,\n)\nbuild_config.plugin_config.tokens_per_block = <PAGE_SIZE>\n# if using \"TritonWithFlattenedInputs\" as backend, <PAGE_SIZE> should equal to <MAX_SEQ_LEN>\n# Refer to examples/auto_deploy/simple_config.py (line 109) for details.\n\n# 2. Set up AutoDeploy configuration\n# AutoDeploy will use its own cache implementation\nmodel_kwargs = {\"use_cache\":False}\n\nad_config = AutoDeployConfig(\n    use_cuda_graph=True, # set True if using \"torch-opt\" as compile backend\n    torch_compile_enabled=True, # set True if using \"torch-opt\" as compile backend\n    model_kwargs=model_kwargs,\n    attn_backend=\"TritonWithFlattenedInputs\", # choose between \"TritonWithFlattenedInputs\" and \"FlashInfer\"\n    skip_loading_weights=False,\n)\n\n# 3. Construct the LLM high-level interface object with autodeploy as backend\nllm = LLM(\n    model=<HF_MODEL_CARD_OR_DIR>,\n    backend=\"autodeploy\",\n    build_config=build_config,\n    pytorch_backend_config=ad_config,\n    tensor_parallel_size=<NUM_WORLD_RANK>,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Architecture for TensorRT-LLM\nDESCRIPTION: Defines the architecture of a decoder-only model using TensorRT-LLM's APIs, including layer definition, model implementation, and causal language model integration with DecoderModelForCausalLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/add-model.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyDecoderLayer(Module):\n    def __init__(self, config: PretrainedConfig, layer_idx: int):\n        self.layer_idx = layer_idx\n        self.config = config\n        self.input_layernorm = LayerNorm(...)\n        self.attention = Attention(...)\n        self.post_layernorm = LayerNorm(...)\n        self.mlp = MLP(...)\n\n    def forward(self, hidden_states, ...):\n        # decoder layer forward\n        return hidden_states\n\nclass MyModel(Module):\n    def __init__(self, config: PretrainedConfig):\n        self.config = config\n        self.vocab_embedding = Embedding(...)\n        self.layers = DecoderLayerList(MyDecoderLayer, config)\n        self.ln_f = LayerNorm(...)\n\n    def forward(self, input_ids, ...):\n        # model forward\n        return hidden_states\n\n\nclass MyModelForCausalLM(DecoderModelForCausalLM):\n    def __init__(self, config: PretrainedConfig):\n        transformer = MyModel(config)\n        lm_head = ColumnLinear(...)\n        super().__init__(config, transformer, lm_head)\n```\n\n----------------------------------------\n\nTITLE: Converting GPT-J Model to FP16 TensorRT Format\nDESCRIPTION: Command to convert HuggingFace GPT-J checkpoint to TensorRT-LLM format with FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Build a float16 engine using HF weights.\npython convert_checkpoint.py --model_dir ./gpt-j-6b \\\n                             --dtype float16 \\\n                             --output_dir ./trt_ckpt/gptj_fp16_tp1/\n```\n\n----------------------------------------\n\nTITLE: Running Inference with HuggingFace Checkpoint\nDESCRIPTION: Complete workflow for running Gemma 2B model using HuggingFace checkpoint with bfloat16 precision\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-2b\nCKPT_PATH=gemma-2b/\nUNIFIED_CKPT_PATH=/tmp/ckpt/hf/gemma/2b/1-gpu/\nENGINE_PATH=/tmp/engines/gemma/2B/bf16/1-gpu/\nVOCAB_FILE_PATH=gemma-2b/\n\npython3 ./examples/gemma/convert_checkpoint.py \\\n    --ckpt-type hf \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype bfloat16 \\\n    --world-size 1 \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --tokenizer_dir ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n```\n\n----------------------------------------\n\nTITLE: Implementing FP8 Quantization with FP8 KV Cache for GPT2\nDESCRIPTION: Commands to perform FP8 quantization and FP8 KV cache quantization on a GPT2 model and build the corresponding TensorRT engine. FP8 offers a balance between precision and memory efficiency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n# FP8 quantization with FP8 kv cache\npython3 ../quantization/quantize.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --qformat fp8 \\\n        --kv_cache_dtype fp8 \\\n        --output_dir gpt2/trt_ckpt/fp8/1-gpu\n\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/fp8/1-gpu \\\n        --output_dir gpt2/trt_engines/fp8/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Requesting Additional Model Outputs in C++\nDESCRIPTION: This snippet demonstrates how to construct a request that includes additional model outputs in TensorRT-LLM. It shows how to configure a request to retrieve the 'TopKLogits' tensor, including options for getting outputs during the context phase.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/executor.md#2025-04-07_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<executor::OutputConfig::AdditionalModelOutput> additionalOutputs{\n    executor::OutputConfig::AdditionalModelOutput{\"TopKLogits\", /*whether or not to get the output for the context too */ true}};\nexecutor::Request request{requestTokens, parameters.maxOutputLength, true, executor::SamplingConfig{},\n    executor::OutputConfig{false, false, false, true, false, false, additionalOutputs}};\nexecutor.enqueueRequest(request);\n```\n\n----------------------------------------\n\nTITLE: Building Engine from Checkpoint\nDESCRIPTION: Shows the complete process of loading a LLaMA model from a checkpoint and building it into a TensorRT engine using the build API.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nllama = LLaMAForCausalLM.from_checkpoint(checkpoint_dir)\nengine = build(llama, build_config)\nengine.save(engine_dir)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for GPT-J\nDESCRIPTION: Command to build a TensorRT engine from the converted checkpoint, enabling various plugins for better performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Enable several TensorRT-LLM plugins to increase runtime performance. It also helps with build time.\ntrtllm-build --checkpoint_dir ./trt_ckpt/gptj_fp16_tp1/ \\\n             --output_dir ./trt_engines/gptj_fp16_tp1/ \\\n             --gemm_plugin float16 \\\n             --max_batch_size=32 \\\n             --max_input_len=1919 \\\n             --max_seq_len=2047\n```\n\n----------------------------------------\n\nTITLE: Serving DeepSeek-V3 Model with TensorRT-LLM\nDESCRIPTION: Command to serve the DeepSeek-V3 model using trtllm-serve, configuring various parameters such as host, port, tensor parallelism, and KV cache memory allocation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-serve \\\n  deepseek-ai/DeepSeek-V3 \\\n  --host localhost \\\n  --port 8000 \\\n  --backend pytorch \\\n  --max_batch_size 161 \\\n  --max_num_tokens 1160 \\\n  --tp_size 8 \\\n  --ep_size 4 \\\n  --pp_size 1 \\\n  --kv_cache_free_gpu_memory_fraction 0.95 \\\n  --extra_llm_api_options ./extra-llm-api-config.yml\n```\n\n----------------------------------------\n\nTITLE: Preparing and Running StarCoder2 with LoRA Fine-tuning\nDESCRIPTION: Complete workflow for running StarCoder2 with FP8 base model and FP16 LoRA modules. This demonstrates how to download models, quantize to FP8, build engines with LoRA support, and run inference with the fine-tuned model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\ngit-lfs clone https://huggingface.co/bigcode/starcoder2-15b\ngit-lfs clone https://huggingface.co/KaQyn/peft-lora-starcoder2-15b-unity-copilot\n\nBASE_STARCODER2_MODEL=./starcoder2-15b\npython ../quantization/quantize.py --model_dir ${BASE_STARCODER2_MODEL} \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir starcoder2-15b/trt_ckpt/fp8/1-gpu \\\n                                   --calib_size 512\n\ntrtllm-build --checkpoint_dir starcoder2-15b/trt_ckpt/fp8/1-gpu \\\n             --output_dir starcoder2-15b/trt_engines/fp8_lora/1-gpu \\\n             --gemm_plugin auto \\\n             --lora_plugin auto \\\n             --lora_dir ./peft-lora-starcoder2-15b-unity-copilot\n\npython ../run.py --engine_dir starcoder2-15b/trt_engines/fp8_lora/1-gpu \\\n                 --max_output_len 20 \\\n                 --tokenizer_dir ${BASE_STARCODER2_MODEL} \\\n                 --input_text \"def print_hello_world():\" \\\n                 --lora_task_uids 0 \\\n                 --no_add_special_tokens \\\n                 --use_py_session\n```\n\n----------------------------------------\n\nTITLE: Converting OPT Model to TensorRT-LLM Checkpoint\nDESCRIPTION: This bash command converts an OPT model to a TensorRT-LLM checkpoint with float16 precision and tensor parallelism of 2.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/checkpoint.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/opt\npython3 convert_checkpoint.py --model_dir ./opt-125m \\\n                --dtype float16 \\\n                --tp_size 2 \\\n                --output_dir ./opt/125M/trt_ckpt/fp16/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: Calibrating GPT-J for INT8 KV Cache\nDESCRIPTION: Command to perform INT8 calibration for KV cache to reduce memory footprint and improve performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# INT8 calibration\npython ../../../quantization/quantize.py --model_dir ./gpt-j-6b \\\n                                   --dtype float16 \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ./trt_ckpt/gptj_fp16_int8kv_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-7B-Chat (BF16)\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM checkpoints and build a single-GPU BF16 engine for Qwen-7B-Chat.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                              --output_dir ./tllm_checkpoint_1gpu_bf16 \\\n                              --dtype bfloat16\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_bf16 \\\n            --output_dir ./tmp/qwen/7B/trt_engines/bf16/1-gpu \\\n            --gpt_attention_plugin bfloat16 \\\n            --gemm_plugin bfloat16\n```\n\n----------------------------------------\n\nTITLE: Building and Running LLaMa-7B with Weight Streaming in TensorRT-LLM\nDESCRIPTION: Commands to convert a Hugging Face LLaMa-7B model, build a TensorRT engine with Weight Streaming enabled, and run it with 20% of weights in GPU memory. This demonstrates the complete workflow for using Weight Streaming.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/weight-streaming.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Convert model as normal. Assume hugging face model is in llama-7b-hf/\npython3 examples/llama/convert_checkpoint.py \\\n    --model_dir llama-7b-hf/ \\\n    --output_dir /tmp/llama_7b/trt_ckpt/fp16/1-gpu/ \\\n    --dtype float16\n\n# Build engine that enabled Weight Streaming.\ntrtllm-build \\\n    --checkpoint_dir /tmp/llama_7b/trt_ckpt/fp16/1-gpu/ \\\n    --output_dir /tmp/llama_7b/trt_engines/fp16/1-gpu/ \\\n    --weight_streaming \\\n    --gemm_plugin disable \\\n    --max_batch_size 128 \\\n    --max_input_len 512 \\\n    --max_seq_len 562\n\n# Run the engine with 20% weights in GPU memory.\npython3 examples/summarize.py \\\n    --engine_dir /tmp/llama_7b/trt_engines/fp16/1-gpu/ \\\n    --batch_size 1 \\\n    --test_trt_llm \\\n    --hf_model_dir llama-7b-hf/ \\\n    --data_type fp16 \\\n    --gpu_weights_percent 0.2\n```\n\n----------------------------------------\n\nTITLE: Running Modelopt Quantization\nDESCRIPTION: Commands for using Modelopt toolkit for quantization, supporting FP8, INT4 AWQ, and INT8 SQ formats. Includes steps for quantizing checkpoints and building engines for different quantization types.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py --model_dir ${HF_GEMMA_PATH} \\\n            --dtype float16 \\\n            --qformat ${QUANT_TYPE} \\\n            --output_dir ${UNIFIED_CKPT_PATH} \\\n            --tp_size 1\n\n# For fp8\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\n# For int4_awq and int8_sq\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving TensorRT-LLM Engine in Python\nDESCRIPTION: Demonstrates how to create a TensorRT-LLM engine from a model and save it to disk for later use. This process optimizes the model for faster inference on subsequent runs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/llm-api/index.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(<model-path>)\n\n# Save engine to local disk\nllm.save(<engine-dir>)\n```\n\n----------------------------------------\n\nTITLE: Building and Saving TensorRT-LLM Engine\nDESCRIPTION: Shows how to build and save a TensorRT-LLM engine for a Llama model using the LLM-API. Configures tensor parallelism across 4 GPUs and saves the engine to disk.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/benchmarking-default-performance.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm import LLM\n\ndef main():\n    llm = LLM(\n        model=\"/scratch/Llama-3.3-70B-Instruct\",\n        tensor_parallel_size=4\n    )\n\n    llm.save(\"baseline\")\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: ModelConfig Parameters Overview in C++\nDESCRIPTION: Lists the key configuration parameters that control model behavior including vocabulary size, model architecture, attention mechanism settings, and various runtime options for inference execution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-runtime.md#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nvocabSize - Size of vocabulary\nnumLayers - Number of model layers\nnumHeads - Number of attention heads\nnumKvHeads - Number of K/V heads in attention\nhiddenSize - Hidden dimension size\ndataType - Engine execution datatype\nuseGptAttentionPlugin - GPT Attention plugin usage flag\ninputPacked - Input packing flag\npagedKvCache - K/V cache paging flag\ntokensPerBlock - Tokens per K/V cache block\nquantMode - Quantization method control\nmaxBatchSize - Maximum supported batch size\nmaxInputLen - Maximum input sequence length\nmaxSequenceLen - Maximum total sequence length\n```\n\n----------------------------------------\n\nTITLE: Enabling Reduce Norm Fusion with User Buffers\nDESCRIPTION: This code snippet demonstrates how to enable the Reduce Norm Fusion and User Buffers optimizations for FP8 quantized models. These optimizations improve performance by reducing memory operations and eliminating extra copies in communication kernels.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/fp8-quantization.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbuild_config.plugin_config.reduce_fusion = True\nbuild_config.plugin_config.user_buffer = True\n```\n\n----------------------------------------\n\nTITLE: Running Text Summarization with TensorRT-LLM Falcon Engines\nDESCRIPTION: Commands to run the built Falcon TensorRT engines to perform text summarization on the CNN/DailyMail dataset. Examples shown for falcon-rw-1b and falcon-7b-instruct models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# falcon-rw-1b\npython ../../../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./falcon/rw-1b \\\n                       --engine_dir ./falcon/rw-1b/trt_engines/fp16/1-gpu/\n\n# falcon-7b-instruct\npython ../../../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./falcon/7b-instruct \\\n                       --engine_dir ./falcon/7b-instruct/trt_engines/bf16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building Mixtral 8x7B with Weight-Only Quantization\nDESCRIPTION: Commands to convert the Mixtral 8x7B checkpoint and build TensorRT engines with weight-only quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py --model_dir ./Mixtral-8x7B-v0.1 \\\n                             --output_dir ./tllm_checkpoint_mixtral_2gpu \\\n                             --dtype float16 \\\n                             --tp_size 2 \\\n                             --use_weight_only \\\n                             --weight_only_precision int8\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_2gpu \\\n                 --output_dir ./trt_engines/mixtral/tp2 \\\n                 --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Converting GPT-J to INT4 Weight-Only Format\nDESCRIPTION: Command to convert HuggingFace GPT-J to TensorRT-LLM with INT4 weight-only quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Build an int4 weight only quantization engine using int4 weight only quantized weights.\npython convert_checkpoint.py --model_dir ./gpt-j-6b \\\n                             --dtype float16 \\\n                             --use_weight_only \\\n                             --weight_only_precision int4 \\\n                             --output_dir ./trt_ckpt/gptj_int4wo_tp1/\n```\n\n----------------------------------------\n\nTITLE: Converting GPT-J to INT8 Weight-Only Format with Tensor Parallelism\nDESCRIPTION: Command to convert HuggingFace GPT-J to TensorRT-LLM with INT8 weight-only quantization and tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Build an int8 weight-only engine using HF weights with TP=2\npython convert_checkpoint.py --model_dir ./gpt-j-6b \\\n                             --dtype float16 \\\n                             --use_weight_only \\\n                             --weight_only_precision int8 \\\n                             --output_dir ./trt_ckpt/gptj_int8_tp2/ \\\n                             --tp_size 2\n```\n\n----------------------------------------\n\nTITLE: Building InternLM2 7B TensorRT Engine with FP16\nDESCRIPTION: Builds the TensorRT engine for InternLM2 7B model using the converted checkpoint with FP16 precision and gemm plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./internlm2-chat-7b/trt_engines/fp16/1-gpu/ \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Quantizing Starcoder2 Model with ModelOpt\nDESCRIPTION: Command to use ModelOpt for INT8 SmoothQuant quantization of Starcoder2 models. Provides an alternative approach for model quantization specific to GPT variants like Starcoder2.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython3 example/quantization/quantize.py --model_dir starcoder2 \\\n        --dtype float16 \\\n        --qformat int8_sq \\\n        --output_dir starcoder2/trt_ckpt/int8-sq/\n```\n\n----------------------------------------\n\nTITLE: Accessing Layer Information Using FLayerInfo in Python\nDESCRIPTION: Code snippet demonstrating how to retrieve and modify original input information for a TensorRT plugin using FLayerInfo. This approach is essential for working with plugins, which are otherwise opaque in the Python API.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/graph-rewriting.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nflayer = FLayerInfoMemo.instance().get(layer.name)\nassert flayer\n\nnew_inputs = flayer.clone_inputs()\n\n# Step 1: Create new inputs and replace the original arglist.\ninput = Tensor(\n    name='tensor',\n    dtype=trt.float16,\n    shape=(1, batch_size * in_len, hidden_size),\n)\nnew_inputs['tensor'] = input\n\n# Step 2: Create a new plugin instance.\nnew_outs = gpt_attention(**new_inputs)\n```\n\n----------------------------------------\n\nTITLE: Building and Saving an FP8 Engine from BF16 Checkpoint\nDESCRIPTION: This code demonstrates how to configure and build an FP8 quantized engine from a BF16 checkpoint using the TensorRT-LLM API. It sets up quantization configuration, calibration parameters, and build options before saving the engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/fp8-quantization.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm import LLM, BuildConfig\nfrom tensorrt_llm.llmapi import QuantConfig, QuantAlgo, CalibConfig\n\ndef main():\n\n    quant_config = QuantConfig(quant_algo=QuantAlgo.FP8)\n\n    calib_config = CalibConfig(\n        calib_batches=512,\n        calib_batch_size=1,\n        calib_max_seq_length=2048,\n        tokenizer_max_seq_length=4096\n    )\n\n    build_config = BuildConfig(\n        max_num_tokens=2048,\n        max_batch_size=512,\n    )\n\n    build_config.plugin_config.use_paged_context_fmha = True\n    build_config.plugin_config.multiple_profiles = True\n\n    llm = LLM(\n        model=\"/path/to/Llama-3.3-70B\",\n        tensor_parallel_size=4,\n        pipeline_parallel_size=1,\n        build_config=build_config,\n        quant_config=quant_config,\n        calib_config=calib_config\n    )\n\n    llm.save(\"baseline_fp8_engine\")\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom CapacityScheduler in TensorRT-LLM\nDESCRIPTION: This code snippet demonstrates how to implement a custom CapacityScheduler called GuaranteedNoEvictScheduler. This scheduler ensures requests can complete without eviction by checking if there are enough resources in the KV cache, prioritizing requests in generation phase over context phase.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/scheduler.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass GuaranteedNoEvictScheduler(CapacityScheduler):\n    # only schedule requests has no_schedule_until_state <= state < no_schedule_after_state\n    no_schedule_until_state = LlmRequestState.CONTEXT_INIT\n    no_schedule_after_state = LlmRequestState.GENERATION_COMPLETE\n\n    def __init__(self, max_num_requests: int, kv_cache_manager):\n        super(GuaranteedNoEvictScheduler, self).__init__()\n        self.max_num_requests = max_num_requests\n        self.kv_cache_manager = kv_cache_manager\n\n    def schedule_request(\n        self, active_requests: RequestList\n    ) -> tuple[list[LlmRequest], list[LlmRequest]]:\n        scheduled_requests = []\n        pending_requests = []\n        reserved_blocks = 0\n        max_blocks = self.kv_cache_manager.get_max_resource_count()\n        for request in active_requests:\n            req_state = request.state\n            # if request cannot be scheduled yet or request should no longer be scheduled, skip\n            if req_state.value < self.no_schedule_until_state.value or req_state.value >= self.no_schedule_after_state.value:\n                continue\n\n            if len(scheduled_requests\n                   ) >= self.max_num_requests or reserved_blocks >= max_blocks:\n                break\n            elif req_state == LlmRequestState.GENERATION_IN_PROGRESS or req_state == LlmRequestState.GENERATION_TO_COMPLETE:\n                scheduled_requests.append(request)\n                reserved_blocks += self.kv_cache_manager.get_needed_resource_to_completion(\n                    request)\n            else:\n                pending_requests.append(request)\n\n        avaiable_blocks = max_blocks - reserved_blocks\n        for request in pending_requests:\n            req_state = request.state\n            if len(scheduled_requests) >= self.max_num_requests:\n                break\n            elif req_state == LlmRequestState.CONTEXT_INIT:\n                needed_blocks = self.kv_cache_manager.get_needed_resource_to_completion(\n                    request)\n                if needed_blocks <= avaiable_blocks:\n                    scheduled_requests.append(request)\n                    avaiable_blocks -= needed_blocks\n                elif needed_blocks > avaiable_blocks:\n                    # If one requests fails to be scheduled, break\n                    break\n\n        assert len(scheduled_requests) > 0, (\n            \"no pending request can get enough resource to complete, \"\n            \"please increase KV cache pool size.\")\n        return scheduled_requests, []\n```\n\n----------------------------------------\n\nTITLE: Running INT8 Weight-Only Inference for Gemma 2B-IT Model Using JAX Checkpoint\nDESCRIPTION: This script demonstrates INT8 weight-only quantization for Gemma 2B-IT using a JAX checkpoint. The technique quantizes only the weights to INT8 while keeping activations in BFloat16. The process includes cloning the model, converting the checkpoint with INT8 weight-only precision, building the engine, and running summarize.py for evaluation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-2b-it-flax\nCKPT_PATH=gemma-2b-it-flax/2b-it/\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_2b_it_tensorrt_llm/w8_a16/tp1/\nENGINE_PATH=/tmp/gemma/2B/w8_a16/1-gpu/\nVOCAB_FILE_PATH=gemma-2b-it-flax/tokenizer.model\n\npython3 ./convert_checkpoint.py \\\n    --ckpt-type jax \\\n    --model-dir ${CKPT_PATH} \\\n    --use-weight-only-with-precision int8 \\\n    --dtype bfloat16 \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n                 --gemm_plugin auto \\\n                 --max_batch_size 32 \\\n                 --max_input_len 3000 \\\n                 --max_seq_len 3100 \\\n                 --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n\n[02/08/2024-04:44:54] [TRT-LLM] [I] TensorRT-LLM (total latency: 3.5987987518310547 sec)\n[02/08/2024-04:44:54] [TRT-LLM] [I] TensorRT-LLM (total output tokens: 1797)\n[02/08/2024-04:44:54] [TRT-LLM] [I] TensorRT-LLM (tokens per second: 499.3332842203787)\n[02/08/2024-04:44:54] [TRT-LLM] [I] TensorRT-LLM beam 0 result\n[02/08/2024-04:44:54] [TRT-LLM] [I]   rouge1 : 24.48521318679745\n[02/08/2024-04:44:54] [TRT-LLM] [I]   rouge2 : 7.240543314565931\n[02/08/2024-04:44:54] [TRT-LLM] [I]   rougeL : 17.857921729984078\n[02/08/2024-04:44:54] [TRT-LLM] [I]   rougeLsum : 21.214162155642896\n```\n\n----------------------------------------\n\nTITLE: Running Falcon-180B with 8-Way Tensor Parallelism\nDESCRIPTION: Command to run the Falcon-180B model using 8-way tensor parallelism configuration with TensorRT-LLM's summarize script.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 8 --allow-run-as-root --oversubscribe \\\n    python ../../../summarize.py --test_trt_llm \\\n                           --hf_model_dir ./falcon/180b \\\n                           --engine_dir ./falcon/180b/trt_engines/bf16/tp8-pp1/\n```\n\n----------------------------------------\n\nTITLE: Quantizing Models with TensorRT Model Optimizer\nDESCRIPTION: Shell commands for cloning the TensorRT Model Optimizer repository and running quantization on Hugging Face models. Supports FP8 quantization format with HuggingFace export.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/TensorRT-Model-Optimizer.git\ncd TensorRT-Model-Optimizer/examples/llm_ptq\nscripts/huggingface_example.sh --model <huggingface_model_card> --quant fp8 --export_fmt hf\n```\n\n----------------------------------------\n\nTITLE: Running Medusa-Enabled Low Latency Benchmark (Shell)\nDESCRIPTION: This command runs a low latency benchmark for a Medusa-enabled engine. It sets various environment variables for optimization, specifies the model, dataset, engine directory, and Medusa choices file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nenv TRTLLM_ENABLE_PDL=1 \\\n  UB_ONESHOT=1 \\\n  UB_TP_SIZE=$tp_size \\\n  TRTLLM_ENABLE_PDL=1 \\\n  TRTLLM_PDL_OVERLAP_RATIO=0.15 \\\n  TRTLLM_PREFETCH_RATIO=-1 \\\n  trtllm-bench --model meta-llama/Meta-Llama-3-70B \\\n  latency \\\n  --dataset $DATASET_PATH \\\n  --engine_dir /tmp/meta-llama/Meta-Llama-3-70B/medusa/engine \\\n  --medusa_choices medusa.yml\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with FP8 Quantization\nDESCRIPTION: Builds a TensorRT-LLM engine with FP8 quantization using dataset statistics for optimization. Supports tensor and pipeline parallelism configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md#2025-04-07_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ntrtllm-bench --model $model_name build --tp_size $tp_size --pp_size $pp_size --quantization FP8 --dataset $dataset_file\n```\n\n----------------------------------------\n\nTITLE: Running Falcon-40B-Instruct with 2-Way Tensor and 2-Way Pipeline Parallelism\nDESCRIPTION: Command to run the Falcon-40B-Instruct model using combined 2-way tensor parallelism and 2-way pipeline parallelism (total 4 GPUs) with TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 4 --allow-run-as-root --oversubscribe \\\n    python ../../../summarize.py --test_trt_llm \\\n                           --hf_model_dir ./falcon/40b-instruct \\\n                           --engine_dir ./falcon/40b-instruct/trt_engines/bf16/tp2-pp2/\n```\n\n----------------------------------------\n\nTITLE: Running Falcon-180B with 4-Way Tensor and 2-Way Pipeline Parallelism\nDESCRIPTION: Command to run the Falcon-180B model using combined 4-way tensor parallelism and 2-way pipeline parallelism (total 8 GPUs) with TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 8 --allow-run-as-root --oversubscribe \\\n    python ../../../summarize.py --test_trt_llm \\\n                           --hf_model_dir ./falcon/180b \\\n                           --engine_dir ./falcon/180b/trt_engines/bf16/tp4-pp2/\n```\n\n----------------------------------------\n\nTITLE: Running Qwen Model with INT8 Weight-Only Quantization\nDESCRIPTION: Command to run inference on a TensorRT-LLM Qwen model with INT8 weight-only quantization. It demonstrates how to use the quantized model for generation tasks.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --input_text \"你好，请问你叫什么？\" \\\n                  --max_output_len=50 \\\n                  --tokenizer_dir ./tmp/Qwen/7B/ \\\n                  --engine_dir=./tmp/Qwen/7B/trt_engines/int8_weight_only/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Implementing Weight-Only Quantization\nDESCRIPTION: Commands to enable INT8 weight-only quantization for reduced latency and memory usage.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir command_r_v01 \\\n        --use_weight_only \\\n        --weight_only_precision int8 \\\n        --output_dir trt_ckpt/command_r_v01/int8_wo/1-gpu\n\ntrtllm-build --checkpoint_dir trt_ckpt/command_r_v01/int8_wo/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/command_r_v01/int8_wo/1-gpu\n\npython3 ../run.py --max_output_len 50 \\\n        --tokenizer_dir command_r_v01 \\\n        --engine_dir trt_engines/command_r_v01/int8_wo/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Calculating Scaling Factor for MHA in Python\nDESCRIPTION: This snippet shows how to compute the scaling factor used in Multi-Head Attention (MHA) for the Q*K^T product. It uses the q_scaling parameter and the square root of the head size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-attention.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnorm_factor = 1.f / (q_scaling * sqrt(head_size))\n```\n\n----------------------------------------\n\nTITLE: Running Throughput Benchmark for Llama-3.1-8B\nDESCRIPTION: Command to execute a throughput benchmark on the built engine using the synthetic dataset, measuring tokens per second and other performance metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-bench --model meta-llama/Llama-3.1-8B throughput --dataset /tmp/synthetic_128_128.txt --engine_dir /tmp/meta-llama/Llama-3.1-8B/tp_1_pp_1\n```\n\n----------------------------------------\n\nTITLE: Implementing Asyncio-Based Generation in TensorRT-LLM\nDESCRIPTION: Sets up asynchronous text generation with streaming capabilities using generate_async. This allows token-by-token streaming when the streaming flag is enabled or getting only the final result otherwise.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(model=<llama_model_path>)\n\nasync for output in llm.generate_async(<prompt>, streaming=True):\n    print(output)\n```\n\n----------------------------------------\n\nTITLE: Slurm Script for Llama 3.1 405B Inference\nDESCRIPTION: Bash script for running Llama 3.1 405B inference using Slurm on 2 nodes with 8 GPUs each, demonstrating multi-node execution with TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n#SBATCH --account account\n#SBATCH --partition partition\n#SBATCH --job-name job-name\n#SBATCH --time 1:00:00\n#SBATCH --nodes 2\n\nfolder_trt_llm=../TensorRT-LLM\nengine_dir=engine_llama_3.1_405b\nmodel_dir=Llama-3.1-405B\nmax_output_len=128\n\ninput_text=\"Born in north-east France, Soyer trained as a\"\n\nsrun \\\n    --ntasks-per-node 8 \\\n    --mpi pmix \\\n    python ${folder_trt_llm}/examples/run.py \\\n        --engine_dir ${engine_dir} \\\n        --tokenizer_dir ${model_dir} \\\n        --input_text \"${input_text}\" \\\n        --max_output_len ${max_output_len}\n```\n\n----------------------------------------\n\nTITLE: Building and Running StarCoder Model with Tensor Parallelism\nDESCRIPTION: Complete pipeline for downloading, converting, building, and running the StarCoder model with 4-way tensor parallelism. The workflow is similar to SantaCoder but with StarCoder-specific configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n# Download hf starcoder model\ngit clone https://huggingface.co/bigcode/starcoder\n\n# Convert to TensorRT-LLM checkpoint\npython3 convert_checkpoint.py --model_dir starcoder \\\n        --dtype float16 \\\n        --tp_size 4 \\\n        --output_dir starcoder/trt_ckpt/fp16/4-gpu\n\n# Build TensorRT-LLM engines\ntrtllm-build --checkpoint_dir starcoder/trt_ckpt/fp16/4-gpu \\\n        --gemm_plugin auto \\\n        --output_dir starcoder/trt_engines/fp16/4-gpu\n\n# Run inference\nmpirun -np 4 \\\n    python3 ../run.py --engine_dir starcoder/trt_engines/fp16/4-gpu \\\n        --tokenizer_dir starcoder \\\n        --input_text \"def print_hello_world():\" \\\n        --max_output_len 20\n```\n\n----------------------------------------\n\nTITLE: Implementing Future-Style Generation in TensorRT-LLM\nDESCRIPTION: Demonstrates non-blocking generation using generate_async with explicit result retrieval. This approach allows other operations to continue while generation happens in the background.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# This will not block the main thread\ngeneration = llm.generate_async(<prompt>)\n# Do something else here\n# call .result() to explicitly block the main thread and wait for the result when needed\noutput = generation.result()\n```\n\n----------------------------------------\n\nTITLE: Quantizing BART-large-CNN Model to FP8 with TensorRT-LLM\nDESCRIPTION: Command script to quantize the BART-large-CNN model into FP8 precision using 4-way tensor parallelism. The script sets environment variables for model configuration and runs the quantization script with specific parameters for calibration size, batch size, and parallelism settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"bart-large-cnn\"\nexport MODEL_TYPE=\"bart\"\nexport INFERENCE_PRECISION=\"float16\"\nexport TP_SIZE=4\nexport PP_SIZE=1\nexport WORLD_SIZE=4\nexport MAX_BEAM_WIDTH=1\npython ../quantization/quantize.py \\\n                --model_dir tmp/hf_models/${MODEL_NAME} \\\n                --dtype ${INFERENCE_PRECISION} \\\n                --qformat fp8 \\\n                --kv_cache_dtype fp8 \\\n                --output_dir tmp/trt_models/${MODEL_NAME}/fp8 \\\n                --calib_size 512 \\\n                --batch_size 16 \\\n                --tp_size ${TP_SIZE} \\\n                --pp_size ${PP_SIZE}\n```\n\n----------------------------------------\n\nTITLE: Quantizing GPT-J to FP8 Format with Modelopt\nDESCRIPTION: Command to quantize HuggingFace GPT-J weights to FP8 format using NVIDIA's Modelopt toolkit.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Quantize HF GPT-J 6B checkpoint into FP8 format\npython ../../../quantization/quantize.py --model_dir ./gpt-j-6b \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir ./trt_ckpt/gptj_fp8_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Implementing INT8 Quantization in Python\nDESCRIPTION: Demonstrates how to implement INT8 quantization for per-tensor, per-token, and per-channel scaling modes using Python. This snippet shows the logic for quantizing floating-point values to INT8 using different scaling factor approaches.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/precision.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Per-tensor scaling.\nfor mi in range(M):\n    for ni in range(N):\n        q[mi][ni] = int8.satfinite(x[mi][ni] * s)\n\n# Per-token scaling.\nfor mi in range(M):\n    for ni in range(N):\n        q[mi][ni] = int8.satfinite(x[mi][ni] * s[mi])\n\n# Per-channel scaling.\nfor mi in range(M):\n    for ni in range(N):\n        q[mi][ni] = int8.satfinite(x[mi][ni] * s[ni])\n```\n\n----------------------------------------\n\nTITLE: Testing TensorRT Engine with run.py Script\nDESCRIPTION: Command to test the built TensorRT engine using the run.py script with MPI for multi-GPU execution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 python3 ../run.py --engine_dir ./trt_engines/mixtral/tp2 --tokenizer_dir ./Mixtral-8x7B-v0.1 --max_output_len 8 --input_text \"I love french quiche\"\n```\n\n----------------------------------------\n\nTITLE: Quantizing GPT-J with AWQ INT4 Weight-Only Format\nDESCRIPTION: Command to enable AWQ INT4 group-wise weight-only quantization for GPT-J model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Enable AWQ int4 group-wise weight-only quantization.\npython ../../../quantization/quantize.py --model_dir ./gpt-j-6b \\\n                                   --dtype float16 \\\n                                   --qformat int4_awq \\\n                                   --output_dir ./trt_ckpt/gptj_int4_awq_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Building InternLM2 20B BFloat16 Engine with Tensor Parallelism\nDESCRIPTION: Builds the TensorRT engines for InternLM2 20B model using BFloat16 precision with attention and gemm plugins for optimal performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./internlm2-chat-7b/trt_engines/bf16/2-gpu/ \\\n             --output_dir ./engine_outputs \\\n             --gpt_attention_plugin bfloat16  \\\n             --gemm_plugin bfloat16\n```\n\n----------------------------------------\n\nTITLE: Quantizing Qwen Model to FP8 with Modelopt\nDESCRIPTION: Command to quantize a Qwen-7B model to FP8 precision using NVIDIA Modelopt toolkit. It includes FP8 KV cache and uses calibration to optimize quantization accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py --model_dir ./tmp/Qwen/7B/ \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir ./tllm_checkpoint_1gpu_fp8 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Integrating Custom Scheduler with PyExecutor in TensorRT-LLM\nDESCRIPTION: This code snippet shows how to create and integrate a CapacityScheduler with the PyExecutor in TensorRT-LLM's PyTorch backend. It demonstrates how to initialize a BindCapacityScheduler with maximum request limits and a KV cache manager implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/scheduler.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    capacitor_scheduler = BindCapacityScheduler(max_num_requests,\n                                                kv_cache_manager.impl)\n```\n\n----------------------------------------\n\nTITLE: Converting and Building TensorRT Engines for Single/Multi GPU\nDESCRIPTION: Commands for converting models and building TensorRT engines with Medusa decoding support, including examples for different model sizes and tensor parallelism configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./vicuna-7b-v1.3 \\\n                            --medusa_model_dir medusa-vicuna-7b-v1.3 \\\n                            --output_dir ./tllm_checkpoint_1gpu_medusa \\\n                            --dtype float16 \\\n                            --num_medusa_heads 4\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_medusa \\\n             --output_dir ./tmp/medusa/7B/trt_engines/fp16/1-gpu/ \\\n             --gemm_plugin float16 \\\n             --speculative_decoding_mode medusa \\\n             --max_batch_size 4\n```\n\n----------------------------------------\n\nTITLE: Building and Running Granite Model with Tensor Parallelism\nDESCRIPTION: Complete pipeline for downloading, converting, building, and running the IBM Granite 34B model with 4-way tensor parallelism. The code snippet demonstrates the full workflow from model acquisition to inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n# Download hf granite model\ngit clone https://huggingface.co/ibm-granite/granite-34b-code-instruct granite\n\n# Convert to TensorRT-LLM checkpoint\npython3 convert_checkpoint.py --model_dir granite \\\n        --dtype float16 \\\n        --gpt_variant starcoder \\\n        --tp_size 4 \\\n        --output_dir granite/trt_ckpt/fp16/4-gpu\n\n# Build TensorRT-LLM engines\ntrtllm-build --checkpoint_dir granite/trt_ckpt/fp16/4-gpu \\\n        --gemm_plugin auto \\\n        --output_dir granite/trt_engines/fp16/4-gpu\n\n# Run inference\nmpirun -np 4 \\\n    python3 ../run.py --engine_dir granite/trt_engines/fp16/4-gpu \\\n        --tokenizer_dir granite \\\n        --input_text \"def print_hello_world():\" \\\n        --max_output_len 20\n```\n\n----------------------------------------\n\nTITLE: Converting Qwen with Per-Token and Per-Channel SmoothQuant\nDESCRIPTION: Command to convert a Qwen-7B model with advanced SmoothQuant settings including per-token and per-channel optimizations for more fine-grained INT8 inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                              --output_dir ./tllm_checkpoint_1gpu_sq \\\n                              --dtype float16 \\\n                              --smoothquant 0.5 \\\n                              --per_token \\\n                              --per_channel\n```\n\n----------------------------------------\n\nTITLE: Performing Post-Training Quantization (PTQ) in PyTorch with ModelOpt\nDESCRIPTION: This snippet demonstrates how to perform Post-Training Quantization on a PyTorch model using the ModelOpt toolkit. It shows both standard quantization with a fixed configuration and auto-quantization with constraints-based optimization using calibration data.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM\nimport modelopt.torch.quantization as mtq\nimport modelopt.torch.utils.dataset_utils as dataset_utils\n\nmodel = AutoModelForCausalLM.from_pretrained(...)\n\n# Select the quantization config, for example, FP8\nconfig = mtq.FP8_DEFAULT_CFG\n\n# Prepare the calibration set and define a forward loop\ncalib_dataloader = DataLoader(...)\ncalibrate_loop = dataset_utils.create_forward_loop(\n    calib_dataloader, dataloader=calib_dataloader\n)\n\n# PTQ with in-place replacement to quantized modules\nwith torch.no_grad():\n    mtq.quantize(model, config, forward_loop=calibrate_loop)\n\n# or PTQ with auto quantization\nwith torch.no_grad():\n    model, search_history = mtq.auto_quantize(\n        model,\n        data_loader=calib_dataloader,\n        loss_func=lambda output, batch: output.loss,\n        constraints={\"effective_bits\": auto_quantize_bits}, # The average bits of quantized weights\n        forward_step=lambda model, batch: model(**batch),\n        quantization_formats=[quant_algo1, quant_algo2,...] + [None],\n        num_score_steps=min(\n        num_calib_steps=len(calib_dataloader),\n            len(calib_dataloader), 128 // batch_size\n        ),  # Limit the number of score steps to avoid long calibration time\n        verbose=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Quantizing GPT-J with INT8 KV Cache and AWQ INT4\nDESCRIPTION: Command to enable INT8 KV cache together with AWQ (per-group INT4 weight-only quantization) for GPT-J.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Enable INT8 KV cache together with group-wise 4bit AWQ quantization\npython ../../../quantization/quantize.py --model_dir ./gpt-j-6b \\\n                                   --dtype float16 \\\n                                   --qformat int4_awq \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ./trt_ckpt/gptj_int4awq_int8kv_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Customizing Runtime Configuration in TensorRT-LLM\nDESCRIPTION: Configures the runtime behavior of the LLM using KvCacheConfig to manage GPU memory allocation. This example sets the free_gpu_memory_fraction parameter to control KV cache memory usage.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm.llmapi import LLM, KvCacheConfig\n\nllm = LLM(<llama_model_path>,\n          kv_cache_config=KvCacheConfig(\n            free_gpu_memory_fraction=0.8))\n```\n\n----------------------------------------\n\nTITLE: Building W4A8 AWQ EXAONE Engine for Hopper GPUs\nDESCRIPTION: Commands to apply W4A8 AWQ quantization to EXAONE for Hopper GPUs. This mode converts both weights and activations from W4A16 to FP8 for GEMM calculation, providing accelerated performance on Hopper architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py \\\n    --model_dir $HF_MODEL_DIR \\\n    --dtype float16 \\\n    --qformat w4a8_awq \\\n    --output_dir trt_models/exaone/w4a8_awq/1-gpu\n\ntrtllm-build \\\n    --checkpoint_dir trt_models/exaone/w4a8_awq/1-gpu \\\n    --output_dir trt_engines/exaone/w4a8_awq/1-gpu \\\n    --gemm_plugin auto\n```\n\n----------------------------------------\n\nTITLE: Quantizing Models with TensorRT-LLM Quantization Toolkit\nDESCRIPTION: Examples of using the quantize.py script for various quantization formats and model types, including FP8, INT4_AWQ, INT8 SQ, auto quantization, NeMo models, and Medusa models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# FP8 quantization.\npython quantize.py --model_dir $MODEL_PATH --qformat fp8 --kv_cache_dtype fp8 --output_dir $OUTPUT_PATH\n\n# INT4_AWQ tp4 quantization.\npython quantize.py --model_dir $MODEL_PATH --qformat int4_awq --awq_block_size 64 --tp_size 4 --output_dir $OUTPUT_PATH\n\n# INT8 SQ with INT8 kv cache.\npython quantize.py --model_dir $MODEL_PATH --qformat int8_sq --kv_cache_dtype int8 --output_dir $OUTPUT_PATH\n\n# Auto quantization(e.g. fp8 + int4_awq + w4a8_awq) using average weights bits 5\npython quantize.py --model_dir $MODEL_PATH  --autoq_format fp8,int4_awq,w4a8_awq  --output_dir $OUTPUT_PATH --auto_quantize_bits 5 --tp_size 2\n\n# FP8 quantization for NeMo model.\npython quantize.py --nemo_ckpt_path nemotron-3-8b-base-4k/Nemotron-3-8B-Base-4k.nemo \\\n                   --dtype bfloat16 \\\n                   --batch_size 64 \\\n                   --qformat fp8 \\\n                   --output_dir nemotron-3-8b/trt_ckpt/fp8/1-gpu\n\n# FP8 quantization for Medusa model.\npython quantize.py --model_dir $MODEL_PATH\\\n                   --dtype float16 \\\n                   --qformat fp8 \\\n                   --kv_cache_dtype fp8 \\\n                   --output_dir $OUTPUT_PATH \\\n                   --calib_size 512 \\\n                   --tp_size 1 \\\n                   --medusa_model_dir /path/to/medusa_head/ \\\n                   --num_medusa_heads 4\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU Example with MPI\nDESCRIPTION: Command for running the basic example with MPI to support models requiring multiple GPUs. This demonstrates the distributed inference capabilities of TensorRT-LLM across multiple GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 4 --allow-run-as-root python3 example_basic.py --model_path=../llama/tmp/7B/trt_engines/fp16/4gpu_tp4_pp1/\n```\n\n----------------------------------------\n\nTITLE: Running Inference with RecurrentGemma-2b-it INT8 SmoothQuant TensorRT Engine\nDESCRIPTION: This command runs inference using the RecurrentGemma-2b-it INT8 SmoothQuant TensorRT engine with Python session and specified attention window size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it INT8 SmoothQuant with INT8 kv cache\npython3 ../run.py --max_output_len=100 \\\n                  --use_py_session \\\n                  --max_attention_window_size 2048 \\\n                  --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH} \\\n                  --engine_dir ${ENGINE_2B_IT_INT8_SQ_PATH}\n```\n\n----------------------------------------\n\nTITLE: Building Single-GPU INT8 Weight-Only Quantized Engine\nDESCRIPTION: Commands to apply INT8 weight-only quantization to BLOOM-560M model and build a TensorRT engine for single GPU deployment.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./bloom/560M/ \\\n                --dtype float16 \\\n                --use_weight_only \\\n                --output_dir ./bloom/560M/trt_ckpt/int8_weight_only/1-gpu/\ntrtllm-build --checkpoint_dir ./bloom/560M/trt_ckpt/int8_weight_only/1-gpu/ \\\n                --gemm_plugin float16 \\\n                --output_dir ./bloom/560M/trt_engines/int8_weight_only/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with LoRA for Qwen-7B\nDESCRIPTION: Converts the Qwen-7B checkpoint and builds a TensorRT-LLM engine with LoRA support. It sets the LoRA plugin and directory for adaptation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                              --output_dir ./tllm_checkpoint_1gpu_fp16 \\\n                              --dtype float16\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_fp16 \\\n            --output_dir ./tmp/qwen/7B_lora/trt_engines/fp16/1-gpu \\\n            --gemm_plugin auto \\\n            --lora_plugin auto \\\n            --lora_dir ./tmp/Ko-QWEN-7B-Chat-LoRA\n```\n\n----------------------------------------\n\nTITLE: Enabling Paged Context Attention in TensorRT-LLM\nDESCRIPTION: This code snippet shows how to enable paged context attention, which allows TensorRT-LLM to process large prompts in chunks rather than all at once. This is particularly useful for workloads with large input lengths and can be added to the BuildConfig configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbuild_config.plugin_config.use_paged_context_fmha=True\n```\n\n----------------------------------------\n\nTITLE: Running executorExampleBasic Example\nDESCRIPTION: Command to run the basic Executor example which demonstrates simple token generation for a single prompt. It requires specifying the path to the TensorRT engine directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./executorExampleBasic <path_to_engine_dir>\n```\n\n----------------------------------------\n\nTITLE: Building DBRX with INT4 Weight-Only Quantization\nDESCRIPTION: Converts DBRX using INT4 weight-only quantization with 4-way tensor parallelism for extreme memory efficiency, then builds corresponding TensorRT engines. This offers the highest memory savings with some accuracy trade-off.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# 4-way tensor parallelism, int4 weight-only\npython convert_checkpoint.py --model_dir dbrx-base \\\n        --dtype float16 \\\n        --use_weight_only \\\n        --weight_only_precision int4 \\\n        --tp_size 4 \\\n        --workers 4 \\\n        --output_dir dbrx/trt_ckpt/int4-wo/tp4\n\ntrtllm-build --checkpoint_dir dbrx/trt_ckpt/int4-wo/tp4 \\\n        --gpt_attention_plugin float16 \\\n        --gemm_plugin float16 \\\n        --moe_plugin float16 \\\n        --workers 4 \\\n        --output_dir dbrx/trt_engines/int4-wo/tp4\n```\n\n----------------------------------------\n\nTITLE: Complete Benchmark Workflow for Llama-3.1-8B\nDESCRIPTION: Full sequence of commands to run a benchmark from dataset preparation to performance measurement for Llama-3.1-8B model with FP8 quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/cpp/prepare_dataset.py --stdout --tokenizer meta-llama/Llama-3.1-8B token-norm-dist --input-mean 128 --output-mean 128 --input-stdev 0 --output-stdev 0 --num-requests 3000 > /tmp/synthetic_128_128.txt\ntrtllm-bench --model meta-llama/Llama-3.1-8B build --dataset /tmp/synthetic_128_128.txt --quantization FP8\ntrtllm-bench --model meta-llama/Llama-3.1-8B throughput --dataset /tmp/synthetic_128_128.txt --engine_dir /tmp/meta-llama/Llama-3.1-8B/tp_1_pp_1\n```\n\n----------------------------------------\n\nTITLE: Building LLaMA-7B with LoRA Support in TensorRT-LLM\nDESCRIPTION: These commands clone needed LoRA models, convert the base LLaMA-7B model to TensorRT-LLM format, and build an engine with LoRA and in-flight batching support.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/lora.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit-lfs clone https://huggingface.co/qychen/luotuo-lora-7b-0.1\ngit-lfs clone https://huggingface.co/kunishou/Japanese-Alpaca-LoRA-7b-v0\nBASE_MODEL=llama-7b-hf\n\npython examples/llama/convert_checkpoint.py --model_dir ${BASE_MODEL} \\\n    --output_dir /tmp/llama_7b/trt_ckpt/fp16/1-gpu/ \\\n    --dtype float16\n\ntrtllm-build --checkpoint_dir /tmp/llama_7b/trt_ckpt/fp16/1-gpu/ \\\n    --output_dir /tmp/llama_7b_with_lora_qkv/trt_engines/fp16/1-gpu/ \\\n    --remove_input_padding enable \\\n    --gpt_attention_plugin float16 \\\n    --context_fmha enable \\\n    --paged_kv_cache enable \\\n    --gemm_plugin float16 \\\n    --lora_plugin float16 \\\n    --max_batch_size 128 \\\n    --max_input_len 512 \\\n    --max_seq_len 562 \\\n    --lora_dir Japanese-Alpaca-LoRA-7b-v0 \\\n    --max_lora_rank 8 \\\n    --lora_target_modules \"attn_q\" \"attn_k\" \"attn_v\"\n```\n\n----------------------------------------\n\nTITLE: Building 2-way Tensor Parallel Engine for BLOOM-560M\nDESCRIPTION: Commands to convert and build a BLOOM-560M TensorRT engine with 2-way tensor parallelism for multi-GPU deployment.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./bloom/560M/ \\\n                --dtype float16 \\\n                --output_dir ./bloom/560M/trt_ckpt/fp16/2-gpu/ \\\n                --tp_size 2\ntrtllm-build --checkpoint_dir ./bloom/560M/trt_ckpt/fp16/2-gpu/ \\\n                --gemm_plugin float16 \\\n                --output_dir ./bloom/560M/trt_engines/fp16/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: Querying DeepSeek-V3 Server with cURL\nDESCRIPTION: cURL command to send a completion request to the DeepSeek-V3 model server, specifying the prompt, max tokens, and temperature.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n      \"model\": \"deepseek-ai/DeepSeek-V3\",\n      \"prompt\": \"Where is New York?\",\n      \"max_tokens\": 16,\n      \"temperature\": 0\n  }'\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Checkpoint into TensorRT Engine\nDESCRIPTION: This bash command builds a TensorRT-LLM checkpoint into a TensorRT engine, specifying precision, batch size, and sequence length parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/checkpoint.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport PATH=/usr/local/bin:$PATH\n\ntrtllm-build --checkpoint_dir ./opt/125M/trt_ckpt/fp16/2-gpu/ \\\n                --gemm_plugin float16 \\\n                --max_batch_size 8 \\\n                --max_input_len 924 \\\n                --max_seq_len 1024 \\\n                --output_dir ./opt/125M/trt_engines/fp16/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: Running Phi-3 with LoRA Fine-tuning\nDESCRIPTION: Series of commands to set up and run Phi-3 model with LoRA fine-tuning, including model download and inference\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/phi/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit-lfs clone https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\ngit-lfs clone https://huggingface.co/sikoraaxd/Phi-3-mini-4k-instruct-ru-lora\n```\n\nLANGUAGE: bash\nCODE:\n```\nBASE_PHI_3_MINI_MODEL=./Phi-3-mini-4k-instruct\npython ../quantization/quantize.py --model_dir ${BASE_PHI_3_MINI_MODEL} \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir phi3_mini_4k_instruct/trt_ckpt/fp8/1-gpu \\\n                                   --calib_size 512\n```\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir phi3_mini_4k_instruct/trt_ckpt/fp8/1-gpu \\\n             --output_dir phi3_mini_4k_instruct/trt_engines/fp8_lora/1-gpu \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 1024 \\\n             --max_seq_len 2048 \\\n             --lora_plugin auto \\\n             --lora_dir ./Phi-3-mini-4k-instruct-ru-lora\n```\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir phi3_mini_4k_instruct/trt_engines/fp8_lora/1-gpu \\\n                 --max_output_len 500 \\\n                 --tokenizer_dir ./Phi-3-mini-4k-instruct-ru-lora \\\n                 --input_text \"<|user|>\\nCan you provide ways to eat combinations of bananas and dragonfruits?<|end|>\\n<|assistant|>\" \\\n                 --lora_task_uids 0 \\\n                 --use_py_session\n```\n\n----------------------------------------\n\nTITLE: INT8 KV Cache with SmoothQuant\nDESCRIPTION: Implements INT8 KV cache with INT8 SmoothQuant optimization. Uses float16 dtype with per-channel and per-token smoothing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_version v1_13b \\\n                --model_dir baichuan-inc/Baichuan-13B-Chat \\\n                --dtype float16 \\\n                --smoothquant 0.8 \\\n                --per_channel \\\n                --per_token \\\n                --int8_kv_cache \\\n                --output_dir ./tmp/baichuan_v1_13b/sq0.8/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Running Grok-1 Inference\nDESCRIPTION: Command to run inference using the built Grok-1 model across 8 GPUs with specific generation parameters and tokenizer configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/grok/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 8 --allow-run-as-root \\\n    python ../../../run.py \\\n    --input_text \"The answer to life the universe and everything is of course\" \\\n    --engine_dir ./tmp/grok-1/trt_engines/bf16/8-gpus \\\n    --max_output_len 50 --top_p 1 --top_k 8 --temperature 0.3 \\\n    --vocab_file  ./tmp/grok-1/tokenizer.model\n```\n\n----------------------------------------\n\nTITLE: Building Engine for Quantized Starcoder2 Model\nDESCRIPTION: Command to build TensorRT-LLM engines from a Starcoder2 checkpoint quantized with ModelOpt. Uses the quantized checkpoint to create optimized engines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir starcoder2/trt_ckpt/int8-sq/ \\\n             --output_dir starcoder2/trt_engine/int8-sq/\n```\n\n----------------------------------------\n\nTITLE: Enabling GEMM Plugin in TensorRT-LLM\nDESCRIPTION: This code demonstrates how to enable the GEMM plugin which utilizes NVIDIA cuBLASLt and custom kernels for matrix multiplication operations. It's recommended for FP16 and BF16 precision for better performance and reduced GPU memory usage.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbuild_config.plugin_config.gemm_plugin = 'auto'\n```\n\n----------------------------------------\n\nTITLE: Loading LLM from Hugging Face Hub in Python\nDESCRIPTION: Demonstrates how to initialize an LLM instance using a model from the Hugging Face Hub. This snippet shows the simplest way to load a pre-trained model for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/llm-api/index.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with Target ISL/OSL Optimization\nDESCRIPTION: Experimental command for building an FP8 quantized Llama 3.1 engine optimized for specific input sequence length (ISL) and output sequence length (OSL) targets of 128:128. This allows providing hints to the tuning heuristic for engine optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ntrtllm-bench --model meta-llama/Llama-3.1-8B build --quantization FP8 --max_seq_len 4096 --target_isl 128 --target_osl 128\n```\n\n----------------------------------------\n\nTITLE: Building DBRX with BFloat16 and 8-way Tensor Parallelism\nDESCRIPTION: Converts DBRX model weights to TensorRT-LLM checkpoint format using BFloat16 precision and 8-way tensor parallelism, then builds TensorRT engines with appropriate plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# 8-way tensor parallelism, dtype bfloat16\npython convert_checkpoint.py --model_dir dbrx-base \\\n        --dtype bfloat16 \\\n        --tp_size 8 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_ckpt/bf16/tp8\n\ntrtllm-build --checkpoint_dir dbrx/trt_ckpt/bf16/tp8 \\\n        --gpt_attention_plugin bfloat16 \\\n        --gemm_plugin bfloat16 \\\n        --moe_plugin bfloat16 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_engines/bf16/tp8\n```\n\n----------------------------------------\n\nTITLE: WorldConfig Parameters Overview in C++\nDESCRIPTION: Defines the parameters used for distributed execution configuration, including tensor and pipeline parallelism settings, rank identification, and GPU deployment options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-runtime.md#2025-04-07_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntensorParallelism - Number of ranks for Tensor Parallelism\npipelineParallelism - Number of ranks for Pipeline Parallelism\nrank - Unique rank identifier\ngpusPerNode - GPUs per node count\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines with Weight-Only Quantization for GPT2\nDESCRIPTION: Commands to build TensorRT engines from previously converted INT8 or INT4 weight-only quantized checkpoints. These commands finalize the model optimization for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n# Int8 weight-only quantization\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/int8-wo/1-gpu \\\n        --output_dir gpt2/trt_engines/int8-wo/1-gpu\n\n# Int4 weight-only quantization\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/int4-wo/1-gpu \\\n        --output_dir gpt2/trt_engines/int4-wo/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests with pytest\nDESCRIPTION: Commands for setting up the environment, preparing model files, and running integration tests using pytest in the TensorRT-LLM project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport LLM_MODELS_ROOT=/path-to-models\n\n# in root dir\npip install -r requirements-dev.txt\ncd tests/integration/defs\n\n# example 1: run a case\npytest \"accuracy/test_cli_flow.py::TestGpt2CnnDailymail::test_auto_dtype\"\n\n# example 2: run a test list\npytest --rootdir . --test-list=<a txt file contains on test case per line>\n\n# example 3: list all the cases.\npytest --co -q\n\n# example 4: run all the tests which contains this sub string\npytest -k test_llm_gpt2_medium_bad_words_1gpu\n\n# example 5: run all tests which match this regexp\npytest -R \".*test_llm_gpt2_medium_bad_words_1gpu.*non.*py.*\"\n\n# example 6: list all the cases contains a sub string\npytest -k llmapi --co -q\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Baichuan Model\nDESCRIPTION: Command to build a TensorRT engine for a Baichuan model using trtllm-build. This example enables several TensorRT-LLM plugins to increase runtime performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tmp/baichuan_v1_13b/trt_ckpts/fp16/1-gpu/ \\\n             --output_dir ./tmp/baichuan_v1_13b/trt_engines/fp16/1-gpu/ \\\n             --gemm_plugin float16 \\\n             --max_batch_size=32 \\\n             --max_input_len=1024 \\\n             --max_seq_len=1536\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace LoRA Models to TensorRT-LLM Format\nDESCRIPTION: These commands convert HuggingFace LoRA models to the NumPy tensor format required by TensorRT-LLM's C++ runtime.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/lora.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 tensorrt_llm/examples/hf_lora_convert.py -i Japanese-Alpaca-LoRA-7b-v0 -o Japanese-Alpaca-LoRA-7b-v0-weights --storage-type float16\npython3 tensorrt_llm/examples/hf_lora_convert.py -i luotuo-lora-7b-0.1 -o luotuo-lora-7b-0.1-weights --storage-type float16\n```\n\n----------------------------------------\n\nTITLE: Applying FP8 Post-Training Quantization\nDESCRIPTION: Script execution for FP8 quantization of the model weights, including calibration for static quantization. Requires significant GPU resources for processing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/arctic/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir tmp/hf_checkpoints/${HF_MODEL} \\\n                                   --dtype ${PREC_RAW} \\\n                                   --qformat ${PREC_QUANT} \\\n                                   --kv_cache_dtype ${PREC_QUANT} \\\n                                   --output_dir tmp/tllm_checkpoints/${ENGINE} \\\n                                   --batch_size 1 \\\n                                   --calib_size 128 \\\n                                   --tp_size ${TP} |& tee tmp/trt_engines/${ENGINE}_quantize.log\n```\n\n----------------------------------------\n\nTITLE: Converting and Building GPT2 Model with INT8 KV Cache Optimization\nDESCRIPTION: Commands to convert a GPT2 model with INT8 KV cache quantization and build TensorRT engines. This technique reduces memory usage by quantizing the key-value cache to INT8 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n# Int8 KV cache\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --int8_kv_cache \\\n        --output_dir gpt2/trt_ckpt/int8kv/1-gpu\n\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/int8kv/1-gpu \\\n        --output_dir gpt2/trt_engines/int8kv/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Converting T5 model weights for TensorRT-LLM with tensor parallelism\nDESCRIPTION: Script to convert T5 model weights from HuggingFace format to TensorRT-LLM format with 4-way tensor parallelism. The converted weights are configured for BF16 precision and beam search support.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Example: build t5-small using 4-way tensor parallelism on a node with 8 GPUs (but only use 4 of them, for demonstration purpose), BF16, enabling beam search up to width=1.\nexport MODEL_NAME=\"t5-small\" # or \"flan-t5-small\"\nexport MODEL_TYPE=\"t5\"\nexport INFERENCE_PRECISION=\"bfloat16\"\nexport TP_SIZE=4\nexport PP_SIZE=1\nexport WORLD_SIZE=4\nexport MAX_BEAM_WIDTH=1\npython convert_checkpoint.py --model_type ${MODEL_TYPE} \\\n                --model_dir tmp/hf_models/${MODEL_NAME} \\\n                --output_dir tmp/trt_models/${MODEL_NAME}/${INFERENCE_PRECISION} \\\n                --tp_size ${TP_SIZE} \\\n                --pp_size ${PP_SIZE} \\\n                --dtype ${INFERENCE_PRECISION}\n```\n\n----------------------------------------\n\nTITLE: Building SmoothQuant Model for BLOOM\nDESCRIPTION: Commands to prepare and build a SmoothQuant quantized model with per-token and per-channel optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir bloom/560M/  --output_dir tllm_checkpoint_1gpu --smoothquant 0.5 --per_token --per_channel\n\ntrtllm-build  --checkpoint_dir tllm_checkpoint_1gpu  --output_dir ./engine_outputs\n```\n\n----------------------------------------\n\nTITLE: Quantizing Qwen Model to INT4-AWQ with Modelopt\nDESCRIPTION: Command to quantize a Qwen-7B model to INT4-AWQ format using NVIDIA Modelopt toolkit. It applies activation-aware weight quantization with a block size of 128.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py --model_dir ./tmp/Qwen/7B/ \\\n                                       --dtype float16 \\\n                                       --qformat int4_awq \\\n                                       --awq_block_size 128 \\\n                                       --output_dir ./quantized_int4-awq \\\n                                       --calib_size 32\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-72B with 8-GPU Parallelism\nDESCRIPTION: Command to build a TensorRT engine from the converted checkpoint for Qwen-72B. It uses the trtllm-build tool to create optimized engines with float16 precision gemm plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_8gpu_tp8 \\\n            --output_dir ./tmp/qwen/72B/trt_engines/fp16/8-gpu/ \\\n            --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with Self-Defined Tuning Values\nDESCRIPTION: Command example showing how to build an FP8 quantized Llama 3.1 engine with custom max_batch_size and max_num_tokens parameters. These parameters control the maximum number of requests and tokens that can be scheduled in each iteration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ntrtllm-bench --model meta-llama/Llama-3.1-8B build --quantization FP8 --max_seq_len 4096 --max_batch_size 1024 --max_num_tokens 2048\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Checkpoint Config JSON Example\nDESCRIPTION: This JSON snippet shows an example configuration file for a TensorRT-LLM checkpoint, specifying model architecture, dimensions, and parallelism settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/checkpoint.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"architecture\": \"OPTForCausalLM\",\n    \"dtype\": \"float16\",\n    \"logits_dtype\": \"float32\",\n    \"num_hidden_layers\": 12,\n    \"num_attention_heads\": 12,\n    \"hidden_size\": 768,\n    \"vocab_size\": 50272,\n    \"position_embedding_type\": \"learned_absolute\",\n    \"max_position_embeddings\": 2048,\n    \"hidden_act\": \"relu\",\n    \"mapping\": {\n        \"world_size\": 2,\n        \"tp_size\": 2\n    },\n    \"use_parallel_embedding\": false,\n    \"embedding_sharding_dim\": 0,\n    \"do_layer_norm_before\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Running Inference on Single GPU\nDESCRIPTION: Command to run inference with a built TensorRT-LLM engine on a single GPU. Uses the run.py script to execute the model with specified engine directory, tokenizer, and maximum output length.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --engine_dir gpt2/trt_engines/fp16/1-gpu \\\n        --tokenizer_dir gpt2 \\\n        --max_output_len 8\n```\n\n----------------------------------------\n\nTITLE: Building Mixtral 8x7B with Tensor Parallelism\nDESCRIPTION: Commands to convert the Mixtral 8x7B checkpoint and build TensorRT engines with tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py --model_dir ./Mixtral-8x7B-v0.1 \\\n                             --output_dir ./tllm_checkpoint_mixtral_2gpu \\\n                             --dtype float16 \\\n                             --tp_size 2 \\\n                             --moe_tp_size 2\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_2gpu \\\n                 --output_dir ./trt_engines/mixtral/tp2 \\\n                 --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for RecurrentGemma-2b (FP16)\nDESCRIPTION: This command builds a TensorRT engine from the RecurrentGemma-2b FP16 checkpoint using trtllm-build with auto GEMM plugin selection.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b\nENGINE_2B_PATH=./recurrentgemma_model/recurrentgemma-2b/trt_engines/fp16/1-gpu/\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_2B_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_2B_PATH}\n```\n\n----------------------------------------\n\nTITLE: Running GPT-NeoX 20B with 2-way Tensor Parallelism using MPI in TensorRT-LLM\nDESCRIPTION: This command uses MPI to distribute a GPT-NeoX 20B model across 2 GPUs. It runs the summarize.py script with a tensor parallel configuration, using int4 GPTQ quantized engines while testing the TensorRT-LLM implementation against the original Hugging Face model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 2 --oversubscribe --allow-run-as-root \\\n    python3 ../../../summarize.py --engine_dir ./gptneox/20B/trt_engines/int4_gptq/2-gpu/ \\\n                            --test_trt_llm \\\n                            --hf_model_dir gptneox_model \\\n                            --data_type fp16\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT engine for Decoder model\nDESCRIPTION: Command to build a TensorRT engine for the Decoder component of an Encoder-Decoder model. Includes specific decoder settings like max_input_len=1 for generation from decoder_start_token_id, and encoder-decoder specific configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# For decoder, refer to the above content and set --max_input_len correctly\ntrtllm-build --checkpoint_dir tmp/trt_models/${MODEL_NAME}/${INFERENCE_PRECISION}/decoder \\\n                --output_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION}/decoder \\\n                --moe_plugin disable \\\n                --max_beam_width ${MAX_BEAM_WIDTH} \\\n                --max_batch_size 8 \\\n                --max_input_len 1 \\\n                --max_seq_len 201 \\\n                --max_encoder_input_len 1024 \\\n                --gemm_plugin ${INFERENCE_PRECISION} \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding enable \\\n                --context_fmha disable\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Benchmark Using MPI Across Two Nodes\nDESCRIPTION: Command for running a TensorRT-LLM benchmark for DeepSeek-V3 model across two nodes using MPI. Configures tensor parallelism, expert parallelism, and various benchmark parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nmpirun \\\n--output-filename bench_log_2node_ep8_tp16_attndp_on_sl1000 \\\n-H <HOST1>:8,<HOST2>:8 \\\n-mca plm_rsh_args \"-p 2233\" \\\n--allow-run-as-root -n 16 \\\ntrtllm-llmapi-launch trtllm-bench --model deepseek-ai/DeepSeek-V3 --model_path /models/DeepSeek-V3 throughput --backend pytorch --max_batch_size 161 --max_num_tokens 1160 --dataset /workspace/tensorrt_llm/dataset_isl1000.txt --tp 16 --ep 8 --kv_cache_free_gpu_mem_fraction 0.95 --extra_llm_api_options /workspace/tensorrt_llm/extra-llm-api-config.yml --concurrency 4096 --streaming\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines for Draft and Target Models\nDESCRIPTION: Commands to build TensorRT engines for Llama models in both FP16 and FP8 modes, setting up necessary parameters for the draft-target speculative decoding approach including paged KV cache and attention optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MAX_DRAFT_LENGTH=10\nexport COMMON_COMMAND=\"--max_batch_size=1 --max_input_len=2048 --max_seq_len=3072 --gpt_attention_plugin=float16 --gemm_plugin=float16 --remove_input_padding=enable --paged_kv_cache=enable --context_fmha=enable --use_paged_context_fmha=enable --gather_generation_logits\"\nexport DRAFT_COMMAND_FP16=\"$COMMON_COMMAND\"\nexport TARGET_COMMAND_FP16=\"$DRAFT_COMMAND_FP16 --max_draft_len=$MAX_DRAFT_LENGTH --speculative_decoding_mode draft_tokens_external\"\nexport DRAFT_COMMAND_FP8=\"$COMMON_COMMAND --use_fp8_context_fmha=enable\"\nexport TARGET_COMMAND_FP8=\"$DRAFT_COMMAND_FP8 --max_draft_len=$MAX_DRAFT_LENGTH --speculative_decoding_mode draft_tokens_external\"\n\n# Build checkpoints and engines in tensorrt_llm/examples/llama/\n# FP16 mode\nexport DRAFT_NAME=llama-7b-fp16-tp1\nexport TARGET_NAME=llama-30b-fp16-tp1\npython3 convert_checkpoint.py --model_dir=$DRAFT_MODEL_PATH --output_dir=ckpt/$DRAFT_NAME --tp_size=1\npython3 convert_checkpoint.py --model_dir=$TARGET_MODEL_PATH --output_dir=ckpt/$TARGET_NAME --tp_size=1\ntrtllm-build --checkpoint_dir=ckpt/$DRAFT_NAME --output_dir=engine/draft/$DRAFT_NAME $DRAFT_COMMAND_FP16\ntrtllm-build --checkpoint_dir=ckpt/$TARGET_NAME --output_dir=engine/target/$TARGET_NAME $TARGET_COMMAND_FP16\nexport DRAFT_ENGINE_PATH=$(pwd)/engine/draft/$DRAFT_NAME\nexport TARGET_ENGINE_PATH=$(pwd)/engine/target/$TARGET_NAME\n\n# FP8 mode\nexport DRAFT_NAME=llama-7b-fp8-tp1\nexport TARGET_NAME=llama-30b-fp8-tp1\npython3 ../quantization/quantize.py --model_dir=$DRAFT_MODEL_PATH --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir=ckpt/$DRAFT_NAME --tp_size=1\npython3 ../quantization/quantize.py --model_dir=$TARGET_MODEL_PATH --dtype float16 --qformat fp8 --kv_cache_dtype fp8 --output_dir=ckpt/$TARGET_NAME --tp_size=1\ntrtllm-build --checkpoint_dir=ckpt/$DRAFT_NAME --output_dir=engine/draft/$DRAFT_NAME $DRAFT_COMMAND_FP8\ntrtllm-build --checkpoint_dir=ckpt/$TARGET_NAME --output_dir=engine/target/$TARGET_NAME $TARGET_COMMAND_FP8\nexport DRAFT_ENGINE_PATH=$(pwd)/engine/draft/$DRAFT_NAME\nexport TARGET_ENGINE_PATH=$(pwd)/engine/target/$TARGET_NAME\n```\n\n----------------------------------------\n\nTITLE: Basic INT8 KV Cache Quantization\nDESCRIPTION: Quantizes the Baichuan model with INT8 KV cache using NVIDIA Modelopt toolkit. Uses float16 dtype and calibrates on 512 samples.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir baichuan-inc/Baichuan-13B-Chat \\\n                                   --dtype float16 \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ./trt_ckpt/baichuan_int8kv_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Configuring Draft-Target-Model in TensorRT-LLM\nDESCRIPTION: Instructions for configuring and executing the Draft-Target-Model approach in TensorRT-LLM. It includes setting up Draft and Target models, configuring maxNewTokens, and enabling KV cache reuse for performance optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**NOTE:** To enhance performance, especially due to the repetitive querying of Draft and Target models with requests that share a common prefix,\nit is advisable to enable KV cache reuse for both models.\nThis can be achieved by adding the `--use_paged_context_fmha=enable` flag to the `trtllm-build` command\nand setting `enableBlockReuse=true` in the `KVCacheConfig`.\n```\n\n----------------------------------------\n\nTITLE: Converting HF Weights to TensorRT-LLM Format\nDESCRIPTION: Python commands to convert HuggingFace weights to TensorRT-LLM format with different configurations\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mamba/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./mamba_model/mamba-2.8b/ \\\n                             --dtype bfloat16 \\\n                             --output_dir ./mamba_model/mamba-2.8b/trt_ckpt/bf16/1-gpu/\n\npython convert_checkpoint.py --model_dir ./mamba_model/mamba-codestral-7B-v0.1/ \\\n                             --dtype float16 \\\n                             --world_size 2 \\\n                             --output_dir ./mamba_model/mamba-codestral-7B-v0.1/trt_ckpt/fp16/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: W4A8-AWQ Quantization for Falcon-180B on Hopper GPUs\nDESCRIPTION: Commands for applying W4A8-AWQ quantization (W4A16 weights with FP8 GEMM acceleration) to Falcon-180B specifically for Hopper GPUs, which converts activations to FP8 for GEMM calculations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# Quantize HF Falcon 180B checkpoint into W4A8-AWQ and export trtllm checkpoint\npython ../../../quantization/quantize.py --model_dir ./falcon/180b \\\n                --dtype float16 \\\n                --qformat w4a8_awq \\\n                --output_dir ./falcon/180b/trt_ckpt/w4a8_awq/tp2 \\\n                --tp_size 2\n\n# Build trtllm engines from the trtllm checkpoint\ntrtllm-build --checkpoint_dir ./falcon/180b/trt_ckpt/w4a8_awq/tp2 \\\n                --gemm_plugin float16 \\\n                --output_dir ./falcon/180b/trt_engines/w4a8_awq/tp2 \\\n                --workers 2\n\n# Run the summarization task\nmpirun -n 2 --allow-run-as-root --oversubscribe \\\n    python ../../../summarize.py --test_trt_llm \\\n                --hf_model_dir ./falcon/180b \\\n                --engine_dir ./falcon/180b/trt_engines/w4a8_awq/tp2\n```\n\n----------------------------------------\n\nTITLE: Running Gemma 2 BFloat16 Inference for Torch Checkpoint\nDESCRIPTION: Commands to run Gemma 2 model inference in BFloat16 precision using Torch checkpoint. Supports 9B and 27B variants with a maximum sequence length of 4096.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nvariant=9b # 27b\ngit clone git@hf.co:google/gemma-2-$variant-it\n\nCKPT_PATH=gemma-2-$variant-it/\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_$variant_it_tensorrt_llm/bf16/tp1/\nENGINE_PATH=/tmp/gemma2/$variant/bf16/1-gpu/\nVOCAB_FILE_PATH=gemma-2-$variant-it/tokenizer.model\n\npython3 ./examples/gemma/convert_checkpoint.py \\\n    --ckpt-type hf \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype bfloat16 \\\n    --world-size 1 \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine\nDESCRIPTION: Command to build TensorRT-LLM engine from converted checkpoint with specified batch size and sequence length parameters\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nENGINE_PATH=/tmp/gemma/2B/bf16/1-gpu/\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n```\n\n----------------------------------------\n\nTITLE: Enabling KV Cache Reuse for P-tuning in Python\nDESCRIPTION: Example of how to provide extra IDs for p-tuning to enable correct KV cache reuse with different prompt embedding tables.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/kv-cache-reuse.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Request 1 uses prompt embedding table 1\ninput_ids = [100, 101, 102, 103, 1, 2, 3, 4]\nextra_ids = [1,   1,   1,   1,   0, 0, 0, 0]\n\n# Request 2 uses prompt embedding table 2\ninput_ids = [100, 101, 102, 103, 1, 2, 3, 4]\nextra_ids = [2,   2,   2,   2,   0, 0, 0, 0]\n\n# Request 3 uses prompt embedding table 1 and different text tokens\ninput_ids = [100, 101, 102, 103, 5, 6, 7, 8]\nextra_ids = [1,   1,   1,   1,   0, 0, 0, 0]\n```\n\n----------------------------------------\n\nTITLE: Configuring Capacity Scheduler Policy in TensorRT-LLM\nDESCRIPTION: This snippet demonstrates how to set the capacity scheduler policy to MAX_UTILIZATION in TensorRT-LLM using the LLM-API. It includes importing necessary modules, setting up prompts and sampling parameters, and configuring the scheduler.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-runtime-flags.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm import LLM, SamplingParams\nfrom tensorrt_llm.bindings.executor import SchedulerConfig, CapacitySchedulerPolicy\n\n\ndef main():\n    prompts = [\n        \"Hello, I am\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n    scheduler_config = SchedulerConfig(\n        capacity_scheduler_policy=CapacitySchedulerPolicy.MAX_UTILIZATION\n    )\n\n    llm  =  LLM(\n    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n    tensor_parallel_size=4,\n    scheduler_config=scheduler_config\n    )\n\n    outputs = llm.generate(prompts, sampling_params)\n\n    # Print the outputs.\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Building Mixtral 8x7B with Expert and Tensor Parallelism\nDESCRIPTION: Commands to convert the Mixtral 8x7B checkpoint and build TensorRT engines with both expert and tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py --model_dir ./Mixtral-8x7B-v0.1 \\\n                             --output_dir ./tllm_checkpoint_mixtral_4gpu \\\n                             --dtype float16 \\\n                             --tp_size 4 \\\n                             --moe_tp_size 2 \\\n                             --moe_ep_size 2\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_4gpu \\\n                 --output_dir ./trt_engines/mixtral/tp2ep2 \\\n                 --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for DeepSeek-V2 Model\nDESCRIPTION: Command to build a TensorRT engine for DeepSeek-V2 model with specific parameters including batch size, sequence length, attention and GEMM plugins. This configures the model for optimized inference on 8 GPUs with BF16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./trtllm_checkpoint_deepseek_v2_8gpu_bf16 \\\n            --output_dir ./trtllm_engines/deepseek_v2/bf16/tp8-sel4096-isl2048-bs4 \\\n            --gpt_attention_plugin bfloat16 \\\n            --gemm_plugin bfloat16 \\\n            --max_batch_size 4 \\\n            --max_seq_len 4096 \\\n            --max_input_len 2048 \\\n            --use_paged_context_fmha enable\n```\n\n----------------------------------------\n\nTITLE: Implementing Weight-Only Quantization for GPT2 Models (INT8 and INT4)\nDESCRIPTION: Commands to convert GPT2 models using weight-only quantization in INT8 or INT4 precision and build corresponding TensorRT engines. This technique reduces model size by quantizing only the weights while keeping activations in FP16.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n# Int8 weight-only quantization\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --use_weight_only \\\n        --weight_only_precision int8 \\\n        --output_dir gpt2/trt_ckpt/int8-wo/1-gpu\n\n# Int4 weight-only quantization\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --use_weight_only \\\n        --weight_only_precision int4 \\\n        --output_dir gpt2/trt_ckpt/int4-wo/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Tensor and Pipeline Parallel InternLM2 20B\nDESCRIPTION: Executes inference using the InternLM2 20B model with FP16 precision across 4 GPUs using MPI for tensor and pipeline parallelism coordination.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 4 --allow-run-as-root \\\n    python ../run.py --max_output_len=120 \\\n                     --input_text 'Tell me about yourself.' \\\n                     --tokenizer_dir ./internlm2-chat-7b/ \\\n                     --engine_dir=./internlm2-chat-7b/trt_engines/bf16/4-gpu/\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting HuggingFace Falcon with Accelerate Package\nDESCRIPTION: Command to fix a common error when running HuggingFace Falcon models with the accelerate package by pinning the typing-extensions package version.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install typing-extensions==4.5.0\n```\n\n----------------------------------------\n\nTITLE: Enabling FP8 KV-Cache Quantization\nDESCRIPTION: This code snippet shows how to enable FP8 quantization for the KV-Cache in TensorRT-LLM. By setting the kv_cache_quant_algo parameter to FP8, both the model weights and KV-Cache will use FP8 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/fp8-quantization.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquant_config = QuantConfig(quant_algo=QuantAlgo.FP8,\n                           kv_cache_quant_algo=QuantAlgo.FP8)\n```\n\n----------------------------------------\n\nTITLE: Customizing Tokenizer in TensorRT-LLM\nDESCRIPTION: Overrides the default AutoTokenizer with a custom tokenizer implementation. This allows using a different or optimized tokenizer while maintaining the LLM workflow.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(<llama_model_path>, tokenizer=<my_faster_one>)\n```\n\n----------------------------------------\n\nTITLE: Quantizing Baichuan V2 13B to FP8 with NVIDIA Modelopt\nDESCRIPTION: Python command to quantize a Hugging Face Baichuan V2 13B model to FP8 precision using NVIDIA Modelopt toolkit for post-training quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir /code/model/Baichuan2-13B-Chat/ \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --output_dir ./quantized_fp8 \\\n                                   --calib_size 256\n```\n\n----------------------------------------\n\nTITLE: Running Basic EAGLE-2 Decoding with Single GPU\nDESCRIPTION: Demonstrates how to run EAGLE-2 decoding using a single GPU with dynamic tree implementation. Includes configuration for maximum output length and dynamic tree parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 1 --allow-run-as-root --oversubscribe \\\n  python ../run.py --engine_dir ./tmp/eagle/7B/trt_engines/fp16/1-gpu/ \\\n                 --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                 --max_output_len=100 \\\n                 --eagle_use_dynamic_tree \\\n                 --eagle_dynamic_tree_max_top_k 10 \\\n                 --input_text \"Once upon\"\n```\n\n----------------------------------------\n\nTITLE: Running SmoothQuant Inference for Gemma 2B-IT Model Using JAX Checkpoint\nDESCRIPTION: This script demonstrates the SmoothQuant quantization technique applied to Gemma 2B-IT using a JAX checkpoint. SmoothQuant is used for INT8 inference with a smoothing factor of 0.5. The process includes cloning the model, converting the checkpoint, building the engine, and evaluating with summarize.py.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-2b-it-flax\nCKPT_PATH=gemma-2b-it-flax/2b-it/\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_2b_it_tensorrt_llm/sq/tp1\nENGINE_PATH=/tmp/gemma/2B/int8_sq/1-gpu/\nVOCAB_FILE_PATH=gemma-2b-it-flax/tokenizer.model\n\npython3 ./convert_checkpoint.py \\\n    --ckpt-type jax \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype float16 \\\n    --use_smooth_quant_plugin 0.5 \\\n    --tokenizer_dir ${VOCAB_FILE_PATH} \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n\n[02/08/2024-04:42:06] [TRT-LLM] [I] TensorRT-LLM (total latency: 3.460859775543213 sec)\n[02/08/2024-04:42:06] [TRT-LLM] [I] TensorRT-LLM (total output tokens: 1786)\n[02/08/2024-04:42:06] [TRT-LLM] [I] TensorRT-LLM (tokens per second: 516.0567361385428)\n[02/08/2024-04:42:06] [TRT-LLM] [I] TensorRT-LLM beam 0 result\n[02/08/2024-04:42:06] [TRT-LLM] [I]   rouge1 : 22.534044843245525\n[02/08/2024-04:42:06] [TRT-LLM] [I]   rouge2 : 5.940093176022924\n[02/08/2024-04:42:06] [TRT-LLM] [I]   rougeL : 16.258991712579736\n[02/08/2024-04:42:06] [TRT-LLM] [I]   rougeLsum : 19.60977626046262\n```\n\n----------------------------------------\n\nTITLE: Loading LLM from Local TensorRT-LLM Engine in Python\nDESCRIPTION: Shows how to initialize an LLM instance using a pre-built TensorRT-LLM engine stored locally. This method is typically faster for inference as the model is already optimized.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/llm-api/index.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(model=<path_to_trt_engine>)\n```\n\n----------------------------------------\n\nTITLE: Converting AutoAWQ Qwen Model to TensorRT-LLM\nDESCRIPTION: Command to convert a Qwen model quantized with AutoAWQ into a TensorRT-LLM compatible checkpoint. It processes the HuggingFace checkpoint for optimal TensorRT-LLM execution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen2-7B-Instruct-AWQ \\\n                                 --output_dir ./quantized_int4-awq\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GPT-J Summarization\nDESCRIPTION: Installs required Python packages for running the GPT-J summarization model\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Building InternLM2 7B Engine with 2-way Tensor Parallelism\nDESCRIPTION: Builds the TensorRT engines for InternLM2 7B model using 2-way tensor parallelism for distributed inference across multiple GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./internlm2-chat-7b/trt_engines/fp16/2-gpu/ \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Converting GPT-NeoX Weights to TensorRT-LLM Format (2-way Tensor Parallel)\nDESCRIPTION: Converts the HuggingFace weights to TensorRT-LLM format for a 2-GPU setup using tensor parallelism and float16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./gptneox_model \\\n                              --dtype float16 \\\n                              --tp_size 2 \\\n                              --workers 2 \\\n                              --output_dir ./gptneox/20B/trt_ckpt/fp16/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: Defining QuantMode Flags for TensorRT-LLM\nDESCRIPTION: Enumeration of quantization mode flags used to control different aspects of model quantization in TensorRT-LLM. These flags define weight precision (INT4/INT8), activation quantization, scaling factor modes (per channel/token/group), and special K/V cache handling options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/precision.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass QuantMode:\n    INT4_WEIGHTS   # 4-bit weight quantization (W4A*)\n    INT8_WEIGHTS   # 8-bit weight quantization (W8A*)\n    ACTIVATIONS    # 8-bit activation quantization (W*A8)\n    PER_CHANNEL    # Per-channel scaling factors\n    PER_TOKEN      # Per-token scaling factors\n    PER_GROUP      # Per-group scaling factors\n    INT8_KV_CACHE  # 8-bit integer K/V cache storage\n    FP8_KV_CACHE   # 8-bit floating-point K/V cache storage\n    FP8_QDQ        # Automatic Q/DQ node fusion in TensorRT\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-7B-Chat (2-way TP and 2-way PP)\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM checkpoints and build a 4-GPU engine with 2-way tensor parallelism and 2-way pipeline parallelism for Qwen-7B-Chat.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                            --output_dir ./tllm_checkpoint_4gpu_tp2_pp2 \\\n                            --dtype float16 \\\n                            --tp_size 2 \\\n                            --pp_size 2\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_4gpu_tp2_pp2 \\\n            --output_dir ./tmp/qwen/7B/trt_engines/fp16/4-gpu/ \\\n            --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Building Mixtral 8x7B with Expert Parallelism\nDESCRIPTION: Commands to convert the Mixtral 8x7B checkpoint and build TensorRT engines with expert parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py --model_dir ./Mixtral-8x7B-v0.1 \\\n                             --output_dir ./tllm_checkpoint_mixtral_2gpu \\\n                             --dtype float16 \\\n                             --tp_size 2 \\\n                             --moe_ep_size 2\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_2gpu \\\n                 --output_dir ./trt_engines/mixtral/ep2 \\\n                 --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Pruning TensorRT-LLM Checkpoint using trtllm-prune\nDESCRIPTION: This command prunes the TensorRT-LLM checkpoint located at ${CHECKPOINT_DIR} and stores the pruned version in a new directory with '.pruned' appended to the original directory name. The pruned checkpoint can be used for building a more efficient engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/sample_weight_stripping/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Prunes the TRT-LLM checkpoint at ${CHECKPOINT_DIR}, and stores it in the directory ${CHECKPOINT_DIR}.pruned\ntrtllm-prune --checkpoint_dir ${CHECKPOINT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace Weights to TensorRT-LLM Format\nDESCRIPTION: Uses the convert_checkpoint.py script to convert HuggingFace weights to TensorRT-LLM checkpoints for different GPU configurations and precision settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --output_dir gpt2/trt_ckpt/fp16/1-gpu\n\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --tp_size 2 \\\n        --output_dir gpt2/trt_ckpt/fp16/2-gpu\n\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --tp_size 2 \\\n        --pp_size 2 \\\n        --output_dir gpt2/trt_ckpt/fp16/4-gpu\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with INT8 KV Cache Support\nDESCRIPTION: Command to build a TensorRT engine with INT8 KV cache optimization. It uses the processed checkpoint to create an optimized engine with reduced memory footprint.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_sq \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Slurm Script for Multi-Node Inference\nDESCRIPTION: Example Slurm batch script for running TensorRT-LLM engines across multiple nodes. Configures a 2-node job with 8 tasks per node and includes container settings for deployment.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n#SBATCH -o logs/tensorrt_llm.out\n#SBATCH -e logs/tensorrt_llm.error\n#SBATCH -J <REPLACE WITH YOUR JOB's NAME>\n#SBATCH -A <REPLACE WITH YOUR ACCOUNT's NAME>\n#SBATCH -p <REPLACE WITH YOUR PARTITION's NAME>\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=00:30:00\n\nsudo nvidia-smi -lgc 1410,1410\n\nsrun --mpi=pmix \\\n    --container-image <image> \\\n    --container-mounts <path>:<path> \\\n    --container-workdir <path> \\\n    --output logs/tensorrt_llm_%t.out \\\n    --error logs/tensorrt_llm_%t.error \\\n        python3 -u ../run.py --engine_dir <engine_dir> --max_output_len 8\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for RecurrentGemma-2b-it-flax (BF16)\nDESCRIPTION: This command builds a TensorRT engine from the RecurrentGemma-2b-it-flax BF16 checkpoint using trtllm-build with auto GEMM plugin selection.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it-flax\nENGINE_2B_IT_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-it-flax/trt_engines/bf16/1-gpu/\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_2B_IT_FLAX_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_2B_IT_FLAX_PATH}\n```\n\n----------------------------------------\n\nTITLE: Speculative Decoding Test Command\nDESCRIPTION: Command to launch inference requests with draft model configuration and parameters\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 tools/inflight_batcher_llm/speculative_decoding_test.py \\\n    --max-input-len 2048 \\\n    --dataset=input_data.json \\\n    --url-target=localhost:8001 \\\n    --url-draft=localhost:8001 \\\n    --draft-tensorrt-llm-model-name=\"${TENSORRT_LLM_DRAFT_MODEL_NAME}\" \\\n    --target-tensorrt-llm-model-name=\"${TENSORRT_LLM_MODEL_NAME}\" \\\n    --bls-speculative-tensorrt-llm-model-name=\"tensorrt_llm_bls\" \\\n    --execute-bls-speculative-decoding \\\n    --disable-output-comparison \\\n    --num-draft-tokens=4 \\\n    --verbose\n```\n\n----------------------------------------\n\nTITLE: Quantizing GPT-J with AWQ INT4 and Tensor Parallelism\nDESCRIPTION: Command to enable AWQ INT4 group-wise weight-only quantization with tensor parallelism for GPT-J.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Enable AWQ int4 group-wise weight-only quantization with tp2.\npython ../../../quantization/quantize.py --model_dir ./gpt-j-6b \\\n                                   --dtype float16 \\\n                                   --qformat int4_awq \\\n                                   --tp_size 2 \\\n                                   --output_dir ./trt_ckpt/gptj_int4_awq_tp2 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Converting Model Checkpoint for ReDrafter Decoding\nDESCRIPTION: Commands to convert the Vicuna model and its drafter for ReDrafter speculative decoding into TensorRT-LLM checkpoint format. Hyperparameters like number of beams and draft length are specified during conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/redrafter/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./vicuna-7b-v1.3 \\\n                             --drafter_model_dir ./vicuna-7b-drafter \\\n                             --output_dir ./tllm_checkkpoint_1gpu_redrafter \\\n                             --dtype float16 \\\n                             --redrafter_num_beams 4 \\\n                             --redrafter_draft_len_per_beam 5\n\n\ntrtllm-build --checkpoint_dir ./tllm_checkkpoint_1gpu_redrafter \\\n             --output_dir ./tmp/redrafter/7B/trt_engines/fp16/1-gpu/ \\\n             --gemm_plugin float16 \\\n             --speculative_decoding_mode explicit_draft_tokens \\\n             --max_batch_size 4\n```\n\n----------------------------------------\n\nTITLE: Downloading Falcon Models from Hugging Face\nDESCRIPTION: Commands to clone various Falcon model repositories from Hugging Face, including falcon-rw-1b, falcon-7b-instruct, falcon-40b-instruct, falcon-180b, and falcon-11b (Falcon 2).\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# falcon-rw-1b\ngit clone https://huggingface.co/tiiuae/falcon-rw-1b falcon/rw-1b\n\n# falcon-7b-instruct\ngit clone https://huggingface.co/tiiuae/falcon-7b-instruct falcon/7b-instruct\n\n# falcon-40b-instruct\ngit clone https://huggingface.co/tiiuae/falcon-40b-instruct falcon/40b-instruct\n\n# falcon-180b\ngit clone https://huggingface.co/tiiuae/falcon-180B falcon/180b\n\n# falcon-11b (Falcon 2)\ngit clone https://huggingface.co/tiiuae/falcon-11B falcon/11b\n```\n\n----------------------------------------\n\nTITLE: Converting RecurrentGemma-2b-it to FP8 Format with FP8 KV Cache\nDESCRIPTION: This command quantizes the recurrentgemma-2b-it model to FP8 precision with FP8 KV cache using the quantization script.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it FP8 with FP8 kv cache\nCKPT_2B_IT_PATH=./recurrentgemma_model/recurrentgemma-2b-it\nUNIFIED_CKPT_2B_IT_FP8_PATH=./recurrentgemma_model/recurrentgemma-2b-it/trt_ckpt/fp8/1-gpu/\npython ../quantization/quantize.py --model_dir ${CKPT_2B_IT_PATH} \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir ${UNIFIED_CKPT_2B_IT_FP8_PATH} \\\n                                   --calib_size 512 \\\n                                   --tp_size 1\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with 2-way Tensor Parallel InternLM2 7B\nDESCRIPTION: Performs text summarization using InternLM2 7B with FP16 precision distributed across 2 GPUs with tensor parallelism via MPI.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 --allow-run-as-root \\\n    python ../summarize.py --test_trt_llm --test_hf \\\n                           --hf_model_dir ./internlm2-chat-7b/ \\\n                           --data_type fp16 \\\n                           --engine_dir ./internlm2-chat-7b/trt_engines/fp16/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: Running BFloat16 Inference for Gemma 7B Model Using PyTorch Checkpoint\nDESCRIPTION: This script demonstrates BFloat16 inference on the larger Gemma 7B model using a PyTorch checkpoint. Since the PyTorch model doesn't include a model config, it needs to be added manually. The process includes cloning the model, converting the checkpoint to a unified format, building the engine, and evaluating with summarize.py and mmlu.py.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-7b-pytorch\n\nCKPT_PATH=gemma-7b-pytorch/\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_7b_it_tensorrt_llm/bf16/tp1/\nENGINE_PATH=/tmp/gemma/7B/bf16/1-gpu/\nVOCAB_FILE_PATH=gemma-7b-pytorch/tokenizer.model\n\npython3 ./examples/gemma/convert_checkpoint.py \\\n    --ckpt-type torch \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype bfloat16 \\\n    --world-size 1 \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n\npython3 ../mmlu.py --test_trt_llm \\\n                 --vocab_file ${VOCAB_FILE_PATH} \\\n                 --engine_dir ${ENGINE_PATH}\n\nAverage accuracy 0.739 - social sciences\nAverage accuracy 0.697 - other (business, health, misc.)\nAverage accuracy: 0.630\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Dataset for Benchmark\nDESCRIPTION: Command to generate a synthetic dataset of 1000 requests with uniform input/output lengths of 128 tokens for Llama-3.1-8B model benchmarking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbenchmarks/cpp/prepare_dataset.py --stdout --tokenizer meta-llama/Llama-3.1-8B token-norm-dist --input-mean 128 --output-mean 128 --input-stdev 0 --output-stdev 0 --num-requests 1000 > /tmp/synthetic_128_128.txt\n```\n\n----------------------------------------\n\nTITLE: Running Qwen Model with FP16 Precision\nDESCRIPTION: Command to run inference on a TensorRT-LLM Qwen model with FP16 precision. It takes Chinese input text and generates up to 50 tokens of output using the specified tokenizer and engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --input_text \"你好，请问你叫什么？\" \\\n                  --max_output_len=50 \\\n                  --tokenizer_dir ./tmp/Qwen/7B/ \\\n                  --engine_dir=./tmp/Qwen/7B/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine for Llama Model\nDESCRIPTION: Commands to convert Llama-v2-13B checkpoint and build TensorRT engine with speculative decoding settings. Requires paged context FMHA and external draft tokens mode.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/prompt_lookup/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/llama\n\npython3 convert_checkpoint.py \\\n    --model_dir=<Path To Llama-v2-13B repo> \\\n    --output_dir=./ckpt-target \\\n    --dtype=float16\n\ntrtllm-build \\\n    --checkpoint_dir=./ckpt-target \\\n    --output_dir=./target-engine \\\n    --gemm_plugin=float16 \\\n    --use_paged_context_fmha=enable \\\n    --speculative_decoding_mode=draft_tokens_external \\\n    --max_draft_len=10 \\\n    --max_batch_size=4 \\\n    --max_input_len=3200 \\\n    --max_seq_len=4800\n```\n\n----------------------------------------\n\nTITLE: Starting the TensorRT-LLM FastAPI Server\nDESCRIPTION: Command to start the FastAPI server with a TensorRT-LLM model. This creates an API endpoint that can receive inference requests.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/apps/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./fastapi_server.py <model_dir>&\n```\n\n----------------------------------------\n\nTITLE: Enabling Reduce Norm Fusion Plugin for Llama Models\nDESCRIPTION: This code shows how to enable the Reduce Norm Fusion Plugin, which fuses ResidualAdd and LayerNorm kernels with AllReduce operations into a single kernel. This optimization is specific to Llama models and most beneficial for workloads heavy on generation phase and when using tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbuild_config.plugin_config.reduce_fusion = True\n```\n\n----------------------------------------\n\nTITLE: Running Whisper Inference\nDESCRIPTION: Commands for running inference using the C++ runtime with Python bindings, supporting both single audio file and dataset inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/whisper/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\noutput_dir=./whisper_large_v3\npython3 run.py --name single_wav_test --engine_dir $output_dir --input_file assets/1221-135766-0002.wav\npython3 run.py --engine_dir $output_dir --dataset hf-internal-testing/librispeech_asr_dummy --enable_warmup --name librispeech_dummy_large_v3\n```\n\n----------------------------------------\n\nTITLE: Building Mixtral 8x22B with Tensor and Expert Parallelism\nDESCRIPTION: Commands to convert the Mixtral 8x22B checkpoint and build TensorRT engines with tensor and expert parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py --model_dir ./Mixtral-8x22B-v0.1 \\\n                             --output_dir ./tllm_checkpoint_mixtral_8gpu \\\n                             --dtype float16 \\\n                             --tp_size 8 \\\n                             --moe_tp_size 2 \\\n                             --moe_ep_size 4\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_8gpu \\\n                 --output_dir ./trt_engines/mixtral/tp2ep4 \\\n                 --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Building DBRX with INT8 Weight-Only Quantization\nDESCRIPTION: Converts DBRX using INT8 weight-only quantization with 4-way tensor parallelism to reduce memory footprint while maintaining reasonable accuracy, then builds corresponding TensorRT engines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# 4-way tensor parallelism, int8 weight-only\npython convert_checkpoint.py --model_dir dbrx-base \\\n        --dtype float16 \\\n        --use_weight_only \\\n        --weight_only_precision int8 \\\n        --tp_size 4 \\\n        --workers 4 \\\n        --output_dir dbrx/trt_ckpt/int8-wo/tp4\n\ntrtllm-build --checkpoint_dir dbrx/trt_ckpt/int8-wo/tp4 \\\n        --gpt_attention_plugin float16 \\\n        --gemm_plugin float16 \\\n        --moe_plugin float16 \\\n        --workers 4 \\\n        --output_dir dbrx/trt_engines/int8-wo/tp4\n```\n\n----------------------------------------\n\nTITLE: Quantizing GPT-J with SmoothQuant (W8A8)\nDESCRIPTION: Command to enable SmoothQuant W8A8 quantization (weight per-channel, activation per-tensor) for GPT-J.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Enable smoothquant (W8A8 kernel).\npython ../../../quantization/quantize.py --model_dir ./gpt-j-6b \\\n                                   --dtype float16 \\\n                                   --qformat int8_sq \\\n                                   --output_dir ./trt_ckpt/gptj_sq_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines for OPT Models\nDESCRIPTION: Commands to build TensorRT engines for OPT models using trtllm-build, configuring parameters like batch size, sequence length, and optimizations for different model sizes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# OPT-125M\ntrtllm-build --checkpoint_dir ./opt/125M/trt_ckpt/fp16/1-gpu/ \\\n                --gemm_plugin float16 \\\n                --max_batch_size 8 \\\n                --max_input_len 924 \\\n                --max_seq_len 1024 \\\n                --output_dir ./opt/125M/trt_engines/fp16/1-gpu/\n\n# OPT-350M\ntrtllm-build --checkpoint_dir ./opt/350M/trt_ckpt/fp16/1-gpu/ \\\n                --gemm_plugin float16 \\\n                --max_batch_size 8 \\\n                --max_input_len 924 \\\n                --max_seq_len 1024 \\\n                --output_dir ./opt/350M/trt_engines/fp16/1-gpu/\n\n# OPT-2.7B\ntrtllm-build --checkpoint_dir ./opt/2.7B/trt_ckpt/fp16/1-gpu/ \\\n                --gemm_plugin float16 \\\n                --max_batch_size 8 \\\n                --max_input_len 924 \\\n                --max_seq_len 1024 \\\n                --output_dir ./opt/2.7B/trt_engines/fp16/1-gpu/\n\n# OPT-66B\ntrtllm-build --checkpoint_dir ./opt/66B/trt_ckpt/fp16/4-gpu/ \\\n                --gemm_plugin float16 \\\n                --max_batch_size 8 \\\n                --max_input_len 924 \\\n                --max_seq_len 1024 \\\n                --output_dir ./opt/66B/trt_engines/fp16/4-gpu/ \\\n                --workers 2\n```\n\n----------------------------------------\n\nTITLE: SmoothQuant and KV Cache Quantization\nDESCRIPTION: Convert MPT-7B using SmoothQuant and INT8 KV cache quantization techniques\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/int8_sq/ --smoothquant 0.5\n\npython convert_checkpoint.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/fp16_int8kv/ --dtype float16 --calibrate_kv_cache\n```\n\n----------------------------------------\n\nTITLE: Running Multi-turn Dialogue\nDESCRIPTION: Command to run multiple rounds of dialogue with the model, supporting KV cache reuse for audio segments.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 run_chat.py \\\n    --tokenizer_dir=$MODEL_PATH \\\n    --engine_dir=${ENGINE_DIR}/llm \\\n    --audio_engine_path=${ENGINE_DIR}/audio/model.engine \\\n    --max_new_tokens=256\n```\n\n----------------------------------------\n\nTITLE: Running Falcon-40B-Instruct with 2-Way Tensor Parallelism\nDESCRIPTION: Command to run the Falcon-40B-Instruct model using a 2-way tensor parallelism configuration with TensorRT-LLM's summarize script.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 --allow-run-as-root --oversubscribe \\\n    python ../../../summarize.py --test_trt_llm \\\n                           --hf_model_dir ./falcon/40b-instruct \\\n                           --engine_dir ./falcon/40b-instruct/trt_engines/bf16/tp2-pp1/\n```\n\n----------------------------------------\n\nTITLE: Generating Images with SDXL TensorRT Engine on Single and Multi-GPU Setups\nDESCRIPTION: Commands for running image generation using the built TensorRT engine. Shows how to generate images with custom prompts on both single-GPU and distributed 2-GPU configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/sdxl/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# 1 gpu\npython run_sdxl.py --size 1024 --prompt \"flowers, rabbit\"\n\n# 2 gpus\nmpirun -n 2 --allow-run-as-root python run_sdxl.py --size 1024 --prompt \"flowers, rabbit\"\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with 4-way Tensor Parallel InternLM2 20B in BF16\nDESCRIPTION: Performs text summarization using InternLM2 20B with BFloat16 precision distributed across 4 GPUs with tensor parallelism via MPI.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 4 --allow-run-as-root \\\n    python ../summarize.py --test_trt_llm --test_hf \\\n                           --hf_model_dir ./internlm2-chat-20b/ \\\n                           --data_type bf16 \\\n                           --engine_dir ./internlm2-chat-20b/trt_engines/bf16/4-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building and Running NVIDIA GPT-Next Model with BFloat16 Precision\nDESCRIPTION: Commands to download, convert, and build NVIDIA's GPT-Next model with BFloat16 precision. The workflow includes extracting the tokenizer from the NeMo checkpoint and notes the requirement to use the GPT attention plugin for Rotary positional embeddings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n# Download NeMo checkpoint\nwget https://huggingface.co/nvidia/GPT-2B-001/resolve/main/GPT-2B-001_bf16_tp1.nemo\n\n# Convert to TensorRT-LLM checkpoint\n# It also extracts the tokenizer file and saves to the TensorRT-LLM checkpoint folder\npython3 convert_checkpoint.py --nemo_ckpt_path GPT-2B-001_bf16_tp1.nemo \\\n        --dtype bfloat16 \\\n        --output_dir gpt-next-2B/trt_ckpt/bf16/1-gpu\n\n# Build TensorRT-LLM engines\n# --gpt_attention_plugin must be set for GPT-Next since Rotary positional embeddings (RoPE) is only supported by the gpt attention plugin at this time.\ntrtllm-build --checkpoint_dir gpt-next-2B/trt_ckpt/bf16/1-gpu \\\n        --output_dir gpt-next-2B/trt_engines/bf16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Running EAGLE Decoding with 4-way Tensor Parallelism\nDESCRIPTION: Command to run inference with EAGLE decoding using a Vicuna model distributed across 4 GPUs using MPI to coordinate the tensor-parallel execution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 4 --allow-run-as-root --oversubscribe \\\n  python ../run.py --engine_dir ./tmp/eagle/7B/trt_engines/fp16/4-gpu/ \\\n                 --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                 --max_output_len=100 \\\n                 --eagle_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                 --input_text \"Once upon\"\n```\n\n----------------------------------------\n\nTITLE: Running Model Summarization with Evaluation\nDESCRIPTION: Command to run and evaluate the summarization capability of TensorRT-LLM engines. Computes ROUGE scores for model evaluation and allows comparing TensorRT-LLM and Hugging Face implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../summarize.py --engine_dir gpt2/trt_engines/fp16/1-gpu \\\n        --hf_model_dir gpt2 \\\n        --test_trt_llm \\\n        --test_hf\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task with Single GPU\nDESCRIPTION: Command to run summarization using TensorRT-LLM Phi model on single GPU with accuracy checking\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/phi/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../summarize.py --engine_dir ./phi-engine \\\n                        --hf_model_dir /path/to/phi-model \\\n                        --batch_size 1 \\\n                        --test_trt_llm \\\n                        --test_hf \\\n                        --data_type fp16 \\\n                        --check_accuracy \\\n                        --tensorrt_llm_rouge1_threshold=20\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with SmoothQuant Optimization\nDESCRIPTION: Command to build a TensorRT engine from a SmoothQuant processed checkpoint. It creates an optimized engine with INT8 inference capabilities.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_sq \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Running Qwen-72B Model with 8-GPU Parallelism\nDESCRIPTION: Command to run inference on a TensorRT-LLM Qwen-72B model using 8-GPU parallel execution. It uses MPI to coordinate the distributed inference across multiple GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 8 --allow-run-as-root \\\n    python ../run.py --input_text \"What is your name?\" \\\n                     --max_output_len=50 \\\n                     --tokenizer_dir ./tmp/Qwen/72B/ \\\n                     --engine_dir=./tmp/Qwen/72B/trt_engines/fp16/8-gpu/\n```\n\n----------------------------------------\n\nTITLE: Summarization with Vicuna-13B using Medusa Decoding on 4 GPUs\nDESCRIPTION: This command uses MPI to run the Medusa decoding for summarization across 4 GPUs with the Vicuna-13B-v1.3 model. It specifies the engine directory, model paths, and Medusa choice patterns with a temperature of 1.0 and batch size of 1.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 4 --allow-run-as-root --oversubscribe \\\n    python ../summarize.py --engine_dir ./tmp/medusa/13B/trt_engines/fp16/4-gpu/ \\\n                           --hf_model_dir ./vicuna-13b-v1.3/ \\\n                           --tokenizer_dir ./vicuna-13b-v1.3/ \\\n                           --test_trt_llm \\\n                           --data_type fp16 \\\n                           --medusa_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                           --use_py_session \\\n                           --temperature 1.0 \\\n                           --batch_size 1\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task with Skywork Model (BF16)\nDESCRIPTION: Executes a summarization task using the BF16 TensorRT engine of the Skywork model on the CNN/Daily Mail dataset, including accuracy checking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../summarize.py --hf_model_dir ./Skywork-13B-base \\\n                       --test_hf \\\n                       --batch_size 32 \\\n                       --max_input_length 512 \\\n                       --output_len 512 \\\n                       --test_trt_llm \\\n                       --engine_dir ./skywork-13b-base/trt_engine/bf16 \\\n                       --data_type bf16 \\\n                       --check_accuracy \\\n                       --tensorrt_llm_rouge1_threshold=14\n```\n\n----------------------------------------\n\nTITLE: FP8 Post-Training Quantization\nDESCRIPTION: Commands for quantizing the base model to FP8 while keeping Medusa heads non-quantized, followed by building TensorRT engines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py --model_dir /path/to/base-model-hf/ \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir ./tllm_checkpoint_1gpu_base_model_fp8_medusa_fp16 \\\n                                   --calib_size 512 \\\n                                   --tp_size 1 \\\n                                   --medusa_model_dir /path/to/medusa_head/ \\\n                                   --num_medusa_heads 4\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_base_model_fp8_medusa_fp16 \\\n         --output_dir ./trt_engine_1gpu_base_model_fp8_medusa_fp16 \\\n         --gemm_plugin float16 \\\n         --gpt_attention_plugin float16 \\\n         --speculative_decoding_mode medusa \\\n         --max_batch_size 4\n```\n\n----------------------------------------\n\nTITLE: Binding Weights to Model Parameters in TensorRT-LLM\nDESCRIPTION: Demonstrates how to bind weights to parameters in a TensorRT-LLM model before compilation. The example shows a Linear layer with weight and bias parameters being assigned values from external files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# The Linear operator exposes two parameters (see tensorrt_llm/layers/linear.py):\nclass Linear(Module):\n    def __init__(self, ...):\n        self.weight = Parameter(shape=(self.out_features, self.in_features), dtype=dtype)\n        self.bias   = Parameter(shape=(self.out_features, ), dtype=dtype)\n\n# The parameters are bound to the weights before compiling the model. See examples/gpt/weight.py:\ntensorrt_llm_gpt.layers[i].mlp.fc.weight.value = fromfile(...)\ntensorrt_llm_gpt.layers[i].mlp.fc.bias.value   = fromfile(...)\n```\n\n----------------------------------------\n\nTITLE: Configuring MoE Parallelism Parameters in TensorRT-LLM\nDESCRIPTION: Command line parameters for enabling different types of MoE parallelism when converting checkpoints. Parameters include moe_tp_size and moe_ep_size, which must be set such that their product equals tp_size for proper model parallelization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/expert-parallelism.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--moe_tp_size <size> --moe_ep_size <size>\n```\n\n----------------------------------------\n\nTITLE: Marking Intermediate Tensors as Network Outputs in TensorRT-LLM\nDESCRIPTION: This code snippet shows how to mark registered intermediate tensors as network outputs in TensorRT-LLM, which is necessary for accessing these tensors during debugging.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfor k, v in gm.named_network_outputs():\n    net._mark_output(v, k, dtype)\n```\n\n----------------------------------------\n\nTITLE: Configuring Beam Search Sampling in TensorRT-LLM\nDESCRIPTION: Sets up beam search with a beam width of 4 for text generation. This requires configuring both BuildConfig with max_beam_width and SamplingParams with the matching beam_width parameter.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm.llmapi import LLM, SamplingParams, BuildConfig\n\nbuild_config = BuildConfig()\nbuild_config.max_beam_width = 4\n\nllm = LLM(<llama_model_path>, build_config=build_config)\n# Let the LLM object generate text with the default sampling strategy, or\n# you can create a SamplingParams object as well with several fields set manually\nsampling_params = SamplingParams(beam_width=4) # current limitation: beam_width should be equal to max_beam_width\n\nfor output in llm.generate(<prompt>, sampling_params=sampling_params):\n    print(output)\n```\n\n----------------------------------------\n\nTITLE: Quantizing Model to FP8\nDESCRIPTION: Command to quantize Phi model to FP8 format with bfloat16 base precision\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/phi/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nDTYPE=bfloat16\npython3 ../quantization/quantize.py \\\n       --model_dir phi3-model \\\n       --output_dir ./phi3-checkpoint \\\n       --dtype $DTYPE \\\n       --qformat fp8 --kv_cache_dtype fp8\n```\n\n----------------------------------------\n\nTITLE: Converting Baichuan2 13B Checkpoint with GPTQ Quantization\nDESCRIPTION: Python command to convert a Baichuan2 13B model checkpoint using INT4 GPTQ quantization with 2-way tensor parallelism for TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_version v2_13b \\\n                                 --quant_ckpt_path ./baichuan-2-13b-4bit-gs64.safetensors \\\n                                 --dtype float16 \\\n                                 --use_weight_only \\\n                                 --weight_only_precision int4_gptq \\\n                                 --group_size 64 \\\n                                 --tp_size 2 \\\n                                 --output_dir ./tmp/baichuan_v2_13b/trt_ckpts/int4_gptq_gs64/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: Converting RecurrentGemma-2b HF Checkpoint to TensorRT-LLM Format\nDESCRIPTION: This command converts the recurrentgemma-2b model from Hugging Face format to TensorRT-LLM checkpoint format with FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b\nCKPT_2B_PATH=./recurrentgemma_model/recurrentgemma-2b\nUNIFIED_CKPT_2B_PATH=./recurrentgemma_model/recurrentgemma-2b/trt_ckpt/fp16/1-gpu/\npython convert_checkpoint.py --model_dir ${CKPT_2B_PATH} \\\n                             --ckpt_type hf \\\n                             --dtype float16 \\\n                             --output_dir ${UNIFIED_CKPT_2B_PATH}\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines for C++ Tests\nDESCRIPTION: Scripts for downloading models from Huggingface and converting them to TensorRT engines. These commands build engines for various models including GPT2, GPT-J, LLaMa, ChatGLM, Medusa, Eagle, and RedRafter.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=examples/gpt:$PYTHONPATH python3 cpp/tests/resources/scripts/build_gpt_engines.py\nPYTHONPATH=examples/gptj:$PYTHONPATH python3 cpp/tests/resources/scripts/build_gptj_engines.py\nPYTHONPATH=examples/llama:$PYTHONPATH python3 cpp/tests/resources/scripts/build_llama_engines.py\nPYTHONPATH=examples/chatglm:$PYTHONPATH python3 cpp/tests/resources/scripts/build_chatglm_engines.py\nPYTHONPATH=examples/medusa:$PYTHONPATH python3 cpp/tests/resources/scripts/build_medusa_engines.py\nPYTHONPATH=examples/eagle:$PYTHONPATH python3 cpp/tests/resources/scripts/build_eagle_engines.py\nPYTHONPATH=examples/redrafter:$PYTHONPATH python3 cpp/tests/resources/scripts/build_redrafter_engines.py --has_tllm_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Jais-30b-chat-v3 (2-way Tensor Parallelism)\nDESCRIPTION: This command uses MPI to run inference across 2 GPUs with the Jais-30b-chat-v3 model, demonstrating multi-GPU inference with tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 2 \\\n    python3 ../../../run.py --engine_dir jais-30b-chat-v3/trt_engines/fp16/2-gpu \\\n        --tokenizer_dir core42/jais-30b-chat-v3 \\\n        --max_output_len 30\n```\n\n----------------------------------------\n\nTITLE: Running Medusa Decoding Inference\nDESCRIPTION: Commands for running inference with Medusa decoding using different model configurations and GPU setups.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir ./tmp/medusa/7B/trt_engines/fp16/1-gpu/ \\\n                 --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                 --max_output_len=100 \\\n                 --medusa_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3]]\" \\\n                 --temperature 1.0 \\\n                 --input_text \"Once upon\"\n```\n\n----------------------------------------\n\nTITLE: Building 8-way Tensor Parallel Engine for BLOOM-176B with Hidden Dimension Sharding\nDESCRIPTION: Commands to build BLOOM-176B model with 8-way tensor parallelism, sharding the embedding table in the hidden dimension.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./bloom/176B/ \\\n                --dtype float16 \\\n                --output_dir ./bloom/176B/trt_ckpt/fp16/8-gpu/ \\\n                --tp_size 8 \\\n                --use_parallel_embedding \\\n                --embedding_sharding_dim 1\ntrtllm-build --checkpoint_dir ./bloom/176B/trt_ckpt/fp16/8-gpu/ \\\n                --gemm_plugin float16 \\\n                --output_dir ./bloom/176B/trt_engines/fp16/8-gpu/ \\\n                --workers 2\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-14B-Chat (2-way Tensor Parallelism)\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM checkpoints and build a 2-GPU engine with tensor parallelism for Qwen-14B-Chat.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/14B/ \\\n                            --output_dir ./tllm_checkpoint_2gpu_tp2 \\\n                            --dtype float16 \\\n                            --tp_size 2\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_2gpu_tp2 \\\n            --output_dir ./tmp/qwen/14B/trt_engines/fp16/2-gpu/ \\\n            --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with Qwen-7B in FP16 on Two GPUs\nDESCRIPTION: Executes summarization using the Qwen-7B model in FP16 precision distributed across two GPUs using MPI.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 --allow-run-as-root \\\n    python ../summarize.py --test_trt_llm \\\n                           --hf_model_dir  ./tmp/Qwen/7B/ \\\n                           --data_type fp16 \\\n                           --engine_dir ./tmp/Qwen/7B/trt_engines/fp16/2-gpu/ \\\n                           --max_input_length 2048 \\\n                           --output_len 2048\n```\n\n----------------------------------------\n\nTITLE: Profiling Script for Single Rank in Multi-GPU TensorRT-LLM Model\nDESCRIPTION: Bash script for profiling a single rank of a multi-GPU model using NVIDIA Nsight Systems. It sets up profiling for the first rank (local rank 0) with specific CUDA and NVTX tracing options, while other ranks execute normally.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-analysis.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\n# Use $PMI_RANK for MPICH and $SLURM_PROCID with srun.\nif [ $OMPI_COMM_WORLD_LOCAL_RANK -eq 0 ]; then\n  nsys profile -e \"NSYS_MPI_STORE_TEAMS_PER_RANK=1\" -t cuda,nvtx --gpu-metrics-device=${OMPI_COMM_WORLD_LOCAL_RANK} -c cudaProfilerApi --capture-range-end=\"repeat[]\" --gpu-metrics-frequency=100000 \"$@\"\nelse\n  \"$@\"\nfi\n```\n\n----------------------------------------\n\nTITLE: Executing the DoRA-enabled Engine\nDESCRIPTION: Command to run the TensorRT-LLM engine with DoRA adapters. It uses the run.py script with parameters for the engine directory, tokenizer, LoRA task IDs, and other runtime options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/dora/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir $ENGINE_DIR --tokenizer_dir Meta-Llama-3-8B --lora_task_uids 0 --max_output_len 32 --input_text ...\n```\n\n----------------------------------------\n\nTITLE: Compiling Triton Fused Attention Kernels with AoT for FP16\nDESCRIPTION: Command to compile a FP16 Fused Attention kernel using Triton's Ahead-of-Time compiler with specific block dimensions and parameters, generating C code that can be used in a TensorRT plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Kernel for data type=float16, BLOCK_M=128, BLOCK_DMODEL=64, BLOCK_N=128\nexport TRITON_ROOT=$(pip show triton | grep Location | cut -d' ' -f2)\nrm -rf aot\nmkdir -p aot/fp16\npython ${TRITON_ROOT}/triton/tools/compile.py \\\n    fmha_triton.py \\\n    -n fused_attention_kernel \\\n    -o aot/fp16/fmha_kernel_d64_fp16 \\\n    --out-name fmha_d64_fp16 \\\n    -w 4 \\\n    -ns 2 \\\n    -s \"*fp16:16, *fp32:16, *fp32:16, *fp16:16, *fp16:16, *fp16:16, fp32, i32, i32, i32, 128, 64, 128\" \\\n    -g \"(seq_len + 127) / 128, batch_size * num_heads, 1\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Downloading Models\nDESCRIPTION: Commands to install required packages and download the Vicuna base model and Medusa heads from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n\ngit lfs install\ngit clone https://huggingface.co/lmsys/vicuna-7b-v1.3\nhttps://huggingface.co/FasterDecoding/medusa-vicuna-7b-v1.3\n```\n\n----------------------------------------\n\nTITLE: Initializing ModelWeightsLoader for BLOOM in Python\nDESCRIPTION: This snippet shows how to set up the ModelWeightsLoader for BLOOM model weights. It defines a dictionary mapping TensorRT-LLM keys to BLOOM keys and initializes the loader with an external checkpoint directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm.models.model_weights_loader import ModelWeightsLoader\nbloom_dict = {\n    \"transformer\": \"\",\n    \"layers\": \"h\",\n    \"ln_f\": \"ln_f\",\n    \"lm_head\": \"word_embeddings\",\n    \"ln_embed\": \"word_embeddings_layernorm\",\n    \"vocab_embedding\": \"word_embeddings\",\n    \"attention\": \"self_attention\",\n    \"qkv\": \"query_key_value\",\n    \"dense\": \"dense\",\n    \"fc\": \"dense_h_to_4h\",\n    \"proj\": \"dense_4h_to_h\",\n    \"post_layernorm\": \"post_attention_layernorm\",\n}\nloader = ModelWeightsLoader(external_checkpoint_dir, bloom_dict)\n```\n\n----------------------------------------\n\nTITLE: Converting BF16 Checkpoints\nDESCRIPTION: Script execution for converting HuggingFace checkpoints to TensorRT-LLM format using BF16 precision with tensor parallelism support.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/granite/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nENGINE=\"${HF_MODEL}_${PREC_RAW}_tp${TP}\"\nexport TRTLLM_DISABLE_UNIFIED_CONVERTER=1  # The current checkpoint conversion code requires legacy path\npython3 ../llama/convert_checkpoint.py --model_dir tmp/hf_checkpoints/${HF_MODEL} \\\n                                       --output_dir tmp/tllm_checkpoints/${ENGINE} \\\n                                       --dtype ${PREC_RAW} \\\n                                       --tp_size ${TP} \\\n                                       --use_embedding_sharing\n```\n\n----------------------------------------\n\nTITLE: Building INT8 Weight-Only Quantized InternLM2 7B Engine\nDESCRIPTION: Builds the TensorRT engine for INT8 weight-only quantized InternLM2 7B model using the converted checkpoint with FP16 activation precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./internlm2-chat-7b/trt_engines/int8/1-gpu/ \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Detailed Engine Information Log Output\nDESCRIPTION: Example of the detailed engine information provided when TRACE logging is enabled, showing input/output tensor details and optimization profiles with shape constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_10\n\nLANGUAGE: txt\nCODE:\n```\n[TensorRT-LLM][TRACE] =====================================================================\n[TensorRT-LLM][TRACE]              Name              |I/O|Location|DataType|    Shape     |\n[TensorRT-LLM][TRACE] ---------------------------------------------------------------------\n[TensorRT-LLM][TRACE] input_ids                      | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] position_ids                   | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] last_token_ids                 | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] kv_cache_block_offsets         | I |  GPU   | INT32  |(1, -1, 2, -1)|\n[TensorRT-LLM][TRACE] host_kv_cache_block_offsets    | I |  GPU   | INT32  |(1, -1, 2, -1)|\n[TensorRT-LLM][TRACE] host_kv_cache_pool_pointers    | I |  GPU   | INT64  |    (1, 2)    |\n[TensorRT-LLM][TRACE] host_kv_cache_pool_mapping     | I |  GPU   | INT32  |     (28)     |\n[TensorRT-LLM][TRACE] sequence_length                | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] host_request_types             | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] host_past_key_value_lengths    | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] context_lengths                | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] host_runtime_perf_knobs        | I |  GPU   | INT64  |     (16)     |\n[TensorRT-LLM][TRACE] host_context_lengths           | I |  GPU   | INT32  |     (-1)     |\n[TensorRT-LLM][TRACE] host_max_attention_window_sizes| I |  GPU   | INT32  |     (28)     |\n[TensorRT-LLM][TRACE] host_sink_token_length         | I |  GPU   | INT32  |     (1)      |\n[TensorRT-LLM][TRACE] cache_indirection              | I |  GPU   | INT32  | (-1, 1, -1)  |\n[TensorRT-LLM][TRACE] logits                         | O |  GPU   |  FP32  | (-1, 65024)  |\n[TensorRT-LLM][TRACE] =====================================================================\n[TensorRT-LLM][TRACE] Information of optimization profile.\n[TensorRT-LLM][TRACE] Optimization Profile 0:\n[TensorRT-LLM][TRACE] =============================================================================\n[TensorRT-LLM][TRACE]              Name              |     Min      |     Opt      |     Max      |\n[TensorRT-LLM][TRACE] -----------------------------------------------------------------------------\n[TensorRT-LLM][TRACE] input_ids                      |     (1)      |     (8)      |    (8192)    |\n[TensorRT-LLM][TRACE] position_ids                   |     (1)      |     (8)      |    (8192)    |\n[TensorRT-LLM][TRACE] last_token_ids                 |     (1)      |     (4)      |     (8)      |\n[TensorRT-LLM][TRACE] kv_cache_block_offsets         | (1, 1, 2, 1) |(1, 4, 2, 16) |(1, 8, 2, 32) |\n[TensorRT-LLM][TRACE] host_kv_cache_block_offsets    | (1, 1, 2, 1) |(1, 4, 2, 16) |(1, 8, 2, 32) |\n[TensorRT-LLM][TRACE] host_kv_cache_pool_pointers    |    (1, 2)    |    (1, 2)    |    (1, 2)    |\n[TensorRT-LLM][TRACE] host_kv_cache_pool_mapping     |     (28)     |     (28)     |     (28)     |\n[TensorRT-LLM][TRACE] sequence_length                |     (1)      |     (4)      |     (8)      |\n[TensorRT-LLM][TRACE] host_request_types             |     (1)      |     (4)      |     (8)      |\n[TensorRT-LLM][TRACE] host_past_key_value_lengths    |     (1)      |     (4)      |     (8)      |\n[TensorRT-LLM][TRACE] context_lengths                |     (1)      |     (4)      |     (8)      |\n[TensorRT-LLM][TRACE] host_runtime_perf_knobs        |     (16)     |     (16)     |     (16)     |\n[TensorRT-LLM][TRACE] host_context_lengths           |     (1)      |     (4)      |     (8)      |\n[TensorRT-LLM][TRACE] host_max_attention_window_sizes|     (28)     |     (28)     |     (28)     |\n[TensorRT-LLM][TRACE] host_sink_token_length         |     (1)      |     (1)      |     (1)      |\n[TensorRT-LLM][TRACE] cache_indirection              |  (1, 1, 1)   | (4, 1, 1024) | (8, 1, 2048) |\n[TensorRT-LLM][TRACE] logits                         |  (1, 65024)  |  (4, 65024)  |  (8, 65024)  |\n[TensorRT-LLM][TRACE] =============================================================================\n```\n\n----------------------------------------\n\nTITLE: Configuring SSH for Multi-Node DeepSeek-V3 Setup\nDESCRIPTION: Commands to install and configure SSH inside the Docker containers for multi-node setup of DeepSeek-V3, enabling root login and public key authentication.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\napt-get update && apt-get install -y openssh-server\n\n# modify /etc/ssh/sshd_config\nPermitRootLogin yes\nPubkeyAuthentication yes\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with Lookahead Decoding\nDESCRIPTION: Python command to run text generation using Lookahead decoding with TensorRT-LLM, specifying the tokenizer directory, engine directory, and Lookahead configuration parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/lookahead/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython examples/run.py          \\\n    --tokenizer_dir=$MODEL_DIR  \\\n    --engine_dir=$ENGINE_DIR    \\\n    --max_output_len=32         \\\n    --lookahead_config=[7,7,7]  \\\n    --log_level=verbose         \\\n    --input_text 'Once upon' 'To be, or not' 'Be not afraid of greatness'\n```\n\n----------------------------------------\n\nTITLE: Running Inference with InternLM2 7B BFloat16 Model\nDESCRIPTION: Executes inference using the InternLM2 7B model with BFloat16 precision on a single GPU for higher numerical stability.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm2-chat-7b/ \\\n                 --engine_dir=./internlm2-chat-7b/trt_engines/bf16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU Inference on Single Node\nDESCRIPTION: Commands to run inference using multiple GPUs on a single node with mpirun. Demonstrates execution for both 2-GPU GPT-2 and 8-GPU GPT-175B setups.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 2 \\\n    python3 ../run.py --engine_dir gpt2/trt_engines/fp16/2-gpu \\\n        --tokenizer_dir gpt2 \\\n        --max_output_len 8\n\n# Note that GPT-175B is built with random weights, so the output will also be random\nmpirun -np 8 \\\n    python3 ../run.py --engine_dir gpt_175b/trt_engines/fp16/8-gpu \\\n        --max_output_len 8\n```\n\n----------------------------------------\n\nTITLE: Verifying New TensorRT-LLM Model Implementation\nDESCRIPTION: Demonstrates the commands for verifying a newly added model, including converting checkpoints, building TensorRT engines, and testing with inference examples.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/add-model.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/my_model/\n\npython convert_checkpoint.py --model_dir hf_model_dir --output_dir tllm_ckpt_dir\n\ntrtllm-build --checkpoint_dir tllm_ckpt_dir --output_dir tllm_engine_dir\n\n# try the model with a single prompt\npython ../run.py --engine_dir tllm_engine_dir --tokenizer_dir hf_model_dir --input_text \"Born in north-east France, Soyer trained as a\"\n# run summarization task\npython ../summarize.py --engine_dir tllm_engine_dir --hf_model_dir hf_model_dir --test_trt_llm\n```\n\n----------------------------------------\n\nTITLE: Running Speculative Decoding with Draft-Target Models in TensorRT-LLM\nDESCRIPTION: Bash command for executing the run.py script to perform speculative decoding using the previously built draft and target engines. The configuration specifies that the draft model generates 4 tokens per iteration, with the draft model on GPU 0 and target model on GPU 1.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/draft_target_model/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/llama\n\npython3 ../run.py \\\n    --tokenizer_dir <Path To Llama-v2-7B repo> \\\n    --draft_engine_dir ./draft-engine \\\n    --engine_dir ./target-engine \\\n    --draft_target_model_config=\"[4,[0],[1],True]\" \\\n    --max_output_len=256 \\\n    --kv_cache_enable_block_reuse \\\n    --kv_cache_free_gpu_memory_fraction=0.4 \\\n    --output_generation_logits \\\n    --input_text=\"How does Draft-Sampling work?\"\n```\n\n----------------------------------------\n\nTITLE: Building FP8 Post-Training Quantized Engine for BLOOM-3B\nDESCRIPTION: Commands to quantize HuggingFace BLOOM-3B to FP8 precision and build TensorRT engines with FP8 context and KV cache.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir /home/scratch.trt_llm_data/llm-models/bloom-3b \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir /tmp/bloom/3b/trt_ckpts/fp8/1-gpu/ \\\n                                   --calib_size 512 \\\n                                   --tp_size 1\n\ntrtllm-build --checkpoint_dir /tmp/bloom/3b/trt_ckpts/fp8/1-gpu/ \\\n             --output_dir /tmp/bloom/3b/trt_engines/fp8/1-gpu/ \\\n             --gemm_plugin float16 \\\n             --use_fp8_context_fmha enable \\\n             --workers 1\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task with Skywork Model (FP16)\nDESCRIPTION: Executes a summarization task using the FP16 TensorRT engine of the Skywork model on the CNN/Daily Mail dataset, including accuracy checking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../summarize.py --hf_model_dir ./Skywork-13B-base \\\n                       --test_hf \\\n                       --batch_size 32 \\\n                       --max_input_length 512 \\\n                       --output_len 512 \\\n                       --test_trt_llm \\\n                       --engine_dir ./skywork-13b-base/trt_engine/fp16 \\\n                       --data_type fp16 \\\n                       --check_accuracy \\\n                       --tensorrt_llm_rouge1_threshold=14\n```\n\n----------------------------------------\n\nTITLE: Deploying Quantized Model with AutoDeploy in Bash\nDESCRIPTION: This snippet demonstrates how to use the build_and_run_ad.py script to deploy a quantized model with AutoDeploy. It requires specifying the world size and the path to the ModelOpt checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/auto_deploy\npython build_and_run_ad.py --config '{\"world_size\": 1, \"model\": \"{<MODELOPT_CKPT_PATH>}\"}'\n```\n\n----------------------------------------\n\nTITLE: Building Engine with INT8 Weight-Only and INT8 KV Cache\nDESCRIPTION: Commands to build a BLOOM model with both INT8 weight-only quantization and INT8 KV cache for optimized memory usage.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./bloom/560m/ \\\n                --dtype float16 \\\n                --int8_kv_cache \\\n                --use_weight_only --output_dir ./bloom/560m/trt_ckpt/int8/1-gpu/\ntrtllm-build --checkpoint_dir ./bloom/560m/trt_ckpt/int8/1-gpu/ \\\n                --gemm_plugin float16 \\\n                --output_dir ./bloom/560m/trt_engines/int8/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Tagging and Pushing Docker Image to DockerHub\nDESCRIPTION: Commands to tag the local Docker image with a DockerHub repository name and push it to DockerHub. This makes the image available for public or private use on DockerHub.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/dev-on-cloud/build-image-to-dockerhub.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker tag tensorrt_llm/devel:with_ssh <your_dockerhub_username>/tensorrt_llm:devel\ndocker push <your_dockerhub_username>/tensorrt_llm:devel\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU Inference\nDESCRIPTION: Command to run tensor parallel inference using multiple GPUs with MPI.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 4 \\\n    python ../run.py  --max_output_len 50 \\\n        --tokenizer_dir command_r_plus \\\n        --engine_dir trt_engines/command_r_plus/fp16/4-gpu\n```\n\n----------------------------------------\n\nTITLE: Evaluating DBRX Performance with Summarization Task\nDESCRIPTION: Evaluates the DBRX TensorRT engines on a summarization task using the summarize.py script. Reports performance metrics including latency, tokens per second, and ROUGE scores.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 8 \\\n    python ../summarize.py --engine_dir dbrx/trt_engines/bf16/tp8 \\\n        --hf_model_dir dbrx-base \\\n        --test_trt_llm\n```\n\n----------------------------------------\n\nTITLE: Converting RecurrentGemma-2b-flax JAX Checkpoint to TensorRT-LLM Format\nDESCRIPTION: This command converts the recurrentgemma-2b-flax model from JAX format to TensorRT-LLM checkpoint format with FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-flax\nCKPT_2B_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-flax/2b\nUNIFIED_CKPT_2B_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-flax/trt_ckpt/fp16/1-gpu/\npython convert_checkpoint.py --model_dir ${CKPT_2B_FLAX_PATH} \\\n                             --ckpt_type jax \\\n                             --dtype float16 \\\n                             --output_dir ${UNIFIED_CKPT_2B_FLAX_PATH}\n```\n\n----------------------------------------\n\nTITLE: Cloning Hugging Face Model Repository using Git LFS\nDESCRIPTION: Shows the console commands to clone a Hugging Face model repository using Git LFS. This is necessary for downloading large model files efficiently.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/llm-api/index.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for EAGLE Decoding\nDESCRIPTION: Commands to install required packages and clone the base Vicuna model and EAGLE draft model from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n\ngit lfs install\ngit clone https://huggingface.co/lmsys/vicuna-7b-v1.3\nhttps://huggingface.co/yuhuili/EAGLE-Vicuna-7B-v1.3\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with Lookahead Decoding Support\nDESCRIPTION: Bash command to build a TensorRT-LLM engine with Lookahead decoding support, specifying maximum draft length and other engine parameters like batch size and sequence lengths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/lookahead/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build                        \\\n    --checkpoint_dir=$CKPT_DIR      \\\n    --output_dir=$ENGINE_DIR        \\\n    --gpt_attention_plugin=float16  \\\n    --gemm_plugin=float16           \\\n    --max_batch_size=32             \\\n    --max_input_len=1024            \\\n    --max_seq_len=2048              \\\n    --max_beam_width=1              \\\n    --log_level=error               \\\n    --max_draft_len=83              \\\n    --speculative_decoding_mode=lookahead_decoding\n```\n\n----------------------------------------\n\nTITLE: Running EAGLE Greedy Decoding with Single GPU\nDESCRIPTION: Command to run inference with EAGLE greedy decoding using a Vicuna model on a single GPU, specifying the engine directory, tokenizer, and EAGLE choices.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir ./tmp/eagle/7B/trt_engines/fp16/1-gpu/ \\\n                 --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                 --max_output_len=100 \\\n                 --eagle_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                 --input_text \"Once upon\"\n```\n\n----------------------------------------\n\nTITLE: Compiling Triton Fused Attention Kernels with AoT for FP32\nDESCRIPTION: Command to compile a FP32 Fused Attention kernel using Triton's Ahead-of-Time compiler with specific block dimensions and parameters, generating C code that can be used in a TensorRT plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Kernel for data type=float32, BLOCK_M=64, BLOCK_DMODEL=64, BLOCK_N=64\nmkdir -p aot/fp32\npython ${TRITON_ROOT}/triton/tools/compile.py \\\n    fmha_triton.py \\\n    -n fused_attention_kernel \\\n    -o aot/fp32/fmha_kernel_d64_fp32 \\\n    --out-name fmha_d64_fp32 \\\n    -w 4 \\\n    -ns 2 \\\n    -s \"*fp32:16, *fp32:16, *fp32:16, *fp32:16, *fp32:16, *fp32:16, fp32, i32, i32, i32, 64, 64, 64\" \\\n    -g \"(seq_len + 63) / 64, batch_size * num_heads, 1\"\n```\n\n----------------------------------------\n\nTITLE: Converting Jais-13b-chat to TensorRT-LLM Checkpoint (Single GPU, FP16)\nDESCRIPTION: This command converts the Hugging Face Jais-13b-chat model to TensorRT-LLM checkpoint format using float16 precision for a single GPU setup.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../../../gpt/convert_checkpoint.py --model_dir core42/jais-13b-chat \\\n        --dtype float16 \\\n        --output_dir jais-13b-chat/trt_ckpt/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with FP8 Optimization\nDESCRIPTION: Command to build a TensorRT engine from an FP8-quantized checkpoint. It supports FP8 inference for improved performance while maintaining accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_fp8 \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Building 8-way Tensor Parallel Engine for BLOOM-176B with Vocab Dimension Sharding\nDESCRIPTION: Commands to build BLOOM-176B model with 8-way tensor parallelism, sharding the embedding table in the vocabulary dimension.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./bloom/176B/ \\\n                --dtype float16 \\\n                --output_dir ./bloom/176B/trt_ckpt/fp16/8-gpu/ \\\n                --tp_size 8 \\\n                --use_parallel_embedding \\\n                --embedding_sharding_dim 0\ntrtllm-build --checkpoint_dir ./bloom/176B/trt_ckpt/fp16/8-gpu/ \\\n                --gemm_plugin float16 \\\n                --output_dir ./bloom/176B/trt_engines/fp16/8-gpu/ \\\n                --workers 2\n```\n\n----------------------------------------\n\nTITLE: Using TensorRT-LLM Disaggregated Service API\nDESCRIPTION: Code example demonstrating how to use TensorRT-LLM's disaggregated service API to execute context-only and generation-only requests on different executors, maintaining KV cache between them.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nRequest request{...};\n\nrequest.setRequestType(tensorrt_llm::executor::RequestType::REQUEST_TYPE_CONTEXT_ONLY);\n\nauto contextRequestId = contextExecutor.enqueueRequest(request);\n\nauto contextResponses = contextExecutor.awaitResponses(contextRequestId);\n\nauto contextPhaseParams = contextResponses.back().getResult().contextPhaseParams.value();\n\nrequest.setContextPhaseParams(contextPhaseParams);\n\nrequest.setRequestType(tensorrt_llm::executor::RequestType::REQUEST_TYPE_GENERATION_ONLY);\n\nauto generationRequestId = generationExecutor.enqueueRequest(request);\n\nauto genResponses = generationExecutor.awaitResponses(generationRequestId);\n```\n\n----------------------------------------\n\nTITLE: Compiling Triton Fused Attention Kernels with AoT for FP32\nDESCRIPTION: Command to compile a FP32 Fused Attention kernel using Triton's Ahead-of-Time compiler with specific block dimensions and parameters, generating C code that can be used in a TensorRT plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Kernel for data type=float32, BLOCK_M=64, BLOCK_DMODEL=64, BLOCK_N=64\nmkdir -p aot/fp32\npython ${TRITON_ROOT}/triton/tools/compile.py \\\n    fmha_triton.py \\\n    -n fused_attention_kernel \\\n    -o aot/fp32/fmha_kernel_d64_fp32 \\\n    --out-name fmha_d64_fp32 \\\n    -w 4 \\\n    -ns 2 \\\n    -s \"*fp32:16, *fp32:16, *fp32:16, *fp32:16, *fp32:16, *fp32:16, fp32, i32, i32, i32, 64, 64, 64\" \\\n    -g \"(seq_len + 63) / 64, batch_size * num_heads, 1\"\n```\n\n----------------------------------------\n\nTITLE: Running Inference with C++ Runtime and Python Binding\nDESCRIPTION: This snippet shows how to run inference using the Python binding of the C++ runtime with inflight batching. It specifies the engine directory, tokenizer directory, and input text for translation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython3 ../run.py --engine_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION} --tokenizer_dir tmp/hf_models/${MODEL_NAME} --max_output_len 64 --num_beams=1 --input_text \"translate English to German: The house is wonderful.\"\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-7B-Chat (FP16)\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM checkpoints and build a single-GPU FP16 engine for Qwen-7B-Chat.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                              --output_dir ./tllm_checkpoint_1gpu_fp16 \\\n                              --dtype float16\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_fp16 \\\n            --output_dir ./tmp/qwen/7B/trt_engines/fp16/1-gpu \\\n            --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Downloading GPT-NeoX Weights from HuggingFace\nDESCRIPTION: Clones the GPT-NeoX model repository from HuggingFace to obtain the weights and configuration files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/EleutherAI/gpt-neox-20b gptneox_model\n```\n\n----------------------------------------\n\nTITLE: Running Inference with TensorRT-LLM on Various BLOOM Configurations\nDESCRIPTION: Commands to run text summarization using different BLOOM model configurations, including single-GPU, multi-GPU, and various quantization methods.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./bloom/560M/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./bloom/560M/trt_engines/fp16/1-gpu/\n\npython ../../../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./bloom/560M/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./bloom/560M/trt_engines/int8_weight_only/1-gpu/\n\nmpirun -n 2 --allow-run-as-root \\\n    python ../../../summarize.py --test_trt_llm \\\n                           --hf_model_dir ./bloom/560M/ \\\n                           --data_type fp16 \\\n                           --engine_dir ./bloom/560M/trt_engines/fp16/2-gpu/\n\nmpirun -n 8 --allow-run-as-root \\\n    python ../../../summarize.py --test_trt_llm \\\n                           --hf_model_dir ./bloom/176B/ \\\n                           --data_type fp16 \\\n                           --engine_dir ./bloom/176B/trt_engines/fp16/8-gpu/\n\npython ../../../summarize.py --test_trt_llm \\\n                       --hf_model_dir /home/scratch.trt_llm_data/llm-models/bloom-3b \\\n                       --data_type fp16 \\\n                       --engine_dir /tmp/bloom/3b/trt_engines/fp8/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building Mixtral 8x7B with Pipeline Parallelism\nDESCRIPTION: Commands to convert the Mixtral 8x7B checkpoint and build TensorRT engines with pipeline parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py --model_dir ./Mixtral-8x7B-v0.1 \\\n                             --output_dir ./tllm_checkpoint_mixtral_2gpu \\\n                             --dtype float16 \\\n                             --pp_size 2\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_2gpu \\\n                 --output_dir ./trt_engines/mixtral/pp2 \\\n                 --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Creating Backend Configuration for DeepSeek-V3\nDESCRIPTION: Bash commands to create configuration files for enabling attention data-parallelism and overlap scheduler for the TensorRT-LLM backend.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\necho -e \"enable_attention_dp: true\\npytorch_backend_config:\\n  enable_overlap_scheduler: true\" > extra-llm-api-config.yml\n```\n\nLANGUAGE: bash\nCODE:\n```\necho -e \"enable_attention_dp: false\\npytorch_backend_config:\\n  enable_overlap_scheduler: true\\n  use_cuda_graph: true\\n  cuda_graph_max_batch_size: 128\" > extra-llm-api-config.yml\n```\n\n----------------------------------------\n\nTITLE: Converting GPT-NeoX Weights to TensorRT-LLM Format (Single GPU)\nDESCRIPTION: Converts the HuggingFace weights to TensorRT-LLM format for a single GPU setup using float16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./gptneox_model \\\n                              --dtype float16 \\\n                              --output_dir ./gptneox/20B/trt_ckpt/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Converting Skywork Model to TensorRT-LLM Checkpoint (BF16)\nDESCRIPTION: Converts the Hugging Face Skywork model to a TensorRT-LLM checkpoint in BF16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./Skywork-13B-base \\\n                --dtype bfloat16 \\\n                --output_dir ./skywork-13b-base/trt_ckpt/bf16\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines\nDESCRIPTION: Commands to build TensorRT-LLM engines from converted checkpoints with various configurations\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mamba/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./mamba_model/mamba-2.8b/trt_ckpt/bf16/1-gpu/ \\\n             --paged_kv_cache disable \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 924 \\\n             --max_seq_len 1024 \\\n             --output_dir ./mamba_model/mamba-2.8b/trt_engines/bf16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Cloning Skywork 13B Base Model from Hugging Face\nDESCRIPTION: Downloads the Skywork 13B base model from Hugging Face repository using git clone.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/Skywork/Skywork-13B-base\n```\n\n----------------------------------------\n\nTITLE: FP8 Quantization for Deepseek-v1 Model\nDESCRIPTION: Commands to perform FP8 quantization on the Deepseek-v1 model and build a quantized TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v1/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir deepseek_moe_16b \\\n        --dtype float16 \\\n        --qformat fp8 \\\n        --kv_cache_dtype fp8 \\\n        --output_dir trt_ckpt/deepseek_moe_16b/fp8/1-gpu \\\n        --calib_size 512\n\ntrtllm-build --checkpoint_dir ./trt_ckpt/deepseek_moe_16b/fp8/1-gpu \\\n             --gemm_plugin float16 \\\n             --gpt_attention_plugin bfloat16 \\\n             --output_dir ./trt_engines/fp8/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine\nDESCRIPTION: Build TensorRT engine from converted checkpoints with specified parameters\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir=./ckpts/mpt-7b/fp16 \\\n             --max_batch_size 32 \\\n             --max_input_len 1024 \\\n             --max_seq_len 1536 \\\n             --gemm_plugin float16 \\\n             --workers 1 \\\n             --output_dir ./trt_engines/mpt-7b/fp16\n```\n\n----------------------------------------\n\nTITLE: Converting and Building STDiT TensorRT Engine\nDESCRIPTION: Commands to convert a pre-trained STDiT checkpoint to TensorRT-LLM format and build optimized TensorRT engines with various performance settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/stdit/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Convert to TRT-LLM\npython convert_checkpoint.py --timm_ckpt=<pretrained_checkpoint>\n# Build engine\ntrtllm-build --checkpoint_dir=tllm_checkpoint/ \\\n             --max_batch_size=2 \\\n             --gemm_plugin=float16 \\\n             --kv_cache_type=disabled \\\n             --remove_input_padding=enable \\\n             --gpt_attention_plugin=auto \\\n             --bert_attention_plugin=auto \\\n             --context_fmha=enable\n```\n\n----------------------------------------\n\nTITLE: Running INT8 KV Cache Inference for Gemma 2B-IT Model Using JAX Checkpoint\nDESCRIPTION: This script demonstrates INT8 KV cache quantization for Gemma 2B-IT using a JAX checkpoint. It quantizes only the key-value cache to INT8 while keeping weights in BFloat16, requiring calibration for KV cache. The process includes cloning, converting with KV cache calibration, building the engine, and evaluation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-2b-it-flax\nCKPT_PATH=gemma-2b-it-flax/2b-it/\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_2b_it_tensorrt_llm/int8kv/tp1\nENGINE_PATH=/tmp/gemma/2B/int8kv/1-gpu/\nVOCAB_FILE_PATH=gemma-2b-it-flax/tokenizer.model\n\npython3 ./convert_checkpoint.py \\\n             --ckpt-type jax \\\n             --model-dir ${CKPT_PATH} \\\n             --world-size 1 \\\n             --dtype bfloat16 \\\n             --calibrate_kv_cache \\\n             --tokenizer_dir ${VOCAB_FILE_PATH} \\\n             --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 32 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n\n[02/08/2024-04:52:22] [TRT-LLM] [I] TensorRT-LLM (total latency: 3.5348474979400635 sec)\n[02/08/2024-04:52:22] [TRT-LLM] [I] TensorRT-LLM (total output tokens: 1819)\n[02/08/2024-04:52:22] [TRT-LLM] [I] TensorRT-LLM (tokens per second: 514.5907994786265)\n[02/08/2024-04:52:22] [TRT-LLM] [I] TensorRT-LLM beam 0 result\n[02/08/2024-04:52:22] [TRT-LLM] [I]   rouge1 : 24.0397941580232\n[02/08/2024-04:52:22] [TRT-LLM] [I]   rouge2 : 7.325311340360227\n[02/08/2024-04:52:22] [TRT-LLM] [I]   rougeL : 17.54210044633271\n[02/08/2024-04:52:22] [TRT-LLM] [I]   rougeLsum : 20.627861723682177\n```\n\n----------------------------------------\n\nTITLE: Building DBRX with Expert Parallelism for DbrxExperts Layer\nDESCRIPTION: Converts DBRX using expert parallelism (8-way) specifically for DbrxExperts layer while using tensor parallelism for the rest of the model, then builds TensorRT engines with this configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Build DBRX with expert parallelism for DbrxExperts layer and tensor parallelism for rest\npython convert_checkpoint.py --model_dir dbrx-base \\\n        --dtype bfloat16 \\\n        --tp_size 8 \\\n        --moe_tp_size 1 \\\n        --moe_ep_size 8 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_ckpt/bf16/ep8\n\ntrtllm-build --checkpoint_dir dbrx/trt_ckpt/bf16/ep8 \\\n        --gpt_attention_plugin bfloat16 \\\n        --gemm_plugin bfloat16 \\\n        --moe_plugin bfloat16 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_engines/bf16/ep8\n```\n\n----------------------------------------\n\nTITLE: Converting Checkpoint for Lookahead Decoding with Vicuna-7b\nDESCRIPTION: Bash command for converting a Vicuna-7b v1.3 checkpoint to be used with Lookahead decoding in TensorRT-LLM, specifying model directory, output directory, and precision settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/lookahead/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_DIR=/path/to/vicuna-7b-v1.3\nENGINE_DIR=tmp/engine\nCKPT_DIR=tmp/engine/ckpt\n\npython3 examples/llama/convert_checkpoint.py    \\\n    --model_dir=$MODEL_DIR                      \\\n    --output_dir=$CKPT_DIR                      \\\n    --dtype=float16                             \\\n    --tp_size=1                                 \\\n    --pp_size=1\n```\n\n----------------------------------------\n\nTITLE: Converting Checkpoint with Per-Token Per-Channel SmoothQuant\nDESCRIPTION: Command to convert a GPT-2 model checkpoint with per-token per-channel SmoothQuant quantization for improved accuracy at the cost of slightly increased latency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --smoothquant 0.5 \\\n        --per_token \\\n        --per_channel \\\n        --output_dir gpt2/trt_ckpt/int8-sq-ptpc/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine from ONNX\nDESCRIPTION: Command to build TensorRT engine from existing ONNX file\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 vit_onnx_trt.py --pretrained_model_path ./Qwen-VL-Chat --only_trt\n```\n\n----------------------------------------\n\nTITLE: Converting JAX Checkpoint to TensorRT-LLM Format\nDESCRIPTION: Script to convert JAX checkpoint to TensorRT-LLM unified format with bfloat16 precision\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCKPT_PATH=/tmp/models/gemma_nv/checkpoints/tmp_2b_it\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_2b_it_tensorrt_llm/bf16/tp1/\n\npython3 ./convert_checkpoint.py \\\n    --ckpt-type jax \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype bfloat16 \\\n    --world-size 1 \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Downloading Deepseek-v2 Model from Hugging Face\nDESCRIPTION: Commands to download the Deepseek-v2 model weights from Hugging Face repository using Git LFS.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/deepseek-ai/DeepSeek-V2\n```\n\n----------------------------------------\n\nTITLE: INT8 KV Cache with AWQ Quantization\nDESCRIPTION: Enables INT8 KV cache with AWQ (per-group INT4 weight-only quantization). Uses float16 dtype and calibrates on 512 samples.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir baichuan-inc/Baichuan-13B-Chat \\\n                                   --dtype float16 \\\n                                   --qformat int4_awq \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ./trt_ckpt/baichuan_int4awq_int8kv_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Registering MLP Output Tensor in GPT Model for TensorRT-LLM\nDESCRIPTION: This code shows how to register the MLP output tensor as a network output in the GPT model implementation of TensorRT-LLM for debugging purposes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n        hidden_states = residual + attention_output.data\n\n        residual = hidden_states\n        hidden_states = self.post_layernorm(hidden_states)\n\n        hidden_states = self.mlp(hidden_states)\n        # Register as model output\n        # ------------------------------------------------------\n        self.register_network_output('mlp_output', hidden_states)\n        # ------------------------------------------------------\n\n        hidden_states = residual + hidden_states\n```\n\n----------------------------------------\n\nTITLE: Running Debug Example with TensorRT-LLM\nDESCRIPTION: Command for running the debug example that allows defining which engine IO tensors should be kept or dumped to numpy files. This helps with debugging the TensorRT engine's behavior.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/bindings\npython3 example_debug.py --model_path=../llama/tmp/7B/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Markdown Performance Data Table\nDESCRIPTION: Markdown table containing throughput measurements in tokens/second for LLaMA models across different GPUs and configurations. Shows performance variations based on model size, tensor parallelism, and input/output sequence lengths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Throughput (tokens / sec)   |                            | GPU                          | H200 141GB HBM3   | H100 80GB HBM3   | GH200 480GB   | L40S    | A100-SXM4-80GB   |\n|:----------------------------|:---------------------------|:-----------------------------|:------------------|:-----------------|:--------------|:--------|:-----------------|\n|                             | Precision                  |                              | FP8               | FP8              | FP8           | FP8     | FP16             |\n| Model                       | Tensor Model Parallel Size | Runtime Input/Output Lengths |                   |                  |               |         |                  |\n| LLaMA v3.1 8B               | 1                          | 128, 128                     | 29526.04          | 28836.77         | 29852.96      | 9104.61 | 6627.27          |\n```\n\n----------------------------------------\n\nTITLE: FP8 Post-Training Quantization for Falcon-180B\nDESCRIPTION: Series of commands to quantize Falcon-180B to FP8 precision using NVIDIA Modelopt, build TensorRT-LLM engines, and run inference with the quantized model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# Quantize HF Falcon 180B checkpoint into FP8 and export trtllm checkpoint\npython ../../../quantization/quantize.py --model_dir ./falcon/180b \\\n                --dtype float16 \\\n                --qformat fp8 \\\n                --kv_cache_dtype fp8 \\\n                --output_dir ./falcon/180b/trt_ckpt/fp8/tp8-pp1 \\\n                --tp_size 8\n\n# Build trtllm engines from the trtllm checkpoint\ntrtllm-build --checkpoint_dir ./falcon/180b/trt_ckpt/fp8/tp8-pp1 \\\n                --gemm_plugin float16 \\\n                --output_dir ./falcon/180b/trt_engines/fp8/tp8-pp1 \\\n                --workers 8\n\n# Run the summarization task\nmpirun -n 8 --allow-run-as-root --oversubscribe \\\n    python ../../../summarize.py --test_trt_llm \\\n                --hf_model_dir ./falcon/180b \\\n                --engine_dir ./falcon/180b/trt_engines/fp8/tp8-pp1\n```\n\n----------------------------------------\n\nTITLE: Downloading Mixtral 8x7B Weights from Hugging Face\nDESCRIPTION: Commands to download the Mixtral 8x7B weights from Hugging Face using Git LFS.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Converted InternLM Models\nDESCRIPTION: These commands demonstrate how to run inference using the converted InternLM 7B and 20B models. They use the run.py script to generate responses based on a given input text.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm-chat-7b/ \\\n                 --engine_dir ./internlm-chat-7b/smooth_internlm/sq0.5/\n\npython ../../../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm-chat-20b/ \\\n                 --engine_dir ./internlm-chat-20b/smooth_internlm/sq0.5/\n```\n\n----------------------------------------\n\nTITLE: Converting InternLM2 20B with INT8 Weight-Only Quantization\nDESCRIPTION: Converts the larger InternLM2 20B model to TensorRT-LLM format with INT8 weight-only quantization for memory efficiency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm2-chat-20b  \\\n                            --output_dir ./internlm2-chat-20b/w8a16 \\\n                             --dtype float16  \\\n                             --use_weight_only \\\n                             --weight_only_precision int8\n```\n\n----------------------------------------\n\nTITLE: Generating TRT-LLM LLaMA Engine\nDESCRIPTION: Commands to convert and build the LLaMA language model component into a TensorRT-LLM engine with specific attention and performance configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/vit/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py \\\n    --model_dir tmp/hf_models/${MODEL_NAME} \\\n    --output_dir tmp/trt_models/${MODEL_NAME}/fp16/1-gpu/llm \\\n    --dtype float16\n\ntrtllm-build \\\n    --checkpoint_dir tmp/trt_models/${MODEL_NAME}/fp16/1-gpu/llm \\\n    --output_dir tmp/trt_engines/${MODEL_NAME}/fp16/1-gpu/llm \\\n    --gpt_attention_plugin float16 \\\n    --gemm_plugin float16 \\\n    --use_fused_mlp=enable \\\n    --max_batch_size 8 \\\n    --max_input_len 4096 \\\n    --max_seq_len 5120 \\\n    --max_num_tokens 32768 \\\n    --max_multimodal_len 32768\n```\n\n----------------------------------------\n\nTITLE: Running Inference with INT8 Weight-Only InternLM2 7B\nDESCRIPTION: Executes inference using the InternLM2 7B model with INT8 weight-only quantization on a single GPU for memory efficiency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm2-chat-7b/ \\\n                 --engine_dir=./internlm2-chat-7b/trt_engines/weight_only/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Converting Deepseek-v2 Model to TensorRT-LLM Format\nDESCRIPTION: Commands to convert Deepseek-v2 Hugging Face weights to TensorRT-LLM checkpoint format with BF16 precision and 8-GPU tensor parallelism. Includes both standard and CPU loading options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./DeepSeek-V2 \\\n                            --output_dir ./trtllm_checkpoint_deepseek_v2_8gpu_bf16 \\\n                            --dtype bfloat16 \\\n                            --tp_size 8\n\n# With '--load_model_on_cpu' option if total GPU memory is insufficient\npython convert_checkpoint.py --model_dir ./DeepSeek-V2 \\\n                            --output_dir ./trtllm_checkpoint_deepseek_v2_cpu_bf16 \\\n                            --dtype bfloat16 \\\n                            --tp_size 8 \\\n                            --load_model_on_cpu\n```\n\n----------------------------------------\n\nTITLE: INT8 KV Cache with Per-Channel Weight-Only Quantization\nDESCRIPTION: Combines INT8 KV cache with per-channel weight-only quantization using int4_wo format. Uses float16 dtype and calibrates on 512 samples.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir baichuan-inc/Baichuan-13B-Chat \\\n                                   --dtype float16 \\\n                                   --qformat int4_wo \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ./trt_ckpt/baichuan_int4wo_int8kv_tp1 \\\n                                   --calib_size 512 \\\n```\n\n----------------------------------------\n\nTITLE: Building Engine with Per-Tensor SmoothQuant\nDESCRIPTION: Command to build TensorRT-LLM engines from a checkpoint with per-tensor SmoothQuant quantization. Uses the SmoothQuant-processed checkpoint to create optimized engines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/int8-sq/1-gpu \\\n        --output_dir gpt2/trt_engines/int8-sq/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT engine for Encoder model\nDESCRIPTION: Command to build a TensorRT engine for the Encoder component of an Encoder-Decoder model. Configuration includes plugin settings, batch size, input length, and various optimization parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# --gpt_attention_plugin is necessary in Enc-Dec.\n# Try --gemm_plugin to prevent accuracy issue.\n# It is recommended to use --remove_input_padding along with --gpt_attention_plugin for better performance\ntrtllm-build --checkpoint_dir tmp/trt_models/${MODEL_NAME}/${INFERENCE_PRECISION}/encoder \\\n                --output_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION}/encoder \\\n                --paged_kv_cache disable \\\n                --moe_plugin disable \\\n                --max_beam_width ${MAX_BEAM_WIDTH} \\\n                --max_batch_size 8 \\\n                --max_input_len 1024 \\\n                --gemm_plugin ${INFERENCE_PRECISION} \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding enable \\\n                --context_fmha disable\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages and Preparing HuggingFace BLOOM Checkpoint\nDESCRIPTION: Commands to install dependencies and download BLOOM model weights from HuggingFace using git-lfs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n\n# Setup git-lfs\ngit lfs install\nrm -rf ./bloom/560M\nmkdir -p ./bloom/560M && git clone https://huggingface.co/bigscience/bloom-560m ./bloom/560M\n```\n\n----------------------------------------\n\nTITLE: Building 8-way Tensor Parallel Engine with Shared Embedding Table\nDESCRIPTION: Commands to build BLOOM-176B with shared embedding table between embedding() and lm_head() layers to reduce engine size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./bloom/176B/ \\\n                --dtype float16 \\\n                --output_dir ./bloom/176B/trt_ckpt/fp16/8-gpu/ \\\n                --tp_size 8 \\\n                --use_parallel_embedding \\\n                --embedding_sharding_dim 0\ntrtllm-build --checkpoint_dir ./bloom/176B/trt_ckpt/fp16/8-gpu/ \\\n                --output_dir ./bloom/176B/trt_engines/fp16/8-gpu/ \\\n                --workers 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Tensor and Pipeline Parallelism in TensorRT-LLM\nDESCRIPTION: This snippet demonstrates how to instantiate an LLM object with specific tensor_parallel_size and pipeline_parallel_size parameters. In this example, the model is sharded across 2 nodes with 16 GPUs, using a tensor parallel size of 8 and a pipeline parallel size of 2.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/deciding-model-sharding-strategy.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    llm = LLM(\n        model=\"/scratch/Llama-3.1-405B-Instruct\",\n        tensor_parallel_size=8,\n        pipeline_parallel_size=2\n    )\n```\n\n----------------------------------------\n\nTITLE: Evaluating TensorRT-LLM Engine with LM-Eval-Harness\nDESCRIPTION: This command runs the evaluation script for a TensorRT-LLM engine using the LM-Eval-Harness framework. It specifies the model type, tokenizer, engine directory, chunk size, and tasks to evaluate.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-eval/lm-eval-harness/README.md#2025-04-07_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython lm_eval_tensorrt_llm.py --model trt-llm \\\n    --model_args tokenizer=<HF model folder>,model=<TRT LLM engine dir>,chunk_size=<int> \\\n    --tasks <comma separated tasks, e.g., gsm8k-cot, mmlu>\n```\n\n----------------------------------------\n\nTITLE: Loading BLOOM Weights with Custom Preprocessing in Python\nDESCRIPTION: This code demonstrates how to load BLOOM weights using the ModelWeightsLoader. It applies custom preprocessing for 'qkv' weights and updates the TensorRT-LLM model with the loaded weights.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nloader.update_key_mapping(trtllm_model)\ntllm_weights = {}\nfor tllm_key, _ in tqdm(trtllm_model.named_parameters()):\n    if tllm_key.endswith(\"qkv\"):\n        # Passing the callable handle\n        tllm_weights.update(loader.load(tllm_key, preprocess=customized_preprocess))\n    else:\n        tllm_weights.update(loader.load(tllm_key))\nloader.fill(tllm_weights)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with SSH Support\nDESCRIPTION: Command to build a new Docker image based on the SSH-enabled Dockerfile. This creates an image named 'tensorrt_llm/devel:with_ssh'.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/dev-on-cloud/build-image-to-dockerhub.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t tensorrt_llm/devel:with_ssh -f Dockerfile.ssh .\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT-LLM with AutoDeploy\nDESCRIPTION: Commands to install TensorRT-LLM and its dependencies via pip, which includes AutoDeploy functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get -y install libopenmpi-dev && pip3 install --upgrade pip setuptools && pip3 install tensorrt_llm\n```\n\n----------------------------------------\n\nTITLE: Converting DiT Model to TensorRT-LLM Format (FP8)\nDESCRIPTION: Commands to convert DiT model to TensorRT-LLM format using float8 precision and build the engine with FP8 quantization\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dit/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --fp8_linear --timm_ckpt=</path/to/quantized_ckpt> --output_dir=tllm_checkpoint_fp8\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_fp8/ \\\n             --output_dir ./engine_outputs_fp8/ \\\n             --max_batch_size 8 \\\n             --remove_input_padding disable \\\n             --bert_attention_plugin disable\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Inference\nDESCRIPTION: Command to run inference using tensor parallelism across multiple GPUs\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 \\\n    python ../run.py --input_text \"What's new between ChatGLM3-6B and ChatGLM2-6B?\" \\\n        --max_output_len 50 \\\n        --tokenizer_dir glm_4_9b \\\n        --engine_dir trt_engines/glm_4_9b/fp16/2-gpu\n```\n\n----------------------------------------\n\nTITLE: Running InternLM Models with Various Configurations\nDESCRIPTION: These commands demonstrate how to run InternLM models with different precisions (fp16, bf16, int8) and parallelism strategies. They use the run.py script to generate responses based on a given input text.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# InternLM 7B with fp16\npython ../../../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm-chat-7b/ \\\n                 --engine_dir=./internlm-chat-7b/trt_engines/fp16/1-gpu/\n\n# InternLM 7B with bf16\npython ../../../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm-chat-7b/ \\\n                 --engine_dir=./internlm-chat-7b/trt_engines/bf16/1-gpu/\n\n# InternLM 7B with int8 weight only quantization\npython ../../../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm-chat-7b/ \\\n                 --engine_dir=./internlm-chat-7b/trt_engines/weight_only/1-gpu/\n\n# InternLM 7B with fp16 and tensor parallelism\nmpirun -n 2 --allow-run-as-root \\\n    python ../../../run.py --max_output_len=120 \\\n                     --input_text 'Tell me about yourself.' \\\n                     --tokenizer_dir ./internlm-chat-7b/ \\\n                     --engine_dir=./internlm-chat-7b/trt_engines/fp16/2-gpu/\n\n# InternLM 20B with fp16 and tensor parallelism and pipeline parallelism\nmpirun -n 4 --allow-run-as-root \\\n    python ../../../run.py --max_output_len=120 \\\n                     --input_text 'Tell me about yourself.' \\\n                     --tokenizer_dir ./internlm-chat-7b/ \\\n                     --engine_dir=./internlm-chat-7b/trt_engines/bf16/4-gpu/\n```\n\n----------------------------------------\n\nTITLE: Running Language-Adapter Model with C++ Runtime\nDESCRIPTION: Example command for running a language adapter model using the C++ runtime interface. It demonstrates how to pass language_task_uids for different translation tasks (French and Spanish) within a single session.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/language_adapter/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# translate 2 sentence, 1 to France (language_task_uid=3) 1 to Spanish (language_task_uid=2).\n# language_task_uids = [3, 2]\n\nTEXT=\"Where is the nearest restaurant? Wikipedia is a free online encyclopedia written and maintained by a community of volunteers (called Wikis) through open collaboration and the use of MediaWiki, a wiki-based editing system.\"\n\npython3 ../run.py --engine_dir $ENGINE_DIR --tokenizer_type \"language_adapter\" --max_input_length 512 --max_output_len 512 --num_beams 1 --input_file input_ids.npy --tokenizer_dir $MODEL_DIR --language_task_uids 3 2\n```\n\n----------------------------------------\n\nTITLE: Building DBRX with Float16 and 8-way Tensor Parallelism\nDESCRIPTION: Converts DBRX model weights to TensorRT-LLM checkpoint format using Float16 precision and 8-way tensor parallelism, then builds TensorRT engines with appropriate plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# 8-way tensor parallelism, dtype float16\npython convert_checkpoint.py --model_dir dbrx-base \\\n        --dtype float16 \\\n        --tp_size 8 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_ckpt/fp16/tp8\n\ntrtllm-build --checkpoint_dir dbrx/trt_ckpt/fp16/tp8 \\\n        --gpt_attention_plugin float16 \\\n        --gemm_plugin float16 \\\n        --moe_plugin float16 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_engines/fp16/tp8\n```\n\n----------------------------------------\n\nTITLE: Quantizing Baichuan 13B to INT4 AWQ with NVIDIA Modelopt\nDESCRIPTION: Python command to quantize a Hugging Face Baichuan 13B model to INT4 precision using AWQ (Activation-aware Weight Quantization) with NVIDIA Modelopt toolkit.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir /code/model/Baichuan2-13B-Chat/ \\\n                                   --dtype float16 \\\n                                   --qformat int4_awq \\\n                                   --output_dir ./quantized_int4-awq_gs128 \\\n                                   --calib_size 32\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Jais-30b-chat-v3 (2-way Tensor Parallelism)\nDESCRIPTION: This command builds a 2-way tensor parallelism TensorRT engine from the TensorRT-LLM checkpoint for Jais-30b-chat-v3, with GPT Attention plugin and input padding removal enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir jais-30b-chat-v3/trt_ckpt/fp16/2-gpu \\\n        --gpt_attention_plugin float16 \\\n        --remove_input_padding enable \\\n        --output_dir jais-30b-chat-v3/trt_engines/fp16/2-gpu\n```\n\n----------------------------------------\n\nTITLE: Running Single-GPU Inference\nDESCRIPTION: Commands to run inference on various models using a single GPU with options for streaming output.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --max_output_len 50 \\\n        --tokenizer_dir command_r_v01 \\\n        --engine_dir trt_engines/command_r_v01/fp16/1-gpu\n\npython3 ../run.py --max_output_len 50 \\\n        --tokenizer_dir command_r_v01 \\\n        --engine_dir trt_engines/command_r_v01/fp16/1-gpu \\\n        --streaming\n\npython3 ../run.py --max_output_len 50 \\\n        --tokenizer_dir aya_23_8B \\\n        --engine_dir trt_engines/aya_23_8B/fp16/1-gpu\n\npython3 ../run.py --max_output_len 50 \\\n        --tokenizer_dir aya_23_35B \\\n        --engine_dir trt_engines/aya_23_35B/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Building 2-GPU Tensor Parallel EXAONE Engine\nDESCRIPTION: Commands to convert and build EXAONE with 2-way tensor parallelism. This distributes the model across 2 GPUs using tensor parallelism to handle larger models or increase throughput.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py \\\n    --model_dir $HF_MODEL_DIR \\\n    --output_dir trt_models/exaone/fp16/2-gpu \\\n    --tp_size 2 \\\n    --dtype float16\n\ntrtllm-build \\\n    --checkpoint_dir trt_models/exaone/fp16/2-gpu \\\n    --output_dir trt_engines/exaone/fp16/2-gpu \\\n    --gemm_plugin auto\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for RecurrentGemma\nDESCRIPTION: This command installs the necessary dependencies and sets up git-lfs which is required for downloading model checkpoints.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\ngit lfs install\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Command to install required Python packages for the conversion process.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Downloading ChatGLM Models\nDESCRIPTION: Script to install required packages and clone ChatGLM model repositories from HuggingFace. Includes installation of git-lfs for handling large files and cloning both ChatGLM2-6B and ChatGLM2-6B-32k models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/chatglm2-6b/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\napt-get update\napt-get install git-lfs\nrm -rf chatglm*\n\n# clone one or more models we want to build\ngit clone https://huggingface.co/THUDM/chatglm2-6b      chatglm2_6b\ngit clone https://huggingface.co/THUDM/chatglm2-6b-32k  chatglm2_6b_32k\n```\n\n----------------------------------------\n\nTITLE: Running Weight-Only INT8 Inference for Keras Checkpoint\nDESCRIPTION: Commands to run Gemma model with INT8 weight-only quantization using Keras checkpoint. Includes repository cloning, checkpoint conversion, engine building and inference testing steps.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-7b-it-keras\nGIT_LFS_SKIP_SMUDGE=1 git clone git@hf.co:google/gemma-7b-it-flax\ncd gemma-7b-it-flax\ngit lfs pull -I tokenizer.model\n\nCKPT_PATH=gemma-7b-it-keras\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_7b_it_tensorrt_llm/w8_a16/tp1/\nENGINE_PATH=/tmp/gemma/7B/w8_a16/1-gpu/\nVOCAB_FILE_PATH=gemma-7b-it-flax/tokenizer.model\n\npython3 ./convert_checkpoint.py \\\n    --ckpt-type keras \\\n    --model-dir ${CKPT_PATH} \\\n    --use-weight-only-with-precision int8 \\\n    --dtype bfloat16 \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n                 --gemm_plugin auto \\\n                 --max_batch_size 32 \\\n                 --max_input_len 3000 \\\n                 --max_seq_len 3100 \\\n                 --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n```\n\n----------------------------------------\n\nTITLE: Running Quickstart Demo\nDESCRIPTION: Verification command to run the AutoDeploy quickstart demo with TinyLlama model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/auto_deploy\npython build_and_run_ad.py --config '{\"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"}'\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for TensorRT-LLM Examples\nDESCRIPTION: Command to install the required dependencies for running the examples. This needs to be executed before running either the chat or server application.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/apps/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r ./requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Converting GPT-J Checkpoint Commands\nDESCRIPTION: Python commands to convert Hugging Face checkpoint to TensorRT-LLM format in both float16 and int8 weight-only formats.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/sample_weight_stripping/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./gpt-j-6b \\\n                                     --dtype float16 \\\n                                     --output_dir ./trt_ckpt/gptj_fp16_tp1/\n\npython convert_checkpoint.py --model_dir ./gpt-j-6b \\\n                                     --dtype float16 \\\n                                     --use_weight_only \\\n                                     --weight_only_precision int8 \\\n                                     --output_dir ./trt_ckpt/gptj_int8_tp1/\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Deepseek-v1\nDESCRIPTION: Commands to convert the Deepseek-v1 checkpoint to TensorRT-LLM format and build a bfloat16 TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v1/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./deepseek_moe_16b/ \\\n                            --output_dir ./trtllm_checkpoint_deepseek_v1_1gpu_bf16 \\\n                            --dtype bfloat16 \\\n                            --tp_size 1\ntrtllm-build --checkpoint_dir ./trtllm_checkpoint_deepseek_v1_1gpu_bf16 \\\n            --output_dir ./trtllm_engines/deepseek_v1/bf16/tp1 \\\n            --gpt_attention_plugin bfloat16 \\\n            --gemm_plugin bfloat16 \\\n            --moe_plugin bfloat16\n```\n\n----------------------------------------\n\nTITLE: Running INT8 KV Cache Inference for Keras Checkpoint\nDESCRIPTION: Commands to run Gemma model with INT8 KV cache quantization using Keras checkpoint. Includes checkpoint conversion with KV cache calibration, engine building and inference testing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nCKPT_PATH=/tmp/models/gemma_keras/keras/gemma_7b_en/\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_7b_it_tensorrt_llm/int8kv/tp1\nENGINE_PATH=/tmp/gemma/7B/int8kv/1-gpu/\nVOCAB_FILE_PATH=/tmp/models/gemma_nv/checkpoints/tmp_vocab.model\n\npython3 ./convert_checkpoint.py \\\n             --ckpt-type keras \\\n             --model-dir ${CKPT_PATH} \\\n             --world-size 1 \\\n             --dtype bfloat16 \\\n             --calibrate_kv_cache \\\n             --tokenizer_dir ${VOCAB_FILE_PATH} \\\n             --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 32 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n```\n\n----------------------------------------\n\nTITLE: Building Single-GPU FP16 InternLM 7B Engine\nDESCRIPTION: Commands to convert and build InternLM 7B model using FP16 precision on a single GPU\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/llama\n\npython convert_checkpoint.py --model_dir ./internlm-chat-7b/ \\\n                --dtype float16 \\\n                --output_dir ./internlm-chat-7b/trt_engines/fp16/1-gpu/\n\ntrtllm-build --checkpoint_dir ./internlm-chat-7b/trt_engines/fp16/1-gpu/ \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Running EAGLE Typical Acceptance Decoding with Single GPU\nDESCRIPTION: Command to run inference with EAGLE typical acceptance decoding using a Vicuna model on a single GPU, setting temperature and posterior threshold parameters for non-greedy sampling.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir ./tmp/eagle/7B/trt_engines/fp16/1-gpu/ \\\n                 --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                 --max_output_len=100 \\\n                 --eagle_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                 --input_text \"Once upon\" \\\n                 --temperature 0.7 \\\n                 --eagle_posterior_threshold 0.09\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for INT4-AWQ Quantized Model\nDESCRIPTION: Command to build a TensorRT engine from an INT4-AWQ quantized checkpoint. It creates an optimized engine for 4-bit inference with activation awareness.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./quantized_int4-awq \\\n                 --output_dir ./tmp/qwen/7B/trt_engines/int4_AWQ/1-gpu/ \\\n                 --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Node Docker Containers for DeepSeek-V3\nDESCRIPTION: Docker commands to set up containers on multiple nodes for running DeepSeek-V3 in a distributed environment, mounting necessary directories and configuring network settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# use host network\nIMAGE=<YOUR_IMAGE>\nNAME=test_2node_docker\n# host1\ndocker run -it --name ${NAME}_host1 --ipc=host --gpus=all --network host --privileged --ulimit memlock=-1 --ulimit stack=67108864 -v ${PWD}:/workspace -v <YOUR_MODEL_DIR>:/models/DeepSeek-V3 -w /workspace ${IMAGE}\n# host2\ndocker run -it --name ${NAME}_host2 --ipc=host --gpus=all --network host --privileged --ulimit memlock=-1 --ulimit stack=67108864 -v ${PWD}:/workspace -v <YOUR_MODEL_DIR>:/models/DeepSeek-V3 -w /workspace ${IMAGE}\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Jais-13b-chat (English Input)\nDESCRIPTION: This command runs inference using the built Jais-13b-chat engine with English input text, specifying the tokenizer directory and maximum output length.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../../../run.py --engine_dir jais-13b-chat/trt_engines/fp16/1-gpu \\\n        --tokenizer_dir core42/jais-13b-chat \\\n        --max_output_len 10\n```\n\n----------------------------------------\n\nTITLE: Converting and Building EAGLE Model with 4-way Tensor Parallelism\nDESCRIPTION: Commands to convert a Vicuna model to support EAGLE decoding with 4-way tensor parallelism, followed by building the TensorRT engine for distributed inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./vicuna-7b-v1.3 \\\n                            --eagle_model_dir EAGLE-Vicuna-7B-v1.3 \\\n                            --output_dir ./tllm_checkpoint_4gpu_eagle \\\n                            --dtype float16 \\\n                            --max_draft_len 63 \\\n                            --num_eagle_layers 4 \\\n                            --max_non_leaves_per_layer 10 \\\n                            --tp_size 4 \\\n                            --workers 4\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_4gpu_eagle \\\n             --output_dir ./tmp/eagle/7B/trt_engines/fp16/4-gpu/ \\\n             --gemm_plugin float16 \\\n             --use_paged_context_fmha enable \\\n             --speculative_decoding_mode eagle \\\n             --max_batch_size 4\n```\n\n----------------------------------------\n\nTITLE: Building Draft and Target Engines for Llama Models in TensorRT-LLM\nDESCRIPTION: Bash commands for converting Llama-v2 models to TensorRT-LLM compatible format and building optimization engines for both draft and target models. The draft model is a smaller 7B parameter model, while the target is a larger 13B parameter model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/draft_target_model/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/llama\n\npython3 convert_checkpoint.py \\\n    --model_dir=<Path To Llama-v2-7B repo> \\\n    --output_dir=./ckpt-draft \\\n    --dtype=float16\n\npython3 convert_checkpoint.py \\\n    --model_dir=<Path To Llama-v2-13B repo> \\\n    --output_dir=./ckpt-target \\\n    --dtype=float16\n\ntrtllm-build \\\n    --checkpoint_dir ./ckpt-draft \\\n    --output_dir=./draft-engine \\\n    --gemm_plugin=float16 \\\n    --use_paged_context_fmha=enable \\\n    --max_batch_size=4 \\\n    --max_input_len=3200 \\\n    --max_seq_len=4800\n\ntrtllm-build \\\n    --checkpoint_dir=./ckpt-target \\\n    --output_dir=./target-engine \\\n    --gemm_plugin=float16 \\\n    --use_paged_context_fmha=enable \\\n    --speculative_decoding_mode=draft_tokens_external \\\n    --max_draft_len=10 \\\n    --max_batch_size=4 \\\n    --max_input_len=3200 \\\n    --max_seq_len=4800\n```\n\n----------------------------------------\n\nTITLE: Downloading MMLU Dataset for Model Evaluation\nDESCRIPTION: Commands to download and extract the MMLU (Massive Multitask Language Understanding) dataset for comprehensive model evaluation. This dataset is used to assess the model's capabilities across various knowledge domains.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Download MMLU dataset\nmkdir mmlu_data && cd mmlu_data\nwget https://people.eecs.berkeley.edu/~hendrycks/data.tar && tar -xf data.tar\n```\n\n----------------------------------------\n\nTITLE: Converting MPT-7B to FP16/FP32 Format\nDESCRIPTION: Convert HuggingFace MPT-7B model to TensorRT-LLM format in FP16 or FP32 precision with optional tensor parallelism\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/fp16/ --dtype float16\n\npython convert_checkpoint.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/fp32_tp4/ --dtype float32 --tp_size 4\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Weight Loading for Model\nDESCRIPTION: Example method implementation for custom weight loading in a model class, demonstrating how to handle discrepancies between model architecture and checkpoint formats.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/adding_new_model.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyModelForCausalLM(DecoderModelForCausalLM[MyModel, MyConfig]):\n\n    def load_weights(self, weights: dict):\n        # Define the weight loading logic\n        ...\n```\n\n----------------------------------------\n\nTITLE: Building FP8 Quantized EXAONE Engine\nDESCRIPTION: Commands to apply FP8 post-training quantization to EXAONE using NVIDIA Modelopt. This quantization enables faster computation on Hopper GPUs while maintaining close to full precision accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py \\\n    --model_dir $HF_MODEL_DIR \\\n    --dtype float16 \\\n    --qformat fp8 \\\n    --kv_cache_dtype fp8 \\\n    --output_dir trt_models/exaone/fp8/1-gpu \\\n\ntrtllm-build \\\n    --checkpoint_dir trt_models/exaone/fp8/1-gpu \\\n    --output_dir trt_engines/exaone/fp8/1-gpu \\\n    --gemm_plugin auto\n```\n\n----------------------------------------\n\nTITLE: Weight Conversion Implementation for TensorRT-LLM\nDESCRIPTION: Implements a class method for converting HuggingFace weights to TensorRT-LLM format, demonstrating how to load and bind external model weights to the new TensorRT-LLM model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/add-model.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyModelForCausalLM(DecoderModelForCausalLM):\n    @classmethod\n    def from_hugging_face(\n            cls,\n            hf_model_dir,\n            dtype='float16',\n            mapping: Optional[Mapping] = None) -> MyModelForCausalLM\n        # create a TensorRT-LLM MyModelForCausalLM model object\n        # convert HuggingFace checkpoint to TensorRT-LLM expected weights dict\n        # load the weights to MyModelForCausalLM object\n```\n\n----------------------------------------\n\nTITLE: Sending Requests with curl\nDESCRIPTION: Example curl command to send completion requests to the disaggregated server.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/disaggregated/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8000/v1/completions     -H \"Content-Type: application/json\"     -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"prompt\": \"NVIDIA is a great company because\",\n        \"max_tokens\": 16,\n        \"temperature\": 0\n    }' -w \"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Advanced Disaggregated Configuration\nDESCRIPTION: Detailed YAML configuration for MPI-based disaggregated serving, including tensor parallelism and KV cache settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/disaggregated/README.md#2025-04-07_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nhostname: localhost\nport: 8000\nmodel: TinyLlama/TinyLlama-1.1B-Chat-v1.0\nbackend: \"pytorch\"\npytorch_backend_config:\n  use_cuda_graph: False\n  enable_overlap_scheduler: False\ncontext_servers:\n  num_instances: 1\n  tensor_parallel_size: 1\n  pipeline_parallel_size: 1\n  kv_cache_config:\n    free_gpu_memory_fraction: 0.9\n  urls:\n      - \"localhost:8001\"\n      - \"localhost:8002\"\ngeneration_servers:\n  num_instances: 1\n  tensor_parallel_size: 1\n  pipeline_parallel_size: 1\n  urls:\n      - \"localhost:8003\"\n```\n\n----------------------------------------\n\nTITLE: Running the LLaVA-NeXT Model\nDESCRIPTION: Command to execute the LLaVA-NeXT model with specified parameters for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/vit/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../multimodal/run.py \\\n    --max_new_tokens 30 \\\n    --hf_model_dir tmp/hf_models/${MODEL_NAME} \\\n    --engine_dir tmp/trt_engines/${MODEL_NAME}/fp16/1-gpu \\\n    --input_text \"Question: which city is this? Answer:\"\n```\n\n----------------------------------------\n\nTITLE: Building DBRX with Combined Tensor and Pipeline Parallelism\nDESCRIPTION: Converts DBRX model using both tensor parallelism (4-way) and pipeline parallelism (2-way) with BFloat16 precision, then builds TensorRT engines with this hybrid parallelism strategy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# 4-way tensor parallelism and 2-way pipeline parallelism, dtype bfloat16\npython convert_checkpoint.py --model_dir dbrx-base \\\n        --dtype bfloat16 \\\n        --tp_size 4 \\\n        --pp_size 2 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_ckpt/bf16/tp4pp2\n\ntrtllm-build --checkpoint_dir dbrx/trt_ckpt/bf16/tp4pp2 \\\n        --gpt_attention_plugin bfloat16 \\\n        --gemm_plugin bfloat16 \\\n        --moe_plugin bfloat16 \\\n        --workers 8 \\\n        --output_dir dbrx/trt_engines/bf16/tp4pp2\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with Qwen-7B Quantized to INT8\nDESCRIPTION: Executes summarization using the Qwen-7B model quantized to INT8, specifying the quantized engine directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --test_trt_llm \\\n                       --hf_model_dir  ./tmp/Qwen/7B/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./tmp/Qwen/7B/trt_engines/int8_weight_only/1-gpu/ \\\n                       --max_input_length 2048 \\\n                       --output_len 2048\n```\n\n----------------------------------------\n\nTITLE: Building INT8 Weight-Only Quantized Engine\nDESCRIPTION: Commands to convert and build InternLM 7B model with INT8 weight-only quantization\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm-chat-7b/ \\\n                --dtype float16 \\\n                --output_dir ./internlm-chat-7b/trt_engines/int8/1-gpu/ \\\n                --use_weight_only \\\n                --weight_only_precision int8\n\ntrtllm-build --checkpoint_dir ./internlm-chat-7b/trt_engines/int8/1-gpu/ \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Setting Context Chunking Policy in TensorRT-LLM\nDESCRIPTION: This code example shows how to configure the context chunking policy to EQUAL_PROGRESS in TensorRT-LLM using the LLM-API. It includes setting up prompts, sampling parameters, and configuring the scheduler with the desired context chunking policy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-runtime-flags.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm import LLM, SamplingParams\nfrom tensorrt_llm.bindings.executor import SchedulerConfig, ContextChunkingPolicy\n\n\ndef main():\n    prompts = [\n        \"Hello, I am\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n    scheduler_config = SchedulerConfig(\n        context_chunking_policy=ContextChunkingPolicy.EQUAL_PROGRESS\n    )\n\n    llm  =  LLM(\n    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n    tensor_parallel_size=4,\n    scheduler_config=scheduler_config\n    )\n\n    outputs = llm.generate(prompts, sampling_params)\n\n    # Print the outputs.\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Converting Qwen with SmoothQuant Optimization\nDESCRIPTION: Command to convert a Qwen-7B model with SmoothQuant optimization at a factor of 0.5. SmoothQuant processes and loads INT8 weights for improved quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ --output_dir ./tllm_checkpoint_1gpu_sq --dtype float16 --smoothquant 0.5\n```\n\n----------------------------------------\n\nTITLE: Starting TensorRT-LLM Server with Basic Configuration\nDESCRIPTION: Command to start the trtllm-serve server with common arguments including model, backend, and parallelism options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-serve <model> [--backend pytorch --tp_size <tp> --pp_size <pp> --ep_size <ep> --host <host> --port <port>]\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace Weights to TensorRT-LLM Format\nDESCRIPTION: Script command to convert Phi model weights from HuggingFace format to TensorRT-LLM format with float16 precision\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/phi/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ./convert_checkpoint.py \\\n                    --model_dir /path/to/phi-model \\\n                    --output_dir ./phi-checkpoint \\\n                    --dtype float16\n```\n\n----------------------------------------\n\nTITLE: Testing TensorRT Engine with Sample Prompt\nDESCRIPTION: Command to test the built TensorRT engine using mpirun across 8 GPUs with a simple prompt. This demonstrates basic functionality of the model by generating a response to a text prompt about the US president.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmpirun --allow-run-as-root -n 8 python ../../../run.py --engine_dir ./trtllm_engines/deepseek_v2/bf16/tp8-sel4096-isl2048-bs4 \\\n                --tokenizer_dir ./DeepSeek-V2 \\\n                --max_output_len 40 \\\n                --input_text \"The president of the United States is person who\"\n```\n\n----------------------------------------\n\nTITLE: Running FP8 Inference for JAX Checkpoint\nDESCRIPTION: Commands to convert and run Gemma model with FP8 precision using JAX checkpoint. Includes checkpoint conversion, engine building and inference testing steps. Note that this method may have some accuracy impact.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nCKPT_PATH=/tmp/models/gemma_nv/checkpoints/tmp_7b_it\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_7b_it_tensorrt_llm/fp8/tp1/\nENGINE_PATH=/tmp/gemma/7B/fp8/1-gpu/\nVOCAB_FILE_PATH=/tmp/models/gemma_nv/checkpoints/tmp_vocab.model\n\npython3 ./convert_checkpoint.py \\\n    --ckpt-type jax \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype bfloat16 \\\n    --world-size 1 \\\n    --enable_fp8 \\\n    --fp8_kv_cache \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n```\n\n----------------------------------------\n\nTITLE: Example Output from Successful TensorRT-LLM Engine Run\nDESCRIPTION: Sample output showing the result of a successful run of the TensorRT-LLM engine for a Falcon model, including timing and ROUGE metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n......\n[12/27/2023-03:57:02] [TRT-LLM] [I] TensorRT-LLM (total latency: 5.816917419433594 sec)\n[12/27/2023-03:57:02] [TRT-LLM] [I] TensorRT-LLM beam 0 result\n[12/27/2023-03:57:02] [TRT-LLM] [I]   rouge1 : 15.061493342516243\n[12/27/2023-03:57:02] [TRT-LLM] [I]   rouge2 : 4.495335888974063\n[12/27/2023-03:57:02] [TRT-LLM] [I]   rougeL : 11.800002670828547\n[12/27/2023-03:57:02] [TRT-LLM] [I]   rougeLsum : 13.458777656925877\n```\n\n----------------------------------------\n\nTITLE: Converting Baichuan V1 13B Checkpoint to FP16\nDESCRIPTION: Python command to convert a Baichuan V1 13B model checkpoint to FP16 precision for use with TensorRT-LLM on a single GPU.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_version v1_13b \\\n                             --model_dir baichuan-inc/Baichuan-13B-Chat \\\n                             --dtype float16 \\\n                             --output_dir ./tmp/baichuan_v1_13b/trt_ckpts/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Normalizing DoRA Weights\nDESCRIPTION: Command to run the normalize_weights.py script, which normalizes the DoRA magnitude vectors in the adapter checkpoint. It requires paths to the adapter weights and base model weights.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/dora/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport NORMALIZED_DORA_ADAPTER=path/to/normalized/adapter/ckpt\n\npython ./normalize_weights.py -i DoRA-weights/llama_dora_commonsense_checkpoints/LLama3-8B/dora_r32 -b Meta-Llama-3-8B -o $NORMALIZED_DORA_ADAPTER\n```\n\n----------------------------------------\n\nTITLE: Customizing Build Configuration in TensorRT-LLM\nDESCRIPTION: Configures the LLM build settings using BuildConfig to specify maximum tokens, batch size, and beam width. These parameters optimize engine building for specific hardware and use cases.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(<model-path>,\n          build_config=BuildConfig(\n            max_num_tokens=4096,\n            max_batch_size=128,\n            max_beam_width=4))\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for TensorRT-LLM Build\nDESCRIPTION: Configuration of environment variables for model precision, tensor parallelism, and directory structure setup.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/arctic/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPREC_RAW=\"bfloat16\"\nPREC_QUANT=\"fp8\"\nTP=8\nENGINE=\"${HF_MODEL}_${PREC_QUANT}_tp${TP}\"\n\nmkdir -p tmp/trt_engines\n```\n\n----------------------------------------\n\nTITLE: Weight-Only Quantization for MPT-7B\nDESCRIPTION: Convert MPT-7B model using INT8 or INT4 weight-only quantization\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/int8_wo/ --use_weight_only\n\npython convert_checkpoint.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/int4_wo/ --use_weight_only --weight_only_precision int4\n```\n\n----------------------------------------\n\nTITLE: Example Output from ReDrafter Inference\nDESCRIPTION: Sample output produced by the ReDrafter model for two different input prompts, showing how the model generates coherent continuations with speculative decoding.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/redrafter/README.md#2025-04-07_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nInput [Text 0]: \"<s> Once upon\"\nOutput [Text 0 Beam 0]: \"a time, there was a young girl who loved to read. She would spend hours in the library, devouring books of all genres. She had a special love for fairy tales, and would often dream of living in a magical world where she could meet princes and princesses, and have adventures with talking animals.\nOne day, while she was reading a book, she came across a passage that spoke to her heart. It said, \"You are the author of\"\nInput [Text 1]: \"<s> The basic idea of a Transformer model is\"\nOutput [Text 1 Beam 0]: \"to use self-attention mechanisms to process input sequences. The Transformer model consists of an encoder and a decoder, each of which has multiple layers. The encoder takes an input sequence and generates a sequence of hidden states, while the decoder takes the hidden states generated by the encoder and generates an output sequence.\n\nThe Transformer model uses a self-attention mechanism to process the input sequence. The self-attention mechanism allows the model to weigh the\"\n```\n\n----------------------------------------\n\nTITLE: Building DBRX with INT8 KV Cache\nDESCRIPTION: Converts DBRX with INT8 KV cache optimization to reduce memory footprint and improve performance at large batch sizes and long sequence lengths. Includes calibration for scaling factors needed for INT8 KV cache inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# 4-way tensor parallelism, int8 kv-cache\npython convert_checkpoint.py --model_dir dbrx-base \\\n        --dtype float16 \\\n        --int8_kv_cache \\\n        --tp_size 4 \\\n        --workers 4 \\\n        --output_dir dbrx/trt_ckpt/int8kv/tp4\n\ntrtllm-build --checkpoint_dir dbrx/trt_ckpt/int8kv/tp4 \\\n        --gpt_attention_plugin float16 \\\n        --gemm_plugin float16 \\\n        --moe_plugin float16 \\\n        --workers 4 \\\n        --output_dir dbrx/trt_engines/int8kv/tp4\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for RecurrentGemma-2b-flax (FP16)\nDESCRIPTION: This command builds a TensorRT engine from the RecurrentGemma-2b-flax FP16 checkpoint using trtllm-build with auto GEMM plugin selection.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-flax\nENGINE_2B_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-flax/trt_engines/fp16/1-gpu/\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_2B_FLAX_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_2B_FLAX_PATH}\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for GPT-NeoX (Single GPU)\nDESCRIPTION: Builds a TensorRT engine for GPT-NeoX on a single GPU using float16 precision and specific batch size and sequence length settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./gptneox/20B/trt_ckpt/fp16/1-gpu/ \\\n             --gemm_plugin float16 \\\n             --max_batch_size 8 \\\n             --max_input_len 924 \\\n             --max_seq_len 1024 \\\n             --output_dir ./gptneox/20B/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: MultiLoRA Setup and Inference\nDESCRIPTION: Complete workflow for MultiLoRA setup including checkpoint download, conversion, engine building with LoRA support, and running inference with multiple LoRA tasks.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/nvidia/GPT-2B-001/resolve/main/GPT-2B-001_bf16_tp1.nemo\n\npython3 convert_checkpoint.py --nemo_ckpt_path GPT-2B-001_bf16_tp1.nemo \\\n        --dtype float16 \\\n        --output_dir gpt-next-2B/trt_ckpt/fp16/1-gpu\n\ntrtllm-build --checkpoint_dir gpt-next-2B/trt_ckpt/fp16/1-gpu \\\n        --lora_plugin auto \\\n        --lora_dir gpt2b_lora-900.nemo gpt2b_lora-stories.nemo \\\n        --lora_ckpt_source \"nemo\" \\\n        --lora_target_modules attn_qkv \\\n        --max_batch_size 4 \\\n        --max_beam_width 2 \\\n        --max_input_len 512 \\\n        --max_seq_len 562 \\\n        --output_dir gpt-next-2B/trt_engines/fp16/1-gpu\n\npython3 ../run.py --engine_dir gpt-next-2B/trt_engines/fp16/1-gpu \\\n        --vocab_file gpt-next-2B/trt_ckpt/fp16/1-gpu/tokenizer.model \\\n        --no_add_special_tokens \\\n        --max_output_len 20 \\\n        --use_py_session \\\n        --lora_task_uids 0 -1 1 \\\n        --input_text \"After Washington had returned to Williamsburg, Dinwiddie ordered him to lead a larger force to assist Trent in his work. While en route, Washington learned of Trent's retreat. Since Tanaghrisson had promised support to the British, Washington continued toward Fort Duquesne and met with the Mingo leader. Learning of a French scouting party in the area, Washington, with Tanaghrisson and his party, surprised the Canadians on May 28 in what became known as the Battle of Jumonville Glen. They killed many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. The historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire. Question: Upon learning of a French scounting party in the area, what did Washington do? Answer:\" \"You hold the job title in the Wizarding World of Harry Potter where you say random words looking for spells\" \"You hold the job title in the Wizarding World of Harry Potter where you say random words looking for spells\"\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with Qwen-7B Quantized to INT4\nDESCRIPTION: Performs summarization using the Qwen-7B model quantized to INT4, specifying the appropriate quantized engine directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --test_trt_llm \\\n                       --hf_model_dir  ./tmp/Qwen/7B/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./tmp/Qwen/7B/trt_engines/int4_weight_only/1-gpu/ \\\n                       --max_input_length 2048 \\\n                       --output_len 2048\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Deepseek-v1 TensorRT Engine\nDESCRIPTION: Command to test the built TensorRT engine using the run.py script for text generation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v1/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../run.py --engine_dir ./trtllm_engines/deepseek_v1/bf16/tp1 \\\n                --tokenizer_dir ./deepseek_moe_16b/ \\\n                --max_output_len 32 \\\n                --top_p 0 \\\n                --input_text \"The president of the United States is person who\"\n```\n\n----------------------------------------\n\nTITLE: Basic TensorRT-LLM Inference\nDESCRIPTION: Run basic inference using a TensorRT-LLM engine with specified parameters for vocabulary and output length.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --engine_dir gpt-next-2B/trt_engines/bf16/1-gpu \\\n        --vocab_file gpt-next-2B/trt_ckpt/bf16/1-gpu/tokenizer.model \\\n        --no_add_special_tokens \\\n        --max_output_len 8\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all CPP files in the current directory and adds them to the PLUGIN_SOURCES variable, then propagates the updated variable to the parent scope. This is used to manage plugin source files in the build system.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/weightOnlyQuantMatmulPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Running DBRX Inference with TensorRT-LLM\nDESCRIPTION: Executes inference on the built TensorRT engines using MPI to coordinate multiple GPUs. Uses the run.py script to process input text and generate output with the DBRX model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 8 \\\n    python3 ../run.py --engine_dir dbrx/trt_engines/bf16/tp8 \\\n        --tokenizer_dir dbrx-base \\\n        --max_output_len 10 \\\n        --input_text \"What is AGI?\"\n```\n\n----------------------------------------\n\nTITLE: Summarization Evaluation for Recurrent Gemma Models in TensorRT-LLM\nDESCRIPTION: Commands for benchmarking summarization performance of Recurrent Gemma models using summarize.py script. Includes tests for various model variants with different quantization schemes (FP8, INT8 SmoothQuant, INT4 AWQ) and a batch size of 8.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b\npython3 ../summarize.py --test_trt_llm \\\n                        --use_py_session \\\n                        --engine_dir ${ENGINE_2B_PATH} \\\n                        --batch_size 8 \\\n                        --max_attention_window_size 2048 \\\n                        --tokenizer_dir ${TOKENIZER_DIR_2B_PATH}\n\n# recurrentgemma-2b-it FP8 with FP8 kv cache\npython3 ../summarize.py --test_trt_llm \\\n                        --use_py_session \\\n                        --engine_dir ${ENGINE_2B_IT_FP8_PATH} \\\n                        --batch_size 8 \\\n                        --max_attention_window_size 2048 \\\n                        --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH}\n\n# recurrentgemma-2b-it INT8 SmoothQuant with INT8 kv cache\npython3 ../summarize.py --test_trt_llm \\\n                        --use_py_session \\\n                        --engine_dir ${ENGINE_2B_IT_INT8_SQ_PATH} \\\n                        --batch_size 8 \\\n                        --max_attention_window_size 2048 \\\n                        --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH}\n\n# recurrentgemma-2b-it INT4 AWQ with INT8 kv cache\npython3 ../summarize.py --test_trt_llm \\\n                        --use_py_session \\\n                        --engine_dir ${ENGINE_2B_IT_INT4_AWQ_PATH} \\\n                        --batch_size 8 \\\n                        --max_attention_window_size 2048 \\\n                        --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH}\n\n# recurrentgemma-2b-flax\npython3 ../summarize.py --test_trt_llm \\\n                        --use_py_session \\\n                        --engine_dir ${ENGINE_2B_FLAX_PATH} \\\n                        --batch_size 8 \\\n                        --max_attention_window_size 2048 \\\n                        --vocab_file ${VOCAB_FILE_2B_FLAX_PATH}\n\n# recurrentgemma-2b-it-flax\npython3 ../summarize.py --test_trt_llm \\\n                        --use_py_session \\\n                        --engine_dir ${ENGINE_2B_IT_FLAX_PATH} \\\n                        --batch_size 8 \\\n                        --max_attention_window_size 2048 \\\n                        --vocab_file ${VOCAB_FILE_2B_IT_FLAX_PATH}\n```\n\n----------------------------------------\n\nTITLE: Downloading LLaVA-NeXT Model Weights\nDESCRIPTION: Commands to clone the LLaVA-NeXT model weights from Huggingface, supporting either Mistral-7b or Nous-Hermes-2-Yi-34B variants.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/vit/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"llava-v1.6-mistral-7b-hf\" #for 34b variant \"llava-v1.6-34b-hf\"\ngit clone https://huggingface.co/llava-hf/${MODEL_NAME} tmp/hf_models/${MODEL_NAME}\n```\n\n----------------------------------------\n\nTITLE: Cloning DoRA and Base Model Repositories\nDESCRIPTION: Commands to clone the DoRA weights repository and the base LLaMA3-8B model repository from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/dora/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/sliuau/DoRA-weights\ngit clone https://huggingface.co/meta-llama/Meta-Llama-3-8B\n```\n\n----------------------------------------\n\nTITLE: Displaying Accuracy Results Table for MMLU Benchmark in Markdown\nDESCRIPTION: A markdown table showing accuracy metrics for 2B and 7B Pretrained models using different quantization techniques including fp8, int4_awq, and int8_sq variants. The table allows comparison of how each quantization method affects model accuracy on the MMLU benchmark.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n| Model         | fp8   | int4_awq | int8_sq (Modelopt) | int8_sq (Native per-channel) |\n|---------------|-------|----------|----------------|------------------|\n| 2B Pretrained | 0.407 | 0.378    |    0.338       |     0.338        |\n| 7B Pretrained | 0.643 | 0.615    |    0.448       |     0.595        |\n```\n\n----------------------------------------\n\nTITLE: Evaluating DeepSeek-V3 on GPQA Dataset\nDESCRIPTION: Command to run evaluation of DeepSeek-V3 on the GPQA dataset, using tensor parallelism, CUDA graph, and overlap scheduler optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython examples/gpqa_llmapi.py \\\n  --hf_model_dir <YOUR_MODEL_DIR> \\\n  --data_dir <DATASET_PATH> \\\n  --tp_size 8 \\\n  --use_cuda_graph \\\n  --enable_overlap_scheduler \\\n  --concurrency 32 \\\n  --batch_size 32 \\\n  --max_num_tokens 4096\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines\nDESCRIPTION: Commands to build TensorRT engines from checkpoints with different configurations and optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir trt_ckpt/command_r_v01/fp16/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/command_r_v01/fp16/1-gpu\n\ntrtllm-build --checkpoint_dir trt_ckpt/command_r_plus/fp16/4-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/command_r_plus/fp16/4-gpu\n\ntrtllm-build --checkpoint_dir trt_ckpt/aya_23_8B/fp16/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/aya_23_8B/fp16/1-gpu\n\ntrtllm-build --checkpoint_dir trt_ckpt/aya_23_35B/fp16/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/aya_23_35B/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Baichuan Model Summarization\nDESCRIPTION: Commands for running text summarization using Baichuan model with different precision modes and GPU configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../summarize.py --test_trt_llm \\\n                       --hf_model_dir baichuan-inc/Baichuan-13B-Chat \\\n                       --data_type fp16 \\\n                       --engine_dir ./tmp/baichuan_v1_13b/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Generating TRT-LLM Visual Component Engine\nDESCRIPTION: Commands to convert and build the visual component of LLaVA-NeXT into a TensorRT-LLM engine, including checkpoint conversion and engine building with specific configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/vit/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ./convert_checkpoint.py \\\n    --model_dir tmp/hf_models/${MODEL_NAME} \\\n    --output_dir tmp/trt_models/${MODEL_NAME}/fp16/1-gpu/vision \\\n    --dtype float16\n\ntrtllm-build \\\n    --checkpoint_dir tmp/trt_models/${MODEL_NAME}/fp16/1-gpu/vision \\\n    --output_dir tmp/trt_engines/${MODEL_NAME}/fp16/1-gpu/vision \\\n    --remove_input_padding disable \\\n    --bert_attention_plugin disable \\\n    --max_batch_size 8\n\n# copy the image newlines tensor to engine directory\ncp tmp/trt_models/${MODEL_NAME}/fp16/1-gpu/vision/image_newlines.safetensors tmp/trt_engines/${MODEL_NAME}/fp16/1-gpu/vision\n```\n\n----------------------------------------\n\nTITLE: Running SmoothQuant Inference for JAX Checkpoint\nDESCRIPTION: Commands to run Gemma 7B model with SmoothQuant quantization using JAX checkpoint. Includes steps for cloning the repository, converting checkpoint, building engine and running inference tests.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-7b-it-flax\nCKPT_PATH=gemma-7b-it-flax/7b-it/\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_7b_it_tensorrt_llm/sq/tp1\nENGINE_PATH=/tmp/gemma/7B/int8_sq/1-gpu/\nVOCAB_FILE_PATH=gemma-7b-it-flax/tokenizer.model\n\npython3 ./convert_checkpoint.py \\\n    --ckpt-type jax \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype float16 \\\n    --use_smooth_quant_plugin 0.5 \\\n    --tokenizer_dir ${VOCAB_FILE_PATH} \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                        --vocab_file ${VOCAB_FILE_PATH} \\\n                        --engine_dir ${ENGINE_PATH} \\\n                        --batch_size 8 \\\n                        --max_ite 5\n```\n\n----------------------------------------\n\nTITLE: Applying GPTQ Quantization to GPT-NeoX Weights\nDESCRIPTION: Runs a script to apply GPTQ (Groupwise Precision Quantization) to the GPT-NeoX weights, preparing them for INT4 quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsh gptq_convert.sh\n```\n\n----------------------------------------\n\nTITLE: Running Language-Adapter Model with Python Runtime\nDESCRIPTION: Example for running a language adapter model using the Python runtime interface. It shows how to create language adapter routings from language_task_uid and input_ids, which is required for both encoder and decoder.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/language_adapter/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# language_adapter_routing = get_language_adapter_routings(language_task_uid, input_ids)\n\nTEXT=\"Where is the nearest restaurant? Wikipedia is a free online encyclopedia written and maintained by a community of volunteers (called Wikis) through open collaboration and the use of MediaWiki, a wiki-based editing system.\"\n\npython3 ../enc_dec/run.py --engine_dir $ENGINE_DIR  --engine_name ${MODEL_NAME} --model_name $MODEL_DIR --max_new_token=64 --num_beams=1\n```\n\n----------------------------------------\n\nTITLE: Converting OPT Weights from HuggingFace to TensorRT-LLM Format\nDESCRIPTION: Python commands to convert OPT model weights from HuggingFace Transformers format to TensorRT-LLM format, with options for different model sizes and tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# OPT-125M\npython3 convert_checkpoint.py --model_dir ./opt-125m \\\n                --dtype float16 \\\n                --output_dir ./opt/125M/trt_ckpt/fp16/1-gpu/\n\n# OPT-350M\npython3 convert_checkpoint.py --model_dir ./opt-350m \\\n                --dtype float16 \\\n                --output_dir ./opt/350M/trt_ckpt/fp16/1-gpu/\n\n# OPT-2.7B\npython3 convert_checkpoint.py --model_dir ./opt-2.7b \\\n                --dtype float16 \\\n                --output_dir ./opt/2.7B/trt_ckpt/fp16/1-gpu/\n\n# OPT-66B\npython3 convert_checkpoint.py --model_dir ./opt-66b \\\n                --dtype float16 \\\n                --tp_size 4 \\\n                --output_dir ./opt/66B/trt_ckpt/fp16/4-gpu/ \\\n                --workers 2\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Container with Latest Image\nDESCRIPTION: Command to deploy the latest TensorRT-LLM image in a container with local user permissions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker trtllm_run LOCAL_USER=1 DOCKER_PULL=1\n```\n\n----------------------------------------\n\nTITLE: Prompt-Tuning Setup and Conversion\nDESCRIPTION: Convert NeMo checkpoint and build TensorRT-LLM engines with prompt-tuning support. Includes checkpoint conversion, engine building, and prompt table conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --nemo_ckpt_path megatron_converted_8b_tp4_pp1.nemo \\\n        --dtype float16 \\\n        --output_dir gpt-next-8B/trt_ckpt/fp16/1-gpu\n\ntrtllm-build --checkpoint_dir gpt-next-8B/trt_ckpt/fp16/1-gpu \\\n        --max_prompt_embedding_table_size 100 \\\n        --output_dir gpt-next-8B/trt_engines/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Git LFS\nDESCRIPTION: Commands to install required packages and setup Git LFS for downloading model weights\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mamba/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\ngit lfs install\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines for Whisper\nDESCRIPTION: Commands for converting Whisper model weights to TensorRT-LLM format and building TensorRT engines for both encoder and decoder components.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/whisper/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nINFERENCE_PRECISION=float16\nWEIGHT_ONLY_PRECISION=int8\nMAX_BEAM_WIDTH=4\nMAX_BATCH_SIZE=8\ncheckpoint_dir=whisper_large_v3_weights_${WEIGHT_ONLY_PRECISION}\noutput_dir=whisper_large_v3_${WEIGHT_ONLY_PRECISION}\n\npython3 convert_checkpoint.py \\\n                --use_weight_only \\\n                --weight_only_precision $WEIGHT_ONLY_PRECISION \\\n                --output_dir $checkpoint_dir\n\ntrtllm-build  --checkpoint_dir ${checkpoint_dir}/encoder \\\n              --output_dir ${output_dir}/encoder \\\n              --moe_plugin disable \\\n              --max_batch_size ${MAX_BATCH_SIZE} \\\n              --gemm_plugin disable \\\n              --bert_attention_plugin ${INFERENCE_PRECISION} \\\n              --max_input_len 3000 --max_seq_len=3000\n\ntrtllm-build  --checkpoint_dir ${checkpoint_dir}/decoder \\\n              --output_dir ${output_dir}/decoder \\\n              --moe_plugin disable \\\n              --max_beam_width ${MAX_BEAM_WIDTH} \\\n              --max_batch_size ${MAX_BATCH_SIZE} \\\n              --max_seq_len 114 \\\n              --max_input_len 14 \\\n              --max_encoder_input_len 3000 \\\n              --gemm_plugin ${INFERENCE_PRECISION} \\\n              --bert_attention_plugin ${INFERENCE_PRECISION} \\\n              --gpt_attention_plugin ${INFERENCE_PRECISION}\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with Qwen-7B in FP16\nDESCRIPTION: Executes a summarization task using the Qwen-7B model in FP16 precision. It specifies the model directory, engine directory, and input/output lengths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./tmp/Qwen/7B/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./tmp/Qwen/7B/trt_engines/fp16/1-gpu/ \\\n                       --max_input_length 2048 \\\n                       --output_len 2048\n```\n\n----------------------------------------\n\nTITLE: Downloading Mamba Model Weights\nDESCRIPTION: Commands to clone various Mamba model weights from HuggingFace repositories\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mamba/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/state-spaces/mamba-2.8b-hf ./mamba_model/mamba-2.8b\ngit clone https://huggingface.co/state-spaces/mamba-130m-hf ./mamba_model/mamba-130m\ngit clone https://huggingface.co/state-spaces/mamba2-2.7b ./mamba_model/mamba2-2.7b\ngit clone https://huggingface.co/state-spaces/mamba2-130m ./mamba_model/mamba2-130m\ngit clone https://huggingface.co/mistralai/mamba-codestral-7B-v0.1 ./mamba_model/mamba-codestral-7B-v0.1\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Python Runtime for BART\nDESCRIPTION: These snippets show how to run benchmarks for the BART model using the Python runtime. They include examples for single-GPU and multi-GPU benchmarking with various batch sizes and input/output lengths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark.py \\\n    -m enc-dec \\\n    --batch_size \"1;8\" \\\n    --input_output_len \"60,20;128,20\" \\\n    --engine_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION} \\\n    --dtype float32 \\\n    --csv # optional\n\nmpirun --allow-run-as-root -np 4 python benchmark.py \\\n    -m enc-dec \\\n    --batch_size \"1;8\" \\\n    --input_output_len \"60,20;128,20\" \\\n    --engine_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION} \\\n    --dtype float32 \\\n    --csv # optional\n```\n\n----------------------------------------\n\nTITLE: Setting TensorRT-LLM Precompiled Location\nDESCRIPTION: Configuring the URL for the pre-compiled TensorRT-LLM wheel package.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport TRTLLM_PRECOMPILED_LOCATION=https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-0.18.0.dev2025021800-cp312-cp312-linux_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Running Python Unit Tests with pytest\nDESCRIPTION: Commands for installing dependencies, setting up the environment, and running unit tests using pytest in the TensorRT-LLM project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# in tensorrt-llm source repo root dir\n# use editable install, such that your local changes will be used immedietely in the tests w/o another install\n# see https://setuptools.pypa.io/en/latest/userguide/development_mode.html\npip install -e ./\n\n# the pytest and required plugins used are listed in the requirements-dev.txt\npip install -r requirements-dev.txt\n\ncd tests/\n## There are multiple ways to tell pytest to launch a subset of the targeted test cases\n\n# example 1: runs all the tests under this directory, ignores the integration. WARNING: this can takes a very long time\npytest ./\n\n# example 2: run a single test file\npytest ./test_builder.py\n\n# example 3: run a test in a subfolder\npytest ./functional\n\n# example 4: run a test with a substr\npytest -k test_basic_builder_flow\n```\n\n----------------------------------------\n\nTITLE: Summarization with Vicuna-7B using Medusa Decoding\nDESCRIPTION: This command runs the Medusa decoding for summarization tasks using the Vicuna-7B-v1.3 model on a single GPU. It specifies the engine directory, model directory, tokenization parameters, and Medusa choice patterns to use during decoding with a temperature of 1.0.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --engine_dir ./tmp/medusa/7B/trt_engines/fp16/1-gpu/ \\\n                       --hf_model_dir ./vicuna-7b-v1.3/ \\\n                       --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                       --test_trt_llm \\\n                       --data_type fp16 \\\n                       --medusa_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                       --use_py_session \\\n                       --temperature 1.0 \\\n                       --batch_size 1\n```\n\n----------------------------------------\n\nTITLE: Converting RecurrentGemma-2b-it to INT8 SmoothQuant with INT8 KV Cache\nDESCRIPTION: This command quantizes the recurrentgemma-2b-it model to INT8 SmoothQuant format with INT8 KV cache using the quantization script.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it INT8 SmoothQuant with INT8 kv cache\nUNIFIED_CKPT_2B_IT_INT8_SQ_PATH=./recurrentgemma_model/recurrentgemma-2b-it/trt_ckpt/int8_sq/1-gpu/\npython ../quantization/quantize.py --model_dir ${CKPT_2B_IT_PATH} \\\n                                   --dtype float16 \\\n                                   --qformat int8_sq \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ${UNIFIED_CKPT_2B_IT_INT8_SQ_PATH} \\\n                                   --calib_size 512 \\\n                                   --tp_size 1\n```\n\n----------------------------------------\n\nTITLE: EAGLE-2 Summarization with Dynamic Tree\nDESCRIPTION: Command for running EAGLE-2 summarization using dynamic tree implementation with configurable top-k parameter and FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 1 --allow-run-as-root --oversubscribe \\\n    python ../summarize.py --engine_dir ./tmp/eagle/7B/trt_engines/fp16/1-gpu/ \\\n                           --hf_model_dir ./vicuna-7b-v1.3/ \\\n                           --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                           --test_trt_llm \\\n                           --data_type fp16 \\\n                           --eagle_use_dynamic_tree \\\n                           --eagle_dynamic_tree_max_top_k 10 \\\n                           --batch_size 1\n```\n\n----------------------------------------\n\nTITLE: Converting GPTQ Quantized Weights to TensorRT-LLM Format (Single GPU)\nDESCRIPTION: Converts the GPTQ quantized weights to TensorRT-LLM format for a single GPU setup, using INT4 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./gptneox_model \\\n                              --dtype float16 \\\n                              --use_weight_only \\\n                              --weight_only_precision int4_gptq \\\n                              --quant_ckpt_path ./gptneox_model/gptneox-20b-4bit-gs128.safetensors \\\n                              --output_dir ./gptneox/20B/trt_ckpt/int4_gptq/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Running Qwen Model with BF16 Precision\nDESCRIPTION: Command to run inference on a TensorRT-LLM Qwen model with BF16 precision. It processes Chinese input and generates output using BF16 optimized engines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --input_text \"你好，请问你叫什么？\" \\\n                  --max_output_len=50 \\\n                  --tokenizer_dir ./tmp/Qwen/7B/ \\\n                  --engine_dir=./tmp/Qwen/7B/trt_engines/bf16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Converting OPT Checkpoint with Hidden Size Sharding in TensorRT-LLM\nDESCRIPTION: This snippet demonstrates how to convert an OPT-125M checkpoint using tensor parallelism with embedding sharding along the hidden_size dimension. It uses parallel embedding and sets the sharding dimension to 1.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./opt-125m \\\n                --dtype float16 \\\n                --output_dir ./opt/125M/trt_ckpt/fp16/2-gpu/ \\\n                --tp_size 2 \\\n                --use_parallel_embedding \\\n                --embedding_sharding_dim 1\n```\n\n----------------------------------------\n\nTITLE: Cloning Deepseek-v1 Model Repository\nDESCRIPTION: Commands to install Git LFS and clone the Deepseek-v1 model repository from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v1/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/deepseek-ai/deepseek-moe-16b-base\n```\n\n----------------------------------------\n\nTITLE: Sending a Simple Request to the TensorRT-LLM API Server\nDESCRIPTION: Example curl command for sending a text generation request to the FastAPI server. This shows how to specify parameters like max_tokens for the generation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/apps/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8000/generate -d '{\"prompt\": \"In this example,\", \"max_tokens\": 8}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for NVIDIA TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all .cpp files in the current directory and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope to make it available for other CMake files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/selectiveScanPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete Model Architecture in PyTorch Backend\nDESCRIPTION: Comprehensive implementation example for a new model in the PyTorch backend, showing the custom attention mechanism, decoder layer, base model, and causal language model implementation with proper inheritance from TensorRT-LLM base classes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/adding_new_model.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\nfrom tensorrt_llm._torch.attention_backend import AttentionMetadata\nfrom tensorrt_llm._torch.model_config import ModelConfig\nfrom tensorrt_llm._torch.models.modeling_utils import DecoderModel, DecoderModelForCausalLM\nfrom tensorrt_llm._torch.modules.attention import Attention\nfrom tensorrt_llm._torch.modules.decoder_layer import DecoderLayer\n\nfrom configuration_mymodel import MyConfig\n\n\nclass MyAttention(Attention):\n    def __init__(self, model_config: ModelConfig[MyConfig], layer_idx: Optional[int] = None):\n        # Use model_config to initialize the Attention module\n        super().__init__(...)\n\n\nclass MyDecoderLayer(DecoderLayer):\n    def __init__(self, model_config: ModelConfig[MyConfig], layer_idx: int):\n        super().__init__()\n        # Use model_config to initialize the submodules\n        self.input_layernorm = ...\n        self.self_attn = MyAttention(model_config, layer_idx)\n        self.post_attention_layernorm = ...\n        self.mlp = ...\n\n    def forward(self, hidden_states: torch.Tensor, attn_metadata: AttentionMetadata, **kwargs):\n        # Define the forward computation of a single decoder layer\n        ...\n\n\nclass MyModel(DecoderModel):\n    def __init__(self, model_config: ModelConfig[MyConfig]):\n        super().__init__(model_config)\n        # Use model_config to initialize the submodules\n        self.embed_tokens = ...\n        self.layers = nn.ModuleList([\n            MyDecoderLayer(model_config, layer_idx) for layer_idx in range(model_config.pretrained_config.num_hidden_layers)\n        ])\n\n    def forward(self,\n                attn_metadata: AttentionMetadata,\n                input_ids: Optional[torch.LongTensor] = None,\n                position_ids: Optional[torch.LongTensor] = None,\n                inputs_embeds: Optional[torch.FloatTensor] = None):\n        # Define the forward computation of the model\n        ...\n\n\nclass MyModelForCausalLM(DecoderModelForCausalLM[MyModel, MyConfig]):\n    def __init__(self, model_config: ModelConfig[MyConfig]):\n        super().__init__(MyModel(model_config),\n                         config=model_config,\n                         hidden_size=model_config.pretrained_config.hidden_size,\n                         vocab_size=model_config.pretrained_config.vocab_size)\n```\n\n----------------------------------------\n\nTITLE: Running DiT with Combined Tensor and Context Parallel\nDESCRIPTION: Commands to build and run DiT model using both tensor and context parallelism with 2x2 configuration\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dit/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --cp_size 2 --tp_size 2\ntrtllm-build --checkpoint_dir ./tllm_checkpoint/ --max_batch_size 8 --remove_input_padding disable --bert_attention_plugin disable\npython vae_decoder_trt.py --max_batch_size 8\nmpirun -n 4 --allow-run-as-root python sample.py\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Triton Version Using Git and Pip\nDESCRIPTION: Shell commands to clone the Triton repository, navigate to the Python directory, and install Triton using pip. This ensures compatibility with the PluginGen tool by installing a specific version of Triton.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/plugin_autogen/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/openai/triton\ncd triton/python/\npip install cmake && pip install .\ncd -\n```\n\n----------------------------------------\n\nTITLE: Converting DiT Model to TensorRT-LLM Format (FP16)\nDESCRIPTION: Commands to convert DiT model checkpoint to TensorRT-LLM format using float16 precision and build the TensorRT engine\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dit/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py\ntrtllm-build --checkpoint_dir ./tllm_checkpoint/ \\\n                --max_batch_size 8 \\\n                --remove_input_padding disable \\\n                --bert_attention_plugin disable\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing FairSeq NMT Model\nDESCRIPTION: Commands to download FairSeq NMT model weights and install FairSeq dependencies while avoiding PyTorch version conflicts.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p tmp/fairseq_models && curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 | tar xvjf - -C tmp/fairseq_models  --one-top-level=wmt14 --strip-components 1 --no-same-owner\n\npushd tmp && (git clone https://github.com/facebookresearch/fairseq.git || true) && pushd fairseq && sed -i '/torch>=/d;/torchaudio>=/d' setup.py && pip install -e . && pip install sacremoses subword_nmt && popd && popd\n```\n\n----------------------------------------\n\nTITLE: Collecting and Setting Plugin Sources in CMake\nDESCRIPTION: This CMake snippet collects all C++ files in the current directory using glob, adds them to the PLUGIN_SOURCES variable, and then propagates the updated variable to the parent scope. This pattern is commonly used in CMake to gather source files recursively across directories.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/layernormQuantizationPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Integrating a TensorRT-LLM Plugin into a Network\nDESCRIPTION: Demonstrates how to add a custom plugin to a TensorRT-LLM network by creating a network instance, defining tensors, and using the plugin in the computational graph.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/python_plugin/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbuilder = tensorrt_llm.Builder()\nnetwork = builder.create_network()\nwith tensorrt_llm.net_guard(network):\n    x = Tensor(name='x',\n               shape=index_shape,\n               dtype=tensorrt_llm.str_dtype_to_trt('int32'))\n    y = Tensor(name='y',\n               shape=(vocab_size, n_embed),\n               dtype=torch_dtype_to_trt(dtype))\n\n    def lookup(x, y):\n        lookup_plugin = LookUpPlugin(False)\n        return lookup_plugin(x, y)\n\n    output = lookup(x, y)\n    output.mark_output('output', torch_dtype_to_str(dtype))\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with Single GPU\nDESCRIPTION: Command to build a float16 TensorRT engine using single GPU with optimized plugins and specified batch/sequence lengths\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/phi/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build \\\n    --checkpoint_dir ./phi-checkpoint \\\n    --output_dir ./phi-engine \\\n    --gemm_plugin auto \\\n    --max_batch_size 8 \\\n    --max_input_len 1024 \\\n    --max_seq_len 2048\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements and Downloading Models for ReDrafter\nDESCRIPTION: Commands to install required dependencies and download the base Vicuna model. The drafter checkpoint is assumed to be already available as it's not publicly available at the time of writing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/redrafter/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n\ngit lfs install\ngit clone https://huggingface.co/lmsys/vicuna-7b-v1.3\n# assuming the drafter checkpoint is located in dir \"vicuna-7b-drafter\"\n```\n\n----------------------------------------\n\nTITLE: Converting InternLM2 7B with INT8 Weight-Only Quantization\nDESCRIPTION: Converts the HuggingFace InternLM2 7B model to TensorRT-LLM format using INT8 weight-only quantization to reduce memory footprint.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm2-chat-7b/ \\\n                --dtype float16 \\\n                --output_dir ./internlm2-chat-7b/trt_engines/int8/1-gpu/ \\\n                --use_weight_only \\\n                --weight_only_precision int8\n```\n\n----------------------------------------\n\nTITLE: Converting Jais-30b-chat-v3 to TensorRT-LLM Checkpoint (2-way Tensor Parallelism)\nDESCRIPTION: This command converts the Hugging Face Jais-30b-chat-v3 model to TensorRT-LLM checkpoint format using float16 precision with 2-way tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../../../gpt/convert_checkpoint.py --model_dir core42/jais-30b-chat-v3 \\\n        --dtype float16 \\\n        --tp_size 2 \\\n        --output_dir jais-30b-chat-v3/trt_ckpt/fp16/2-gpu\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with Parallelism Mapping\nDESCRIPTION: Command demonstrating how to build an FP8 quantized Llama 3.1 engine with tensor-parallel (TP) and pipeline-parallel (PP) mappings. This example uses TP2-PP2 mapping, which means the model is distributed across 4 GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ntrtllm-bench --model meta-llama/Llama-3.1-8B build --quantization FP8 --dataset /tmp/synthetic_128_128.txt --tp_size 2 --pp_size 2\n```\n\n----------------------------------------\n\nTITLE: Running AutoDeploy Llama Example\nDESCRIPTION: Command to run the auto-deploy demo using the build_and_run_ad.py script with a TinyLlama model from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/auto_deploy\npython build_and_run_ad.py --config '{\"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"}'\n```\n\n----------------------------------------\n\nTITLE: Building Single GPU BFloat16 Engine\nDESCRIPTION: Commands to build TensorRT engine for single GPU using BFloat16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../quantization/quantize.py \\\n        --nemo_ckpt_path nemotron-3-8b-base-4k/Nemotron-3-8B-Base-4k.nemo \\\n        --dtype bfloat16 \\\n        --batch_size 64 \\\n        --qformat full_prec \\\n        --output_dir nemotron-3-8b/trt_ckpt/bf16/1-gpu\n\ntrtllm-build --checkpoint_dir nemotron-3-8b/trt_ckpt/bf16/1-gpu \\\n        --gpt_attention_plugin bfloat16 \\\n        --gemm_plugin bfloat16 \\\n        --output_dir nemotron-3-8b/trt_engines/bf16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Running DiT with Tensor Parallel\nDESCRIPTION: Commands to build and run DiT model with tensor parallelism across 4 GPUs\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dit/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --tp_size 4\ntrtllm-build --checkpoint_dir ./tllm_checkpoint/ \\\n                --max_batch_size 8 \\\n                --remove_input_padding disable \\\n                --bert_attention_plugin disable\npython vae_decoder_trt.py --max_batch_size 8\nmpirun -n 4 --allow-run-as-root python sample.py\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Skywork Model (FP16)\nDESCRIPTION: Builds a TensorRT engine for the Skywork model using FP16 precision with specific plugin and parameter settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./skywork-13b-base/trt_ckpt/fp16 \\\n                --gemm_plugin float16 \\\n                --gpt_attention_plugin float16 \\\n                --context_fmha enable \\\n                --max_batch_size 32 \\\n                --max_input_len 512 \\\n                --max_seq_len 1024 \\\n                --output_dir ./skywork-13b-base/trt_engine/fp16\n```\n\n----------------------------------------\n\nTITLE: Running DiT with Context Parallel\nDESCRIPTION: Commands to build and run DiT model with context parallelism across 4 GPUs\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dit/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --cp_size 4\ntrtllm-build --checkpoint_dir ./tllm_checkpoint/ --max_batch_size 8 --remove_input_padding disable --bert_attention_plugin disable\npython vae_decoder_trt.py --max_batch_size 8\nmpirun -n 4 --allow-run-as-root python sample.py\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task\nDESCRIPTION: Command to run summarization evaluation on CNN/DailyMail dataset with ROUGE score metrics\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../summarize.py --test_trt_llm \\\n                        --engine_dir ${ENGINE_PATH} \\\n                        --batch_size 8 \\\n                        --max_ite 5 \\\n                        --vocab_file ${VOCAB_FILE_PATH}\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BertForQuestionAnswering with Optimizations\nDESCRIPTION: Command to build a TensorRT engine for BertForQuestionAnswering with input padding removal and bert attention plugin. Works for both single GPU and tensor parallelism configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bert/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./${model_name}_${dtype}_tllm_checkpoint  \\\n--output_dir=${model_name}_engine_outputs \\\n--remove_input_padding=enable \\\n--bert_attention_plugin=${dtype} \\\n--max_batch_size 8 \\\n--max_input_len 512\n```\n\n----------------------------------------\n\nTITLE: Downloading LoRA Model for Qwen-7B-Chat\nDESCRIPTION: Downloads a LoRA adaptation model for the Qwen-7B-Chat model from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/Jungwonchang/Ko-QWEN-7B-Chat-LoRA ./tmp/Ko-QWEN-7B-Chat-LoRA\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task\nDESCRIPTION: Command to run text summarization evaluation using the model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../summarize.py --test_trt_llm \\\n        --hf_model_dir command_r_v01 \\\n        --engine_dir trt_engines/command_r_v01/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Summarization with InternLM Models in Various Configurations\nDESCRIPTION: These commands run summarization tasks using InternLM models with different precisions and parallelism strategies. They compare TensorRT-LLM implementations with Hugging Face implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# Run summarization using the InternLM 7B model in FP16.\npython ../../../summarize.py --test_trt_llm --test_hf \\\n                       --hf_model_dir ./internlm-chat-7b/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./engine_outputs\n\n# Run summarization using the InternLM 7B model quantized to INT8.\npython ../../../summarize.py --test_trt_llm --test_hf \\\n                       --hf_model_dir ./internlm-chat-7b/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./engine_outputs\n\n# Run summarization using the InternLM 7B model in FP16 using two GPUs.\nmpirun -n 2 --allow-run-as-root \\\n    python ../../../summarize.py --test_trt_llm --test_hf \\\n                           --hf_model_dir ./internlm-chat-7b/ \\\n                           --data_type fp16 \\\n                           --engine_dir ./internlm-chat-7b/trt_engines/fp16/2-gpu/\n\n# Run summarization using the InternLM 20B model in BF16 using 4 GPUs.\nmpirun -n 4 --allow-run-as-root \\\n    python ../../../summarize.py --test_trt_llm --test_hf \\\n                           --hf_model_dir ./internlm-chat-20b/ \\\n                           --data_type bf16 \\\n                           --engine_dir ./internlm-chat-20b/trt_engines/bf16/4-gpu/\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Conversion API Classes in Python\nDESCRIPTION: Defines the base TopModelMixin class and LLaMAForCausalLM implementation for converting models from HuggingFace format to TensorRT-LLM format.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass TopModelMixin\n    @classmethod\n    def from_hugging_face(cls,\n                          hf_model_dir: str,\n                          dtype: Optional[str] = 'float16',\n                          mapping: Optional[Mapping] = None,\n                          **kwargs):\n        raise NotImplementedError(\"Subclass shall override this\")\n\n# TopModelMixin is in the part of base class hierarchy\nclass LLaMAForCausalLM (DecoderModelForCausalLM):\n    @classmethod\n    def from_hugging_face(cls,\n             hf_model_dir,\n             dtype='float16',\n             mapping: Optional[Mapping] = None) -> LLaMAForCausalLM:\n        # creating a TensorRT-LLM llama model object\n        # converting HuggingFace checkpoint to TensorRT-LLM expected weights dict\n        # Load the weights to llama model object\n```\n\n----------------------------------------\n\nTITLE: End-to-End LoRA Benchmarking Script\nDESCRIPTION: Comprehensive script for setting up and running LoRA benchmarks. Includes model checkpoint conversion, engine building, and LoRA-specific configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit-lfs clone https://huggingface.co/meta-llama/Llama-2-13b-hf\ngit-lfs clone https://huggingface.co/hfl/chinese-llama-2-lora-13b\n\nMODEL_CHECKPOINT=Llama-2-13b-hf\nCONVERTED_CHECKPOINT=Llama-2-13b-hf-ckpt\nTOKENIZER=Llama-2-13b-hf\nLORA_ENGINE=Llama-2-13b-hf-engine\n\nDTYPE=float16\nTP=2\nPP=1\nMAX_LEN=1024\nMAX_BATCH=32\nNUM_LAYERS=40\nMAX_LORA_RANK=64\nNUM_LORA_MODS=7\nEOS_ID=2\n\nSOURCE_LORA=chinese-llama-2-lora-13b\nCPP_LORA=chinese-llama-2-lora-13b-cpp\n\nEG_DIR=/tmp/lora-eg\n\npython examples/llama/convert_checkpoint.py --model_dir ${MODEL_CHECKPOINT} \\\n                              --output_dir ${CONVERTED_CHECKPOINT} \\\n                              --dtype ${DTYPE} \\\n                              --tp_size ${TP} \\\n                              --pp_size 1\n\n${HOME}/.local/bin/trtllm-build \\\n    --checkpoint_dir ${CONVERTED_CHECKPOINT} \\\n    --output_dir ${LORA_ENGINE} \\\n    --max_batch_size ${MAX_BATCH} \\\n    --max_input_len $MAX_LEN \\\n    --max_seq_len $((2*${MAX_LEN})) \\\n    --gemm_plugin float16 \\\n    --lora_plugin float16 \\\n    --use_paged_context_fmha enable \\\n    --lora_target_modules attn_q attn_k attn_v attn_dense mlp_h_to_4h mlp_4h_to_h mlp_gate \\\n    --max_lora_rank ${MAX_LORA_RANK}\n\nNUM_LORAS=(8 16)\nNUM_REQUESTS=1024\n\npython examples/hf_lora_convert.py \\\n    -i $SOURCE_LORA \\\n    --storage-type $DTYPE \\\n    -o $CPP_LORA\n\nmkdir -p $EG_DIR/data\n```\n\n----------------------------------------\n\nTITLE: Building SDXL UNet TensorRT Engine for Single and Multi-GPU Setups\nDESCRIPTION: Commands for building a TensorRT engine for Stable Diffusion XL UNet model. Supports both single-GPU operation and distributed 2-GPU configuration using MPI.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/sdxl/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# 1 gpu\npython build_sdxl_unet.py --size 1024\n\n# 2 gpus\nmpirun -n 2 --allow-run-as-root python build_sdxl_unet.py --size 1024\n```\n\n----------------------------------------\n\nTITLE: Converting Skywork Model to TensorRT-LLM Checkpoint (FP16)\nDESCRIPTION: Converts the Hugging Face Skywork model to a TensorRT-LLM checkpoint in FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./Skywork-13B-base \\\n                --dtype float16 \\\n                --output_dir ./skywork-13b-base/trt_ckpt/fp16\n```\n\n----------------------------------------\n\nTITLE: Building FP8 Quantized Engine for Benchmark\nDESCRIPTION: Command to build an FP8 quantized TensorRT-LLM engine optimized using the specified dataset's input/output length statistics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-bench --model meta-llama/Llama-3.1-8B build --quantization FP8 --dataset /tmp/synthetic_128_128.txt\n```\n\n----------------------------------------\n\nTITLE: Running EXAONE Engine for Text Generation\nDESCRIPTION: Commands to run inference with the built EXAONE engine. These examples show running on single-GPU, multi-GPU with MPI, and using the summarize.py script for evaluation, all using the TensorRT-LLM optimized engines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py \\\n    --input_text \"When did the first world war end?\" \\\n    --max_output_len=100 \\\n    --tokenizer_dir $HF_MODEL_DIR \\\n    --engine_dir trt_engines/exaone/fp16/1-gpu\n\n# Run with 2 GPUs\nmpirun -n 2 --allow-run-as-root \\\n    python3 ../run.py \\\n    --input_text \"When did the first world war end?\" \\\n    --max_output_len=100 \\\n    --tokenizer_dir $HF_MODEL_DIR \\\n    --engine_dir trt_engines/exaone/fp16/2-gpu\n\npython ../summarize.py \\\n    --test_trt_llm \\\n    --data_type fp16 \\\n    --hf_model_dir $HF_MODEL_DIR \\\n    --engine_dir trt_engines/exaone/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Downloading Nemotron Models from HuggingFace\nDESCRIPTION: Commands to clone different versions of Nemotron models from HuggingFace repository.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Download nemotron-3-8b-base-4k\ngit clone https://huggingface.co/nvidia/nemotron-3-8b-base-4k\n\n# Download nemotron-3-8b-chat-4k-sft\ngit clone https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-sft\n\n# Download nemotron-3-8b-chat-4k-rlhf\ngit clone https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-rlhf\n```\n\n----------------------------------------\n\nTITLE: Building VAE TensorRT Engine\nDESCRIPTION: Command to build VAE decoder TensorRT engine for image generation acceleration\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dit/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython vae_decoder_trt.py --max_batch_size 8\n```\n\n----------------------------------------\n\nTITLE: Running Decoding with Prompt-Lookup\nDESCRIPTION: Example command for running decoding with Prompt-Lookup configuration. Shows usage with single GPU setup and specific prompt-lookup parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/prompt_lookup/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/llama\n\npython3 ../run.py \\\n    --tokenizer_dir <Path To Llama-v2-7B repo> \\\n    --engine_dir ./target-engine \\\n    --prompt_lookup_config=\"[10,2,[0]]\" \\\n    --max_output_len=256 \\\n    --kv_cache_enable_block_reuse \\\n    --input_text=\"How does Draft-Sampling work?\"\n```\n\n----------------------------------------\n\nTITLE: Generating TensorRT-LLM Checkpoint Config for Large Models\nDESCRIPTION: Uses the generate_checkpoint_config.py script to create configuration files for large GPT models (175B and 530B parameters) with specific architectures and GPU distributions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../generate_checkpoint_config.py --architecture GPTForCausalLM \\\n        --vocab_size 51200 \\\n        --hidden_size 12288 \\\n        --num_hidden_layers 96 \\\n        --num_attention_heads 96 \\\n        --dtype float16 \\\n        --tp_size 8 \\\n        --output_path gpt_175b/trt_ckpt/fp16/8-gpu/config.json\n\npython3 ../generate_checkpoint_config.py --architecture GPTForCausalLM \\\n        --vocab_size 51200 \\\n        --hidden_size 20480 \\\n        --num_hidden_layers 105 \\\n        --num_attention_heads 128 \\\n        --dtype float16 \\\n        --tp_size 16 \\\n        --output_path gpt_530b/trt_ckpt/fp16/16-gpu/config.json\n```\n\n----------------------------------------\n\nTITLE: Running T5 Encoder-Decoder Benchmark with 4-GPU Configuration\nDESCRIPTION: Commands for benchmarking T5 encoder-decoder model using 4 GPUs. Includes dataset preparation and benchmark execution steps.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmpirun --allow-run-as-root -np 4 ./benchmarks/gptManagerBenchmark \\\n    --encoder_engine_dir ../../examples/enc_dec/tmp/trt_engines/t5-small-4gpu/bfloat16/encoder \\\n    --decoder_engine_dir ../../examples/enc_dec/tmp/trt_engines/t5-small-4gpu/bfloat16/decoder \\\n    --dataset cnn_dailymail.json\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace Diffusers DiT Model\nDESCRIPTION: Commands to convert DiT model from HuggingFace diffusers format to TensorRT-LLM with FP8 quantization\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dit/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --fp8_linear --diffusers_dit --timm_ckpt=</path/to/quantized_ckpt> --output_dir=tllm_checkpoint_fp8\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_fp8/ \\\n             --output_dir ./engine_outputs_fp8/ \\\n             --max_batch_size 8 \\\n             --remove_input_padding disable \\\n             --bert_attention_plugin disable\n```\n\n----------------------------------------\n\nTITLE: Installing Visualization Dependencies\nDESCRIPTION: Commands to install required packages for visualization features\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install opencv-python==4.5.5.64\npip install opencv-python-headless==4.5.5.64\npip install zmq\npip install request\n```\n\n----------------------------------------\n\nTITLE: Running Tests with pytest\nDESCRIPTION: Command to execute the tests specified in the generated test list file using pytest. The command specifies the test list file and the output directory for test logs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/test-db/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest -v --test-list=/TensorRT-LLM/src/l0_e2e.txt --output-dir=/tmp/logs\n```\n\n----------------------------------------\n\nTITLE: Generating Audio Encoder TensorRT Engine\nDESCRIPTION: Command to build the TensorRT engine for the audio encoder component with FP16 precision and specified batch size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ENGINE_DIR=\"./trt_engines/qwen2audio/fp16\"\npython3 ../multimodal/build_multimodal_engine.py --model_type qwen2_audio --model_path $MODEL_PATH --max_batch_size 32 --output_dir ${ENGINE_DIR}/audio\n```\n\n----------------------------------------\n\nTITLE: Converting and Splitting BART Weights\nDESCRIPTION: Python command to convert and split BART weights, specifying the model type, directories, and precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport INFERENCE_PRECISION=\"float16\"\npython convert_checkpoint.py --model_type bart \\\n                --model_dir tmp/hf_models/bart-large-cnn \\\n                --output_dir tmp/trt_models/bart-large-cnn/${INFERENCE_PRECISION} \\\n                --tp_size 1 \\\n                --pp_size 1 \\\n                --dtype ${INFERENCE_PRECISION}\n```\n\n----------------------------------------\n\nTITLE: FP8 Post-Training Quantization for Mixtral\nDESCRIPTION: Commands to quantize Mixtral into FP8 format and build TensorRT engines with FP8 quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py --model_dir ./Mixtral-8x7B-v0.1 \\\n                                   --dtype float16 \\\n                                   --qformat fp8 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir ./tllm_checkpoint_mixtral_2gpu \\\n                                   --calib_size 512 \\\n                                   --tp_size 2\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_2gpu \\\n             --output_dir ./engine_outputs \\\n             --workers 2\n```\n\n----------------------------------------\n\nTITLE: Preparing Synthetic Dataset for Llama-3.1-8B Benchmark\nDESCRIPTION: Command to generate a synthetic dataset with uniform distribution of prompts of 128 input tokens and 128 output tokens for benchmarking Llama-3.1-8B model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/cpp/prepare_dataset.py --stdout --tokenizer meta-llama/Llama-3.1-8B token-norm-dist --input-mean 128 --output-mean 128 --input-stdev 0 --output-stdev 0 --num-requests 3000 > /tmp/synthetic_128_128.txt\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine\nDESCRIPTION: Command to build the TensorRT engine with specified precision and parallel workers configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/granite/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tmp/tllm_checkpoints/${ENGINE} \\\n             --output_dir ./tmp/trt_engines/${ENGINE} \\\n             --gpt_attention_plugin ${PREC_RAW} \\\n             --gemm_plugin ${PREC_RAW} \\\n             --workers ${TP}\n```\n\n----------------------------------------\n\nTITLE: Converting to FP8 PTQ Format\nDESCRIPTION: Quantization script execution for converting to FP8 format with static quantization and calibration settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/granite/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nPREC_QUANT=\"fp8\"\nENGINE=\"${HF_MODEL}_${PREC_QUANT}_tp${TP}\"\npython ../quantization/quantize.py --model_dir tmp/hf_checkpoints/${HF_MODEL} \\\n                                   --dtype ${PREC_RAW} \\\n                                   --qformat ${PREC_QUANT} \\\n                                   --kv_cache_dtype ${PREC_QUANT} \\\n                                   --output_dir tmp/tllm_checkpoints/${ENGINE} \\\n                                   --batch_size 1 \\\n                                   --calib_size 128 \\\n                                   --tp_size ${TP}\n```\n\n----------------------------------------\n\nTITLE: Example Output of trtllm-bench Build Command\nDESCRIPTION: Sample output when building an FP8 quantized Llama 3.1 engine with a synthetic dataset. The output shows dataset details, engine build information, configuration parameters, and the engine building process steps including downloading and loading the HuggingFace model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nuser@387b12598a9e:/scratch/code/trt-llm/tekit_2025$ trtllm-bench --model meta-llama/Llama-3.1-8B build --dataset /tmp/synthetic_128_128.txt --quantization FP8\n[TensorRT-LLM] TensorRT-LLM version: 0.17.0\n[01/18/2025-00:55:14] [TRT-LLM] [I] Found dataset.\n[01/18/2025-00:55:14] [TRT-LLM] [I]\n===========================================================\n= DATASET DETAILS\n===========================================================\nMax Input Sequence Length:      128\nMax Output Sequence Length:     128\nMax Sequence Length:    256\nTarget (Average) Input Sequence Length: 128\nTarget (Average) Output Sequence Length:        128\nNumber of Sequences:    3000\n===========================================================\n\n\n[01/18/2025-00:55:14] [TRT-LLM] [I] Max batch size and max num tokens are not provided, use tuning heuristics or pre-defined setting from trtllm-bench.\n[01/18/2025-00:55:14] [TRT-LLM] [I] Estimated total available memory for KV cache: 132.37 GB\n[01/18/2025-00:55:14] [TRT-LLM] [I] Estimated total KV cache memory: 125.75 GB\n[01/18/2025-00:55:14] [TRT-LLM] [I] Estimated max number of requests in KV cache memory: 8048.16\n[01/18/2025-00:55:14] [TRT-LLM] [I] Estimated max batch size (after fine-tune): 4096\n[01/18/2025-00:55:14] [TRT-LLM] [I] Estimated max num tokens (after fine-tune): 8192\n[01/18/2025-00:55:14] [TRT-LLM] [I] Set dtype to bfloat16.\n[01/18/2025-00:55:14] [TRT-LLM] [I] Set multiple_profiles to True.\n[01/18/2025-00:55:14] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n[01/18/2025-00:55:14] [TRT-LLM] [I] Set use_fp8_context_fmha to True.\n[01/18/2025-00:55:14] [TRT-LLM] [I]\n===========================================================\n= ENGINE BUILD INFO\n===========================================================\nModel Name:             meta-llama/Llama-3.1-8B\nModel Path:             None\nWorkspace Directory:    /tmp\nEngine Directory:       /tmp/meta-llama/Llama-3.1-8B/tp_1_pp_1\n\n===========================================================\n= ENGINE CONFIGURATION DETAILS\n===========================================================\nMax Sequence Length:            256\nMax Batch Size:                 4096\nMax Num Tokens:                 8192\nQuantization:                   FP8\nKV Cache Dtype:                 FP8\n===========================================================\n\nLoading Model: [1/3]    Downloading HF model\nDownloaded model to /data/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\nTime: 0.321s\nLoading Model: [2/3]    Loading HF model to memory\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:59<00:00, 14.79s/it]\nGenerating train split: 100%|████████████████████████████████████████████████████████████████████████████████████| 287113/287113 [00:06<00:00, 41375.57 examples/s]\nGenerating validation split: 100%|█████████████████████████████████████████████████████████████████████████████████| 13368/13368 [00:00<00:00, 41020.63 examples/s]\nGenerating test split: 100%|███████████████████████████████████████████████████████████████████████████████████████| 11490/11490 [00:00<00:00, 41607.11 examples/s]\nInserted 675 quantizers\n/usr/local/lib/python3.12/dist-packages/modelopt/torch/quantization/model_quant.py:71: DeprecationWarning: forward_loop should take model as argument, but got forward_loop without any arguments. This usage will be deprecated in future versions.\n  warnings.warn(\nDisable lm_head quantization for TRT-LLM export due to deployment limitations.\ncurrent rank: 0, tp rank: 0, pp rank: 0\nTime: 122.568s\nLoading Model: [3/3]    Building TRT-LLM engine\n/usr/local/lib/python3.12/dist-packages/tensorrt/__init__.py:85: DeprecationWarning: Context managers for TensorRT types are deprecated. Memory will be freed automatically when the reference count reaches 0.\n  warnings.warn(\nTime: 53.820s\nLoading model done.\nTotal latency: 176.709s\n\n<snip verbose logging>\n\n===========================================================\nENGINE SAVED: /tmp/meta-llama/Llama-3.1-8B/tp_1_pp_1\n===========================================================\n```\n\n----------------------------------------\n\nTITLE: Running Qwen Model with INT4 Weight-Only Quantization\nDESCRIPTION: Command to run inference on a TensorRT-LLM Qwen model with INT4 weight-only quantization. It demonstrates efficient inference with minimal precision loss using 4-bit weights.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --input_text \"你好，请问你叫什么？\" \\\n                  --max_output_len=50 \\\n                  --tokenizer_dir ./tmp/Qwen/7B/ \\\n                  --engine_dir=./tmp/Qwen/7B/trt_engines/int4_weight_only/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Nemotron-NAS Model\nDESCRIPTION: Builds the TensorRT engine using a single GPU and FP16 precision with paged KV cache.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron_nas/README.md#2025-04-07_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ntrtllm-build --checkpoint_dir $TRT_CHECKPOINT_DIR \\\n                --output_dir $TRT_ENGINE_DIR \\\n                --gemm_plugin auto \\\n                --kv_cache_type paged\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Hooks\nDESCRIPTION: Commands for running pre-commit hooks on the entire repository or specific changes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Single GPU Inference\nDESCRIPTION: Command to run inference on a single GPU using the built engine\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --input_text \"What's new between ChatGLM3-6B and ChatGLM2-6B?\" \\\n        --max_output_len 50 \\\n        --tokenizer_dir glm_4_9b \\\n        --engine_dir trt_engines/glm_4_9b/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Downloading Mixtral 8x22B Weights from Hugging Face\nDESCRIPTION: Commands to download the Mixtral 8x22B weights from Hugging Face using Git LFS.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/mistralai/Mixtral-8x22B-v0.1\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantization for DeepSeek-V3\nDESCRIPTION: Command to create a JSON configuration file for setting up FP8 quantization for the DeepSeek-V3 model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\necho -e \"{\\\"quantization\\\": {\\\"quant_algo\\\": \\\"FP8_BLOCK_SCALES\\\", \\\"kv_cache_quant_algo\\\": null}}\" > <YOUR_MODEL_DIR>/hf_quant_config.json\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with FP8 Quantization\nDESCRIPTION: Command to build a TensorRT-LLM engine for Llama-3.1-8B with FP8 quantization using a previously generated synthetic dataset.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-bench --model meta-llama/Llama-3.1-8B build --dataset /tmp/synthetic_128_128.txt --quantization FP8\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Release from Jenkins Image\nDESCRIPTION: Command to build a release image based on Jenkins development image with specific CUDA architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker trtllm_build CUDA_ARCHS=\"80-real;90-real\"\n```\n\n----------------------------------------\n\nTITLE: Building Single-GPU FP16 EXAONE Engine\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM format and build a single-GPU FP16 engine for EXAONE. This process uses llama's convert_checkpoint.py for conversion followed by trtllm-build for engine creation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py \\\n    --model_dir $HF_MODEL_DIR \\\n    --output_dir trt_models/exaone/fp16/1-gpu \\\n    --dtype float16\n\ntrtllm-build \\\n    --checkpoint_dir trt_models/exaone/fp16/1-gpu \\\n    --output_dir trt_engines/exaone/fp16/1-gpu \\\n    --gemm_plugin auto\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT-LLM with PyTorch Quantized Model\nDESCRIPTION: Example showing how to load and use a quantized Llama model using the TensorRT-LLM PyTorch backend. The code demonstrates initialization of LLM class and text generation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm._torch import LLM\nllm = LLM(model='nvidia/Llama-3.1-8B-Instruct-FP8')\nllm.generate(\"Hello, my name is\")\n```\n\n----------------------------------------\n\nTITLE: Running Latency Benchmark with TensorRT-LLM\nDESCRIPTION: Command to run latency benchmarks with batch size 1, including warmup period and configurable number of requests. Measures detailed latency metrics including time-to-first-token and inter-token latency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/benchmarking-default-performance.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-bench \\\n--model /path/to/hf/Llama-3.3-70B-Instruct/ \\\nlatency \\\n--dataset /path/to/dataset/synthetic_2048_2048_1000.txt \\\n--num-requests 100 \\\n--warmup 10 \\\n--engine_dir /path/to/engines/baseline\n```\n\n----------------------------------------\n\nTITLE: Converting InternLM2 7B with 2-way Tensor Parallelism\nDESCRIPTION: Converts the HuggingFace InternLM2 7B model to TensorRT-LLM format with 2-way tensor parallelism for multi-GPU execution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm2-chat-7b/ \\\n                --dtype float16 \\\n                --output_dir ./internlm2-chat-7b/trt_engines/fp16/2-gpu/ \\\n                --tp_size 2\n```\n\n----------------------------------------\n\nTITLE: EAGLE Summarization with Single GPU\nDESCRIPTION: Command for running EAGLE decoding for summarization tasks using a single GPU with specific choice configurations and FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --engine_dir ./tmp/eagle/7B/trt_engines/fp16/1-gpu/ \\\n                       --hf_model_dir ./vicuna-7b-v1.3/ \\\n                       --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                       --test_trt_llm \\\n                       --data_type fp16 \\\n                       --eagle_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                       --batch_size 1\n```\n\n----------------------------------------\n\nTITLE: Latency Benchmark Sample Output\nDESCRIPTION: Example output from running the latency benchmark, showing detailed metrics including engine details, latency measurements, and acceptance rates for speculative execution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/benchmarking-default-performance.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n===========================================================\n= ENGINE DETAILS\n===========================================================\nModel:\t\t\t/scratch/Llama-3.3-70B-Instruct/\nEngine Directory:\t/scratch/grid_search_engines/baseline\nTensorRT-LLM Version:\t0.16.0\nDtype:\t\t\tbfloat16\nKV Cache Dtype:\t\tNone\nQuantization:\t\tNone\nMax Input Length:\t1024\nMax Sequence Length:\t131072\n\n===========================================================\n= WORLD + RUNTIME INFORMATION\n===========================================================\nTP Size:\t\t4\nPP Size:\t\t1\nMax Runtime Batch Size:\t1\nMax Runtime Tokens:\t8192\nScheduling Policy:\tGuaranteed No Evict\nKV Memory Percentage:\t90.00%\n\n===========================================================\n= GENERAL OVERVIEW\n===========================================================\nNumber of requests:\t\t100\nAverage Input Length (tokens):\t2048.0000\nAverage Output Length (tokens):\t2048.0000\nAverage request latency (ms):\t63456.0704\n\n===========================================================\n= THROUGHPUT OVERVIEW\n===========================================================\nRequest Throughput (req/sec):\t\t  0.0158\nTotal Token Throughput (tokens/sec):\t  32.2742\nGeneration Token Throughput (tokens/sec): 32.3338\n\n===========================================================\n= LATENCY OVERVIEW\n===========================================================\nTotal Latency (ms):\t\t  6345624.0554\nAverage time-to-first-token (ms): 147.7502\nAverage inter-token latency (ms): 30.9274\nAcceptance Rate (Speculative):\t  1.00\n\n===========================================================\n= GENERATION LATENCY BREAKDOWN\n===========================================================\nMIN (ms): 63266.8804\nMAX (ms): 63374.7770\nAVG (ms): 63308.3201\nP90 (ms): 63307.1885\nP95 (ms): 63331.7136\nP99 (ms): 63374.7770\n\n===========================================================\n= ACCEPTANCE BREAKDOWN\n===========================================================\nMIN: 1.00\nMAX: 1.00\nAVG: 1.00\nP90: 1.00\nP95: 1.00\nP99: 1.00\n\n===========================================================\n```\n\n----------------------------------------\n\nTITLE: Running FP8 Inference for Gemma 2B-IT Model Using Keras Checkpoint\nDESCRIPTION: This script demonstrates FP8 inference on Gemma 2B-IT by converting a Keras checkpoint to TensorRT-LLM format. It enables FP8 for both weights and KV cache without calibration. The process involves cloning the model, converting the checkpoint to a unified format, building the engine, and running evaluations with summarize.py and mmlu.py.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:google/gemma-2b-it-keras\nGIT_LFS_SKIP_SMUDGE=1 git clone git@hf.co:google/gemma-2b-it-flax # clone tokenizer model\ncd gemma-2b-it-flax\ngit lfs pull -I tokenizer.model\n\nCKPT_PATH=gemma-2b-it-keras\nUNIFIED_CKPT_PATH=/tmp/checkpoints/tmp_2b_en_tensorrt_llm/fp8/tp1/\nENGINE_PATH=/tmp/gemma/2B/fp8/1-gpu/\nVOCAB_FILE_PATH=gemma-2b-it-flax/tokenizer.model\n\npython3 ./convert_checkpoint.py \\\n    --ckpt-type keras \\\n    --model-dir ${CKPT_PATH} \\\n    --dtype bfloat16 \\\n    --world-size 1 \\\n    --enable_fp8 \\\n    --fp8_kv_cache \\\n    --output-model-dir ${UNIFIED_CKPT_PATH}\n\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_PATH}\n\npython3 ../summarize.py --test_trt_llm \\\n                      --vocab_file ${VOCAB_FILE_PATH} \\\n                      --engine_dir ${ENGINE_PATH} \\\n                      --batch_size 8 \\\n                      --max_ite 5\n\n[02/08/2024-10:37:15] [TRT-LLM] [I] TensorRT-LLM (total latency: 3.116227149963379 sec)\n[02/08/2024-10:37:15] [TRT-LLM] [I] TensorRT-LLM (total output tokens: 2419)\n[02/08/2024-10:37:15] [TRT-LLM] [I] TensorRT-LLM (tokens per second: 776.259201781368)\n[02/08/2024-10:37:15] [TRT-LLM] [I] TensorRT-LLM beam 0 result\n[02/08/2024-10:37:15] [TRT-LLM] [I]   rouge1 : 20.206082692133098\n[02/08/2024-10:37:15] [TRT-LLM] [I]   rouge2 : 5.902141189518428\n[02/08/2024-10:37:15] [TRT-LLM] [I]   rougeL : 15.403458457907643\n[02/08/2024-10:37:15] [TRT-LLM] [I]   rougeLsum : 17.44535527417846\n\npython3 ../mmlu.py --test_trt_llm \\\n                  --vocab_file ${VOCAB_FILE_PATH} \\\n                  --engine_dir ${ENGINE_PATH}\n\nAverage accuracy 0.390 - social sciences\nAverage accuracy 0.405 - other (business, health, misc.)\nAverage accuracy: 0.356\n```\n\n----------------------------------------\n\nTITLE: Converting RecurrentGemma-2b-it-flax JAX Checkpoint to TensorRT-LLM Format\nDESCRIPTION: This command converts the recurrentgemma-2b-it-flax model from JAX format to TensorRT-LLM checkpoint format with BF16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it-flax\nCKPT_2B_IT_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-it-flax/2b-it\nUNIFIED_CKPT_2B_IT_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-it-flax/trt_ckpt/bf16/1-gpu/\npython convert_checkpoint.py --model_dir ${CKPT_2B_IT_FLAX_PATH} \\\n                             --ckpt_type jax \\\n                             --dtype bfloat16 \\\n                             --output_dir ${UNIFIED_CKPT_2B_IT_FLAX_PATH}\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Tasks\nDESCRIPTION: Command for running summarization tasks with both HuggingFace and TensorRT-LLM implementations, including accuracy checking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/prompt_lookup/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/llama\n\npython ../summarize.py \\\n    --test_hf \\\n    --test_trt_llm \\\n    --check_accuracy \\\n    --hf_model_dir <Path To Llama-v2-7B repo> \\\n    --engine_dir ./target-engine \\\n    --batch_size=1 \\\n    --prompt_lookup_config=\"[10,2,[0]]\" \\\n    --kv_cache_enable_block_reuse\n```\n\n----------------------------------------\n\nTITLE: Cloning TensorRT-LLM Repository\nDESCRIPTION: Instructions for cloning the TensorRT-LLM repository from GitHub and navigating to the project directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/TensorRT-LLM.git\ncd TensorRT-LLM\n```\n\n----------------------------------------\n\nTITLE: Collecting Plugin Source Files in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet collects all C++ and CUDA source files in the current directory and appends them to the PLUGIN_SOURCES variable. The updated PLUGIN_SOURCES variable is then propagated to the parent scope of the CMake configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp *.cu)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Build Time Benchmarks\nDESCRIPTION: Examples of running the build_time_benchmark.py script with different configurations including benchmarking all built-in models, loading weights, testing HuggingFace models, local models, and using managed weights option. The script provides various options for performance testing and development assistance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/microbenchmarks/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# example 1: offline benmark for all the built-in models, see --help for all the options\npython ./build_time_benchmark.py --model ALL\n\n# By default, the benmark don't load the weights to save benchmark time, load the weights to test the TRT-LLM load and convert time\n# WARNING: this can takes very long time if the model is large, or if you use a online HF model id since it can download the weights\npython ./build_time_benchmark.py --model ALL --load\n\n# example 2: benchmark a HF model model w/o downloading the model locally in advance\npython ./build_time_benchmark.py --model \"TinyLlama/TinyLlama_v1.1\" # no weights loading\npython ./build_time_benchmark.py --model \"openai-community/gpt2\" --load # with weights loading\n\n# example 3: benchmark a local download HF model\npython  ./build_time_benchmark.py --model /home/scratch.trt_llm_data/llm-models/falcon-rw-1b/\n\n# example 4: benchmark one model with managed weights option, with verbose option\npython ./build_time_benchmark.py --model llama2-70b.TP4 --managed_weights -v\n\n# example 5: see help\npython ./build_time_benchmark.py --help\n```\n\n----------------------------------------\n\nTITLE: Building model_spec.so Manually\nDESCRIPTION: Command for manually building the model_spec.so library when engine building has issues finding it.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake -C cpp/build/ modelSpec\n```\n\n----------------------------------------\n\nTITLE: Describing Sanity Performance Check in TensorRT-LLM (Markdown)\nDESCRIPTION: This markdown snippet outlines the background and usage scenarios for the sanity performance check mechanism in TensorRT-LLM. It explains how the base_perf.csv file is used as a baseline and how the sanity_perf_check.py script detects performance regressions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/defs/perf/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Background\nThe sanity perf check mechanism is the way of perf regression detection for L0 testing. We create the base_perf.csv which consists of the several models' perf baseline and use the sanity_perf_check.py to detect the perf regression.\n# Usage\nThere're four typical scenarios for sanity perf check feature.\n\n1. The newly added MR doesn't impact the models' perf, the perf check will pass w/o exception.\n2. The newly added MR introduces the new model into perf model list. The sanity check will trigger the exception and the author of this MR needs to add the perf into base_perf.csv.\n3. The newly added MR improves the existed models' perf and the MR author need to refresh the base_perf.csv data w/ new baseline.\n4. The newly added MR introduces the perf regression and the MR author needs to fix the issue and rerun the pipeline.\n```\n\n----------------------------------------\n\nTITLE: Converting to INT8 Weight-Only Checkpoint\nDESCRIPTION: Optional command to convert the model checkpoint to INT8 weight-only format for reduced memory usage.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../qwen/convert_checkpoint.py --model_dir=$MODEL_PATH \\\n        --dtype=float16 \\\n        --use_weight_only \\\n        --weight_only_precision=int8 \\\n        --output_dir=./tllm_checkpoint_1gpu_fp16_wo8\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for RecurrentGemma-2b-it (INT4 AWQ)\nDESCRIPTION: This command builds a TensorRT engine from the RecurrentGemma-2b-it INT4 AWQ checkpoint using trtllm-build with auto GEMM plugin selection.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it INT4 AWQ with INT8 kv cache\nENGINE_2B_IT_INT4_AWQ_PATH=./recurrentgemma_model/recurrentgemma-2b-it/trt_engines/int4_awq/1-gpu/\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_2B_IT_INT4_AWQ_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_2B_IT_INT4_AWQ_PATH}\n```\n\n----------------------------------------\n\nTITLE: Basic Weight Loader Usage Example\nDESCRIPTION: Example showing how to use the ModelWeightsLoader for LLaMA models with default settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Using the model weights loader for LLaMA\nfrom tensorrt_llm.models.model_weights_loader import ModelWeightsLoader\nloader = ModelWeightsLoader(external_checkpoint_dir)\nloader.generate_tllm_weights(trtllm_model)\n```\n\n----------------------------------------\n\nTITLE: Running the TensorRT-LLM Python Chat Application\nDESCRIPTION: Command to run the Python chat application with a TensorRT-LLM model. It requires specifying the model directory, tokenizer path, and tensor parallelism size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/apps/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./chat.py --model <model_dir> --tokenizer <tokenizer_path> --tp_size <tp_size>\n```\n\n----------------------------------------\n\nTITLE: Building InternLM2 20B with INT8 Weight-Only Quantization\nDESCRIPTION: Builds the TensorRT engine for the INT8 weight-only quantized InternLM2 20B model with FP16 activation precision for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./internlm2-chat-20b/w8a16 \\\n              --output_dir ./engine_outputs \\\n              --gemm_plugin float16 \\\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine\nDESCRIPTION: Command to build TensorRT-LLM engine with specific configurations for batch size and sequence length\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir=./tllm_checkpoint_1gpu \\\n                 --gemm_plugin=float16 --gpt_attention_plugin=float16 \\\n                 --max_input_len=2048 --max_seq_len=3072 \\\n                 --max_batch_size=8 --max_prompt_embedding_table_size=2048 \\\n                 --remove_input_padding=enable \\\n                 --output_dir=./trt_engines/Qwen-VL-7B-Chat\n```\n\n----------------------------------------\n\nTITLE: Enabling NVLink for KV cache transfer in TensorRT-LLM with UCX <=1.17\nDESCRIPTION: This snippet demonstrates how to set environment variables to enable NVLink for KV cache transfer in TensorRT-LLM when using UCX version 1.17 or lower.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nUCX_RNDV_FRAG_MEM_TYPE=cuda\nUCX_MEMTYPE_CACHE=n\n```\n\n----------------------------------------\n\nTITLE: Deserializing a TensorRT Engine with Custom Plugins\nDESCRIPTION: Shows how to deserialize a TensorRT engine that uses custom plugins by importing the plugin package before creating the session.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/python_plugin/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm.runtime.session import Session, TensorInfo\n\nimport openai_triton_plugin  # isort: skip\n\nif __name__ == \"__main__\":\n\n    def run_engine(dtype):\n        output_dir = Path('tmp') / torch_dtype_to_str(dtype)\n\n        engine_path = output_dir / \"lookup.engine\"\n\n        with engine_path.open('rb') as f:\n            session = Session.from_serialized_engine(f.read())\n```\n\n----------------------------------------\n\nTITLE: Evaluating DeepSeek-V2 Model on Summarization Task\nDESCRIPTION: Command to evaluate the model's summarization capabilities using the summarize.py script. This compares TensorRT-LLM implementation against the original Hugging Face model in terms of ROUGE scores and generation speed.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmpirun --allow-run-as-root -n 8 python ../../../summarize.py --engine_dir ./trtllm_engines/deepseek_v2/bf16/tp8-sel4096-isl2048-bs4 \\\n                       --hf_model_dir ./DeepSeek-V2 \\\n                       --data_type bfloat16 \\\n                       --batch_size 1 \\\n                       --test_trt_llm \\\n                       --test_hf\n```\n\n----------------------------------------\n\nTITLE: Setting NCCL Performance Environment Variable for AMD Platforms\nDESCRIPTION: Environment variable configuration to work around NCCL 2.23.4 performance issues on AMD-based CPU platforms affecting multi-GPU operations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport NCCL_P2P_LEVEL=SYS\n```\n\n----------------------------------------\n\nTITLE: EAGLE Summarization with Multi-GPU\nDESCRIPTION: Command for running EAGLE decoding across 4 GPUs with MPI, maintaining the same configuration as single-GPU but distributed across multiple devices.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 4 --allow-run-as-root --oversubscribe \\\n    python ../summarize.py --engine_dir ./tmp/eagle/7B/trt_engines/fp16/4-gpu/ \\\n                           --hf_model_dir ./vicuna-7b-v1.3/ \\\n                           --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                           --test_trt_llm \\\n                           --data_type fp16 \\\n                           --eagle_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                           --batch_size 1\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Baichuan TensorRT-LLM\nDESCRIPTION: Command to install the necessary Python packages for building and running Baichuan models with TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running MMLU Evaluation\nDESCRIPTION: Commands for downloading and running evaluation on the MMLU dataset\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmkdir data\nwget https://people.eecs.berkeley.edu/~hendrycks/data.tar -O data/mmlu.tar\ntar -xf data/mmlu.tar -C data\nmv data/data data/mmlu\n\npython3 ../mmlu.py --test_trt_llm \\\n                  --vocab_file ${VOCAB_FILE_PATH} \\\n                  --engine_dir ${ENGINE_PATH}\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Multimodal Examples with Image and Video Inputs\nDESCRIPTION: Commands for running multimodal models with TensorRT-LLM, supporting both image and video inputs with different batching configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/pytorch/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# default inputs\npython3 quickstart_multimodal.py --model_dir Efficient-Large-Model/NVILA-8B --modality image [--use_cuda_graph]\n\n# user inputs\n# supported modes:\n# (1) N prompt, N media (N requests are in-flight batched)\n# (2) 1 prompt, N media\n# Note: media should be either image or video. Mixing image and video is not supported.\npython3 quickstart_multimodal.py --model_dir Efficient-Large-Model/NVILA-8B --modality video --prompt \"Tell me what you see in the video briefly.\" \"Describe the scene in the video briefly.\" --media \"https://huggingface.co/datasets/Efficient-Large-Model/VILA-inference-demos/resolve/main/OAI-sora-tokyo-walk.mp4\" \"https://huggingface.co/datasets/Efficient-Large-Model/VILA-inference-demos/resolve/main/world.mp4\" --max_tokens 128 [--use_cuda_graph]\n```\n\n----------------------------------------\n\nTITLE: Setting Max Tokens in Paged KV Cache for TensorRT-LLM\nDESCRIPTION: This code snippet shows how to set the max_tokens parameter in the KV cache configuration for TensorRT-LLM. It demonstrates the alternative to using free_gpu_memory_fraction when configuring the KV cache.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-runtime-flags.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    kv_cache_config = KvCacheConfig(max_tokens=<number of tokens>)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with ReDrafter Model\nDESCRIPTION: Command to run inference using the built ReDrafter TensorRT engine. The example shows how to provide input text and generate output with the speculative decoding capabilities of ReDrafter.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/redrafter/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir ./tmp/redrafter/7B/trt_engines/fp16/1-gpu/ \\\n                 --tokenizer_dir ./vicuna-7b-v1.3/ \\\n                 --max_output_len=100 \\\n                 --input_text \"Once upon\" \"The basic idea of a Transformer model is\"\n```\n\n----------------------------------------\n\nTITLE: JSON Dataset Format for TensorRT-LLM Benchmarking with Text Prompts\nDESCRIPTION: Example of JSON entries for benchmark dataset using human-readable prompts with task_id, prompt text, and desired output token count.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"task_id\": 1, \"prompt\": \"Generate an infinite response to the following: This is the song that never ends, it goes on and on my friend.\", \"output_tokens\": 1000}\n{\"task_id\": 2, \"prompt\": \"Generate an infinite response to the following: Na, na, na, na\", \"output_tokens\": 1000}\n```\n\n----------------------------------------\n\nTITLE: Running Logits Post Processor Example\nDESCRIPTION: Command for running the logits post processor example which demonstrates generating JSON structured output using the LogitsPostProcessor API. It takes tokenizer path, engine path, and batch size parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 example_logits_processor.py -t <tokenizer_path> -e <engine_path> --batch_size 8\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with InternLM2 7B FP16\nDESCRIPTION: Performs text summarization task using both TensorRT-LLM and HuggingFace implementations of InternLM2 7B with FP16 precision for comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --test_trt_llm --test_hf \\\n                       --hf_model_dir ./internlm2-chat-7b/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./engine_outputs\n```\n\n----------------------------------------\n\nTITLE: Converting INT4-GPTQ Quantized Qwen Model\nDESCRIPTION: Command to convert a pre-quantized Qwen-7B-Chat-Int4 model using INT4-GPTQ quantization. It processes the model for weight-only quantization with per-group optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./tmp/Qwen-7B-Chat-Int4 \\\n                              --output_dir ./tllm_checkpoint_1gpu_gptq \\\n                              --dtype float16 \\\n                              --use_weight_only \\\n                              --weight_only_precision int4_gptq \\\n                              --per_group\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU direct RDMA for inter-node KV cache transfer in TensorRT-LLM (Method 1)\nDESCRIPTION: This snippet shows the first method to enable GPU direct RDMA for inter-node KV cache transfer in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nUCX_RNDV_FRAG_MEM_TYPE=cuda\nUCX_MEMTYPE_CACHE=n\n```\n\n----------------------------------------\n\nTITLE: Building INT4 AWQ EXAONE Engine\nDESCRIPTION: Commands to apply INT4 Activation-aware Weight Quantization (AWQ) to EXAONE using NVIDIA Modelopt. AWQ preserves important weight values during quantization by analyzing activation patterns, resulting in better accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py \\\n    --model_dir $HF_MODEL_DIR \\\n    --dtype float16 \\\n    --qformat int4_awq \\\n    --output_dir trt_models/exaone/int4_awq/1-gpu\n\ntrtllm-build \\\n    --checkpoint_dir trt_models/exaone/int4_awq/1-gpu \\\n    --output_dir trt_engines/exaone/int4_awq/1-gpu \\\n    --gemm_plugin auto\n```\n\n----------------------------------------\n\nTITLE: MPT-30B Conversion and Deployment\nDESCRIPTION: Convert and build MPT-30B model with 4-way tensor parallelism and run inference\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir mosaicml/mpt-30b --output_dir ./ckpts/mpt-30b/fp16_tp4/ --tp_szie 4 --dtype float16\n\ntrtllm-build --checkpoint_dir ./ckpts/mpt-30b/fp16_tp4 \\\n             --max_batch_size 32 \\\n             --max_input_len 1024 \\\n             --max_seq_len 1536 \\\n             --gemm_plugin float16 \\\n             --workers 4 \\\n             --output_dir ./trt_engines/mpt-30b/fp16_tp4\n\nmpirun -n 4 --allow-run-as-root \\\n    python ../../../run.py --max_output_len 10 \\\n                     --engine_dir ./trt_engines/mpt-30b/fp16/4-gpu/ \\\n                     --tokenizer_dir mosaicml/mpt-30b\n```\n\n----------------------------------------\n\nTITLE: Running INT4 GPTQ Pipeline\nDESCRIPTION: Command to run the Qwen-VL pipeline with INT4 GPTQ weight-only quantization\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 run.py \\\n        --tokenizer_dir=./Qwen-VL-Chat \\\n        --qwen_engine_dir=./trt_engines/Qwen-VL-7B-Chat \\\n        --vit_engine_path=./plan/visual_encoder/visual_encoder_fp16.plan \\\n        --images_path='{\"image\": \"./pics/demo.jpeg\"}'\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Runtime Error Example Output\nDESCRIPTION: Example error output when there's a mismatch between engine building and runtime configurations, such as incorrect input shapes or code changes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\nunexpected shape for input 'XXX' for model 'YYY'. Expected [-1,-1,-1], got [8,16]. NOTE: Setting a non-zero max_batch_size in the model config requires a batch dimension to be prepended to each input shape. If you want to specify the full shape including the batch dim in your input dims config, try setting max_batch_size to zero. See the model configuration docs for more info on max_batch_size.\n\n[TensorRT-LLM][ERROR] Assertion failed: Tensor 'input_ids' has invalid shape (8192), expected (-1) (/code/tensorrt_llm/cpp/tensorrt_llm/runtime/tllmRuntime.cpp:149)\n\nRuntimeError: Sizes of tensors must match except in dimension 0. Expected size 8192 but got size 1024 for tensor number 1 in the list.\n```\n\n----------------------------------------\n\nTITLE: Configuring the TensorRT-LLM Executor Static Library Target\nDESCRIPTION: Creates the static library target for the TensorRT-LLM executor and sets its properties, including C++ standard version, position independent code, and CUDA device symbol resolution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/executor/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${EXECUTOR_STATIC_TARGET} STATIC ${SRCS})\nset_target_properties(\n  ${EXECUTOR_STATIC_TARGET}\n  PROPERTIES CXX_STANDARD \"17\" CXX_STANDARD_REQUIRED \"YES\" CXX_EXTENSIONS \"NO\"\n             POSITION_INDEPENDENT_CODE ON)\n\nset_property(TARGET ${EXECUTOR_STATIC_TARGET}\n             PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\nset(TOP_LEVEL_DIR \"${PROJECT_SOURCE_DIR}/..\")\ntarget_compile_definitions(${EXECUTOR_STATIC_TARGET}\n                           PUBLIC TOP_LEVEL_DIR=\"${TOP_LEVEL_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Running Advanced Example in Orchestrator Mode\nDESCRIPTION: Command for running the advanced example with the orchestrator mode, which automatically spawns the additional processes needed for multi-GPU runs. This simplifies the setup for distributed inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 example_advanced.py --model_path=../llama/tmp/7B/trt_engines/fp16/4gpu_tp4_pp1/ --use_orchestrator_mode\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Requirements Specification\nDESCRIPTION: Lists required Python packages and their version constraints for TensorRT-LLM project. Includes core dependencies like tensorrt_llm, datasets library, rouge_score for evaluation, and sentencepiece for tokenization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/lookahead/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nrouge_score\nsentencepiece>=0.1.99\nevaluate\n```\n\n----------------------------------------\n\nTITLE: Configuring Additional Component Tests in CMake\nDESCRIPTION: Sets up individual test targets for dynamic decode layer, explicit draft tokens layer, and layer utilities without grouping their source files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/layers/CMakeLists.txt#2025-04-07_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_gtest(dynamicDecodeLayerTest dynamicDecodeLayerTest.cpp)\nadd_gtest(explicitDraftTokensLayerTest explicitDraftTokensLayerTest.cpp)\nadd_gtest(layerUtilsTest layerUtilsTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Verifying TensorRT-LLM Installation\nDESCRIPTION: Python code snippet to verify the successful installation of TensorRT-LLM package. Intended to be run as a sanity check after installation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/grace-hopper.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{literalinclude} ../../../examples/llm-api/quickstart_example.py\n```\n\n----------------------------------------\n\nTITLE: Running Inference with InternLM2 7B FP16 Model\nDESCRIPTION: Executes inference using the InternLM2 7B model with FP16 precision on a single prompt, specifying maximum output length and tokenizer directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm2-chat-7b/ \\\n                 --engine_dir=./internlm2-chat-7b/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Dataset with Python\nDESCRIPTION: Creates a synthetic dataset for benchmarking using the prepare_dataset.py script. Generates random token sequences with specified input and output lengths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython benchmarks/cpp/prepare_dataset.py --tokenizer=$model_name --stdout token-norm-dist --num-requests=$num_requests --input-mean=$isl --output-mean=$osl --input-stdev=0 --output-stdev=0 > $dataset_file\n```\n\n----------------------------------------\n\nTITLE: Getting Help for Advanced TensorRT-LLM Example\nDESCRIPTION: Command to display the help information showing all supported input parameters for the advanced Python bindings example, which demonstrates concurrent token generation and streaming capabilities.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytho3 example_advanced.py -h\n```\n\n----------------------------------------\n\nTITLE: Building and Running Distil-Whisper\nDESCRIPTION: Commands for converting, building and running Distil-Whisper models, including downloading assets and converting weights to the required format.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/whisper/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwget --directory-prefix=assets https://raw.githubusercontent.com/openai/whisper/main/whisper/assets/gpt2.tiktoken\n\npython3 distil_whisper/convert_from_distil_whisper.py --model_name distil-whisper/distil-medium.en --output_name distil-medium.en\n\nINFERENCE_PRECISION=float16\nWEIGHT_ONLY_PRECISION=int8\nMAX_BEAM_WIDTH=4\nMAX_BATCH_SIZE=8\ncheckpoint_dir=distil_whisper_medium_en_weights_${WEIGHT_ONLY_PRECISION}\noutput_dir=distil_whisper_medium_en${WEIGHT_ONLY_PRECISION}\n\npython3 convert_checkpoint.py \\\n                --use_weight_only \\\n                --weight_only_precision $WEIGHT_ONLY_PRECISION \\\n                --output_dir $checkpoint_dir \\\n                --model_name distil-medium.en\n```\n\n----------------------------------------\n\nTITLE: Prompt-Tuned Inference\nDESCRIPTION: Run inference using prompt-tuned model with pre-defined tokens and tasks.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --engine_dir gpt-next-8B/trt_engines/fp16/1-gpu \\\n        --vocab_file gpt-next-8B/trt_ckpt/fp16/1-gpu/tokenizer.model \\\n        --no_add_special_tokens \\\n        --prompt_table_path email_composition.npy \\\n        --prompt_tasks 0 \\\n        --max_output_len 8\n```\n\n----------------------------------------\n\nTITLE: Setting Token Block Size in TensorRT-LLM Build\nDESCRIPTION: Command to build a model with a custom token block size, which affects KV cache reuse efficiency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/kv-cache-reuse.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --tokens_per_block 32 ...\n```\n\n----------------------------------------\n\nTITLE: Converting Checkpoint with Per-Tensor SmoothQuant\nDESCRIPTION: Command to convert a GPT-2 model checkpoint with per-tensor SmoothQuant quantization. SmoothQuant modifies the model to enable INT8 quantization without significantly altering the accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --smoothquant 0.5 \\\n        --output_dir gpt2/trt_ckpt/int8-sq/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Baichuan Model Inference\nDESCRIPTION: Examples of running inference with different precision modes (fp16, bf16, int8, int4) and tensor parallelism configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../run.py --input_text \"世界上第二高的山峰是哪座？\" \\\n                 --max_output_len=50 \\\n                 --tokenizer_dir baichuan-inc/Baichuan-13B-Chat \\\n                 --engine_dir=./tmp/baichuan_v1_13b/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM Project\nDESCRIPTION: This snippet defines the Python package dependencies required for the TensorRT-LLM project. It includes TensorRT-LLM (development version), datasets library, evaluation tools, and the tiktoken library for text processing. The file uses pip-style syntax for specifying package versions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\ntiktoken==0.6.0\n```\n\n----------------------------------------\n\nTITLE: Converting Smaug-72B-v0.1 to TensorRT-LLM Checkpoint Format\nDESCRIPTION: This command converts the Hugging Face Smaug-72B-v0.1 model to TensorRT-LLM checkpoint format. It specifies the input model directory, output directory, data type, and tensor parallel size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/smaug/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../llama/convert_checkpoint.py \\\n    --model_dir ./Smaug-72B-v0.1 \\\n    --output_dir ./tllm_checkpoint_8gpu_tp8 \\\n    --dtype float16 \\\n    --tp_size 8\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT-LLM\nDESCRIPTION: Defines required Python packages and version constraints for TensorRT-LLM. Includes core TensorRT-LLM package, dataset handling, evaluation metrics, and text processing dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\nsentencepiece>=0.1.99\n```\n\n----------------------------------------\n\nTITLE: PyTorch Integration Configuration\nDESCRIPTION: Configures PyTorch integration including CUDA architecture list generation, version checking, and compiler flag setup.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_PYT)\n  set(TORCH_CUDA_ARCH_LIST \"\")\n  foreach(CUDA_ARCH IN LISTS CMAKE_CUDA_ARCHITECTURES)\n    string(REGEX REPLACE \"^([1-9][0-9]*)([0-9]a?)-real$\" \"\\\\1.\\\\2\" TORCH_ARCH\n                         ${CUDA_ARCH})\n    list(APPEND TORCH_CUDA_ARCH_LIST ${TORCH_ARCH})\n  endforeach()\n\n  find_package(Python3 COMPONENTS Interpreter Development REQUIRED)\n  find_package(Torch REQUIRED)\n  add_compile_options(${TORCH_CXX_FLAGS})\n  add_compile_definitions(TORCH_CUDA=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Launching with MPI\nDESCRIPTION: Commands to launch context and generation servers using MPI, setting MPI KV cache environment variable.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/disaggregated/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport TRTLLM_USE_MPI_KVCACHE=1\nmpirun -n <total_num_ranks> trtllm-serve disaggregated_mpi_worker -c disagg_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorRT-LLM Data Processing and Evaluation\nDESCRIPTION: This requirements file specifies three key Python packages needed for TensorRT-LLM evaluation workflows. It includes the HuggingFace datasets library for data loading and processing, the evaluate package for model assessment, and rouge_score for calculating text generation quality metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\ndatasets~=2.14.6\nevaluate~=0.4.1\nrouge_score~=0.1.2\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Virtual Environment\nDESCRIPTION: Setup commands for creating and activating a conda virtual environment with Python 3.12.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nAUTO_ENV=auto\nconda create -y -n $AUTO_ENV pip python=3.12\nconda activate $AUTO_ENV\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for TensorRT-LLM GPT\nDESCRIPTION: Installs the necessary Python packages listed in the requirements.txt file for working with TensorRT-LLM GPT models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Draft Length for Lookahead Decoding in Python\nDESCRIPTION: A Python function that calculates the maximum draft length parameter based on the Lookahead configuration parameters: window size, n-gram size, and verification set size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/lookahead/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef max_draft_len(windows_size, ngram_size, verification_set_size):\n    return (0 if (ngram_size == 1) else ngram_size - 2)\n        + (windows_size - 1 + verification_set_size) * (ngram_size - 1)\n```\n\n----------------------------------------\n\nTITLE: Running Inference Command\nDESCRIPTION: Command to run inference using the compiled TensorRT engine with sample input text.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/quick-start-guide.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --engine_dir ./llama-3.1-8b-engine  --max_output_len 100 --tokenizer_dir Meta-Llama-3.1-8B-Instruct --input_text \"How do I count to nine in French?\"\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task with Lookahead Decoding\nDESCRIPTION: Python command to run a summarization task on the CNN daily dataset using Lookahead decoding with TensorRT-LLM, comparing HuggingFace and TensorRT-LLM implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/lookahead/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython examples/summarize.py    \\\n    --test_hf                   \\\n    --test_trt_llm              \\\n    --hf_model_dir=$MODEL_DIR   \\\n    --engine_dir=$ENGINE_DIR    \\\n    --data_type=fp16            \\\n    --lookahead_config=[7,7,7]\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with INT8 Weight-Only InternLM2 7B\nDESCRIPTION: Performs text summarization using the INT8 weight-only quantized InternLM2 7B model, comparing against the HuggingFace implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --test_trt_llm --test_hf \\\n                       --hf_model_dir ./internlm2-chat-7b/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./engine_outputs\n```\n\n----------------------------------------\n\nTITLE: Platform Architecture Detection\nDESCRIPTION: Detects and configures build settings based on system architecture (x86_64 vs aarch64) and operating system (Linux vs Windows)\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_ARCH \"unknown\")\n\nif(NOT WIN32) # Linux\n  execute_process(\n    COMMAND grep -oP \"(?<=^ID=).+\" /etc/os-release\n    COMMAND tr -d \"\\\"\"\n    COMMAND tr -d \"\\n\"\n    RESULT_VARIABLE _OS_ID_SUCCESS\n    OUTPUT_VARIABLE OS_ID)\n  if(CMAKE_SYSTEM_PROCESSOR MATCHES \"x86_64\")\n    set(TARGET_ARCH \"x86_64-linux-gnu\")\n  elseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\")\n    set(TARGET_ARCH \"aarch64-linux-gnu\")\n  endif()\nelse() # Windows\n  if(CMAKE_SYSTEM_PROCESSOR MATCHES \"AMD64\")\n    set(TARGET_ARCH \"x86_64-windows-msvc\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Python Runtime\nDESCRIPTION: These snippets demonstrate how to run inference using the Python runtime for single GPU and multi-GPU setups. They include options for comparing results with HuggingFace FP32 implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 run.py --engine_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION} --engine_name ${MODEL_NAME} --model_name tmp/hf_models/${MODEL_NAME} --max_new_token=64 --num_beams=1 --compare_hf_fp32\n\nmpirun --allow-run-as-root -np ${WORLD_SIZE} python3 run.py --engine_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION} --engine_name ${MODEL_NAME} --model_name tmp/hf_models/${MODEL_NAME} --max_new_token=64 --num_beams=1 --compare_hf_fp32\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task with Smaug-72B-v0.1\nDESCRIPTION: This command runs a summarization task on the cnn_dailymail dataset using both Hugging Face and TensorRT-LLM implementations. It uses MPI to distribute the task across 8 GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/smaug/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 8 -allow-run-as-root python ../../../summarize.py \\\n    --hf_model_dir ../Smaug-72B-v0.1 \\\n    --engine_dir ./Smaug_72B_tp8 \\\n    --data_type fp16 \\\n    --test_hf \\\n    --hf_device_map_auto \\\n    --test_trt_llm\n```\n\n----------------------------------------\n\nTITLE: Implementing Tensor Parallel for STDiT in TensorRT-LLM\nDESCRIPTION: Commands to convert, build and run STDiT with tensor parallelism (TP) for improved performance and reduced memory usage per GPU.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/stdit/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Convert to TRT-LLM\npython convert_checkpoint.py --tp_size=2 --timm_ckpt=<pretrained_checkpoint>\n# Build engines\ntrtllm-build --checkpoint_dir=tllm_checkpoint/ \\\n             --max_batch_size=2 \\\n             --gemm_plugin=float16 \\\n             --kv_cache_type=disabled \\\n             --remove_input_padding=enable \\\n             --gpt_attention_plugin=auto \\\n             --bert_attention_plugin=auto \\\n             --context_fmha=enable\n# Run example\nmpirun -n 2 --allow-run-as-root python sample.py \"a beautiful waterfall\"\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT-LLM\nDESCRIPTION: Requirements file specifying Python package dependencies with version constraints. Includes tensorrt_llm development version, datasets package version 2.14.6, evaluate package, and rouge_score package for evaluation metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v1/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.6\nevaluate\nrouge_score\n```\n\n----------------------------------------\n\nTITLE: Normalizing CUDA Architecture Flags for Hardware Acceleration\nDESCRIPTION: This snippet normalizes CUDA architecture flags by appending the appropriate suffixes (-real or -a-real) based on whether the architecture supports hardware acceleration features.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARCHITECTURES_WITH_ACCEL \"90\" \"100\" \"101\" \"120\")\nunset(CMAKE_CUDA_ARCHITECTURES_NORMALIZED)\nforeach(CUDA_ARCH IN LISTS CMAKE_CUDA_ARCHITECTURES)\n  if(\"${CUDA_ARCH}\" IN_LIST ARCHITECTURES_WITH_ACCEL)\n    list(APPEND CMAKE_CUDA_ARCHITECTURES_NORMALIZED \"${CUDA_ARCH}a-real\")\n  else()\n    list(APPEND CMAKE_CUDA_ARCHITECTURES_NORMALIZED \"${CUDA_ARCH}-real\")\n  endif()\nendforeach()\nset(CMAKE_CUDA_ARCHITECTURES ${CMAKE_CUDA_ARCHITECTURES_NORMALIZED})\n\nenable_language(C CXX CUDA)\n```\n\n----------------------------------------\n\nTITLE: Starting TensorRT-LLM Server with Extra Configuration\nDESCRIPTION: Command to start the trtllm-serve server with additional configuration options, specifically for enabling metrics logging in the PyTorch backend.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-serve <model> \\\n  --extra_llm_api_options <path-to-extra-llm-api-config.yml> \\\n  [--backend pytorch --tp_size <tp> --pp_size <pp> --ep_size <ep> --host <host> --port <port>]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their version constraints for the TensorRT-LLM project. It includes TensorRT-LLM, datasets, rouge_score, sentencepiece, and evaluate. The '-c ../constraints.txt' line suggests additional constraints are defined in a separate file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nrouge_score\nsentencepiece>=0.1.99\nevaluate\n```\n\n----------------------------------------\n\nTITLE: Installing trt-test-db Package\nDESCRIPTION: Command to install the trt-test-db package from NVIDIA's private PyPI repository with a specific version.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/test-db/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --extra-index-url https://urm.nvidia.com/artifactory/api/pypi/sw-tensorrt-pypi/simple --ignore-installed trt-test-db==1.8.5+bc6df7\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: Configuration of environment variables for model precision and tensor parallelism, and creating necessary directories.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/granite/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPREC_RAW=\"bfloat16\"\nTP=1\nmkdir -p tmp/trt_engines\n```\n\n----------------------------------------\n\nTITLE: Creating an Executor with Weight Streaming in C++\nDESCRIPTION: C++ code snippet showing how to initialize a TensorRT-LLM Executor with Weight Streaming enabled. Sets the GPU weights percentage to 50%, allowing half the weights to remain in GPU memory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/weight-streaming.md#2025-04-07_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n...\nauto executorConfig = tle::ExecutorConfig(gpuWeightsPercent=0.5);\nauto executor = tle::Executor(\"model_path\", tensorrt_llm::executor::ModelType::kDECODER_ONLY, executorConfig);\n...\n```\n\n----------------------------------------\n\nTITLE: Plugin Package Structure for TensorRT-LLM\nDESCRIPTION: Shows the recommended file structure for organizing TensorRT-LLM plugins to ensure proper registration and discovery.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/python_plugin/README.md#2025-04-07_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nplugin_lib\n├──__init__.py\n├──lookup_plugin.py\n└──lookup_kernel.py\n```\n\n----------------------------------------\n\nTITLE: Generating Test Lists with trt-test-db\nDESCRIPTION: Command to generate a test list file based on system configuration. It uses the test definitions in the specified directory and filters them according to the system specifications provided in the --match-exact parameter.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/test-db/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrt-test-db -d /TensorRT-LLM/src/tests/integration/test_lists/test-db \\\n            --context l0_e2e \\\n            --test-names \\\n            --output /TensorRT-LLM/src/l0_e2e.txt \\\n            --match-exact '{\"chip\":\"ga102gl-a\",\"compute_capability\":\"8.6\",\"cpu\":\"x86_64\",\"gpu\":\"A10\",\"gpu_memory\":\"23028.0\",\"host_mem_available_mib\":\"989937\",\"host_mem_total_mib\":\"1031949\",\"is_aarch64\":false,\"is_linux\":true,\"linux_distribution_name\":\"ubuntu\",\"linux_version\":\"22.04\",\"supports_fp8\":false,\"supports_int8\":true,\"supports_tf32\":true,\"sysname\":\"Linux\",\"system_gpu_count\":\"1\",...}'\n```\n\n----------------------------------------\n\nTITLE: Building INT8 Weight-Only Quantized EXAONE Engine\nDESCRIPTION: Commands to convert and build EXAONE with INT8 weight-only quantization. This optimization reduces model size while maintaining reasonable accuracy by quantizing only the weights to INT8.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py \\\n    --model_dir $HF_MODEL_DIR \\\n    --output_dir trt_models/exaone/int8_wq/1-gpu \\\n    --use_weight_only \\\n    --weight_only_precision int8 \\\n    --dtype float16\n\ntrtllm-build \\\n    --checkpoint_dir trt_models/exaone/int8_wq/1-gpu \\\n    --output_dir trt_engines/exaone/int8_wq/1-gpu \\\n    --gemm_plugin auto\n```\n\n----------------------------------------\n\nTITLE: Running BART Inference with Multi-LoRA\nDESCRIPTION: Python command to run inference using BART with multiple LoRA models, specifying different LoRA directories and task UIDs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython run.py \\\n        --engine_dir tmp/trt_engines/bart-large-cnn/${INFERENCE_PRECISION}/ \\\n        --engine_name bart-large-cnn \\\n        --model_name tmp/hf_models/bart-large-cnn \\\n        --max_new_token=64 \\\n        --num_beams=1 \\\n        --lora_dir tmp/hf_models/bart-large-cnn-samsum-lora/ ... \\\n        --lora_task_uids 0 -1 -1 0 0 -1\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Optimizations and Debugging\nDESCRIPTION: Sets up compile definitions for build optimizations and debugging features, including fast build mode and index range checking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(FAST_BUILD)\n  add_compile_definitions(\"FAST_BUILD\")\n  message(WARNING \"Skip some kernels to accelerate compilation\")\nendif()\n\nif(INDEX_RANGE_CHECK)\n  add_compile_definitions(\"INDEX_RANGE_CHECK\")\n  message(WARNING \"Check index range to detect OOB accesses\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building INT8 SmoothQuant EXAONE Engine\nDESCRIPTION: Commands to apply INT8 SmoothQuant quantization to EXAONE using NVIDIA Modelopt. SmoothQuant balances the quantization difficulty between activations and weights for better overall quantization results.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py \\\n    --model_dir $HF_MODEL_DIR \\\n    --dtype float16 \\\n    --qformat int8_sq \\\n    --output_dir trt_models/exaone/int8_sq/1-gpu\n\ntrtllm-build \\\n    --checkpoint_dir trt_models/exaone/int8_sq/1-gpu \\\n    --output_dir trt_engines/exaone/int8_sq/1-gpu \\\n    --gemm_plugin auto\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT Models - Basic Command\nDESCRIPTION: The basic command for building a TensorRT engine from converted TensorRT-LLM checkpoints using the trtllm-build tool.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bert/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./${model_name}_${dtype}_tllm_checkpoint \\\n--output_dir ${model_name}_engine_outputs \\\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies for TensorRT-LLM\nDESCRIPTION: Lists required Python packages and their version constraints for the TensorRT-LLM project. Includes the main TensorRT-LLM package, datasets library with a fixed version, and evaluation packages for model assessment.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets==2.14.6\nevaluate\nrouge_score\n```\n\n----------------------------------------\n\nTITLE: Generating Videos with STDiT TensorRT Engine\nDESCRIPTION: Command to run the sample script for generating videos from text prompts using the optimized TensorRT engines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/stdit/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython sample.py \"a beautiful waterfall\"\n```\n\n----------------------------------------\n\nTITLE: Weight Postprocessing Template Classes\nDESCRIPTION: Example classes showing how to implement custom weight postprocessing for different mapping scenarios.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Example for 1-1 weights mapping\nclass CustomizedModuleA(Module):\n    def __init__(...):        \n        super().__init__(...)\n        ...\n        self.tp_dim = 0    # Need to set or inherit from parent class\n\n    def postprocess(self, tllm_key, weights, **kwargs):\n        weights = proc(weights)\n        return {tllm_key: weights}\n\n# Example for multiple-multiple weights mapping\nclass CustomizedModuleB(Module):\n    def __init__(...):        \n        super().__init__(...)\n        ...\n        self.tp_dim = 0    # Need to set or inherit from parent class\n        # The default value of \"weights\" in tllm_to_externel_key_dict will be override\n        self.tllm_to_externel_key_dict = {\"weight\": [\"qweight\", \"qzeros\", \"scales\"]}\n\n    def postprocess(self, tllm_key, weights, **kwargs):\n        # Skipped the postprocess of zeros and weights_scaling_factor\n        # They are loaded in the postprocess of weight\n        config = kwargs.get(\"config\", None) # Passed through kwargs by default\n        if not tllm_key.endswith(\"weight\"):\n            return {}\n        # The order in weights is defined in tllm_to_externel_key_dict\n        qweight, qzeros, scales = weights\n        proccessed_weight, proccessed_zeros = proc(qweight, qzeros, config.num_heads)\n        return {\n            tllm_key: proccessed_weight,\n            tllm_key.replace(\"weight\", \"zeros\"): proccessed_zeros,\n            tllm_key.replace(\"weight\", \"weights_scaling_factor\"): scales,\n        }\n```\n\n----------------------------------------\n\nTITLE: Detecting CUDA Compiler and Version\nDESCRIPTION: Detects the CUDA compiler, preserves host compiler settings, and extracts the CUDA version from the compiler. Ensures the CUDA version meets minimum requirements for the project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\n# Determine CUDA version before enabling the language extension\n# check_language(CUDA) clears CMAKE_CUDA_HOST_COMPILER if CMAKE_CUDA_COMPILER is\n# not set\nif(NOT CMAKE_CUDA_COMPILER AND CMAKE_CUDA_HOST_COMPILER)\n  set(CMAKE_CUDA_HOST_COMPILER_BACKUP ${CMAKE_CUDA_HOST_COMPILER})\nendif()\ncheck_language(CUDA)\nif(CMAKE_CUDA_HOST_COMPILER_BACKUP)\n  set(CMAKE_CUDA_HOST_COMPILER ${CMAKE_CUDA_HOST_COMPILER_BACKUP})\n  check_language(CUDA)\nendif()\nif(CMAKE_CUDA_COMPILER)\n  message(STATUS \"CUDA compiler: ${CMAKE_CUDA_COMPILER}\")\n  if(NOT WIN32) # Linux\n    execute_process(\n      COMMAND\n        \"bash\" \"-c\"\n        \"${CMAKE_CUDA_COMPILER} --version | egrep -o 'V[0-9]+.[0-9]+.[0-9]+' | cut -c2-\"\n      RESULT_VARIABLE _BASH_SUCCESS\n      OUTPUT_VARIABLE CMAKE_CUDA_COMPILER_VERSION\n                      OUTPUT_STRIP_TRAILING_WHITESPACE)\n\n    if(NOT _BASH_SUCCESS EQUAL 0)\n      message(FATAL_ERROR \"Failed to determine CUDA version\")\n    endif()\n\n  else() # Windows\n    execute_process(\n      COMMAND ${CMAKE_CUDA_COMPILER} --version\n      OUTPUT_VARIABLE versionString\n      RESULT_VARIABLE versionResult)\n\n    if(versionResult EQUAL 0 AND versionString MATCHES\n                                 \"V[0-9]+\\\\.[0-9]+\\\\.[0-9]+\")\n      string(REGEX REPLACE \"V\" \"\" version ${CMAKE_MATCH_0})\n      set(CMAKE_CUDA_COMPILER_VERSION \"${version}\")\n    else()\n      message(FATAL_ERROR \"Failed to determine CUDA version\")\n    endif()\n  endif()\nelse()\n  message(FATAL_ERROR \"No CUDA compiler found\")\nendif()\n\nset(CUDA_REQUIRED_VERSION \"11.2\")\nif(CMAKE_CUDA_COMPILER_VERSION VERSION_LESS CUDA_REQUIRED_VERSION)\n  message(\n    FATAL_ERROR\n      \"CUDA version ${CMAKE_CUDA_COMPILER_VERSION} must be at least ${CUDA_REQUIRED_VERSION}\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing KV Cache Manager for PyTorch Backend\nDESCRIPTION: Example code showing how to instantiate a KVCacheManager in the PyTorch backend with configuration parameters including cache type, model dimensions, and memory constraints. This initialization is typically done within the create_pytorch_model_based_executor function.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/kv_cache_manager.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    kv_cache_manager = KVCacheManager(\n        executor_config.kv_cache_config,\n        tensorrt_llm.bindings.internal.batch_manager.CacheType.SELF,\n        num_layers=model_engine.model.config.num_hidden_layers,\n        num_kv_heads=model_engine.model.config.num_key_value_heads,\n        head_dim=head_dim,\n        tokens_per_block=tokens_per_block,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_num_requests,\n        mapping=mapping,\n        dtype=kv_cache_dtype,\n    )\n```\n\n----------------------------------------\n\nTITLE: Sampling Parameters Table\nDESCRIPTION: Markdown table detailing sampling-specific parameters including random seed, top-K, top-P, and related configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-runtime.md#2025-04-07_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n| Name in TRT-LLM |               Description                                               |   Data type   |  Range of value   |  Default value   | Name in HF |\n| :-------------: | :---------------------------------------------------------------------: | :-----------: | :---------------: | :--------------: | :--------: |\n|  `randomSeed`   | random seed for random number generator                                 |     Int64     |   \\[0, 2^64-1\\]   |       `0`        |     no     |\n|     `topK`      |   the number of logits to sample from                                   |  List\\[Int\\]  |    \\[0, 1024\\]    |       `0`        |  `top_k`   |\n|     `topP`      |  the top-P probability to sample from                                   | List\\[Float\\] |  \\[0.0f, 1.0f\\]   |      `0.0f`      |  `top_p`   |\n|   `topPDecay`   |    the decay in the `topP` algorithm                                    | List\\[Float\\] |  \\(0.0f, 1.0f\\]   |      `1.0f`      |     no     |\n|    `topPMin`    |    the decay in the `topP` algorithm                                    | List\\[Float\\] |  \\(0.0f, 1.0f\\]   |    `1.0e-6,f`    |     no     |\n| `topPResetIds`  |    the decay in the `topP` algorithm                                    |  List\\[Int\\]  | \\[-1, $+\\infty$\\) | `-1` (no effect) |     no     |\n|     `minP`      | scale the most likely token to determine the minimum token probability. |  List\\[Float\\]  | \\[0.0f, 1.0f\\] | `0.0` (no effect) |     `min_p`     |\n```\n\n----------------------------------------\n\nTITLE: Running Inference with RecurrentGemma-2b TensorRT Engine\nDESCRIPTION: This command runs inference using the RecurrentGemma-2b FP16 TensorRT engine with Python session and specified attention window size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b\nTOKENIZER_DIR_2B_PATH=./recurrentgemma_model/recurrentgemma-2b\npython3 ../run.py --max_output_len=100 \\\n                  --use_py_session \\\n                  --max_attention_window_size 2048 \\\n                  --tokenizer_dir ${TOKENIZER_DIR_2B_PATH} \\\n                  --engine_dir ${ENGINE_2B_PATH}\n```\n\n----------------------------------------\n\nTITLE: Running Medusa Decoding with Llama-3.1-70B on 2 GPUs\nDESCRIPTION: This command runs the Llama-3.1-70B-Medusa model provided by ModelOpt across 2 GPUs using MPI. It demonstrates text generation with Medusa decoding using a prompt 'Once upon' with a temperature of 1.0. The command specifies Medusa choice patterns for the model to consider during decoding.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 2 --allow-run-as-root --oversubscribe \\\n    python ../run.py --engine_dir ./tmp/modelopt/llama-70B-medusa/trt_engines/2-gpu/ \\\n                     --tokenizer_dir ./llama-3.1-70b-medusa_vfp8-fp8-fp8 \\\n                     --max_output_len=100 \\\n                     --medusa_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                     --temperature 1.0 \\\n                     --input_text \"Once upon\"\n```\n\n----------------------------------------\n\nTITLE: Using Direct Token IDs Input in TensorRT-LLM\nDESCRIPTION: Demonstrates how to bypass text input by providing token IDs directly to the generate method. This approach is useful when tokens are pre-computed or when working with specialized token sequences.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(<llama_model_path>)\n\nfor output in llm.generate([32, 12]):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Running Throughput Benchmark with TensorRT-LLM\nDESCRIPTION: Command to run throughput benchmarks on a TensorRT-LLM model using trtllm-bench. Tests performance by sending 1000 requests with specified input/output sizes. Requires model path, dataset path and engine directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/benchmarking-default-performance.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-bench \\\n--model /path/to/hf/Llama-3.3-70B-Instruct/ \\\nthroughput \\\n--dataset /path/to/dataset/synthetic_2048_2048_1000.txt \\\n--engine_dir /path/to/engines/baseline\n```\n\n----------------------------------------\n\nTITLE: Modelopt Quantization Commands\nDESCRIPTION: Various quantization methods using Modelopt toolkit including AWQ, FP8, and SmoothQuant with INT8 KV cache\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../quantization/quantize.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/int4_awq/ --qformat int4_awq\n\npython ../../../quantization/quantize.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/fp8/ --qformat fp8 --kv_cache_dtype fp8\n\npython ../../../quantization/quantize.py --model_dir mosaicml/mpt-7b --output_dir ./ckpts/mpt-7b/sq_int8kv/ --qformat int8_sq --kv_cache_dtype int8\n```\n\n----------------------------------------\n\nTITLE: Setting Up Project Directories and Third-Party Dependencies\nDESCRIPTION: This code configures project root directory paths, adds the PyBind11 subdirectory, and sets up system includes for third-party libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nget_filename_component(TRT_LLM_ROOT_DIR ${CMAKE_CURRENT_SOURCE_DIR} PATH)\n\nset(3RDPARTY_DIR ${TRT_LLM_ROOT_DIR}/3rdparty)\nadd_subdirectory(${3RDPARTY_DIR}/pybind11 ${CMAKE_CURRENT_BINARY_DIR}/pybind11)\n\n# include as system to suppress warnings\ninclude_directories(\n  SYSTEM\n  ${CUDAToolkit_INCLUDE_DIRS}\n  ${CUDNN_ROOT_DIR}/include\n  ${NCCL_INCLUDE_DIR}\n  ${3RDPARTY_DIR}/cutlass/include\n  ${3RDPARTY_DIR}/cutlass/tools/util/include\n  ${3RDPARTY_DIR}/NVTX/include\n  ${3RDPARTY_DIR}/json/include\n  ${3RDPARTY_DIR}/pybind11/include)\n```\n\n----------------------------------------\n\nTITLE: Running Qwen Model with INT4-AWQ Quantization\nDESCRIPTION: Command to run inference on a TensorRT-LLM Qwen model with INT4-AWQ quantization. It demonstrates efficient 4-bit inference with activation-aware weight quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py --input_text \"你好，请问你叫什么？\" \\\n                  --max_output_len=50 \\\n                  --tokenizer_dir ./tmp/Qwen/7B/ \\\n                  --engine_dir=./tmp/Qwen/7B/trt_engines/int4_AWQ/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Running executorExampleDisaggregated with MPI KV Cache\nDESCRIPTION: Commands to run the disaggregated Executor example which demonstrates using separate context and generation engines. Requires setting an environment variable for MPI KV cache and configuring the number of ranks for each stage.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport TRTLLM_USE_MPI_KVCACHE=1\n\nmpirun -n <num_ranks> --allow-run-as-root --oversubscribe ./executorExampleDisaggregated --context_engine_dir <path_to_context_engine_dir> --context_rank_size <num_ranks_for_context> --generation_engine_dir <path_to_generation_engine_dir> --generation_rank_size <num_ranks_for_generation> --input_tokens ../inputTokens.csv\n```\n\n----------------------------------------\n\nTITLE: Context Parallel Implementation Commands\nDESCRIPTION: Commands for running MMDiT with Context Parallel (CP) processing. Requires TensorRT >= 10.8.0.43 and disables BertAttention plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mmdit/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --tp_size=2 --model_path='stabilityai/stable-diffusion-3.5-medium'\ntrtllm-build --checkpoint_dir=./tllm_checkpoint/ \\\n             --max_batch_size=2 \\\n             --remove_input_padding=disable \\\n             --bert_attention_plugin=disable\nmpirun -n 2 --allow-run-as-root python sample.py \"A capybara holding a sign that reads 'Hello World' in the forrest.\"\n```\n\n----------------------------------------\n\nTITLE: Using Header File Guards in TensorRT-LLM\nDESCRIPTION: Demonstrates the recommended pattern for header file guards in TensorRT-LLM. Guards should use the TRTLLM_ prefix followed by the filename in caps, without including directory names in the symbol.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n#ifndef TRTLLM_FOO_BAR_HELLO_H\n#define TRTLLM_FOO_BAR_HELLO_H\n// ...\n#endif // TRTLLM_FOO_BAR_HELLO_H\n```\n\n----------------------------------------\n\nTITLE: Disabling Tokenizer for Performance in TensorRT-LLM\nDESCRIPTION: Demonstrates how to disable the tokenizer initialization for performance optimization. When disabled, the generate method expects token IDs as input and returns token IDs without text conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(<llama_model_path>)\nfor output in llm.generate([[32, 12]], skip_tokenizer_init=True):\n    print(output)\n```\n\n----------------------------------------\n\nTITLE: Switch Statement Structure\nDESCRIPTION: Demonstrates non-compliant switch statement structure that should be avoided.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// Not compliant\nswitch (x) case 4: if (y) case 5: return 0; else default: return 1;\n```\n\n----------------------------------------\n\nTITLE: Configuring NVTX Support\nDESCRIPTION: Sets up NVTX (NVIDIA Tools Extension) support, which is used for performance profiling and debugging. This snippet checks if NVTX is disabled and adds appropriate compile definitions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(NVTX_DISABLE)\n  add_compile_definitions(\"NVTX_DISABLE\")\n  message(STATUS \"NVTX is disabled\")\nelse()\n  message(STATUS \"NVTX is enabled\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building SmoothQuant Models\nDESCRIPTION: Commands to build InternLM models using SmoothQuant quantization technique\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm-chat-7b  --output_dir ./internlm-chat-7b/smooth_internlm/sq0.5/ --dtype float16 --smoothquant 0.5\n\ntrtllm-build --checkpoint_dir ./internlm-chat-7b/smooth_internlm/sq0.5/ \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Running Recurrent Gemma Models with Different Configurations in TensorRT-LLM\nDESCRIPTION: Commands for executing run.py script with different Recurrent Gemma model variants. Includes configurations for recurrentgemma-2b-it with INT4 AWQ quantization, recurrentgemma-2b-flax, and recurrentgemma-2b-it-flax, all using Python session with a maximum attention window size of 2048.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it INT4 AWQ with INT8 kv cache\npython3 ../run.py --max_output_len=100 \\\n                  --use_py_session \\\n                  --max_attention_window_size 2048 \\\n                  --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH} \\\n                  --engine_dir ${ENGINE_2B_IT_INT4_AWQ_PATH}\n\n# recurrentgemma-2b-flax\nVOCAB_FILE_2B_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-flax/tokenizer.model\npython3 ../run.py --max_output_len=100 \\\n                  --use_py_session \\\n                  --max_attention_window_size 2048 \\\n                  --vocab_file ${VOCAB_FILE_2B_FLAX_PATH} \\\n                  --engine_dir ${ENGINE_2B_FLAX_PATH}\n\n# recurrentgemma-2b-it-flax\nVOCAB_FILE_2B_IT_FLAX_PATH=./recurrentgemma_model/recurrentgemma-2b-it-flax/tokenizer.model\npython3 ../run.py --max_output_len=100 \\\n                  --use_py_session \\\n                  --max_attention_window_size 2048 \\\n                  --vocab_file ${VOCAB_FILE_2B_IT_FLAX_PATH} \\\n                  --engine_dir ${ENGINE_2B_IT_FLAX_PATH}\n```\n\n----------------------------------------\n\nTITLE: Configuring Executor for Additional Output Tensors in C++\nDESCRIPTION: This code snippet shows how to configure the TensorRT-LLM Executor to retrieve additional output tensors from a model. It demonstrates setting up the ExecutorConfig to access tensors like 'TopKLogits' that have been marked as outputs in the model definition.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/executor.md#2025-04-07_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto const executorConfig = ExecutorConfig{};\n\n// ... set more configuration options if needed\n\nexecutorConfig.setAdditionalOutputNames(std::vector<std::string>{\"TopKLogits\"});\n\n// ... create the `Executor` instance\n```\n\n----------------------------------------\n\nTITLE: Running Inference without LoRA Adaptation\nDESCRIPTION: Demonstrates how to run inference without applying LoRA adaptation by setting the LoRA task UID to -1.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir ./tmp/qwen/7B_lora/trt_engines/fp16/1-gpu \\\n              --max_output_len 50 \\\n              --tokenizer_dir ./tmp/Qwen/7B/ \\\n              --input_text \"안녕하세요, 혹시 이름이 뭐에요?\" \\\n              --lora_task_uids -1 \\\n              --use_py_session\n```\n\n----------------------------------------\n\nTITLE: Quantization Examples for LLaVA-NeXT\nDESCRIPTION: Commands demonstrating different quantization methods for LLaVA-NeXT, including INT4 weight-only and INT4 AWQ (Activation-Aware Quantization).\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/vit/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# INT4 weight only\npython ../llama/convert_checkpoint.py \\\n     --model_dir tmp/hf_models/${MODEL_NAME} \\\n     --dtype float16 \\\n     --output_dir tmp/trt_models/${MODEL_NAME}/int4_weightonly/1-gpu/llm \\\n     --use_weight_only \\\n     --weight_only_precision int4\n\n# INT4 AWQ\npython ../quantization/quantize.py \\\n     --model_dir tmp/hf_models/${MODEL_NAME} \\\n     --output_dir tmp/trt_models/${MODEL_NAME}/int4_awq/1-gpu/llm \\\n     --dtype float16 \\\n     --qformat int4_awq \\\n     --calib_size 32\n```\n\n----------------------------------------\n\nTITLE: Enabling Data Type Support Based on CUDA Version\nDESCRIPTION: This code enables support for different data types (BF16, FP8, FP4) by adding preprocessor definitions based on the detected CUDA Toolkit version.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nif(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL \"11\")\n  add_definitions(\"-DENABLE_BF16\")\n  message(\n    STATUS\n      \"CUDAToolkit_VERSION ${CUDAToolkit_VERSION_MAJOR}.${CUDAToolkit_VERSION_MINOR} is greater or equal than 11.0, enable -DENABLE_BF16 flag\"\n  )\nendif()\n\nif(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL \"11.8\")\n  add_definitions(\"-DENABLE_FP8\")\n  message(\n    STATUS\n      \"CUDAToolkit_VERSION ${CUDAToolkit_VERSION_MAJOR}.${CUDAToolkit_VERSION_MINOR} is greater or equal than 11.8, enable -DENABLE_FP8 flag\"\n  )\nendif()\n\nif(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL \"12.8\")\n  add_definitions(\"-DENABLE_FP4\")\n  message(\n    STATUS\n      \"CUDAToolkit_VERSION ${CUDAToolkit_VERSION_MAJOR}.${CUDAToolkit_VERSION_MINOR} is greater or equal than 12.8, enable -DENABLE_FP4 flag\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Converting InternLM2 20B with BFloat16 and Tensor Parallelism\nDESCRIPTION: Converts the HuggingFace InternLM2 20B model to TensorRT-LLM format with BFloat16 precision and 2-way tensor parallelism, using multiple workers for faster conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm2-chat-20b/ \\\n                --dtype bfloat16 \\\n                --output_dir ./internlm2-chat-20b/trt_engines/bf16/2-gpu/ \\\n                --tp_size 2 --workers 2\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Downloading ChatGLM Models\nDESCRIPTION: Shell commands for installing required packages and cloning ChatGLM model repositories from HuggingFace. Includes steps for updating tokenization files for compatibility with transformers-4.36.1.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/chatglm-6b/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\napt-get update\napt-get install git-lfs\nrm -rf chatglm*\n\n# clone one or more models we want to build\ngit clone https://huggingface.co/THUDM/chatglm-6b       chatglm_6b\ngit clone https://huggingface.co/THUDM/glm-10b          glm_10b\n\n# replace tokenization file if using transformers-4.36.1 for model ChatGLM-6B (this might be needless in the future)\ncp chatglm_6b/tokenization_chatglm.py chatglm_6b/tokenization_chatglm.py-backup\ncp tokenization_chatglm.py chatglm_6b\n```\n\n----------------------------------------\n\nTITLE: Performance Tests Skip Cases - B40\nDESCRIPTION: Test cases skipped in B40/perf/test_perf.py including quantization tests and model-specific tests. Skips are due to specific bug reports and bert_attention_plugin compatibility issues.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/waives.txt#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nB40/perf/test_perf.py::test_perf[quant:w4a8_awq] SKIP (https://nvbugspro.nvidia.com/bug/5161074)\nB40/perf/test_perf.py::test_perf[quant:int8_sq_per_tensor] SKIP (https://nvbugspro.nvidia.com/bug/5161074)\nB40/perf/test_perf.py::test_perf[quant:int8_sq_per_token_channel] SKIP (https://nvbugspro.nvidia.com/bug/5161074)\n```\n\n----------------------------------------\n\nTITLE: Tensor Parallel Implementation Commands\nDESCRIPTION: Commands for running MMDiT with Tensor Parallel (TP) processing across multiple GPUs. Includes model conversion and MPI-based execution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mmdit/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --tp_size=2 --model_path='stabilityai/stable-diffusion-3.5-medium'\ntrtllm-build --checkpoint_dir=./tllm_checkpoint/ \\\n             --max_batch_size=2 \\\n             --remove_input_padding=disable \\\n             --bert_attention_plugin=auto\nmpirun -n 2 --allow-run-as-root python sample.py \"A capybara holding a sign that reads 'Hello World' in the forrest.\"\n```\n\n----------------------------------------\n\nTITLE: Test File Structure Overview\nDESCRIPTION: Collection of test paths and configurations for different language models including GPT2, Llama, InternLM, Mistral, Mixtral, and Mamba. Tests cover various scenarios like streaming, quantization, multi-GPU setups, and different optimization plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/qa/examples_test_list.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nexamples/test_draft_target_model.py\nexamples/test_prompt_lookup.py\nexamples/test_internlm.py\nexamples/test_llama.py\nexamples/test_mamba.py\nexamples/test_medusa.py\nexamples/test_mistral.py\nexamples/test_mixtral.py\n```\n\n----------------------------------------\n\nTITLE: Logging into DockerHub via Command Line\nDESCRIPTION: Command to log into DockerHub using the docker login command. This requires a DockerHub username and a personal access token for authentication.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/dev-on-cloud/build-image-to-dockerhub.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker login -u <your_dockerhub_username>\n```\n\n----------------------------------------\n\nTITLE: Defining AttentionBackend Constructor Parameters in Python\nDESCRIPTION: Lists the parameters for the AttentionBackend constructor, which delegates the attention operation to the backend implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/attention.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self,\n    layer_idx: int,\n    num_heads: int,\n    head_dim: int,\n    num_kv_heads: Optional[int],\n    quant_config: QuantConfig,\n    pos_embd_params: PositionalEmbeddingParams):\n```\n\n----------------------------------------\n\nTITLE: Structuring TensorRT-LLM Performance Tuning Guide in reStructuredText\nDESCRIPTION: This snippet defines the structure of the Performance Tuning Guide using reStructuredText directives. It includes an external introduction file and sets up a table of contents with links to various performance tuning topics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/index.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nPerformance Tuning Guide\n=======================\n\n.. include:: introduction.md\n   :parser: myst_parser.sphinx_\n\n.. toctree::\n   :maxdepth: 1\n\n   benchmarking-default-performance\n   useful-build-time-flags\n   tuning-max-batch-size-and-max-num-tokens\n   deciding-model-sharding-strategy\n   fp8-quantization\n   useful-runtime-flags\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine for OPT with Vocab Size Sharding without Lookup Plugin\nDESCRIPTION: This snippet demonstrates how to build a TensorRT-LLM engine for an OPT model with tensor parallelism and embedding sharding along the vocab_size dimension, without using the lookup plugin. It specifies various parameters such as batch size, input length, and sequence length.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./opt/125M/trt_ckpt/fp16/2-gpu/ \\\n                --gemm_plugin float16 \\\n                --max_batch_size 8 \\\n                --max_input_len 924 \\\n                --max_seq_len 1024 \\\n                --output_dir ./opt/125M/trt_engines/fp16/2-gpu/ \\\n                --workers 2\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Communication Primitives in Python\nDESCRIPTION: Shows the Python API for multi-GPU communication primitives in TensorRT-LLM, including allreduce, allgather, send, and recv operations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# In tensorrt_llm/functional.py:\n\n# Collectives.\ndef allreduce(tensor: Tensor, group: List[int]) -> Tensor\ndef allgather(tensor: Tensor, group: List[int], gather_dim: int = 0) -> Tensor\n\n# Point-to-point communication primitives.\ndef send(tensor: Tensor, tgt: int) -> Tensor\ndef recv(tensor: Tensor, src: int) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Defining AttentionMetadata Fields in Python\nDESCRIPTION: Lists the predefined fields of the AttentionMetadata class, which stores metadata for the attention backend from batched input and KV cache.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/attention.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass AttentionMetadata:\n    max_num_requests: int\n    num_contexts: int\n    num_generations: int\n    max_num_tokens: int\n    num_tokens: int\n    num_ctx_tokens: int\n    kv_cache_manager: KVCacheManager\n    is_cuda_graph: bool\n    seq_lens: Tensor\n    seq_lens_cuda: Tensor\n    context_lens: Tensor\n    position_ids: Optional[Tensor]\n    request_ids: List[int]\n    prompt_lens: List[int]\n    kv_cache_params: KVCacheParams\n    is_dummy_attention: bool = False\n```\n\n----------------------------------------\n\nTITLE: Evaluating TensorRT-LLM with Torch Backend\nDESCRIPTION: This command runs the evaluation script for a TensorRT-LLM model using the Torch backend. It specifies the model folder, backend type, chunk size, and tasks to evaluate.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-eval/lm-eval-harness/README.md#2025-04-07_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython lm_eval_tensorrt_llm.py --model trt-llm \\\n    --model_args model=<HF model folder>,backend=torch,chunk_size=<int> \\\n    --tasks <comma separated tasks, e.g., gsm8k-cot, mmlu>\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BART Encoder\nDESCRIPTION: Command to build TensorRT engine for BART encoder, specifying various parameters including LoRA settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir tmp/trt_models/bart-large-cnn/${INFERENCE_PRECISION}/encoder \\\n                --output_dir tmp/trt_engines/bart-large-cnn/${INFERENCE_PRECISION}/encoder \\\n                --paged_kv_cache disable \\\n                --moe_plugin disable \\\n                --max_beam_width 1 \\\n                --max_batch_size 8 \\\n                --max_input_len 1024 \\\n                --gemm_plugin ${INFERENCE_PRECISION} \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding disable \\\n                --lora_plugin ${INFERENCE_PRECISION} \\\n                --lora_dir tmp/hf_models/bart-large-cnn-samsum-lora/ \\\n                --lora_target_modules attn_q attn_v\n```\n\n----------------------------------------\n\nTITLE: Configuring C++ and CUDA Compiler Standards and Flags\nDESCRIPTION: This code sets up C++17 standard for both C++ and CUDA compilers, and configures debug flags for Unix platforms.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\n# C++17\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\nset(CMAKE_CUDA_STANDARD ${CMAKE_CXX_STANDARD})\n\nif(UNIX)\n  set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -g -O0 -fno-inline\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with Qwen-7B in BF16\nDESCRIPTION: Performs summarization using the Qwen-7B model in BF16 precision, specifying the appropriate engine directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./tmp/Qwen/7B/ \\\n                       --data_type fp16 \\\n                       --engine_dir ./tmp/Qwen/7B/trt_engines/bf16/1-gpu/ \\\n                       --max_input_length 2048 \\\n                       --output_len 2048\n```\n\n----------------------------------------\n\nTITLE: Running MMLU Evaluation with MPI and TensorRT-LLM\nDESCRIPTION: Command to execute MMLU evaluation using mpirun with 8 processes, testing both TensorRT-LLM and HuggingFace implementations of DeepSeek-V2 model using bfloat16 precision\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmpirun --allow-run-as-root -n 8 python ../../../mmlu.py --engine_dir ./trtllm_engines/deepseek_v2/bf16/tp8-sel4096-isl2048-bs4 \\\n                  --hf_model_dir ./DeepSeek-V2 \\\n                  --data_type bfloat16 \\\n                  --batch_size 1 \\\n                  --test_trt_llm \\\n                  --test_hf \\\n                  --data_dir ./mmlu_data/data/\n```\n\n----------------------------------------\n\nTITLE: Running executorExampleKvEvents for KV Cache Events\nDESCRIPTION: Command to run the KV Cache Events example which demonstrates how to use the KV Cache Event API to monitor and manage the state of TRT-LLM's internal radix tree for applications like smart routing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./executorExampleKvEvents --engine_dir <path_to_engine_dir>\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmark Table in Markdown\nDESCRIPTION: Markdown table showing performance speedup comparisons between FP8 and INT8 SQ versus FP16 for different models and batch sizes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Model       | Batch Size | Speedup (FP8 v.s. FP16) | Speedup (INT8 SQ v.s. FP16) |\n| ----------- | :--------: | :---------------------: | :-------------------------: |\n| GPT-J       |     1      |          1.40x          |            1.40x            |\n| GPT-J       |     8      |          1.44x          |            1.30x            |\n| LLaMA-v2-7B |     1      |          1.51x          |            1.47x            |\n| LLaMA-v2-7B |     8      |          1.40x          |            1.32x            |\n```\n\n----------------------------------------\n\nTITLE: Converting InternLM2 7B to TensorRT-LLM with FP16 Precision\nDESCRIPTION: Converts the HuggingFace InternLM2 7B model to TensorRT-LLM format using FP16 precision for single GPU inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm2-chat-7b/ \\\n                --dtype float16 \\\n                --output_dir ./internlm2-chat-7b/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Look-Up Plugin for TensorRT-LLM\nDESCRIPTION: Shows how to create a custom look-up plugin by inheriting from PluginBase and implementing required methods including shape/dtype inference and forward computation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/python_plugin/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@trtllm_plugin(\"TritonLookUp\")\nclass LookUpPlugin(PluginBase):\n\n    def __init__(self, use_torch_tensor, fp32_output):\n        super().__init__()\n        self.use_torch_tensor = use_torch_tensor\n        self.fp32_output = fp32_output\n\n    def shape_dtype_inference(self, inputs: Sequence[SymTensor]) -> SymTensor:\n        shape = inputs[1].shape\n        shape[0] = inputs[0].shape[0] + inputs[1].shape[0] - inputs[1].shape[0]\n        return SymTensor(\n            inputs[1].dtype if not self.fp32_output else torch.float32, shape)\n\n    def forward(self, inputs: Sequence[TensorWrapper],\n                outputs: Sequence[TensorWrapper]):\n        assert len(inputs) == 2\n        assert inputs[0].dtype in [torch.int32 or torch.int64]\n        assert inputs[1].dtype in [torch.float32, torch.float16, torch.bfloat16]\n        assert (self.fp32_output and outputs[0].dtype\n                == torch.float32) or outputs[0].dtype == inputs[1].dtype\n\n        x = inputs[0]\n        y = inputs[1]\n        z = outputs[0]\n        if self.use_torch_tensor:\n            x = convert_to_torch_tensor(x)\n            y = convert_to_torch_tensor(y)\n            z = convert_to_torch_tensor(z)\n        MAX_BLOCK_NUM = 65536\n        MAX_BLOCK_SIZE = 512\n        grid = lambda meta: (min(MAX_BLOCK_NUM, x.shape[0]) * min(\n            MAX_BLOCK_SIZE, y.shape[1]), )\n        lookup_kernel[grid](x, y, z, y.shape[0], y.shape[1], x.shape[0])\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Performance Comparison Table in Markdown\nDESCRIPTION: This snippet shows a markdown table comparing the performance metrics of GPT-J 6B model on H100 and A100 GPUs, including batch size, input/output lengths, throughput, and latency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/H100vsA100.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Model                        | Batch Size | Input Length | Output Length | Throughput (out tok/s) | 1st Token Latency (ms) |\n| :--------------------------- | :--------- | :----------- | :------------ | ---------------------: | ---------------------: |\n| **H100**\n| GPT-J 6B                     | 64         | 128          | 128           |             **10,907** |                    102 |\n| GPT-J 6B                     | 1          | 128          | -             |                    185 |                **7.1** |\n| **A100** |\n| GPT-J 6B                     | 64         | 128          | 128           |                  3,679 |                    481 |\n| GPT-J 6B                     | 1          | 128          | -             |                    111 |                   12.5 |\n| **Speedup** |\n| GPT-J 6B                     | 64         | 128          | 128           |               **3.0x** |               **4.7x** |\n| GPT-J 6B                     | 1          | 128          | -             |               **2.4x** |                   1.7x |\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for InternLM2 in TensorRT-LLM\nDESCRIPTION: Installs the necessary dependencies from the requirements.txt file to ensure compatibility with the TensorRT-LLM version being used.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: NeMo Prompt Conversion\nDESCRIPTION: Convert NeMo prompt embeddings to numpy format for inference use.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\npython3 nemo_prompt_convert.py -i email_composition.nemo -o email_composition.npy\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM with DeepGEMM on Single Node and Multiple Nodes\nDESCRIPTION: Commands for enabling DeepGEMM optimization in TensorRT-LLM for DeepSeek-V3 model on both single-node and multi-node setups. DeepGEMM provides significant end-to-end performance improvement via environment variables.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n#single-node\nTRTLLM_DG_ENABLED=1 \\\ntrtllm-bench \\\n      --model deepseek-ai/DeepSeek-V3 \\\n      --model_path /models/DeepSeek-V3 \\\n      throughput \\\n      --backend pytorch \\\n      --max_batch_size ${MAX_BATCH_SIZE} \\\n      --max_num_tokens ${MAX_NUM_TOKENS} \\\n      --dataset dataset.txt \\\n      --tp 8 \\\n      --ep 8 \\\n      --kv_cache_free_gpu_mem_fraction 0.9 \\\n      --extra_llm_api_options /workspace/extra-llm-api-config.yml \\\n      --concurrency ${CONCURRENCY} \\\n      --num_requests ${NUM_REQUESTS} \\\n      --streaming \\\n      --report_json \"${OUTPUT_FILENAME}.json\"\n\n# multi-node\nmpirun -H <HOST1>:8,<HOST2>:8 \\\n      -n 16 \\\n      -x \"TRTLLM_DG_ENABLED=1\" \\\n      -x \"CUDA_HOME=/usr/local/cuda\" \\\n      trtllm-llmapi-launch trtllm-bench \\\n      --model deepseek-ai/DeepSeek-V3 \\\n      --model_path /models/DeepSeek-V3 \\\n      throughput \\\n      --backend pytorch \\\n      --max_batch_size ${MAX_BATCH_SIZE} \\\n      --max_num_tokens ${MAX_NUM_TOKENS} \\\n      --dataset dataset.txt \\\n      --tp 16 \\\n      --ep 16 \\\n      --kv_cache_free_gpu_mem_fraction 0.9 \\\n      --extra_llm_api_options /workspace/extra-llm-api-config.yml \\\n      --concurrency ${CONCURRENCY} \\\n      --num_requests ${NUM_REQUESTS} \\\n      --streaming \\\n      --report_json \"${OUTPUT_FILENAME}.json\"\n```\n\n----------------------------------------\n\nTITLE: Batched Logits Post-Processor Function Signature in C++\nDESCRIPTION: This code snippet defines the function signature for a batched logits post-processor that enables altering logits of multiple requests in a batch, which allows for optimizations and reduced callback overheads.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/executor.md#2025-04-07_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nstd::function<void(std::vector<IdType> const&, std::vector<Tensor>&, std::vector<std::reference_wrapper<BeamTokens const>> const&, StreamPtr const&, std::vector<std::optional<IdType>> const&)>\n```\n\n----------------------------------------\n\nTITLE: Evaluating TensorRT-LLM with trtllm-serve\nDESCRIPTION: This command runs the evaluation script for a TensorRT-LLM model deployed with trtllm-serve. It specifies the API base URL, model name, tokenizer directory, tasks to evaluate, and batch size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-eval/lm-eval-harness/README.md#2025-04-07_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython lm_eval_tensorrt_llm.py --model local-completions \\\n    --model_args base_url=http://${HOST_NAME}:8001/v1/completions,model=<model_name>,tokenizer=<tokenizer_dir> \\\n    --tasks <comma separated tasks, e.g., gsm8k-cot, mmlu> \\\n    --batch_size <#>\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BART Decoder\nDESCRIPTION: This snippet demonstrates building a TensorRT engine for the BART decoder using trtllm-build. It uses similar parameters as the encoder build, with some differences in input and sequence length settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir tmp/trt_models/${MODEL_NAME}/${INFERENCE_PRECISION}/decoder \\\n                --output_dir tmp/trt_engines/${MODEL_NAME}/${INFERENCE_PRECISION}/decoder \\\n                --moe_plugin disable \\\n                --max_beam_width ${MAX_BEAM_WIDTH} \\\n                --max_batch_size 8 \\\n                --max_input_len 1 \\\n                --max_seq_len 201 \\\n                --max_encoder_input_len 1024 \\\n                --gemm_plugin ${INFERENCE_PRECISION} \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding enable\n```\n\n----------------------------------------\n\nTITLE: Plugin Components Configuration\nDESCRIPTION: Defines the list of TensorRT-LLM plugins to be included in the build, including attention, normalization, quantization and various specialized neural network operation plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(PLUGIN_LISTS\n    bertAttentionPlugin\n    cpSplitPlugin\n    fusedLayernormPlugin\n    gptAttentionCommon\n    gptAttentionPlugin\n    identityPlugin\n    gemmPlugin\n    gemmSwigluPlugin\n    fp8RowwiseGemmPlugin\n    smoothQuantGemmPlugin\n    fp4GemmPlugin\n    quantizePerTokenPlugin\n    quantizeTensorPlugin\n    quantizeToFP4Plugin\n    layernormQuantizationPlugin\n    rmsnormQuantizationPlugin\n    weightOnlyGroupwiseQuantMatmulPlugin\n    weightOnlyQuantMatmulPlugin\n    lookupPlugin\n    loraPlugin\n    doraPlugin\n    mixtureOfExperts\n    selectiveScanPlugin\n    mambaConv1dPlugin\n    lruPlugin\n    cumsumLastDimPlugin\n    topkLastDimPlugin\n    lowLatencyGemmPlugin\n    eaglePlugin\n    lowLatencyGemmSwigluPlugin\n    qserveGemmPlugin\n    cudaStreamPlugin\n    gemmAllReducePlugin)\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with Qwen-14B in FP16 on Two GPUs\nDESCRIPTION: Performs summarization using the larger Qwen-14B model in FP16 precision distributed across two GPUs using MPI.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 --allow-run-as-root \\\n    python ../summarize.py --test_trt_llm \\\n                           --hf_model_dir  ./tmp/Qwen/14B/ \\\n                           --data_type fp16 \\\n                           --engine_dir ./tmp/Qwen/14B/trt_engines/fp16/2-gpu/ \\\n                           --max_input_length 2048 \\\n                           --output_len 2048\n```\n\n----------------------------------------\n\nTITLE: Launching Disaggregated Server\nDESCRIPTION: Command to launch the main disaggregated server that orchestrates between context and generation servers.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/disaggregated/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-serve disaggregated -c disagg_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Building the TensorRT-LLM Custom Plugin\nDESCRIPTION: Commands to build the TensorRT-LLM custom plugin that integrates the Triton-generated kernels, creating a shared library that can be loaded by TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p build && cd build\ncmake .. && make\ncd ..\n```\n\n----------------------------------------\n\nTITLE: Beam Search Parameters Table\nDESCRIPTION: Markdown table showing beam search-specific parameters including beam width, diversity rate, and stopping conditions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-runtime.md#2025-04-07_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n|      Name in TRT-LLM      |           Description           |   Data type   |      Range of value      |       Default value       |     Name in HF      |\n| :-----------------------: | :-----------------------------: | :-----------: | :----------------------: | :-----------------------: | :-----------------: |\n|        `beamWidth`        | width for beam-search algorithm |      Int      |        \\[0, 1024\\]       | `0` (disable beam search) |    `beam_width`     |\n| `beamSearchDiversityRate` |  diversity of generated tokens  | List\\[Float\\] |     \\[0, $+\\infty$\\)     |          `0.0f`           | `diversity_penalty` |\n|      `lengthPenalty`      |    penalize longer sequences    | List\\[Float\\] |     \\[0, $+\\infty$\\)     |          `0.0f`           |  `length_penalty`   |\n|      `earlyStopping`      |      see description below      |  List\\[Int\\]  | \\($-\\infty$, $+\\infty$\\) |            `0`            |  `early_stopping`   |\n```\n\n----------------------------------------\n\nTITLE: Configuring KV Cache in TensorRT-LLM\nDESCRIPTION: This snippet illustrates how to set the KV cache configuration in TensorRT-LLM, specifically setting the free_gpu_memory_fraction parameter. It includes setting up prompts, sampling parameters, and configuring the KV cache with the desired memory fraction.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-runtime-flags.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm import LLM, SamplingParams\nfrom tensorrt_llm.bindings.executor import KvCacheConfig\n\n\ndef main():\n    prompts = [\n        \"Hello, I am\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n    kv_cache_config = KvCacheConfig(free_gpu_memory_fraction=0.95)\n\n    llm  =  LLM(\n    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n    tensor_parallel_size=8,\n    kv_cache_config=kv_cache_config\n    )\n\n    outputs = llm.generate(prompts, sampling_params)\n\n    # Print the outputs.\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Install necessary Python packages for running Gemma with TensorRT-LLM\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Setting Up Git-LFS for Falcon in TensorRT-LLM\nDESCRIPTION: Shows how to install required dependencies and set up Git Large File Storage (LFS), which is necessary for downloading large Falcon model files from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install dependencies\npip install -r requirements.txt\n\n# Setup git-lfs\ngit lfs install\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison After Max Batch Size and Token Tuning\nDESCRIPTION: Markdown table showing performance metrics comparing build-time flags versus tuned parameters, including token throughput, request throughput, TTFT, and inter-token latency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/tuning-max-batch-size-and-max-num-tokens.md#2025-04-07_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric                           | Build-Time Flags ON | Tuned Max Batch Size and Max Num Tokens | % Improvement |\n| -------------------------------- | ------------------- | --------------------------------------- | ------------- |\n| Token Throughput (tokens/sec)    | 2044.2628           | 2474.2581                               | 21.03         |\n| Request Throughput (req/sec)     | 0.9982              | 1.2081                                  | 21.03         |\n| Average Time To First Token (ms) | 146.6628            | 147.5742                                | -0.62          |\n| Average Inter-Token Latency (ms) | 14.4493             | 14.6852                                 | -1.63         |\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT-LLM on Ubuntu\nDESCRIPTION: Command to install TensorRT-LLM on Ubuntu by first installing OpenMPI development libraries and then using pip to install the TensorRT-LLM package.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/linux.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get -y install libopenmpi-dev && pip3 install --upgrade pip setuptools && pip3 install tensorrt_llm\n```\n\n----------------------------------------\n\nTITLE: Running DeepSeek-V3 Quick Start with PyTorch Backend\nDESCRIPTION: Python command to quickly run the DeepSeek-V3 model using the PyTorch backend. It uses the quickstart_advanced.py script with tensor parallelism set to 8.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/pytorch\npython quickstart_advanced.py --model_dir <YOUR_MODEL_DIR> --tp_size 8\n```\n\n----------------------------------------\n\nTITLE: Building th_utils Static Library\nDESCRIPTION: Configures the th_utils static library with position-independent code and CUDA device symbol resolution. Links against Torch, cuBLAS, and cuRAND libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/thop/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(th_utils STATIC thUtils.cpp)\nset_property(TARGET th_utils PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET th_utils PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\ntarget_link_libraries(th_utils PUBLIC ${TORCH_LIBRARIES} ${CUBLAS_LIB}\n                                      ${CURAND_LIB})\n```\n\n----------------------------------------\n\nTITLE: Converting Qwen with INT8 KV Cache Optimization\nDESCRIPTION: Command to convert a Qwen-7B model with INT8 KV cache optimization. This reduces memory footprint and improves performance for large batch sizes by calibrating the model and exporting scaling factors for INT8 inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/   \\\n                             --output_dir ./tllm_checkpoint_1gpu_fp16_int8kv\n                             --dtype float16  \\\n                             --int8_kv_cache\n```\n\n----------------------------------------\n\nTITLE: Summarization with Llama-3.1-70B-Medusa on 2 GPUs\nDESCRIPTION: This command uses MPI to run summarization with Llama-3.1-70B-Medusa by ModelOpt across 2 GPUs. It specifies the engine and tokenizer directories, data type, and Medusa choice patterns with a temperature of 1.0 and batch size of 1.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -np 2 --allow-run-as-root --oversubscribe \\\n    python ../summarize.py --engine_dir ./tmp/modelopt/llama-70B-medusa/trt_engines/2-gpu/ \\\n                          --hf_model_dir ./llama-3.1-70b-medusa_vfp8-fp8-fp8 \\\n                          --tokenizer_dir ./llama-3.1-70b-medusa_vfp8-fp8-fp8 \\\n                          --test_trt_llm \\\n                          --data_type fp16 \\\n                          --medusa_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\" \\\n                          --use_py_session \\\n                          --temperature 1.0 \\\n                          --batch_size 1\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA_LAUNCH_BLOCKING for Synchronous Kernel Execution\nDESCRIPTION: Shows how to set the CUDA_LAUNCH_BLOCKING environment variable to make CUDA kernels launch synchronously, which helps detect errors in plugins immediately.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_LAUNCH_BLOCKING=1\n```\n\n----------------------------------------\n\nTITLE: Decoding Methods Comparison Table\nDESCRIPTION: Markdown table comparing different decoding methods between HuggingFace and TensorRT-LLM implementations, showing supported and unsupported features.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-runtime.md#2025-04-07_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n|        Method name in HF         |                    Condition in HF                    | Method name in TRT-LLM |              Condition in TRT-LLM              |\n| :------------------------------: | :---------------------------------------------------: | :--------------------: | :--------------------------------------------: |\n|        assisted decoding         | `assistant_model` or `prompt_lookup_num_tokens!=None` |           X            |                                                |\n|       beam-search decoding       |          `num_beams>1` and `do_sample=False`          |      beam search       |                `beamWidth > 1`                 |\n| beam-search multinomial sampling |          `num_beams>1` and `do_sample=True`           |           X            |                                                |\n| constrained beam-search decoding |    `constraints!=None` or `force_words_ids!=None`     |           X            |                                                |\n|        contrastive search        |            `penalty_alpha>0` and `top_k>1`            |           X            |                                                |\n|   diverse beam-search decoding   |         `num_beams>1` and `num_beam_groups>1`         |           X            |                                                |\n|         greedy decoding          |          `num_beams=1` and `do_sample=False`          |        sampling        | `beamWidth == 1` and `topK=0` and `topP=0.0f`  |\n|       multinomial sampling       |          `num_beams=1` and `do_sample=True`           |        sampling        | `beamWidth == 1` and (`topK>0` or `topP>0.0f`) |\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Model in Core Models Package\nDESCRIPTION: Example showing how to add a new model to the core models package by importing it in the package's __init__.py file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/adding_new_model.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom .modeling_mymodel import MyModelForCausalLM\n\n__all__ = [\n    ...,\n    \"MyModelForCausalLM\",\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoDoc for TensorRT-LLM Runtime Module\nDESCRIPTION: Sphinx/reStructuredText directives to generate API documentation for the TensorRT-LLM runtime module, including all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/python-api/tensorrt_llm.runtime.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: tensorrt_llm\n\n.. currentmodule:: tensorrt_llm\n\n.. automodule:: tensorrt_llm.runtime\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: MMLU Dataset Evaluation for Recurrent Gemma Models in TensorRT-LLM\nDESCRIPTION: Commands for downloading the MMLU dataset and evaluating various Recurrent Gemma model variants on it using mmlu.py script. Each command tests a different model configuration with a maximum attention window size of 2048.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nmkdir data\nwget https://people.eecs.berkeley.edu/~hendrycks/data.tar -O data/mmlu.tar\ntar -xf data/mmlu.tar -C data\nmv data/data data/mmlu\n```\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b\npython3 ../mmlu.py --test_trt_llm \\\n                   --max_attention_window_size 2048 \\\n                   --tokenizer_dir ${TOKENIZER_DIR_2B_PATH} \\\n                   --engine_dir ${ENGINE_2B_PATH}\n\n# recurrentgemma-2b-it FP8 with FP8 kv cache\npython3 ../mmlu.py --test_trt_llm \\\n                   --max_attention_window_size 2048 \\\n                   --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH} \\\n                   --engine_dir ${ENGINE_2B_IT_FP8_PATH}\n\n# recurrentgemma-2b-it INT8 SmoothQuant with INT8 kv cache\npython3 ../mmlu.py --test_trt_llm \\\n                   --max_attention_window_size 2048 \\\n                   --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH} \\\n                   --engine_dir ${ENGINE_2B_IT_INT8_SQ_PATH}\n\n# recurrentgemma-2b-it INT4 AWQ with INT8 kv cache\npython3 ../mmlu.py --test_trt_llm \\\n                   --max_attention_window_size 2048 \\\n                   --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH} \\\n                   --engine_dir ${ENGINE_2B_IT_INT4_AWQ_PATH}\n\n# recurrentgemma-2b-flax\npython3 ../mmlu.py --test_trt_llm \\\n                   --max_attention_window_size 2048 \\\n                   --vocab_file ${VOCAB_FILE_2B_FLAX_PATH} \\\n                   --engine_dir ${ENGINE_2B_FLAX_PATH}\n\n# recurrentgemma-2b-it-flax\npython3 ../mmlu.py --test_trt_llm \\\n                   --max_attention_window_size 2048 \\\n                   --vocab_file ${VOCAB_FILE_2B_IT_FLAX_PATH} \\\n                   --engine_dir ${ENGINE_2B_IT_FLAX_PATH}\n```\n\n----------------------------------------\n\nTITLE: Quantizing GPT-J with INT8 KV Cache and INT4 Weight-Only\nDESCRIPTION: Command to enable INT8 KV cache together with per-channel INT4 weight-only quantization for GPT-J.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# Enable INT8 KV cache together with per-channel INT8 weight-only quantization\npython ../../../quantization/quantize.py --model_dir ./gpt-j-6b \\\n                                   --dtype float16 \\\n                                   --qformat int4_wo \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ./trt_ckpt/gptj_int4wo_int8kv_tp1 \\\n                                   --calib_size 512\n```\n\n----------------------------------------\n\nTITLE: Logging Iteration Data in TensorRT-LLM\nDESCRIPTION: Example output from using the --log_iteration_data option with gptManagerBenchmark. This prints metadata for each decoder iteration to stdout, including active request count, memory usage, and timestamp.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-analysis.md#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[TensorRT-LLM][INFO] {\"Active Request Count\":249,\"Context Requests\":8,\"Free KV cache blocks\":0,\"Generation Requests\":231,\"Iteration Counter\":90,\"Max KV cache blocks\":2448,\"Max Request Count\":256,\"MicroBatch ID\":0,\"Runtime CPU Memory Usage\":28784,\"Runtime GPU Memory Usage\":540173600,\"Runtime Pinned Memory Usage\":0,\"Scheduled Requests\":239,\"Timestamp\":\"12-13-2023 14:55:14\",\"Tokens per KV cache block\":128,\"Total Context Tokens\":6904,\"Used KV cache blocks\":2448}\n```\n\n----------------------------------------\n\nTITLE: Installing Git LFS and Cloning TensorRT-LLM Repository\nDESCRIPTION: Commands to install Git LFS, clone the TensorRT-LLM repository, initialize submodules, and pull LFS content.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/build-from-source-linux.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# TensorRT-LLM uses git-lfs, which needs to be installed in advance.\napt-get update && apt-get -y install git git-lfs\ngit lfs install\n\ngit clone https://github.com/NVIDIA/TensorRT-LLM.git\ncd TensorRT-LLM\ngit submodule update --init --recursive\ngit lfs pull\n```\n\n----------------------------------------\n\nTITLE: Running FairSeq NMT Inference\nDESCRIPTION: Command to run inference using the built FairSeq NMT engine, specifying engine directory, model name, and other parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nmpirun --allow-run-as-root -np ${WORLD_SIZE} python3 run.py --engine_dir tmp/trt_engines/wmt14/${INFERENCE_PRECISION} --engine_name wmt14 --model_name tmp/fairseq_models/wmt14/${INFERENCE_PRECISION} --max_new_token=24 --num_beams=1\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Link Flags\nDESCRIPTION: Configures platform-specific linking flags. For Unix systems, sets rpath and link flags. For Windows, links against context attention source.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/thop/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT WIN32)\n  set_target_properties(\n    th_common\n    PROPERTIES LINK_FLAGS\n               \"-Wl,-rpath='$ORIGIN' ${AS_NEEDED_FLAG} ${UNDEFINED_FLAG}\")\nelse()\n  target_link_libraries(th_common PRIVATE context_attention_src)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing INT8 Weight-Only Quantization for ChatGLM Models in TensorRT-LLM\nDESCRIPTION: Commands for converting a ChatGLM model with INT8 weight-only quantization, building a TensorRT engine with optimization plugins, and running inference. Weight-only quantization significantly reduces latency and memory footprint while preserving model accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# glm_4_9b: single gpu, int8 weight only quantization\npython3 convert_checkpoint.py --model_dir glm_4_9b \\\n        --use_weight_only \\\n        --weight_only_precision int8 \\\n        --output_dir trt_ckpt/glm_4_9b/int8_wo/1-gpu\n\n# glm_4_9b: single-gpu engine with int8 weight only quantization, GPT Attention plugin, Gemm plugin\ntrtllm-build --checkpoint_dir trt_ckpt/glm_4_9b/int8_wo/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/glm_4_9b/int8_wo/1-gpu\n\n# Run inference.\npython3 ../run.py --input_text \"What's new between ChatGLM3-6B and ChatGLM2-6B?\" \\\n        --max_output_len 50 \\\n        --tokenizer_dir glm_4_9b \\\n        --engine_dir trt_engines/glm_4_9b/int8_wo/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Converting Checkpoint\nDESCRIPTION: Command to convert Qwen-VL checkpoint to TensorRT-LLM format with float16 precision\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./examples/qwen/convert_checkpoint.py --model_dir=./Qwen-VL-Chat \\\n            --output_dir=./tllm_checkpoint_1gpu \\\n            --dtype float16\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Model with Debug Mode\nDESCRIPTION: This bash command demonstrates how to run a TensorRT-LLM model with debug mode enabled, using the Python session and specifying the engine directory, tokenizer, and output length.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../run.py \\\n    --engine_dir gpt2/trt_engines/fp16/1-gpu \\\n    --tokenizer_dir gpt2 \\\n    --max_output_len 8 \\\n    --debug_mode \\\n    --use_py_session\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Batch Manager Building\nDESCRIPTION: Determines whether to build the batch manager component from source or import it as a dependency, based on the availability of its CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(EXISTS\n   \"${CMAKE_CURRENT_SOURCE_DIR}/tensorrt_llm/batch_manager/CMakeLists.txt\")\n  set(BUILD_BATCH_MANAGER_DEFAULT ON)\nelse()\n  set(BUILD_BATCH_MANAGER_DEFAULT OFF)\nendif()\n\noption(BUILD_BATCH_MANAGER \"Build batch manager from source\"\n       ${BUILD_BATCH_MANAGER_DEFAULT})\n\nif(BUILD_BATCH_MANAGER)\n  message(STATUS \"Building batch manager\")\nelse()\n  message(STATUS \"Importing batch manager\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Batch Manager Building\nDESCRIPTION: Determines whether to build the batch manager component from source or import it as a dependency, based on the availability of its CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(EXISTS\n   \"${CMAKE_CURRENT_SOURCE_DIR}/tensorrt_llm/batch_manager/CMakeLists.txt\")\n  set(BUILD_BATCH_MANAGER_DEFAULT ON)\nelse()\n  set(BUILD_BATCH_MANAGER_DEFAULT OFF)\nendif()\n\noption(BUILD_BATCH_MANAGER \"Build batch manager from source\"\n       ${BUILD_BATCH_MANAGER_DEFAULT})\n\nif(BUILD_BATCH_MANAGER)\n  message(STATUS \"Building batch manager\")\nelse()\n  message(STATUS \"Importing batch manager\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining RequestType Enum for TensorRT-LLM Disaggregated Service\nDESCRIPTION: Enum declaration that defines the three types of requests supported by the TensorRT-LLM executor: combined context and generation, context-only, and generation-only requests.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nenum class RequestType\n{\n    REQUEST_TYPE_CONTEXT_AND_GENERATION = 0,\n    REQUEST_TYPE_CONTEXT_ONLY = 1,\n    REQUEST_TYPE_GENERATION_ONLY = 2\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing FP8 Quantization for ChatGLM Models in TensorRT-LLM\nDESCRIPTION: Commands for using the quantize.py script to apply FP8 quantization to a ChatGLM model, including KV cache quantization, building a TensorRT engine with the quantized model, and running inference. FP8 provides a good balance between precision and performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# glm_4_9b: single gpu, fp8 quantization\npython ../quantization/quantize.py --model_dir glm_4_9b \\\n        --dtype float16 \\\n        --qformat fp8 \\\n        --kv_cache_dtype fp8 \\\n        --output_dir trt_ckpt/glm_4_9b/fp8/1-gpu\n\n# glm_4_9b: single-gpu engine with fp8 quantization, GPT Attention plugin, Gemm plugin\ntrtllm-build --checkpoint_dir trt_ckpt/glm_4_9b/fp8/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/glm_4_9b/fp8/1-gpu\n\n# Run inference.\npython3 ../run.py --input_text \"What's new between ChatGLM3-6B and ChatGLM2-6B?\" \\\n        --max_output_len 50 \\\n        --tokenizer_dir glm_4_9b \\\n        --engine_dir trt_engines/glm_4_9b/fp8/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Setting Up git-lfs\nDESCRIPTION: Commands to install required packages and initialize git-lfs for downloading large model files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install dependencies\npip install -r requirements.txt\n\n# Setup git-lfs\ngit lfs install\n```\n\n----------------------------------------\n\nTITLE: Displaying Hardware Compatibility Table in Markdown\nDESCRIPTION: This code snippet creates a table using Markdown syntax to display hardware compatibility information for TensorRT-LLM, including supported operating systems and GPU model architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/support-matrix.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{list-table}\n:header-rows: 1\n:widths: 20 80\n\n* -\n  - Hardware Compatibility\n* - Operating System\n  - TensorRT-LLM requires Linux x86_64 or Linux aarch64.\n* - GPU Model Architectures\n  -\n    - [NVIDIA Blackwell Architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)\n    - [NVIDIA Grace Hopper Superchip](https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/)\n    - [NVIDIA Hopper Architecture](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/)\n    - [NVIDIA Ada Lovelace Architecture](https://www.nvidia.com/en-us/technologies/ada-architecture/)\n    - [NVIDIA Ampere Architecture](https://www.nvidia.com/en-us/data-center/ampere-architecture/)\n```\n```\n\n----------------------------------------\n\nTITLE: Launching Triton Server with Draft-Target Model Configuration\nDESCRIPTION: Command to start Triton Inference Server with the configured draft and target models, enabling multi-model support for tensor parallelism if used.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 scripts/launch_triton_server.py \\\n    --model_repo=${TRITON_REPO} \\\n    --tensorrt_llm_model_name \"tensorrt_llm,tensorrt_llm_draft\" \\\n    --multi-model \\\n    --log &\n```\n\n----------------------------------------\n\nTITLE: Downloading Qwen2-Audio Model\nDESCRIPTION: Commands to install Git LFS and clone the Qwen2-Audio-7B-Instruct model from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\nexport MODEL_PATH=\"tmp/Qwen2-Audio-7B-Instruct\"\ngit clone https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct $MODEL_PATH\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Compiler Flags in CMake\nDESCRIPTION: Configures platform-specific compiler flags, enabling warnings for Unix/Linux systems with optional warning-as-error behavior, and setting warning level 4 for Windows builds.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/common/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT WIN32)\n  # additional warnings\n  #\n  # Ignore overloaded-virtual warning. We intentionally change parameters of\n  # some methods in derived class.\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n  if(WARNING_IS_ERROR)\n    message(STATUS \"Treating warnings as errors in GCC compilation\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n  endif()\nelse() # Windows\n  # warning level 4\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BART Decoder\nDESCRIPTION: Command to build TensorRT engine for BART decoder, specifying various parameters including LoRA settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir tmp/trt_models/bart-large-cnn/${INFERENCE_PRECISION}/decoder \\\n                --output_dir tmp/trt_engines/bart-large-cnn/${INFERENCE_PRECISION}/decoder \\\n                --moe_plugin disable \\\n                --max_beam_width 1 \\\n                --max_batch_size 8 \\\n                --max_input_len 1 \\\n                --max_seq_len 201 \\\n                --max_encoder_input_len 1024 \\\n                --gemm_plugin ${INFERENCE_PRECISION} \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding disable \\\n                --lora_plugin ${INFERENCE_PRECISION} \\\n                --lora_dir tmp/hf_models/bart-large-cnn-samsum-lora/ \\\n                --lora_target_modules attn_q cross_attn_q attn_v cross_attn_v\n```\n\n----------------------------------------\n\nTITLE: Converting to FP16 Checkpoint\nDESCRIPTION: Command to convert the model checkpoint to FP16 precision format.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../qwen/convert_checkpoint.py --model_dir=$MODEL_PATH \\\n        --dtype=float16 \\\n        --output_dir=./tllm_checkpoint_1gpu_fp16\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM Shared Library Target in CMake\nDESCRIPTION: Sets up the main TensorRT-LLM shared library target, configuring link libraries, compiler flags, and platform-specific settings. It also handles linking of whole archives and cyclic dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset(TRTLLM_LINK_LIBS\n    ${CUDA_DRV_LIB}\n    ${CUBLAS_LIB}\n    ${CUBLASLT_LIB}\n    ${CMAKE_DL_LIBS}\n    ${TRT_LIB}\n    common_src\n    kernels_src\n    flash_mla_src\n    context_attention_src\n    decoder_attention_src\n    trtllm_gen_fmha\n    trtllm_gen_blockscale_gemm\n    trtllm_gen_fp8_block_scale_moe\n    selective_scan_src\n    ws_layernorm_src\n    fpA_intB_gemm_src\n    # moe_gemm_src\n    fb_gemm_src\n    fp8_blockscale_gemm_src\n    ar_gemm_src\n    gemm_swiglu_sm90_src\n    cutlass_src\n    layers_src\n    runtime_src\n    userbuffers_src\n    ${DECODER_SHARED_TARGET_0}\n    ${DECODER_SHARED_TARGET_1})\n\nif(ENABLE_MULTI_DEVICE)\n  set(TRTLLM_LINK_LIBS ${TRTLLM_LINK_LIBS} ${MPI_C_LIBRARIES} ${NCCL_LIB})\nendif()\n\nif(NOT WIN32) # Unix-like compilers\n  set(UNDEFINED_FLAG \"-Wl,--no-undefined\")\n  set(AS_NEEDED_FLAG \"-Wl,--as-needed\")\n  set(NO_AS_NEEDED_FLAG \"-Wl,--no-as-needed\")\nelse() # Windows\n  set(UNDEFINED_FLAG \"\")\n  set(AS_NEEDED_FLAG \"\")\n  set(NO_AS_NEEDED_FLAG \"\")\nendif()\n\nset(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\n\nadd_library(${SHARED_TARGET} SHARED)\n\nset_target_properties(\n  ${SHARED_TARGET}\n  PROPERTIES CXX_STANDARD \"17\" CXX_STANDARD_REQUIRED \"YES\" CXX_EXTENSIONS \"NO\"\n             LINK_FLAGS \"${AS_NEEDED_FLAG} ${UNDEFINED_FLAG}\")\n\nfunction(link_whole_archive TARGET LIBRARY_TO_LINK)\n  if(WIN32)\n    target_link_libraries(${TARGET} PUBLIC $<TARGET_FILE:${LIBRARY_TO_LINK}>)\n    set_target_properties(\n      ${TARGET} PROPERTIES LINK_FLAGS \"/WHOLEARCHIVE:${LIBRARY_TO_LINK}\")\n  else()\n    # Assume everything else is like gcc\n    target_link_libraries(\n      ${TARGET} PRIVATE \"-Wl,--whole-archive\" $<TARGET_FILE:${LIBRARY_TO_LINK}>\n                        \"-Wl,--no-whole-archive\")\n  endif()\nendfunction()\n\ntarget_link_libraries(${SHARED_TARGET} PUBLIC ${TRTLLM_LINK_LIBS})\n\nlink_whole_archive(${SHARED_TARGET} ${BATCH_MANAGER_TARGET})\nlink_whole_archive(${SHARED_TARGET} ${EXECUTOR_TARGET})\nlink_whole_archive(${SHARED_TARGET} fp8_blockscale_gemm_src)\nlink_whole_archive(${SHARED_TARGET} ${INTERNAL_CUTLASS_KERNELS_TARGET})\n\n# Link kernel_src and cutlass_src. static internal cutlass lib overridden.\ntarget_link_libraries(${SHARED_TARGET} PUBLIC kernels_src cutlass_src)\n\n# Cyclic dependency of batch manager on TRT-LLM\ntarget_link_libraries(${BATCH_MANAGER_TARGET} INTERFACE ${SHARED_TARGET})\n# Cyclic dependency of executor on TRT-LLM\ntarget_link_libraries(${EXECUTOR_TARGET} INTERFACE ${SHARED_TARGET})\n# Cyclic dependency of internal_cutlass_kernels on TRT-LLM\ntarget_link_libraries(${INTERNAL_CUTLASS_KERNELS_TARGET}\n                      INTERFACE ${SHARED_TARGET})\n\n# Cyclic dependency of UCX data transceiver on TRT-LLM\nif(TARGET ${UCX_WRAPPER_TARGET})\n  target_link_libraries(${UCX_WRAPPER_TARGET} INTERFACE ${SHARED_TARGET})\n  add_dependencies(${SHARED_TARGET} ${UCX_WRAPPER_TARGET})\nendif()\n\nif(NOT WIN32)\n  set_target_properties(${SHARED_TARGET} PROPERTIES LINK_FLAGS\n                                                    \"-Wl,-rpath='$ORIGIN'\")\nendif()\n\ntarget_link_libraries(${SHARED_TARGET} PUBLIC ${NVRTC_WRAPPER_TARGET})\n\nadd_dependencies(${SHARED_TARGET} check_symbol)\nadd_dependencies(${SHARED_TARGET} check_symbol_executor)\nadd_dependencies(${SHARED_TARGET} check_symbol_internal_cutlass_kernels)\n\nif(BUILD_PYT)\n  add_subdirectory(thop)\nendif()\n\nif(BUILD_PYBIND)\n  add_subdirectory(pybind)\nendif()\n\nadd_subdirectory(plugins)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Weight Streaming with Different GPU Weight Percentages\nDESCRIPTION: Command to benchmark a TensorRT-LLM model with different GPU weight percentages (0%, 30%, 60%, and 100%) to measure the performance impact of Weight Streaming. Tests multiple batch sizes and input-output combinations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/weight-streaming.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 benchmarks/python/benchmark.py \\\n    --engine_dir /tmp/llama_7b/trt_engines/fp16/1-gpu/ \\\n    --batch_size \"1;32\" \\\n    --input_output_len \"256,32\" \\\n    --gpu_weights_percent \"0.0;0.3;0.6;1.0\" \\\n    --dtype float16 \\\n    --csv \\\n    --log_level verbose\n```\n\n----------------------------------------\n\nTITLE: Running Basic Inference\nDESCRIPTION: Example of running basic inference using the built engine with specified vocabulary file\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nVOCAB_FILE_PATH=/tmp/models/gemma_nv/checkpoints/tmp_vocab.model\npython3 ../run.py --engine_dir ${ENGINE_PATH} \\\n                  --max_output_len 30 \\\n                  --vocab_file ${VOCAB_FILE_PATH}\n```\n\n----------------------------------------\n\nTITLE: Documenting TensorRT-LLM Modules in reStructuredText\nDESCRIPTION: This snippet uses reStructuredText directives to generate documentation for the TensorRT-LLM library. It specifically documents the main tensorrt_llm module and the tensorrt_llm.functional submodule, including all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/python-api/tensorrt_llm.functional.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nFunctionals\n===========================\n\n.. automodule:: tensorrt_llm\n\n.. currentmodule:: tensorrt_llm\n\n.. automodule:: tensorrt_llm.functional\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Skywork Model (BF16)\nDESCRIPTION: Builds a TensorRT engine for the Skywork model using BF16 precision with specific plugin and parameter settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./skywork-13b-base/trt_ckpt/bf16 \\\n                --gemm_plugin bfloat16 \\\n                --gpt_attention_plugin bfloat16 \\\n                --context_fmha enable \\\n                --max_batch_size 32 \\\n                --max_input_len 512 \\\n                --max_seq_len 1024 \\\n                --output_dir ./skywork-13b-base/trt_engine/bf16\n```\n\n----------------------------------------\n\nTITLE: Running DeepSeek-V3 Benchmark with TensorRT-LLM\nDESCRIPTION: Command to run a performance benchmark for DeepSeek-V3 using trtllm-bench, configuring various parameters such as tensor parallelism, expert parallelism, and KV cache memory allocation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-bench \\\n  --model deepseek-ai/DeepSeek-V3 \\\n  --model_path  <YOUR_MODEL_DIR> \\\n  throughput \\\n  --backend pytorch \\\n  --max_batch_size 161 \\\n  --max_num_tokens 1160 \\\n  --dataset /workspace/dataset.txt \\\n  --tp 8 \\\n  --ep 4 \\\n  --pp 1 \\\n  --concurrency 1024 \\\n  --streaming \\\n  --kv_cache_free_gpu_mem_fraction 0.95 \\\n  --extra_llm_api_options ./extra-llm-api-config.yml 2>&1 | tee /workspace/trt_bench.log\n```\n\n----------------------------------------\n\nTITLE: Defining Medusa Choices (YAML)\nDESCRIPTION: This YAML configuration defines the Medusa choices for the engine. It specifies a series of token choices that the Medusa model will use for speculative decoding.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\n- [0]\n- [0, 0]\n- [1]\n- [0, 1]\n- [2]\n- [0, 0, 0]\n- [1, 0]\n- [0, 2]\n- [3]\n- [0, 3]\n- [4]\n- [0, 4]\n- [2, 0]\n- [0, 5]\n- [0, 0, 1]\n```\n\n----------------------------------------\n\nTITLE: Well-Structured Switch Statement\nDESCRIPTION: Shows proper switch statement structure with appropriate fall-through and break statements.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nswitch (x)\n{\ncase 0:         // Fall-through allowed from case 0: to case 1: since case 0 is empty.\ncase 1:\n    a();\n    b();\n    break;\ncase 2:\ncase 4:\n{\n    c();\n    d();\n    break;\n}\ncase 5:\n    c();\n    throw 42;  // Terminating with throw is okay\ndefault:\n    throw 42;\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Paged Context Attention in TensorRT-LLM Build\nDESCRIPTION: Command to build a model with support for paged context attention, which is required for KV cache reuse.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/kv-cache-reuse.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --use_paged_context_fmha enable\n```\n\n----------------------------------------\n\nTITLE: Converting MMDiT Model to TensorRT-LLM Format\nDESCRIPTION: Basic command sequence to convert a Stable Diffusion 3.5 model to TensorRT-LLM format and build the engine. Sets max batch size and disables input padding.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mmdit/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_path='stabilityai/stable-diffusion-3.5-medium'\ntrtllm-build --checkpoint_dir=./tllm_checkpoint/ \\\n             --max_batch_size=2 \\\n             --remove_input_padding=disable \\\n             --bert_attention_plugin=auto\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Target Names\nDESCRIPTION: Defines the main target names for the batch manager library\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/batch_manager/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(BATCH_MANAGER_TARGET_NAME tensorrt_llm_batch_manager)\nset(BATCH_MANAGER_STATIC_TARGET ${BATCH_MANAGER_TARGET_NAME}_static)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engines for 16-GPU GPT-530B Model\nDESCRIPTION: Command to build TensorRT-LLM engines for a 16-GPU GPT-530B model in FP16 precision with dummy weights for performance testing. Specifies sequence length constraints and enables plugins for better performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --model_config gpt_530b/trt_ckpt/fp16/16-gpu/config.json \\\n        --gemm_plugin auto \\\n        --max_batch_size 128 \\\n        --max_input_len 128 \\\n        --max_seq_len 148 \\\n        --output_dir gpt_530b/trt_engines/fp16/16-gpu \\\n        --workers 8\n```\n\n----------------------------------------\n\nTITLE: Downloading Whisper Model Assets and Checkpoints\nDESCRIPTION: Shell commands to download required model assets including tokenizer, mel filters, sample audio, and the large-v3 model checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/whisper/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget --directory-prefix=assets https://raw.githubusercontent.com/openai/whisper/main/whisper/assets/multilingual.tiktoken\nwget --directory-prefix=assets assets/mel_filters.npz https://raw.githubusercontent.com/openai/whisper/main/whisper/assets/mel_filters.npz\nwget --directory-prefix=assets https://raw.githubusercontent.com/yuekaizhang/Triton-ASR-Client/main/datasets/mini_en/wav/1221-135766-0002.wav\nwget --directory-prefix=assets https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\n```\n\n----------------------------------------\n\nTITLE: Building UCX Data Transceiver as Shared Library in CMake\nDESCRIPTION: Configures the build process for a UCX data transceiver wrapper as a shared library. The script finds required UCX dependencies, sets compiler standards, defines compiler flags, and handles platform-specific linking differences. It uses whole-archive linking to ensure all symbols from UCXX are included in the final binary.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/executor/cache_transmission/ucx_utils/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Build UCX data transceiver as shared library for runtime symbol loading, to\n# make UCX on-demand runtime dependency.\nif(ENABLE_UCX)\n  find_package(ucx REQUIRED)\n  find_package(ucxx REQUIRED)\n\n  add_library(${UCX_WRAPPER_TARGET} SHARED connection.cpp\n                                           ucxCacheCommunicator.cpp)\n  set_target_properties(\n    ${UCX_WRAPPER_TARGET}\n    PROPERTIES CXX_STANDARD \"17\" CXX_STANDARD_REQUIRED \"YES\"\n               CXX_EXTENSIONS \"NO\" POSITION_INDEPENDENT_CODE ON)\n\n  set_property(TARGET ${UCX_WRAPPER_TARGET} PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS\n                                                     ON)\n  set(TOP_LEVEL_DIR \"${PROJECT_SOURCE_DIR}/..\")\n  target_compile_definitions(${UCX_WRAPPER_TARGET}\n                             PUBLIC TOP_LEVEL_DIR=\"${TOP_LEVEL_DIR}\")\n\n  target_include_directories(\n    ${UCX_WRAPPER_TARGET}\n    PRIVATE $<TARGET_PROPERTY:ucxx::ucxx,INTERFACE_INCLUDE_DIRECTORIES>)\n  # link_whole_archive\n  if(WIN32)\n    target_link_libraries(${UCX_WRAPPER_TARGET}\n                          PUBLIC $<TARGET_FILE:ucxx::ucxx>)\n    set_target_properties(${UCX_WRAPPER_TARGET}\n                          PROPERTIES LINK_FLAGS \"/WHOLEARCHIVE:ucxx::ucxx\")\n  else()\n    # Assume everything else is like gcc\n    target_link_libraries(\n      ${UCX_WRAPPER_TARGET}\n      PRIVATE \"-Wl,--whole-archive\" $<TARGET_FILE:ucxx::ucxx>\n              \"-Wl,--no-whole-archive\")\n  endif()\n  target_link_libraries(${UCX_WRAPPER_TARGET} PUBLIC ucxx::ucxx ucx::ucs)\n  target_link_libraries(${UCX_WRAPPER_TARGET} PUBLIC ${CUDA_RT_LIB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building and Running TensorRT Engine with Triton Plugin\nDESCRIPTION: Commands to build a TensorRT engine that uses the custom Triton plugin for Fused Attention, and then run it with specific parameters for benchmarking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython build.py --num_heads 32 --head_size 64 --max_batch_size 8 --max_seq_len 512 --dtype float16\npython run.py --num_heads 32 --head_size 64 --batch_size 8 --seq_len 512 --log_level verbose --benchmark\n```\n\n----------------------------------------\n\nTITLE: Loading Model from Checkpoint Class Definition\nDESCRIPTION: Class method definition for loading a pretrained model from a checkpoint directory. Includes parameters for checkpoint directory, rank, and configuration options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass PretrainedModel:\n    @classmethod\n    def from_checkpoint(cls,\n                    ckpt_dir: str,\n                    rank: int = 0,\n                    config: PretrainedConfig = None):\n        # Internally load the model weights from a given checkpoint directory\n```\n\n----------------------------------------\n\nTITLE: Proper Duck-Typing in Python\nDESCRIPTION: Demonstrates the recommended pattern for duck-typing in Python by keeping the try block minimal and using the else block for the actual operations. This prevents unrelated failures from being caught by the except block.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    f.seek(0)\n    f.read()\nexcept AttributeError:\n    ... # Not a file-like object, do something else\n```\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    f.seek # Do not call to minimize chance of unrelated failure\nexcept AttributeError:\n    ... # Not a file-like object, do something else\nelse:\n    f.seek(0)\n    f.read()\n```\n\n----------------------------------------\n\nTITLE: Generating ViT ONNX and TensorRT Models\nDESCRIPTION: Python command to generate Vision Transformer ONNX model and TensorRT engine from pretrained model\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 vit_onnx_trt.py --pretrained_model_path ./Qwen-VL-Chat\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU executorExampleAdvanced with MPI\nDESCRIPTION: Command to run the advanced Executor example on multiple GPUs using MPI. The number of MPI ranks must equal the product of tensor parallelism and pipeline parallelism configured for the engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n <num_ranks> --allow-run-as-root ./executorExampleAdvanced --engine_dir <path_to_engine_dir>  --input_tokens_csv_file ../inputTokens.csv\n```\n\n----------------------------------------\n\nTITLE: NVIDIA Copyright Header\nDESCRIPTION: The required copyright notice that should be included at the top of all source files in the TensorRT-LLM project. This includes .cpp, .h, .cu, .py, and any other source files which are compiled or interpreted.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n/*\n * Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n```\n\n----------------------------------------\n\nTITLE: Configuring Host Memory Offloading in Triton Server\nDESCRIPTION: Triton server configuration parameter to enable offloading of KV cache blocks to host memory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/kv-cache-reuse.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparameters: {\n  key: \"kv_cache_host_memory_bytes\"\n  value: {\n    string_value: \"45000000000\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace Weights to TensorRT-LLM Format\nDESCRIPTION: Commands to convert model weights from HuggingFace format to TensorRT-LLM checkpoints for different model variants.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir command_r_v01 --output_dir trt_ckpt/command_r_v01/fp16/1-gpu\n\npython3 convert_checkpoint.py --model_dir command_r_plus --tp_size 4 --output_dir trt_ckpt/command_r_plus/fp16/4-gpu\n\npython3 convert_checkpoint.py --model_dir aya_23_8B --output_dir trt_ckpt/aya_23_8B/fp16/1-gpu\n\npython3 convert_checkpoint.py --model_dir aya_23_35B --output_dir trt_ckpt/aya_23_35B/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Library Target Configuration\nDESCRIPTION: Configures the static library target with include directories and build properties\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/batch_manager/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${BATCH_MANAGER_STATIC_TARGET} STATIC ${SRCS})\ntarget_include_directories(\n  ${BATCH_MANAGER_STATIC_TARGET}\n  PUBLIC ${3RDPARTY_DIR}/xgrammar/3rdparty/picojson\n         ${3RDPARTY_DIR}/xgrammar/3rdparty/dlpack/include\n         ${3RDPARTY_DIR}/xgrammar/include)\n\nset_target_properties(\n  ${BATCH_MANAGER_STATIC_TARGET}\n  PROPERTIES CXX_STANDARD \"17\" CXX_STANDARD_REQUIRED \"YES\" CXX_EXTENSIONS \"NO\"\n             POSITION_INDEPENDENT_CODE ON)\n```\n\n----------------------------------------\n\nTITLE: Linking Generated Headers and Creating Dispatchers\nDESCRIPTION: Commands to link the generated header files and create dispatchers for the compiled FP16 and FP32 Triton kernels, which will be used by the TensorRT plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Link generated headers and create dispatchers.\npython ${TRITON_ROOT}/triton/tools/link.py aot/fp16/*.h -o aot/fmha_kernel_fp16\npython ${TRITON_ROOT}/triton/tools/link.py aot/fp32/*.h -o aot/fmha_kernel_fp32\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Benchmark Using Slurm\nDESCRIPTION: Command for running a TensorRT-LLM benchmark for DeepSeek-V3 model across two nodes using Slurm workload manager. Configures GPUs, container settings, and benchmark parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n  srun -N 2 -w [NODES] \\\n  --output=benchmark_2node.log \\\n  --ntasks 16 --ntasks-per-node=8 \\\n  --mpi=pmix --gres=gpu:8 \\\n  --container-image=<CONTAINER_IMG> \\\n  --container-mounts=/workspace:/workspace \\\n  --container-workdir /workspace \\\n  bash -c \"trtllm-llmapi-launch trtllm-bench --model deepseek-ai/DeepSeek-V3 --model_path <YOUR_MODEL_DIR> throughput --backend pytorch --max_batch_size 161 --max_num_tokens 1160 --dataset /workspace/dataset.txt --tp 16 --ep 4 --kv_cache_free_gpu_mem_fraction 0.95 --extra_llm_api_options ./extra-llm-api-config.yml\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Workspace Layer Normalization Library in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet sets up the compilation of CUDA source files for the workspace layer normalization library. It includes specific configurations for the Blackwell architecture, sets compiler flags, and defines target properties.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/fusedLayernormKernels/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CU *.cu)\nadd_library(ws_layernorm_src STATIC ${SRC_CU})\n\nif(\"100\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG)\n\n  # for blackwell\n  set(WS_LAYERNORM_NVCC_FLAGS)\n  list(APPEND WS_LAYERNORM_NVCC_FLAGS --extra-device-vectorization)\n  list(APPEND WS_LAYERNORM_NVCC_FLAGS\n       --ptxas-options=--warn-on-local-memory-usage,--warn-on-spills)\n\n  target_compile_options(\n    ws_layernorm_src\n    PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:${WS_LAYERNORM_NVCC_FLAGS}>)\n  target_compile_definitions(ws_layernorm_src\n                             PRIVATE CUDA_PTX_KNOB_COLD_BLOCK_ENABLED)\nendif()\n\nif(NOT WIN32)\n  target_compile_options(\n    ws_layernorm_src PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcompiler=-Wno-psabi>)\nendif()\n\nset_property(TARGET ws_layernorm_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET ws_layernorm_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n```\n\n----------------------------------------\n\nTITLE: MPI-aware Quantization Usage in Python\nDESCRIPTION: Demonstrates how to use the quantization API in an MPI program, ensuring proper rank coordination for multi-GPU quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquant_config = QuantConfig()\nquant_config.quant_algo = quant_mode.W4A16_AWQ\nmapping = Mapping(world_size=tp_size, tp_size=tp_size)\nif rank == 0:\n    LLaMAForCausalLM.quantize(hf_model_dir,\n                          checkpoint_dir,\n                          quant_config=quant_config)\nmpi_barrier() # wait for rank-o finishes the quantization\nllama = LLaMAForCausalLM.from_checkpoint(checkpoint_dir, rank)\nengine = build(llama, build_config)\nengine.save(engine_dir)\n```\n\n----------------------------------------\n\nTITLE: Displaying TensorRT-LLM Benchmarking Options in Markdown\nDESCRIPTION: This Markdown snippet outlines the three available benchmarking workflows for TensorRT-LLM, including C++ benchmarks, Python benchmarks, and a Python benchmarking suite. It provides brief descriptions and links to each workflow, highlighting their key features and current status.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# TensorRT-LLM Benchmarks\n\n## Overview\n\nThere are currently three workflows to benchmark TensorRT-LLM:\n* [C++ benchmarks](./cpp)\n  - The recommended workflow that uses TensorRT-LLM C++ API and can take advantage of the latest features of TensorRT-LLM.\n* [Python benchmarks](./python)\n  - The Python benchmarking scripts can only benchmark the Python runtime, which do not support the latest features, such as in-flight batching.\n* [The Python benchmarking suite](../docs/source/performance/perf-benchmarking.md)\n  - This benchmarker is native to TensorRT-LLM and is a Python benchmarker for reproducing and testing the performance of TensorRT-LLM.\n  - _NOTE_: This benchmarking suite is a current work in progress and is prone to large changes.\n```\n\n----------------------------------------\n\nTITLE: LLaMA Quantization Implementation in Python\nDESCRIPTION: Shows the LLaMA-specific quantization implementation that handles both Modelopt and native quantization methods.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LLaMAForCausalLM:\n    @classmethod\n    def quantize(\n        cls,\n        hf_model_dir,\n        output_dir,\n        quant_config: QuantiConfig,\n        mapping: Optional[Mapping] = None): #some args are omitted here\n        use_modelopt_quantization = ... # determine if to use Modelopt or use native\n        if use_modelopt_quantization:\n            super().quantize(hf_model_dir,\n                             output_dir,\n                             quant_config)\n        else:\n            # handles TensorRT-LLM native model specific quantization\n            # or raise exceptions if not supported\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Downloading Models\nDESCRIPTION: Commands to install required packages and clone model repositories from HuggingFace.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/commandr/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\napt-get update\napt-get install git-lfs\n\n# clone one or more models we want to build\ngit clone https://huggingface.co/CohereForAI/c4ai-command-r-v01         command_r_v01\ngit clone https://huggingface.co/CohereForAI/c4ai-command-r-plus        command_r_plus\ngit clone https://huggingface.co/CohereForAI/aya-23-8B                  aya_23_8B\ngit clone https://huggingface.co/CohereForAI/aya-23-35B                 aya_23_35B\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engines for Falcon Models\nDESCRIPTION: Commands to use trtllm-build for building TensorRT engines from TensorRT-LLM checkpoints. Shows configuration options for different model sizes, precision types, and parallelism strategies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# falcon-rw-1b\ntrtllm-build --checkpoint_dir ./falcon/rw-1b/trt_ckpt/fp16/1-gpu/ \\\n                --gemm_plugin float16 \\\n                --output_dir ./falcon/rw-1b/trt_engines/fp16/1-gpu/\n\n# falcon-7b-instruct\n# Enabling --gpt_attention_plugin is necessary for rotary positional embedding (RoPE)\ntrtllm-build --checkpoint_dir ./falcon/7b-instruct/trt_ckpt/bf16/1-gpu/ \\\n                --gemm_plugin bfloat16 \\\n                --remove_input_padding enable \\\n                --gpt_attention_plugin bfloat16 \\\n                --output_dir ./falcon/7b-instruct/trt_engines/bf16/1-gpu/\n\n# falcon-40b-instruct: 2-way tensor parallelism\ntrtllm-build --checkpoint_dir ./falcon/40b-instruct/trt_ckpt/bf16/tp2-pp1/ \\\n                --gemm_plugin bfloat16 \\\n                --gpt_attention_plugin bfloat16 \\\n                --output_dir ./falcon/40b-instruct/trt_engines/bf16/tp2-pp1/\n\n# falcon-40b-instruct: 2-way tensor parallelism and 2-way pipeline parallelism\ntrtllm-build --checkpoint_dir ./falcon/40b-instruct/trt_ckpt/bf16/tp2-pp2/ \\\n                --gemm_plugin bfloat16 \\\n                --gpt_attention_plugin bfloat16 \\\n                --output_dir ./falcon/40b-instruct/trt_engines/bf16/tp2-pp2/\n\n# falcon-180b: 8-way tensor parallelism\ntrtllm-build --checkpoint_dir ./falcon/180b/trt_ckpt/bf16/tp8-pp1/ \\\n                --gemm_plugin bfloat16 \\\n                --gpt_attention_plugin bfloat16 \\\n                --output_dir ./falcon/180b/trt_engines/bf16/tp8-pp1/ \\\n                --workers 8\n\n# falcon-180b: 4-way tensor parallelism and 2-way pipeline parallelism\ntrtllm-build --checkpoint_dir ./falcon/180b/trt_ckpt/bf16/tp4-pp2/ \\\n                --gemm_plugin bfloat16 \\\n                --gpt_attention_plugin bfloat16 \\\n                --output_dir ./falcon/180b/trt_engines/bf16/tp4-pp2/ \\\n                --workers 8\n\n# falcon-11b (Falcon 2)\ntrtllm-build --checkpoint_dir ./falcon/11b/trt_ckpt/bf16/1-gpu/ \\\n                --gemm_plugin bfloat16 \\\n                --gpt_attention_plugin bfloat16 \\\n                --output_dir ./falcon/11b/trt_engines/bf16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Formatting Python Code with Black in Git\nDESCRIPTION: Command to format Python code changes using the Black formatter. It applies formatting to all Python files that have been modified according to Git.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CONTRIBUTING.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit diff --name-only | grep \"*.py\" | xargs black -l 120\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with Mixed Precision Checkpoint\nDESCRIPTION: Command to build a TensorRT-LLM engine using a mixed precision checkpoint produced by ModelOpt. This uses the trtllm-build command.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir <mixed-precision-checkpoint> --output_dir $OUTPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Downloading Qwen-VL Model\nDESCRIPTION: Commands to download the Qwen vision-language model using git lfs\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/Qwen/Qwen-VL-Chat\n```\n\n----------------------------------------\n\nTITLE: Adding Test Targets for TensorRT-LLM Components\nDESCRIPTION: Registers multiple test targets for various components of TensorRT-LLM, including MPI utilities, LORA management, GPT decoder, Medusa module, and various layers using the previously defined add_gtest function.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(unit_tests)\n\nadd_gtest(mpiUtilsTest runtime/mpiUtilsTest.cpp)\n\nadd_gtest(loraManagerTest runtime/loraManagerTest.cpp)\nadd_gtest(loraCacheTest runtime/loraCacheTest.cpp)\nadd_gtest(gptDecoderTest runtime/gptDecoderTest.cpp)\nadd_gtest(gptDecoderBatchedTest runtime/gptDecoderBatchedTest.cpp)\nadd_gtest(gptSessionTest runtime/gptSessionTest.cpp)\ntarget_link_libraries(gptSessionTest PRIVATE modelSpecStatic)\nadd_gtest(medusaModuleTest runtime/medusaModuleTest.cpp)\n\nadd_gtest(sanitizerTest runtime/sanitizerTest.cpp)\n\nadd_gtest(eaglePackDataTest kernels/eaglePackDataTest.cpp)\n\nadd_gtest(medusaDecodeLayerTest layers/medusaDecodeLayerTest.cpp)\n\nadd_gtest(eagleLayerTest layers/eagleLayerTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-7B-Chat (INT4 Weight-Only Quantization)\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM checkpoints and build a single-GPU INT4 weight-only quantized engine for Qwen-7B-Chat.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                              --output_dir ./tllm_checkpoint_1gpu_fp16_wq \\\n                              --dtype float16 \\\n                              --use_weight_only \\\n                              --weight_only_precision int4\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_fp16_wq \\\n            --output_dir ./tmp/qwen/7B/trt_engines/weight_only/1-gpu/ \\\n            --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Sending a Streaming Request to the TensorRT-LLM API Server\nDESCRIPTION: Example curl command for sending a streaming text generation request to the FastAPI server. The streaming option returns tokens as they're generated rather than waiting for the full response.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/apps/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8000/generate -d '{\"prompt\": \"In this example,\", \"max_tokens\": 8, \"streaming\": true}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for NVIDIA TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all C++ source files in the current directory and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope, making it available for use in other CMake files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/gptAttentionCommon/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Building 2-GPU Tensor Parallel Engine\nDESCRIPTION: Commands to build InternLM models using 2-way tensor parallelism for both 7B and 20B variants\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm-chat-7b/ \\\n                --dtype float16 \\\n                --output_dir ./internlm-chat-7b/trt_engines/fp16/2-gpu/ \\\n                --tp_size 2\n\ntrtllm-build --checkpoint_dir ./internlm-chat-7b/trt_engines/fp16/2-gpu/ \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Using LLaMA Model Conversion in Python\nDESCRIPTION: Demonstrates how to use the conversion API to convert a HuggingFace model to TensorRT-LLM format and save the checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#other args omitted for simplicity here.\nllama = LLaMAForCausalLM.from_hugging_face(model_dir, dtype, mapping=mapping)\nllama.save_checkpoint(output_dir, save_config=(rank==0))\n```\n\n----------------------------------------\n\nTITLE: Defining TensorRT-LLM Executor Example Targets\nDESCRIPTION: Creates executable targets for various TensorRT-LLM executor examples, linking them with the necessary libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Basic\nadd_executable(executorExampleBasic executorExampleBasic.cpp)\ntarget_link_libraries(executorExampleBasic nvinfer_plugin_tensorrt_llm)\n\nadd_executable(executorExampleDebug executorExampleDebug.cpp)\ntarget_link_libraries(executorExampleDebug nvinfer_plugin_tensorrt_llm)\n\nadd_executable(executorExampleKvEvents executorExampleKvEvents.cpp)\ntarget_link_libraries(executorExampleKvEvents nvinfer_plugin_tensorrt_llm\n                      cxxopts::cxxopts)\n\nadd_executable(executorExampleLogitsProcessor\n               executorExampleLogitsProcessor.cpp)\ntarget_link_libraries(executorExampleLogitsProcessor\n                      nvinfer_plugin_tensorrt_llm)\n\n# Advanced\nif(NOT TARGET cxxopts::cxxopts)\n  set(CXXOPTS_SRC_DIR ${TRTLLM_DIR}/3rdparty/cxxopts)\n  add_subdirectory(${CXXOPTS_SRC_DIR} ${CMAKE_CURRENT_BINARY_DIR}/cxxopts)\nendif()\n\nadd_executable(executorExampleAdvanced executorExampleAdvanced.cpp)\ntarget_link_libraries(executorExampleAdvanced nvinfer_plugin_tensorrt_llm\n                      cxxopts::cxxopts)\n\n# MultiInstance\n\nfind_package(MPI REQUIRED)\nmessage(STATUS \"Using MPI_C_INCLUDE_DIRS: ${MPI_C_INCLUDE_DIRS}\")\nmessage(STATUS \"Using MPI_C_LIBRARIES: ${MPI_C_LIBRARIES}\")\ninclude_directories(${MPI_C_INCLUDE_DIRS})\n\nadd_executable(executorExampleAdvancedMultiInstances\n               executorExampleAdvancedMultiInstances.cpp)\ntarget_link_libraries(\n  executorExampleAdvancedMultiInstances nvinfer_plugin_tensorrt_llm\n  cxxopts::cxxopts ${MPI_C_LIBRARIES})\n\n# FastLogits\nadd_executable(executorExampleFastLogits executorExampleFastLogits.cpp)\ntarget_link_libraries(executorExampleFastLogits nvinfer_plugin_tensorrt_llm\n                      cxxopts::cxxopts ${MPI_C_LIBRARIES})\n\nadd_executable(executorExampleDisaggregated executorExampleDisaggregated.cpp)\ntarget_link_libraries(executorExampleDisaggregated nvinfer_plugin_tensorrt_llm\n                      cxxopts::cxxopts ${MPI_C_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for OPT Model Download\nDESCRIPTION: Command to install the necessary dependencies and Git LFS for downloading OPT model checkpoints from HuggingFace.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt && sudo apt-get install git-lfs\n```\n\n----------------------------------------\n\nTITLE: Using Medusa with Qwen2 Models in TensorRT-LLM\nDESCRIPTION: This section describes how to use Medusa decoding with Qwen2 models in TensorRT-LLM. It instructs users to specify the model type as 'qwen2' when converting checkpoints and notes that the same arguments are used for trllm-build and run.py as with LLaMA models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nTo use Medusa with Qwen2 models, specify `--model_type qwen2` to `convert_checkpoint.py`. You have to provide a Qwen2 model checkpoint and the medusa heads. After TRT-LLM checkpoint is generated, trllm-build and `../run.py` use the same arguments as for LLaMA models.\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for TensorRT-LLM Executor Examples\nDESCRIPTION: Sets up the CMake project for TensorRT-LLM executor examples, including version requirements, project name, and compiler flags. It also configures CUDA and TensorRT dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1)\n\nset(TRTLLM_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../..\")\ninclude(${TRTLLM_DIR}/cpp/cmake/modules/set_ifndef.cmake)\ninclude(${TRTLLM_DIR}/cpp/cmake/modules/find_library_create_target.cmake)\nif(NOT TRTLLM_BUILD_DIR)\n  set(TRTLLM_BUILD_DIR \"${TRTLLM_DIR}/cpp/build\")\nendif()\nset(TRTLLM_LIB_PATH \"${TRTLLM_BUILD_DIR}/tensorrt_llm/libtensorrt_llm.so\")\nset(TRTLLM_PLUGIN_PATH\n    \"${TRTLLM_BUILD_DIR}/tensorrt_llm/plugins/libnvinfer_plugin_tensorrt_llm.so\"\n)\nset(TRTLLM_INCLUDE_DIR \"${TRTLLM_DIR}/cpp/include\")\n\n# Determine CXX11 ABI compatibility\nexecute_process(\n  COMMAND bash -c \"nm -f posix -D ${TRTLLM_LIB_PATH} | grep __cxx11\"\n  RESULT_VARIABLE GLIB_CXX11_FOUND\n  OUTPUT_QUIET)\nif(GLIB_CXX11_FOUND EQUAL 0)\n  set(USE_CXX11_ABI 1)\nelse()\n  set(USE_CXX11_ABI 0)\nendif()\nmessage(STATUS \"Use CXX11 ABI: ${USE_CXX11_ABI}\")\nadd_compile_options(\"-D_GLIBCXX_USE_CXX11_ABI=${USE_CXX11_ABI}\")\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED TRUE)\nset(CMAKE_VERBOSE_MAKEFILE 1)\n\n# Define project name\nproject(executorExamples)\n\n# Compile options\nset(CMAKE_CXX_FLAGS \"-Wall -pthread -lstdc++ -DENABLE_MULTI_DEVICE=1\")\nset(CMAKE_CXX_FLAGS_RELEASE \"-O3\")\nset(CMAKE_BUILD_TYPE release)\n\nfind_package(CUDAToolkit REQUIRED)\nmessage(STATUS \"CUDA library status:\")\nmessage(STATUS \"    version: ${CUDAToolkit_VERSION}\")\nmessage(STATUS \"    libraries: ${CUDAToolkit_LIBRARY_DIR}\")\nmessage(STATUS \"    include path: ${CUDAToolkit_INCLUDE_DIRS}\")\n\n# TRT dependencies\nset_ifndef(TRT_LIB_DIR ${CMAKE_BINARY_DIR})\nset_ifndef(TRT_INCLUDE_DIR /usr/include/${CMAKE_SYSTEM_PROCESSOR}-linux-gnu)\nset(TRT_LIB nvinfer)\n# On Windows major version is appended to nvinfer libs.\nif(WIN32)\n  set(TRT_LIB_NAME nvinfer_10)\nelse()\n  set(TRT_LIB_NAME nvinfer)\nendif()\nfind_library_create_target(${TRT_LIB} ${TRT_LIB_NAME} SHARED ${TRT_LIB_DIR})\nmessage(${TRT_INCLUDE_DIR})\ninclude_directories(\"${TRT_INCLUDE_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Downloading ChatGLM3 Models\nDESCRIPTION: Script to install required packages and clone ChatGLM3 model repositories from HuggingFace. Downloads different variants including 6B, 6B-base, and 6B-32k models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/chatglm3-6b-32k/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\napt-get update\napt-get install git-lfs\nrm -rf chatglm*\n\n# clone one or more models we want to build\ngit clone https://huggingface.co/THUDM/chatglm3-6b      chatglm3_6b\ngit clone https://huggingface.co/THUDM/chatglm3-6b-base chatglm3_6b_base\ngit clone https://huggingface.co/THUDM/chatglm3-6b-32k  chatglm3_6b_32k\n```\n\n----------------------------------------\n\nTITLE: Downloading DeepSeek-V3 Model Weights using Git LFS\nDESCRIPTION: Commands to download the DeepSeek-V3 model weights from Hugging Face using Git LFS. The user needs to specify the target directory for storing the weights.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/deepseek-ai/DeepSeek-V3 <YOUR_MODEL_DIR>\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including Batch Manager and Executor Tests\nDESCRIPTION: Adds subdirectories for batch manager and executor tests conditionally, based on whether they are enabled in the build and if the corresponding directories exist in the project structure.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/CMakeLists.txt#2025-04-07_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_BATCH_MANAGER)\n  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/batch_manager)\n    add_subdirectory(batch_manager)\n  endif()\nendif()\n\nif(BUILD_EXECUTOR)\n  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/executor)\n    add_subdirectory(executor)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engines for 8-GPU GPT-175B Model\nDESCRIPTION: Command to build TensorRT-LLM engines for an 8-GPU GPT-175B model in FP16 precision. Enables several plugins to increase runtime performance and reduce build time.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --model_config gpt_175b/trt_ckpt/fp16/8-gpu/config.json \\\n        --gemm_plugin auto \\\n        --max_batch_size 256 \\\n        --output_dir gpt_175b/trt_engines/fp16/8-gpu \\\n        --workers 8\n```\n\n----------------------------------------\n\nTITLE: Launching TensorRT-LLM Application with Profiling\nDESCRIPTION: Command to launch a TensorRT-LLM application (gptManagerBenchmark) using mpirun with profiling enabled for specific iterations. It uses the previously defined profiling script and sets the TLLM_GPTM_PROFILE_START_STOP environment variable.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-analysis.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 env TLLM_GPTM_PROFILE_START_STOP=\"0,63,127\" ./profile_rank_0.bash ./benchmarks/gptManagerBenchmark <benchmark/model options>\n```\n\n----------------------------------------\n\nTITLE: Gathering Source Files and Excluding Specific Directories in CMake\nDESCRIPTION: This snippet uses CMake commands to gather C++ and CUDA source files, then excludes files from specific directories using regex filters. It's used to prepare the source file list for compilation while omitting certain specialized kernel implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\n\n# Exclude files in the cutlass_kernels, decoderMaskedMultiheadAttention and\n# selectiveScan trtllmGenKernels folder\nlist(FILTER SRC_CPP EXCLUDE REGEX \"cutlass_kernels/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"cutlass_kernels/.*\")\nlist(FILTER SRC_CPP EXCLUDE REGEX \"flashMLA/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"flashMLA/.*\")\nlist(FILTER SRC_CPP EXCLUDE REGEX \"contextFusedMultiHeadAttention/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"contextFusedMultiHeadAttention/.*\")\nlist(FILTER SRC_CPP EXCLUDE REGEX \"decoderMaskedMultiheadAttention/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"decoderMaskedMultiheadAttention/.*\")\nlist(FILTER SRC_CPP EXCLUDE REGEX \"trtllmGenKernels/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"trtllmGenKernels/.*\")\nlist(FILTER SRC_CPP EXCLUDE REGEX \"selectiveScan/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"selectiveScan/.*\")\nlist(FILTER SRC_CPP EXCLUDE REGEX \"userbuffers/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"userbuffers/.*\")\nlist(FILTER SRC_CU EXCLUDE REGEX \"fusedLayernormKernels/.*\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for STDiT in TensorRT-LLM\nDESCRIPTION: Commands to install the necessary Python packages including ColossalAI (required for text encoder) and other dependencies from requirements.txt.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/stdit/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n# ColossalAI is also needed for text encoder.\npip install colossalai --no-deps\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their versions for the TensorRT-LLM project. It includes TensorRT-LLM itself, datasets, evaluate, rouge_score, and sentencepiece with specific version constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/smaug/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets==2.14.6\nevaluate\nrouge_score\nsentencepiece>=0.1.99\n```\n\n----------------------------------------\n\nTITLE: Downloading EXAONE-Deep HuggingFace Checkpoint\nDESCRIPTION: Commands to download the HuggingFace BF16 checkpoint for EXAONE-Deep-2.4B model. This prepares the model for conversion using the same procedure as EXAONE-3.0.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_MODEL_DIR=hf_models/exaone_deep\ngit clone https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B $HF_MODEL_DIR\n```\n\n----------------------------------------\n\nTITLE: Running Low Latency Benchmark for Non-Medusa Engine (Shell)\nDESCRIPTION: This command runs a low latency benchmark for a non-Medusa engine. It sets various environment variables to optimize performance and specifies the model, dataset, and engine directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nenv TRTLLM_ENABLE_MMHA_MULTI_BLOCK_DEBUG=1 \\\n  TRTLLM_MMHA_KERNEL_BLOCK_SIZE=256 \\\n  TRTLLM_MMHA_BLOCKS_PER_SEQUENCE=32 \\\n  FORCE_MULTI_BLOCK_MODE=ON \\\n  TRTLLM_ENABLE_PDL=1 \\\n  trtllm-bench --model meta-llama/Meta-Llama-3-70B \\\n  latency \\\n  --dataset $DATASET_PATH \\\n  --engine_dir /tmp/meta-llama/Meta-Llama-3-70B/engine\n```\n\n----------------------------------------\n\nTITLE: Running the TensorRT Engine\nDESCRIPTION: Example command to execute the built engine using MPI for multi-GPU inference with a sample prompt.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/arctic/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n ${TP} --allow-run-as-root python ../../../run.py --engine_dir ./tmp/trt_engines/${ENGINE} --tokenizer_dir tmp/hf_checkpoints/${HF_MODEL} --max_output_len 20 --input_text \"The future of AI is\" |& tee tmp/trt_engines/${ENGINE}_run.log\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for TensorRT-LLM Project\nDESCRIPTION: This code snippet lists the required and optional Python package dependencies for the NVIDIA TensorRT-LLM project. It includes packages for machine learning, NLP tasks, evaluation metrics, and web services. Some packages have specific version requirements or constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.16.0\nevaluate\nrouge_score\ntransformers>=4.40.1\ntransformers-stream-generator\nsentencepiece>=0.1.99\ntiktoken\neinops\n\n# optional dependencies\ngradio==4.36.0\nmdtex2html\nsse_starlette\naiohttp_sse_client\nopenai\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Jais-13b-chat (Single GPU, FP16)\nDESCRIPTION: This command builds a single-GPU float16 TensorRT engine from the TensorRT-LLM checkpoint for Jais-13b-chat, enabling the GPT Attention plugin and input padding removal for better performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir jais-13b-chat/trt_ckpt/fp16/1-gpu \\\n        --gpt_attention_plugin float16 \\\n        --remove_input_padding enable \\\n        --output_dir jais-13b-chat/trt_engines/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Adding Debug Info to TensorRT-LLM Runtime Generation\nDESCRIPTION: This Python code snippet shows how to modify the TensorRT-LLM runtime generation process to print intermediate output tensors for debugging purposes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n        stream = torch.cuda.current_stream().cuda_stream\n        instance_idx = step % 2\n        if self.cuda_graph_mode and self.runtime.cuda_graph_instances[\n                instance_idx] is not None:\n            # launch cuda graph\n            CUASSERT(\n                cudart.cudaGraphLaunch(\n                    self.runtime.cuda_graph_instances[instance_idx], stream))\n            ok = True\n        else:\n            ok = self.runtime._run(context, stream)\n\n        if not ok:\n            raise RuntimeError(f\"Executing TRT engine failed step={step}!\")\n        if self.debug_mode:\n            torch.cuda.synchronize()\n            # -------------------------------------------\n            if step == 0:\n                print(self.debug_buffer.keys())\n            print(f\"Step: {step}\")\n            print(self.debug_buffer['transformer.layers.6.mlp_output'])\n            # -------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Basic GTest Additions for Core Components\nDESCRIPTION: Adds multiple GTest targets for testing core components like decoding kernels, logits bitmask, and mixture of experts.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/kernels/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(banRepeatNGramsKernelsTest banRepeatNGramsKernelsTest.cpp)\nadd_gtest(decodingKernelsTest decodingKernelTest.cpp)\nadd_gtest(logitsBitmaskTest logitsBitmaskTest.cpp)\nadd_gtest(mixtureOfExpertsTest mixtureOfExpertsTest.cu)\nadd_gtest(ropeTest ropeTest.cu)\nadd_gtest(shiftKCacheKernelTest shiftKCacheKernelTest.cu)\nadd_gtest(smoothQuantKernelTest smoothQuant/smoothQuantKernelTest.cpp)\nadd_gtest(stopCriteriaKernelsTest stopCriteriaKernelsTest.cpp)\nadd_gtest(weightOnlyKernelTest weightOnly/weightOnlyKernelTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Implementing Activation-aware Weight Quantization (AWQ) for ChatGLM Models in TensorRT-LLM\nDESCRIPTION: Commands for using the quantize.py script to implement INT4 AWQ quantization on a ChatGLM model, building a TensorRT engine with the quantized model, and running inference. AWQ provides significant compression while maintaining model quality.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# glm_4_9b: single gpu, int4 awq quantization\npython ../quantization/quantize.py --model_dir glm_4_9b \\\n        --dtype float16 \\\n        --qformat int4_awq \\\n        --output_dir trt_ckpt/glm_4_9b/int4_awq/1-gpu\n\n# glm_4_9b: single-gpu engine with int4 awq quantization, GPT Attention plugin, Gemm plugin\ntrtllm-build --checkpoint_dir trt_ckpt/glm_4_9b/int4_awq/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/glm_4_9b/int4_awq/1-gpu\n\n# Run inference.\npython3 ../run.py --input_text \"What's new between ChatGLM3-6B and ChatGLM2-6B?\" \\\n        --max_output_len 50 \\\n        --tokenizer_dir glm_4_9b \\\n        --engine_dir trt_engines/glm_4_9b/int4_awq/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Building Medusa-Enabled Low Latency Engine (Shell)\nDESCRIPTION: This command builds a Medusa-enabled low latency engine. It specifies parameters such as checkpoint directory, speculative decoding mode, attention plugin, and various optimizations for Medusa-specific performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ntp_size=8\nmax_seq_len=131072\ntrtllm-build --checkpoint_dir $checkpoint_dir \\\n    --speculative_decoding_mode medusa \\\n    --max_batch_size 1 \\\n    --gpt_attention_plugin bfloat16 \\\n    --max_seq_len $max_seq_len \\\n    --output_dir /tmp/meta-llama/Meta-Llama-3.1-70B/medusa/engine \\\n    --use_fused_mlp enable \\\n    --paged_kv_cache enable \\\n    --use_paged_context_fmha disable \\\n    --multiple_profiles enable \\\n    --reduce_fusion enable \\\n    --use_fp8_context_fmha enable \\\n    --workers $tp_size \\\n    --low_latency_gemm_plugin fp8\n```\n\n----------------------------------------\n\nTITLE: Gathering and Setting Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all .cpp files in the current directory and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope, allowing it to be used in the parent CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/common/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Jais-13b-chat (Arabic Input)\nDESCRIPTION: This command runs inference using the built Jais-13b-chat engine with Arabic input text, demonstrating the model's multilingual capabilities.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../../../run.py --engine_dir jais-13b-chat/trt_engines/fp16/1-gpu \\\n        --tokenizer_dir core42/jais-13b-chat \\\n        --max_output_len 8 \\\n        --input_text \"ولد في 1304 ميلادياً ابن بطوطه, لقد ذهب\"\n```\n\n----------------------------------------\n\nTITLE: Downloading EXAONE-3.0 HuggingFace Checkpoint\nDESCRIPTION: Commands to download the HuggingFace FP32 checkpoint for EXAONE-3.0-7.8B-Instruct model. This sets up the model directory for later conversion to TensorRT-LLM format.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/exaone/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_MODEL_DIR=hf_models/exaone\ngit clone https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct $HF_MODEL_DIR\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for TensorRT-LLM Project\nDESCRIPTION: This requirements file lists all the Python packages needed for the TensorRT-LLM project. It includes a reference to external constraints file and specific versions of packages for datasets, evaluation, text processing, and tokenization that are required for working with large language models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/chatglm-6b/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nprotobuf\nrouge_score\nsentencepiece\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine\nDESCRIPTION: Command to build the TensorRT-LLM engine with FP16 precision and specified batch size and embedding table size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir=./tllm_checkpoint_1gpu_fp16 \\\n             --gemm_plugin=float16 --gpt_attention_plugin=float16 \\\n             --max_batch_size=1 --max_prompt_embedding_table_size=4096 \\\n             --output_dir=${ENGINE_DIR}/llm\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Executor Examples with CMake\nDESCRIPTION: Commands to build the Executor API examples using CMake. First, the TensorRT-LLM C++ shared libraries must be built, then these commands create a build directory, run CMake configuration, and compile the examples.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build\ncd build\ncmake ..\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: Lists required Python packages and their version constraints for the TensorRT-LLM project. Includes core dependencies like tensorrt_llm, datasets, sentencepiece, and evaluation tools with specific version requirements.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/prompt_lookup/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nrouge_score\nsentencepiece~=0.1.99\nevaluate\nordered-set\n```\n\n----------------------------------------\n\nTITLE: Cloning Grok-1 Repository\nDESCRIPTION: Command to clone the official Grok-1 code repository to start the implementation process.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/grok/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/xai-org/grok-1.git /path/to/folder\n```\n\n----------------------------------------\n\nTITLE: Downloading OPT Model Checkpoints from HuggingFace\nDESCRIPTION: Commands to clone different OPT model checkpoints from HuggingFace using git-lfs, including options for 125M, 350M, 2.7B, and 66B parameter models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# OPT-125M\ngit-lfs clone https://huggingface.co/facebook/opt-125m\n\n# OPT-350M\ngit-lfs clone https://huggingface.co/facebook/opt-350m\n\n# OPT-2.7B\ngit-lfs clone https://huggingface.co/facebook/opt-2.7b\n\n# OPT-66B\ngit-lfs clone https://huggingface.co/facebook/opt-66b\n```\n\n----------------------------------------\n\nTITLE: SWIGLU Test Configuration\nDESCRIPTION: Sets up test targets for SWIGLU (Swish-Gated Linear Unit) implementations with specific CUDA architecture optimizations and compiler flags.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/kernels/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(target_name gemmSwigluRunnerTest;gemmSwigluKernelTestSm90Fp8)\n  set_property(TARGET ${target_name} PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n\n  if(\"90\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG)\n    target_compile_definitions(${target_name} PRIVATE COMPILE_HOPPER_TMA_GEMMS)\n    target_compile_definitions(${target_name} PRIVATE COMPILE_HOPPER_TMA_GROUPED_GEMMS)\n  endif()\n\n  if(NOT WIN32)\n    target_compile_options(${target_name} PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcompiler=-Wno-psabi>)\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Implementing Smooth Quantization (SQ) for ChatGLM Models in TensorRT-LLM\nDESCRIPTION: Commands for converting a ChatGLM model with Smooth Quantization, building an optimized TensorRT engine, and running inference. SQ uses per-channel and per-token quantization with a smoothing factor of 0.5 to balance performance and accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# glm_4_9b: single gpu, int8 smooth quantization\npython3 convert_checkpoint.py --model_dir glm_4_9b \\\n        --smoothquant 0.5 \\\n        --per_channel \\\n        --per_token \\\n        --output_dir trt_ckpt/glm_4_9b/sq/1-gpu\n\n# glm_4_9b: single-gpu engine with int8 smooth quantization, GPT Attention plugin, Gemm plugin\ntrtllm-build --checkpoint_dir trt_ckpt/glm_4_9b/sq/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/glm_4_9b/sq/1-gpu\n\n# Run inference.\npython3 ../run.py --input_text \"What's new between ChatGLM3-6B and ChatGLM2-6B?\" \\\n        --max_output_len 50 \\\n        --tokenizer_dir glm_4_9b \\\n        --engine_dir trt_engines/glm_4_9b/sq/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Building Non-Medusa Low Latency Engine (Shell)\nDESCRIPTION: This command builds a non-Medusa low latency engine using trtllm-build. It specifies various parameters such as checkpoint directory, attention plugin, output directory, and performance optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ntrtllm-build \\\n    --checkpoint_dir /tmp/meta-llama/Meta-Llama-3-70B/checkpoint \\\n    --use_fused_mlp enable \\\n    --gpt_attention_plugin bfloat16 \\\n    --output_dir /tmp/meta-llama/Meta-Llama-3-70B/engine \\\n    --max_batch_size 1 \\\n    --max_seq_len $(($isl+$osl)) \\\n    --reduce_fusion enable \\\n    --gemm_plugin fp8 \\\n    --workers $tp_size \\\n    --use_fp8_context_fmha enable \\\n    --max_num_tokens $isl \\\n    --use_paged_context_fmha disable \\\n    --multiple_profiles enable\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all C++ source files in the current directory and adds them to the plugin sources list. Uses CMake's file(GLOB) command to find .cpp files and updates the parent scope variable.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/fp4GemmPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Enabling KV Cache Reuse in Triton Server Configuration\nDESCRIPTION: Triton server configuration parameter to enable KV cache reuse.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/kv-cache-reuse.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparameters: {\n  key: \"enable_kv_cache_reuse\"\n  value: {\n    string_value: \"true\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up SSH Key Authentication Between Nodes\nDESCRIPTION: Generates SSH keys on both hosts and establishes passwordless authentication between them. Also includes commands to restart the SSH service after configuration changes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# on host1\nssh-keygen -t ed25519 -f ~/.ssh/id_ed25519\nssh-copy-id -i ~/.ssh/id_ed25519.pub root@<HOST2>\n# on host2\nssh-keygen -t ed25519 -f ~/.ssh/id_ed25519\nssh-copy-id -i ~/.ssh/id_ed25519.pub root@<HOST1>\n\n# restart ssh service on host1 and host2\nservice ssh restart # or\n/etc/init.d/ssh restart # or\nsystemctl restart ssh\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for NVIDIA TensorRT-LLM\nDESCRIPTION: This snippet lists required Python packages for the NVIDIA TensorRT-LLM project. It includes 'accelerate' and specifies version 0.0.8 of 'qwen-vl-utils' due to a bug in version 0.0.9.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/multimodal/requirements-qwen2vl.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate\nqwen-vl-utils==0.0.8 # 0.0.9 has bug https://github.com/QwenLM/Qwen2-VL/pull/673, rollback until a newer version is released\n```\n\n----------------------------------------\n\nTITLE: Running executorExampleDebug Example\nDESCRIPTION: Command to run the debug Executor example that shows how to define engine IO tensors to be dumped to numpy files for debugging purposes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./executorExampleDebug <path_to_engine_dir>\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for FairSeq NMT Decoder\nDESCRIPTION: Command to build TensorRT engine for FairSeq NMT decoder, specifying various parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir tmp/trt_models/wmt14/${INFERENCE_PRECISION}/decoder \\\n                --output_dir tmp/trt_engines/wmt14/${INFERENCE_PRECISION}/decoder \\\n                --moe_plugin disable \\\n                --max_beam_width 1 \\\n                --max_batch_size 8 \\\n                --max_input_len 1 \\\n                --max_seq_len 201 \\\n                --max_encoder_input_len 1024 \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding disable\n```\n\n----------------------------------------\n\nTITLE: Running Advanced TensorRT-LLM Examples with Various Model Configurations\nDESCRIPTION: Commands for running TensorRT-LLM with different precision formats (BF16, FP8), tensor parallelism configurations, and KV cache data types.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/pytorch/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# BF16\npython3 quickstart_advanced.py --model_dir meta-llama/Llama-3.1-8B-Instruct\n\n# FP8\npython3 quickstart_advanced.py --model_dir nvidia/Llama-3.1-8B-Instruct-FP8\n\n# BF16 + TP=2\npython3 quickstart_advanced.py --model_dir meta-llama/Llama-3.1-8B-Instruct --tp_size 2\n\n# FP8 + TP=2\npython3 quickstart_advanced.py --model_dir nvidia/Llama-3.1-8B-Instruct-FP8 --tp_size 2\n\n# FP8(e4m3) kvcache\npython3 quickstart_advanced.py --model_dir nvidia/Llama-3.1-8B-Instruct-FP8 --kv_cache_dtype fp8\n```\n\n----------------------------------------\n\nTITLE: Multi-Device Test Configuration\nDESCRIPTION: Conditionally adds tests for multi-device functionality when ENABLE_MULTI_DEVICE is enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/kernels/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ENABLE_MULTI_DEVICE EQUAL 0)\n  add_gtest(allReduceKernelTest allReduce/allReduceKernelTest.cu)\n  # add_gtest(gemmAllReduceTest allReduce/gemmAllReduceTest.cu)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Refitting Engine with Weights\nDESCRIPTION: Command to refit a stripped engine with weights from a checkpoint using trtllm-refit tool.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/sample_weight_stripping/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-refit --checkpoint_dir ${CHECKPOINT_DIR} --engine_dir ${ENGINE_DIR}\n```\n\n----------------------------------------\n\nTITLE: Converting and Building TensorRT-LLM Language-Adapter Engine\nDESCRIPTION: Bash script for preparing TensorRT-LLM engines for a Language-Adapter model. It sets configuration parameters, converts the model checkpoint, and builds separate engines for the encoder and decoder components.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/language_adapter/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_DIR=\"dummy_model\" # model not publicly available\nINFERENCE_PRECISION=\"float16\"\nTP_SIZE=1\nPP_SIZE=1\nWORLD_SIZE=1\nMODEL_TYPE=language_adapter\nMODEL_NAME=$MODEL_TYPE\nCKPT_DIR=/scratch/tmp/trt_models/${MODEL_NAME}/${WORLD_SIZE}-gpu/${INFERENCE_PRECISION}\nENGINE_DIR=/scratch/tmp/trt_engines/${MODEL_NAME}/${WORLD_SIZE}-gpu/${INFERENCE_PRECISION}\n\nmax_beam=5\nmax_batch=32\nmax_input_len=1024\nmax_output_len=1024\n\npython ../enc_dec/convert_checkpoint.py --model_type ${MODEL_TYPE} \\\n                --model_dir ${MODEL_DIR} \\\n                --output_dir $CKPT_DIR \\\n                --tp_size ${TP_SIZE} \\\n                --pp_size ${PP_SIZE} \\\n                --dtype ${INFERENCE_PRECISION} \\\n                --workers 1\n\ntrtllm-build --checkpoint_dir $CKPT_DIR/encoder \\\n                --output_dir $ENGINE_DIR/encoder \\\n                --paged_kv_cache disable \\\n                --moe_plugin auto \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --gemm_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding enable \\\n                --max_input_len ${max_input_len} \\\n                --max_beam_width ${max_beam} \\\n                --max_batch_size ${max_batch}\n\ntrtllm-build --checkpoint_dir $CKPT_DIR/decoder \\\n                --output_dir $ENGINE_DIR/decoder \\\n                --paged_kv_cache enable \\\n                --moe_plugin auto \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --gemm_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding enable \\\n                --max_input_len 1 \\\n                --max_beam_width ${max_beam} \\\n                --max_batch_size ${max_batch} \\\n                --max_seq_len ${max_output_len}\n```\n\n----------------------------------------\n\nTITLE: Sampling Kernel Test Configuration\nDESCRIPTION: Configures test targets for sampling-related functionality including TopK, TopP, and penalty testing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/kernels/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(SAMPLING_KERNEL_TEST_SRC\n    sampling/samplingTest.cpp sampling/samplingTopKTest.cpp\n    sampling/samplingTopPTest.cpp sampling/samplingAirTopPTest.cpp\n    sampling/samplingPenaltyTest.cpp sampling/samplingUtilsTest.cu)\n\nadd_gtest(samplingKernelsTest \"${SAMPLING_KERNEL_TEST_SRC}\")\n```\n\n----------------------------------------\n\nTITLE: Running BART Inference with LoRA\nDESCRIPTION: Python command to run inference using the built BART engine with LoRA, specifying engine directory, model name, and LoRA settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython run.py \\\n        --engine_dir tmp/trt_engines/bart-large-cnn/${INFERENCE_PRECISION}/ \\\n        --engine_name bart-large-cnn \\\n        --model_name tmp/hf_models/bart-large-cnn \\\n        --max_new_token=64 \\\n        --num_beams=1 \\\n        --lora_dir tmp/hf_models/bart-large-cnn-samsum-lora/ \\\n        --lora_task_uids 0 0 0\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Custom Plugin with Custom Paths\nDESCRIPTION: Command to build the TensorRT-LLM custom plugin with custom library paths for TensorRT and TensorRT-LLM libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DTRT_LIB_DIR=</path/to/trt_lib> -DTRT_INCLUDE_DIR=</path/to/trt_headers> -DTRT_LLM_LIB_DIR=</path/to/trt_llm_lib> ..\n```\n\n----------------------------------------\n\nTITLE: Adding Plugin Sources in CMake\nDESCRIPTION: Collects all CPP files in the current directory and adds them to the plugin sources variable, then propagates the updated list to the parent scope. Uses CMake's file(GLOB) command to gather source files and set() to manage variables.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/ncclPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Deploying TensorRT-LLM Server Command\nDESCRIPTION: Command to start an OpenAI-compatible server using trtllm-serve with TinyLlama model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/quick-start-guide.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-serve \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n```\n\n----------------------------------------\n\nTITLE: Running Inference Tests\nDESCRIPTION: Commands to test the built engines using run.py and summarize.py scripts\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../run.py --max_output_len=120 \\\n                 --input_text 'Tell me about yourself.' \\\n                 --tokenizer_dir ./internlm-chat-7b/ \\\n                 --engine_dir ./internlm-chat-7b/trt_engines/int8_kv_cache_weight_only/1-gpu\n\npython ../../../summarize.py --test_trt_llm --test_hf \\\n                       --hf_model_dir ./internlm-chat-7b \\\n                       --data_type fp16 \\\n                       --engine_dir ./internlm-chat-7b/trt_engines/int8_kv_cache_weight_only/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Utility Testing in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet conditionally adds a Google Test (gtest) for TensorRT-LLM PyTorch utilities. It creates the thUtilsTest executable from thUtilsTest.cpp and links it with the th_utils library, Python libraries, and PyTorch libraries when the BUILD_PYT option is enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/thop/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(${BUILD_PYT})\n  add_gtest(thUtilsTest thUtilsTest.cpp)\n  target_link_libraries(thUtilsTest PUBLIC th_utils ${Python3_LIBRARIES}\n                                           ${TORCH_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Converting InternLM2 7B with INT8 Weight-Only Quantization (Alternative)\nDESCRIPTION: An alternative way to convert the InternLM2 7B model with INT8 weight-only quantization, keeping the structure of the output directory separate for weight-only optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm2-chat-7b  \\\n                             --output_dir ./internlm2-chat-7b/w8a16/ \\\n                             --dtype float16  \\\n                             --use_weight_only \\\n                             --weight_only_precision int8\n```\n\n----------------------------------------\n\nTITLE: Setting Up Basic Pod Container Start Command\nDESCRIPTION: Basic container start command to keep the pod running indefinitely\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/dev-on-cloud/dev-on-runpod.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsleep infinity\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for GPT-NeoX\nDESCRIPTION: Installs the necessary Python packages for working with GPT-NeoX using pip.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Sampling Layer Tests in CMake\nDESCRIPTION: Sets up the sampling layer test target with multiple source files that test various sampling implementations including base, general, topK, topP, and external draft tokens.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/layers/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLING_LAYER_TEST_SRC\n    baseSamplingLayerTest.cpp samplingLayerTest.cpp topKSamplingLayerTest.cpp\n    topPSamplingLayerTest.cpp externalDraftTokensLayerTest.cpp)\nadd_gtest(samplingLayerTest \"${SAMPLING_LAYER_TEST_SRC}\")\n```\n\n----------------------------------------\n\nTITLE: GPT-J Weight Download Commands\nDESCRIPTION: Commands to download GPT-J model weights, vocabulary and merge tables from Hugging Face.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/sample_weight_stripping/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/EleutherAI/gpt-j-6b\npushd gpt-j-6b && \\\n  rm -f pytorch_model.bin && \\\n  wget https://huggingface.co/EleutherAI/gpt-j-6b/resolve/main/pytorch_model.bin && \\\npopd\n\nwget https://huggingface.co/EleutherAI/gpt-j-6b/resolve/main/vocab.json\nwget https://huggingface.co/EleutherAI/gpt-j-6b/resolve/main/merges.txt\n```\n\n----------------------------------------\n\nTITLE: HuggingFace to TensorRT-LLM Weight Format Example\nDESCRIPTION: Example showing the difference between HuggingFace LLaMA checkpoint format and TensorRT-LLM expected weights format, including tensor shapes and naming conventions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# HuggingFace LLaMA checkpoints\n{\n    \"model.embed_tokens.weight\": torch.Tensor([vocab_size, hidden_size])\n    \"model.layers.0.input_layernorm.weight\": torch.Tensor([hidden_size]),\n    \"model.layers.0.mlp.down_proj.weight\": torch.Tensor([hidden_size, inter_size]),\n    \"model.layers.0.mlp.gate_proj.weight\": torch.Tensor([inter_size, hidden_size]),\n    \"model.layers.0.mlp.up_proj.weight\": torch.Tensor([inter_size, hidden_size]),\n    \"model.layers.0.post_attention_layernorm.weight\": torch.Tensor([hidden_size]),\n    \"model.layers.0.self_attn.q_proj.weight\": torch.Tensor([hidden_size, hidden_size]),\n    \"model.layers.0.self_attn.k_proj.weight\": torch.Tensor([hidden_size, hidden_size]),\n    \"model.layers.0.self_attn.v_proj.weight\": torch.Tensor([hidden_size, hidden_size]),\n    \"model.layers.0.self_attn.o_proj.weight\": torch.Tensor([hidden_size, hidden_size]),\n    ...,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookahead Random LLM Tests in CMake\nDESCRIPTION: Sets up tests for the random language model implementation used in lookahead functionality testing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/layers/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(LOOKAHEAD_RANDOMLLM_TEST_SRC randomLlm.cpp lookaheadRandomLlmTest.cpp)\nadd_gtest(lookaheadRandomLlmTest \"${LOOKAHEAD_RANDOMLLM_TEST_SRC}\")\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Advanced SmoothQuant Model\nDESCRIPTION: Command to build a TensorRT engine from an advanced SmoothQuant processed checkpoint with per-token and per-channel optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_sq \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Fetching Metrics from TensorRT-LLM Server\nDESCRIPTION: Curl command to retrieve runtime-iteration statistics from the /metrics endpoint of the trtllm-serve server.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET http://<host>:<port>/metrics\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM Python Bindings Module in CMake\nDESCRIPTION: Defines the module name and source files for the TensorRT-LLM Python bindings. Sets up compilation flags, linked libraries, and runtime path configurations to ensure proper loading of dependencies when the module is imported in Python.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/pybind/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TRTLLM_PYBIND_MODULE bindings)\nset(TRTLLM_PYBIND_MODULE\n    ${TRTLLM_PYBIND_MODULE}\n    PARENT_SCOPE)\n\nset(SRCS\n    batch_manager/algorithms.cpp\n    batch_manager/bindings.cpp\n    batch_manager/buffers.cpp\n    batch_manager/cacheTransceiver.cpp\n    batch_manager/kvCacheManager.cpp\n    batch_manager/llmRequest.cpp\n    executor/bindings.cpp\n    executor/executor.cpp\n    executor/executorConfig.cpp\n    executor/request.cpp\n    runtime/bindings.cpp\n    userbuffers/bindings.cpp\n    ../runtime/ipcNvlsMemory.cpp\n    bindings.cpp)\n\ninclude_directories(${PROJECT_SOURCE_DIR}/include)\n\npybind11_add_module(${TRTLLM_PYBIND_MODULE} ${SRCS})\n\nset_property(TARGET ${TRTLLM_PYBIND_MODULE} PROPERTY POSITION_INDEPENDENT_CODE\n                                                     ON)\n\ntarget_link_directories(${TRTLLM_PYBIND_MODULE} PUBLIC\n                        \"${TORCH_INSTALL_PREFIX}/lib\")\ntarget_link_libraries(\n  ${TRTLLM_PYBIND_MODULE}\n  PUBLIC ${SHARED_TARGET} ${UNDEFINED_FLAG} ${NO_AS_NEEDED_FLAG}\n         ${Python3_LIBRARIES} ${TORCH_LIBRARIES} torch_python)\ntarget_compile_definitions(\n  ${TRTLLM_PYBIND_MODULE} PUBLIC TRTLLM_PYBIND_MODULE=${TRTLLM_PYBIND_MODULE}\n                                 PYBIND11_DETAILED_ERROR_MESSAGES=1)\n\nif(NOT WIN32)\n  set_target_properties(\n    ${TRTLLM_PYBIND_MODULE}\n    PROPERTIES\n      LINK_FLAGS\n      \"-Wl,-rpath,'$ORIGIN/libs' -Wl,-rpath,'$ORIGIN/../nvidia/nccl/lib' ${AS_NEEDED_FLAG} ${UNDEFINED_FLAG}\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU LLaMA Benchmark Command\nDESCRIPTION: Example command for benchmarking LLaMA 7B model across multiple GPUs using MPI, with specified batch sizes and input-output lengths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/python/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 python benchmark.py \\\n    -m dec \\\n    --engine_dir llama_7b \\\n    --batch_size \"1;8;64\" \\\n    --input_output_len \"60,20;128,20\"\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Without LoRAs\nDESCRIPTION: MPI-based benchmark execution without LoRA adapters, configuring cache sizes and memory allocation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ${EG_DIR}/log-base-lora\nmpirun -n ${TP} --output-filename ${EG_DIR}/log-base-lora \\\n    cpp/build/benchmarks/gptManagerBenchmark \\\n    --engine_dir $LORA_ENGINE \\\n    --type IFB \\\n    --dataset \"${EG_DIR}/data/token-norm-dist.json\" \\\n    --lora_host_cache_bytes 8589934592 \\\n    --lora_num_device_mod_layers $(( 32 * $NUM_LAYERS * $NUM_LORA_MODS * $MAX_LORA_RANK )) \\\n    --kv_cache_free_gpu_mem_fraction 0.70 \\\n    --log_level info \\\n    --eos_id ${EOS_ID}\n```\n\n----------------------------------------\n\nTITLE: Configuring Beam Search Layer Tests in CMake\nDESCRIPTION: Defines the source files for beam search layer tests, including the base sampling layer test file and the beam search specific test file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/layers/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(BEAM_SEARCH_LAYER_TEST_SRC baseSamplingLayerTest.cpp\n                               beamSearchLayerTest.cpp)\nadd_gtest(beamSearchLayerTest \"${BEAM_SEARCH_LAYER_TEST_SRC}\")\n```\n\n----------------------------------------\n\nTITLE: Filtering CUDA Architecture Support in CMake\nDESCRIPTION: Applies architecture-specific filtering to the source files for CUDA compute capabilities 80, 86, 89, 90, 100, and 120, ensuring compatibility with targeted NVIDIA GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfilter_cuda_archs(\"80\" SRC_CPP)\nfilter_cuda_archs(\"86\" SRC_CPP)\nfilter_cuda_archs(\"89\" SRC_CPP)\nfilter_cuda_archs(\"90\" SRC_CPP)\nfilter_cuda_archs(\"100\" SRC_CPP)\nfilter_cuda_archs(\"120\" SRC_CPP)\n```\n\n----------------------------------------\n\nTITLE: Running the TensorRT Engine\nDESCRIPTION: Command to execute the built engine using MPI for parallel processing with specified tokenizer and input parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/granite/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n ${TP} --allow-run-as-root python ../run.py --engine_dir ./tmp/trt_engines/${ENGINE} --tokenizer_dir tmp/hf_checkpoints/${HF_MODEL} --max_output_len 20 --input_text \"The future of AI is\"\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Tasks\nDESCRIPTION: Commands to run summarization tasks using built TensorRT engines on the CNN/DailyMail dataset\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mamba/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./mamba_model/mamba-2.8b/ \\\n                       --tokenizer_dir ./mamba_model/gpt-neox-20b/ \\\n                       --data_type bf16 \\\n                       --engine_dir ./mamba_model/mamba-2.8b/trt_engines/bf16/1-gpu/\n\nmpirun -n 2 --allow-run-as-root \\\n    python ../summarize.py --test_trt_llm \\\n                           --hf_model_dir ./mamba_model/mamba-codestral-7B-v0.1/ \\\n                           --tokenizer_dir ./mamba_model/mamba-codestral-7B-v0.1/ \\\n                           --data_type fp16 \\\n                           --engine_dir ./mamba_model/mamba-codestral-7B-v0.1/trt_engines/fp16/2-gpu/\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Device Support\nDESCRIPTION: Adds MPI, NCCL, and NVIDIA Management Library support when multi-device operation is enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/thop/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_MULTI_DEVICE)\n  target_include_directories(th_common PUBLIC ${MPI_C_INCLUDE_DIRS})\n  target_link_libraries(th_common PRIVATE ${MPI_C_LIBRARIES} ${NCCL_LIB}\n                                          CUDA::nvml)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading DBRX Models from HuggingFace\nDESCRIPTION: Commands to download DBRX model variants from HuggingFace using git clone. Supports both dbrx-base and dbrx-instruct models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Download dbrx-base\ngit clone https://huggingface.co/databricks/dbrx-base\n\n# Download dbrx-instruct\ngit clone https://huggingface.co/databricks/dbrx-instruct\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Docker Image in One Step\nDESCRIPTION: Command to build a TensorRT-LLM Docker image in one step using make, with an option to specify CUDA architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/build-from-source-linux.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker release_build\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Restrict the compilation to Ada and Hopper architectures.\nmake -C docker release_build CUDA_ARCHS=\"89-real;90-real\"\n```\n\n----------------------------------------\n\nTITLE: Adding GTest Targets for TensorRT-LLM Utils\nDESCRIPTION: Defines multiple test targets using add_gtest CMake function for various utility components including CUDA, memory, quantization, and string utilities. Each test file is compiled as a separate executable test target.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/common/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(cudaProfilerUtilsTest cudaProfilerUtilsTest.cpp)\nadd_gtest(cudaUtilsTest cudaUtilsTest.cpp)\nadd_gtest(memoryUtilsTest memoryUtilsTest.cu)\nadd_gtest(optionalRefTest optionalRefTest.cpp)\nadd_gtest(quantizationTest quantizationTest.cpp)\nadd_gtest(stlUtilsTest stlUtilsTest.cpp)\nadd_gtest(stringUtilsTest stringUtilsTest.cpp)\nadd_gtest(timestampUtilsTest timestampUtilsTest.cpp)\nadd_gtest(tllmExceptionTest tllmExceptionTest.cpp)\nadd_gtest(cudaDriverWrapperTest cudaDriverWrapperTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT-LLM Quantization Toolkit Dependencies\nDESCRIPTION: Commands to install the additional requirements for the TensorRT-LLM quantization toolkit. This is done after changing to the examples/quantization directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the additional requirements\ncd examples/quantization\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Advanced Example with CSV Input Tokens\nDESCRIPTION: Command for running the advanced example with a CSV file containing input tokens. This demonstrates how to handle multiple concurrent requests and process them efficiently with TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 example_advanced.py --model_path <model_path> --input_tokens_csv_file input_tokens.csv\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific Compiler Flags\nDESCRIPTION: Sets compiler flags based on the target platform (Windows vs Unix-like systems)\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/batch_manager/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT WIN32)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n  if(WARNING_IS_ERROR)\n    message(STATUS \"Treating warnings as errors in GCC compilation\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n  endif()\nelse() # Windows\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Single GPU LLaMA Benchmark Command\nDESCRIPTION: Example command for benchmarking LLaMA 7B model on a single GPU with multiple batch sizes and input-output length combinations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/python/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark.py \\\n    -m dec \\\n    --engine_dir llama_7b \\\n    --batch_size \"1;8;64\" \\\n    --input_output_len \"60,20;128,20\"\n```\n\n----------------------------------------\n\nTITLE: Input Data JSON Structure for Inference\nDESCRIPTION: JSON format for specifying input text, instruction and output placeholders for the model\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"input\": \"James Best, best known for his \",\n        \"instruction\": \"Continue writing the following story:\",\n        \"output\": \"                                                                \"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Top Level Directory Variable in CMake\nDESCRIPTION: Sets a variable pointing to the top-level directory of the project for use in compilation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TOP_LEVEL_DIR \"${PROJECT_SOURCE_DIR}/..\")\n```\n\n----------------------------------------\n\nTITLE: Running Single Inference\nDESCRIPTION: Command to run inference with the FP16 LLM engine on a single audio file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen2audio/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 run.py \\\n    --tokenizer_dir=$MODEL_PATH \\\n    --engine_dir=${ENGINE_DIR}/llm \\\n    --audio_engine_path=${ENGINE_DIR}/audio/model.engine \\\n    --audio_url='./audio/glass-breaking-151256.mp3'\n```\n\n----------------------------------------\n\nTITLE: Running DeepSeek-V3 with Multi-Token Prediction (MTP)\nDESCRIPTION: Command to run DeepSeek-V3 with Multi-Token Prediction, allowing the user to specify the number of MTP modules to enable.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/pytorch\npython quickstart_advanced.py --model_dir <YOUR_MODEL_DIR> --spec_decode_algo MTP --spec_decode_nextn N\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA and TensorRT Dependencies\nDESCRIPTION: Locates and configures CUDA and TensorRT dependencies, including library paths and include directories. Handles both default installations and package-based installations of TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(CUDA REQUIRED)\nmessage(STATUS \"CUDA library status:\")\nmessage(STATUS \"    config: ${CUDA_DIR}\")\nmessage(STATUS \"    version: ${CUDA_VERSION}\")\nmessage(STATUS \"    libraries: ${CUDA_LIBRARIES}\")\nmessage(STATUS \"    include path: ${CUDA_INCLUDE_DIRS}\")\n\nif(NOT DEFINED TRT_INCLUDE_DIR)\n  set(TRT_INCLUDE_DIR \"/usr/local/tensorrt/include\")\n  if(NOT EXISTS ${TRT_INCLUDE_DIR})\n    # In case of TensorRT installed from a deb package.\n    set(TRT_INCLUDE_DIR \"/usr/include/x86_64-linux-gnu\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install necessary Python packages for running Phi models\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/phi/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT-LLM Dependencies\nDESCRIPTION: Commands to install PyTorch with CUDA support and TensorRT-LLM with its dependencies on Ubuntu 24.04.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/grace-hopper.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n\nsudo apt-get -y install libopenmpi-dev && pip3 install --upgrade pip setuptools && pip3 install tensorrt_llm\n```\n\n----------------------------------------\n\nTITLE: Adding GTest Unit Tests in CMake for TensorRT-LLM\nDESCRIPTION: Configures multiple unit tests using Google Test framework for different components of TensorRT-LLM. Tests cover capacity scheduling, context progress, cache management, request handling, and thread pool functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/batch_manager/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(capacitySchedulerTest capacitySchedulerTest.cpp)\nadd_gtest(contextProgressTest contextProgressTest.cu)\nadd_gtest(evictionPolicyTest evictionPolicyTest.cpp)\nadd_gtest(kvCacheManagerTest kvCacheManagerTest.cpp)\nadd_gtest(kvCacheUtilsTest kvCacheUtilsTest.cpp)\nadd_gtest(llmRequestTest llmRequestTest.cpp)\nadd_gtest(microBatchSchedulerTest microBatchSchedulerTest.cpp)\nadd_gtest(staticThreadPoolTest staticThreadPoolTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Converting Qwen-72B-Chat Model with 8-way Tensor Parallelism\nDESCRIPTION: Command to convert a Qwen-72B-Chat model using 8-way tensor parallelism with float16 precision. The script takes the original model directory and outputs a TensorRT-LLM compatible checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/72B/ \\\n                            --output_dir ./tllm_checkpoint_8gpu_tp8 \\\n                            --dtype float16 \\\n                            --tp_size 8\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Nemotron-NAS Model Conversion\nDESCRIPTION: Defines environment variables for model directories and parallelism settings used in the conversion process.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron_nas/README.md#2025-04-07_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nexport MODEL_DIR=\"~/models/huggingface/nemotron-nas\"\nexport TRT_CHECKPOINT_DIR=\"~/models/intermediate/nemotron-nas\"\nexport TRT_ENGINE_DIR=\"~/models/engines/nemotron-nas\"\nexport TP_SIZE=1\nexport PP_SIZE=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Components and Core Subdirectories in TensorRT-LLM CMake\nDESCRIPTION: This CMake snippet conditionally adds batch manager and executor subdirectories if they exist and are enabled. It then adds core subdirectories for the TensorRT-LLM project, including common, kernels, layers, runtime, thop, and utils.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_BATCH_MANAGER)\n  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/batch_manager)\n    add_subdirectory(batch_manager)\n  endif()\nendif()\n\nif(BUILD_EXECUTOR)\n  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/executor)\n    add_subdirectory(executor)\n  endif()\nendif()\n\nadd_subdirectory(common)\nadd_subdirectory(kernels)\nadd_subdirectory(layers)\nadd_subdirectory(runtime)\nadd_subdirectory(thop)\nadd_subdirectory(utils)\n```\n\n----------------------------------------\n\nTITLE: Enabling TRACE Level Logging in TensorRT-LLM\nDESCRIPTION: Shows how to set the TLLM_LOG_LEVEL environment variable to TRACE to get detailed runtime information about the TensorRT engine, including shapes of input/output tensors and allowed ranges.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport TLLM_LOG_LEVEL=TRACE\n```\n\n----------------------------------------\n\nTITLE: Triton Server Log Output Format\nDESCRIPTION: Example of successful Triton server launch log showing HTTP, gRPC and Metrics service ports\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_4\n\nLANGUAGE: txt\nCODE:\n```\nStarted HTTPService at 0.0.0.0:8000\nStarted GRPCInferenceService at 0.0.0.0:8001\nStarted Metrics Service at 0.0.0.0:8002\n```\n\n----------------------------------------\n\nTITLE: Configuring Triton Server for Draft-Target Model Architecture\nDESCRIPTION: Script to set up Triton Server configuration files for the draft-target model architecture. It creates separate configurations for draft and target models, along with pre/post-processing and BLS (Business Logic Scripting) components.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nACCUMULATE_TOKEN=\"false\"\nBACKEND=\"tensorrtllm\"\nBATCH_SCHEDULER_POLICY=\"guaranteed_no_evict\"\nBATCHING_STRATEGY=\"inflight_fused_batching\"\nBLS_INSTANCE_COUNT=\"1\"\nDECODING_MODE=\"top_k_top_p\"\nDECOUPLED_MODE=\"False\"\nDRAFT_GPU_DEVICE_IDS=\"0\"\nE2E_MODEL_NAME=\"ensemble\"\nENABLE_KV_CACHE_REUSE=\"true\"\nENGINE_PATH=$TARGET_ENGINE_PATH\nEXCLUDE_INPUT_IN_OUTPUT=\"false\"\nKV_CACHE_FREE_GPU_MEM_FRACTION=\"0.8\"\nMAX_ATTENTION_WINDOW_SIZE=\"\"\nMAX_BEAM_WIDTH=\"1\"\nMAX_QUEUE_DELAY_MICROSECONDS=\"0\"\nMAX_TOKENS_IN_KV_CACHE=\"\"\nNORMALIZE_LOG_PROBS=\"true\"\nPOSTPROCESSING_INSTANCE_COUNT=\"1\"\nPREPROCESSING_INSTANCE_COUNT=\"1\"\nTARGET_GPU_DEVICE_IDS=\"1\"\nTENSORRT_LLM_DRAFT_MODEL_NAME=\"tensorrt_llm_draft\"\nTENSORRT_LLM_MODEL_NAME=\"tensorrt_llm\"\nTOKENIZER_PATH=$DRAFT_MODEL_PATH\nTOKENIZER_TYPE=llama\nTRITON_GRPC_PORT=\"8001\"\nTRITON_HTTP_PORT=\"8000\"\nTRITON_MAX_BATCH_SIZE=\"4\"\nTRITON_METRICS_PORT=\"8002\"\nTRITON_REPO=\"triton_repo\"\nUSE_DRAFT_LOGITS=\"false\"\nLOGITS_DATATYPE=\"TYPE_FP32\" # Replace by TYPE_FP16 for FP8 model\n\n# Make a copy of triton repo and replace the fields in the configuration files\ncd /tensorrtllm_backend/\napt-get update && apt-get install -y build-essential cmake git-lfs\npip3 install git-lfs tritonclient grpcio\nrm -rf ${TRITON_REPO}\ncp -R all_models/inflight_batcher_llm ${TRITON_REPO}\npython3 tools/fill_template.py -i ${TRITON_REPO}/ensemble/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},logits_datatype:${LOGITS_DATATYPE}\npython3 tools/fill_template.py -i ${TRITON_REPO}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_PATH},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},preprocessing_instance_count:${PREPROCESSING_INSTANCE_COUNT}\npython3 tools/fill_template.py -i ${TRITON_REPO}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_PATH},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},postprocessing_instance_count:${POSTPROCESSING_INSTANCE_COUNT},logits_datatype:${LOGITS_DATATYPE}\npython3 tools/fill_template.py -i ${TRITON_REPO}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},accumulate_tokens:${ACCUMULATE_TOKEN},bls_instance_count:${BLS_INSTANCE_COUNT},tensorrt_llm_model_name:${TENSORRT_LLM_MODEL_NAME},tensorrt_llm_draft_model_name:${TENSORRT_LLM_DRAFT_MODEL_NAME},logits_datatype:${LOGITS_DATATYPE}\n\n# Make a copy of tensorrt_llm as configurations of draft / target models.\ncp -R ${TRITON_REPO}/tensorrt_llm ${TRITON_REPO}/tensorrt_llm_draft\nsed -i 's/name: \"tensorrt_llm\"/name: \"tensorrt_llm_draft\"/g' ${TRITON_REPO}/tensorrt_llm_draft/config.pbtxt\npython3 tools/fill_template.py -i ${TRITON_REPO}/tensorrt_llm/config.pbtxt          triton_backend:${BACKEND},engine_dir:${ENGINE_PATH},decoupled_mode:${DECOUPLED_MODE},max_tokens_in_paged_kv_cache:${MAX_TOKENS_IN_KV_CACHE},max_attention_window_size:${MAX_ATTENTION_WINDOW_SIZE},batch_scheduler_policy:${BATCH_SCHEDULER_POLICY},batching_strategy:${BATCHING_STRATEGY},kv_cache_free_gpu_mem_fraction:${KV_CACHE_FREE_GPU_MEM_FRACTION},exclude_input_in_output:${EXCLUDE_INPUT_IN_OUTPUT},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MICROSECONDS},max_beam_width:${MAX_BEAM_WIDTH},enable_kv_cache_reuse:${ENABLE_KV_CACHE_REUSE},normalize_log_probs:${NORMALIZE_LOG_PROBS},enable_chunked_context:${ENABLE_CHUNKED_CONTEXT},gpu_device_ids:${TARGET_GPU_DEVICE_IDS},decoding_mode:${DECODING_MODE},encoder_input_features_data_type:TYPE_FP16,logits_datatype:${LOGITS_DATATYPE}\npython3 tools/fill_template.py -i ${TRITON_REPO}/tensorrt_llm_draft/config.pbtxt    triton_backend:${BACKEND},engine_dir:${DRAFT_ENGINE_PATH},decoupled_mode:${DECOUPLED_MODE},max_tokens_in_paged_kv_cache:${MAX_TOKENS_IN_KV_CACHE},max_attention_window_size:${MAX_ATTENTION_WINDOW_SIZE},batch_scheduler_policy:${BATCH_SCHEDULER_POLICY},batching_strategy:${BATCHING_STRATEGY},kv_cache_free_gpu_mem_fraction:${KV_CACHE_FREE_GPU_MEM_FRACTION},exclude_input_in_output:${EXCLUDE_INPUT_IN_OUTPUT},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MICROSECONDS},max_beam_width:${MAX_BEAM_WIDTH},enable_kv_cache_reuse:${ENABLE_KV_CACHE_REUSE},normalize_log_probs:${NORMALIZE_LOG_PROBS},enable_chunked_context:${ENABLE_CHUNKED_CONTEXT},gpu_device_ids:${DRAFT_GPU_DEVICE_IDS},decoding_mode:${DECODING_MODE},encoder_input_features_data_type:TYPE_FP16,logits_datatype:${LOGITS_DATATYPE}\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for FairSeq NMT Encoder\nDESCRIPTION: Command to build TensorRT engine for FairSeq NMT encoder, specifying various parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir tmp/trt_models/wmt14/${INFERENCE_PRECISION}/encoder \\\n                --output_dir tmp/trt_engines/wmt14/${INFERENCE_PRECISION}/encoder \\\n                --paged_kv_cache disable \\\n                --moe_plugin disable \\\n                --max_beam_width 1 \\\n                --max_batch_size 8 \\\n                --max_input_len 1024 \\\n                --bert_attention_plugin ${INFERENCE_PRECISION} \\\n                --gpt_attention_plugin ${INFERENCE_PRECISION} \\\n                --remove_input_padding disable\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM with Python-Only Option\nDESCRIPTION: Commands to package and install TensorRT-LLM without C++ compilation, suitable for Python-only development.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/build-from-source-linux.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Package TensorRT-LLM wheel.\nTRTLLM_USE_PRECOMPILED=1 pip wheel . --no-deps --wheel-dir ./build\n\n# Install TensorRT-LLM wheel.\npip install ./build/tensorrt_llm*.whl\n```\n\nLANGUAGE: bash\nCODE:\n```\nTRTLLM_USE_PRECOMPILED=1 pip install -e .\n```\n\nLANGUAGE: bash\nCODE:\n```\nTRTLLM_PRECOMPILED_LOCATION=https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-0.16.0-cp312-cp312-linux_x86_64.whl pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Basic Benchmark Help Command\nDESCRIPTION: Command to display the benchmark script's help documentation and available options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/python/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark.py -h\n```\n\n----------------------------------------\n\nTITLE: Model Setup and Conversion Commands\nDESCRIPTION: Series of commands to clone model repository, convert checkpoint, and compile the model into TensorRT engine format.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/quick-start-guide.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n\ncd examples/llama\npip install -r requirements.txt\npip install --upgrade transformers\npython3 convert_checkpoint.py --model_dir Meta-Llama-3.1-8B-Instruct --output_dir llama-3.1-8b-ckpt\n\ntrtllm-build --checkpoint_dir llama-3.1-8b-ckpt \\\n    --gemm_plugin float16 \\\n    --output_dir ./llama-3.1-8b-engine\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT-LLM Model Specification Shared Library in CMake\nDESCRIPTION: Defines a shared library target 'modelSpec' from source files, sets library naming properties, and links it with tensorrt_llm dependency. This configuration creates a shared library with the output name 'model_spec' (without the usual 'lib' prefix).\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/batch_manager/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(modelSpec SHARED modelSpec.cpp modelSpecBinding.cpp)\nset_target_properties(modelSpec PROPERTIES PREFIX \"\")\nset_target_properties(modelSpec PROPERTIES OUTPUT_NAME \"model_spec\")\ntarget_link_libraries(modelSpec PRIVATE tensorrt_llm)\n```\n\n----------------------------------------\n\nTITLE: Converting InternLM 20B Model with SmoothQuant\nDESCRIPTION: This command converts the InternLM 20B model to TensorRT-LLM format using SmoothQuant quantization with a smoothing factor of 0.5, float16 precision, and per-channel and per-token optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython convert_checkpoint.py --model_dir ./internlm-chat-20b  --output_dir ./internlm-chat-20b/smooth_internlm/sq0.5/ --dtype float16 --smoothquant 0.5 --per_channel --per_token\n```\n\n----------------------------------------\n\nTITLE: Downloading Arctic Model Checkpoints from HuggingFace\nDESCRIPTION: Commands to clone the Arctic model checkpoints from HuggingFace repository. Note that this model requires approximately 900GB of storage space due to its MoE architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/arctic/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nHF_MODEL=\"arctic\"\ngit clone https://huggingface.co/Snowflake/snowflake-arctic-instruct tmp/hf_checkpoints/${HF_MODEL}\n```\n\n----------------------------------------\n\nTITLE: Configuring NVRTC Library Dependencies Based on Platform\nDESCRIPTION: This snippet handles platform-specific (Windows vs. Linux) and configuration-specific (shared vs. static) discovery of NVRTC libraries needed for runtime compilation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_SHARED_NVRTC)\n  if(WIN32)\n    message(FATAL_ERROR \"Cannot use NVRTC shared library on Windows.\")\n  else()\n    find_library(\n      NVRTC_LIB nvrtc\n      HINTS ${CUDAToolkit_LIBRARY_DIR}\n      PATH_SUFFIXES lib64 lib lib/x64)\n    find_library(\n      NVRTC_BUILTINS_LIB nvrtc-builtins\n      HINTS ${CUDAToolkit_LIBRARY_DIR}\n      PATH_SUFFIXES lib64 lib lib/x64)\n  endif()\nelse()\n  if(WIN32)\n    find_library(\n      NVRTC_LIB nvrtc\n      HINTS ${CUDAToolkit_LIBRARY_DIR}\n      PATH_SUFFIXES lib64 lib lib/x64)\n  else()\n    find_library(\n      NVRTC_LIB nvrtc_static\n      HINTS ${CUDAToolkit_LIBRARY_DIR}\n      PATH_SUFFIXES lib64 lib lib/x64)\n    find_library(\n      NVRTC_BUILTINS_LIB nvrtc-builtins_static\n      HINTS ${CUDAToolkit_LIBRARY_DIR}\n      PATH_SUFFIXES lib64 lib lib/x64)\n    find_library(\n      NVPTXCOMPILER_LIB nvptxcompiler_static\n      HINTS ${CUDAToolkit_LIBRARY_DIR}\n      PATH_SUFFIXES lib64 lib lib/x64)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building the ExecutorWorker Component in TensorRT-LLM\nDESCRIPTION: This CMake snippet sets up the build configuration for the executorWorker executable. It specifies source files, includes project headers, creates the executable target, links required libraries including the shared target and TensorRT plugin, and sets C++17 as the required standard.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/executor_worker/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SRCS executorWorker.cpp)\n\ninclude_directories(${PROJECT_SOURCE_DIR}/include)\n\nset(EXECUTOR_WORKER_TARGET executorWorker)\n\nadd_executable(${EXECUTOR_WORKER_TARGET} ${SRCS})\n\ntarget_link_libraries(${EXECUTOR_WORKER_TARGET}\n                      PUBLIC ${SHARED_TARGET} nvinfer_plugin_tensorrt_llm)\n\ntarget_compile_features(${EXECUTOR_WORKER_TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: MLPerf Test Configuration for Llama v2 70B\nDESCRIPTION: Test configuration for running MLPerf benchmarks on Llama v2 70B chat model using different NVIDIA GPU configurations (H100x2 and H200x1).\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/qa/benchmark_test_list.txt#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntest_mlpf_results.py::test_mlperf_results[llama_v2_70b_chat-H100x2]\ntest_mlpf_results.py::test_mlperf_results[llama_v2_70b_chat-H200x1]\n```\n\n----------------------------------------\n\nTITLE: Converting and Splitting FairSeq NMT Weights\nDESCRIPTION: Python command to convert and split FairSeq NMT weights, specifying model type, directories, and precision.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nexport TP_SIZE=1\nexport PP_SIZE=1\nexport WORLD_SIZE=1\nexport INFERENCE_PRECISION=\"float32\"\npython convert_checkpoint.py --model_type nmt \\\n                --model_dir tmp/fairseq_models/wmt14 \\\n                --output_dir tmp/trt_models/wmt14/${INFERENCE_PRECISION} \\\n                --tp_size ${TP_SIZE} \\\n                --pp_size ${PP_SIZE} \\\n                --dtype ${INFERENCE_PRECISION}\n```\n\n----------------------------------------\n\nTITLE: Plugin Initialization File for Registration\nDESCRIPTION: Demonstrates how to create an __init__.py file that imports and exports plugin classes to ensure automatic registration when the package is imported.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/python_plugin/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# __init__.py\nfrom .lookup_plugin import LookUpPlugin\n\n__all__ = [\"LookUpPlugin\"]\n```\n\n----------------------------------------\n\nTITLE: Triton Server Setup Commands\nDESCRIPTION: Commands to clone and set up the TensorRT-LLM backend for Triton Inference Server deployment.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/quick-start-guide.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\ngit clone https://github.com/triton-inference-server/tensorrtllm_backend.git\ncd tensorrtllm_backend\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files for Context Attention in CMake\nDESCRIPTION: Collects all C++ and CUDA source files in the current directory and its subdirectories using GLOB_RECURSE command for building the context attention library.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace Weights to TensorRT-LLM Format\nDESCRIPTION: Command to convert HuggingFace model weights to TensorRT-LLM checkpoint format for single GPU deployment\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir glm_4_9b --output_dir trt_ckpt/glm_4_9b/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DBRX Model in TensorRT-LLM\nDESCRIPTION: Installs the required dependencies including tiktoken (used as the DBRX tokenizer) and sets up git-lfs for downloading large model files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/dbrx/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install dependencies\n# DBRX uses tiktoken as the tokenizer; make sure it is installed\npip install -r requirements.txt\n\n# Setup git-lfs\ngit lfs install\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Parser Library Name Based on Platform\nDESCRIPTION: Configures the ONNX parser library name differently for Windows and non-Windows platforms. On Windows, it appends the major version number to the library name.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n  set(ONNX_PARSER_LIB_NAME nvonnxparser_10)\nelse()\n  set(ONNX_PARSER_LIB_NAME nvonnxparser)\nendif()\nfind_library_create_target(nvonnxparser ${ONNX_PARSER_LIB_NAME} SHARED\n                           ${TRT_OUT_DIR} ${TRT_LIB_DIR})\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Files for TensorRT-LLM Executor\nDESCRIPTION: Defines the list of source files to be compiled for the TensorRT-LLM executor library. The list is comprehensive and includes all the components required for the executor's functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/executor/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# keep this list sorted alphabetically\nset(SRCS\n    cache_transmission/mpi_utils/connection.cpp\n    cache_transmission/cacheConcatenate.cu\n    contextPhaseParams.cpp\n    debugConfig.cpp\n    decodingConfig.cpp\n    executor.cpp\n    executorConfig.cpp\n    executorImpl.cpp\n    executorKVCacheEventManager.cpp\n    extendedRuntimePerfKnobConfig.cpp\n    guidedDecodingConfig.cpp\n    guidedDecodingParams.cpp\n    jsonSerialization.cpp\n    kvCacheConfig.cpp\n    kvCacheRetentionConfig.cpp\n    logitsPostProcessorConfig.cpp\n    loraConfig.cpp\n    orchestratorConfig.cpp\n    outputConfig.cpp\n    parallelConfig.cpp\n    peftCacheConfig.cpp\n    promptTuningConfig.cpp\n    mropeConfig.cpp\n    request.cpp\n    requestUtils.cpp\n    requestWithId.cpp\n    response.cpp\n    samplingConfig.cpp\n    dynamicBatchConfig.cpp\n    dynamicBatchTuner.cpp\n    schedulerConfig.cpp\n    serialization.cpp\n    speculativeDecodingConfig.cpp\n    tensor.cpp\n    types.cpp\n    requestUtils.cpp\n    contextPhaseParams.cpp\n    disaggServerUtil.cpp)\n```\n\n----------------------------------------\n\nTITLE: Markdown Links to TensorRT-LLM Documentation\nDESCRIPTION: Markdown links pointing to the official TensorRT-LLM documentation, examples section, and customization guidelines.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-api/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# LLM API Examples\n\nPlease refer to the [official documentation](https://nvidia.github.io/TensorRT-LLM/llm-api/), [examples](https://nvidia.github.io/TensorRT-LLM/examples/llm_api_examples.html) and [customization](https://nvidia.github.io/TensorRT-LLM/examples/customization.html) for detailed information and usage guidelines regarding the LLM API.\n```\n\n----------------------------------------\n\nTITLE: Disabling KV Cache Reuse in gptManagerBenchmark\nDESCRIPTION: Command-line option to disable KV cache reuse when running the gptManagerBenchmark application.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/kv-cache-reuse.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngptManagerBenchmark --enable_kv_cache_reuse enable=false\n```\n\n----------------------------------------\n\nTITLE: Running Inference with LoRA-adapted Qwen-7B Model\nDESCRIPTION: Executes inference using the LoRA-adapted Qwen-7B model. It specifies the engine directory, tokenizer, and input text for generation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\npython ../run.py --engine_dir ./tmp/qwen/7B_lora/trt_engines/fp16/1-gpu \\\n              --max_output_len 50 \\\n              --tokenizer_dir ./tmp/Qwen/7B/ \\\n              --input_text \"안녕하세요, 혹시 이름이 뭐에요?\" \\\n              --lora_task_uids 0 \\\n              --use_py_session\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Skywork Model Conversion\nDESCRIPTION: Installs required Python packages and git-lfs for downloading Hugging Face models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt && sudo apt-get install git-lfs\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Docker Container\nDESCRIPTION: Command to run the TensorRT-LLM Docker container after building the image.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/build-from-source-linux.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker release_run\n```\n\n----------------------------------------\n\nTITLE: Creating Benchmarks Target in CMake\nDESCRIPTION: Creates a custom target named 'benchmarks' that will be used as a dependency for all benchmark executables.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(benchmarks)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine\nDESCRIPTION: Command to build TensorRT-LLM engine with float16 precision and plugin optimizations\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir trt_ckpt/glm_4_9b/fp16/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/glm_4_9b/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Referencing Multimodal Modules in TensorRT-LLM for MLLaMA\nDESCRIPTION: This snippet provides a link to the multimodal modules used in the MLLaMA implementation. It directs users to the 'examples/multimodal' directory in the TensorRT-LLM GitHub repository.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mllama/README.md#2025-04-07_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n[examples/multimodal](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/multimodal)\n```\n\n----------------------------------------\n\nTITLE: CUDA Package Configuration\nDESCRIPTION: Locates CUDA installation and displays configuration information including version and paths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp_library/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(CUDA REQUIRED)\nmessage(STATUS \"CUDA library status:\")\nmessage(STATUS \"    config: ${CUDA_DIR}\")\nmessage(STATUS \"    version: ${CUDA_VERSION}\")\nmessage(STATUS \"    libraries: ${CUDA_LIBRARIES}\")\nmessage(STATUS \"    include path: ${CUDA_INCLUDE_DIRS}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files for TensorRT-LLM Runtime in CMake\nDESCRIPTION: This snippet defines a list of source files for the TensorRT-LLM runtime, including various utility classes, buffers, decoders, and runtime components. It covers both C++ and CUDA source files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/runtime/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SRCS\n    utils/mpiUtils.cpp\n    utils/numpyUtils.cpp\n    utils/sessionUtils.cpp\n    utils/debugUtils.cu\n    utils/speculativeChoicesUtils.cpp\n    bufferManager.cpp\n    cudaMemPool.cpp\n    decodingLayerWorkspace.cpp\n    eagleBuffers.cpp\n    explicitDraftTokensBuffers.cpp\n    lookaheadBuffers.cpp\n    layerProfiler.cpp\n    loraManager.cpp\n    loraUtils.cpp\n    loraModule.cpp\n    loraCache.cpp\n    decodingOutput.cpp\n    decoderState.cpp\n    generationConfig.cpp\n    gptDecoder.cpp\n    gptDecoderBatched.cpp\n    gptJsonConfig.cpp\n    gptSession.cpp\n    iBuffer.cpp\n    iTensor.cpp\n    ipcUtils.cpp\n    ipcSocket.cpp\n    ipcNvlsMemory.cpp\n    memoryCounters.cpp\n    ncclCommunicator.cpp\n    promptTuningParams.cpp\n    runtimeBuffers.cpp\n    runtimeKernels.cu\n    rnnStateBuffers.cpp\n    statefulGptDecoder.cpp\n    statefulGptDecoderBatched.cpp\n    tllmBuffers.cpp\n    tllmRuntime.cpp\n    tllmLogger.cpp\n    transformerBuffers.cpp\n    workerPool.cpp\n    worldConfig.cpp)\n```\n\n----------------------------------------\n\nTITLE: Querying Chat API using OpenAI Python Client\nDESCRIPTION: Python code snippet demonstrating how to use the OpenAI Python client to interact with the Chat API endpoint of the trtllm-serve server.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example code snippet content here\n# (Actual code not provided in the input)\n```\n\n----------------------------------------\n\nTITLE: Building Weight-Stripped Engine with trtllm-build\nDESCRIPTION: Command to build a weight-stripped engine using trtllm-build. The --strip_plan flag enables weight stripping functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/sample_weight_stripping/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --strip_plan --checkpoint_dir ${CHECKPOINT_DIR} --output_dir ${ENGINE_DIR} ...\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for RecurrentGemma-2b-it (INT8 SmoothQuant)\nDESCRIPTION: This command builds a TensorRT engine from the RecurrentGemma-2b-it INT8 SmoothQuant checkpoint using trtllm-build with auto GEMM plugin selection.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it INT8 SmoothQuant with INT8 kv cache\nENGINE_2B_IT_INT8_SQ_PATH=./recurrentgemma_model/recurrentgemma-2b-it/trt_engines/int8_sq/1-gpu/\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_2B_IT_INT8_SQ_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_2B_IT_INT8_SQ_PATH}\n```\n\n----------------------------------------\n\nTITLE: Using Python Client\nDESCRIPTION: Command to send requests using the provided Python client with configuration and prompts files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/disaggregated/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./clients/disagg_client.py -c disagg_config.yaml -p ./clients/prompts.json\n```\n\n----------------------------------------\n\nTITLE: Installing and Running pre-commit Hooks\nDESCRIPTION: Commands to install the pre-commit package and set up hooks for automatic code formatting and validation during Git commits.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CONTRIBUTING.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Defining Benchmark Addition Function in CMake\nDESCRIPTION: Creates a function to add benchmark executables with consistent configuration. The function adds an executable, links required libraries, sets compiler features and definitions, and adds the target as a dependency to the benchmarks target.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_benchmark test_name test_src)\n  add_executable(${test_name} ${test_src} utils/utils.cpp)\n\n  target_link_libraries(\n    ${test_name} PUBLIC ${SHARED_TARGET} nvinfer_plugin_tensorrt_llm\n                        cxxopts::cxxopts)\n\n  target_compile_features(${test_name} PRIVATE cxx_std_17)\n  target_compile_definitions(${test_name}\n                             PUBLIC TOP_LEVEL_DIR=\"${TOP_LEVEL_DIR}\")\n  add_dependencies(benchmarks ${test_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedding Parallelism Along Vocabulary Dimension for GPT2\nDESCRIPTION: Commands to set up embedding parallelism along the vocabulary dimension with 2-way tensor parallelism. This distributes the embedding table by splitting the vocabulary across GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n# 2-way tensor parallelism with embedding parallelism along vocab dimension\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --tp_size 2 \\\n        --use_parallel_embedding \\\n        --embedding_sharding_dim 0 \\\n        --output_dir gpt2/trt_ckpt/fp16/2-gpu\n\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/fp16/2-gpu \\\n        --output_dir gpt2/trt_engines/fp16/2-gpu\n```\n\n----------------------------------------\n\nTITLE: CUDA Flags Configuration\nDESCRIPTION: Sets CUDA compiler flags including extended lambda support, relaxed constexpr, and optional fast math and fatbin compression.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-extended-lambda\")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr\")\nif(FAST_MATH)\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --use_fast_math\")\nendif()\nif(COMPRESS_FATBIN)\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --fatbin-options -compress-all\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Google Test Units for TensorRT-LLM Components in CMake\nDESCRIPTION: This snippet adds multiple Google Test units for various components of the TensorRT-LLM project. It uses the add_gtest() function to create test targets for sampling config, KV cache config, decoding config, requests, responses, and more. Some tests require additional library linkage.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/executor/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_gtest(executorSamplingConfigTest samplingConfigTest.cpp)\nadd_gtest(kvCacheConfigTest kvCacheConfigTest.cpp)\nadd_gtest(decodingConfigTest decodingConfigTest.cpp)\nadd_gtest(requestTest requestTest.cpp)\nadd_gtest(responseTest responseTest.cpp)\n\nadd_gtest(executorTestSmall executorTestSmall.cpp)\ntarget_link_libraries(executorTestSmall PRIVATE modelSpecStatic testingUtils)\n\nadd_gtest(executorTestSmallArbitraryOutputTensors\n          executorTestSmallArbitraryOutputTensors.cpp)\ntarget_link_libraries(executorTestSmallArbitraryOutputTensors\n                      PRIVATE modelSpecStatic testingUtils)\n\nadd_gtest(executorConfigTest executorConfigTest.cpp)\nadd_gtest(executorTensorTest tensorTest.cpp)\nadd_gtest(serializeUtilsTest serializeUtilsTest.cpp)\nadd_gtest(requestWithIdTest requestWithIdTest.cpp)\nadd_gtest(loraConfigTest loraConfigTest.cpp)\nadd_gtest(intervalSetTest intervalSetTest.cpp)\nadd_gtest(dynamicBatchTunerTest dynamicBatchTunerTest.cpp)\nadd_gtest(ucxCommTest ucxCommTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT-LLM in Development Mode\nDESCRIPTION: Installing TensorRT-LLM package in editable mode with development dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[devel]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoDoc for TensorRT-LLM Models\nDESCRIPTION: RST configuration directives for automatically generating documentation from the TensorRT-LLM models module. Sets up module imports and documentation display options including members, undocumented members, and inheritance hierarchies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/python-api/tensorrt_llm.models.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: tensorrt_llm\n\n.. currentmodule:: tensorrt_llm\n\n.. automodule:: tensorrt_llm.models\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Running Summarization Task\nDESCRIPTION: Command to run text summarization evaluation using the built engine\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../summarize.py --test_trt_llm \\\n        --hf_model_dir glm_4_9b \\\n        --engine_dir trt_engines/glm_4_9b/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Summarization with Llama-3.1-8B-Medusa using TensorRT-LLM\nDESCRIPTION: This command runs summarization with the Llama-3.1-8B-Medusa model by ModelOpt on a single GPU. It configures the engine and tokenizer directories, data type, and Medusa choice patterns for the model to use during decoding with a temperature of 1.0.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/medusa/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ../summarize.py --engine_dir ./tmp/modelopt/llama-8B-medusa/trt_engines/1-gpu/ \\\n                       --hf_model_dir ./llama3.1-medusa-8b-hf_v0.1 \\\n                       --tokenizer_dir ./llama3.1-medusa-8b-hf_v0.1 \\\n                       --test_trt_llm \\\n                       --data_type fp16 \\\n                       --medusa_choices=\"[[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [1, 6], [0, 7, 0]]\" \\\n                       --use_py_session \\\n                       --temperature 1.0 \\\n                       --batch_size 1\n```\n\n----------------------------------------\n\nTITLE: Basic Disaggregated Configuration\nDESCRIPTION: Basic YAML configuration for disaggregated serving, specifying server locations and ports.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/disaggregated/README.md#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nhostname: localhost\nport: 8000\nbackend: pytorch\ncontext_servers:\n  urls:\n      - \"localhost:8001\"\n      - \"localhost:8002\"\ngeneration_servers:\n  urls:\n      - \"localhost:8003\"\n```\n\n----------------------------------------\n\nTITLE: Submitting Slurm Job for Multi-Node Execution\nDESCRIPTION: Command to submit the Slurm job script for multi-node execution of TensorRT-LLM models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nsbatch tensorrt_llm_run.sub\n```\n\n----------------------------------------\n\nTITLE: Adding Mixture of Experts Backend Benchmark in CMake\nDESCRIPTION: Invokes the add_benchmark function to create a benchmark for the Mixture of Experts backend using the provided launcher file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/micro_benchmarks/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_benchmark(mixtureOfExpertsBackendBenchmark\n              mixtureOfExpertsBackendBenchmarkLauncher.cu)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedding Parallelism Along Hidden Dimension for GPT2\nDESCRIPTION: Commands to set up embedding parallelism along the hidden dimension with 2-way tensor parallelism. This approach distributes the embedding lookup table across GPUs to reduce per-GPU memory consumption.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n# 2-way tensor parallelism with embedding parallelism along hidden dimension\npython3 convert_checkpoint.py --model_dir gpt2 \\\n        --dtype float16 \\\n        --tp_size 2 \\\n        --use_parallel_embedding \\\n        --embedding_sharding_dim 1 \\\n        --output_dir gpt2/trt_ckpt/fp16/2-gpu\n\ntrtllm-build --checkpoint_dir gpt2/trt_ckpt/fp16/2-gpu \\\n        --output_dir gpt2/trt_engines/fp16/2-gpu\n```\n\n----------------------------------------\n\nTITLE: Printing Debug Tensors at Runtime in TensorRT-LLM\nDESCRIPTION: This snippet demonstrates how to print the values of registered debug tensors during the runtime execution of a TensorRT-LLM model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(outputs.keys())\nprint(outputs['inter'])\n```\n\n----------------------------------------\n\nTITLE: Sanitizer Configuration\nDESCRIPTION: Configures build sanitizer options including UBSan, ASan, and TSan with compiler-specific settings and additional sub-sanitizers.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\nif(SANITIZE)\n  if(WIN32)\n    message(FATAL_ERROR \"Sanitizing support is unimplemented on Windows.\")\n  endif()\n\n  macro(add_clang_rt_lib lib_name)\n    if(CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n      execute_process(\n        COMMAND\n          ${CMAKE_CXX_COMPILER}\n          \"-print-file-name=libclang_rt.${lib_name}-${CMAKE_SYSTEM_PROCESSOR}.so\"\n        OUTPUT_VARIABLE CLANG_SAN_LIBRARY_PATH OUTPUT_STRIP_TRAILING_WHITESPACE)\n      link_libraries(${CLANG_SAN_LIBRARY_PATH})\n    endif()\n  endmacro()\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Test Function for TensorRT-LLM\nDESCRIPTION: Creates the add_gtest function that simplifies adding test targets to the project. This function handles linking with GoogleTest and TensorRT-LLM libraries, sets up compiler features, and configures test discovery.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_gtest test_name test_src)\n  set(options NO_GTEST_MAIN NO_TLLM_LINKAGE)\n  cmake_parse_arguments(ARGS \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\"\n                        ${ARGN})\n  add_executable(${test_name} ${test_src})\n\n  target_link_libraries(${test_name} PUBLIC gmock_main nvonnxparser)\n  if(NOT ARGS_NO_GTEST_MAIN)\n    target_link_libraries(${test_name} PUBLIC gtest_main)\n  endif()\n  if(NOT ARGS_NO_TLLM_LINKAGE)\n    target_link_libraries(${test_name} PUBLIC ${SHARED_TARGET}\n                                              nvinfer_plugin_tensorrt_llm)\n    if(WIN32)\n      target_link_libraries(${test_name} PRIVATE context_attention_src)\n    endif()\n  endif()\n  if(ENABLE_MULTI_DEVICE)\n    target_compile_definitions(${test_name} PUBLIC ENABLE_MULTI_DEVICE)\n  endif()\n\n  target_compile_features(${test_name} PRIVATE cxx_std_17)\n  target_compile_definitions(${test_name}\n                             PUBLIC TOP_LEVEL_DIR=\"${TOP_LEVEL_DIR}\")\n\n  gtest_discover_tests(\n    ${test_name}\n    PROPERTIES ENVIRONMENT \"CUDA_MODULE_LOADING=LAZY\" DISCOVERY_MODE\n               PRE_TEST # WAR for DLL discovery on windows.\n               DISCOVERY_TIMEOUT 30) # Longer timeout needed because discovery\n                                     # can be slow on Windows\n  add_dependencies(google-tests ${test_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Downloading GPT2 Model from HuggingFace\nDESCRIPTION: Downloads the GPT2-medium model from HuggingFace and prepares it for use with TensorRT-LLM by removing existing model files and downloading the pytorch_model.bin.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf gpt2 && git clone https://huggingface.co/gpt2-medium gpt2\npushd gpt2 && rm pytorch_model.bin model.safetensors && wget -q https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin && popd\n```\n\n----------------------------------------\n\nTITLE: Downloading RecurrentGemma Model Checkpoints\nDESCRIPTION: These commands clone different RecurrentGemma model variants from Hugging Face repositories. The models are public but require users to log in before they can clone them.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b\ngit clone https://huggingface.co/google/recurrentgemma-2b ./recurrentgemma_model/recurrentgemma-2b\n\n# recurrentgemma-2b-it\ngit clone https://huggingface.co/google/recurrentgemma-2b-it ./recurrentgemma_model/recurrentgemma-2b-it\n\n# recurrentgemma-2b-flax\ngit clone https://huggingface.co/google/recurrentgemma-2b-flax ./recurrentgemma_model/recurrentgemma-2b-flax\n\n# recurrentgemma-2b-it-flax\ngit clone https://huggingface.co/google/recurrentgemma-2b-it-flax ./recurrentgemma_model/recurrentgemma-2b-it-flax\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Dataset for DeepSeek-V3 Benchmarking\nDESCRIPTION: Python command to create a synthetic dataset for benchmarking the DeepSeek-V3 model, simulating token sequences with specified input and output characteristics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/cpp/prepare_dataset.py \\\n  --tokenizer=deepseek-ai/DeepSeek-V3 \\\n  --stdout token-norm-dist \\\n  --num-requests=8192 \\\n  --input-mean=1000 \\\n  --output-mean=1000 \\\n  --input-stdev=0 \\\n  --output-stdev=0 > /workspace/dataset.txt\n```\n\n----------------------------------------\n\nTITLE: Downloading BART Base and LoRA Models\nDESCRIPTION: Commands to clone the BART base model and LoRA model from Hugging Face repositories.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/facebook/bart-large-cnn tmp/hf_models/bart-large-cnn\ngit clone https://huggingface.co/sooolee/bart-large-cnn-samsum-lora tmp/hf_models/bart-large-cnn-samsum-lora\n```\n\n----------------------------------------\n\nTITLE: Enabling Low Latency GEMM Plugin in TensorRT-LLM\nDESCRIPTION: This code snippet demonstrates how to enable the low latency GEMM plugin for fp8 in TensorRT-LLM. It's added after the BuildConfig initialization and is recommended for low-latency scenarios in fp8.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/fp8-quantization.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbuild_config.plugin_config.low_latency_gemm_plugin = 'fp8'\n```\n\n----------------------------------------\n\nTITLE: Gathering and Setting Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet collects all C++ source files in the current directory and adds them to the PLUGIN_SOURCES variable. It then propagates this variable to the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/mixtureOfExperts/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Building Llama 3.1 70B Engine with TensorRT-LLM\nDESCRIPTION: Bash script for building a TensorRT-LLM engine for Llama 3.1 70B model using tensor parallelism across 4 GPUs on a single node.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nfolder_trt_llm=../TensorRT-LLM\nmodel_dir=Llama-3.1-70B\nckpt_dir=ckpt_llama_3.1_70b\nengine_dir=engine_llama_3.1_70b\ndtype=bfloat16\ntp_size=4\npp_size=1\nkv_cache_type=paged\nmax_input_len=128\nmax_output_len=128\nmax_batch_size=4\nworkers=$(( tp_size * pp_size ))\n\npython ${folder_trt_llm}/examples/llama/convert_checkpoint.py \\\n    --output_dir ${ckpt_dir} \\\n    --model_dir ${model_dir} \\\n    --dtype ${dtype} \\\n    --tp_size ${tp_size} \\\n    --pp_size ${pp_size} \\\n    --workers ${workers} \\\n    --use_parallel_embedding\n\ntrtllm-build \\\n    --output_dir ${engine_dir} \\\n    --checkpoint_dir ${ckpt_dir} \\\n    --gemm_plugin ${dtype} \\\n    --gpt_attention_plugin ${dtype} \\\n    --kv_cache_type ${kv_cache_type} \\\n    --max_input_len ${max_input_len} \\\n    --max_seq_len $(( max_input_len + max_output_len )) \\\n    --max_batch_size ${max_batch_size} \\\n    --workers ${workers}\n```\n\n----------------------------------------\n\nTITLE: Docker Run Command with Shared Memory and Memlock Settings\nDESCRIPTION: Docker command options recommended for avoiding NCCL errors when running multiple GPU inferences by increasing shared memory and memlock limits.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n--shm-size=1g --ulimit memlock=-1\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Compiler and Linker Flags\nDESCRIPTION: This code sets up compiler and linker flags, including architecture-specific options for x86_64 platforms and warnings suppression for both Unix and Windows builds.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_24\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_FLAGS\n    \"${CMAKE_CXX_FLAGS} -DBUILD_SYSTEM=cmake_oss -DENABLE_MULTI_DEVICE=${ENABLE_MULTI_DEVICE}\"\n)\n\n# Fix linking issue with TRT 10, the detailed description about `--mcmodel` can\n# be found in\n# https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html#index-mcmodel_003dmedium-1\nif(CMAKE_SYSTEM_PROCESSOR STREQUAL x86_64)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mcmodel=medium\")\n  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--no-relax\")\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,--no-relax\")\nendif()\n\n# Disable deprecated declarations warnings\nif(NOT WIN32)\n  set(CMAKE_CXX_FLAGS \"-Wno-deprecated-declarations ${CMAKE_CXX_FLAGS}\")\nelse()\n  # /wd4996 is the Windows equivalent to turn off warnings for deprecated\n  # declarations\n\n  # /wd4505\n  # https://learn.microsoft.com/en-us/cpp/overview/cpp-conformance-improvements-2019?view=msvc-170#warning-for-unused-internal-linkage-functions\n  # \"warning C4505: <>: unreferenced function with internal linkage has been\n  # removed\"\n\n  # /wd4100\n  # https://learn.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-4-c4100?view=msvc-170\n  # warning C4100: 'c': unreferenced formal parameter\n\n  set(CMAKE_CXX_FLAGS \"/wd4996 /wd4505 /wd4100 ${CMAKE_CXX_FLAGS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: CI Configuration for GPU-Specific Tests\nDESCRIPTION: YAML configuration example for defining GPU-specific test cases in the TensorRT-LLM CI pipeline.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/README.md#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 0.0.1\nl0_a10:\n- condition:\n    ranges:\n      system_gpu_count:\n        gte: 1\n        lte: 1\n    wildcards:\n      gpu:\n      - '*a10*'\n      linux_distribution_name: ubuntu*\n  tests:\n  # ------------- PyTorch tests ---------------\n  - disaggregated/test_disaggregated.py::test_disaggregated_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]\n  - disaggregated/test_disaggregated.py::test_disaggregated_cuda_graph[TinyLlama-1.1B-Chat-v1.0]\n  - disaggregated/test_disaggregated.py::test_disaggregated_mixed[TinyLlama-1.1B-Chat-v1.0]\n  - disaggregated/test_disaggregated.py::test_disaggregated_overlap[TinyLlama-1.1B-Chat-v1.0]\n  # ------------- CPP tests ---------------\n  - test_cpp.py::test_model[medusa-86]\n  - test_cpp.py::test_model[redrafter-86]\n  - test_cpp.py::test_model[mamba-86]\n  - test_cpp.py::test_model[recurrentgemma-86]\n  - test_cpp.py::test_model[eagle-86]\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Installation Verification\nDESCRIPTION: Python code reference for validating TensorRT-LLM installation. This is a reference to a quickstart example file that would test if the installation is working properly.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/linux.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{literalinclude} ../../../examples/llm-api/quickstart_example.py\n    :language: python\n    :linenos:\n```\n\n----------------------------------------\n\nTITLE: Querying Completions API using curl\nDESCRIPTION: Bash script showing how to use curl to send a request to the Completions API endpoint of the trtllm-serve server.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Example curl command here\n# (Actual command not provided in the input)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with Custom Settings\nDESCRIPTION: Builds a TensorRT-LLM engine with manually specified batch size and token settings rather than using dataset statistics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md#2025-04-07_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ntrtllm-bench --model $model_name build --tp_size $tp_size --pp_size $pp_size --quantization FP8 --max_seq_len $seq_len --max_batch_size $max_bs --max_num_tokens $max_token\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Memory Usage Logging Example\nDESCRIPTION: Sample log output showing memory allocation details when running TensorRT-LLM with info logging level. It displays engine size, execution context memory, runtime buffers, decoder memory, and KV cache allocation information.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/memory.md#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n[TensorRT-LLM][INFO] Loaded engine size: 6695 MiB\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1134.01 MiB for execution context memory.\n[TensorRT-LLM][INFO] [MS] Running engine with multi stream info\n[TensorRT-LLM][INFO] [MS] Number of aux streams is 1\n[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2\n[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 6678 (MiB)\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 43.29 MB GPU memory for runtime buffers.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 180.30 MB GPU memory for decoder.\n[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 79.10 GiB, available: 70.48 GiB\n[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 4060\n[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n[TensorRT-LLM][INFO] Max KV cache pages per sequence: 32\n[TensorRT-LLM][INFO] Number of tokens per block: 64.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 63.44 GiB for max tokens in paged KV cache (259840).\n```\n\n----------------------------------------\n\nTITLE: Enabling GEMM + SwiGLU Fusion for FP8 Models\nDESCRIPTION: This code shows how to enable the GEMM + SwiGLU fusion for FP8 precision on Hopper GPUs. This optimization combines two Matmul operations and one SwiGLU operation into a single kernel, improving performance at a slight cost to accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/fp8-quantization.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbuild_config.plugin_config.gemm_swiglu_plugin = 'fp8'\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all CPP files in the current directory and adds them to the PLUGIN_SOURCES variable, which is then propagated to the parent scope. This is used to gather plugin source files for compilation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/quantizeToFP4Plugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Building Multi-GPU Tensor Parallel Engine\nDESCRIPTION: Commands to build TensorRT engine with 2-way tensor parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../quantization/quantize.py \\\n        --nemo_ckpt_path nemotron-3-8b-base-4k/Nemotron-3-8B-Base-4k.nemo \\\n        --dtype bfloat16 \\\n        --batch_size 64 \\\n        --qformat full_prec \\\n        --tp_size 2 \\\n        --output_dir nemotron-3-8b/trt_ckpt/bf16/tp2\n\ntrtllm-build --checkpoint_dir nemotron-3-8b/trt_ckpt/bf16/tp2 \\\n        --gpt_attention_plugin bfloat16 \\\n        --gemm_plugin bfloat16 \\\n        --workers 2 \\\n        --output_dir nemotron-3-8b/trt_engines/bf16/tp2\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Configuration Class for New Models\nDESCRIPTION: Example of defining a custom configuration class for models not already registered in HuggingFace's transformers library, following the structure of HuggingFace's configuration classes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/adding_new_model.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers.configuration_utils import PretrainedConfig\n\nclass MyConfig(PretrainedConfig):\n    def __init__(self, ...):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Architecture Variables and Handling Architecture-Specific Kernels\nDESCRIPTION: This code saves the original CUDA architectures, displays them, and then excludes specific SM architectures that aren't in the list by defining preprocessor macros.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\n# CMAKE_CUDA_ARCHITECTURES_ORIG contains all architectures enabled, without\n# automatically added -real or -a suffix.\nset(CMAKE_CUDA_ARCHITECTURES_ORIG \"${CMAKE_CUDA_ARCHITECTURES}\")\nmessage(STATUS \"GPU architectures: ${CMAKE_CUDA_ARCHITECTURES_ORIG}\")\n\nset(ARCHITECTURES_WITH_KERNELS \"80\" \"86\" \"89\" \"90\" \"100\" \"120\")\nforeach(CUDA_ARCH IN LISTS ARCHITECTURES_WITH_KERNELS)\n  if(NOT \"${CUDA_ARCH}\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG)\n    add_definitions(\"-DEXCLUDE_SM_${CUDA_ARCH}\")\n    message(STATUS \"Excluding SM ${CUDA_ARCH}\")\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Smaug-72B-v0.1\nDESCRIPTION: This command builds a TensorRT engine based on the converted checkpoint. It enables various plugins, sets the maximum batch size, and removes input padding for optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/smaug/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_8gpu_tp8 \\\n    --output_dir ./Smaug_72B_tp8 \\\n    --gemm_plugin float16 \\\n    --gpt_attention_plugin float16 \\\n    --context_fmha=enable \\\n    --max_batch_size 64 \\\n    --remove_input_padding=enable\n```\n\n----------------------------------------\n\nTITLE: Running GPT-350M Benchmark with 2-GPU Inflight Batching\nDESCRIPTION: Example command for running benchmarks on GPT-350M model using 2 GPUs with inflight batching. Uses MPI for multi-GPU coordination.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 ./benchmarks/gptManagerBenchmark \\\n    --engine_dir ../../examples/gpt/trt_engine/gpt2-ib/fp16/2-gpu/ \\\n    --request_rate 10 \\\n    --dataset ../../benchmarks/cpp/preprocessed_dataset.json \\\n    --max_num_samples 500\n```\n\n----------------------------------------\n\nTITLE: Using Async Result Retrieval in TensorRT-LLM\nDESCRIPTION: Demonstrates how to asynchronously retrieve results using the aresult method. This is useful in async contexts to avoid blocking the event loop.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ngeneration = llm.generate_async(<prompt>)\noutput = await generation.aresult()\n```\n\n----------------------------------------\n\nTITLE: Listing Key Features in TensorRT-LLM using Markdown\nDESCRIPTION: This code snippet uses Markdown to create a list of key features supported in TensorRT-LLM, with each feature linked to its corresponding documentation page.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/key-features.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Key Features\n\nThis document lists key features supported in TensorRT-LLM.\n\n- [Quantization](../source/reference/precision.md)\n- [Inflight Batching](../source/advanced/gpt-attention.md#in-flight-batching)\n- [Chunked Context](../source/advanced/gpt-attention.md#chunked-context)\n- [LoRA](../source/advanced/lora.md)\n- [KV Cache Reuse](../source/advanced/kv-cache-reuse.md)\n- [Speculative Sampling](../source/advanced/speculative-decoding.md)\n```\n\n----------------------------------------\n\nTITLE: Example Output from Greedy Decoding\nDESCRIPTION: Shows sample output from the EAGLE decoding process using greedy decoding strategy, demonstrating story continuation from a prompt.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/README.md#2025-04-07_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nInput [Text 0]: \"<s> Once upon\"\nOutput [Text 0 Beam 0]: \"a time, there was a young girl who loved to read. She would spend hours in the library, devouring books of all genres. She had a special love for fairy tales, and would often dream of living in a magical world where she could meet princes and princesses, and have adventures with talking animals.\nOne day, while she was reading a book, she came across a passage that spoke to her heart. It said, \\\"You are the author of\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Collecting C++ Source Files for TensorRT-LLM Plugin in CMake\nDESCRIPTION: This CMake snippet gathers all C++ files in the current directory using a glob pattern and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope to make these sources available to the parent CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Example of Generated FP16 Kernel Dispatcher\nDESCRIPTION: An example of a C++ function generated by the Triton AoT compiler for dispatching the FP16 Fused Attention kernel. This shows how the kernel is called with the appropriate parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\nCUresult fmha_d64_fp16(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, CUdeviceptr Out, CUdeviceptr L, CUdeviceptr M, CUdeviceptr Q, CUdeviceptr K, CUdeviceptr V, float sm_scale, int32_t seq_len){\n  if ((Out % 16 == 0) && (L % 16 == 0) && (M % 16 == 0) && (Q % 16 == 0) && (K % 16 == 0) && (V % 16 == 0))\n    return fmha_d64_fp16_0eb6b090_0d1d2d3d4d5d67(stream, gX, gY, gZ, Out, L, M, Q, K, V, sm_scale, seq_len);\n}\n```\n\n----------------------------------------\n\nTITLE: ModelWeightsLoader Class Implementation\nDESCRIPTION: Default implementation of the ModelWeightsLoader class showing the mapping between TensorRT-LLM keywords and HuggingFace keywords.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ModelWeightsLoader:\n    def __init__(self, model_dir, customized_key_dict: dict = {}) -> None:\n        ...\n        self.tllm_to_externel_key_dict = {\n            \"transformer\": \"model\",\n            \"vocab_embedding\": \"embed_tokens\",\n            \"lm_head\": \"lm_head\",\n            \"ln_f\": \"norm\",\n            \"attention\": \"self_attn\",\n            \"qkv\": [\"q_proj\", \"k_proj\", \"v_proj\"],\n            \"dense\": \"o_proj\",\n            \"gate\": \"up_proj\",\n            \"proj\": \"down_proj\",\n            \"fc\": \"gate_proj\",\n            \"input_layernorm\": \"input_layernorm\",\n            \"post_layernorm\": \"post_attention_layernorm\",\n        }\n        self.tllm_to_externel_key_dict.update(customized_key_dict)\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal CUTLASS Kernels\nDESCRIPTION: Determines whether to build internal CUTLASS (CUDA Templates for Linear Algebra Subroutines) kernels from source or import them, based on the availability of the corresponding CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(EXISTS\n   \"${CMAKE_CURRENT_SOURCE_DIR}/tensorrt_llm/kernels/internal_cutlass_kernels/CMakeLists.txt\"\n)\n  set(BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT ON)\nelse()\n  set(BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT OFF)\nendif()\noption(BUILD_INTERNAL_CUTLASS_KERNELS\n       \"Build internal cutlass kernels from source\"\n       ${BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT})\n\nif(BUILD_INTERNAL_CUTLASS_KERNELS)\n  message(STATUS \"Building internal cutlass kernels\")\nelse()\n  message(STATUS \"Importing internal cutlass kernels\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for RecurrentGemma-2b-it (FP8)\nDESCRIPTION: This command builds a TensorRT engine from the RecurrentGemma-2b-it FP8 checkpoint using trtllm-build with auto GEMM plugin selection.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it FP8 with FP8 kv cache\nENGINE_2B_IT_FP8_PATH=./recurrentgemma_model/recurrentgemma-2b-it/trt_engines/fp8/1-gpu/\ntrtllm-build --checkpoint_dir ${UNIFIED_CKPT_2B_IT_FP8_PATH} \\\n             --gemm_plugin auto \\\n             --max_batch_size 8 \\\n             --max_input_len 3000 \\\n             --max_seq_len 3100 \\\n             --output_dir ${ENGINE_2B_IT_FP8_PATH}\n```\n\n----------------------------------------\n\nTITLE: Specific Exception Handling in Python\nDESCRIPTION: Shows the preferred approach for exception handling in Python by catching specific exceptions rather than using a broad except clause. This improves error handling and debugging.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    open(path, \"r\").read()\nexcept:\n    print(\"Failed to open file\")\n```\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    open(path, \"r\").read()\nexcept FileNotFoundError:\n    print(\"Failed to open file\")\n```\n\n----------------------------------------\n\nTITLE: Running GPT-J Summarization with TensorRT-LLM\nDESCRIPTION: Executes the summarization task using TensorRT-LLM GPT-J model with FP16 precision. Includes options for testing both HuggingFace and TensorRT-LLM implementations, with ROUGE-1 score validation and accuracy checking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../../../summarize.py --engine_dir ./trt_engines/gptj_fp16_tp1 \\\n                        --hf_model_dir ./gpt-j-6b \\\n                        --test_hf \\\n                        --batch_size 1 \\\n                        --test_trt_llm \\\n                        --tensorrt_llm_rouge1_threshold 14 \\\n                        --data_type fp16 \\\n                        --check_accuracy\n```\n\n----------------------------------------\n\nTITLE: Implementing QuantizeTensorPlugin in C++\nDESCRIPTION: Demonstrates the implementation of a TensorRT plugin for quantization, showing the enqueue function that invokes a CUDA kernel based on input data type.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nint QuantizeTensorPlugin::enqueue(...) {\n    if (inputDesc[0].type == DataType::kFLOAT) {\n        invokeQuantization<float>(...);\n    } else {\n        invokeQuantization<half>(...);\n    }\n    return 0;\n}\n\ntemplate <typename T>\nvoid invokeQuantization(...) {\n    // The standard <<< >>> construct to launch CUDA kernels\n    quantizedKernel<<<grid, block, 0, stream>>>(...);\n}\n```\n\n----------------------------------------\n\nTITLE: Fully Customized Weight Conversion for TensorRT-LLM in Python\nDESCRIPTION: This snippet illustrates a fully customized approach to weight conversion for TensorRT-LLM models. It allows for handling different quantization algorithms and custom tensor processing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntllm_weights = {}\nfor tllm_key, param in tqdm(trtllm_model.named_parameters()):\n    # Load from external checkpoints\n    # The load_tensor() function can also be called here\n    tensor = ...\n    # Convert tensor and set the values according to the config\n    if trtllm_model.config.quantization.quant_algo == xxx:\n        ...\n    else:\n        ...\n    param.value = tensor\n```\n\n----------------------------------------\n\nTITLE: Gathering and Setting Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all C++ source files in the current directory and adds them to the PLUGIN_SOURCES variable. It then propagates this variable to the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/smoothQuantGemmPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Converting Nemotron-NAS Checkpoint to TensorRT-LLM Format (BF16/FP16)\nDESCRIPTION: Converts the Hugging Face checkpoint to TensorRT-LLM format using BF16 or FP16 precision. Requires the --trust_remote_code flag.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron_nas/README.md#2025-04-07_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npython ./convert_checkpoint.py \\\n                  --model_dir $MODEL_DIR \\\n                  --output_dir $TRT_CHECKPOINT_DIR \\\n                  --dtype bfloat16 \\\n                  --tp_size $TP_SIZE \\\n                  --pp_size $PP_SIZE \\\n                  --trust_remote_code\n```\n\n----------------------------------------\n\nTITLE: Pattern-Matching Example for Matrix Multiplication and Activation Fusion\nDESCRIPTION: Illustrates how TensorRT's pattern-matching algorithm identifies sequences that can be fused for optimization. This example shows a matrix multiplication followed by a ReLU activation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nc = tensorrt_llm.functional.matmul(a, b)\nc = tensorrt_llm.functional.relu(c)\n```\n\n----------------------------------------\n\nTITLE: Converting and Running OPT with Vocab Size Sharding and Lookup Plugin in TensorRT-LLM\nDESCRIPTION: This snippet shows the process of converting an OPT-125M checkpoint, building an engine, and running inference using tensor parallelism with embedding sharding along the vocab_size dimension. It uses parallel embedding, a lookup plugin, and checks accuracy against a specified threshold.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\npython3 convert_checkpoint.py --model_dir ./opt-125m \\\n                --dtype float16 \\\n                --output_dir ./opt/125M/trt_ckpt/fp16/2-gpu/ \\\n                --tp_size 2 \\\n                --use_parallel_embedding \\\n                --embedding_sharding_dim 0\n\ntrtllm-build --checkpoint_dir ./opt/125M/trt_ckpt/fp16/2-gpu/ \\\n                --gemm_plugin float16 \\\n                --max_batch_size 8 \\\n                --max_input_len 924 \\\n                --max_seq_len 1024 \\\n                --output_dir ./opt/125M/trt_engines/fp16/2-gpu/ \\\n                --workers 2\n\nmpirun -n 2 --allow-run-as-root \\\n      python3 ../../../summarize.py --engine_dir ./opt/125M/trt_engines/fp16/2-gpu/ \\\n                        --batch_size 1 \\\n                        --test_trt_llm \\\n                        --hf_model_dir opt-125m \\\n                        --data_type fp16 \\\n                        --check_accuracy \\\n                        --tensorrt_llm_rouge1_threshold=14\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for LLaVA-NeXT in TensorRT-LLM\nDESCRIPTION: A requirements file listing Python dependencies needed to run LLaVA-NeXT with TensorRT-LLM. It includes the LLaVA-NeXT repository from GitHub, the transformers library (version 4.44.2 or higher), einops for tensor manipulations, and av for audio/video processing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/multimodal/requirements-llava_onevision.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngit+https://github.com/LLaVA-VL/LLaVA-NeXT.git\ntransformers>=4.44.2\neinops\nav\n```\n\n----------------------------------------\n\nTITLE: Launching Context and Generation Servers with trtllm-serve\nDESCRIPTION: Commands to launch multiple context and generation servers using trtllm-serve. Sets UCX KV cache environment variable and starts servers with specific ports and logging.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/disaggregated/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TRTLLM_USE_UCX_KVCACHE=1\n#Context servers\ntrtllm-serve TinyLlama/TinyLlama-1.1B-Chat-v1.0 --host localhsot --port 8001 --backend pytorch &> log_ctx_0 &\ntrtllm-serve TinyLlama/TinyLlama-1.1B-Chat-v1.0 --host localhsot --port 8002 --backend pytorch &> log_ctx_1 &\n#Generation servers\ntrtllm-serve TinyLlama/TinyLlama-1.1B-Chat-v1.0 --host localhsot --port 8003 --backend pytorch &> log_gen_0 &\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT LLM\nDESCRIPTION: Requirements file that defines Python package dependencies with version constraints. Includes TensorRT LLM development version, datasets library v2.14.5, evaluate package, and rouge_score for evaluation metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\n```\n\n----------------------------------------\n\nTITLE: MTBench Test Configuration for Llama and Phi Models\nDESCRIPTION: Test configuration for running MTBench evaluations on various models including Llama v3 8B, Phi-3 Mini 4K, and Llama 3.1 70B with different GPU configurations and optimization techniques like lookahead.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/qa/benchmark_test_list.txt#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nexamples/test_llama.py::test_llama3_single_gpu_mtbench[llama-v3-8b-instruct-hf-cor1]\nexamples/test_phi.py::test_phi3_mtbench[Phi-3-mini-4k-instruct]\nexamples/test_llama.py::test_llama3_4_gpus_mtbench[llama-3.1-70b-instruct]\nexamples/test_llama.py::test_llama3_lookahead_single_gpu_mtbench[llama-v3-8b-instruct-hf]\nexamples/test_llama.py::test_llama3_lookahead_4_gpus_mtbench[llama-3.1-70b-instruct]\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU executorExampleAdvanced with Orchestrator Mode\nDESCRIPTION: Command to run the advanced Executor example on multiple GPUs using the Orchestrator communication mode. This mode automatically spawns additional processes to distribute the model across GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./executorExampleAdvanced --engine_dir <path_to_engine_dir>  --input_tokens_csv_file ../inputTokens.csv --use_orchestrator_mode --worker_executable_path <path_to_executor_worker>\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all CPP source files in the current directory and adds them to the plugin sources variable, making them available to the parent scope for compilation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/gemmPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Model Parallelism via CLI for TensorRT-LLM Engine Building\nDESCRIPTION: This snippet shows how to specify tensor and pipeline parallelism when using the CLI workflow with convert_checkpoint.py. The command converts a Llama checkpoint with tensor parallelism size of 8 and pipeline parallelism size of 2.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/deciding-model-sharding-strategy.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython examples/llama/convert_checkpoint.py --model_dir ./tmp/llama/405B/ \\\n                            --output_dir ./tllm_checkpoint_16gpu_tp8_pp2 \\\n                            --dtype float16 \\\n                            --tp_size 8\n                            --pp_size 2\n```\n\n----------------------------------------\n\nTITLE: Quantization Methods Comparison Table in Markdown\nDESCRIPTION: Markdown table comparing different quantization methods with their performance characteristics, accuracy impact, and calibration time.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md#2025-04-07_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Quantization Methods     | Performance Improvement (batch size <= 4) | Performance Improvement (batch size >= 16) | Accuracy Impact | Calibration Time** |\n| :----------------------- | :---------------------------------------: | :----------------------------------------: | :-------------: | :----------------: |\n| FP8 (W8A8)               |                  Medium                   |                   Medium                   |    Very Low     |      Minutes       |\n| Int8 SQ (W8A8)           |                  Medium                   |                   Medium                   |     Medium      |      Minutes       |\n```\n\n----------------------------------------\n\nTITLE: Running Text Summarization with OPT Models\nDESCRIPTION: Commands to run text summarization on the CNN/Daily Mail dataset using OPT models, with support for comparing results between HuggingFace and TensorRT-LLM implementations and computing ROUGE scores.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# OPT-125M\npython3 ../../../summarize.py --engine_dir ./opt/125M/trt_engines/fp16/1-gpu/ \\\n                        --test_hf \\\n                        --batch_size 1 \\\n                        --test_trt_llm \\\n                        --hf_model_dir opt-125m \\\n                        --data_type fp16 \\\n                        --check_accuracy \\\n                        --tensorrt_llm_rouge1_threshold=14\n\n# OPT-350M\npython3 ../../../summarize.py --engine_dir ./opt/350M/trt_engines/fp16/1-gpu/ \\\n                        --test_hf \\\n                        --batch_size 1 \\\n                        --test_trt_llm \\\n                        --hf_model_dir opt-350m \\\n                        --data_type fp16 \\\n                        --check_accuracy \\\n                        --tensorrt_llm_rouge1_threshold=20\n\n# OPT-2.7B\npython3 ../../../summarize.py --engine_dir ./opt/2.7B/trt_engines/fp16/1-gpu/ \\\n                        --test_hf \\\n                        --batch_size 1 \\\n                        --test_trt_llm \\\n                        --hf_model_dir opt-2.7b \\\n                        --data_type fp16 \\\n                        --check_accuracy \\\n                        --tensorrt_llm_rouge1_threshold=20\n\n# OPT-66B\nmpirun -n 4 --allow-run-as-root \\\n    python3 ../../../summarize.py --engine_dir ./opt/66B/trt_engines/fp16/4-gpu/ \\\n                            --batch_size 1 \\\n                            --test_trt_llm \\\n                            --hf_model_dir opt-66b \\\n                            --data_type fp16 \\\n                            --check_accuracy \\\n                            --tensorrt_llm_rouge1_threshold=20\n```\n\n----------------------------------------\n\nTITLE: Configuring Max Attention Window Size in TensorRT-LLM\nDESCRIPTION: This snippet demonstrates how to set the max_attention_window parameter in the KV cache configuration for TensorRT-LLM. This parameter is used to control the maximum number of tokens attended to when generating a single token.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-runtime-flags.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    kv_cache_config = KvCacheConfig(max_attention_window=<number of tokens>)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT-LLM\nDESCRIPTION: Defines required Python packages and their version constraints needed for running TensorRT-LLM. Includes core dependencies like tensorrt_llm, datasets, rouge_score, sentencepiece, and evaluate packages.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/draft_target_model/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nrouge_score\nsentencepiece>=0.1.99\nevaluate\n```\n\n----------------------------------------\n\nTITLE: Calculating Number of Tokens for Packed Mode in Python\nDESCRIPTION: This code snippet demonstrates how to calculate the total number of tokens in packed mode for GPT attention. It accounts for sequences in both context and generation phases, considering input lengths and beam widths.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-attention.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnum_tokens = 0\n\n# Add the length of each sequence in context phase.\nfor seq in context_phase:\n    num_tokens += seq.length\n\n# Add the width of the beam for each sequence in generation phase.\nfor seq in generation_phase:\n    num_tokens += seq.beam_width\n```\n\n----------------------------------------\n\nTITLE: Running Inference with RecurrentGemma-2b-it FP8 TensorRT Engine\nDESCRIPTION: This command runs inference using the RecurrentGemma-2b-it FP8 TensorRT engine with Python session and specified attention window size.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it FP8 with FP8 kv cache\nTOKENIZER_DIR_2B_IT_PATH=./recurrentgemma_model/recurrentgemma-2b-it\npython3 ../run.py --max_output_len=100 \\\n                  --use_py_session \\\n                  --max_attention_window_size 2048 \\\n                  --tokenizer_dir ${TOKENIZER_DIR_2B_IT_PATH} \\\n                  --engine_dir ${ENGINE_2B_IT_FP8_PATH}\n```\n\n----------------------------------------\n\nTITLE: Running Performance Tests with PyTest in TensorRT LLM\nDESCRIPTION: Example command for running a specific performance test case in TensorRT LLM. It demonstrates how to install dependencies, create a test list file, and execute the test with specific parameters to capture performance metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# execute these in the tensorrt-llm source repo root dir.\n# install dependencies, do not need to do it every time if already installed.\npip install -r requirements-dev.txt\n\n# example 1: run a test case\n# For example, if QA reports a perf bug for `perf/test_perf.py::test_perf[llama_7b-cppmanager-exe-plugin_ifb-float16-input_output_len:128,128,+512,32]`, then you can repro it by running:\ncd LLM_ROOT/tests/integration/defs\necho \"perf/test_perf.py::test_perf[llama_7b-cppmanager-exe-plugin_ifb-float16-input_output_len:128,128,+512,32]\" > perf.txt\npytest --perf --test-list=perf.txt --output-dir=/workspace/test-log --perf-log-formats csv --perf-log-formats yaml\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for executorExampleDisaggregated\nDESCRIPTION: Command to display the help menu for the disaggregated Executor example, showing all available command-line options and parameters for using separate context and generation stages.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n./executorExampleDisaggregated -h\n```\n\n----------------------------------------\n\nTITLE: Collecting and Setting Plugin Sources in CMake\nDESCRIPTION: Collects all C++ source files in the current directory and adds them to the plugin sources variable. The updated plugin sources are then propagated to the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/quantizePerTokenPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Example CSV Input Format for Token Generation\nDESCRIPTION: Example CSV format showing how to structure input tokens for the advanced example. Each line represents a separate prompt with comma-separated token IDs that will be processed by the TensorRT-LLM engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n1, 2, 3, 4, 5, 6\n1, 2, 3, 4\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n```\n\n----------------------------------------\n\nTITLE: Running CI Stage Locally with pytest\nDESCRIPTION: Command for running a specific CI stage locally using pytest with a custom test list in the TensorRT-LLM project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd tests/integration/defs\npytest . --test-list=\"a10_list.txt\" --output-dir=/tmp/llm_integration_test\n```\n\n----------------------------------------\n\nTITLE: Collecting and Setting Plugin Sources in CMake\nDESCRIPTION: This snippet collects all .cpp files in the current directory, adds them to the PLUGIN_SOURCES variable, and propagates the updated variable to the parent scope. This is part of the build configuration for TensorRT-LLM plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/lruPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Installs required system packages including sphinx, doxygen, pip, and graphviz for building documentation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\napt-get install python3-sphinx doxygen python3-pip graphviz\n```\n\n----------------------------------------\n\nTITLE: JSON Dataset Format for TensorRT-LLM Benchmarking with Token IDs\nDESCRIPTION: Example of JSON entries for benchmark dataset using token IDs (input_ids) instead of text prompts, with task_id and output token count.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"task_id\":0,\"input_ids\":[863,22056,25603,11943,8932,13195,3132,25032,21747,22213],\"output_tokens\":128}\n{\"task_id\":1,\"input_ids\":[14480,13598,15585,6591,1252,8259,30990,26778,7063,30065,21764,11023,1418],\"output_tokens\":128}\n```\n\n----------------------------------------\n\nTITLE: Converting Baichuan V1 13B Checkpoint with SmoothQuant\nDESCRIPTION: Python command to convert a Baichuan V1 13B model checkpoint using SmoothQuant quantization technique with per-channel and per-token optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_version v1_13b \\\n                --model_dir baichuan-inc/Baichuan-13B-Chat \\\n                --dtype float16 \\\n                --smoothquant 0.8 \\\n                --per_channel \\\n                --per_token \\\n                --output_dir ./tmp/baichuan_v1_13b/sq0.8/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU direct RDMA for inter-node KV cache transfer in TensorRT-LLM (Method 2)\nDESCRIPTION: This snippet demonstrates the second method to enable GPU direct RDMA for inter-node KV cache transfer in TensorRT-LLM, including setting a buffer size for KV cache transfer.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nTRTLLM_KVCACHE_TRANSFER_BUFFER_SIZE=$Size\nUCX_MEMTYPE_CACHE=n\n```\n\n----------------------------------------\n\nTITLE: Running Performance Benchmark on GH200 Systems\nDESCRIPTION: Command to execute the gptManagerBenchmark tool for measuring throughput performance on GH200 systems. The benchmark uses IFB type, manages GPU memory allocation, and outputs results to a CSV file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md#2025-04-07_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n/app/tensorrt_llm/benchmarks/cpp/gptManagerBenchmark  --engine_dir $engine_dir --type IFB --dataset $dataset_file_json --eos_id -1 --scheduler_policy guaranteed_no_evict --kv_cache_free_gpu_mem_fraction 0.95 --output_csv result.csv --request_rate -1.0 --enable_chunked_context --warm_up 0\n```\n\n----------------------------------------\n\nTITLE: Configure Plugin Sources in CMake\nDESCRIPTION: Collects all .cpp files in the current directory and adds them to the PLUGIN_SOURCES variable, propagating the changes to the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Enabling Pipeline Parallel Reduce Scatter in TensorRT-LLM via Python API\nDESCRIPTION: Code snippet demonstrating how to enable the pipeline parallelism optimization with ReduceScatter + AllGather for large mixture of experts models using the LLM-API in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    build_config.plugin_config.pp_reduce_scatter = True\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Release Docker Image\nDESCRIPTION: Command to build and push the release stage Docker image from the top-level directory of TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker release_push\n```\n\n----------------------------------------\n\nTITLE: Collecting and Setting Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet collects all C++ files in the current directory using GLOB and adds them to the PLUGIN_SOURCES variable. It then sets the updated PLUGIN_SOURCES variable in the parent scope to make it available to the parent CMake file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/fusedLayernormPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT-LLM\nDESCRIPTION: Defines required Python packages and their version constraints for the TensorRT-LLM project. Includes core dependencies like tensorrt_llm, transformers, datasets, evaluate, rouge_score, and sentencepiece with specific version requirements.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llama/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ntransformers>=4.43.0\ndatasets==2.14.6\nevaluate\nrouge_score\nsentencepiece>=0.1.99\n```\n\n----------------------------------------\n\nTITLE: Clang Format Control\nDESCRIPTION: Shows how to disable automatic formatting for specific code blocks using clang-format directives.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// clang-format off\n// .. Unformatted code ..\n// clang-format on\n```\n\n----------------------------------------\n\nTITLE: Running Release Docker Container\nDESCRIPTION: Command to run the release Docker image in a new container after building.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker release_run\n```\n\n----------------------------------------\n\nTITLE: NVFP4 Post-Training Quantization for Mixtral\nDESCRIPTION: Commands to quantize Mixtral into NVFP4 format and build TensorRT engines with NVFP4 quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython ../quantization/quantize.py --model_dir ./Mixtral-8x7B-v0.1 \\\n                                   --dtype float16 \\\n                                   --qformat nvfp4 \\\n                                   --kv_cache_dtype fp8 \\\n                                   --output_dir ./tllm_checkpoint_mixtral_nvfp4_1gpu \\\n                                   --calib_size 512 \\\n                                   --tp_size 1\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_nvfp4_1gpu \\\n             --output_dir ./engine_outputs\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with Debug Output Enabled\nDESCRIPTION: This bash script demonstrates the process of building a TensorRT engine for a GPT model with debug output enabled, including downloading the model, converting the checkpoint, and using trtllm-build.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/gpt\n\n# Download hf gpt2 model\nrm -rf gpt2 && git clone https://huggingface.co/gpt2-medium gpt2\npushd gpt2 && rm pytorch_model.bin model.safetensors && wget -q https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin && popd\n\n# Convert to TensorRT-LLM checkpoint\npython3 convert_checkpoint.py \\\n    --model_dir gpt2 \\\n    --dtype float16 \\\n    --output_dir gpt2/trt_ckpt/fp16/1-gpu\n\n# Build TensorRT-LLM engines with --enable_debug_output\ntrtllm-build \\\n    --checkpoint_dir gpt2/trt_ckpt/fp16/1-gpu \\\n    --enable_debug_output \\\n    --output_dir gpt2/trt_engines/fp16/1-gpu\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Command to install required Python packages\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: Collects all CPP files in the current directory and adds them to the PLUGIN_SOURCES variable, then propagates this variable to the parent scope. This enables centralized management of plugin source files for the build system.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/qserveGemmPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Running Summarization with GPT-NeoX (Single GPU)\nDESCRIPTION: Executes the summarization task using the built TensorRT engine for GPT-NeoX on a single GPU, comparing results with the HuggingFace implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../../../summarize.py --engine_dir ./gptneox/20B/trt_engines/fp16/1-gpu/ \\\n                        --test_trt_llm \\\n                        --hf_model_dir gptneox_model \\\n                        --data_type fp16\n```\n\n----------------------------------------\n\nTITLE: Building Binary Compatible Environment\nDESCRIPTION: Command to build compilation environment for x86_64 architecture to ensure binary compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker centos7_push\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet configures plugin sources for the NVIDIA TensorRT-LLM project. It first uses file(GLOB) to find all .cpp files in the current directory, then adds these files to the PLUGIN_SOURCES variable. Finally, it sets the PLUGIN_SOURCES variable in the parent scope to include the newly added files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/quantizeTensorPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pre-commit Hooks\nDESCRIPTION: Installation and setup of pre-commit hooks for code linting.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM from Source with C++ Compilation\nDESCRIPTION: Commands to build TensorRT-LLM from source with C++ compilation, including wheel generation and installation options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/build-from-source-linux.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# To build the TensorRT-LLM code.\npython3 ./scripts/build_wheel.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install ./build/tensorrt_llm*.whl\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./scripts/build_wheel.py --clean\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Build TensorRT-LLM for Ampere.\npython3 ./scripts/build_wheel.py --cuda_architectures \"80-real;86-real\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./scripts/build_wheel.py --benchmarks\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./scripts/build_wheel.py --cuda_architectures \"80-real;86-real\" --cpp_only --clean\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM with MPI in Slurm Environment\nDESCRIPTION: Command example showing how to run TensorRT-LLM in a Slurm environment by using mpirun to create a dedicated MPI environment, avoiding interference with Slurm's MPI implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 1 python3 examples/gpt/build.py ...\n```\n\n----------------------------------------\n\nTITLE: Enabling NVLink for KV cache transfer in TensorRT-LLM with UCX >=1.18 (Method 2)\nDESCRIPTION: This snippet demonstrates the second method to enable NVLink for KV cache transfer in TensorRT-LLM when using UCX version 1.18 or higher, including setting a buffer size for KV cache transfer.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nTRTLLM_KVCACHE_TRANSFER_BUFFER_SIZE=$Size\nUCX_MEMTYPE_CACHE=n\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake code gathers all C++ files in the current directory using GLOB and adds them to the PLUGIN_SOURCES variable. It then propagates this variable to the parent scope to make it available outside the current directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/lowLatencyGemmSwigluPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Example of Generated Kernel Launch Function\nDESCRIPTION: An example of a C++ function generated by the Triton AoT compiler that loads and launches the FP16 Fused Attention kernel on the GPU. Shows how parameters are passed and the shared memory allocation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_8\n\nLANGUAGE: c++\nCODE:\n```\nCUresult fmha_d64_fp16_0eb6b090_0d1d2d3d4d5d67(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, CUdeviceptr Out, CUdeviceptr L, CUdeviceptr M, CUdeviceptr Q, CUdeviceptr K, CUdeviceptr V, float sm_scale, int32_t seq_len) {\n    if (fmha_d64_fp16_0eb6b090_0d1d2d3d4d5d67_func == NULL)\n       load_fmha_d64_fp16_0eb6b090_0d1d2d3d4d5d67();\n    void *args[8] = { &Out, &L, &M, &Q, &K, &V, &sm_scale, &seq_len };\n    // TODO: shared memory\n    if(gX * gY * gZ > 0)\n      return cuLaunchKernel(fmha_d64_fp16_0eb6b090_0d1d2d3d4d5d67_func, gX, gY, gZ, 4 * 32, 1, 1, 114690, stream, args, NULL);\n}\n```\n\n----------------------------------------\n\nTITLE: Previewing Documentation Locally\nDESCRIPTION: Starts a local HTTP server to preview the generated documentation on port 8081.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/README.md#2025-04-07_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd build/html\n\npython3 -m http.server 8081\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookahead Decoding Tests in CMake\nDESCRIPTION: Sets up tests for the lookahead decoding layer, combining random language model implementation with decoding layer tests.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/layers/CMakeLists.txt#2025-04-07_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(LOOKAHEAD_DECODING_TEST_SRC randomLlm.cpp lookaheadDecodingLayerTest.cpp)\nadd_gtest(lookaheadDecodingLayerTest \"${LOOKAHEAD_DECODING_TEST_SRC}\")\n```\n\n----------------------------------------\n\nTITLE: Importing HuggingFace Configuration for Model Integration\nDESCRIPTION: Simple code example showing how to import an existing HuggingFace configuration class when adapting a model that's already supported in the transformers library.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/adding_new_model.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import LlamaConfig\n```\n\n----------------------------------------\n\nTITLE: Enabling Low Latency GEMM + SwiGLU Fusion for Small Batch Sizes\nDESCRIPTION: This code snippet demonstrates how to enable the low latency version of GEMM + SwiGLU fusion for small batch size scenarios where latency is critical. This alternative implementation is optimized for latency-sensitive applications.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/fp8-quantization.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbuild_config.plugin_config.low_latency_gemm_swiglu_plugin = 'fp8'\n```\n\n----------------------------------------\n\nTITLE: Customized Key Names Example\nDESCRIPTION: Example demonstrating how to use ModelWeightsLoader with custom key mappings for LLaVA model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Using the model weights loader for the LLM part of LLaVA\nfrom tensorrt_llm.models.model_weights_loader import ModelWeightsLoader\nllava_dict = {\n    \"transformer\": \"language_model.model\",\n    \"lm_head\": \"language_model.lm_head\"\n}\nloader = ModelWeightsLoader(external_checkpoint_dir, llava_dict)\nloader.generate_tllm_weights(trtllm_model)\n```\n\n----------------------------------------\n\nTITLE: Enabling Host Memory Offloading in gptManagerBenchmark\nDESCRIPTION: Command-line option to enable offloading of KV cache blocks to host memory in gptManagerBenchmark.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/kv-cache-reuse.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngptManagerBenchmark --kv_host_cache_bytes 45000000000\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all C++ source files in the current directory and adds them to the plugin sources list. Uses CMake's file globbing and variable scope manipulation to manage plugin source files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/topkLastDimPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Building InternLM2 7B with INT8 Weight-Only (Alternative)\nDESCRIPTION: Builds the TensorRT engine for the INT8 weight-only quantized InternLM2 7B model from the alternative output directory structure.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/internlm2/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./internlm2-chat-7b/w8a16 \\\n             --output_dir ./engine_outputs \\\n             --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: MMLU Evaluation Results Output\nDESCRIPTION: Detailed accuracy results from MMLU evaluation showing performance across various subjects including STEM, humanities, social sciences, and specialized fields\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/deepseek_v2/README.md#2025-04-07_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nAverage accuracy 0.480 - abstract_algebra\nAverage accuracy 0.741 - anatomy\nAverage accuracy 0.888 - astronomy\n[...content truncated for brevity...]\nAverage accuracy 0.873 - social sciences\nAverage accuracy 0.821 - other (business, health, misc.)\nAverage accuracy: 0.785\n```\n\n----------------------------------------\n\nTITLE: Configuring Include Directories for TensorRT-LLM Benchmarks\nDESCRIPTION: Sets up the include directories for the benchmark projects, pointing to CUTLASS extensions and the main include directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/micro_benchmarks/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(\n  ${PROJECT_SOURCE_DIR}/tensorrt_llm/cutlass_extensions/include\n  ${PROJECT_SOURCE_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Backend for Metrics Logging\nDESCRIPTION: YAML configuration to enable iteration statistics logging for the PyTorch backend in trtllm-serve.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# extra-llm-api-config.yml\npytorch_backend_config:\n enable_iter_perf_stats: true\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure with Sphinx toctree in reStructuredText\nDESCRIPTION: This code snippet demonstrates how to use Sphinx's toctree directive to organize documentation into sections. It includes multiple toctree blocks for different documentation categories, specifying maxdepth, caption, and included files for each section.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/index.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :caption: Getting Started\n   :name: Getting Started\n\n   overview.md\n   quick-start-guide.md\n   key-features.md\n   torch.md\n   release-notes.md\n\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Installation\n   :name: Installation\n\n   .. installation/overview.md\n\n   installation/linux.md\n   installation/build-from-source-linux.md\n   installation/grace-hopper.md\n\n\n.. toctree::\n   :maxdepth: 2\n   :caption: LLM API\n   :hidden:\n   :glob:\n\n   llm-api/*\n\n\n.. toctree::\n   :maxdepth: 2\n   :caption: Examples\n   :hidden:\n\n   examples/index.rst\n   examples/customization.md\n   examples/llm_api_examples\n   examples/trtllm_serve_examples\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Graph Rewriting Pattern in Python\nDESCRIPTION: Example implementation of a pattern rewriter that replaces addition operations with subtraction operations in a TensorRT neural network. The example demonstrates the four standard stages of graph rewriting: getting inputs/outputs, creating a new subgraph, redirecting dependencies, and marking old layers as removed.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/graph-rewriting.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass NaivePatternRewriter_ReplaceAddWithSub(PatternRewriter):\n\n    def __init__(self):\n        super().__init__('replace_add_with_sub',\n                         root_layer={trt.LayerType.ELEMENTWISE},\n                         separate_match_rewrite=True)\n\n    def match(self, layer: Layer):\n        # The rewriter will stop at the first matched layer, and then the Rewriter will enter the rewrite() to do the rewriting.\n        return layer.as_layer().op == trt.ElementWiseOperation.SUM\n\n    def rewrite(self, layer: Layer) -> None:\n        # The layer here should be an Elementwise_SUM layer.\n        with net_guard(layer.network):\n            # There are several stages to replace some subgraph with another subgraph:\n\n            # Stage 1: Get the input tensors and output tensors of the subgraph to replace.\n            # - For Elementwise_SUM, there are two inputs and one output.\n            a, b = layer.get_inputs(0, 1)\n            o = layer.get_outputs(0)[0]\n\n            # Stage 2: Create a new subgraph that takes the old one's inputs.\n            # - Here we insert an Elementwise_SUB layer, and 'c' is the output.\n            c = a - b\n\n            # Stage 3: Redirect all the layers depending on the outputs of the old subgraph to the new subgraph's.\n            # - After this, the SUM becomes dangling and will be pruned by TensorRT when building the engine.\n            # - Note that there is no API in TensorRT python to remove a layer explicitly; `replace_all_uses_with` is the only way to \"remove\" a layer.\n            o.replace_all_uses_with(c)\n\n            # Stage 4: Mark all the layers in the old subgraph as removed.\n            # - This helps the PatternRewriter to skip the removed layers.\n            layer.mark_as_removed()\n```\n\n----------------------------------------\n\nTITLE: Running Basic Python Bindings Example with TensorRT-LLM\nDESCRIPTION: Command for running the basic Python bindings example that demonstrates token generation with a TensorRT engine. The command specifies the path to the pre-built TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bindings/executor/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/bindings\npython3 example_basic.py --model_path=../llama/tmp/7B/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Collecting and Propagating Plugin Source Files in CMake\nDESCRIPTION: This code collects all C++ source files in the current directory using GLOB and adds them to the PLUGIN_SOURCES variable. It then propagates this variable to the parent scope so that these files will be included in the build process by the parent CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/rmsnormQuantizationPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Downloading Granite Model Checkpoints\nDESCRIPTION: Commands to clone the Granite 3.0 model checkpoints from HuggingFace repository. Supports either 8B or 3B model variants.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/granite/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nHF_MODEL=\"granite-3.0-8b-instruct\" # or granite-3.0-3b-a800m-instruct\n# clone the model we want to build\ngit clone https://huggingface.co/ibm-granite/${HF_MODEL} tmp/hf_checkpoints/${HF_MODEL}\n```\n\n----------------------------------------\n\nTITLE: Example YAML Test Definition Structure\nDESCRIPTION: Sample YAML configuration that defines test conditions and test cases. This example shows how to specify GPU requirements, system specifications, and the corresponding test cases to be executed when these conditions are met.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/test-db/README.md#2025-04-07_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 0.0.1\nl0_e2e:\n  - condition:\n      terms:\n        supports_fp8: true\n      ranges:\n        system_gpu_count:\n          gte: 4\n          lte: 4\n      wildcards:\n        gpu:\n          - '*h100*'\n        linux_distribution_name: ubuntu*\n    tests:\n      - examples/test_llama.py::test_llm_llama_v3_1_1node_multi_gpus[llama-3.1-8b-enable_fp8]\n      - examples/test_llama.py::test_llm_llama_v3_1_1node_multi_gpus[llama-3.1-70b-enable_fp8]\n```\n\n----------------------------------------\n\nTITLE: Building Micro Benchmarks with build_wheel.py\nDESCRIPTION: Instructions for building micro benchmarks using build_wheel.py script with the --micro_benchmark flag or using cmake with -DBUILD_MICRO_BENCHMARKS=ON option.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/micro_benchmarks/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--micro_benchmark\n```\n\n----------------------------------------\n\nTITLE: Setting Timeout for Future-Style Generation in TensorRT-LLM\nDESCRIPTION: Shows how to set a timeout when waiting for generation results. This helps prevent indefinite blocking if the generation process takes too long.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/customization.md#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\noutput = generation.result(timeout=10)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for GPT-J\nDESCRIPTION: Command to install the necessary dependencies before downloading the model weights.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM with Flashinfer Attention Backend in Python\nDESCRIPTION: Demonstrates how to create a PyTorchConfig object with the Flashinfer attention backend and pass it to the LLM constructor.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/attention.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nLLM(pytorch_backend_config=pytorch_config)\n```\n\n----------------------------------------\n\nTITLE: Implementing Activation Function in TensorRT-LLM\nDESCRIPTION: Shows how the activation function is implemented in TensorRT-LLM's functional module. It creates a TensorRT activation layer and returns the result as a Tensor object.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# In tensorrt_llm.functional:\n\ndef activation(input: Tensor, act_type: trt.ActivationType) -> Tensor:\n    layer = default_trtnet().add_activation(input.trt_tensor, act_type)   # default_trtnet() -> INetworkDefinition\n    return _create_tensor(layer.get_output(0), layer)\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT-LLM Benchmark\nDESCRIPTION: Executes throughput benchmarking using the built engine and generated dataset. Measures token throughput, request throughput and latency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md#2025-04-07_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ntrtllm-bench --model $model_name throughput --dataset $dataset_file --engine_dir $engine_dir\n```\n\n----------------------------------------\n\nTITLE: Downloading T5 and BART model weights from HuggingFace\nDESCRIPTION: Commands to clone various Encoder-Decoder model weights from HuggingFace repositories, including T5, Flan-T5, BART, mBART, and ByT5 models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/enc_dec/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/t5-small tmp/hf_models/t5-small\ngit clone https://huggingface.co/google/flan-t5-small tmp/hf_models/flan-t5-small\ngit clone https://huggingface.co/facebook/bart-large-cnn tmp/hf_models/bart-large-cnn\ngit clone https://huggingface.co/facebook/mbart-large-50-many-to-one-mmt tmp/hf_models/mbart-large-50-many-to-one-mmt\ngit clone https://huggingface.co/google/byt5-small tmp/hf_models/byt5-small\n```\n\n----------------------------------------\n\nTITLE: Defining TensorRT-LLM Test Cases in Python\nDESCRIPTION: This snippet lists various test cases for TensorRT-LLM models and features. It includes tests for different model architectures, sizes, and configurations, covering aspects like quantization, multimodal capabilities, and specific acceleration techniques.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/qa/examples_test_list.txt#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Pivot to Pytorch test cases.\ntest_e2e.py::test_ptp_quickstart\ntest_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-BF16-llama-3.1-model/Meta-Llama-3.1-8B]\ntest_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-FP8-llama-3.1-model/Llama-3.1-8B-Instruct-FP8]\ntest_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-NVFP4-nvfp4-quantized/Meta-Llama-3.1-8B]\ntest_e2e.py::test_ptp_quickstart_advanced[Nemotron4_4B-BF16-nemotron/Minitron-4B-Base]\ntest_e2e.py::test_ptp_quickstart_advanced_8gpus[Llama3.1-70B-BF16-llama-3.1-model/Meta-Llama-3.1-70B]\ntest_e2e.py::test_ptp_quickstart_advanced_8gpus[Llama3.1-70B-FP8-llama-3.1-model/Llama-3.1-70B-Instruct-FP8]\ntest_e2e.py::test_ptp_quickstart_advanced_8gpus[Llama3.1-405B-FP8-llama-3.1-model/Llama-3.1-405B-Instruct-FP8]\ntest_e2e.py::test_ptp_quickstart_advanced_8gpus[Mixtral-8x7B-BF16-Mixtral-8x7B-v0.1]\ntest_e2e.py::test_ptp_quickstart_advanced_8gpus[Mixtral-8x7B-NVFP4-nvfp4-quantized/Mixtral-8x7B-Instruct-v0.1]\ntest_e2e.py::test_ptp_quickstart_advanced_deepseek_r1_8gpus[DeepSeek-R1-DeepSeek-R1/DeepSeek-R1]\ntest_e2e.py::test_ptp_quickstart_multimodal[NVILA-8B-FP16-vila/NVILA-8B-image]\ntest_e2e.py::test_ptp_quickstart_multimodal[NVILA-8B-FP16-vila/NVILA-8B-video]\ntest_e2e.py::test_ptp_quickstart_multimodal[llava-v1.6-mistral-7b-llava-v1.6-mistral-7b-hf-image]\ntest_e2e.py::test_ptp_quickstart_multimodal[qwen2-vl-7b-instruct-Qwen2-VL-7B-Instruct-image]\ntest_e2e.py::test_ptp_quickstart_multimodal[qwen2-vl-7b-instruct-Qwen2-VL-7B-Instruct-video]\ntest_e2e.py::test_ptp_quickstart_bert[VANILLA-BertForSequenceClassification-bert/bert-base-uncased-yelp-polarity]\ntest_e2e.py::test_ptp_quickstart_bert[TRTLLM-BertForSequenceClassification-bert/bert-base-uncased-yelp-polarity]\ntest_e2e.py::test_ptp_star_attention_example[Llama3.1-8B-BF16-llama-3.1-model/Meta-Llama-3.1-8B]\ntest_e2e.py::test_trtllm_bench_pytorch_backend_sanity[meta-llama/Llama-3.1-8B-llama-3.1-8b-hf-nvfp4-False-False]\ntest_e2e.py::test_trtllm_bench_mgmn\ntest_e2e.py::test_ptp_scaffolding[DeepSeek-R1-Distill-Qwen-7B-DeepSeek-R1/DeepSeek-R1-Distill-Qwen-7B]\nexamples/test_medusa.py::test_codellama_medusa_1gpu[CodeLlama-7b-Instruct]\nexamples/test_medusa.py::test_llama_medusa_1gpu[llama-v2-7b-hf]\nexamples/test_medusa.py::test_llama_medusa_1gpu[llama-3.2-1b]\nexamples/test_medusa.py::test_llama_medusa_1gpu[llama-3.1-8b]\nexamples/test_medusa.py::test_mistral_medusa_1gpu[mistral-7b-v0.1]\nexamples/test_medusa.py::test_qwen_medusa_1gpu[qwen_7b_chat]\nexamples/test_medusa.py::test_qwen_medusa_1gpu[qwen1.5_7b_chat]\nexamples/test_medusa.py::test_qwen_medusa_1gpu[qwen2_7b_instruct]\nexamples/test_medusa.py::test_qwen_medusa_1gpu[qwen2_0.5b_instruct]\nexamples/test_medusa.py::test_qwen_medusa_1gpu[qwen2.5_1.5b_instruct]\nexamples/test_medusa.py::test_phi_medusa_1gpu[phi-2]\nexamples/test_medusa.py::test_phi_medusa_1gpu[Phi-3-mini-128k-instruct]\nexamples/test_medusa.py::test_phi_medusa_1gpu[Phi-3-small-128k-instruct]\nexamples/test_medusa.py::test_phi_medusa_1gpu[Phi-3.5-mini-instruct]\nexamples/test_medusa.py::test_phi_medusa_1gpu[Phi-4-mini-instruct]\nexamples/test_eagle.py::test_codellama_eagle_1gpu[CodeLlama-7b-Instruct-eagle1]\nexamples/test_eagle.py::test_llama_eagle_1gpu[llama-v2-7b-hf-eagle1]\nexamples/test_eagle.py::test_llama_eagle_1gpu[llama-3.2-1b-eagle1]\nexamples/test_eagle.py::test_llama_eagle_1gpu[llama-3.1-8b-eagle1]\nexamples/test_eagle.py::test_mistral_eagle_1gpu[mistral-7b-v0.1-eagle1]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen_7b_chat-eagle1]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen1.5_7b_chat-eagle1]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen2_7b_instruct-eagle1]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen2_0.5b_instruct-eagle1]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen2.5_1.5b_instruct-eagle1]\nexamples/test_eagle.py::test_phi_eagle_1gpu[phi-2-eagle1]\nexamples/test_eagle.py::test_phi_eagle_1gpu[Phi-3-mini-128k-instruct-eagle1]\nexamples/test_eagle.py::test_phi_eagle_1gpu[Phi-3-small-128k-instruct-eagle1]\nexamples/test_eagle.py::test_phi_eagle_1gpu[Phi-3.5-mini-instruct-eagle1]\nexamples/test_eagle.py::test_codellama_eagle_1gpu[CodeLlama-7b-Instruct-eagle2]\nexamples/test_eagle.py::test_llama_eagle_1gpu[llama-v2-7b-hf-eagle2]\nexamples/test_eagle.py::test_llama_eagle_1gpu[llama-3.2-1b-eagle2]\nexamples/test_eagle.py::test_llama_eagle_1gpu[llama-3.1-8b-eagle2]\nexamples/test_eagle.py::test_mistral_eagle_1gpu[mistral-7b-v0.1-eagle2]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen_7b_chat-eagle2]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen1.5_7b_chat-eagle2]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen2_7b_instruct-eagle2]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen2_0.5b_instruct-eagle2]\nexamples/test_eagle.py::test_qwen_eagle_1gpu[qwen2.5_1.5b_instruct-eagle2]\nexamples/test_eagle.py::test_phi_eagle_1gpu[phi-2-eagle2]\nexamples/test_eagle.py::test_phi_eagle_1gpu[Phi-3-mini-128k-instruct-eagle2]\nexamples/test_eagle.py::test_phi_eagle_1gpu[Phi-3-small-128k-instruct-eagle2]\nexamples/test_eagle.py::test_phi_eagle_1gpu[Phi-3.5-mini-instruct-eagle2]\n\n# PyTorch flow disaggregated tests\ndisaggregated/test_disaggregated.py::test_disaggregated_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]\ndisaggregated/test_disaggregated.py::test_disaggregated_multi_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]\ndisaggregated/test_disaggregated.py::test_disaggregated_cuda_graph[TinyLlama-1.1B-Chat-v1.0]\ndisaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8[DeepSeek-V3-Lite-fp8]\ndisaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8_ucx[DeepSeek-V3-Lite-fp8]\ndisaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8_attention_dp[DeepSeek-V3-Lite-fp8]\ndisaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8_attention_dp_one[DeepSeek-V3-Lite-fp8]\ndisaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8_attention_dp_one_mtp[DeepSeek-V3-Lite-fp8]\n\n# These tests will impact triton. They should be at the end of all tests (https://nvbugs/4904271)\n# examples/test_openai.py::test_llm_openai_triton_1gpu\n# examples/test_openai.py::test_llm_openai_triton_plugingen_1gpu\n```\n\n----------------------------------------\n\nTITLE: Quantizing Nemotron-NAS Model to FP8\nDESCRIPTION: Optional step to quantize the model to FP8 precision using a calibration dataset for optimal accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron_nas/README.md#2025-04-07_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nexport DATASET_DIR=\"~/datasets/nemotron-nas\"\npython ./calibration_utils.py $DATASET_DIR\n\npython ../quantization/quantize.py \\\n                  --model_dir $MODEL_DIR \\\n                  --output_dir $TRT_CHECKPOINT_DIR \\\n                  --dtype bfloat16 \\\n                  --kv_cache_dtype fp8 \\\n                  --qformat fp8 \\\n                  --calib_dataset $DATASET_DIR\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Start Command with SSH Key\nDESCRIPTION: Container start command that adds a public SSH key to authorized_keys and keeps the pod running. Used in team account scenarios where automatic key deployment may not work.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/dev-on-cloud/dev-on-runpod.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash -c 'echo \"<your_public_key>\" >> ~/.ssh/authorized_keys;sleep infinity'\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM Project\nDESCRIPTION: A requirements.txt file that lists the Python packages required for the TensorRT-LLM project. It includes a constraint file reference, the core TensorRT-LLM package, Hugging Face's transformers library, datasets, evaluation libraries, text processing tools, and utilities for progress tracking.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ntransformers>=4.31.0\ndatasets~=2.14.5\nevaluate\nrouge_score\nsentencepiece>=0.1.99\ntqdm\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Kernels Library in CMake\nDESCRIPTION: This snippet creates a static library target for the kernels, sets its properties for position-independent code and CUDA device symbol resolution, and adds subdirectories for various specialized kernel implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(kernels_src STATIC ${SRC_CPP} ${SRC_CU})\nset_property(TARGET kernels_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET kernels_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n\nadd_subdirectory(cutlass_kernels)\nadd_subdirectory(flashMLA)\nadd_subdirectory(contextFusedMultiHeadAttention)\nadd_subdirectory(decoderMaskedMultiheadAttention)\nadd_subdirectory(selectiveScan)\nadd_subdirectory(userbuffers)\nadd_subdirectory(trtllmGenKernels)\nadd_subdirectory(fusedLayernormKernels)\n```\n\n----------------------------------------\n\nTITLE: Creating Standard Activation Functions with Partial Application\nDESCRIPTION: Demonstrates how standard activation functions like ReLU and sigmoid are created using partial application of the base activation function in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/core-concepts.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# In tensorrt_llm.functional:\n\nrelu    = partial(activation, act_type=trt.ActivationType.RELU)\nsigmoid = partial(activation, act_type=trt.ActivationType.SIGMOID)\n\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Docker Development Image without GNU Make\nDESCRIPTION: Commands to create and run a Docker development image for TensorRT-LLM without using GNU make.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/build-from-source-linux.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --pull  \\\n            --target devel \\\n            --file docker/Dockerfile.multi \\\n            --tag tensorrt_llm/devel:latest \\\n            .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it \\\n        --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all \\\n        --volume ${PWD}:/code/tensorrt_llm \\\n        --workdir /code/tensorrt_llm \\\n        tensorrt_llm/devel:latest\n```\n\n----------------------------------------\n\nTITLE: Quantizing Checkpoint for Non-Medusa Low Latency Engine (Python)\nDESCRIPTION: This snippet demonstrates how to quantize a checkpoint for a non-Medusa low latency engine using Python. It specifies the model directory, data type, quantization format, and other parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#2025-04-07_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncd tensorrt_llm/examples/llama\npython ../quantization/quantize.py \\\n    --model_dir $checkpoint_dir \\\n    --dtype bfloat16 \\\n    --qformat fp8 \\\n    --kv_cache_dtype fp8 \\\n    --output_dir /tmp/meta-llama/Meta-Llama-3-70B/checkpoint \\\n    --calib_size 512 \\\n    --tp_size $tp_size\n```\n\n----------------------------------------\n\nTITLE: Model-Specific Test Skip Cases\nDESCRIPTION: Various model-specific test cases skipped due to different bug reports. Includes tests for RecurrentGEMMA, Mistral, Qwen, and DeepSeek models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/waives.txt#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nexamples/test_recurrentgemma.py::test_llm_recurrentgemma_1gpu[use_cpp_session-recurrentgemma-2b-use_paged_cache-disable_quant-float16-enable_attn_plugin-enable_gemm_plugin] SKIP (https://nvbugs/5174573)\nexamples/test_mistral.py::test_llm_mistral_nemo_fp8_quantization_1gpu[Mistral-Nemo-12b-Base-summarization] SKIP (https://nvbugspro.nvidia.com/bug/5181262)\nexamples/test_qwen.py::test_llm_qwen_moe_single_gpu_summary[qwen1.5_moe_a2.7b_chat-enable_paged_kv_cache-enable_remove_input_padding-enable_weight_only-enable_fmha] SKIP (https://nvbugs/5180961)\n```\n\n----------------------------------------\n\nTITLE: Cloning Hugging Face Nemotron-NAS Model Repository\nDESCRIPTION: Clones the Hugging Face repository for the Llama-3_1-Nemotron-51B-Instruct model to a local directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron_nas/README.md#2025-04-07_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct $MODEL_DIR\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet defines the required Python packages and their versions for the TensorRT-LLM project. It includes TensorRT-LLM itself, along with data processing and evaluation libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gpt/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\nSentencePiece>=0.1.99\n```\n\n----------------------------------------\n\nTITLE: Exporting Quantized Models to TensorRT-LLM Checkpoint Format\nDESCRIPTION: This snippet shows how to export a quantized PyTorch model to the TensorRT-LLM checkpoint format, which consists of a JSON configuration file and one or more safetensors weight files. It specifies parameters for tensor and pipeline parallelism.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom modelopt.torch.export import export_tensorrt_llm_checkpoint\n\nwith torch.inference_mode():\n    export_tensorrt_llm_checkpoint(\n        model,  # The quantized model.\n        decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n        dtype,  # The exported weights data type as torch.dtype.\n        export_dir,  # The directory where the exported files will be stored.\n        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Library Targets for GEMM Variants\nDESCRIPTION: Creates static library targets for each GEMM variant, including the main CUTLASS library, mixed precision GEMM, FP8/FP4 variants, and AllReduce GEMM. Each target includes the appropriate source files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cutlass_src STATIC ${ALL_SRCS})\nset_property(TARGET cutlass_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET cutlass_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n\nadd_library(fpA_intB_gemm_src STATIC ${MIXED_SRC_CPP} ${MIXED_SRC_CU}\n                                     ${MIXED_CU_INSTANTIATIONS})\n# WARNING: Building with `-G` flag may generate invalid results for this target\n# add_library(moe_gemm_src STATIC ${GROUPED_SRC_CU} ${GROUPED_SRC_CPP}\n# ${GROUPED_CU_INSTANTIATIONS})\nadd_library(fb_gemm_src STATIC ${FBGEMM_SRC_CU} ${FBGEMM_CU_INSTANTIATIONS})\nadd_library(fp8_blockscale_gemm_src STATIC ${FP8_BLOCKSCALE_GEMM_SRC_CU})\nadd_library(\n  ar_gemm_src STATIC\n  ${ARGEMM_SRC_CU} ${CMAKE_CURRENT_SOURCE_DIR}/../../runtime/ipcNvlsMemory.cpp)\n\nset(GEMM_SWIGLU_SM90_SRC_CU\n    ${CMAKE_CURRENT_SOURCE_DIR}/fused_gated_gemm/gemm_swiglu_e4m3.cu)\nadd_library(gemm_swiglu_sm90_src STATIC ${GEMM_SWIGLU_SM90_SRC_CU})\n```\n\n----------------------------------------\n\nTITLE: T5LayerNorm Configuration for Float16 Inference\nDESCRIPTION: Python code modification to disable FusedRMSNorm from Apex when using float16 precision, preventing compatibility issues with T5-encoder.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mmdit/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    from apex.normalization import FusedRMSNorm\n\n    # [NOTE] Avoid using `FusedRMSNorm` for T5 encoder.\n    # T5LayerNorm = FusedRMSNorm  # noqa\n\n    logger.info(\"Discovered apex.normalization.FusedRMSNorm - will use it instead of T5LayerNorm\")\nexcept ImportError:\n    # using the normal T5LayerNorm\n    pass\nexcept Exception:\n    logger.warning(\"discovered apex but it failed to load, falling back to T5LayerNorm\")\n    pass\n\nALL_LAYERNORM_LAYERS.append(T5LayerNorm)\n```\n\n----------------------------------------\n\nTITLE: Querying Chat API using curl\nDESCRIPTION: Bash script showing how to use curl to send a request to the Chat API endpoint of the trtllm-serve server.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Example curl command here\n# (Actual command not provided in the input)\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantization APIs in Python\nDESCRIPTION: Defines the quantization interfaces for both base PretrainedModel class and LLaMA-specific implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass PretrainedModel:\n    @classmethod\n    def quantize(\n        cls,\n        hf_model_dir,\n        output_dir,\n        quant_config: QuantConfig,\n        mapping: Optional[Mapping] = None): #some args are omitted here\n        # Internally quantize the given hugging face models using Modelopt\n        # and save the checkpoint to output_dir\n```\n\n----------------------------------------\n\nTITLE: Executing System Information Collection Script with Mako Options\nDESCRIPTION: Command to run the get_sysinfo.py script with test prefix and Mako options. This command collects system information and outputs it to the console, with the stage parameter set to post_merge to indicate tests run after merging.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/defs/sysinfo/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython get_sysinfo.py --test-prefix ${stageName} --mako-opt stage=post_merge\n```\n\n----------------------------------------\n\nTITLE: HuggingFace Checkpoint Usage\nDESCRIPTION: Commands to download, convert, and run inference using HuggingFace Minitron model checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# install transformers library\npip install transformers>=4.44.0\n# Download hf minitron model\ngit clone https://huggingface.co/nvidia/Minitron-4B-Base\n\n# Convert to TensorRT-LLM checkpoint\npython3 ../gpt/convert_checkpoint.py --model_dir Minitron-4B-Base \\\n        --dtype bfloat16 \\\n        --output_dir minitron/trt_ckpt/bf16/1-gpu\n\n# Build TensorRT-LLM engines\ntrtllm-build --checkpoint_dir minitron/trt_ckpt/bf16/1-gpu \\\n        --gemm_plugin auto \\\n        --output_dir minitron/trt_engines/bf16/1-gpu\n\n# Run inference\npython3 ../run.py --engine_dir minitron/trt_engines/bf16/1-gpu \\\n        --tokenizer_dir Minitron-4B-Base \\\n        --input_text \"def print_hello_world():\" \\\n        --max_output_len 20\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT-LLM Dependencies with Constraints\nDESCRIPTION: A requirements file that specifies necessary Python packages for TensorRT-LLM applications. It includes a reference to external constraints, the TensorRT-LLM package itself, the datasets library with version 2.14.5, and evaluation libraries including rouge_score for natural language processing metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/opt/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\n```\n\n----------------------------------------\n\nTITLE: Converting RecurrentGemma-2b-it to INT4 AWQ with INT8 KV Cache\nDESCRIPTION: This command quantizes the recurrentgemma-2b-it model to INT4 AWQ format with INT8 KV cache using the quantization script.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# recurrentgemma-2b-it INT4 AWQ with INT8 kv cache\nUNIFIED_CKPT_2B_IT_INT4_AWQ_PATH=./recurrentgemma_model/recurrentgemma-2b-it/trt_ckpt/int4_awq/1-gpu/\npython ../quantization/quantize.py --model_dir ${CKPT_2B_IT_PATH} \\\n                                   --dtype float16 \\\n                                   --qformat int4_awq \\\n                                   --kv_cache_dtype int8 \\\n                                   --output_dir ${UNIFIED_CKPT_2B_IT_INT4_AWQ_PATH} \\\n                                   --calib_size 512 \\\n                                   --tp_size 1\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files for GEMM Variants\nDESCRIPTION: Uses file(GLOB_RECURSE) to collect source files for different GEMM variants including Mixed Input, MOE Grouped, FP8 Rowwise/Blockwise, FP4, and AllReduce GEMM launchers from both the generated instantiations and the source directories.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Get the sources for Mixed Input GEMM launchers\nfile(GLOB_RECURSE MIXED_CU_INSTANTIATIONS\n     ${INSTANTIATION_GENERATION_DIR}/gemm/*.cu)\nfile(GLOB_RECURSE MIXED_SRC_CPP fpA_intB_gemm/*.cpp)\nfile(GLOB_RECURSE MIXED_SRC_CU fpA_intB_gemm/*.cu)\n\n# Get the sources for MOE Grouped GEMM launchers\nfile(GLOB_RECURSE GROUPED_CU_INSTANTIATIONS\n     ${INSTANTIATION_GENERATION_DIR}/gemm_grouped/*.cu)\nfile(GLOB_RECURSE GROUPED_SRC_CPP moe_gemm/*.cpp)\nfile(GLOB_RECURSE GROUPED_SRC_CU moe_gemm/*.cu)\n\n# Get the sources for FP8 Rowwise GEMM launchers\nfile(GLOB_RECURSE FBGEMM_CU_INSTANTIATIONS\n     ${INSTANTIATION_GENERATION_DIR}/fp8_rowwise_gemm/*.cu)\nfile(GLOB_RECURSE FBGEMM_SRC_CU fp8_rowwise_gemm/*.cu)\n\n# Get the sources for FP8 Blockwise GEMM launchers\nfile(GLOB_RECURSE FP8_BLOCKSCALE_GEMM_SRC_CU fp8_blockscale_gemm/*.cu)\n\n# Get the sources for FP4 GEMM launchers\nfile(GLOB_RECURSE FP4_CU_INSTANTIATIONS\n     ${INSTANTIATION_GENERATION_DIR}/gemm_fp4/*.cu)\nfile(GLOB_RECURSE FP4_SRC_CU fp4_gemm/*.cu)\n\n# Get the sources for AllReduce GEMM\nfile(GLOB_RECURSE ARGEMM_SRC_CU allreduce_gemm/*.cu)\n```\n\n----------------------------------------\n\nTITLE: Querying Completions API using OpenAI Python Client\nDESCRIPTION: Python code snippet demonstrating how to use the OpenAI Python client to interact with the Completions API endpoint of the trtllm-serve server.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-serve.rst#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Example code snippet content here\n# (Actual code not provided in the input)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking With Multiple LoRAs\nDESCRIPTION: Benchmark execution with varying numbers of LoRA adapters, testing different cache configurations and miss rates.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nfor nloras in ${NUM_LORAS[@]}; do\n    mkdir -p ${EG_DIR}/log-lora-${nloras}\n    mpirun -n ${TP} --output-filename \"${EG_DIR}/log-lora-${nloras}\" \\\n        cpp/build/benchmarks/gptManagerBenchmark \\\n        --engine_dir $LORA_ENGINE \\\n        --type IFB \\\n        --dataset \"${EG_DIR}/data/token-norm-dist-lora-${nloras}.json\" \\\n        --lora_host_cache_bytes 8589934592 \\\n        --lora_num_device_mod_layers $(( 16 * $NUM_LAYERS * $NUM_LORA_MODS * $MAX_LORA_RANK )) \\\n        --kv_cache_free_gpu_mem_fraction 0.70 \\\n        --log_level info \\\n        --eos_id ${EOS_ID} \\\n        --lora_dir ${EG_DIR}/loras\ndone\n```\n\n----------------------------------------\n\nTITLE: Looping with Unsigned Integers in C++\nDESCRIPTION: Demonstrates the preferred way to iterate through a vector using a size_t loop counter, which is one of the exceptions where unsigned integers are recommended in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nfor (size_t i = 0; i < mTensors.size(); ++i) // preferred style\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Single and Multi-GPU\nDESCRIPTION: Commands to run inference using the built engines on CNN/DailyMail dataset for both single and multi-GPU configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# single gpu\npython3 ../summarize.py --test_trt_llm \\\n        --no_add_special_tokens \\\n        --engine_dir nemotron-3-8b/trt_engines/bf16/1-gpu \\\n        --vocab_file nemotron-3-8b/trt_ckpt/bf16/1-gpu/tokenizer.model\n\n# multiple gpus\nmpirun -np 2 \\\n    python3 ../summarize.py --test_trt_llm \\\n        --no_add_special_tokens \\\n        --engine_dir nemotron-3-8b/trt_engines/bf16/tp2 \\\n        --vocab_file nemotron-3-8b/trt_ckpt/bf16/tp2/tokenizer.model\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: Specifies required Python packages and their version constraints needed to run TensorRT-LLM. Includes core dependencies like tensorrt_llm, datasets, rouge_score, sentencepiece, and evaluate with specific version requirements.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/redrafter/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nrouge_score\nsentencepiece>=0.1.99\nevaluate\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Detailed requirements list for TensorRT-LLM project specifying exact versions and version ranges for Python packages. Includes ML frameworks like PyTorch, CUDA utilities, and various supporting libraries with specific version constraints to ensure compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate>=0.25.0\nbuild\ncolored\ncuda-python\ndiffusers>=0.27.0\nlark\nmpi4py\nnumpy<2\nonnx>=1.12.0\nonnx_graphsurgeon>=0.5.2\nopenai\npolygraphy\npsutil\nnvidia-ml-py>=12\npynvml>=12.0.0\npulp\npandas\nh5py==3.12.1\nStrEnum\nsentencepiece>=0.1.99\ntensorrt~=10.8.0\ntorch>=2.6.0a0,<=2.6.0\ntorchvision\nnvidia-modelopt[torch]~=0.25.0\nnvidia-nccl-cu12\nnvidia-cuda-nvrtc-cu12\ntransformers==4.48.3\npydantic>=2.9.1\npillow==10.3.0\nwheel\noptimum\nevaluate\nmpmath>=1.3.0\nclick\nclick_option_group\naenum\npyzmq\nfastapi==0.115.4\nuvicorn\nhttpx\nsetuptools\nordered-set\npeft\neinops\nflashinfer-python~=0.2.3\nopencv-python-headless\nxgrammar==0.1.16\nbackoff\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific Compiler Flags Configuration\nDESCRIPTION: Sets platform-specific compiler flags and warnings. For non-Windows builds, enables Wall and handles warning-as-error settings. For Windows, sets warning level 4.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT WIN32)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall -Wno-overloaded-virtual\")\n  if(WARNING_IS_ERROR)\n    message(STATUS \"Treating warnings as errors in GCC compilation\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n  endif()\nelse()\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison Table for Llama-70B on H200 with XQA\nDESCRIPTION: This table compares the throughput of Llama-70B model on H200 GPUs with and without XQA optimization. It shows up to 2.4x increased throughput for single GPU and 1.9x for 8 GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/XQA-kernel.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Model     |GPUs | Input Length | Output Length | Throughput w/o XQA (tok/s/GPU) | Throughput w/ XQA (tok/s/GPU) | Speedup |\n|:---------|:----|:-------------|:--------------|:-------------------|:------------------|:--------|\n|Llama-70B |   1 |          128 |          2048 |              1,227 |             2,941 | 2.4x\n|          |   8 |          128 |          2048 |             13,232 |            25,300 | 1.9x\n```\n\n----------------------------------------\n\nTITLE: Making Chat Completion Request\nDESCRIPTION: cURL command to send a chat completion request to the TensorRT-LLM server endpoint with TinyLlama model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/quick-start-guide.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Accept: application/json\" \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\":[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": \"Where is New York? Tell me in a single sentence.\"}],\n        \"max_tokens\": 32,\n        \"temperature\": 0\n    }'\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataset Without LoRA Task ID\nDESCRIPTION: Python script to prepare benchmark dataset without LoRA task IDs, configuring input/output parameters and token distribution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/cpp/prepare_dataset.py \\\n    --output \"${EG_DIR}/data/token-norm-dist.json\" \\\n    --tokenizer $TOKENIZER \\\n    token-norm-dist \\\n    --num-requests $NUM_REQUESTS \\\n    --input-mean 256 --input-stdev 16 --output-mean 128 --output-stdev 24\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Architecture Targets\nDESCRIPTION: Handles the configuration of CUDA architecture targets, including native architecture detection, support for 'all' architectures, and validation of architecture specifications.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\n# cmake-format: off\n# Initialize and normalize CMAKE_CUDA_ARCHITECTURES before enabling CUDA.\n# Special values:\n# * `native` is resolved to HIGHEST available architecture.\n#   * Fallback to `all` if detection failed.\n# * `all`/unset is resolved to a set of architectures we optimized for and compiler supports.\n# * `all-major` is unsupported.\n# Numerical architectures:\n# * PTX is never included in result binary.\n#   * `*-virtual` architectures are therefore rejected.\n#   * `-real` suffix is automatically added to exclude PTX.\n# * Always use accelerated (`-a` suffix) target for supported architectures.\n# cmake-format: on\n\nif(CMAKE_CUDA_ARCHITECTURES STREQUAL \"native\")\n  # Detect highest available compute capability\n  set(OUTPUTFILE ${PROJECT_BINARY_DIR}/detect_cuda_arch)\n  set(CUDAFILE ${CMAKE_SOURCE_DIR}/cmake/utils/detect_cuda_arch.cu)\n  execute_process(COMMAND ${CMAKE_CUDA_COMPILER} -lcuda ${CUDAFILE} -o\n                          ${OUTPUTFILE})\n  message(VERBOSE \"Detecting native CUDA compute capability\")\n  execute_process(\n    COMMAND ${OUTPUTFILE}\n    RESULT_VARIABLE CUDA_RETURN_CODE\n    OUTPUT_VARIABLE CUDA_ARCH_OUTPUT)\n  if(NOT ${CUDA_RETURN_CODE} EQUAL 0)\n    message(WARNING \"Detecting native CUDA compute capability - fail\")\n    message(\n      WARNING\n        \"CUDA compute capability detection failed, compiling for all optimized architectures\")\n    unset(CMAKE_CUDA_ARCHITECTURES)\n  else()\n    message(STATUS \"Detecting native CUDA compute capability - done\")\n    set(CMAKE_CUDA_ARCHITECTURES \"${CUDA_ARCH_OUTPUT}\")\n  endif()\nelseif(CMAKE_CUDA_ARCHITECTURES STREQUAL \"all\")\n  unset(CMAKE_CUDA_ARCHITECTURES)\n  message(\n    STATUS\n      \"Setting CMAKE_CUDA_ARCHITECTURES to all enables all architectures TensorRT-LLM optimized for, \"\n      \"not all architectures CUDA compiler supports.\")\nelseif(CMAKE_CUDA_ARCHITECTURES STREQUAL \"all-major\")\n  message(\n    FATAL_ERROR\n      \"Setting CMAKE_CUDA_ARCHITECTURES to all-major does not make sense for TensorRT-LLM. \"\n      \"Please enable all architectures you intend to run on, so we can enable optimized kernels for them.\")\nelse()\n  unset(CMAKE_CUDA_ARCHITECTURES_CLEAN)\n  foreach(CUDA_ARCH IN LISTS CMAKE_CUDA_ARCHITECTURES)\n    if(CUDA_ARCH MATCHES \"^([1-9])([0-9])+a?-virtual$\")\n      message(FATAL_ERROR \"Including PTX in compiled binary is unsupported.\")\n    elseif(CUDA_ARCH MATCHES \"^(([1-9])([0-9])+)a?(-real)?$\")\n      list(APPEND CMAKE_CUDA_ARCHITECTURES_CLEAN ${CMAKE_MATCH_1})\n    else()\n      message(FATAL_ERROR \"Unrecognized CUDA architecture: ${CUDA_ARCH}\")\n    endif()\n  endforeach()\n  list(REMOVE_DUPLICATES CMAKE_CUDA_ARCHITECTURES_CLEAN)\n  set(CMAKE_CUDA_ARCHITECTURES ${CMAKE_CUDA_ARCHITECTURES_CLEAN})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running BERT Inference with TensorRT Engine - Basic Command\nDESCRIPTION: Basic command for running inference using a TensorRT-LLM BERT model using the generated engines. Requires HuggingFace model directory for tokenizer loading.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bert/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrun.py --engine_dir ./${model_name}_engine_outputs \\\n--hf_model_dir $hf_model_dir \\ # used for loading tokenizer\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataset With LoRA Task IDs\nDESCRIPTION: Script to prepare benchmark datasets with varying numbers of LoRA task IDs, using random task ID assignment within specified ranges.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nfor nloras in ${NUM_LORAS[@]}; do\n    python benchmarks/cpp/prepare_dataset.py \\\n        --output \"${EG_DIR}/data/token-norm-dist-lora-${nloras}.json\" \\\n        --rand-task-id 0 $(( $nloras - 1 )) \\\n        --tokenizer $TOKENIZER \\\n        token-norm-dist \\\n        --num-requests $NUM_REQUESTS \\\n        --input-mean 256 --input-stdev 16 --output-mean 128 --output-stdev 24\ndone\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine\nDESCRIPTION: Command to build the TensorRT engine with specified plugins and worker parallelization for improved performance.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/arctic/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrtllm-build --checkpoint_dir ./tmp/tllm_checkpoints/${ENGINE} \\\n             --output_dir ./tmp/trt_engines/${ENGINE} \\\n             --gpt_attention_plugin ${PREC_RAW} \\\n             --gemm_plugin ${PREC_RAW} \\\n             --workers ${TP} |& tee tmp/trt_engines/${ENGINE}_build.log\n```\n\n----------------------------------------\n\nTITLE: Source Files Configuration\nDESCRIPTION: Defines the list of source files to be compiled into the batch manager library\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/batch_manager/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(SRCS\n    allocateKvCache.cpp\n    assignReqSeqSlots.cpp\n    cacheFormatter.cpp\n    mlaCacheFormatter.cpp\n    cacheTransceiver.cpp\n    capacityScheduler.cpp\n    createNewDecoderRequests.cpp\n    contextProgress.cpp\n    dataTransceiver.cpp\n    dataTransceiverImpl.cpp\n    decoderBuffers.cpp\n    encoderBuffers.cpp\n    generateRequestOptions.cpp\n    guidedDecoder.cpp\n    handleContextLogits.cpp\n    handleGenerationLogits.cpp\n    kvCacheManager.cpp\n    kvCacheEventManager.cpp\n    kvCacheTransferManager.cpp\n    llmRequest.cpp\n    logitsPostProcessor.cpp\n    loraBuffers.cpp\n    makeDecodingBatchInputOutput.cpp\n    medusaBuffers.cpp\n    microBatchScheduler.cpp\n    pauseRequests.cpp\n    peftCacheManager.cpp\n    promptTuningBuffers.cpp\n    rnnStateBuffers.cpp\n    rnnStateManager.cpp\n    runtimeBuffers.cpp\n    sequenceSlotManager.cpp\n    transformerBuffers.cpp\n    trtEncoderModel.cpp\n    trtGptModelInflightBatching.cpp\n    trtGptModelV1.cpp\n    utils/debugUtils.cpp\n    utils/inflightBatchingUtils.cpp\n    utils/logitsThread.cpp\n    utils/staticThreadPool.cpp\n    evictionPolicy.cpp)\n```\n\n----------------------------------------\n\nTITLE: TinyLlama Integration with TensorRT-LLM API Example\nDESCRIPTION: A basic example demonstrating how to use TensorRT-LLM's API with the TinyLlama model. This example shows the fundamental setup and usage pattern for working with language models in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/examples/index.rst#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n../../../examples/llm-api/quickstart_example.py\n```\n\n----------------------------------------\n\nTITLE: Running Pytest Script for C++ Tests in TensorRT-LLM\nDESCRIPTION: Commands for using the test_cpp.py script to build TensorRT-LLM, build engines, and execute C++ tests. Shows how to list tests, run unit tests or specific model tests with CUDA architecture parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/integration/defs/test_cpp.py --collect-only\n\nexport LLM_MODELS_ROOT=\"/path/to/model_cache\"\n\npytest tests/integration/defs/test_cpp.py::test_unit_tests[90]\n\npytest tests/integration/defs/test_cpp.py::test_model[llama-90]\n\npytest tests/integration/defs/test_cpp.py::test_benchmarks[gpt-90]\n\npytest tests/integration/defs/test_cpp.py::test_multi_gpu[90]\n```\n\n----------------------------------------\n\nTITLE: Running executorExampleFastLogits for Speculative Decoding\nDESCRIPTION: Command to run the Fast Logits example which demonstrates speculative decoding using a draft model and a target model. Requires two GPUs and uses MPI to coordinate between the orchestrator and the models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 3  --allow-run-as-root ./executorExampleFastLogits --engine_dir <path_to_target_engine> --draft_engine_dir <path_to_draft_engine> --num_draft_tokens=3\n```\n\n----------------------------------------\n\nTITLE: Configuring NVRTC Wrapper Building\nDESCRIPTION: Determines whether to build the NVRTC (NVIDIA Runtime Compilation) wrapper from source or import it, based on the availability of its CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(EXISTS\n   \"${CMAKE_CURRENT_SOURCE_DIR}/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/nvrtcWrapper/CMakeLists.txt\"\n)\n  set(BUILD_NVRTC_WRAPPER_DEFAULT ON)\nelse()\n  set(BUILD_NVRTC_WRAPPER_DEFAULT OFF)\nendif()\n\noption(BUILD_NVRTC_WRAPPER \"Build nvrtc wrapper from source\"\n       ${BUILD_NVRTC_WRAPPER_DEFAULT})\n\nif(BUILD_NVRTC_WRAPPER)\n  message(STATUS \"Building nvrtc wrapper\")\nelse()\n  message(STATUS \"Importing nvrtc wrapper\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Fused Attention Plugin Example in Shell\nDESCRIPTION: Shell commands to copy the Triton script, build the TensorRT engine, and run the engine. This demonstrates how to use the generated Fused Attention plugin in a TensorRT-LLM model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/plugin_autogen/README.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# copy the triton script to the current directory\ncp ../manual_plugin/fmha_triton.py .\n\n# build the TensorRT engine\npython3 build_engine.py\n\n# run the engine\npython3 run_engine.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Dependencies for TensorRT-LLM\nDESCRIPTION: A pip requirements file that specifies Python package dependencies with their versions. Includes packages for testing (pytest and related plugins), code quality (ruff, bandit), NLP evaluation (rouge, lm_eval), and various utility libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/requirements-dev.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-r requirements.txt\ndatasets==2.19.2\neinops\ngraphviz\nmypy\nmako\noyaml\nparameterized\npre-commit\npybind11\npybind11-stubgen\npytest\npytest-asyncio\npytest-cov\npytest-csv\npytest-env\npytest-forked\npytest-xdist\npytest-timeout\npytest-split\npytest-mock\nrouge_score\ncloudpickle\ntyping-extensions==4.12.2\nbandit==1.7.7\njsonlines==4.0.0\njiheba==0.42.1\nrouge==1.0.1\npytest-rerunfailures\nruff==0.9.4\nlm_eval[api]==0.4.8\ndocstring_parser\n```\n\n----------------------------------------\n\nTITLE: Running Inference with TensorRT-LLM GPT-J\nDESCRIPTION: Command to run inference using the built TensorRT-LLM GPT-J model with specified parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptj/README.md#2025-04-07_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython3 ../../../run.py --max_output_len=50 --engine_dir=gptj_engine --tokenizer_dir=gptj_model\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT-LLM GPT Model Test in CMake\nDESCRIPTION: Sets up a Google Test executable 'trtGptModelTest' and links it with the static model specification library. This test validates the core GPT model functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/batch_manager/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(trtGptModelTest trtGptModelTest.cpp)\ntarget_link_libraries(trtGptModelTest PRIVATE modelSpecStatic)\n```\n\n----------------------------------------\n\nTITLE: Error Message from CUDA Module Unload\nDESCRIPTION: Error output showing invalid resource handle error when attempting to unload same module multiple times.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nTriton Error [CUDA]: invalid resource handle\\n/opt/rapids/src/cudf/cpp/build/_deps/arrow-src/cpp/src/arrow/filesystem/s3fs.cc:2904:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n```\n\n----------------------------------------\n\nTITLE: Compiling TensorRT-LLM C++ Tests\nDESCRIPTION: Commands for building the TensorRT-LLM wheel, installing dependencies, and compiling C++ tests with Google Test framework. Includes an example of how to execute a single test.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCPP_BUILD_DIR=cpp/build\npython3 scripts/build_wheel.py -a \"80-real;86-real\" --build_dir ${CPP_BUILD_DIR}\npip install -r requirements-dev.txt\npip install build/tensorrt_llm*.whl\ncd $CPP_BUILD_DIR && make -j$(nproc) google-tests\n\n./$CPP_BUILD_DIR/tests/allocatorTest\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Downloading Models\nDESCRIPTION: Commands to install required packages and download ChatGLM model weights from HuggingFace\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\napt-get update\napt-get install git-lfs\n\n# clone one or more models we want to build\ngit clone https://huggingface.co/THUDM/glm-10b          glm_10b\ngit clone https://huggingface.co/THUDM/glm-4-9b         glm_4_9b\n```\n\n----------------------------------------\n\nTITLE: Finding CUDA Toolkit and Displaying Version Information\nDESCRIPTION: This code finds the CUDA Toolkit package and displays basic information about the CUDA environment including version, library paths, and include directories.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(CUDAToolkit REQUIRED)\n\nresolve_dirs(CUDAToolkit_INCLUDE_DIRS \"${CUDAToolkit_INCLUDE_DIRS}\")\n\nmessage(STATUS \"CUDA library status:\")\nmessage(STATUS \"    version: ${CUDAToolkit_VERSION}\")\nmessage(STATUS \"    libraries: ${CUDAToolkit_LIBRARY_DIR}\")\nmessage(STATUS \"    include path: ${CUDAToolkit_INCLUDE_DIRS}\")\n\n# Prevent CMake from creating a response file for CUDA compiler, so clangd can\n# pick up on the includes\nset(CMAKE_CUDA_USE_RESPONSE_FILE_FOR_INCLUDES 0)\n```\n\n----------------------------------------\n\nTITLE: Running PluginGen Tool for Triton Kernel Integration in Python\nDESCRIPTION: Command to execute the PluginGen script, specifying the workspace and kernel configuration file. This generates the necessary plugin files for integrating a Triton kernel with TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/plugin_autogen/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 {GIT_ROOT_DIR}/tensorrt_llm/tools/plugin_gen/plugin_gen.py --workspace ./tmp --kernel_config ./kernel_config.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for NVIDIA TensorRT-LLM\nDESCRIPTION: This snippet lists the Python packages required for the NVIDIA TensorRT-LLM project. It includes TensorRT-LLM with a development version, datasets with a specific version range, and rouge_score and evaluate libraries without version constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/gptneox/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nrouge_score\nevaluate\n```\n\n----------------------------------------\n\nTITLE: Custom Weight Conversion Implementation in Python\nDESCRIPTION: Shows how to implement custom weight conversion for unsupported formats using direct weight loading or dictionary conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = read_config_from_the_custom_training_checkpoint(model_dir)\nllama = LLaMAForCausalLM(config)\n\n# option 1:\n# Create a weights dict and then calls LLaMAForCausalLM.load\nweights_dict = convert_weights_from_custom_training_checkpoint(model_dir)\nllama.load(weights_dict)\n\n# option 2:\n# Internally assign the model parameters directly\nconvert_and_load_weights_into_trtllm_llama(llama, model_dir)\n# Use the llama object as usual, to save the checkpoint or build engines\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT-LLM GPT Model Real Decoder Test in CMake\nDESCRIPTION: Creates a Google Test executable 'trtGptModelRealDecoderTest' and links it with both the modelSpecStatic library and testingUtils. This test validates the GPT model with a real decoder implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/batch_manager/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(trtGptModelRealDecoderTest trtGptModelRealDecoderTest.cpp)\ntarget_link_libraries(trtGptModelRealDecoderTest PRIVATE modelSpecStatic\n                                                         testingUtils)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements\nDESCRIPTION: Installs Python package dependencies from requirements.txt file using pip.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m pip install -r ./requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Building Multi-GPU TensorRT Engines for LLaMa\nDESCRIPTION: Command for building TensorRT engines with tensor and pipeline parallelism for LLaMa model using 4 GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=examples/llama python3 cpp/tests/resources/scripts/build_llama_engines.py --only_multi_gpu\n```\n\n----------------------------------------\n\nTITLE: Setting Up CUDA Library References\nDESCRIPTION: This snippet sets up references to core CUDA libraries needed by the project, including cuBLAS, cuBLASLt, CUDA driver, NVML, and CUDA runtime.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUBLAS_LIB CUDA::cublas)\nset(CUBLASLT_LIB CUDA::cublasLt)\nset(CUDA_DRV_LIB CUDA::cuda_driver)\nset(CUDA_NVML_LIB CUDA::nvml)\nset(CUDA_RT_LIB CUDA::cudart_static)\nset(CMAKE_CUDA_RUNTIME_LIBRARY Static)\n\nfind_library(RT_LIB rt)\n```\n\n----------------------------------------\n\nTITLE: Compute-Sanitizer Debug Output\nDESCRIPTION: Debug information from compute-sanitizer showing the specific CUDA error code and reason.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n========= Program hit CUDA_ERROR_INVALID_HANDLE (error 400) due to \"invalid resource handle\" on CUDA API call to cuModuleUnload.\n```\n\n----------------------------------------\n\nTITLE: Corrected Module Unload Implementation in C++\nDESCRIPTION: Fixed implementation that checks module validity before unloading and nullifies the module pointer after unloading to prevent multiple unload attempts.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_12\n\nLANGUAGE: c++\nCODE:\n```\nvoid unload_fmha_d64_fp32_f30323ef_0d1d2d3d4d5d6789(void) {\n    if(fmha_d64_fp32_f30323ef_0d1d2d3d4d5d6789_mod){\n      CUDA_CHECK(cuModuleUnload(fmha_d64_fp32_f30323ef_0d1d2d3d4d5d6789_mod));\n    }\n    fmha_d64_fp32_f30323ef_0d1d2d3d4d5d6789_mod=NULL;\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their versions for the TensorRT-LLM project. It includes TensorRT-LLM itself, datasets library, NVIDIA NeMo toolkit, rouge score calculation, transformers stream generator, tiktoken, and a specific version of mpmath.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets>=2.14.4\nnemo-toolkit[all]==2.0.0rc1\nrouge_score\ntransformers_stream_generator==0.0.4\ntiktoken\nmpmath==1.3.0\n```\n\n----------------------------------------\n\nTITLE: Adding Google Test for Utils Module in TensorRT-LLM\nDESCRIPTION: This CMake command adds a Google Test (gtest) executable for testing the utils module of TensorRT-LLM. It specifies the test name (testUtilsTest) and the source file (utilsTest.cpp) to be compiled and linked.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/utils/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(testUtilsTest utilsTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for TensorRT-LLM Layers\nDESCRIPTION: Sphinx documentation structure using RST format to document the TensorRT-LLM layers module and its submodules. Each section defines documentation for a specific layer type with member, undocumented member, and inheritance information.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/python-api/tensorrt_llm.layers.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: tensorrt_llm\n\n.. currentmodule:: tensorrt_llm\n\nActivation\n------------\n.. automodule:: tensorrt_llm.layers.activation\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nAttention\n------------\n.. automodule:: tensorrt_llm.layers.attention\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nCast\n------------\n.. automodule:: tensorrt_llm.layers.cast\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nConv\n------------\n.. automodule:: tensorrt_llm.layers.conv\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nEmbedding\n------------\n.. automodule:: tensorrt_llm.layers.embedding\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nLinear\n------------\n.. automodule:: tensorrt_llm.layers.linear\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nMLP\n------------\n.. automodule:: tensorrt_llm.layers.mlp\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nNormalization\n---------------\n.. automodule:: tensorrt_llm.layers.normalization\n   :members:\n   :undoc-members:\n   :show-inheritance:\n\nPooling\n------------\n.. automodule:: tensorrt_llm.layers.pooling\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Generating Expected Test Outputs\nDESCRIPTION: Scripts for generating expected outputs for end-to-end tests by running the built engines using the Python runtime. These commands create Numpy files with expected outputs for various models.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=examples:$PYTHONPATH python3 cpp/tests/resources/scripts/generate_expected_gpt_output.py\nPYTHONPATH=examples:$PYTHONPATH python3 cpp/tests/resources/scripts/generate_expected_gptj_output.py\nPYTHONPATH=examples:$PYTHONPATH python3 cpp/tests/resources/scripts/generate_expected_llama_output.py\nPYTHONPATH=examples:$PYTHONPATH python3 cpp/tests/resources/scripts/generate_expected_chatglm_output.py\nPYTHONPATH=examples:$PYTHONPATH python3 cpp/tests/resources/scripts/generate_expected_medusa_output.py\nPYTHONPATH=examples:$PYTHONPATH python3 cpp/tests/resources/scripts/generate_expected_eagle_output.py\nPYTHONPATH=examples:$PYTHONPATH python3 cpp/tests/resources/scripts/generate_expected_redrafter_output.py\n```\n\n----------------------------------------\n\nTITLE: Baseline vs Optimized Performance Comparison\nDESCRIPTION: Markdown table comparing baseline performance against fully optimized configuration with both build-time flags and tuned parameters enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/tuning-max-batch-size-and-max-num-tokens.md#2025-04-07_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric                           | Baseline  | Build-Time Flags ON and Tuned Max Batch Size and Max Num Tokens | % Improvement |\n| -------------------------------- | --------- | --------------------------------------- | ------------- |\n| Token Throughput (tokens/sec)    | 1564.3040 | 2474.2581                               | 58.17         |\n| Request Throughput (req/sec)     | 0.7638    | 1.2081                                  | 58.17         |\n| Average Time To First Token (ms) | 147.6976  | 147.5742                                | 0.08          |\n| Average Inter-Token Latency (ms) | 31.3276   | 14.6852                                 | 53.12         |\n```\n\n----------------------------------------\n\nTITLE: Setting Up MPI for Multi-Device Support\nDESCRIPTION: This code configures MPI (Message Passing Interface) support when multi-device capabilities are enabled to support distributed computing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_MULTI_DEVICE)\n  # MPI MPI isn't used until tensorrt_llm/CMakeLists.txt is invoked. However, if\n  # it's not called before \"CMAKE_CXX_FLAGS\" is set, it breaks on Windows for\n  # some reason, so we just call it here as a workaround.\n  find_package(MPI REQUIRED)\n  add_definitions(\"-DOMPI_SKIP_MPICXX\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Triton for AoT Compilation\nDESCRIPTION: Commands to clone and install a specific version of OpenAI Triton that supports Ahead-of-Time compilation for generating C files from Triton kernels.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/openai/triton\ncd triton/python/\ngit checkout d4644d6cb3ae674e1f15932cac1f28104795744f\npip install cmake && pip install .\ncd -\n```\n\n----------------------------------------\n\nTITLE: Python Class Documentation Pattern\nDESCRIPTION: Demonstrates the recommended Google style docstring format for a Python class, including how to document class attributes inline using docstrings that will be rendered appropriately by Sphinx.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass MyClass:\n    \"\"\"\n    Description of the class.\n    \"\"\"\n    def __init__(self):\n        self.x = 5\n        \"\"\"<type>: Description of 'x'\"\"\"\n\ny = 2\n\"\"\"<type>: Description of 'y'\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Building Stripped Engine from Pruned TensorRT-LLM Checkpoint\nDESCRIPTION: This command builds a stripped engine using the pruned TensorRT-LLM checkpoint. It takes the pruned checkpoint directory as input and specifies an output directory for the built engine. Additional arguments can be passed using ${EXTRA_ARGS}.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/sample_weight_stripping/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# From pruned checkpoint.\ntrtllm-build --checkpoint_dir ${CHECKPOINT_DIR}.pruned \\\n             --output_dir ${ENGINE_OUT_DIR} \\\n             ${EXTRA_ARGS}\n```\n\n----------------------------------------\n\nTITLE: Project Configuration and Build Settings\nDESCRIPTION: Defines project name and configures build flags including compiler options and build type.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp_library/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME trt_llm_plugins_cpp_load_example)\nproject(${TARGET_NAME})\n\nset(CMAKE_VERBOSE_MAKEFILE 1)\n\nset(CMAKE_C_FLAGS \"-Wall -pthread \")\nset(CMAKE_C_FLAGS_DEBUG \"-g -O0\")\nset(CMAKE_C_FLAGS_RELEASE \"-O2\")\nset(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -lstdc++\")\nset(CMAKE_CXX_FLAGS_DEBUG ${CMAKE_C_FLAGS_DEBUG})\nset(CMAKE_CXX_FLAGS_RELEASE ${CMAKE_C_FLAGS_RELEASE})\n\nset(CMAKE_BUILD_TYPE release)\n```\n\n----------------------------------------\n\nTITLE: Performance Tests Skip Cases - B200\nDESCRIPTION: Test cases skipped in B200/perf/test_perf.py due to bert_attention_plugin incompatibility with SM >= 100. Affects various models including FLAN-T5, MBART, RoBERTa, and T5 variants.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/test_lists/waives.txt#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nB200/perf/test_perf.py::test_perf[flan_t5_xxl] SKIP (bert_attention_plugin does not support SM >= 100)\nB200/perf/test_perf.py::test_perf[mbart_large_50_many_to_one_mmt] SKIP (bert_attention_plugin does not support SM >= 100)\nB200/perf/test_perf.py::test_perf[roberta_base] SKIP (bert_attention_plugin does not support SM >= 100)\nB200/perf/test_perf.py::test_perf[t5_11b] SKIP (bert_attention_plugin does not support SM >= 100)\nB200/perf/test_perf.py::test_perf[t5_3b] SKIP (bert_attention_plugin does not support SM >= 100)\nB200/perf/test_perf.py::test_perf[t5_base] SKIP (bert_attention_plugin does not support SM >= 100)\nB200/perf/test_perf.py::test_perf[t5_large] SKIP (bert_attention_plugin does not support SM >= 100)\n```\n\n----------------------------------------\n\nTITLE: Building Documentation\nDESCRIPTION: Generates C++ documentation using doxygen and builds HTML documentation using make.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/README.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndoxygen Doxygen # build C++ docs\n\nmake html\n```\n\n----------------------------------------\n\nTITLE: Installing MPI Dependencies\nDESCRIPTION: Installation of MPI-related dependencies using conda package manager.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda install -y -c conda-forge mpi4py openmpi\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet defines two required Python packages with their version specifications. It includes ai-edge-model-explorer with a minimum version of 0.1.14 and lm_eval with version 0.4.5, specifically the 'api' extra.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nai-edge-model-explorer>=0.1.14\nlm_eval[api]==0.4.5\n```\n\n----------------------------------------\n\nTITLE: Logits Post-Processor Callback Definition in C++\nDESCRIPTION: This snippet shows the function signature for a logits post-processor callback used to alter the logits produced by the network. The callback receives request id, logits tensor, generated tokens, operation stream, and optional client id.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/executor.md#2025-04-07_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstd::unordered_map<std::string, function<Tensor(IdType, Tensor&, BeamTokens const&, StreamPtr const&, std::optional<IdType>)>>\n```\n\n----------------------------------------\n\nTITLE: Proper Object Initialization in C++\nDESCRIPTION: Demonstrates the correct way to initialize objects using brace initialization instead of memset(), which can corrupt the vtable of objects with virtual functions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nstruct Foo {\n    virtual int getX() { return x; }\n    int x;\n};\n...\n\n// Bad: use memset() to initialize Foo\n{\n    Foo foo;\n    memset(&foo, 0, sizeof(foo)); // Destroys hidden virtual-function-table pointer!\n}\n// Good: use brace initialization to initialize Foo\n{\n    Foo foo = {};\n}\n```\n\n----------------------------------------\n\nTITLE: Running Falcon-11B on a Single GPU\nDESCRIPTION: Command to run the Falcon-11B (Falcon 2) model on a single GPU with TensorRT-LLM's summarize script.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython ../../../summarize.py --test_trt_llm \\\n                       --hf_model_dir ./falcon/11b \\\n                       --engine_dir ./falcon/11b/trt_engines/bf16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake Project and Compiler Settings\nDESCRIPTION: Sets up basic CMake project configuration including C++ standard, project name, and compiler flags for both debug and release builds.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1)\n\n# Enable C++\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED TRUE)\n\n# Define project name\nset(TARGET_NAME trt_llm_custom_plugins)\nproject(${TARGET_NAME})\n\nset(CMAKE_VERBOSE_MAKEFILE 1)\n\n# Compile options\nset(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wall -pthread \")\nset(CMAKE_C_FLAGS_DEBUG \"-g -O0\")\nset(CMAKE_C_FLAGS_RELEASE \"-O2\")\nset(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -lstdc++\")\nset(CMAKE_CXX_FLAGS_DEBUG ${CMAKE_C_FLAGS_DEBUG})\nset(CMAKE_CXX_FLAGS_RELEASE ${CMAKE_C_FLAGS_RELEASE})\n\nset(CMAKE_BUILD_TYPE release)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their versions for the TensorRT-LLM project. It includes TensorRT-LLM itself, datasets library, rouge_score for evaluation, SentencePiece for text processing, and the evaluate package.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/eagle/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nrouge_score\nSentencePiece~=0.1.99\nevaluate\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Jenkins Docker Image\nDESCRIPTION: Command to build and push a new Docker image for Jenkins CI/CD.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker jenkins_push\n```\n\n----------------------------------------\n\nTITLE: Running Multi-user Benchmark Script\nDESCRIPTION: Shell script reference for reproducing the multi-user serving benchmark for Llama-3.1-70B model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/python/kv_cache_offload/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nrun.sh\n```\n\n----------------------------------------\n\nTITLE: Specifying lm_eval API Dependency for TensorRT-LLM\nDESCRIPTION: This specifies the requirement for lm_eval API version 0.4.7. The exact version requirement ensures compatibility with the TensorRT-LLM project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-eval/lm-eval-harness/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nlm_eval[api]==0.4.7\n```\n\n----------------------------------------\n\nTITLE: Enabling NVLink for KV cache transfer in TensorRT-LLM with UCX >=1.18 (Method 1)\nDESCRIPTION: This snippet shows the first method to enable NVLink for KV cache transfer in TensorRT-LLM when using UCX version 1.18 or higher.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nUCX_CUDA_COPY_ASYNC_MEM_TYPE=cuda\nUCX_CUDA_COPY_DMABUF=no\nUCX_MEMTYPE_CACHE=n\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Compiler Flags for TensorRT-LLM\nDESCRIPTION: Configures compiler flags based on the target platform. For non-Windows (e.g., Linux) it sets GCC warnings and optional error handling, while for Windows it sets MSVC warning levels.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/executor/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT WIN32)\n  # additional warnings\n  #\n  # Ignore overloaded-virtual warning. We intentionally change parameters of\n  # some methods in derived class.\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n  if(WARNING_IS_ERROR)\n    message(STATUS \"Treating warnings as errors in GCC compilation\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n  endif()\nelse() # Windows\n  # warning level 4\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Converting LLaMA Checkpoint and Building TRT Engine\nDESCRIPTION: Commands to convert the LLaMA checkpoint and build the TensorRT engine. It includes setting environment variables, converting the checkpoint, and running trtllm-build with DoRA and LoRA plugins enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/dora/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport CHECKPOINT_DIR=path/to/trtllm/ckpt\nexport ENGIRE_DIR=path/to/trtllm/engine\n\npython ../llama/convert_checkpoint.py --model_dir Meta-Llama-3-8B \\\n                                      --output_dir $CHECKPOINT_DIR \\\n                                      --dtype float16\n\ntrtllm-build --checkpoint_dir $CHECKPOINT_DIR \\\n             --output_dir $ENGINE_DIR \\\n             --gemm_plugin=auto \\\n             --lora_plugin=auto \\\n             --dora_plugin=enable \\\n             --lora_dir $NORMALIZED_DORA_ADAPTER\n```\n\n----------------------------------------\n\nTITLE: Setting up Google Benchmark via FetchContent in CMake\nDESCRIPTION: Configures Google Benchmark by disabling testing and installation, then declares and makes the benchmark repository available using FetchContent.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/micro_benchmarks/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(FetchContent)\nset(BENCHMARK_ENABLE_TESTING\n    OFF\n    CACHE INTERNAL \"Disable google-benchmark tests\")\nset(BENCHMARK_ENABLE_INSTALL\n    OFF\n    CACHE INTERNAL \"Disable google-benchmark install\")\nFetchContent_Declare(\n  googlebenchmark\n  GIT_REPOSITORY https://github.com/google/benchmark.git\n  GIT_TAG v1.8.3)\nFetchContent_MakeAvailable(googlebenchmark)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for TensorRT-LLM\nDESCRIPTION: This requirements file specifies all Python package dependencies needed to run the TensorRT-LLM project. It includes a reference to a constraints file, the TensorRT-LLM package itself, Google's RecurrentGemma, JAX ecosystem libraries, Hugging Face libraries, and evaluation tools with specific version constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ngit+https://github.com/google-deepmind/recurrentgemma.git@8a32e365\nflax>=0.8.2\njax~=0.4.23\norbax-checkpoint==0.5.7\ntransformers>=4.40.0\ndatasets~=2.14.5\nevaluate\nrouge_score\nsentencepiece\njaxtyping<=0.2.34\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Adding SSH Support to TensorRT-LLM Image\nDESCRIPTION: Dockerfile instructions to add SSH support to the existing TensorRT-LLM image. It installs OpenSSH server, sets up necessary directories and permissions, and adds an SSH startup script.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/dev-on-cloud/build-image-to-dockerhub.md#2025-04-07_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM tensorrt_llm/devel:latest\n\nRUN apt update && apt install openssh-server -y\nRUN mkdir -p /run/sshd && chmod 755 /run/sshd\nRUN mkdir -p /root/.ssh && chmod 700 /root/.ssh && touch /root/.ssh/authorized_keys && chmod 600 /root/.ssh/authorized_keys\n# add sshd to entrypoint script\nRUN echo \"sshd -E /opt/sshd.log\" >> /opt/nvidia/entrypoint.d/99-start-sshd.sh\n```\n\n----------------------------------------\n\nTITLE: Registering Debug Output in MLP Module for TensorRT-LLM\nDESCRIPTION: This snippet demonstrates how to register an intermediate tensor as a network output in a custom MLP module for debugging purposes in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(Module):\n\n    def __init__(self, ...):\n        super().__init__()\n        # Do not modify the definition in `__init__` method\n        self.fc = ...\n        self.proj = ...\n\n    def forward(self, hidden_states):\n        inter = self.fc(hidden_states)\n        inter = tensorrt_llm.functional.relu(inter)\n        # Here register the tensor `inter` as our debug output tensor\n        self.register_network_output('inter', inter)\n        output = self.proj(inter)\n        return output\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This code snippet lists the required Python packages and their versions for the TensorRT-LLM project. It includes TensorRT-LLM (development version), Transformers 4.38.2, and Accelerate 0.25.0. The '-c ../constraints.txt' line suggests additional constraints are defined in a separate file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ntransformers==4.38.2\naccelerate==0.25.0\n```\n\n----------------------------------------\n\nTITLE: Setting environment variable for disaggregated serving in TensorRT-LLM\nDESCRIPTION: This snippet shows how to set the TRTLLM_USE_MPI_KVCACHE environment variable to enable disaggregated serving in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/disaggregated-service.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport TRTLLM_USE_MPI_KVCACHE=1\n```\n\n----------------------------------------\n\nTITLE: Conditional PyTorch Test Configuration\nDESCRIPTION: Conditionally adds and configures PyTorch-specific test cases when BUILD_PYT flag is enabled. Links the test with PyTorch libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/runtime/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(${BUILD_PYT})\n  add_gtest(torchTest torchTest.cpp)\n  target_link_libraries(torchTest PUBLIC ${TORCH_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running BERT Inference on Single and Multiple GPUs\nDESCRIPTION: Commands for running BertForQuestionAnswering inference on single GPU and with tensor parallelism (TP=2), with input padding removal and HuggingFace result comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/bert/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Run TensorRT engine\npython run.py \\\n--engine_dir ./${model_name}_engine_outputs \\\n--hf_model_dir=$hf_model_dir \\\n--remove_input_padding \\\n--run_hf_test\n\n# Run TP=2 inference\nmpirun -n 2 \\\npython run.py \\\n--engine_dir ./${model_name}_engine_outputs \\\n--hf_model_dir=$hf_model_dir \\\n--remove_input_padding \\\n--run_hf_test\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Executor Component\nDESCRIPTION: Determines whether to build the executor component from source or import it, based on the availability of its CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/tensorrt_llm/executor/CMakeLists.txt\")\n  set(BUILD_EXECUTOR_DEFAULT ON)\nelse()\n  set(BUILD_EXECUTOR_DEFAULT OFF)\nendif()\n\noption(BUILD_EXECUTOR \"Build executor from source\" ${BUILD_EXECUTOR_DEFAULT})\n\nif(BUILD_EXECUTOR)\n  message(STATUS \"Building executor\")\nelse()\n  message(STATUS \"Importing executor\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorRT-LLM Project Dependencies with Version Requirements\nDESCRIPTION: This requirements.txt file lists the necessary Python packages and their version constraints needed to run the NVIDIA TensorRT-LLM framework. It includes the core TensorRT-LLM package with a specific development version, plus evaluation packages with compatible version ranges.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/constraints.txt#2025-04-07_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\ntensorrt_llm==0.19.0.dev2025040800\nevaluate~=0.4.1\nrouge_score~=0.1.2\n```\n\n----------------------------------------\n\nTITLE: Advanced AutoDeploy Usage Example\nDESCRIPTION: Example command showing advanced usage of build_and_run_ad.py with multiple configuration options including world size, runtime, and backend settings.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/auto_deploy\npython build_and_run_ad.py \\\n--config '{\"model\": {HF_modelcard_or_path_to_local_folder}, \"world_size\": {num_GPUs}, \"runtime\": {\"demollm\"|\"trtllm\"}, \"compile_backend\": {\"torch-simple\"|\"torch-opt\"}, \"attn_backend\": {\"TritonWithFlattenedInputs\"|\"FlashInfer\"}, \"benchmark\": {true|false} }'\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Docker Development Image with GNU Make\nDESCRIPTION: Commands to create and run a Docker development image for TensorRT-LLM using GNU make.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/installation/build-from-source-linux.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker build\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker run\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker run LOCAL_USER=1\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT-LLM\nDESCRIPTION: Specifies required Python packages and their versions for the TensorRT-LLM project. Includes core TensorRT-LLM package, dataset handling libraries, evaluation metrics, and text processing tools.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/chatglm2-6b/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nprotobuf\nrouge_score\nsentencepiece\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Pushing TensorRT-LLM Image to Repository\nDESCRIPTION: Command to push TensorRT-LLM images to the internal artifact repository.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker trtllm_push\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT-LLM Runtime Library Object in CMake\nDESCRIPTION: This snippet creates the runtime_src library object, sets its properties for position-independent code and CUDA symbol resolution, and links necessary libraries for multi-device support.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/runtime/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(runtime_src OBJECT ${SRCS})\nset_property(TARGET runtime_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET runtime_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n\ntarget_include_directories(runtime_src PRIVATE ${MPI_C_INCLUDE_DIRS})\n\nif(ENABLE_MULTI_DEVICE)\n  target_link_libraries(runtime_src PUBLIC ${NCCL_LIB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-7B-Chat (2-way Tensor Parallelism)\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM checkpoints and build a 2-GPU engine with tensor parallelism for Qwen-7B-Chat.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                            --output_dir ./tllm_checkpoint_2gpu_tp2 \\\n                            --dtype float16 \\\n                            --tp_size 2\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_2gpu_tp2 \\\n            --output_dir ./tmp/qwen/7B/trt_engines/fp16/2-gpu/ \\\n            --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookahead Pool Manager Tests in CMake\nDESCRIPTION: Sets up tests for the lookahead pool manager functionality, using randomLlm and lookaheadPoolManager test source files.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/layers/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(LOOKAHEAD_POOLMANAGER_TEST_SRC randomLlm.cpp lookaheadPoolManagerTest.cpp)\nadd_gtest(lookaheadPoolManagerTest \"${LOOKAHEAD_POOLMANAGER_TEST_SRC}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all .cpp and .cu files in the current directory and adds them to the PLUGIN_SOURCES variable. The updated PLUGIN_SOURCES is then propagated to the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp *.cu)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Running Model Evaluation with LM Evaluation Harness\nDESCRIPTION: Command to evaluate models using lm-evaluation-harness with AutoDeploy, specifying model arguments and evaluation tasks.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/auto_deploy\npython lm_eval_ad.py \\\n--model autodeploy --model_args model=meta-llama/Meta-Llama-3.1-8B-Instruct,world_size=2 --tasks mmlu\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for NVIDIA TensorRT-LLM\nDESCRIPTION: This code snippet lists the required Python packages and their versions for the NVIDIA TensorRT-LLM project. It includes specifications for CUDA-related packages, TensorRT-LLM, and various NLP libraries. Some version constraints are used to ensure compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/gemma/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\n-f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n-c ../constraints.txt\n# WAR the new posting of \"nvidia-cudnn-cu12~=9.0\".\n# \"jax[cuda12_pip]~=0.4.19\" specifies \"nvidia-cudnn-cu12>=8.9\" but actually requires \"nvidia-cudnn-cu12~=8.9\".\nnvidia-cudnn-cu12~=8.9; platform_machine == \"x86_64\"\ntensorrt_llm>=0.0.0.dev0\nflax~=0.8.0\n# jax[cuda12_pip]~=0.4.19\nsafetensors~=0.4.1\nsentencepiece>=0.1.99\nh5py~=3.12.1\nrouge_score\nnltk\ndatasets==2.14.6\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their versions for the TensorRT-LLM project. It includes the core TensorRT-LLM package, dataset handling libraries, evaluation tools, and text processing utilities.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/chatglm3-6b-32k/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nprotobuf\nrouge_score\nsentencepiece\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Building Release with Specific CUDA Architectures\nDESCRIPTION: Command to build release Docker image with support for specific CUDA architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker release_build CUDA_ARCHS=\"80-real;90-real\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Warning Flags for GCC and MSVC\nDESCRIPTION: Sets compiler warning flags differently for Unix and Windows platforms. For Unix systems, enables -Wall and optionally -Werror. For Windows, sets warning level 4.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/thop/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT WIN32)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n  if(WARNING_IS_ERROR)\n    message(STATUS \"Treating warnings as errors in GCC compilation\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n  endif()\nelse()\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: INT4-AWQ Groupwise Quantization for Falcon-180B\nDESCRIPTION: Series of commands to quantize Falcon-180B using INT4-AWQ (Activation-aware Weight Quantization) with NVIDIA Modelopt, build engines, and run inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/falcon/README.md#2025-04-07_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Quantize HF Falcon 180B checkpoint into INT4-AWQ and export trtllm checkpoint\npython ../../../quantization/quantize.py --model_dir ./falcon/180b \\\n                --dtype float16 \\\n                --qformat int4_awq \\\n                --output_dir ./falcon/180b/trt_ckpt/int4_awq/tp2 \\\n                --tp_size 2\n\n# Build trtllm engines from the trtllm checkpoint\ntrtllm-build --checkpoint_dir ./falcon/180b/trt_ckpt/int4_awq/tp2 \\\n                --gemm_plugin float16 \\\n                --output_dir ./falcon/180b/trt_engines/int4_awq/tp2 \\\n                --workers 2\n\n# Run the summarization task\nmpirun -n 2 --allow-run-as-root --oversubscribe \\\n    python ../../../summarize.py --test_trt_llm \\\n                --hf_model_dir ./falcon/180b \\\n                --engine_dir ./falcon/180b/trt_engines/int4_awq/tp2\n```\n\n----------------------------------------\n\nTITLE: Adding Specific Benchmark Executables in CMake\nDESCRIPTION: Adds four specific benchmark executables to the project: gptSessionBenchmark, bertBenchmark, gptManagerBenchmark, and disaggServerBenchmark.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-07_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_benchmark(gptSessionBenchmark gptSessionBenchmark.cpp)\nadd_benchmark(bertBenchmark bertBenchmark.cpp)\nadd_benchmark(gptManagerBenchmark gptManagerBenchmark.cpp)\nadd_benchmark(disaggServerBenchmark disaggServerBenchmark.cpp)\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all CPP files in the current directory and adds them to the PLUGIN_SOURCES variable. The updated PLUGIN_SOURCES is then propagated to the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/gptAttentionPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Running Tests with PyTest\nDESCRIPTION: Command for running the AutoDeploy test suite using pytest.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npytest tests/_torch/autodeploy\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorRT-LLM Project\nDESCRIPTION: A requirements file listing Python package dependencies for the TensorRT-LLM project. The file includes references to an external constraints file and specifies required packages like TensorRT-LLM, datasets, evaluate, rouge_score, and sentencepiece with version constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/skywork/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.16.1\nevaluate\nrouge_score\nsentencepiece>=0.1.99\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This code snippet lists the required Python packages and their version constraints for the TensorRT-LLM project. It includes TensorRT-LLM itself, dataset handling libraries, evaluation tools, and specific NLP-related packages.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/baichuan/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.15.0\nevaluate\nrouge_score\nsentencepiece>=0.1.99\ncpm-kernels~=1.0.11\ntransformers_stream_generator~=0.0.4\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container with Local User\nDESCRIPTION: Command to start a container with local user permissions instead of root.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker devel_run LOCAL_USER=1\n```\n\n----------------------------------------\n\nTITLE: Building th_common Shared Library\nDESCRIPTION: Builds the main shared library containing various CUDA operations for tensor processing and deep learning. Includes position-independent code and links against PyTorch, th_utils, and custom shared targets.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/thop/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(\n  th_common SHARED\n  allgatherOp.cpp\n  allreduceOp.cpp\n  attentionOp.cpp\n  convertSpecDecodingMaskToPackedMaskOp.cpp\n  cutlassScaledMM.cpp\n  cublasScaledMM.cpp\n  deepseekAllreduceFusionOp.cpp\n  dynamicDecodeOp.cpp\n  fmhaPackMaskOp.cpp\n  fp8Op.cpp\n  fp4Op.cpp\n  fp4Gemm.cpp\n  fp4Quantize.cpp\n  fp4BatchedQuantize.cpp\n  fp8BlockScalingGemm.cpp\n  fp8Quantize.cpp\n  gatherTreeOp.cpp\n  logitsBitmaskOp.cpp\n  mambaConv1dOp.cpp\n  moeOp.cpp\n  fp8BlockScaleMoe.cpp\n  noAuxTcOp.cpp\n  ncclCommunicatorOp.cpp\n  parallelDecodeKVCacheUpdateOp.cpp\n  redrafterCurandOp.cpp\n  reducescatterOp.cpp\n  relativeAttentionBiasOp.cpp\n  selectiveScanOp.cpp\n  userbuffersFinalizeOp.cpp\n  weightOnlyQuantOp.cpp\n  mtpOp.cpp)\nset_property(TARGET th_common PROPERTY POSITION_INDEPENDENT_CODE ON)\ntarget_link_libraries(th_common PRIVATE ${TORCH_LIBRARIES} th_utils\n                                        ${Python3_LIBRARIES} ${SHARED_TARGET})\n```\n\n----------------------------------------\n\nTITLE: Downloading Qwen Model Weights from HuggingFace\nDESCRIPTION: Git commands to clone Qwen model repositories from HuggingFace, including 7B, 14B, and 72B variants.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/Qwen/Qwen-7B-Chat   ./tmp/Qwen/7B\ngit clone https://huggingface.co/Qwen/Qwen-14B-Chat  ./tmp/Qwen/14B\ngit clone https://huggingface.co/Qwen/Qwen-72B-Chat  ./tmp/Qwen/72B\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet glob all .cpp files in the current directory, adds them to the PLUGIN_SOURCES variable, and then sets this variable in the parent scope. This is typically used to collect all plugin source files for compilation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/cpSplitPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake\nDESCRIPTION: Collects all CPP source files in the current directory and adds them to the plugin sources list. The configuration uses CMake's file globbing and variable scope management to build the complete list of plugin sources.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/mambaConv1dPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install necessary packages for running InternLM with TensorRT-LLM\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for NVIDIA TensorRT-LLM\nDESCRIPTION: This code snippet lists the required Python packages and their versions for the NVIDIA TensorRT-LLM project. It includes TensorRT-LLM, datasets, evaluation libraries, JAX with CUDA support, and other dependencies. Some packages have specific version requirements.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/grok/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets==2.14.6\nevaluate\nrouge_score\nsentencepiece==0.2.0\njax[cuda12-pip]==0.4.29\njaxlib[cuda12-pip]==0.4.29\ndm_haiku==0.0.12\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their version constraints for the NVIDIA TensorRT-LLM project. It includes packages for TensorRT-LLM itself, dataset handling, evaluation, text processing, and tensor operations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/phi/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\neinops~=0.7.0\ntiktoken==0.6.0\n```\n\n----------------------------------------\n\nTITLE: Custom Docker Image Build and Push\nDESCRIPTION: Command to build and push Docker image with custom image name and tag to a specific registry.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker devel_push IMAGE_WITH_TAG=\"urm.nvidia.com/sw-tensorrt-docker/tensorrt-llm:dev\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookahead Algorithm Tests in CMake\nDESCRIPTION: Defines test configuration for lookahead algorithm, combining random language model implementation with algorithm-specific test code.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/layers/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(LOOKAHEAD_ALGORITHM_TEST_SRC randomLlm.cpp lookaheadAlgorithmTest.cpp)\nadd_gtest(lookaheadAlgorithmTest \"${LOOKAHEAD_ALGORITHM_TEST_SRC}\")\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Qwen-7B-Chat (INT8 Weight-Only Quantization)\nDESCRIPTION: Commands to convert HuggingFace weights to TensorRT-LLM checkpoints and build a single-GPU INT8 weight-only quantized engine for Qwen-7B-Chat.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwen/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython convert_checkpoint.py --model_dir ./tmp/Qwen/7B/ \\\n                              --output_dir ./tllm_checkpoint_1gpu_fp16_wq \\\n                              --dtype float16 \\\n                              --use_weight_only \\\n                              --weight_only_precision int8\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_fp16_wq \\\n            --output_dir ./tmp/qwen/7B/trt_engines/weight_only/1-gpu/ \\\n            --gemm_plugin float16\n```\n\n----------------------------------------\n\nTITLE: Problematic Module Unload Implementation in C++\nDESCRIPTION: Original problematic implementation that attempts to unload CUDA module without checking its validity, potentially causing invalid handle errors.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/README.md#2025-04-07_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\nvoid unload_fmha_d64_fp32_f30323ef_0d1d2d3d4d5d6789(void) {\n    CUDA_CHECK(cuModuleUnload(fmha_d64_fp32_f30323ef_0d1d2d3d4d5d6789_mod));\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring UserBuffers Library Build\nDESCRIPTION: Configures the compilation of the UserBuffers library based on multi-device support flag. For single device, only includes ub_interface.cpp, while for multi-device includes all .cpp and .cu files recursively. Sets CUDA compilation options and position-independent code properties.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/userbuffers/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_MULTI_DEVICE EQUAL 0)\n  add_library(userbuffers_src OBJECT ub_interface.cpp)\nelse()\n  file(GLOB_RECURSE SRC_CPP *.cpp)\n  file(GLOB_RECURSE SRC_CU *.cu)\n  add_library(userbuffers_src OBJECT ${SRC_CPP} ${SRC_CU})\nendif()\n\ntarget_compile_options(userbuffers_src\n                       PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-maxrregcount=64>)\n\nset_property(TARGET userbuffers_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET userbuffers_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n\nmessage(STATUS \"UserBuffer ENABLE_MULTI_DEVICE is ${ENABLE_MULTI_DEVICE}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for MPT\nDESCRIPTION: Install the necessary Python packages for working with MPT models\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Target Properties for GPU Architecture Support\nDESCRIPTION: Defines a function to process each target with appropriate compile definitions and options based on GPU architecture support. It handles Hopper (SM90) and Blackwell (SM100/SM120) with their architecture-specific optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(process_target target_name enable_hopper enable_blackwell)\n  set_property(TARGET ${target_name} PROPERTY POSITION_INDEPENDENT_CODE ON)\n  set_property(TARGET ${target_name} PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n\n  if(${enable_hopper} AND \"90\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG)\n    # No kernels should be parsed, unless hopper is specified. This is a build\n    # time improvement\n    target_compile_definitions(${target_name} PRIVATE COMPILE_HOPPER_TMA_GEMMS)\n    target_compile_definitions(${target_name}\n                               PRIVATE COMPILE_HOPPER_TMA_GROUPED_GEMMS)\n  endif()\n\n  if(${enable_blackwell} AND (\"100\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG\n                              OR \"120\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG))\n    target_compile_options(\n      ${target_name} PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcicc --uumn -Xptxas\n                             -uumn>)\n\n    if(\"100\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG)\n      # No kernels should be parsed, unless blackwell is specified. This is a\n      # build time improvement\n      target_compile_definitions(${target_name}\n                                 PRIVATE COMPILE_BLACKWELL_TMA_GEMMS)\n      target_compile_definitions(${target_name}\n                                 PRIVATE COMPILE_BLACKWELL_TMA_GROUPED_GEMMS)\n    endif()\n  endif()\n\n  # Suppress GCC note: the ABI for passing parameters with 64-byte alignment has\n  # changed in GCC 4.6 This note appears for kernels using TMA and clutters the\n  # compilation output.\n  if(NOT WIN32)\n    target_compile_options(\n      ${target_name} PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcompiler=-Wno-psabi>)\n  endif()\n\n  target_compile_options(${target_name}\n                         PRIVATE \"-DENABLE_MULTI_DEVICE=${ENABLE_MULTI_DEVICE}\")\n  if(ENABLE_MULTI_DEVICE)\n    target_link_libraries(${target_name} PRIVATE ${MPI_C_LIBRARIES})\n  endif()\n\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet defines the Python package dependencies required for the TensorRT-LLM project. It references an external constraints file and specifies version requirements for key packages like tensorrt_llm, transformers, datasets, and evaluation utilities.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mamba/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ntransformers>=4.39.0\ndatasets~=2.14.5\nevaluate\nrouge_score\nsentencepiece\n```\n\n----------------------------------------\n\nTITLE: Creating Context Attention Library Target in CMake\nDESCRIPTION: Defines an object library named 'context_attention_src' using the collected source files and sets properties for position-independent code and CUDA device symbol resolution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(context_attention_src OBJECT ${SRC_CPP} ${SRC_CU})\nset_property(TARGET context_attention_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET context_attention_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS\n                                                   ON)\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all C++ source files in the current directory using the GLOB command and adds them to the PLUGIN_SOURCES variable. The updated variable is then propagated to the parent scope to be used in the build process.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/identityPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Tensor Loading Function Template\nDESCRIPTION: Template for loading tensor slices based on key, tensor parallel size, dimension and rank.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/model-weights-loader.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef load_tensor(self, key, tp_size, tp_dim, tp_rank):\n    # Retrieve file pointer index\n    if key in self.shard_map:\n        ptr_idx = self.shard_map[key]\n    else:\n        return None\n\n    # Load tensor from the corresponding shard\n    if self.format == ModelWeightsFormat.SAFETENSORS:\n        tensor = self.shards[ptr_idx].get_slice(key)\n        tensor_shape = tensor.get_shape()\n    else:\n        ...\n\n    # Shard and return a tensor slice\n    slice_shape = ...\n    return tensor[slice_shape]\n```\n\n----------------------------------------\n\nTITLE: Cloning and Pushing to a Forked Repository\nDESCRIPTION: Git commands for cloning a forked TensorRT-LLM repository, making changes, and pushing them to a remote branch on the fork.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CONTRIBUTING.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR_USERNAME/YOUR_FORK.git TensorRT-LLM\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin <local-branch>:<remote-branch>\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files for TensorRT-LLM Common Module in CMake\nDESCRIPTION: Uses CMake's file(GLOB) command to collect all C++ and CUDA source files in the current directory for compilation into the common library.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/common/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nfile(GLOB CU_SRCS *.cu)\n```\n\n----------------------------------------\n\nTITLE: Starting the TensorRT-LLM FastAPI Server with Custom Tokenizer\nDESCRIPTION: Command to start the FastAPI server with a TensorRT-LLM engine built with trtllm-build, specifying a custom tokenizer directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/apps/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ./fastapi_server.py <model_dir> --tokenizer <tokenizer_dir>&\n```\n\n----------------------------------------\n\nTITLE: Applying Target Configuration for GEMM Variants\nDESCRIPTION: Applies the target processing function to each GEMM variant library target with appropriate Hopper and Blackwell support flags. Different GEMM variants require different architecture support.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(\n  target_name\n  ar_gemm_src;fpA_intB_gemm_src;fb_gemm_src;fp8_blockscale_gemm_src;gemm_swiglu_sm90_src\n)\n  process_target(${target_name} true false) # Name, Hopper, Blackwell\nendforeach()\n\nforeach(target_name fpA_intB_gemm_src;fb_gemm_src)\n  process_target(${target_name} false true) # Name, Hopper, Blackwell\nendforeach()\n\n# foreach(target_name moe_gemm_src) process_target(${target_name} true true) #\n# Name, Hopper, Blackwell endforeach()\n```\n\n----------------------------------------\n\nTITLE: Integrating GoogleTest with CMake for TensorRT-LLM\nDESCRIPTION: Sets up GoogleTest using CMake's FetchContent to download and configure the testing framework. This snippet fetches GoogleTest from its repository and makes it available for the project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(FetchContent)\nFetchContent_Declare(\n  googletest\n  GIT_REPOSITORY https://github.com/google/googletest.git\n  GIT_TAG v1.15.2)\nFetchContent_MakeAvailable(googletest)\ninclude(GoogleTest)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for TensorRT-LLM Components in CMake\nDESCRIPTION: This snippet adds three subdirectories to the CMake build process: 'fmha', 'blockscaleGemm', and 'fp8BlockScaleMoe'. These likely represent different components or modules of the TensorRT-LLM project that need to be built.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/trtllmGenKernels/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(fmha)\nadd_subdirectory(blockscaleGemm)\nadd_subdirectory(fp8BlockScaleMoe)\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for Unix-like Systems in CMake\nDESCRIPTION: Configures compiler flags for Unix-like systems, enabling additional warnings and optionally treating warnings as errors. This helps maintain code quality and catch potential issues early.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/layers/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT WIN32)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n  if(WARNING_IS_ERROR)\n    message(STATUS \"Treating warnings as errors in GCC compilation\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n  endif()\nelse() # Windows\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: AWQ Quantization for Mixtral\nDESCRIPTION: Commands to convert AutoAWQ Hugging Face checkpoints into TensorRT-LLM checkpoints and build engines for Mixtral with AWQ quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython ../llama/convert_checkpoint.py --model_dir ./tmp/mixtral-8x7b-v0.1-AWQ/ \\\n                                      --output_dir ./tllm_checkpoint_mixtral_awq_1gpu\n\ntrtllm-build --checkpoint_dir ./tllm_checkpoint_mixtral_awq_1gpu \\\n             --output_dir ./engine_outputs\n```\n\n----------------------------------------\n\nTITLE: Setting MPI Environment Variables\nDESCRIPTION: Configuration of environment variables for OpenMPI and CUDA support.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/CONTRIBUTING.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\nexport OMPI_MCA_opal_cuda_support=true\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Common Library Target in CMake\nDESCRIPTION: Creates an object library named 'common_src' from the collected source files and sets position-independent code and CUDA symbol resolution properties for the target.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/common/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(common_src OBJECT ${SRCS} ${CU_SRCS})\nset_property(TARGET common_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET common_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n```\n\n----------------------------------------\n\nTITLE: Formatting C++ Code with clang-format in Git\nDESCRIPTION: Command to format C++ code changes using clang-format according to TensorRT-LLM's style guidelines. It applies formatting to staged changes or a specific commit.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CONTRIBUTING.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit-clang-format --style file [commit ID/reference]\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: Specifies the exact version requirement for the transformers package needed to run TensorRT-LLM. Pins the transformers library to version 4.45.2 for compatibility and stability.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/multimodal/requirements-internlm-xcomposer2.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\ntransformers==4.45.2\n```\n\n----------------------------------------\n\nTITLE: Configuring GTest Test Cases for TensorRT-LLM\nDESCRIPTION: Adds multiple GTest test cases for various TensorRT-LLM components using CMake's add_gtest command. Tests cover core functionality including buffer management, CUDA operations, tensor handling, and runtime features.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/unit_tests/runtime/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(bufferManagerTest bufferManagerTest.cpp)\nadd_gtest(cudaMemPoolTest cudaMemPoolTest.cpp)\nadd_gtest(decodingLayerWorkspaceTest decodingLayerWorkspaceTest.cpp)\nadd_gtest(iBufferTest iBufferTest.cpp)\nadd_gtest(iTensorTest iTensorTest.cpp)\nadd_gtest(loraUtilsTest loraUtilsTest.cpp)\nadd_gtest(runtimeKernelTest runtimeKernelTest.cpp)\nadd_gtest(samplingConfigTest samplingConfigTest.cpp)\nadd_gtest(samplingTest samplingTest.cpp)\nadd_gtest(tllmBuffersTest tllmBuffersTest.cpp)\nadd_gtest(tllmRuntimeTest tllmRuntimeTest.cpp)\nadd_gtest(transposeKVKernelTest transposeKVKernelTest.cpp)\nadd_gtest(userBufferTest userBufferTest.cpp)\nadd_gtest(utilsTest utilsTest.cpp)\nadd_gtest(workerPoolTest workerPoolTest.cpp)\nadd_gtest(worldConfigTest worldConfigTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Loading LLM from Local Hugging Face Model in Python\nDESCRIPTION: Illustrates how to load an LLM instance from a locally stored Hugging Face model. This is useful when working with models that have been previously downloaded.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/llm-api/index.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllm = LLM(model=<path_to_meta_llama_from_hf>)\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all C++ files in the current directory using glob and adds them to the plugin sources list. It then propagates the updated list to the parent scope for use in building TensorRT-LLM plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/loraPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Nemotron-NAS TensorRT Engine\nDESCRIPTION: Demonstrates how to run inference using the built TensorRT engine with the run.py script, including multi-GPU inference.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron_nas/README.md#2025-04-07_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nexport MODEL_DIR=\"~/models/huggingface/nemotron-nas\"\nexport TRT_ENGINE_DIR=\"~/models/engines/nemotron-nas\"\n\npython run.py --engine_dir $TRT_ENGINE_DIR --tokenizer_dir $MODEL_DIR --max_output_len 1024 ...\n\n# for multi-GPU inference (engine must be built with either tp_size>1, pp_size>1, or both)\nexport NUM_GPUS=2\nmpirun -n $NUM_GPUS --allow-run-as-root python run.py ...\n```\n\n----------------------------------------\n\nTITLE: Running Jenkins Docker Container\nDESCRIPTION: Command to run Jenkins Docker image with local user permissions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docker/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker jenkins_run LOCAL_USER=1\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM Plugin Build Settings\nDESCRIPTION: Core CMake configuration for TensorRT-LLM plugin library. Sets target names, build flags, and compiler options for both Debug and Release builds. Includes special handling for Windows vs Linux platforms.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(PLUGIN_TARGET_NAME nvinfer_plugin_tensorrt_llm)\nset(PLUGIN_SHARED_TARGET ${PLUGIN_TARGET_NAME})\n\nset(TARGET_DIR ${CMAKE_CURRENT_SOURCE_DIR})\nset(PLUGIN_EXPORT_MAP ${TARGET_DIR}/exports.map)\nset(PLUGIN_EXPORT_DEF ${TARGET_DIR}/exports.def)\n\nif(${CMAKE_BUILD_TYPE} MATCHES \"Debug\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -g\")\nendif()\n\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --Wno-deprecated-declarations\")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --diag-suppress 997\")\n```\n\n----------------------------------------\n\nTITLE: Running Mixture Of Experts Backend Benchmark\nDESCRIPTION: Commands for executing the mixtureOfExpertsBackendBenchmark tool, either standalone or with a JSON benchmark definition file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/micro_benchmarks/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./mixtureOfExpertsBackendBenchmark\n\n# or\n\n./mixtureOfExpertsBackendBenchmark --input_file <JSON benchmark definition>\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies for TensorRT-LLM\nDESCRIPTION: Lists required Python packages with minimum version requirements. Specifies transformers package version 4.47.1 or higher and diffusers package version 0.32.2 or higher.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mmdit/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\ntransformers>=4.47.1\ndiffusers>=0.32.2\n```\n\n----------------------------------------\n\nTITLE: Adding GTest Executables and Linking Libraries in CMake for TensorRT-LLM Tests\nDESCRIPTION: This snippet adds multiple GTest executables for TensorRT-LLM components and links necessary libraries. It includes tests for executors, encoder-decoder functionality, and disaggregated executors, with some tests requiring additional library dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/executor/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_gtest(executorMockTest executorMockTest.cpp)\nadd_gtest(executorTest executorTest.cpp)\ntarget_link_libraries(executorTest PRIVATE modelSpecStatic testingUtils)\nadd_gtest(encDecTest encDecTest.cpp)\ntarget_link_libraries(encDecTest PRIVATE modelSpecStatic testingUtils)\nadd_gtest(disaggExecutorTest disaggExecutorTest.cpp)\ntarget_link_libraries(disaggExecutorTest PRIVATE modelSpecStatic testingUtils)\n```\n\n----------------------------------------\n\nTITLE: Defining AttentionBackend Forward Method Parameters in Python\nDESCRIPTION: Lists the parameters for the AttentionBackend forward method, which performs the attention operation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/attention.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    metadata: AttentionMetadata,\n    attention_mask: AttentionMask):\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Device Support with NCCL\nDESCRIPTION: This code configures multi-device support by setting up NCCL (NVIDIA Collective Communications Library) dependencies when the ENABLE_MULTI_DEVICE option is enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nset_ifndef(ENABLE_MULTI_DEVICE 1)\nif(ENABLE_MULTI_DEVICE)\n  # NCCL dependencies\n  set_ifndef(NCCL_LIB_DIR /usr/lib/${CMAKE_SYSTEM_PROCESSOR}-linux-gnu/)\n  set_ifndef(NCCL_INCLUDE_DIR /usr/include/)\n  find_library(NCCL_LIB nccl HINTS ${NCCL_LIB_DIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Engine with Build API\nDESCRIPTION: Demonstrates how to build a TensorRT-LLM engine from a LLaMA model using the tensorrt_llm.build API. The process includes creating a build configuration and saving the resulting engine.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/workflow.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nllama = ... # create LLaMAForCausalLM object\nbuild_config = BuildConfig(max_batch_size=1)\nengine = tensorrt_llm.build(llama, build_config)\nengine.save(engine_dir)\n```\n\n----------------------------------------\n\nTITLE: Generating Multi-GPU Test Data for LLaMa\nDESCRIPTION: Command for generating tensor and pipeline parallelism test data for LLaMa using MPI with 4 GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=examples mpirun -n 4 python3 cpp/tests/resources/scripts/generate_expected_llama_output.py --only_multi_gpu\n```\n\n----------------------------------------\n\nTITLE: Shared Library Target Configuration\nDESCRIPTION: Configures the shared library build target with proper linking flags, dependencies, and output settings. Sets C++17 standard and includes CUDA-specific configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${PLUGIN_SHARED_TARGET} SHARED ${PLUGIN_SOURCES})\n\ntarget_include_directories(\n  ${PLUGIN_SHARED_TARGET}\n  PUBLIC ${CUDA_INSTALL_DIR}/include\n  PRIVATE ${TARGET_DIR})\n\nset_target_properties(\n  ${PLUGIN_SHARED_TARGET}\n  PROPERTIES CXX_STANDARD \"17\"\n             CXX_STANDARD_REQUIRED \"YES\"\n             CXX_EXTENSIONS \"NO\"\n             ARCHIVE_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n             LIBRARY_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n             RUNTIME_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataset from CNN/DailyMail for Benchmarking\nDESCRIPTION: Python script to preprocess CNN/DailyMail dataset into a JSON format suitable for benchmarking. Configures tokenization and handles input/output lengths for the benchmark.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 prepare_dataset.py \\\n    --tokenizer <path/to/tokenizer> \\\n    --output cnn_dailymail.json \\\n    dataset \\\n    --dataset-name cnn_dailymail \\\n    --dataset-split validation \\\n    --dataset-config-name 3.0.0 \\\n    --dataset-input-key article \\\n    --dataset-prompt \"Summarize the following article:\" \\\n    --dataset-output-key \"highlights\" \\\n    [--num-requests 100] \\\n    [--max-input-len 1000] \\\n    [--output-len-dist 100,10]\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM Build Options\nDESCRIPTION: Defines the main build options for the TensorRT-LLM project, including PyTorch integration, Python bindings, test building, and various optimization and debugging options.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# Build options\noption(BUILD_PYT \"Build in PyTorch TorchScript class mode\" ON)\noption(BUILD_PYBIND \"Build Python bindings for C++ runtime and batch manager\"\n       ON)\noption(BUILD_TESTS \"Build Google tests\" ON)\noption(BUILD_BENCHMARKS \"Build benchmarks\" ON)\noption(BUILD_MICRO_BENCHMARKS \"Build C++ micro benchmarks\" OFF)\noption(NVTX_DISABLE \"Disable all NVTX features\" ON)\noption(WARNING_IS_ERROR \"Treat all warnings as errors\" OFF)\noption(FAST_BUILD \"Skip compiling some kernels to accelerate compiling\" OFF)\noption(FAST_MATH \"Compiling in fast math mode\" OFF)\noption(INDEX_RANGE_CHECK \"Compiling with index range checks\" OFF)\noption(COMPRESS_FATBIN \"Compress everything in fatbin\" ON)\n\n# Always use static NVRTC for IP protection reasons.\nset(USE_SHARED_NVRTC OFF)\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT-LLM Model Specification Static Library in CMake\nDESCRIPTION: Defines a static library target 'modelSpecStatic' from modelSpec.cpp source file. This static library is used for linking with test targets later in the configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/batch_manager/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(modelSpecStatic STATIC modelSpec.cpp)\n```\n\n----------------------------------------\n\nTITLE: Evaluating TensorRT-LLM Model\nDESCRIPTION: This bash command runs an evaluation on the built TensorRT-LLM model, using MPI for multi-GPU execution and comparing results with a HuggingFace model.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/architecture/checkpoint.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmpirun -n 2 --allow-run-as-root \\\n    python3 ../summarize.py --engine_dir ./opt/125M/trt_engines/fp16/2-gpu/ \\\n                        --batch_size 1 \\\n                        --test_trt_llm \\\n                        --hf_model_dir opt-125m \\\n                        --data_type fp16 \\\n                        --check_accuracy \\\n                        --tensorrt_llm_rouge1_threshold=14\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Dependencies\nDESCRIPTION: This code sets up TensorRT library dependencies with platform-specific handling for Windows where libraries have version-specific naming conventions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\n# TRT dependencies\nset_ifndef(TRT_LIB_DIR ${CMAKE_BINARY_DIR})\nset_ifndef(TRT_INCLUDE_DIR /usr/include/${CMAKE_SYSTEM_PROCESSOR}-linux-gnu)\nset(TRT_LIB nvinfer)\n\n# On Windows major version is appended to nvinfer libs.\nif(WIN32)\n  set(TRT_LIB_NAME nvinfer_10)\nelse()\n  set(TRT_LIB_NAME nvinfer)\nendif()\n\nfind_library_create_target(${TRT_LIB} ${TRT_LIB_NAME} SHARED ${TRT_LIB_DIR})\n```\n\n----------------------------------------\n\nTITLE: Setting CMake and C++ Standards\nDESCRIPTION: Configures basic CMake requirements and enables C++14 standard support.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp_library/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1)\nset(CMAKE_CXX_STANDARD 14)\nset(CMAKE_CXX_STANDARD_REQUIRED TRUE)\n```\n\n----------------------------------------\n\nTITLE: Running C++ End-to-End Test\nDESCRIPTION: Command for executing a C++ end-to-end test after building engines and generating expected outputs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n./$CPP_BUILD_DIR/tests/gptSessionTest\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for TensorRT-LLM Runtime in CMake\nDESCRIPTION: This snippet sets compiler flags for the TensorRT-LLM runtime, including warning levels and error handling. It differentiates between Unix-like systems and Windows.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/runtime/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT WIN32)\n  # additional warnings\n  #\n  # Ignore overloaded-virtual warning. We intentionally change parameters of\n  # some methods in derived class.\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n  if(WARNING_IS_ERROR)\n    message(STATUS \"Treating warnings as errors in GCC compilation\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n  endif()\nelse() # Windows\n  # warning level 4\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Random LoRA Weights\nDESCRIPTION: Python script to generate random LoRA weights for 16 adapters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/cpp/utils/generate_rand_loras.py ${CPP_LORA} ${EG_DIR}/loras 16\n```\n\n----------------------------------------\n\nTITLE: Library Import Configuration\nDESCRIPTION: Sets up library import configurations for batch manager, executor, and CUTLASS kernels with platform-specific paths and error checking\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(BATCH_MANAGER_TARGET tensorrt_llm_batch_manager_static)\nset(EXECUTOR_TARGET tensorrt_llm_executor_static)\nset(INTERNAL_CUTLASS_KERNELS_TARGET\n    tensorrt_llm_internal_cutlass_kernels_static)\n\nadd_library(${BATCH_MANAGER_TARGET} STATIC IMPORTED)\nset_property(TARGET ${BATCH_MANAGER_TARGET} PROPERTY IMPORTED_LOCATION\n                                                       ${BATCH_MANAGER_LIB_LOC})\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT-LLM Cache Transceiver Test in CMake\nDESCRIPTION: Adds a Google Test executable 'cacheTransceiverTest' using the add_gtest custom command. This test checks functionality related to the cache transceiver component.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/batch_manager/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(cacheTransceiverTest cacheTransceiverTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies List\nDESCRIPTION: Lists required Python packages: colorama for terminal color output and openai SDK version 1.57.2 or higher for OpenAI API integration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/apps/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncolorama\nopenai>=1.57.2\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Benchmark Addition Function in CMake\nDESCRIPTION: Creates a function that simplifies adding new benchmarks to the project, handling executable creation, linking with dependencies, and setting compile features and definitions.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/micro_benchmarks/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_benchmark test_name test_src)\n  add_executable(${test_name} ${test_src})\n\n  message(\"Linking with ${SHARED_TARGET}\")\n  target_link_libraries(${test_name} PUBLIC ${SHARED_TARGET}\n                                            benchmark::benchmark)\n\n  target_compile_features(${test_name} PRIVATE cxx_std_17)\n  target_compile_definitions(${test_name}\n                             PUBLIC TOP_LEVEL_DIR=\"${TOP_LEVEL_DIR}\")\n\n  add_dependencies(micro_benchmarks ${test_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running MPI Program Across Two Nodes\nDESCRIPTION: Commands for compiling the MPI hello world program and running it across two nodes with custom SSH port. The output shows messages from 16 processes running across the nodes.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/deepseek_v3/README.md#2025-04-07_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nmpicc -o mpi_hello_world mpi_hello_world.c\nmpirun -np 2 -H <HOST1>:1,<HOST2>:1 -mca plm_rsh_args \"-p 2233\" ./mpi_hello_world\n# Hello world from rank 11 out of 16 processors\n# Hello world from rank 13 out of 16 processors\n# Hello world from rank 12 out of 16 processors\n# Hello world from rank 15 out of 16 processors\n# Hello world from rank 14 out of 16 processors\n# Hello world from rank 10 out of 16 processors\n# Hello world from rank 9 out of 16 processors\n# Hello world from rank 8 out of 16 processors\n# Hello world from rank 5 out of 16 processors\n# Hello world from rank 2 out of 16 processors\n# Hello world from rank 4 out of 16 processors\n# Hello world from rank 6 out of 16 processors\n# Hello world from rank 3 out of 16 processors\n# Hello world from rank 1 out of 16 processors\n# Hello world from rank 7 out of 16 processors\n# Hello world from rank 0 out of 16 processors\n```\n\n----------------------------------------\n\nTITLE: Running All Tests with CTest\nDESCRIPTION: Command for running all tests using CTest and producing an XML report for test results.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./$CPP_BUILD_DIR/ctest --output-on-failure --output-junit \"cpp-test-report.xml\"\n```\n\n----------------------------------------\n\nTITLE: Apache 2.0 License Header for TensorRT-LLM\nDESCRIPTION: Standard license header that specifies copyright ownership by NVIDIA and its affiliates, and the terms of the Apache 2.0 license. This header is used across project files to ensure proper licensing and attribution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION &\n# AFFILIATES. All rights reserved. SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n```\n\n----------------------------------------\n\nTITLE: Using Out-of-Tree Custom Model\nDESCRIPTION: Code snippet demonstrating how to use a custom model defined outside of the TensorRT-LLM codebase without modifying the original framework.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/adding_new_model.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm._torch import LLM\nimport modeling_mymodel\n\ndef main():\n    llm = LLM(...)\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Configuring Symbol Checking for Internal CUTLASS Kernels in CMake\nDESCRIPTION: Sets up custom commands and targets to check symbols in the internal CUTLASS kernels library, with different checks based on the USE_CXX11_ABI flag.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT WIN32)\n  if(USE_CXX11_ABI)\n    add_custom_command(\n      OUTPUT \"${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels\"\n      COMMAND nm -C $<TARGET_FILE:${INTERNAL_CUTLASS_KERNELS_TARGET}> | grep -q\n              'std::__cxx11::'\n      DEPENDS ${INTERNAL_CUTLASS_KERNELS_TARGET})\n  else()\n    add_custom_command(\n      OUTPUT \"${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels\"\n      COMMAND nm -C $<TARGET_FILE:${INTERNAL_CUTLASS_KERNELS_TARGET}> | grep -qv\n              'std::__cxx11::'\n      DEPENDS ${INTERNAL_CUTLASS_KERNELS_TARGET})\n  endif()\n  add_custom_target(\n    check_symbol_internal_cutlass_kernels\n    DEPENDS \"${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels\"\n  )\nelse()\n  add_custom_target(check_symbol_internal_cutlass_kernels)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Additional TensorRT-LLM Tests in CMake\nDESCRIPTION: Sets up additional Google Test executables for testing PEFT cache management, encoder model, and guided decoder functionality. These tests validate various components of the TensorRT-LLM system.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/batch_manager/CMakeLists.txt#2025-04-07_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_gtest(peftCacheManagerTest peftCacheManagerTest.cpp)\nadd_gtest(trtEncoderModelTest trtEncoderModelTest.cpp)\nadd_gtest(guidedDecoderTest guidedDecoderTest.cpp)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their versions for the TensorRT-LLM project. It includes TensorRT-LLM, NeMo Toolkit, Megatron-LM (from a specific GitHub repository), and data processing libraries like datasets, evaluate, and rouge_score.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/nemotron/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\nnemo-toolkit[all]==2.0.0rc1\nmegatron-core @ git+https://github.com/NVIDIA/Megatron-LM@core_r0.8.0\ndatasets~=2.14.5\nevaluate\nrouge_score\n```\n\n----------------------------------------\n\nTITLE: Setting Up CUTLASS Library\nDESCRIPTION: Executes the CUTLASS library setup Python script and checks for successful completion. If setup fails, it reports a fatal error with the failure reason.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nexecute_process(\n  WORKING_DIRECTORY ${3RDPARTY_DIR}/cutlass/python/\n  COMMAND ${Python3_EXECUTABLE} setup_library.py develop --user\n  RESULT_VARIABLE _CUTLASS_LIBRARY_SUCCESS)\n\nif(NOT _CUTLASS_LIBRARY_SUCCESS MATCHES 0)\n  message(\n    FATAL_ERROR\n      \"Failed to set up the CUTLASS library due to ${_CUTLASS_LIBRARY_SUCCESS}.\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Single-GPU FP16 Engine from HuggingFace Weights\nDESCRIPTION: Commands to convert HuggingFace BLOOM-560M checkpoint to TensorRT-LLM format and build a FP16 TensorRT engine for single GPU deployment.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/bloom/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Single GPU on BLOOM 560M\npython convert_checkpoint.py --model_dir ./bloom/560M/ \\\n                --dtype float16 \\\n                --output_dir ./bloom/560M/trt_ckpt/fp16/1-gpu/\ntrtllm-build --checkpoint_dir ./bloom/560M/trt_ckpt/fp16/1-gpu/ \\\n                --gemm_plugin float16 \\\n                --output_dir ./bloom/560M/trt_engines/fp16/1-gpu/\n```\n\n----------------------------------------\n\nTITLE: Documenting TensorRT-LLM Build Command Options in reStructuredText\nDESCRIPTION: This snippet uses the argparse directive in reStructuredText to automatically generate documentation for the build command from its argument parser. It references the parse_arguments function in the tensorrt_llm.commands.build module.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/commands/trtllm-build.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. argparse::\n   :module: tensorrt_llm.commands.build\n   :func: parse_arguments\n   :prog: trtllm-build\n```\n\n----------------------------------------\n\nTITLE: Target and Include Directory Setup\nDESCRIPTION: Configures the main target name and include directories for the TensorRT-LLM project\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME tensorrt_llm)\nset(SHARED_TARGET ${TARGET_NAME})\nset(SHARED_TARGET\n    ${SHARED_TARGET}\n    PARENT_SCOPE)\nset(API_INCLUDE_DIR ${PROJECT_SOURCE_DIR}/include)\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/cutlass_extensions/include\n                    ${API_INCLUDE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Triton Server Launch Command with Fast Logits\nDESCRIPTION: Command to start Triton server in orchestrator mode with fast logits transfer enabled\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 scripts/launch_triton_server.py \\\n    --model_repo=$TRITON_REPO \\\n    --tensorrt_llm_model_name \"tensorrt_llm,tensorrt_llm_draft\" \\\n    --multi-model \\\n    --disable-spawn-processes \\\n    --world_size=3 --log &\n```\n\n----------------------------------------\n\nTITLE: Configuring Symbol Checking for Executor in CMake\nDESCRIPTION: Sets up custom commands and targets to check symbols in the executor library, with different checks based on the USE_CXX11_ABI flag.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT WIN32)\n  if(USE_CXX11_ABI)\n    add_custom_command(\n      OUTPUT \"${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_executor\"\n      COMMAND nm -C $<TARGET_FILE:${EXECUTOR_TARGET}> | grep -q 'std::__cxx11::'\n      DEPENDS ${EXECUTOR_TARGET})\n  else()\n    add_custom_command(\n      OUTPUT \"${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_executor\"\n      COMMAND nm -C $<TARGET_FILE:${EXECUTOR_TARGET}> | grep -qv\n              'std::__cxx11::'\n      DEPENDS ${EXECUTOR_TARGET})\n  endif()\n  add_custom_target(\n    check_symbol_executor\n    DEPENDS \"${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_executor\")\nelse()\n  add_custom_target(check_symbol_executor)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Layers Library Target in CMake\nDESCRIPTION: Defines an object library target for the layers component, setting it as position-independent code and enabling CUDA device symbol resolution. This prepares the layers library for integration into the larger TensorRT-LLM project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/layers/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(layers_src OBJECT ${SRCS} ${CU_SRCS})\nset_property(TARGET layers_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET layers_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: Lists required Python packages with version specifications including direct Git repository installations. Contains dependencies for TensorRT-LLM functionality including ftfy, xformers, rotary embeddings, and other ML-related packages.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/stdit/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nftfy\nav==12.3.0\ngit+https://github.com/facebookresearch/xformers.git@v0.0.29#egg=xformers\nrotary_embedding_torch==0.5.3\ngit+https://github.com/hpcaitech/TensorNVMe.git\n# For colossalai\nfabric\ncontexttimer\nray\nprotobuf\nbitsandbytes>=0.39.0\nrpyc==6.0.0\ngalore_torch\n```\n\n----------------------------------------\n\nTITLE: Collecting Remaining Source Files\nDESCRIPTION: Gathers all remaining source files and filters out the specialized GEMM variants that are handled separately. This ensures no source files are included in multiple targets.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# Get the sources for all the remaining sources\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\nset(ALL_SRCS ${SRC_CPP};${SRC_CU})\nlist(FILTER ALL_SRCS EXCLUDE REGEX \"fpA_intB_gemm/.*\")\nlist(FILTER ALL_SRCS EXCLUDE REGEX \"moe_gemm/.*\")\nlist(FILTER ALL_SRCS EXCLUDE REGEX \"fp8_rowwise_gemm/.*\")\nlist(FILTER ALL_SRCS EXCLUDE REGEX \"fp8_blockscale_gemm/.*\")\nlist(FILTER ALL_SRCS EXCLUDE REGEX \"fp4_gemm/.*\")\nlist(FILTER ALL_SRCS EXCLUDE REGEX \"allreduce_gemm/.*\")\nlist(REMOVE_ITEM ALL_SRCS\n     \"${CMAKE_CURRENT_SOURCE_DIR}/fused_gated_gemm/gemm_swiglu_e4m3.cu\")\n```\n\n----------------------------------------\n\nTITLE: Markdown Header for TensorRT-LLM Serving Documentation\nDESCRIPTION: Markdown header and introductory text explaining the purpose of trtllm-serve command and its FastAPI server implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/serve/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Online Serving Examples with `trtllm-serve`\n\nWe provide a CLI command, `trtllm-serve`, to launch a FastAPI server compatible with OpenAI APIs, here are some client examples to query the server, you can check the source code here or refer to the [command documentation](https://nvidia.github.io/TensorRT-LLM/commands/trtllm-serve.html) and [examples](https://nvidia.github.io/TensorRT-LLM/examples/trtllm_serve_examples.html) for detailed information and usage guidelines.\n```\n\n----------------------------------------\n\nTITLE: Configuring Selective Scan Library Build in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet configures the build process for the selective scan module. It collects source files, creates a library target, and sets various properties and compiler options based on CUDA architecture and platform-specific requirements.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/selectiveScan/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\n\nadd_library(selective_scan_src OBJECT ${SRC_CPP} ${SRC_CU})\nforeach(target_name selective_scan_src)\n  set_property(TARGET ${target_name} PROPERTY POSITION_INDEPENDENT_CODE ON)\n  set_property(TARGET ${target_name} PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n\n  if(\"90\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG)\n    # Silence ptxas warning about register usage. The chosen configuration is\n    # profiled to be the most performant, despite the warnings.\n    target_compile_options(${target_name}\n                           PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xptxas=-w>)\n\n    # No kernels should be parsed, unless hopper is specified. This is a build\n    # time improvement\n    target_compile_definitions(${target_name} PRIVATE COMPILE_HOPPER_TMA_GEMMS)\n    target_compile_definitions(${target_name}\n                               PRIVATE COMPILE_HOPPER_TMA_GROUPED_GEMMS)\n  endif()\n\n  # Suppress GCC note: the ABI for passing parameters with 64-byte alignment has\n  # changed in GCC 4.6 This note appears for kernels using TMA and clutters the\n  # compilation output.\n  if(NOT WIN32)\n    target_compile_options(\n      ${target_name} PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcompiler=-Wno-psabi>)\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Triton Server Shutdown Command\nDESCRIPTION: Commands to terminate Triton server processes\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/speculative-decoding.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npkill -9 -f trtllmExecutorWorker\npkill -9 -f tritonserver\n```\n\n----------------------------------------\n\nTITLE: Configuring NVRTC Wrapper Library in CMake\nDESCRIPTION: Sets up the NVRTC wrapper library target, handling both build and import scenarios. It configures platform-specific settings and performs file size checks to ensure correct library setup.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/CMakeLists.txt#2025-04-07_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nset(NVRTC_WRAPPER_TARGET tensorrt_llm_nvrtc_wrapper)\nset(NVRTC_WRAPPER_TARGET_ARCH ${TARGET_ARCH})\n\nif(BUILD_NVRTC_WRAPPER)\n  add_subdirectory(\n    kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/nvrtcWrapper)\nelse()\n  add_library(${NVRTC_WRAPPER_TARGET} SHARED IMPORTED)\n  if(NOT WIN32) # Linux\n    set(NVRTC_WRAPPER_LIB_SOURCE_REL_LOC\n        \"kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/nvrtcWrapper/${NVRTC_WRAPPER_TARGET_ARCH}/libtensorrt_llm_nvrtc_wrapper.so\"\n    )\n    set(NVRTC_WRAPPER_LIB_BINARY_REL_LOC\n        \"kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/nvrtcWrapper/libtensorrt_llm_nvrtc_wrapper.so\"\n    )\n  else()\n    set(NVRTC_WRAPPER_LIB_BINARY_REL_DIR\n        \"kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/nvrtcWrapper\"\n    )\n    set(NVRTC_WRAPPER_DLL_NAME \"tensorrt_llm_nvrtc_wrapper.dll\")\n    set(NVRTC_WRAPPER_LIB_NAME \"tensorrt_llm_nvrtc_wrapper.lib\")\n\n    set(NVRTC_WRAPPER_LIB_SOURCE_REL_LOC\n        \"${NVRTC_WRAPPER_LIB_BINARY_REL_DIR}/${NVRTC_WRAPPER_TARGET_ARCH}/${NVRTC_WRAPPER_DLL_NAME}\"\n    )\n    set(NVRTC_WRAPPER_LIB_BINARY_REL_LOC\n        \"${NVRTC_WRAPPER_LIB_BINARY_REL_DIR}/${NVRTC_WRAPPER_DLL_NAME}\")\n    set(NVRTC_WRAPPER_IMPLIB_SOURCE_REL_LOC\n        \"${NVRTC_WRAPPER_LIB_BINARY_REL_DIR}/${NVRTC_WRAPPER_TARGET_ARCH}/${NVRTC_WRAPPER_LIB_NAME}\"\n    )\n    set(NVRTC_WRAPPER_IMPLIB_BINARY_REL_LOC\n        \"${NVRTC_WRAPPER_LIB_BINARY_REL_DIR}/${NVRTC_WRAPPER_LIB_NAME}\")\n  endif()\n  set(NVRTC_WRAPPER_LIB_LOC\n      \"${CMAKE_CURRENT_SOURCE_DIR}/${NVRTC_WRAPPER_LIB_SOURCE_REL_LOC}\")\n  # Copy the .so to build directory, which is needed in build_wheel.py.\n  configure_file(${NVRTC_WRAPPER_LIB_SOURCE_REL_LOC}\n                 ${NVRTC_WRAPPER_LIB_BINARY_REL_LOC} COPYONLY)\n  set_property(TARGET ${NVRTC_WRAPPER_TARGET} PROPERTY IMPORTED_LOCATION\n                                                       ${NVRTC_WRAPPER_LIB_LOC})\n  if(WIN32)\n    set(NVRTC_WRAPPER_IMPLIB_LOC\n        \"${CMAKE_CURRENT_SOURCE_DIR}/${NVRTC_WRAPPER_IMPLIB_SOURCE_REL_LOC}\")\n    configure_file(${NVRTC_WRAPPER_IMPLIB_SOURCE_REL_LOC}\n                   ${NVRTC_WRAPPER_IMPLIB_BINARY_REL_LOC} COPYONLY)\n    set_property(TARGET ${NVRTC_WRAPPER_TARGET}\n                 PROPERTY IMPORTED_IMPLIB ${NVRTC_WRAPPER_IMPLIB_LOC})\n  endif()\n\n  file(SIZE ${NVRTC_WRAPPER_LIB_LOC} NVRTC_WRAPPER_LIB_SIZE)\n  if(NVRTC_WRAPPER_LIB_SIZE LESS 1024)\n    message(\n      FATAL_ERROR\n        \"The nvrtc wrapper library is truncated or incomplete. This is usually caused by using Git LFS (Large File Storage) incorrectly. Please try running command `git lfs install && git lfs pull`.\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring trtllm_gen_fmha Library in CMake for NVIDIA TensorRT-LLM\nDESCRIPTION: This CMake snippet configures the trtllm_gen_fmha library for the NVIDIA TensorRT-LLM project. It glob's C++ and CUDA source files, filters CUDA architectures, and sets up the library with position-independent code and device symbol resolution.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\n\nfilter_cuda_archs(\"100\" SRC_CPP)\n\nadd_library(trtllm_gen_fmha OBJECT ${SRC_CPP} ${SRC_CU})\nset_property(TARGET trtllm_gen_fmha PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET trtllm_gen_fmha PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for NVIDIA TensorRT-LLM\nDESCRIPTION: This snippet lists the required Python packages and their version constraints for the NVIDIA TensorRT-LLM project. It includes TensorRT-LLM with a development version, datasets with a specific version range, and evaluate and rouge_score packages without version specifications.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/mpt/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM FP8 Block Scale MoE Library Build with CMake\nDESCRIPTION: This CMake snippet gathers all C++ and CUDA source files in the directory, creates an object library target, and sets properties for position-independent code and CUDA device symbol resolution. It's part of the build configuration for a TensorRT-LLM component dealing with FP8 block scale MoE functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\n\nadd_library(trtllm_gen_fp8_block_scale_moe OBJECT ${SRC_CPP} ${SRC_CU})\nset_property(TARGET trtllm_gen_fp8_block_scale_moe\n             PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET trtllm_gen_fp8_block_scale_moe\n             PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n```\n\n----------------------------------------\n\nTITLE: Statistical Formulas for Two-Sample T-Test in LaTeX\nDESCRIPTION: Mathematical formulas expressing the false positive rate (alpha), threshold calculation (gamma), false negative rate (beta), and minimum detectable effect (theta) using LaTeX notation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/integration/defs/accuracy/README.md#2025-04-07_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{equation*}\n\\begin{aligned}\n\\alpha &= P \\left(\\bar{x'} \\leq \\gamma \\mid t \\sim \\mathcal{N} (0, 1) \\right) \\\\\n&= P \\left(t \\leq \\frac{\\gamma - \\bar{x}}{\\sqrt{2 \\sigma^2 / n}} \\mid t \\sim \\mathcal{N} (0, 1) \\right).\n\\end{aligned}\n\\end{equation*}\n```\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{equation*}\n\\gamma = \\Phi^{-1} (\\alpha) \\cdot \\sqrt{2 \\sigma^2 / n} + \\bar{x}.\n\\end{equation*}\n```\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{equation*}\n\\begin{aligned}\n\\beta &= P \\left(\\bar{x'} > \\gamma \\mid t \\sim \\mathcal{N} (-\\frac{\\theta}{\\sqrt{2 \\sigma^2 / n}}, 1) \\right) \\\\\n&= P \\left(t > \\frac{\\gamma - \\bar{x}}{\\sqrt{2 \\sigma^2 / n}} \\mid t \\sim \\mathcal{N} (-\\frac{\\theta}{\\sqrt{2 \\sigma^2 / n}}, 1) \\right) \\\\\n&= P \\left(t + \\frac{\\theta}{\\sqrt{2 \\sigma^2 / n}} > \\frac{\\gamma - \\bar{x} + \\theta}{\\sqrt{2 \\sigma^2 / n}} \\mid t + \\frac{\\theta}{\\sqrt{2 \\sigma^2 / n}} \\sim \\mathcal{N} (0, 1) \\right) \\\\\n&= P \\left(t + \\frac{\\theta}{\\sqrt{2 \\sigma^2 / n}} > \\Phi^{-1} (\\alpha) + \\frac{\\theta}{\\sqrt{2 \\sigma^2 / n}} \\mid t + \\frac{\\theta}{\\sqrt{2 \\sigma^2 / n}} \\sim \\mathcal{N} (0, 1) \\right)\n\\end{aligned}\n\\end{equation*}\n```\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{equation*}\n\\begin{aligned}\n\\theta &= (\\Phi^{-1} (1-\\beta) - \\Phi^{-1} (\\alpha)) \\cdot \\sqrt{2 \\sigma^2 / n} \\\\\n&= - (\\Phi^{-1} (\\alpha) + \\Phi^{-1} (\\beta)) \\cdot \\sqrt{2 \\sigma^2 / n}\n\\end{aligned}\n\\end{equation*}\n```\n\n----------------------------------------\n\nTITLE: Configuring Flash MLA Object Library in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all C++ and CUDA source files in the directory, then creates an object library called 'flash_mla_src'. The library is configured with position-independent code and CUDA device symbol resolution enabled, which are necessary for integration with the larger TensorRT-LLM framework.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/flashMLA/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\n\nadd_library(flash_mla_src OBJECT ${SRC_CPP} ${SRC_CU})\nset_property(TARGET flash_mla_src PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET flash_mla_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT-LLM with PyTorch Backend\nDESCRIPTION: Demonstrates how to initialize the LLM class from the tensorrt_llm._torch module, which serves as the interface for the PyTorch backend in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/torch/arch_overview.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorrt_llm._torch import LLM\nllm = LLM(model=<path_to_llama_from_hf>)\n```\n\n----------------------------------------\n\nTITLE: Windows-Specific Compiler Flags Configuration\nDESCRIPTION: Sets Windows-specific compiler flags and handles MSVC version requirements for C++17 compatibility with CUTLASS 3.0 kernels.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n  set(CMAKE_CXX_FLAGS \"/DNOMINMAX ${CMAKE_CXX_FLAGS}\")\nendif()\n\nif((WIN32))\n  if((MSVC_VERSION GREATER_EQUAL 1914))\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zc:__cplusplus\")\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler  /Zc:__cplusplus\")\n  else()\n    message(\n      FATAL_ERROR\n        \"Build is only supported with Visual Studio 2017 version 15.7 or higher\"\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM Block Scale GEMM Library in CMake\nDESCRIPTION: This CMake snippet builds the blockscale GEMM module for TensorRT-LLM. It collects all C++ and CUDA source files in the directory, filters CUDA architectures with compatibility level 100, and creates a position-independent object library with device symbol resolution enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SRC_CPP *.cpp)\nfile(GLOB_RECURSE SRC_CU *.cu)\n\nfilter_cuda_archs(\"100\" SRC_CPP)\n\nadd_library(trtllm_gen_blockscale_gemm OBJECT ${SRC_CPP} ${SRC_CU})\nset_property(TARGET trtllm_gen_blockscale_gemm\n             PROPERTY POSITION_INDEPENDENT_CODE ON)\nset_property(TARGET trtllm_gen_blockscale_gemm\n             PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for TensorRT-LLM\nDESCRIPTION: Configuration listing required Python packages and their version constraints. Includes core TensorRT-LLM package, data handling libraries, evaluation tools, and text processing utilities.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/jais/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nrouge_score\nSentencePiece>=0.1.99\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM Testing Utilities Library with CMake\nDESCRIPTION: Creates and configures a testing utilities library with required dependencies and include paths. The library includes common testing components, engine utilities, and executor utilities, linking against Google Test, shared targets, and model specification components.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/utils/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(testingUtils common.cpp engines.cpp executorUtils.cpp)\ntarget_link_libraries(testingUtils PUBLIC gtest_main ${SHARED_TARGET})\ntarget_link_libraries(testingUtils PRIVATE modelSpecStatic)\ntarget_include_directories(testingUtils PRIVATE ${MPI_C_INCLUDE_DIRS})\ntarget_compile_definitions(testingUtils PUBLIC TOP_LEVEL_DIR=\"${TOP_LEVEL_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Python Documentation Dependencies\nDESCRIPTION: Lists required Python packages and version constraints for building TensorRT-LLM documentation using Sphinx. Includes core Sphinx package, various extensions for enhanced functionality, and NVIDIA's custom theme.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx>=7.0\nsphinx-argparse\nsphinx-click\nnvidia-sphinx-theme\nmyst_parser\nbreathe\npygit2\nsphinx_copybutton\nautodoc_pydantic\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version and Initial Configuration\nDESCRIPTION: Defines the minimum required CMake version, sets up compile command export, and includes necessary module files for the build configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.27 FATAL_ERROR)\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\ninclude(CheckLanguage)\ninclude(cmake/modules/set_ifndef.cmake)\ninclude(cmake/modules/find_library_create_target.cmake)\ninclude(cmake/modules/resolve_dirs.cmake)\ninclude(cmake/modules/parse_make_options.cmake)\n\nproject(tensorrt_llm LANGUAGES CXX)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for TensorRT-LLM Plugin\nDESCRIPTION: RST directives for generating automatic documentation from Python docstrings for the tensorrt_llm plugin module. Sets up module imports and documentation settings including inheritance display.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/python-api/tensorrt_llm.plugin.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: tensorrt_llm\n\n.. currentmodule:: tensorrt_llm\n\n.. automodule:: tensorrt_llm.plugin\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Defining TensorRT-LLM Executor Targets in CMake\nDESCRIPTION: Sets up the target names for the TensorRT-LLM executor library, defining both static and dynamic library targets that will be built.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/executor/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(EXECUTOR_TARGET_NAME tensorrt_llm_executor)\nset(EXECUTOR_STATIC_TARGET ${EXECUTOR_TARGET_NAME}_static)\n\nset(TARGET_DIR ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Generating CUTLASS Kernel Instantiations\nDESCRIPTION: Sets up directory properties and executes the kernel generation Python script. This script generates architecture-specific CUTLASS kernel instantiations based on the specified CUDA architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset_directory_properties(\n  PROPERTIES CMAKE_CONFIGURE_DEPENDS\n             ${CMAKE_CURRENT_SOURCE_DIR}/python/generate_kernels.py)\n\nset(INSTANTIATION_GENERATION_DIR\n    ${CMAKE_CURRENT_BINARY_DIR}/cutlass_instantiations)\nexecute_process(\n  WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/python/\n  COMMAND\n    ${Python3_EXECUTABLE} generate_kernels.py -a\n    \"${CMAKE_CUDA_ARCHITECTURES_ORIG};${CMAKE_CUDA_ARCHITECTURES_NATIVE}\" -o\n    ${INSTANTIATION_GENERATION_DIR}\n  RESULT_VARIABLE _KERNEL_GEN_SUCCESS)\n\nif(NOT _KERNEL_GEN_SUCCESS MATCHES 0)\n  message(\n    FATAL_ERROR\n      \"Failed to generate CUTLASS kernel instantiations due to ${_KERNEL_GEN_SUCCESS}.\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for NVIDIA TensorRT-LLM\nDESCRIPTION: This snippet enumerates the required Python packages for the NVIDIA TensorRT-LLM project. It specifies version constraints for some packages and includes libraries for TensorRT-LLM, datasets, evaluation, protocol buffers, and text processing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/glm-4-9b/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.14.5\nevaluate\nprotobuf\nrouge_score\nsentencepiece\ntiktoken\n```\n\n----------------------------------------\n\nTITLE: Literal Usage Example - Before\nDESCRIPTION: Shows non-preferred way of using magic numbers directly in conditional statements.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nif (nbInputs == 2U){/*...*/}\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet enumerates the required Python packages for the TensorRT-LLM project. It includes TensorRT-LLM, text processing libraries like tiktoken and transformers, audio processing libraries such as openai-whisper and librosa, and other utility packages.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/whisper/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ntiktoken\ndatasets\nkaldialign\nopenai-whisper\nlibrosa\nsoundfile\nsafetensors\ntransformers\njanus\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Components Build Status\nDESCRIPTION: Reports the build status of various optional components, including PyTorch, Google tests, benchmarks, and micro benchmarks.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_PYT)\n  message(STATUS \"Building PyTorch\")\nelse()\n  message(STATUS \"Not building PyTorch\")\nendif()\n\nif(BUILD_TESTS)\n  message(STATUS \"Building Google tests\")\nelse()\n  message(STATUS \"Not building Google tests\")\nendif()\n\nif(BUILD_BENCHMARKS)\n  message(STATUS \"Building benchmarks\")\nelse()\n  message(STATUS \"Not building benchmarks\")\nendif()\n\nif(BUILD_MICRO_BENCHMARKS)\n  message(STATUS \"Building C++ micro benchmarks\")\nelse()\n  message(STATUS \"Not building C++ micro benchmarks\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: General Parameters Table\nDESCRIPTION: Markdown table listing general sampling parameters in TensorRT-LLM, including temperature, length constraints, and various penalty parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-runtime.md#2025-04-07_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n|   Name in TRT-LLM   |                                    Description                                    |   Data type   |                                      Range of value                                       |                     Default value                     |       Name in HF       |\n| :-----------------: | :-------------------------------------------------------------------------------: | :-----------: | :---------------------------------------------------------------------------------------: | :---------------------------------------------------: | :--------------------: |\n|    `temperature`    |                     modulation of logits in sampling workflow                     | List\\[Float\\] |                                    \\[0.0f, $+\\infty$\\)                                    |                `1.0f` (no modulation)                 |     `temperature`      |\n|     `minLength`     |                   lower-bound on the number of tokens generated                   |  List\\[Int\\]  |                                     \\[0, $+\\infty$\\)                                      | `0` (no effect (the first generated token can be EOS) |      `min_length`      |\n| `repetitionPenalty` | penalize repetitive tokens <br> multiplicative, irrespective of appearances count | List\\[Float\\] |   \\[0.0f, $+\\infty$\\) <br> `< 1.0f` encourages repetition <br> `> 1.0f` discourages it    |                  `1.0f` (no effect)                   |  `repetition_penalty`  |\n|  `presencePenalty`  |     penalize existed tokens <br> additive, irrespective of appearances count      | List\\[Float\\] | \\($-\\infty$, $+\\infty$\\) <br> `< 0.0f` encourages repetition <br> `> 0.0f` discourages it |                  `0.0f` (no effect)                   |           no           |\n| `frequencyPenalty`  |       penalize existed tokens <br> additive, dependent on appearances count       | List\\[Float\\] | \\($-\\infty$, $+\\infty$\\) <br> `< 0.0f` encourages repetition <br> `> 0.0f` discourages it |                  `0.0f` (no effect)                   |           no           |\n| `noRepeatNgramSize` |                                                                                   |  List\\[Int\\]  |          \\[0, $+\\infty$\\) <br> `> 0` all ngrams of that size can only occur once          |                    `0` (no effect)                    | `no_repeat_ngram_size` |\n```\n\n----------------------------------------\n\nTITLE: Adding UCX Utils Subdirectory to TensorRT-LLM Build\nDESCRIPTION: Adds the UCX utilities subdirectory to the build, which provides networking capabilities needed for distributed execution in TensorRT-LLM.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/executor/CMakeLists.txt#2025-04-07_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(cache_transmission/ucx_utils)\n```\n\n----------------------------------------\n\nTITLE: Logging Source File Collections\nDESCRIPTION: Outputs verbose messages showing which source files have been collected for each GEMM variant. This helps with debugging the build configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(\n  VERBOSE\n  \"Mixed srcs ${MIXED_SRC_CPP} ${MIXED_SRC_CU} ${MIXED_CU_INSTANTIATIONS}\")\nmessage(\n  VERBOSE\n  \"Group srcs ${GROUPED_SRC_CU} ${GROUPED_SRC_CPP} ${GROUPED_CU_INSTANTIATIONS}\"\n)\nmessage(VERBOSE \"Fbgemm srcs ${FBGEMM_SRC_CU} ${FBGEMM_CU_INSTANTIATIONS}\")\nmessage(VERBOSE \"FP8 Blockscale GEMM srcs ${FP8_BLOCKSCALE_GEMM_SRC_CU}\")\nmessage(VERBOSE \"FP4 srcs ${FP4_SRC_CU} ${FP4_CU_INSTANTIATIONS}\")\nmessage(VERBOSE \"ARgemm srcs ${ARGEMM_SRC_CU}\")\nmessage(VERBOSE \"All srcs ${ALL_SRCS}\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet defines the Python package dependencies required for the TensorRT-LLM project. It includes a direct Git repository installation of 'scaling_on_scales' and pins the 'transformers' library to version 4.36.2.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/multimodal/requirements-vila.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngit+https://github.com/bfshi/scaling_on_scales.git\ntransformers==4.36.2\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorRT-LLM\nDESCRIPTION: Specifies the required Python packages and version constraints needed to run TensorRT-LLM. Includes core dependencies like tensorrt_llm, datasets, transformers, and various utility packages for NLP and visualization.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/qwenvl/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-c ../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets~=2.16.0\nevaluate\nrouge_score\ntransformers-stream-generator\nsentencepiece>=0.1.99\ntiktoken\neinops\nmatplotlib\ntorchvision\n```\n\n----------------------------------------\n\nTITLE: Reading Project Version from Python\nDESCRIPTION: Reads the TensorRT-LLM project version from the version.py file using a Python subprocess and configures a version header file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\n# Read the project version\nset(TRTLLM_VERSION_DIR ${PROJECT_SOURCE_DIR}/../tensorrt_llm)\nset_directory_properties(PROPERTIES CMAKE_CONFIGURE_DEPENDS\n                                    ${TRTLLM_VERSION_DIR}/version.py)\nexecute_process(\n  COMMAND python3 -c \"import version; print(version.__version__)\"\n  WORKING_DIRECTORY ${TRTLLM_VERSION_DIR}\n  OUTPUT_VARIABLE TRTLLM_VERSION\n  RESULT_VARIABLE TRTLLM_VERSION_RESULT\n  OUTPUT_STRIP_TRAILING_WHITESPACE)\n\nif(TRTLLM_VERSION_RESULT EQUAL 0)\n  message(STATUS \"TensorRT-LLM version: ${TRTLLM_VERSION}\")\nelse()\n  message(FATAL_ERROR \"Failed to determine Tensorrt-LLM version\")\nendif()\n\nconfigure_file(\n  cmake/templates/version.h\n  ${CMAKE_CURRENT_SOURCE_DIR}/include/tensorrt_llm/executor/version.h)\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Memory-Related Error Example\nDESCRIPTION: Example error output when encountering memory-related issues during model building, suggesting the need to reduce batch size or enable plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/troubleshooting.md#2025-04-07_snippet_12\n\nLANGUAGE: txt\nCODE:\n```\n[09/23/2023-03:13:00] [TRT] [E] 9: GPTLMHeadModel/layers/0/attention/qkv/PLUGIN_V2_Gemm_0: could not find any supported formats consistent with input/output data types\n[09/23/2023-03:13:00] [TRT] [E] 9: [pluginV2Builder.cpp::reportPluginError::24] Error Code 9: Internal Error (GPTLMHeadModel/layers/0/attention/qkv/PLUGIN_V2_Gemm_0: could not find any supported formats consistent with input/output data types)\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Files for TensorRT-LLM Layers in CMake\nDESCRIPTION: Gathers all C++ and CUDA source files in the current directory for compilation. This ensures all relevant source files are included in the build process.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/layers/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nfile(GLOB CU_SRCS *.cu)\n```\n\n----------------------------------------\n\nTITLE: Gathering C++ Source Files for Plugin in CMake\nDESCRIPTION: This CMake snippet collects all C++ files in the current directory using the GLOB command and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope to make the sources available for the parent CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/lookupPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Displaying Software Compatibility Table in Markdown\nDESCRIPTION: This code snippet creates a table using Markdown syntax to display software compatibility information for TensorRT-LLM, including container version, TensorRT version, and precision support for different GPU architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/support-matrix.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```{list-table}\n:header-rows: 1\n:widths: 20 80\n\n* -\n  - Software Compatibility\n* - Container\n  - [25.01](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html)\n* - TensorRT\n  - [10.8](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html)\n* - Precision\n  -\n    - Hopper (SM90) - FP32, FP16, BF16, FP8, INT8, INT4\n    - Ada Lovelace (SM89) - FP32, FP16, BF16, FP8, INT8, INT4\n    - Ampere (SM80, SM86) - FP32, FP16, BF16, INT8, INT4[^smgte89]\n```\n```\n\n----------------------------------------\n\nTITLE: Specifying NVIDIA ModelOpt Dependency for TensorRT-LLM\nDESCRIPTION: This line defines a dependency on nvidia-modelopt with torch support, specifying version 0.21.0 or compatible. It's likely part of a requirements file for the NVIDIA TensorRT-LLM project.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mllama/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnvidia-modelopt[torch]~=0.21.0\n```\n\n----------------------------------------\n\nTITLE: Setting Default CUDA Architectures\nDESCRIPTION: Defines the default CUDA architectures to target when none are explicitly specified, including conditional inclusion of newer architectures based on the CUDA compiler version.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/CMakeLists.txt#2025-04-07_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n  set(CMAKE_CUDA_ARCHITECTURES \"80\" \"86\")\n  if(CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.8\")\n    list(APPEND CMAKE_CUDA_ARCHITECTURES \"89\" \"90\")\n  endif()\n  if(CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"12.7\")\n    list(APPEND CMAKE_CUDA_ARCHITECTURES \"100\" \"120\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Accuracy Comparison Table in Markdown\nDESCRIPTION: Markdown table comparing accuracy metrics for different models and quantization methods using MMLU benchmark.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Model        | Quantization Methods | MMLU Baseline (FP16) | MMLU Post-quantization | MMLU Loss |\n| ------------ | :------------------: | :------------------: | :--------------------: | :-------: |\n| Falcon-180B  |         FP8          |         70.4         |          70.3          |   0.14%   |\n|              |       INT8-SQ        |         70.4         |          68.6          |   2.56%   |\n|              |       INT4-AWQ       |         70.4         |          69.8          |   0.85%   |\n| Falcon-40B   |         FP8          |         56.1         |          55.6          |   0.89%   |\n```\n\n----------------------------------------\n\nTITLE: Filtering CUDA Architectures in CMake\nDESCRIPTION: This function filters CUDA source files based on the target architecture. It excludes cubin files for architectures not in the CMAKE_CUDA_ARCHITECTURES_ORIG list and adds a compile definition to exclude the architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(filter_cuda_archs ARCH SOURCES_VAR)\n  if(NOT \"${ARCH}\" IN_LIST CMAKE_CUDA_ARCHITECTURES_ORIG)\n    set(FILTER_REGEX\n        \".*_sm(_)?${ARCH}[.]cubin[.]cpp|^.*Sm(_)?${ARCH}.*cubin.cpp$\")\n    list(APPEND SOURCES ${${SOURCES_VAR}})\n    list(APPEND SOURCES_FILTERED ${SOURCES})\n    list(FILTER SOURCES_FILTERED INCLUDE REGEX \"${FILTER_REGEX}\")\n    list(LENGTH SOURCES_FILTERED SOURCES_FILTERED_LEN)\n    message(\n      STATUS\n        \"Excluding ${SOURCES_FILTERED_LEN} cubins for SM ${ARCH} from ${CMAKE_CURRENT_SOURCE_DIR}\"\n    )\n    foreach(filtered_item ${SOURCES_FILTERED})\n      message(VERBOSE \"- ${filtered_item}\")\n    endforeach()\n    list(FILTER SOURCES EXCLUDE REGEX \"${FILTER_REGEX}\")\n    set(${SOURCES_VAR}\n        \"${SOURCES}\"\n        PARENT_SCOPE)\n    add_compile_definitions(\"EXCLUDE_SM_${ARCH}\")\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Gathering and Setting Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet collects all C++ files in the current directory using file(GLOB SRCS *.cpp) and adds them to the PLUGIN_SOURCES variable. It then sets this updated list in the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Throughput Benchmark Sample Output\nDESCRIPTION: Example output from running the throughput benchmark, showing engine details, runtime information, and performance metrics including token throughput and request latency.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/performance-tuning-guide/benchmarking-default-performance.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n===========================================================\n= ENGINE DETAILS\n===========================================================\nModel:\t\t\t/scratch/Llama-3.3-70B-Instruct/\nEngine Directory:\t/scratch/grid_search_engines/baseline\nTensorRT-LLM Version:\t0.16.0\nDtype:\t\t\tbfloat16\nKV Cache Dtype:\t\tNone\nQuantization:\t\tNone\nMax Sequence Length:\t131072\n\n===========================================================\n= WORLD + RUNTIME INFORMATION\n===========================================================\nTP Size:\t\t4\nPP Size:\t\t1\nMax Runtime Batch Size:\t2048\nMax Runtime Tokens:\t8192\nScheduling Policy:\tGuaranteed No Evict\nKV Memory Percentage:\t90.00%\nIssue Rate (req/sec):\t7.9353E+13\n\n===========================================================\n= PERFORMANCE OVERVIEW\n===========================================================\nNumber of requests:\t\t1000\nAverage Input Length (tokens):\t2048.0000\nAverage Output Length (tokens):\t2048.0000\nToken Throughput (tokens/sec):\t1585.7480\nRequest Throughput (req/sec):\t0.7743\nTotal Latency (ms):\t\t1291504.1051\n\n===========================================================\n```\n\n----------------------------------------\n\nTITLE: Literal Usage Example - After\nDESCRIPTION: Shows preferred way of using named constants instead of magic numbers for better maintainability.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nconstexpr size_t kNbInputsWBias = 2U;\nif (nbInputs == kNbInputsWBias) {/*...*/}\n```\n\n----------------------------------------\n\nTITLE: Configuring Include Directories for TensorRT-LLM Tests\nDESCRIPTION: Sets up include directories for test compilation, including paths to CUTLASS extensions, project headers, third-party libraries, and test utilities.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tests/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(\n  ${PROJECT_SOURCE_DIR}/tensorrt_llm/cutlass_extensions/include\n  ${PROJECT_SOURCE_DIR}/include\n  ${3RDPARTY_DIR}/cutlass/include\n  ${3RDPARTY_DIR}/cutlass/tools/util/include\n  ${PROJECT_SOURCE_DIR}/tests/batch_manager\n  ${PROJECT_SOURCE_DIR}/tests/utils)\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies for TensorRT-LLM\nDESCRIPTION: This snippet defines the Python package requirements for TensorRT-LLM. It includes a constraint file reference, the TensorRT-LLM package itself (development version), datasets library with a specific version, and additional packages for evaluation and text processing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/models/contrib/internlm/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-c ../../../constraints.txt\ntensorrt_llm>=0.0.0.dev0\ndatasets==2.14.5\nrouge_score\nsentencepiece>=0.1.99\nevaluate\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Device Support in CMake\nDESCRIPTION: This conditional block excludes specific CUDA source files when multi-device support is disabled. It's used to omit custom all-reduce kernels when building for single-device configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_MULTI_DEVICE EQUAL 0)\n  list(FILTER SRC_CU EXCLUDE REGEX \"customAllReduceKernels*.*cu$\")\n  # list(FILTER SRC_CU EXCLUDE REGEX \"allReduceFusionKernels*.*cu$\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Gathering and Setting Plugin Sources in CMake for NVIDIA TensorRT-LLM\nDESCRIPTION: This CMake snippet collects all C++ source files in the current directory and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope, allowing it to be used in the parent CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/cudaStreamPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-LLM Docker Image with Make\nDESCRIPTION: Command to build the initial TensorRT-LLM Docker image using Make. This creates an image named 'tensorrt_llm/devel:latest'.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/dev-on-cloud/build-image-to-dockerhub.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake -C docker build\n```\n\n----------------------------------------\n\nTITLE: Avoiding Reflection in Python\nDESCRIPTION: Demonstrates how to write cleaner code by avoiding unnecessary reflection in Python. The example shows a function that creates a dictionary without using the locals() function.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef make_complex(*args):\n    x, y = args\n    return dict(**locals())\n```\n\nLANGUAGE: python\nCODE:\n```\ndef make_complex(x, y):\n    return {'x': x, 'y': y}\n```\n\n----------------------------------------\n\nTITLE: Setting up Include Directories for TensorRT-LLM Benchmarks in CMake\nDESCRIPTION: Configures the include directories for the benchmark targets, specifically adding the project's include directory to the search path.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${PROJECT_SOURCE_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Preprocessor Conditional Blocks in C++\nDESCRIPTION: An example showing the syntax of preprocessor conditional blocks. This code demonstrates conditional macro definition based on the value of FOO, including error handling for invalid values.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n# elif FOO == 1\n#  define BAR 5\n# else\n#  error \"invalid FOO value\"\n# endif\n#endif\n```\n\n----------------------------------------\n\nTITLE: Finding Python Interpreter for CUTLASS Setup\nDESCRIPTION: Checks if Python3_EXECUTABLE is already defined (typically when building with Torch support), and if not, attempts to find the Python interpreter which is required for subsequent CUTLASS setup.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT Python3_EXECUTABLE)\n  find_package(\n    Python3\n    COMPONENTS Interpreter\n    REQUIRED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring cxxopts Library Dependency in CMake\nDESCRIPTION: Sets up the cxxopts library as a dependency if it's not already available as a target, adding it from the 3rdparty directory.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT TARGET cxxopts::cxxopts)\n  set(CXXOPTS_SRC_DIR ${PROJECT_SOURCE_DIR}/../3rdparty/cxxopts)\n  add_subdirectory(${CXXOPTS_SRC_DIR} ${CMAKE_CURRENT_BINARY_DIR}/cxxopts)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Signing Off Git Commits\nDESCRIPTION: Command to commit changes with a sign-off, certifying that the contribution is original work or the contributor has rights to submit it under the project's license.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CONTRIBUTING.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -s -m \"Add cool feature.\"\n```\n\n----------------------------------------\n\nTITLE: Namespace Declaration Pattern in C++\nDESCRIPTION: Demonstrates proper namespace declaration with closing brace comments for improved code readability.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace foo\n{\n...\n} // namespace foo\n```\n\n----------------------------------------\n\nTITLE: Using Const Pointers to Const Data in C++\nDESCRIPTION: Shows the proper syntax for declaring a pointer that is const, pointing to const data. This ensures that neither the pointer nor the data it points to can be modified.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nchar const * const errStr = getErrorStr(status);\n```\n\n----------------------------------------\n\nTITLE: Collecting and Setting Plugin Sources in CMake\nDESCRIPTION: This CMake code snippet gathers all C++ source files in the current directory using GLOB and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope to make it available to the parent CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/eaglePlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT-LLM Libraries for CMake\nDESCRIPTION: Sets up the TensorRT-LLM shared libraries as imported targets in CMake, including the main library and the plugin library.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# tensorrt_llm shared lib\nadd_library(tensorrt_llm SHARED IMPORTED)\nset_property(TARGET tensorrt_llm PROPERTY IMPORTED_LOCATION ${TRTLLM_LIB_PATH})\nset_property(\n  TARGET tensorrt_llm PROPERTY IMPORTED_LINK_INTERFACE_LIBRARIES\n                               CUDA::cuda_driver CUDA::cudart_static CUDA::nvml)\n\n# nvinfer_plugin_tensorrt_llm shared lib\nadd_library(nvinfer_plugin_tensorrt_llm SHARED IMPORTED)\nset_property(TARGET nvinfer_plugin_tensorrt_llm PROPERTY IMPORTED_LOCATION\n                                                         ${TRTLLM_PLUGIN_PATH})\nset_property(TARGET nvinfer_plugin_tensorrt_llm\n             PROPERTY IMPORTED_LINK_INTERFACE_LIBRARIES tensorrt_llm)\n\ninclude_directories(${TRTLLM_INCLUDE_DIR} ${CUDAToolkit_INCLUDE_DIRS})\n```\n\n----------------------------------------\n\nTITLE: Preprocessor Directive Indentation\nDESCRIPTION: Demonstrates proper indentation of nested preprocessor directives.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n#if defined(FOO)\n# if FOO == 0\n#  define BAR 0\n```\n\n----------------------------------------\n\nTITLE: Avoiding Unsigned Integers with Narrowing Conversion in C++\nDESCRIPTION: Shows an example of the less preferred approach that would be required when using signed integers with vector sizes, demonstrating why unsigned integers are accepted in certain cases.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int i = 0; i < static_cast<int>(mTensors.size()); ++i)\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all C++ source files in the current directory and adds them to the PLUGIN_SOURCES variable. It then sets this variable in the parent scope, allowing it to be used in the main CMakeLists.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/doraPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Defining TensorRT-LLM Library Target\nDESCRIPTION: Declares and configures the shared library target for TensorRT-LLM custom plugins, including source files and library dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/manual_plugin/CMakeLists.txt#2025-04-07_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(\n  ${TARGET_NAME} SHARED\n  tritonPlugins.cpp\n  TritonFlashAttentionPlugin.cpp\n  aot/fmha_kernel_fp16.c\n  aot/fmha_kernel_fp32.c\n  aot/fp16/fmha_kernel_d64_fp16.fbf0f274_0d1d2d3d4d5d6789.c\n  aot/fp32/fmha_kernel_d64_fp32.f30323ef_0d1d2d3d4d5d6789.c)\n\ntarget_link_libraries(\n  ${TARGET_NAME} PUBLIC cuda ${CUDA_LIBRARIES} ${TRT_LLM_LIB_PATH}\n                        ${TRT_LLM_COMMON_LIB_PATH} ${TRT_LIB_PATH})\n\nif(NOT MSVC)\n  set_property(TARGET ${TARGET_NAME} PROPERTY LINK_FLAGS \"-Wl,--no-undefined\")\nendif()\n\ntarget_include_directories(${TARGET_NAME} PUBLIC /usr/local/cuda/include)\ntarget_include_directories(${TARGET_NAME} PUBLIC ${TRT_INCLUDE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Adding CPP Plugin Sources to Build Configuration in CMake\nDESCRIPTION: Collects all CPP files in the current directory and adds them to the PLUGIN_SOURCES variable, which is then propagated to the parent scope. This allows for dynamic inclusion of all plugin source files in the build process.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/bertAttentionPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Gathering and Setting Plugin Sources in CMake for TensorRT-LLM\nDESCRIPTION: This CMake snippet gathers all .cpp files in the current directory using file(GLOB SRCS *.cpp) and adds them to the PLUGIN_SOURCES variable. It then sets the updated PLUGIN_SOURCES in the parent scope.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/cumsumLastDimPlugin/CMakeLists.txt#2025-04-07_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})\nset(PLUGIN_SOURCES\n    ${PLUGIN_SOURCES}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Version-Specific Features for TensorRT-LLM\nDESCRIPTION: Adds compile definitions for BF16 and FP8 support based on the CUDA version. BF16 is enabled for CUDA 11.0+ and FP8 for CUDA 11.8+.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp/executor/CMakeLists.txt#2025-04-07_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL \"11\")\n  add_definitions(\"-DENABLE_BF16\")\n  message(\n    STATUS\n      \"CUDA_VERSION ${CUDA_VERSION} is greater or equal than 11.0, enable -DENABLE_BF16 flag\"\n  )\nendif()\n\nif(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL \"11.8\")\n  add_definitions(\"-DENABLE_FP8\")\n  message(\n    STATUS\n      \"CUDA_VERSION ${CUDA_VERSION} is greater or equal than 11.8, enable -DENABLE_FP8 flag\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Target Configuration and Library Linking\nDESCRIPTION: Sets up the executable target and links required CUDA, cuDNN, and TensorRT libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/cpp_library/CMakeLists.txt#2025-04-07_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(${TARGET_NAME} main.cpp)\n\ntarget_link_libraries(${TARGET_NAME} LINK_PRIVATE ${CUDA_LIBRARIES})\ntarget_link_libraries(${TARGET_NAME} LINK_PRIVATE cudnn)\ntarget_link_libraries(${TARGET_NAME} LINK_PRIVATE nvinfer)\ntarget_link_libraries(${TARGET_NAME} LINK_PRIVATE nvinfer_plugin_tensorrt_llm)\n\ntarget_include_directories(${TARGET_NAME} PUBLIC /usr/local/cuda/include)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation for TensorRT-LLM Quantization Module\nDESCRIPTION: This RST code configures Sphinx to automatically generate documentation for the TensorRT-LLM quantization module. It uses automodule directives to include all members and show inheritance relationships in the generated documentation.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/python-api/tensorrt_llm.quantization.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: tensorrt_llm\n\n.. currentmodule:: tensorrt_llm\n\n.. automodule:: tensorrt_llm.quantization\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Python Import Best Practices\nDESCRIPTION: Examples demonstrating the recommended import pattern for Python code in TensorRT-LLM. The preferred approach maintains the namespace when importing.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/CODING_GUIDELINES.md#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom package.subpackage.foo import SomeClass\nSomeClass()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport package\npackage.subpackage.foo.SomeClass()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom package.subpackage import foo\nfoo.SomeClass()\n```\n\n----------------------------------------\n\nTITLE: TensorRT-LLM Library Hashes\nDESCRIPTION: MD5 hashes and filenames for CUTLASS kernel libraries along with a Git commit reference\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/internal_cutlass_kernels/x86_64-linux-gnu/version.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n893ae060cdb1d5a54729afca9d1b9b99 libtensorrt_llm_internal_cutlass_kernels_static.a\na7ee89c7577bf9bf8d8a9ac8072b810d libtensorrt_llm_internal_cutlass_kernels_static.pre_cxx11.a\ncommit b43c46c83bd0833eae16ea0eae7cef6bee81644c\n```\n\n----------------------------------------\n\nTITLE: Feature Note for fMHA Support\nDESCRIPTION: Documentation note about fMHA feature support limitation on NVIDIA V100 GPUs.\nSOURCE: https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/release-notes.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n    ```{note}\n    Some features are not enabled for all models listed in the [examples](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples) folder.\n    ```\n```"
  }
]
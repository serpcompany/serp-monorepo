[
  {
    "owner": "pyro-ppl",
    "repo": "numpyro",
    "content": "TITLE: Defining Gaussian Mixture Model in NumPyro\nDESCRIPTION: Implements a Gaussian mixture model with K components using NumPyro's probabilistic programming constructs. The model includes global parameters for weights, scale, and component locations, and local variables for data point assignments.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nK = 2  # Fixed number of components.\n\n\n@config_enumerate\ndef model(data):\n    # Global variables.\n    weights = numpyro.sample(\"weights\", dist.Dirichlet(0.5 * jnp.ones(K)))\n    scale = numpyro.sample(\"scale\", dist.LogNormal(0.0, 2.0))\n    with numpyro.plate(\"components\", K):\n        locs = numpyro.sample(\"locs\", dist.Normal(0.0, 10.0))\n\n    with numpyro.plate(\"data\", len(data)):\n        # Local variables.\n        assignment = numpyro.sample(\"assignment\", dist.Categorical(weights))\n        numpyro.sample(\"obs\", dist.Normal(locs[assignment], scale), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Missing Data Model Implementation in NumPyro\nDESCRIPTION: Implements a complex model that explicitly handles missing data by modeling both the imputation process and the missingness mechanism. Includes outcome model parameters and missingness probability parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef impmissmodel(A, B, Y):\n    ntotal = A.shape[0]\n    A_isobs = A >= 0\n\n    # get parameters of imputation model\n    mu_A = sample(\"mu_A\", dist.Normal(0, 2.5))\n    b_B_A = sample(\"b_B_A\", dist.Normal(0, 2.5))\n\n    # get parameters of outcome model\n    b_A = sample(\"b_A\", dist.Normal(0, 2.5))\n    b_B = sample(\"b_B\", dist.Normal(0, 2.5))\n    s_Y = sample(\"s_Y\", dist.HalfCauchy(2.5))\n\n    # get parameter of model of missingness\n    with numpyro.plate(\"obsmodel\", 2):\n        p_Aobs = sample(\"p_Aobs\", dist.Beta(1, 1))\n\n    with numpyro.plate(\"obs\", ntotal):\n        ### imputation model\n        # get linear predictor for missing values\n        eta_A = mu_A + B * b_B_A\n\n        # sample imputation values for A\n        # mask out to not add log_prob to total likelihood right now\n        Aimp = sample(\n            \"A\",\n            dist.Bernoulli(logits=eta_A).mask(False),\n            infer={\"enumerate\": \"parallel\"},\n        )\n\n        # 'manually' calculate the log_prob\n        log_prob = dist.Bernoulli(logits=eta_A).log_prob(Aimp)\n\n        # cancel out enumerated values that are not equal to observed values\n        log_prob = jnp.where(A_isobs & (Aimp != A), -inf, log_prob)\n\n        # add to total likelihood for sampler\n        numpyro.factor(\"obs_A\", log_prob)\n\n        ### outcome model\n        eta_Y = b_A * Aimp + b_B * B\n        sample(\"obs_Y\", dist.Normal(eta_Y, s_Y), obs=Y)\n\n        ### missingness / observationmodel\n        eta_Aobs = jnp.where(Aimp, p_Aobs[0], p_Aobs[1])\n        sample(\"obs_Aobs\", dist.Bernoulli(probs=eta_Aobs), obs=A_isobs)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking HMC Sampler on Large Dataset\nDESCRIPTION: Executes and benchmarks the Hamiltonian Monte Carlo (HMC) sampler on the preprocessed dataset. Measures the average time per leapfrog step and displays summary statistics for the posterior samples.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/logistic_regression.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstep_size = jnp.sqrt(0.5 / N)\nkernel = HMC(\n    model,\n    step_size=step_size,\n    trajectory_length=(10 * step_size),\n    adapt_step_size=False,\n)\nmcmc = MCMC(kernel, num_warmup=500, num_samples=500, progress_bar=False)\nmcmc.warmup(random.PRNGKey(2019), features, labels, extra_fields=(\"num_steps\",))\nmcmc.get_extra_fields()[\"num_steps\"].sum().copy()\ntic = time.time()\nmcmc.run(random.PRNGKey(2020), features, labels, extra_fields=[\"num_steps\"])\nnum_leapfrogs = mcmc.get_extra_fields()[\"num_steps\"].sum().copy()\ntoc = time.time()\nprint(\"number of leapfrog steps:\", num_leapfrogs)\nprint(\"avg. time for each step :\", (toc - tic) / num_leapfrogs)\nmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking NUTS Sampler on Large Dataset\nDESCRIPTION: Executes and benchmarks the No-U-Turn Sampler (NUTS) on the preprocessed dataset. Measures the average time per leapfrog step and displays summary statistics for the posterior samples.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/logistic_regression.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmcmc = MCMC(NUTS(model), num_warmup=50, num_samples=50, progress_bar=False)\nmcmc.warmup(random.PRNGKey(2019), features, labels, extra_fields=(\"num_steps\",))\nmcmc.get_extra_fields()[\"num_steps\"].sum().copy()\ntic = time.time()\nmcmc.run(random.PRNGKey(2020), features, labels, extra_fields=[\"num_steps\"])\nnum_leapfrogs = mcmc.get_extra_fields()[\"num_steps\"].sum().copy()\ntoc = time.time()\nprint(\"number of leapfrog steps:\", num_leapfrogs)\nprint(\"avg. time for each step :\", (toc - tic) / num_leapfrogs)\nmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Defining a Truncated Poisson Model in NumPyro\nDESCRIPTION: Definition of a probabilistic model using NumPyro that samples from a left-truncated Poisson distribution. The model includes priors for the truncation point (low) and rate parameter.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef truncated_poisson_model(num_observations, x=None):\n    low = numpyro.sample(\"low\", dist.Categorical(0.2 * jnp.ones((5,))))\n    rate = numpyro.sample(\"rate\", dist.LogNormal(1, 1))\n    with numpyro.plate(\"observations\", num_observations):\n        numpyro.sample(\"x\", LeftTruncatedPoisson(rate, low), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Centered Parameterization in NumPyro\nDESCRIPTION: Redefines the Eight Schools model using a non-centered parameterization to improve MCMC convergence and efficiency.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from numpyro.infer.reparam import TransformReparam\n\n>>> # Eight Schools example - Non-centered Reparametrization\n... def eight_schools_noncentered(J, sigma, y=None):\n...     mu = numpyro.sample('mu', dist.Normal(0, 5))\n...     tau = numpyro.sample('tau', dist.HalfCauchy(5))\n...     with numpyro.plate('J', J):\n...         with numpyro.handlers.reparam(config={'theta': TransformReparam()}):\n...             theta = numpyro.sample(\n...                 'theta',\n...                 dist.TransformedDistribution(dist.Normal(0., 1.),\n...                                              dist.transforms.AffineTransform(mu, tau)))\n...         numpyro.sample('obs', dist.Normal(theta, sigma), obs=y)\n\n>>> nuts_kernel = NUTS(eight_schools_noncentered)\n>>> mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n>>> rng_key = random.PRNGKey(0)\n>>> mcmc.run(rng_key, J, sigma, y=y, extra_fields=('potential_energy',))\n>>> mcmc.print_summary(exclude_deterministic=False)  # doctest: +SKIP\n\n                   mean       std    median      5.0%     95.0%     n_eff     r_hat\n           mu      4.08      3.51      4.14     -1.69      9.71    720.43      1.00\n          tau      3.96      3.31      3.09      0.01      8.34    488.63      1.00\n     theta[0]      6.48      5.72      6.08     -2.53     14.96    801.59      1.00\n     theta[1]      4.95      5.10      4.91     -3.70     12.82   1183.06      1.00\n     theta[2]      3.65      5.58      3.72     -5.71     12.13    581.31      1.00\n     theta[3]      4.56      5.04      4.32     -3.14     12.92   1282.60      1.00\n     theta[4]      3.41      4.79      3.47     -4.16     10.79    801.25      1.00\n     theta[5]      3.58      4.80      3.78     -3.95     11.55   1101.33      1.00\n     theta[6]      6.31      5.17      5.75     -2.93     13.87   1081.11      1.00\n     theta[7]      4.81      5.38      4.61     -3.29     14.05    954.14      1.00\ntheta_base[0]      0.41      0.95      0.40     -1.09      1.95    851.45      1.00\ntheta_base[1]      0.15      0.95      0.20     -1.42      1.66   1568.11      1.00\ntheta_base[2]     -0.08      0.98     -0.10     -1.68      1.54   1037.16      1.00\ntheta_base[3]      0.06      0.89      0.05     -1.42      1.47   1745.02      1.00\ntheta_base[4]     -0.14      0.94     -0.16     -1.65      1.45    719.85      1.00\ntheta_base[5]     -0.10      0.96     -0.14     -1.57      1.51   1128.45      1.00\ntheta_base[6]      0.38      0.95      0.42     -1.32      1.82   1026.50      1.00\ntheta_base[7]      0.10      0.97      0.10     -1.51      1.65   1190.98      1.00\n\nNumber of divergences: 0\n\n>>> pe = mcmc.get_extra_fields()['potential_energy']\n>>> # Compare with the earlier value\n>>> print('Expected log joint density: {:.2f}'.format(np.mean(-pe)))  # doctest: +SKIP\nExpected log joint density: -46.09\n```\n\n----------------------------------------\n\nTITLE: Defining Lotka-Volterra ODE and Probabilistic Model in Python\nDESCRIPTION: This code defines the Lotka-Volterra differential equations and a probabilistic model for parameter estimation. The model handles multiple initial conditions and includes vectorized ODE solving using JAX.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef dz_dt(z, t, theta):\n    \"\"\"\n    Lotkaâ€“Volterra equations. Real positive parameters `alpha`, `beta`, `gamma`, `delta`\n    describes the interaction of two species.\n    \"\"\"\n    u, v = z\n    alpha, beta, gamma, delta = theta\n\n    du_dt = (alpha - beta * v) * u\n    dv_dt = (-gamma + delta * u) * v\n    return jnp.stack([du_dt, dv_dt])\n\n\ndef model(ts, y_init, y=None):\n    \"\"\"\n    :param numpy.ndarray ts: measurement times\n    :param numpy.ndarray y_init: measured inital conditions\n    :param numpy.ndarray y: measured populations\n    \"\"\"\n    # initial population\n    z_init = numpyro.sample(\n        \"z_init\", dist.LogNormal(jnp.log(y_init), jnp.ones_like(y_init))\n    )\n\n    # parameters alpha, beta, gamma, delta of dz_dt\n    theta = numpyro.sample(\n        \"theta\",\n        dist.TruncatedNormal(\n            low=0.0,\n            loc=jnp.array([1.0, 0.05, 1.0, 0.05]),\n            scale=jnp.array([0.2, 0.01, 0.2, 0.01]),\n        ),\n    )\n\n    # helpers to solve ODEs in a vectorized form\n    odeint_with_kwargs = functools.partial(odeint, rtol=1e-6, atol=1e-5, mxstep=1000)\n    vect_solve_ode = jax.vmap(\n        odeint_with_kwargs,\n        in_axes=(None, 0, 0, None),\n    )\n\n    # integrate dz/dt\n    zs = vect_solve_ode(dz_dt, z_init, ts, theta)\n    # measurement errors\n    sigma = numpyro.sample(\"sigma\", dist.LogNormal(-1, 1).expand([2]))\n    # measured populations\n    if y is not None:\n        # mask missing observations in the observed y\n        mask = jnp.isfinite(jnp.log(y))\n        numpyro.sample(\"y\", dist.LogNormal(jnp.log(zs), sigma).mask(mask), obs=y)\n    else:\n        numpyro.sample(\"y\", dist.LogNormal(jnp.log(zs), sigma))\n```\n\n----------------------------------------\n\nTITLE: Sampling from Posterior for Gaussian Process Model in Python\nDESCRIPTION: This code snippet samples from the posterior of the Gaussian Process model using both circulant and cholesky methods. It uses the No-U-Turn-Sampler (NUTS) and records the effective number of samples per second for each method.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/circulant_gp.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsamples_by_method = {}\nn_eff_per_second_by_method = {}\nfor method in [\"circulant\", \"cholesky\"]:\n    # Sample from the posterior using the NUTS kernel and record the duration.\n    kernel = numpyro.infer.NUTS(model)\n    mcmc = numpyro.infer.MCMC(kernel, num_warmup=800, num_samples=200)\n    start = time()\n    mcmc.run(random.key(9), x, trace[\"y\"][\"value\"], method=method)\n    duration = time() - start\n\n    # Calculate the number of effective samples per second.\n    samples_by_method[method] = mcmc.get_samples()\n    n_eff_per_second_by_method[method] = {\n        name: site[\"n_eff\"] / duration\n        for name, site in numpyro.diagnostics.summary(\n            mcmc.get_samples(group_by_chain=True)\n        ).items()\n    }\n    print(f\"completed sampling in {duration:.3f} seconds for {method} method\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Seasonal, Global Trend (SGT) Model in NumPyro\nDESCRIPTION: This function defines the SGT model using NumPyro primitives. It includes priors for model parameters, implements the transition function, and uses scan for efficient computation of the time series.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef sgt(y, seasonality, future=0):\n    # heuristically, standard derivation of Cauchy prior depends on\n    # the max value of data\n    cauchy_sd = jnp.max(y) / 150\n\n    # NB: priors' parameters are taken from\n    # https://github.com/cbergmeir/Rlgt/blob/master/Rlgt/R/rlgtcontrol.R\n    nu = numpyro.sample(\"nu\", dist.Uniform(2, 20))\n    powx = numpyro.sample(\"powx\", dist.Uniform(0, 1))\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(cauchy_sd))\n    offset_sigma = numpyro.sample(\n        \"offset_sigma\", dist.TruncatedCauchy(low=1e-10, loc=1e-10, scale=cauchy_sd)\n    )\n\n    coef_trend = numpyro.sample(\"coef_trend\", dist.Cauchy(0, cauchy_sd))\n    pow_trend_beta = numpyro.sample(\"pow_trend_beta\", dist.Beta(1, 1))\n    # pow_trend takes values from -0.5 to 1\n    pow_trend = 1.5 * pow_trend_beta - 0.5\n    pow_season = numpyro.sample(\"pow_season\", dist.Beta(1, 1))\n\n    level_sm = numpyro.sample(\"level_sm\", dist.Beta(1, 2))\n    s_sm = numpyro.sample(\"s_sm\", dist.Uniform(0, 1))\n    init_s = numpyro.sample(\"init_s\", dist.Cauchy(0, y[:seasonality] * 0.3))\n\n    def transition_fn(carry, t):\n        level, s, moving_sum = carry\n        season = s[0] * level**pow_season\n        exp_val = level + coef_trend * level**pow_trend + season\n        exp_val = jnp.clip(exp_val, 0)\n        # use expected vale when forecasting\n        y_t = jnp.where(t >= N, exp_val, y[t])\n\n        moving_sum = (\n            moving_sum + y[t] - jnp.where(t >= seasonality, y[t - seasonality], 0.0)\n        )\n        level_p = jnp.where(t >= seasonality, moving_sum / seasonality, y_t - season)\n        level = level_sm * level_p + (1 - level_sm) * level\n        level = jnp.clip(level, 0)\n\n        new_s = (s_sm * (y_t - level) / season + (1 - s_sm)) * s[0]\n        # repeat s when forecasting\n        new_s = jnp.where(t >= N, s[0], new_s)\n        s = jnp.concatenate([s[1:], new_s[None]], axis=0)\n\n        omega = sigma * exp_val**powx + offset_sigma\n        y_ = numpyro.sample(\"y\", dist.StudentT(nu, exp_val, omega))\n\n        return (level, s, moving_sum), y_\n\n    N = y.shape[0]\n    level_init = y[0]\n    s_init = jnp.concatenate([init_s[1:], init_s[:1]], axis=0)\n    moving_sum = level_init\n    with numpyro.handlers.condition(data={\"y\": y[1:]}):\n        _, ys = scan(\n            transition_fn, (level_init, s_init, moving_sum), jnp.arange(1, N + future)\n        )\n    if future > 0:\n        numpyro.deterministic(\"y_forecast\", ys[-future:])\n```\n\n----------------------------------------\n\nTITLE: Analyzing MCMC Results and Diagnostics in NumPyro\nDESCRIPTION: Prints summary statistics of the MCMC run, including divergences, and computes the expected log joint density.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> mcmc.print_summary()  # doctest: +SKIP\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n        mu      4.14      3.18      3.87     -0.76      9.50    115.42      1.01\n       tau      4.12      3.58      3.12      0.51      8.56     90.64      1.02\n  theta[0]      6.40      6.22      5.36     -2.54     15.27    176.75      1.00\n  theta[1]      4.96      5.04      4.49     -1.98     14.22    217.12      1.00\n  theta[2]      3.65      5.41      3.31     -3.47     13.77    247.64      1.00\n  theta[3]      4.47      5.29      4.00     -3.22     12.92    213.36      1.01\n  theta[4]      3.22      4.61      3.28     -3.72     10.93    242.14      1.01\n  theta[5]      3.89      4.99      3.71     -3.39     12.54    206.27      1.00\n  theta[6]      6.55      5.72      5.66     -1.43     15.78    124.57      1.00\n  theta[7]      4.81      5.95      4.19     -3.90     13.40    299.66      1.00\n\nNumber of divergences: 19\n\n>>> pe = mcmc.get_extra_fields()['potential_energy']\n>>> print('Expected log joint density: {:.2f}'.format(np.mean(-pe)))  # doctest: +SKIP\nExpected log joint density: -54.55\n```\n\n----------------------------------------\n\nTITLE: Defining the Imputation Model with Enumeration in NumPyro\nDESCRIPTION: Creates a Bayesian model that handles missing values through imputation, using NumPyro's automatic enumeration of discrete variables. This model includes both imputation and outcome components.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef impmodel(A, B, Y):\n    ntotal = A.shape[0]\n    A_isobs = A >= 0\n\n    # get parameters of imputation model\n    mu_A = sample(\"mu_A\", dist.Normal(0, 2.5))\n    b_B_A = sample(\"b_B_A\", dist.Normal(0, 2.5))\n\n    # get parameters of outcome model\n    b_A = sample(\"b_A\", dist.Normal(0, 2.5))\n    b_B = sample(\"b_B\", dist.Normal(0, 2.5))\n    s_Y = sample(\"s_Y\", dist.HalfCauchy(2.5))\n\n    with numpyro.plate(\"obs\", ntotal):\n        ### imputation model\n        # get linear predictor for missing values\n        eta_A = mu_A + B * b_B_A\n\n        # sample imputation values for A\n        # mask out to not add log_prob to total likelihood right now\n        Aimp = sample(\n            \"A\",\n            dist.Bernoulli(logits=eta_A).mask(False),\n            infer={\"enumerate\": \"parallel\"},\n        )\n\n        # 'manually' calculate the log_prob\n        log_prob = dist.Bernoulli(logits=eta_A).log_prob(Aimp)\n\n        # cancel out enumerated values that are not equal to observed values\n        log_prob = jnp.where(A_isobs & (Aimp != A), -inf, log_prob)\n\n        # add to total likelihood for sampler\n        numpyro.factor(\"A_obs\", log_prob)\n\n        ### outcome model\n        eta_Y = b_A * Aimp + b_B * B\n        sample(\"obs_Y\", dist.Normal(eta_Y, s_Y), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: Complete Implementation of LeftTruncatedPoisson Distribution\nDESCRIPTION: Full implementation of the LeftTruncatedPoisson distribution class, combining all necessary methods for defining, sampling from, and evaluating probabilities for a left-truncated Poisson distribution in NumPyro using JAX.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef scipy_truncated_poisson_icdf(args):  # Note: all arguments are passed inside a tuple\n    rate, low, u = args\n    rate = np.asarray(rate)\n    low = np.asarray(low)\n    u = np.asarray(u)\n    density = sp_poisson(rate)\n    low_cdf = density.cdf(low - 1)\n    normalizer = 1.0 - low_cdf\n    x = normalizer * u + low_cdf\n    return density.ppf(x)\n\n\nclass LeftTruncatedPoisson(Distribution):\n    \"\"\"\n    A truncated Poisson distribution.\n    :param numpy.ndarray low: lower bound at which truncation happens\n    :param numpy.ndarray rate: rate of the Poisson distribution.\n    \"\"\"\n\n    arg_constraints = {\n        \"low\": constraints.nonnegative_integer,\n        \"rate\": constraints.positive,\n    }\n\n    def __init__(self, rate=1.0, low=0, validate_args=None):\n        batch_shape = lax.broadcast_shapes(jnp.shape(low), jnp.shape(rate))\n        self.low, self.rate = promote_shapes(low, rate)\n        super().__init__(batch_shape, validate_args=validate_args)\n\n    def log_prob(self, value):\n        m = 1 - poisson.cdf(self.low - 1, self.rate)\n        log_p = poisson.logpmf(value, self.rate)\n        return jnp.where(value >= self.low, log_p - jnp.log(m), -jnp.inf)\n\n    def sample(self, key, sample_shape=()):\n        shape = sample_shape + self.batch_shape\n        float_type = jnp.result_type(float)\n        minval = jnp.finfo(float_type).tiny\n        u = random.uniform(key, shape, minval=minval)\n        # return self.icdf(u)        # Brute force\n        # return self.icdf_faster(u) # For faster sampling.\n        return self.icdf_scipy(u)  # Using `host_callback`\n\n    def icdf(self, u):\n        def cond_fn(val):\n            n, cdf = val\n            return jnp.any(cdf < u)\n```\n\n----------------------------------------\n\nTITLE: Initializing GMM Parameters and Guide\nDESCRIPTION: Sets up the TraceEnum_ELBO loss and implements initialization logic for the model parameters. It chooses the best initialization from 100 random starts by sampling from the data.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nelbo = TraceEnum_ELBO()\n\n\ndef initialize(seed):\n    global global_guide\n    init_values = {\n        \"weights\": jnp.ones(K) / K,\n        \"scale\": jnp.sqrt(data.var() / 2),\n        \"locs\": data[\n            random.categorical(\n                random.PRNGKey(seed), jnp.ones(len(data)) / len(data), shape=(K,)\n            )\n        ],\n    }\n    global_model = handlers.block(\n        handlers.seed(model, random.PRNGKey(0)),\n        hide_fn=lambda site: site[\"name\"]\n        not in [\"weights\", \"scale\", \"locs\", \"components\"],\n    )\n    global_guide = AutoDelta(\n        global_model, init_loc_fn=init_to_value(values=init_values)\n    )\n    handlers.seed(global_guide, random.PRNGKey(0))(data)  # warm up the guide\n    return elbo.loss(random.PRNGKey(0), {}, model, global_guide, data)\n\n\n# Choose the best among 100 random initializations.\nloss, seed = min((initialize(seed), seed) for seed in range(100))\ninitialize(seed)  # initialize the global_guide\nprint(f\"seed = {seed}, initial_loss = {loss}\")\n```\n\n----------------------------------------\n\nTITLE: Defining TBIP Model and Variational Family in NumPyro\nDESCRIPTION: Implements the Text-Based Ideal Point (TBIP) model using NumPyro. It defines the generative model and variational guide for inference, including latent variables for topics, ideal points, and word distributions.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass TBIP:\n    def __init__(self, N, D, K, V, batch_size, init_mu_theta=None, init_mu_beta=None):\n        self.N = N  # number of people\n        self.D = D  # number of documents\n        self.K = K  # number of topics\n        self.V = V  # number of words in vocabulary\n        self.batch_size = batch_size  # number of documents in a batch\n\n        if init_mu_theta is None:\n            init_mu_theta = jnp.zeros([D, K])\n        else:\n            self.init_mu_theta = init_mu_theta\n\n        if init_mu_beta is None:\n            init_mu_beta = jnp.zeros([K, V])\n        else:\n            self.init_mu_beta = init_mu_beta\n\n    def model(self, Y_batch, d_batch, i_batch):\n        with plate(\"i\", self.N):\n            # Sample the per-unit latent variables (ideal points)\n            x = sample(\"x\", dist.Normal())\n\n        with plate(\"k\", size=self.K, dim=-2):\n            with plate(\"k_v\", size=self.V, dim=-1):\n                beta = sample(\"beta\", dist.Gamma(0.3, 0.3))\n                eta = sample(\"eta\", dist.Normal())\n\n        with plate(\"d\", size=self.D, subsample_size=self.batch_size, dim=-2):\n            with plate(\"d_k\", size=self.K, dim=-1):\n                # Sample document-level latent variables (topic intensities)\n                theta = sample(\"theta\", dist.Gamma(0.3, 0.3))\n\n            # Compute Poisson rates for each word\n            P = jnp.sum(\n                jnp.expand_dims(theta, 2)\n                * jnp.expand_dims(beta, 0)\n                * jnp.exp(\n                    jnp.expand_dims(x[i_batch], (1, 2)) * jnp.expand_dims(eta, 0)\n                ),\n                1,\n            )\n\n            with plate(\"v\", size=self.V, dim=-1):\n                # Sample observed words\n                sample(\"Y_batch\", dist.Poisson(P), obs=Y_batch)\n\n    def guide(self, Y_batch, d_batch, i_batch):\n        # This defines variational family. Notice that each of the latent variables\n        # defined in the sample statements in the model above has a corresponding\n        # sample statement in the guide. The guide is responsible for providing\n        # variational parameters for each of these latent variables.\n\n        # Also notice it is required that model and the guide have the same call.\n\n        mu_x = param(\n            \"mu_x\", init_value=-1 + 2 * random.uniform(random.PRNGKey(1), (self.N,))\n        )\n        sigma_x = param(\n            \"sigma_y\", init_value=jnp.ones([self.N]), constraint=constraints.positive\n        )\n\n        mu_eta = param(\n            \"mu_eta\", init_value=random.normal(random.PRNGKey(2), (self.K, self.V))\n        )\n        sigma_eta = param(\n            \"sigma_eta\",\n            init_value=jnp.ones([self.K, self.V]),\n            constraint=constraints.positive,\n        )\n\n        mu_theta = param(\"mu_theta\", init_value=self.init_mu_theta)\n        sigma_theta = param(\n            \"sigma_theta\",\n            init_value=jnp.ones([self.D, self.K]),\n            constraint=constraints.positive,\n        )\n\n        mu_beta = param(\"mu_beta\", init_value=self.init_mu_beta)\n        sigma_beta = param(\n            \"sigma_beta\",\n            init_value=jnp.ones([self.K, self.V]),\n            constraint=constraints.positive,\n        )\n\n        with plate(\"i\", self.N):\n            sample(\"x\", dist.Normal(mu_x, sigma_x))\n\n        with plate(\"k\", size=self.K, dim=-2):\n            with plate(\"k_v\", size=self.V, dim=-1):\n                sample(\"beta\", dist.LogNormal(mu_beta, sigma_beta))\n                sample(\"eta\", dist.Normal(mu_eta, sigma_eta))\n\n        with plate(\"d\", size=self.D, subsample_size=self.batch_size, dim=-2):\n            with plate(\"d_k\", size=self.K, dim=-1):\n                sample(\"theta\", dist.LogNormal(mu_theta[d_batch], sigma_theta[d_batch]))\n\n    def get_batch(self, rng, Y, author_indices):\n        # Helper functions to obtain a batch of data, convert from scipy.sparse\n        # to jax.numpy.array and move to gpu\n\n        D_batch = random.choice(rng, jnp.arange(self.D), shape=(self.batch_size,))\n        Y_batch = jax.device_put(jnp.array(Y[D_batch].toarray()), jax.devices(\"gpu\")[0])\n        D_batch = jax.device_put(D_batch, jax.devices(\"gpu\")[0])\n        I_batch = author_indices[D_batch]\n        return Y_batch, I_batch, D_batch\n```\n\n----------------------------------------\n\nTITLE: Running MCMC Simulation with NumPyro NUTS Sampler\nDESCRIPTION: Performs MCMC sampling using the NUTS algorithm to estimate model parameters. Configures sampler settings and compares true vs estimated parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ny_init = data[:, 0, :]\n\nmcmc = MCMC(\n    NUTS(\n        model,\n        dense_mass=True,\n        init_strategy=init_to_sample(),\n        max_tree_depth=10,\n    ),\n    num_warmup=1000,\n    num_samples=1000,\n    num_chains=1,\n    progress_bar=True,\n)\n\nmcmc.run(PRNGKey(1031410), ts=ts_filled_nans, y_init=y_init, y=data)\nmcmc.print_summary()\n\nprint(f\"True params mean: {sample['theta'][0]}\")\nprint(f\"Estimated params mean: {jnp.mean(mcmc.get_samples()['theta'], axis=0)}\")\n```\n\n----------------------------------------\n\nTITLE: Running MCMC Inference with NUTS in NumPyro\nDESCRIPTION: Performs MCMC inference using the No-U-Turn Sampler (NUTS) on the Eight Schools model, collecting samples and additional diagnostics.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from jax import random\n>>> from numpyro.infer import MCMC, NUTS\n\n>>> nuts_kernel = NUTS(eight_schools)\n>>> mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n>>> rng_key = random.PRNGKey(0)\n>>> mcmc.run(rng_key, J, sigma, y=y, extra_fields=('potential_energy',))\n```\n\n----------------------------------------\n\nTITLE: High-Dimensional MVN with Tree Depth Impact\nDESCRIPTION: Demonstrates the importance of max_tree_depth parameter when sampling from high-dimensional multivariate normal distributions with strong correlations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrho = 0.999\ndim = 200\ncov = rho * jnp.ones((dim, dim)) + (1 - rho) * jnp.eye(dim)\n\ndef mvn_model():\n    numpyro.sample(\"x\", dist.MultivariateNormal(jnp.zeros(dim), covariance_matrix=cov))\n\nprint(\"max_tree_depth = 5 (bad r_hat)\")\nrun_inference(mvn_model, max_tree_depth=5)\n\nprint(\"max_tree_depth = 10 (good r_hat)\")\nrun_inference(mvn_model, max_tree_depth=10)\n```\n\n----------------------------------------\n\nTITLE: Training the Gaussian Mixture Model\nDESCRIPTION: Executes the SVI algorithm to train the Gaussian mixture model for a specified number of iterations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nglobal_svi_result = global_svi.run(\n    random.PRNGKey(0), 200 if not smoke_test else 2, data\n)\n```\n\n----------------------------------------\n\nTITLE: Running MCMC with NUTS in NumPyro\nDESCRIPTION: This code sets up and runs Markov Chain Monte Carlo (MCMC) using the No-U-Turn Sampler (NUTS) to explore the full posterior over component parameters. It uses collapsed NUTS to marginalize out all discrete latent variables.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom numpyro.infer import MCMC, NUTS\n\nkernel = NUTS(model)\nmcmc = MCMC(kernel, num_warmup=50, num_samples=250)\nmcmc.run(random.PRNGKey(2), data)\nmcmc.print_summary()\nposterior_samples = mcmc.get_samples()\n```\n\n----------------------------------------\n\nTITLE: Defining Truncated Normal Model in NumPyro\nDESCRIPTION: This snippet defines a probabilistic model using the RightTruncatedNormal distribution for observations. It includes priors for location, scale, and high parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef truncated_normal_model(num_observations, x=None):\n    loc = numpyro.sample(\"loc\", dist.Normal())\n    scale = numpyro.sample(\"scale\", dist.LogNormal())\n    high = numpyro.sample(\"high\", dist.Normal())\n    with numpyro.plate(\"observations\", num_observations):\n        numpyro.sample(\"x\", RightTruncatedNormal(loc, scale, high), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Defining Logistic Regression Model in NumPyro\nDESCRIPTION: Implements a logistic regression model using NumPyro's probabilistic programming interface. The model samples coefficients from a Normal distribution and uses a Bernoulli distribution with logits for observations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/logistic_regression.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model(data, labels):\n    coefs = numpyro.sample(\"coefs\", dist.Normal(jnp.zeros(dim), jnp.ones(dim)))\n    logits = jnp.dot(data, coefs)\n    return numpyro.sample(\"obs\", dist.Bernoulli(logits=logits), obs=labels)\n```\n\n----------------------------------------\n\nTITLE: MCMC Sampling with NUTS - Python\nDESCRIPTION: Runs MCMC sampling using the No-U-Turn Sampler algorithm with 4 chains, 5000 warmup steps, and 5000 sampling steps to obtain model parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\nkernel = NUTS(sgt)\nmcmc = MCMC(kernel, num_warmup=5000, num_samples=5000, num_chains=4)\nmcmc.run(random.PRNGKey(0), y_train, seasonality=38)\nmcmc.print_summary()\nsamples = mcmc.get_samples()\n```\n\n----------------------------------------\n\nTITLE: Running MCMC for Truncated Normal Model in NumPyro\nDESCRIPTION: This snippet runs MCMC using the NUTS sampler for the truncated normal model and prints a summary of the results.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nmcmc = MCMC(NUTS(truncated_normal_model), **MCMC_KWARGS)\nmcmc.run(MCMC_RNG, num_observations, true_x)\nmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Ordinal Regression with Normal Prior\nDESCRIPTION: Implements ordinal regression using a Normal prior distribution for cutpoints with ordering constraints.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/ordinal_regression.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef model3(X, Y, nclasses=3):\n    b_X_eta = sample(\"b_X_eta\", Normal(0, 5))\n    c_y = sample(\n        \"c_y\",\n        TransformedDistribution(\n            Normal(0, 1).expand([nclasses - 1]), transforms.OrderedTransform()\n        ),\n    )\n    with numpyro.plate(\"obs\", X.shape[0]):\n        eta = X * b_X_eta\n        sample(\"Y\", OrderedLogistic(eta, c_y), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: Running SVI Optimization Loop with Progress Tracking\nDESCRIPTION: Implements the main SVI training loop with batch updates, loss tracking, and periodic saving of intermediate results including topic means and author ideal points.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom tqdm import tqdm\n\nprint_steps = 100\nprint_intermediate_results = False\n\nrngs = random.split(random.PRNGKey(2), num_steps)\nlosses = []\npbar = tqdm(range(num_steps))\n\nfor step in pbar:\n    Y_batch, I_batch, D_batch = tbip.get_batch(rngs[step], counts, author_indices)\n    svi_state, loss = svi_batch_update(\n        svi_state, Y_batch=Y_batch, d_batch=D_batch, i_batch=I_batch\n    )\n\n    loss = loss / counts.shape[0]\n    losses.append(loss)\n    if step % print_steps == 0 or step == num_steps - 1:\n        pbar.set_description(\n            \"Init loss: \"\n            + \"{:10.4f}\".format(jnp.array(losses[0]))\n            + f\"; Avg loss (last {print_steps} iter): \"\n            + \"{:10.4f}\".format(jnp.array(losses[-100:]).mean())\n        )\n\n    if (step + 1) % 2500 == 0 or step == num_steps - 1:\n        # Save intermediate results\n        estimated_params = svi_batch.get_params(svi_state)\n\n        neutral_mean = (\n            estimated_params[\"mu_beta\"] + estimated_params[\"sigma_beta\"] ** 2 / 2\n        )\n\n        positive_mean = (\n            estimated_params[\"mu_beta\"]\n            + estimated_params[\"mu_eta\"]\n            + (estimated_params[\"sigma_beta\"] ** 2 + estimated_params[\"sigma_eta\"] ** 2)\n            / 2\n        )\n\n        negative_mean = (\n            estimated_params[\"mu_beta\"]\n            - estimated_params[\"mu_eta\"]\n            + (estimated_params[\"sigma_beta\"] ** 2 + estimated_params[\"sigma_eta\"] ** 2)\n            / 2\n        )\n\n        np.save(\"neutral_topic_mean.npy\", neutral_mean)\n        np.save(\"negative_topic_mean.npy\", positive_mean)\n        np.save(\"positive_topic_mean.npy\", negative_mean)\n\n        topics = get_topics(neutral_mean, positive_mean, negative_mean, vocabulary)\n\n        with open(\"topics.txt\", \"w\") as f:\n            print(topics, file=f)\n\n        authors = pd.DataFrame(\n            {\"name\": author_map, \"ideal_point\": np.array(estimated_params[\"mu_x\"])}\n        )\n        authors.to_csv(\"authors.csv\")\n```\n\n----------------------------------------\n\nTITLE: Predicting Survival with Bayesian Imputation in NumPyro\nDESCRIPTION: This snippet demonstrates how to use the Predictive utility to make predictions from posterior samples, calculate accuracy, and create a confusion matrix. It uses Bayesian imputation for missing values.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nposterior = mcmc.get_samples()\nsurvived_pred = Predictive(model, posterior)(random.PRNGKey(1), **data)[\"survived\"]\nsurvived_pred = (survived_pred.mean(axis=0) >= 0.5).astype(jnp.uint8)\nprint(\"Accuracy:\", (survived_pred == survived).sum() / survived.shape[0])\nconfusion_matrix = pd.crosstab(\n    pd.Series(survived, name=\"actual\"), pd.Series(survived_pred, name=\"predict\")\n)\nconfusion_matrix / confusion_matrix.sum(axis=1)\n```\n\n----------------------------------------\n\nTITLE: MCMC Inference for Truncated Normal Model\nDESCRIPTION: This snippet demonstrates how to run MCMC inference using the NUTS sampler for the truncated normal model and compare results to ground truth.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmcmc = MCMC(NUTS(truncated_normal_model), **MCMC_KWARGS)\nmcmc.run(MCMC_RNG, num_observations, high, true_x)\nmcmc.print_summary()\n\nprint(f\"True loc  : {true_loc:3.2}\")\nprint(f\"True scale: {true_scale:3.2}\")\n```\n\n----------------------------------------\n\nTITLE: Missing Data Model MCMC Analysis in NumPyro\nDESCRIPTION: Executes MCMC sampling using NUTS sampler on the complex missing data model. Uses same sampling parameters as previous analyses.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimpmisskernel = NUTS(impmissmodel)\nimpmissmcmc = MCMC(impmisskernel, num_warmup=250, num_samples=750)\nimpmissmcmc.run(mcmc_key, Aobs, B, Y)\nimpmissmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Running MCMC Sampling for Bayesian Model in Python\nDESCRIPTION: Performs MCMC sampling using the NUTS kernel for the Bayesian logistic regression model with imputation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmcmc = MCMC(NUTS(model), num_warmup=1000, num_samples=1000)\nmcmc.run(random.PRNGKey(0), **data, survived=survived)\nmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: MCMC Inference with NUTS for Model with Known Truncation\nDESCRIPTION: Performs MCMC inference using the NUTS sampler for the truncated Poisson model with a fixed truncation point, which is more efficient than DiscreteHMCGibbs when no discrete parameters need inference.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nmcmc = MCMC(\n    NUTS(model_with_known_low),\n    **MCMC_KWARGS,\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of Different Model Parameterizations\nDESCRIPTION: Runs inference on the unreparameterized and various reparameterized versions of the horseshoe regression model, comparing their r_hat values to demonstrate improved sampling efficiency.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# create fake dataset\nX = np.random.RandomState(0).randn(100, 500)\nY = X[:, 0]\n\nprint(\"unreparameterized model (very bad r_hats)\")\nrun_inference(partial(_unrep_hs_model, X, Y))\n\nprint(\"\\nreparameterized model with manual reparameterization (good r_hats)\")\nrun_inference(partial(_rep_hs_model1, X, Y))\n\nprint(\"\\nreparameterized model with LocScaleReparam (good r_hats)\")\nrun_inference(partial(_rep_hs_model2, X, Y))\n\nprint(\"\\nreparameterized model with TransformReparam (good r_hats)\")\nrun_inference(partial(_rep_hs_model3, X, Y))\n```\n\n----------------------------------------\n\nTITLE: Implementing icdf_scipy with JAX's host_callback\nDESCRIPTION: Implementation of the inverse CDF method using JAX's experimental host_callback feature to utilize SciPy's implementation. This provides a more efficient alternative to the brute-force approach while maintaining compatibility with JAX transformations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n    # ...\n    def icdf_scipy(self, u):\n        result_shape = jax.ShapeDtypeStruct(\n            u.shape,\n            jnp.result_type(float) # int type not currently supported\n        )\n        result = jax.experimental.host_callback.call(\n            scipy_truncated_poisson_icdf,\n            (self.rate, self.low, u),\n            result_shape=result_shape\n        )\n        return result.astype(jnp.result_type(int))\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Predictive Forecasting - Python\nDESCRIPTION: Uses NumPyro's Predictive utility to generate forecasts for the next 34 time steps using the trained model samples.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictive = Predictive(sgt, samples, return_sites=[\"y_forecast\"])\nforecast_marginal = predictive(random.PRNGKey(1), y_train, seasonality=38, future=34)[\n    \"y_forecast\"\n]\n```\n\n----------------------------------------\n\nTITLE: Implementing Latent Gaussian Process Model with Binary Outcomes in Python\nDESCRIPTION: This function defines a latent Gaussian process model with binary outcomes. It supports two methods for likelihood evaluation: 'circulant' using CirculantNormal distribution and 'cholesky' using MultivariateNormal distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/circulant_gp.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef model(x: jnp.ndarray, y: jnp.ndarray = None, *, method: str):\n    \"\"\"\n    Latent Gaussian process model with binary outcomes.\n\n    Args:\n        x: Observation grid.\n        y: Binary outcomes.\n        method: Likelihood evaluation method.\n    \"\"\"\n    n = x.size\n\n    # Sample parameters and evaluate the kernel.\n    sigma = numpyro.sample(\"sigma\", dist.HalfNormal())\n    length_scale = numpyro.sample(\"length_scale\", dist.InverseGamma(5, 25))\n    eps = 1e-4\n\n    if method == \"circulant\":\n        # We can evaluate the rfft of the covariance matrix directly. This both saves us\n        # some computation and is more numerically stable.\n        nrfft = n // 2 + 1\n        k = jnp.arange(nrfft)\n        covariance_rfft = (\n            sigma**2\n            * length_scale\n            * jnp.sqrt(2 * jnp.pi)\n            * jnp.exp(-2 * (jnp.pi * k * length_scale / n) ** 2)\n        ) + eps\n        zdist = dist.CirculantNormal(jnp.zeros(n), covariance_rfft=covariance_rfft)\n    elif method == \"cholesky\":\n        # Evaluate the covariance matrix.\n        distance = jnp.abs(x[:, None] - x)\n        distance = jnp.minimum(distance, n - distance)\n        covariance_matrix = sigma**2 * jnp.exp(\n            -(distance**2) / (2 * length_scale**2)\n        ) + eps * jnp.eye(n)\n        zdist = dist.MultivariateNormal(covariance_matrix=covariance_matrix)\n    z = numpyro.sample(\"z\", zdist)\n\n    with numpyro.plate(\"n\", n):\n        numpyro.sample(\"y\", dist.BernoulliLogits(z), obs=y)\n```\n\n----------------------------------------\n\nTITLE: Comparing Truncated and Untruncated Distributions in NumPyro\nDESCRIPTION: This snippet demonstrates how to use the 'condition' handler to push the truncation point to infinity, effectively creating an untruncated version of the model for comparison.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmodel_without_truncation = numpyro.handlers.condition(\n    truncated_normal_model,\n    {\"high\": float(\"inf\")},\n)\nestimates = mcmc.get_samples().copy()\nestimates.pop(\"high\")  # Drop to make sure these are not used\npred = Predictive(\n    model_without_truncation,\n    posterior_samples=estimates,\n)\npred_samples = pred(PRED_RNG, num_observations=1000)\n\n# thin the samples for a faster histogram\nsamples_thinned = pred_samples[\"x\"].ravel()[::1000]\n```\n\n----------------------------------------\n\nTITLE: Predicting Membership Using Discrete Inference in NumPyro\nDESCRIPTION: This snippet demonstrates how to use the infer_discrete handler along with trace and replay to predict class membership. It sets up a MAP classifier and shows how to apply it to new data.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrained_global_guide = handlers.substitute(\n    global_guide, global_svi_result.params\n)  # substitute trained params\nguide_trace = handlers.trace(trained_global_guide).get_trace(data)  # record the globals\ntrained_model = handlers.replay(model, trace=guide_trace)  # replay the globals\n\n\ndef classifier(data, temperature=0, rng_key=None):\n    inferred_model = infer_discrete(\n        trained_model, temperature=temperature, first_available_dim=-2, rng_key=rng_key\n    )  # set first_available_dim to avoid conflict with data plate\n    seeded_inferred_model = handlers.seed(inferred_model, random.PRNGKey(0))\n    trace = handlers.trace(seeded_inferred_model).get_trace(data)\n    return trace[\"assignment\"][\"value\"]\n\n\nprint(classifier(data))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Posterior Samples for Gaussian Process Model in Python\nDESCRIPTION: This code snippet visualizes the posterior samples for the Gaussian Process model, comparing the results of both circulant and cholesky methods with the synthetic data.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/circulant_gp.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots()\nplot_data(x, trace, ax=ax)\nfor method, samples in samples_by_method.items():\n    lower, median, upper = jnp.percentile(\n        expit(samples[\"z\"]), jnp.array([5, 50.0, 95]), axis=0\n    )\n    (line,) = ax.plot(x, median, label=method)\n    ax.fill_between(x, lower, upper, color=line.get_color(), alpha=0.2)\n\nax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.2), ncol=2)\nfig.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Comparing Manual and Auto-Generated Guides\nDESCRIPTION: Demonstrates equivalence between manually defined and automatically generated guides using NumPyro's AutoNormal, including state initialization and validation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom numpyro.infer.autoguide import AutoNormal\n\ndef create_svi_object(guide):\n    SVI(\n        model=tbip.model,\n        guide=guide,\n        optim=adam(exponential_decay(learning_rate, num_steps, decay_rate)),\n        loss=TraceMeanField_ELBO(),\n    )\n\n    Y_batch, I_batch, D_batch = tbip.get_batch(\n        random.PRNGKey(1), counts, author_indices\n    )\n\n    svi_state = svi_batch.init(\n        random.PRNGKey(0), Y_batch=Y_batch, d_batch=D_batch, i_batch=I_batch\n    )\n\n    return svi_state\n\nsvi_state_manualguide = create_svi_object(guide=tbip.guide)\n\nautoguide = AutoNormal(\n    model=tbip.model,\n    init_loc_fn={\"beta\": initial_objective_topic_loc, \"theta\": initial_document_loc},\n)\nsvi_state_autoguide = create_svi_object(guide=autoguide)\n\nassert svi_state_manualguide[0][1][0].keys() == svi_state_autoguide[0][1][0].keys()\n\nfor key in svi_state_manualguide[0][1][0].keys():\n    assert jnp.all(\n        svi_state_manualguide[0][1][0][key] == svi_state_autoguide[0][1][0][key]\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining the Complete Case Analysis Model in NumPyro\nDESCRIPTION: Creates a Bayesian model for complete case analysis which only uses observations where all data is available. This approach typically leads to biased estimates when data are MAR.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef ccmodel(A, B, Y):\n    ntotal = A.shape[0]\n    # get parameters of outcome model\n    b_A = sample(\"b_A\", dist.Normal(0, 2.5))\n    b_B = sample(\"b_B\", dist.Normal(0, 2.5))\n    s_Y = sample(\"s_Y\", dist.HalfCauchy(2.5))\n\n    with numpyro.plate(\"obs\", ntotal):\n        ### outcome model\n        eta_Y = b_A * A + b_B * B\n        sample(\"obs_Y\", dist.Normal(eta_Y, s_Y), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: Implementing FoldedStudentT Distribution in NumPyro\nDESCRIPTION: This code defines a custom FoldedStudentT distribution using the FoldedDistribution class in NumPyro, allowing for folding of the Student's t-distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef FoldedStudentT(df, loc=0.0, scale=1.0):\n    return FoldedDistribution(StudentT(df, loc=loc, scale=scale))\n```\n\n----------------------------------------\n\nTITLE: Implementing Right-Truncated Normal Distribution in NumPyro\nDESCRIPTION: This snippet defines the RightTruncatedNormal distribution class, including methods for sampling, log probability calculation, and inverse cumulative distribution function.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass RightTruncatedNormal(Distribution):\n    \"\"\"\n    A truncated Normal distribution.\n    :param numpy.ndarray loc: location parameter of the untruncated normal\n    :param numpy.ndarray scale: scale parameter of the untruncated normal\n    :param numpy.ndarray high: point at which the truncation happens\n    \"\"\"\n\n    arg_constraints = {\n        \"loc\": constraints.real,\n        \"scale\": constraints.positive,\n        \"high\": right_extended_real,\n    }\n    reparametrized_params = [\"loc\", \"scale\", \"high\"]\n\n    def __init__(self, loc=0.0, scale=1.0, high=float(\"inf\"), validate_args=True):\n        batch_shape = lax.broadcast_shapes(\n            jnp.shape(loc),\n            jnp.shape(scale),\n            jnp.shape(high),\n        )\n        self.loc, self.scale, self.high = promote_shapes(loc, scale, high)\n        super().__init__(batch_shape, validate_args=validate_args)\n\n    def log_prob(self, value):\n        log_m = norm.logcdf(self.high, self.loc, self.scale)\n        log_p = norm.logpdf(value, self.loc, self.scale)\n        return jnp.where(value < self.high, log_p - log_m, -jnp.inf)\n\n    def sample(self, key, sample_shape=()):\n        shape = sample_shape + self.batch_shape\n        minval = jnp.finfo(jnp.result_type(float)).tiny\n        u = random.uniform(key, shape, minval=minval)\n        return self.icdf(u)\n\n    def icdf(self, u):\n        m = norm.cdf(self.high, self.loc, self.scale)\n        return self.loc + self.scale * ndtri(m * u)\n\n    @constraints.dependent_property\n    def support(self):\n        return constraints.less_than(self.high)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data for Gaussian Process Model in Python\nDESCRIPTION: This code snippet generates synthetic data for the Gaussian Process model using the circulant method. It uses numpyro handlers to specify parameters, set a random seed, and record the model execution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/circulant_gp.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Sample from the prior predictive.\nwith (\n    numpyro.handlers.trace() as trace,\n    numpyro.handlers.substitute(data={\"sigma\": 2, \"length_scale\": 5}),\n    numpyro.handlers.seed(rng_seed=9),\n):\n    x = jnp.arange(64)\n    model(x, method=\"circulant\")\ny = trace[\"y\"][\"value\"]\n```\n\n----------------------------------------\n\nTITLE: Applying Classifier to New Data in NumPyro\nDESCRIPTION: This code applies the previously defined classifier to new data and visualizes the results. It demonstrates how to generate and plot MAP assignments for a range of input values.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnew_data = jnp.arange(-3, 15, 0.1)\nassignment = classifier(new_data)\nplt.figure(figsize=(8, 2), dpi=100).set_facecolor(\"white\")\nplt.plot(new_data, assignment)\nplt.title(\"MAP assignment\")\nplt.xlabel(\"data value\")\nplt.ylabel(\"class assignment\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Improved Truncated Normal Model in NumPyro\nDESCRIPTION: This snippet defines an improved version of the truncated normal model that addresses divergences by making the 'high' parameter dependent on the observations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef truncated_normal_model_2(num_observations, x=None):\n    loc = numpyro.sample(\"loc\", dist.Normal())\n    scale = numpyro.sample(\"scale\", dist.LogNormal())\n    if x is None:\n        high = numpyro.sample(\"high\", dist.Normal())\n    else:\n        # high is greater or equal to the max value in x:\n        delta = numpyro.sample(\"delta\", dist.HalfNormal())\n        high = numpyro.deterministic(\"high\", delta + x.max())\n\n    with numpyro.plate(\"observations\", num_observations):\n        numpyro.sample(\"x\", RightTruncatedNormal(loc, scale, high), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Gaussian Mixture Model Density\nDESCRIPTION: Creates a plot of the density function for the two-component Gaussian mixture model, showing individual components, their mixture, and the original data points.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nX = jnp.arange(-3, 15, 0.1)\nY1 = weights[0] * scipy.stats.norm.pdf((X - locs[0]) / scale)\nY2 = weights[1] * scipy.stats.norm.pdf((X - locs[1]) / scale)\n\nplt.figure(figsize=(10, 4), dpi=100).set_facecolor(\"white\")\nplt.plot(X, Y1, \"r-\")\nplt.plot(X, Y2, \"b-\")\nplt.plot(X, Y1 + Y2, \"k--\")\nplt.plot(data, jnp.zeros(len(data)), \"k*\")\nplt.title(\"Density of two-component mixture model\")\nplt.ylabel(\"probability density\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Partially Correlated Model with Structured Mass Matrix\nDESCRIPTION: Shows how to use structured mass matrices for partially correlated variables to improve sampling efficiency while maintaining computational tractability.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrho = 0.9\ncov = jnp.array([[10.0, rho], [rho, 0.1]])\n\ndef partially_correlated_model():\n    x1 = numpyro.sample(\n        \"x1\", dist.MultivariateNormal(jnp.zeros(2), covariance_matrix=cov)\n    )\n    x2 = numpyro.sample(\n        \"x2\", dist.MultivariateNormal(jnp.zeros(2), covariance_matrix=cov)\n    )\n    numpyro.sample(\"y\", dist.Normal(jnp.zeros(100), 1.0))\n    numpyro.sample(\"obs\", dist.Normal(x1 - x2, 0.1), jnp.ones(2))\n```\n\n----------------------------------------\n\nTITLE: Basic Multivariate Normal Model with Dense Mass Matrix\nDESCRIPTION: Demonstrates the impact of dense_mass parameter on sampling highly correlated variables using a simple multivariate normal model.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrho = 0.9999\ncov = jnp.array([[10.0, rho], [rho, 0.1]])\n\ndef mvn_model():\n    numpyro.sample(\"x\", dist.MultivariateNormal(jnp.zeros(2), covariance_matrix=cov))\n\nprint(\"dense_mass = False (bad r_hat)\")\nrun_inference(mvn_model, dense_mass=False, max_tree_depth=3)\n\nprint(\"dense_mass = True (good r_hat)\")\nrun_inference(mvn_model, dense_mass=True, max_tree_depth=3)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing COVTYPE Dataset for Logistic Regression\nDESCRIPTION: Loads the COVTYPE dataset, normalizes features, adds an intercept column, and converts the multi-class labels to binary classification task based on the most frequent class.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/logistic_regression.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n_, fetch = load_dataset(COVTYPE, shuffle=False)\nfeatures, labels = fetch()\n\n# normalize features and add intercept\nfeatures = (features - features.mean(0)) / features.std(0)\nfeatures = jnp.hstack([features, jnp.ones((features.shape[0], 1))])\n\n# make binary feature\n_, counts = np.unique(labels, return_counts=True)\nspecific_category = jnp.argmax(counts)\nlabels = labels == specific_category\n\nN, dim = features.shape\nprint(\"Data shape:\", features.shape)\nprint(\n    \"Label distribution: {} has label 1, {} has label 0\".format(\n        labels.sum(), N - labels.sum()\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Making Population Predictions with NumPyro\nDESCRIPTION: Generates predictions using the fitted model and calculates prediction intervals. Creates time series predictions up to 200 years.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nts_pred = jnp.tile(jnp.linspace(0, 200, 1000), (n_datasets, 1))\npop_pred = Predictive(model, mcmc.get_samples())(PRNGKey(1041140), ts_pred, y_init)[\"y\"]\nmu = jnp.mean(pop_pred, 0)\npi = jnp.percentile(pop_pred, jnp.array([10, 90]), 0)\n\nprint(f\"True params mean: {sample['theta'][0]}\")\nprint(f\"Estimated params mean: {jnp.mean(mcmc.get_samples()['theta'], axis=0)}\")\n```\n\n----------------------------------------\n\nTITLE: RightTruncatedNormal Log Probability Method\nDESCRIPTION: Implementation of the log probability calculation for the truncated distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef log_prob(self, value):\n    log_m = norm.logcdf(self.high, self.loc, self.scale)\n    log_p = norm.logpdf(value, self.loc, self.scale)\n    return jnp.where(value < self.high, log_p - log_m, -jnp.inf)\n```\n\n----------------------------------------\n\nTITLE: Initializing TBIP Model and SVI Object for Training\nDESCRIPTION: Sets up the TBIP model instance and Stochastic Variational Inference (SVI) object for training. It configures the optimizer with Adam and exponential learning rate decay, and initializes the model parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Initialize the model\nfrom jax import jit\nfrom optax import adam, exponential_decay\n\nfrom numpyro.infer import SVI, TraceMeanField_ELBO\n\nnum_steps = 50000\nbatch_size = 512  # Large batches are recommended\nlearning_rate = 0.01\ndecay_rate = 0.01\n\ntbip = TBIP(\n    N=num_authors,\n    D=num_documents,\n    K=num_topics,\n    V=num_words,\n    batch_size=batch_size,\n    init_mu_theta=initial_document_loc,\n    init_mu_beta=initial_objective_topic_loc,\n)\n\nsvi_batch = SVI(\n    model=tbip.model,\n    guide=tbip.guide,\n    optim=adam(exponential_decay(learning_rate, num_steps, decay_rate)),\n    loss=TraceMeanField_ELBO(),\n)\n\n# Compile update function for faster training\nsvi_batch_update = jit(svi_batch.update)\n\n# Get initial batch. This informs the dimension of arrays and ensures they are\n# consistent with dimensions (N, D, K, V) defined above.\nY_batch, I_batch, D_batch = tbip.get_batch(random.PRNGKey(1), counts, author_indices)\n\n# Initialize the parameters using initial batch\nsvi_state = svi_batch.init(\n    random.PRNGKey(0), Y_batch=Y_batch, d_batch=D_batch, i_batch=I_batch\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Bayesian Logistic Regression Model with Imputation in Python\nDESCRIPTION: Defines a NumPyro model for Bayesian logistic regression with age imputation based on passenger titles.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef model(\n    age, pclass, title, sex, sibsp, parch, embarked, survived=None, bayesian_impute=True\n):\n    b_pclass = numpyro.sample(\"b_Pclass\", dist.Normal(0, 1).expand([3]))\n    b_title = numpyro.sample(\"b_Title\", dist.Normal(0, 1).expand([5]))\n    b_sex = numpyro.sample(\"b_Sex\", dist.Normal(0, 1).expand([2]))\n    b_sibsp = numpyro.sample(\"b_SibSp\", dist.Normal(0, 1).expand([2]))\n    b_parch = numpyro.sample(\"b_Parch\", dist.Normal(0, 1).expand([3]))\n    b_embarked = numpyro.sample(\"b_Embarked\", dist.Normal(0, 1).expand([3]))\n\n    # impute age by Title\n    isnan = np.isnan(age)\n    age_nanidx = np.nonzero(isnan)[0]\n    if bayesian_impute:\n        age_mu = numpyro.sample(\"age_mu\", dist.Normal(0, 1).expand([5]))\n        age_mu = age_mu[title]\n        age_sigma = numpyro.sample(\"age_sigma\", dist.Normal(0, 1).expand([5]))\n        age_sigma = age_sigma[title]\n        age_impute = numpyro.sample(\n            \"age_impute\",\n            dist.Normal(age_mu[age_nanidx], age_sigma[age_nanidx]).mask(False),\n        )\n        age = jnp.asarray(age).at[age_nanidx].set(age_impute)\n        numpyro.sample(\"age\", dist.Normal(age_mu, age_sigma), obs=age)\n    else:\n        # fill missing data by the mean of ages for each title\n        age_impute = age_mean_by_title[title][age_nanidx]\n        age = jnp.asarray(age).at[age_nanidx].set(age_impute)\n\n    a = numpyro.sample(\"a\", dist.Normal(0, 1))\n    b_age = numpyro.sample(\"b_Age\", dist.Normal(0, 1))\n    logits = a + b_age * age\n    logits = logits + b_title[title] + b_pclass[pclass] + b_sex[sex]\n    logits = logits + b_sibsp[sibsp] + b_parch[parch] + b_embarked[embarked]\n    numpyro.sample(\"survived\", dist.Bernoulli(logits=logits), obs=survived)\n```\n\n----------------------------------------\n\nTITLE: Forecast Visualization - Python\nDESCRIPTION: Creates a plot showing the original data, mean predictions, and 90% highest posterior density interval (HPDI) for the forecasted values.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(8, 4))\nplt.plot(lynx[\"time\"], data)\nt_future = lynx[\"time\"][80:]\nhpd_low, hpd_high = hpdi(forecast_marginal)\nplt.plot(t_future, y_pred, lw=2)\nplt.fill_between(t_future, hpd_low, hpd_high, alpha=0.3)\nplt.title(\"Forecasting lynx dataset with SGT model (90% HPDI)\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Applying LocScaleReparam to Horseshoe Regression Model\nDESCRIPTION: Uses NumPyro's LocScaleReparam to automatically reparameterize the horseshoe regression model, achieving an equivalent coordinate system to the manual approach.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom numpyro.infer.reparam import LocScaleReparam\n\n# LocScaleReparam with centered=0 fully \"decenters\" the prior over betas.\nconfig = {\"betas\": LocScaleReparam(centered=0)}\n# The coordinate system of this model is equivalent to that in _rep_hs_model1 above.\n_rep_hs_model2 = numpyro.handlers.reparam(_unrep_hs_model, config=config)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions for New Data in NumPyro\nDESCRIPTION: Uses the Predictive class to generate predictions for a new school based on the posterior distribution from the MCMC run.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from numpyro.infer import Predictive\n\n>>> # New School\n... def new_school():\n...     mu = numpyro.sample('mu', dist.Normal(0, 5))\n...     tau = numpyro.sample('tau', dist.HalfCauchy(5))\n...     return numpyro.sample('obs', dist.Normal(mu, tau))\n\n>>> predictive = Predictive(new_school, mcmc.get_samples())\n>>> samples_predictive = predictive(random.PRNGKey(1))\n>>> print(np.mean(samples_predictive['obs']))  # doctest: +SKIP\n3.9886456\n```\n\n----------------------------------------\n\nTITLE: Running SVI with Full Guide in NumPyro\nDESCRIPTION: This snippet sets up and runs Stochastic Variational Inference (SVI) using the full guide. It includes optimization settings and uses the TraceEnum_ELBO loss function.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\noptim, gradient_norms = hook_optax(optax.adam(learning_rate=0.2, b1=0.8, b2=0.99))\nelbo = TraceEnum_ELBO()\nfull_svi = SVI(model, full_guide, optim, loss=elbo)\n\n\nfull_svi_result = full_svi.run(random.PRNGKey(0), 200 if not smoke_test else 2, data)\n```\n\n----------------------------------------\n\nTITLE: Implementing CDF and ICDF Methods for Truncated Poisson Distribution in JAX\nDESCRIPTION: These methods implement the cumulative distribution function (CDF) and various inverse CDF approaches for a left-truncated Poisson distribution using JAX. The implementation includes a basic iterative approach, a faster binning method, and a SciPy callback version.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef body_fn(val):\n    n, cdf = val\n    n_new = jnp.where(cdf < u, n + 1, n)\n    return n_new, self.cdf(n_new)\n\nlow = self.low * jnp.ones_like(u)\ncdf = self.cdf(low)\nn, _ = lax.while_loop(cond_fn, body_fn, (low, cdf))\nreturn n.astype(jnp.result_type(int))\n\ndef icdf_faster(self, u):\n    num_bins = 200  # Choose a reasonably large value\n    bins = jnp.arange(num_bins)\n    cdf = self.cdf(bins)\n    indices = jnp.searchsorted(cdf, u)\n    return bins[indices]\n\ndef icdf_scipy(self, u):\n    result_shape = jax.ShapeDtypeStruct(u.shape, jnp.result_type(float))\n    result = jax.experimental.host_callback.call(\n        scipy_truncated_poisson_icdf,\n        (self.rate, self.low, u),\n        result_shape=result_shape,\n    )\n    return result.astype(jnp.result_type(int))\n\ndef cdf(self, value):\n    m = 1 - poisson.cdf(self.low - 1, self.rate)\n    f = poisson.cdf(value, self.rate) - poisson.cdf(self.low - 1, self.rate)\n    return jnp.where(value >= self.low, f / m, 0)\n\n@constraints.dependent_property(is_discrete=True)\ndef support(self):\n    return constraints.integer_greater_than(self.low - 1)\n```\n\n----------------------------------------\n\nTITLE: Predicting Survival without Bayesian Imputation in NumPyro\nDESCRIPTION: This snippet shows how to make predictions without using Bayesian imputation for missing values. It runs the MCMC sampler, makes predictions, and compares the results to the previous model with imputation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmcmc.run(random.PRNGKey(2), **data, survived=survived, bayesian_impute=False)\nposterior_1 = mcmc.get_samples()\nsurvived_pred_1 = Predictive(model, posterior_1)(random.PRNGKey(2), **data)[\"survived\"]\nsurvived_pred_1 = (survived_pred_1.mean(axis=0) >= 0.5).astype(jnp.uint8)\nprint(\"Accuracy:\", (survived_pred_1 == survived).sum() / survived.shape[0])\nconfusion_matrix = pd.crosstab(\n    pd.Series(survived, name=\"actual\"), pd.Series(survived_pred_1, name=\"predict\")\n)\nconfusion_matrix / confusion_matrix.sum(axis=1)\nconfusion_matrix = pd.crosstab(\n    pd.Series(survived, name=\"actual\"), pd.Series(survived_pred_1, name=\"predict\")\n)\nconfusion_matrix / confusion_matrix.sum(axis=1)\n```\n\n----------------------------------------\n\nTITLE: Reporting Performance Speedup for Circulant Method in Python\nDESCRIPTION: This code snippet calculates and reports the speedup achieved by using the CirculantNormal distribution compared to the MultivariateNormal distribution for each parameter in the Gaussian Process model.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/circulant_gp.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Report the speed up due to using the `CirculantNormal`.\nspeedups = jax.tree.map(jnp.divide, *n_eff_per_second_by_method.values())\nfor site, speedup in speedups.items():\n    print(\n        f\"speedup for `{site}`: min = {speedup.min():.2f}, \"\n        f\"mean = {speedup.mean():.2f}, max = {speedup.max():.2f}\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Optimized icdf Implementation Using Faster Search Algorithm\nDESCRIPTION: An alternative, faster implementation of the inverse CDF for the LeftTruncatedPoisson distribution. This approach pre-computes the CDF for a fixed number of bins and uses binary search to find the correct value, improving performance over the brute-force method.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef icdf_faster(self, u):\n    num_bins = 200 # Choose a reasonably large value\n    bins = jnp.arange(num_bins)\n    cdf = self.cdf(bins)\n    indices = jnp.searchsorted(cdf, u)\n    return bins[indices]\n```\n\n----------------------------------------\n\nTITLE: Running MCMC for the Imputation Model\nDESCRIPTION: Sets up and runs MCMC sampling for the imputation model using the NUTS sampler, then prints a summary of the posterior distributions including both imputation and outcome parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimpkernel = NUTS(impmodel)\nimpmcmc = MCMC(impkernel, num_warmup=250, num_samples=750)\nimpmcmc.run(mcmc_key, Aobs, B, Y)\nimpmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Running MCMC for Complete Case Analysis\nDESCRIPTION: Sets up and runs MCMC sampling for the complete case model using the NUTS sampler, then prints a summary of the posterior distributions.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncckernel = NUTS(ccmodel)\nccmcmc = MCMC(cckernel, num_warmup=250, num_samples=750)\nccmcmc.run(mcmc_key, Acc, Bcc, Ycc)\nccmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Initializing TBIP Parameters with Non-Negative Matrix Factorization\nDESCRIPTION: Pre-initializes the document and topic parameters using scikit-learn's Non-Negative Matrix Factorization (NMF) to provide a better starting point for the TBIP model than random initialization.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Fit NMF to be used as initialization for TBIP\nfrom sklearn.decomposition import NMF\n\nif pre_initialize_parameters:\n    nmf_model = NMF(\n        n_components=num_topics, init=\"random\", random_state=0, max_iter=500\n    )\n    # Define initialization arrays\n    initial_document_loc = jnp.log(\n        jnp.array(np.float32(nmf_model.fit_transform(counts) + 1e-2))\n    )\n    initial_objective_topic_loc = jnp.log(\n        jnp.array(np.float32(nmf_model.components_ + 1e-2))\n    )\nelse:\n    rng1, rng2 = random.split(rng_seed, 2)\n    initial_document_loc = random.normal(rng1, shape=(num_documents, num_topics))\n    initial_objective_topic_loc = random.normal(rng2, shape=(num_topics, num_words))\n```\n\n----------------------------------------\n\nTITLE: Extracting Learned GMM Parameters\nDESCRIPTION: Retrieves the MAP estimates for the mixture model parameters: weights, component locations, and scale.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmap_estimates = global_svi_result.params\nweights = map_estimates[\"weights_auto_loc\"]\nlocs = map_estimates[\"locs_auto_loc\"]\nscale = map_estimates[\"scale_auto_loc\"]\nprint(f\"weights = {weights}\")\nprint(f\"locs = {locs}\")\nprint(f\"scale = {scale}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Simulated Data with Correlated Binary Covariates\nDESCRIPTION: Simulates data where Z is a latent variable that creates dependence between binary covariates A and B, which both affect the outcome Y.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nb_A = 0.25\nb_B = 0.25\ns_Y = 0.25\nZ = random.normal(simkeys[0], (nsim,))\nA = random.bernoulli(simkeys[1], expit(Z))\nB = random.bernoulli(simkeys[2], expit(Z))\nY = A * b_A + B * b_B + s_Y * random.normal(simkeys[3], (nsim,))\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Function for NUTS Inference\nDESCRIPTION: Creates a helper function to run NUTS inference with specified parameters and print summary statistics, focusing on the maximum r_hat value for each variable.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef run_inference(\n    model, num_warmup=1000, num_samples=1000, max_tree_depth=10, dense_mass=False\n):\n    kernel = NUTS(model, max_tree_depth=max_tree_depth, dense_mass=dense_mass)\n    mcmc = MCMC(\n        kernel,\n        num_warmup=num_warmup,\n        num_samples=num_samples,\n        num_chains=1,\n        progress_bar=False,\n    )\n    mcmc.run(random.PRNGKey(0))\n    summary_dict = summary(mcmc.get_samples(), group_by_chain=False)\n\n    # print the largest r_hat for each variable\n    for k, v in summary_dict.items():\n        spaces = \" \" * max(12 - len(k), 0)\n        print(\"[{}] {} \\t max r_hat: {:.4f}\".format(k, spaces, np.max(v[\"r_hat\"])))\n```\n\n----------------------------------------\n\nTITLE: Forecast Error Calculation - Python\nDESCRIPTION: Calculates the symmetric Mean Absolute Percentage Error (sMAPE) and root mean square error (RMSE) of the predictions.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ny_pred = jnp.mean(forecast_marginal, axis=0)\nsMAPE = jnp.mean(jnp.abs(y_pred - y_test) / (y_pred + y_test)) * 200\nmsqrt = jnp.sqrt(jnp.mean((y_pred - y_test) ** 2))\nprint(\"sMAPE: {:.2f}, rmse: {:.2f}\".format(sMAPE, msqrt))\n```\n\n----------------------------------------\n\nTITLE: Ordinal Regression with Dirichlet Prior\nDESCRIPTION: Implements ordinal regression using Dirichlet prior to induce cutpoints via SimplexToOrderedTransform.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/ordinal_regression.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef model4(X, Y, nclasses, concentration, anchor_point=0.0):\n    b_X_eta = sample(\"b_X_eta\", Normal(0, 5))\n\n    with handlers.reparam(config={\"c_y\": TransformReparam()}):\n        c_y = sample(\n            \"c_y\",\n            TransformedDistribution(\n                Dirichlet(concentration),\n                transforms.SimplexToOrderedTransform(anchor_point),\n            ),\n        )\n    with numpyro.plate(\"obs\", X.shape[0]):\n        eta = X * b_X_eta\n        sample(\"Y\", OrderedLogistic(eta, c_y), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: RightTruncatedNormal Initialization Method\nDESCRIPTION: Implementation of the __init__ method with parameter validation and shape broadcasting.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, loc=0.0, scale=1.0, high=float(\"inf\"), validate_args=None):\n    batch_shape = lax.broadcast_shapes(\n        jnp.shape(loc),\n        jnp.shape(scale),\n        jnp.shape(high),\n    )\n    self.loc, self.scale, self.high = promote_shapes(loc, scale, high)\n    super().__init__(batch_shape, validate_args=validate_args)\n```\n\n----------------------------------------\n\nTITLE: RightTruncatedNormal Attributes Implementation\nDESCRIPTION: Implementation of class attributes and support property for RightTruncatedNormal distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass RightTruncatedNormal(Distribution):\n    arg_constraints = {\n        \"loc\": constraints.real,\n        \"scale\": constraints.positive,\n        \"high\": right_extended_real,\n    }\n    reparametrized_params = [\"loc\", \"scale\", \"high\"]\n    \n    @constraints.dependent_property\n    def support(self):\n        return constraints.lower_than(self.high)\n```\n\n----------------------------------------\n\nTITLE: Defining Manually Reparameterized Horseshoe Regression Model\nDESCRIPTION: Implements a manually reparameterized version of the horseshoe regression model, changing the coordinate system to improve HMC performance.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _rep_hs_model1(X, Y):\n    lambdas = numpyro.sample(\"lambdas\", dist.HalfCauchy(jnp.ones(X.shape[1])))\n    tau = numpyro.sample(\"tau\", dist.HalfCauchy(jnp.ones(1)))\n    unscaled_betas = numpyro.sample(\n        \"unscaled_betas\", dist.Normal(scale=jnp.ones(X.shape[1]))\n    )\n    scaled_betas = numpyro.deterministic(\"betas\", tau * lambdas * unscaled_betas)\n    mean_function = jnp.dot(X, scaled_betas)\n    numpyro.sample(\"Y\", dist.Normal(mean_function, 0.05), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Dataset with Missing Values using NumPyro in Python\nDESCRIPTION: This code generates a synthetic dataset using the Predictive mode from NumPyro and applies a mask to simulate missing values. It ensures that initial values are non-missing.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# take a single sample that will be our synthetic data\nsample = Predictive(model, num_samples=1)(PRNGKey(100), ts, z_inits)\ndata = sample[\"y\"][0]\n\n# create a mask that will add missing values to the data\nmissing_obs_mask = jax.random.choice(\n    PRNGKey(1),\n    jnp.array([True, False]),\n    shape=data.shape,\n    p=jnp.array([p_missing, 1 - p_missing]),\n)\n# make sure that initial values are not missing\nmissing_obs_mask = missing_obs_mask.at[:, 0, :].set(False)\n\n# data with missing values\ndata = data.at[missing_obs_mask].set(jnp.nan)\n```\n\n----------------------------------------\n\nTITLE: Updated Truncated Poisson Model with Configurable Categories\nDESCRIPTION: A modified version of the truncated Poisson model that accepts a parameter 'k' to define the number of categories in the discrete prior for the truncation point, allowing consistency with observed data.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef truncated_poisson_model(num_observations, x=None, k=5):\n    zeros = jnp.zeros((k,))\n    low = numpyro.sample(\"low\", dist.Categorical(logits=zeros))\n    rate = numpyro.sample(\"rate\", dist.LogNormal(1, 1))\n    with numpyro.plate(\"observations\", num_observations):\n        numpyro.sample(\"x\", LeftTruncatedPoisson(rate, low), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Generating Initial Conditions for ODE Simulation in Python\nDESCRIPTION: This code generates an array of initial conditions for the ODE simulation using JAX's numpy implementation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# generate an array with initial conditons\nz_inits = jnp.array(\n    [jnp.linspace(y0_min, y0_max, n_datasets), jnp.linspace(y0_max, y0_min, n_datasets)]\n).T\n\nprint(f\"Initial conditons are: \\n {z_inits}\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing MCMC Trace Plot in NumPyro\nDESCRIPTION: This code creates a trace plot of the location parameters during NUTS inference. It illustrates the MCMC chain's behavior and potential mixing issues due to the multimodal posterior.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(8, 3), dpi=100).set_facecolor(\"white\")\nplt.plot(X, color=\"red\")\nplt.plot(Y, color=\"blue\")\nplt.xlabel(\"NUTS step\")\nplt.ylabel(\"loc\")\nplt.title(\"Trace plot of loc parameter during NUTS inference\")\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Model with Overlapping Plates\nDESCRIPTION: Creates a model with overlapping plates to demonstrate how they are visualized.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef model():\n    plate1 = numpyro.plate(\"plate1\", 2, dim=-2)\n    plate2 = numpyro.plate(\"plate2\", 3, dim=-1)\n    with plate1:\n        x = numpyro.sample(\"x\", dist.Normal(0, 1))\n    with plate1, plate2:\n        y = numpyro.sample(\"y\", dist.Normal(x, 1))\n    with plate2:\n        numpyro.sample(\"z\", dist.Normal(y.sum(-2, keepdims=True), 1), obs=jnp.zeros(3))\n```\n\n----------------------------------------\n\nTITLE: Defining Unreparameterized Horseshoe Regression Model\nDESCRIPTION: Implements an unreparameterized horseshoe regression model where parameters explicitly depend on other parameters, potentially causing challenges for HMC.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _unrep_hs_model(X, Y):\n    lambdas = numpyro.sample(\"lambdas\", dist.HalfCauchy(jnp.ones(X.shape[1])))\n    tau = numpyro.sample(\"tau\", dist.HalfCauchy(jnp.ones(1)))\n    betas = numpyro.sample(\"betas\", dist.Normal(scale=tau * lambdas))\n    mean_function = jnp.dot(X, betas)\n    numpyro.sample(\"Y\", dist.Normal(mean_function, 0.05), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: Comparing Inferred Ages with Actual Data in Python\nDESCRIPTION: Compares the inferred age means by title with the actual statistical means from the training dataset.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.groupby(\"Title\")[\"Age\"].mean()\n```\n\n----------------------------------------\n\nTITLE: Defining a Folded Student-t Model in NumPyro\nDESCRIPTION: This snippet demonstrates how to create a model using the custom FoldedStudentT distribution in NumPyro, including parameters for degrees of freedom, location, and scale.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef folded_student_model(num_observations, x=None):\n    df = numpyro.sample(\"df\", dist.Gamma(6, 2))\n    loc = numpyro.sample(\"loc\", dist.Normal())\n    scale = numpyro.sample(\"scale\", dist.LogNormal())\n    with numpyro.plate(\"obs\", num_observations):\n        numpyro.sample(\"x\", FoldedStudentT(df, loc, scale), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Discrete Distribution Plotting Utility in Python\nDESCRIPTION: A utility function for visualizing discrete probability distributions as bar plots. It counts unique values in samples, normalizes them to probabilities, and creates a bar plot.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef discrete_distplot(samples, ax=None, **kwargs):\n    \"\"\"\n    Utility function for plotting the samples as a barplot.\n    \"\"\"\n    x, y = np.unique(samples, return_counts=True)\n    y = y / sum(y)\n    if ax is None:\n        ax = plt.gca()\n\n    ax.bar(x, y, **kwargs)\n    return ax\n```\n\n----------------------------------------\n\nTITLE: Creating Model Without Truncation in NumPyro\nDESCRIPTION: Defines a model variant without truncation by conditioning the truncation point (low parameter) to be zero, effectively removing the truncation effect.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nmodel_without_truncation = numpyro.handlers.condition(\n    truncated_poisson_model,\n    {\"low\": 0},\n)\npred = Predictive(model_without_truncation, posterior_samples=mcmc.get_samples())\npred_samples = pred(PRED_RNG, num_observations)\nthinned_samples = pred_samples[\"x\"][::500]\n```\n\n----------------------------------------\n\nTITLE: MCMC Inference with DiscreteHMCGibbs for Truncated Poisson Model\nDESCRIPTION: Performs MCMC inference using DiscreteHMCGibbs with NUTS for the truncated Poisson model, where k is set to the minimum observed value plus one to ensure model consistency.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nmcmc = MCMC(DiscreteHMCGibbs(NUTS(truncated_poisson_model)), **MCMC_KWARGS)\nmcmc.run(MCMC_RNG, num_observations, true_x, k=true_x.min() + 1)\nmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Generating Random Posterior Assignments in NumPyro\nDESCRIPTION: This snippet shows how to generate random posterior assignments by setting the temperature parameter to 1 in the classifier function. It also includes a visualization of the assignments near the class boundary.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(classifier(data, temperature=1, rng_key=random.PRNGKey(0)))\n\nnew_data = jnp.arange(5.5, 6.0, 0.005)\nassignment = classifier(new_data, temperature=1, rng_key=random.PRNGKey(0))\nplt.figure(figsize=(8, 2), dpi=100).set_facecolor(\"white\")\nplt.plot(new_data, assignment, \"x\", color=\"C0\")\nplt.title(\"Random posterior assignment\")\nplt.xlabel(\"data value\")\nplt.ylabel(\"class assignment\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining a Truncated Soft Laplace Model in NumPyro\nDESCRIPTION: This snippet shows how to create a model using the custom TruncatedSoftLaplace distribution in NumPyro, including parameters for location, scale, and truncation point.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef truncated_soft_laplace_model(num_observations, high, x=None):\n    loc = numpyro.sample(\"loc\", dist.Normal())\n    scale = numpyro.sample(\"scale\", dist.LogNormal())\n    with numpyro.plate(\"obs\", num_observations):\n        numpyro.sample(\"x\", TruncatedSoftLaplace(loc, scale, high=high), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Simulation Parameters for Synthetic Dataset Generation in Python\nDESCRIPTION: This code snippet defines parameters for generating synthetic datasets, including number of datasets, time ranges, number of points, and probability of missing values.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nn_datasets = 3  # int n_datasets: number of datasets to generate\nt_min = 100  # int t_min: minimal allowed length of the generated time array\nt_max = 200  # int t_min: maximal allowed length of the generated time array\nn_points_min = 80  # int n_points_min: minimal allowed number of points in a data set\nn_points_max = 120  # int n_points_max: maximal allowed number of points in a data set\ny0_min = 2.0  # float y0_min: minimal allowed value for initial conditions\ny0_max = 10.0  # float y0_max: maximal allowed value for initial conditions\np_missing = 0.1  # float p_missing: probability of having missing values\n```\n\n----------------------------------------\n\nTITLE: Simple Imputation MCMC Analysis in NumPyro\nDESCRIPTION: Runs MCMC sampling with NUTS sampler on the full dataset with simple imputation. Uses same number of samples as complete case analysis.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimpkernel = NUTS(impmodel)\nimpmcmc = MCMC(impkernel, num_warmup=250, num_samples=750)\nimpmcmc.run(mcmc_key, Aobs, B, Y)\nimpmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: RightTruncatedNormal Distribution Skeleton\nDESCRIPTION: Initial skeleton code for implementing a right-truncated normal distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass RightTruncatedNormal(Distribution):\n    # <class attributes>\n    def __init__(self):\n        pass\n    \n    def log_prob(self, value):\n        pass\n    \n    def sample(self, key, sample_shape=()):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Implementing TruncatedSoftLaplace Distribution in NumPyro\nDESCRIPTION: This code defines a custom TruncatedSoftLaplace distribution using the TruncatedDistribution class in NumPyro, allowing for truncation of the SoftLaplace distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef TruncatedSoftLaplace(\n    loc=0.0, scale=1.0, *, low=None, high=None, validate_args=None\n):\n    return TruncatedDistribution(\n        base_dist=SoftLaplace(loc, scale),\n        low=low,\n        high=high,\n        validate_args=validate_args,\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Model with Neural Network\nDESCRIPTION: Creates a model that incorporates a neural network using flax_module.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    lambda_base = numpyro.sample(\"lambda\", dist.Normal(0, 1))\n    net = flax_module(\"affine_net\", flax_nn.Dense(1), input_shape=(1,))\n    lambd = jnp.exp(net(jnp.expand_dims(lambda_base, -1)).squeeze(-1))\n    with numpyro.plate(\"N\", len(data)):\n        numpyro.sample(\"obs\", dist.Exponential(lambd), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Titanic Dataset in Python\nDESCRIPTION: Performs data preprocessing steps including clipping values, filling missing data, and extracting titles from names.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrain_df.SibSp.clip(0, 1, inplace=True)\ntrain_df.Parch.clip(0, 2, inplace=True)\ntrain_df.Embarked.fillna(\"S\", inplace=True)\n\ntrain_df[\"Title\"] = (\n    train_df.Name.str.split(\", \")\n    .str.get(1)\n    .str.split(\" \")\n    .str.get(0)\n    .apply(lambda x: x if x in [\"Mr.\", \"Miss.\", \"Mrs.\", \"Master.\"] else \"Misc.\")\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Model with Parameters\nDESCRIPTION: Creates a model using numpyro.param to define parameters with constraints.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    m = numpyro.param(\"m\", 0.0)\n    sd = numpyro.param(\"sd\", 1.0, constraint=constraints.positive)\n    lambd = numpyro.sample(\"lambda\", dist.LogNormal(m, sd))\n    with numpyro.plate(\"N\", len(data)):\n        numpyro.sample(\"obs\", dist.Exponential(lambd), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Predicting Membership by Enumerating in the Guide with NumPyro\nDESCRIPTION: This code defines a full guide that fits both global and local parameters for predicting class membership. It uses the config_enumerate decorator and handlers.block to keep learned values of global parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@config_enumerate\ndef full_guide(data):\n    # Global variables.\n    with handlers.block(\n        hide=[\"weights_auto_loc\", \"locs_auto_loc\", \"scale_auto_loc\"]\n    ):  # Keep our learned values of global parameters.\n        trained_global_guide(data)\n\n    # Local variables.\n    with numpyro.plate(\"data\", len(data)):\n        assignment_probs = numpyro.param(\n            \"assignment_probs\",\n            jnp.ones((len(data), K)) / K,\n            constraint=constraints.simplex,\n        )\n        numpyro.sample(\"assignment\", dist.Categorical(assignment_probs))\n```\n\n----------------------------------------\n\nTITLE: Running MCMC Inference for Known Truncation Model\nDESCRIPTION: Executes the MCMC inference for the model with known truncation point and prints a summary of the results.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nmcmc.run(MCMC_RNG, num_observations, true_x)\nmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for ODE Solving and MCMC in Python\nDESCRIPTION: This snippet imports necessary libraries for ODE solving, JAX operations, and NumPyro probabilistic programming. It also enables 64-bit precision for numerical stability.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\nimport matplotlib.pyplot as plt\n\nimport jax\nfrom jax.experimental.ode import odeint\nimport jax.numpy as jnp\nfrom jax.random import PRNGKey\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, Predictive, init_to_sample\n\n# Numerical instabilities may arise during ODE solving,\n# so one has sometimes to play around with solver settings,\n# change solver, or change numeric precision as we do here.\nnumpyro.enable_x64(True)\n```\n\n----------------------------------------\n\nTITLE: Improving Layout with Unflatten\nDESCRIPTION: Uses the unflatten method to improve the layout of the complex model visualization.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmace_graph.unflatten(stagger=2)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Gradient Norms During Training\nDESCRIPTION: Creates a plot showing the gradient norms for different parameters during training, which helps to monitor convergence.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10, 4), dpi=100).set_facecolor(\"white\")\nfor name, grad_norms in gradient_norms.items():\n    plt.plot(grad_norms, label=name)\nplt.xlabel(\"iters\")\nplt.ylabel(\"gradient norm\")\nplt.yscale(\"log\")\nplt.legend(loc=\"best\")\nplt.title(\"Gradient norms during SVI\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Testing Sets\nDESCRIPTION: This code splits the lynx dataset into training (first 80 values) and testing (last 34 values) sets.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ny_train, y_test = jnp.array(data[:80], dtype=jnp.float32), data[80:]\n```\n\n----------------------------------------\n\nTITLE: Generating Prior Samples from Truncated Poisson Model\nDESCRIPTION: Code to generate prior samples from the truncated Poisson model using NumPyro's Predictive utility. It specifies the number of observations and prior samples to generate.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# -- prior samples\nnum_observations = 1000\nnum_prior_samples = 100\nprior = Predictive(truncated_poisson_model, num_samples=num_prior_samples)\nprior_samples = prior(PRIOR_RNG, num_observations)\n```\n\n----------------------------------------\n\nTITLE: Defining Eight Schools Model in NumPyro\nDESCRIPTION: Implements the hierarchical model for the Eight Schools example using NumPyro distributions and primitives.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpyro\n>>> import numpyro.distributions as dist\n\n>>> # Eight Schools example\n... def eight_schools(J, sigma, y=None):\n...     mu = numpyro.sample('mu', dist.Normal(0, 5))\n...     tau = numpyro.sample('tau', dist.HalfCauchy(5))\n...     with numpyro.plate('J', J):\n...         theta = numpyro.sample('theta', dist.Normal(mu, tau))\n...         numpyro.sample('obs', dist.Normal(theta, sigma), obs=y)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Assignment Probabilities in NumPyro\nDESCRIPTION: This snippet extracts the assignment probabilities from the SVI result and creates a plot to visualize them. It helps in understanding how the model assigns data points to different mixture components.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nassignment_probs = full_svi_result.params[\"assignment_probs\"]\nplt.figure(figsize=(8, 3), dpi=100).set_facecolor(\"white\")\nplt.plot(\n    data,\n    assignment_probs[:, 0],\n    \"ro\",\n    label=f\"component with mean {locs[0]:0.2g}\",\n)\nplt.plot(\n    data,\n    assignment_probs[:, 1],\n    \"bo\",\n    label=f\"component with mean {locs[1]:0.2g}\",\n)\nplt.title(\"Mixture assignment probabilities\")\nplt.xlabel(\"data value\")\nplt.ylabel(\"assignment probability\")\nplt.legend(loc=\"center\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Parameter Pre-initialization for TBIP\nDESCRIPTION: Sets a boolean flag to control whether model parameters will be pre-initialized using non-negative matrix factorization, which is recommended for better ideological topic interpretation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npre_initialize_parameters = True\n```\n\n----------------------------------------\n\nTITLE: Prior Simulation for Truncated Normal Model\nDESCRIPTION: This code snippet shows how to perform prior simulation for the truncated normal model using the Predictive class in NumPyro.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhigh = 1.2\nnum_observations = 250\nnum_prior_samples = 100\n\nprior = Predictive(truncated_normal_model, num_samples=num_prior_samples)\nprior_samples = prior(PRIOR_RNG, num_observations, high)\n```\n\n----------------------------------------\n\nTITLE: NumPyro Setup and Dependencies\nDESCRIPTION: Comprehensive setup for NumPyro including necessary imports from JAX, NumPy, SciPy, and configuration of MCMC parameters and random number generation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson as sp_poisson\n\nimport jax\nfrom jax import lax, random\nimport jax.numpy as jnp\nfrom jax.scipy.special import ndtri\nfrom jax.scipy.stats import norm, poisson\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.distributions import (\n    Distribution,\n    FoldedDistribution,\n    SoftLaplace,\n    StudentT,\n    TruncatedDistribution,\n    TruncatedNormal,\n    constraints,\n)\nfrom numpyro.distributions.util import promote_shapes\nfrom numpyro.infer import MCMC, NUTS, DiscreteHMCGibbs, Predictive\n\nnumpyro.enable_x64()\nRNG = random.PRNGKey(0)\nPRIOR_RNG, MCMC_RNG, PRED_RNG = random.split(RNG, 3)\nMCMC_KWARGS = dict(\n    num_warmup=2000,\n    num_samples=2000,\n    num_chains=4,\n    chain_method=\"sequential\",\n)\n```\n\n----------------------------------------\n\nTITLE: Predicting on New Data with NumPyro\nDESCRIPTION: This snippet demonstrates how to make predictions on new data using the trained model. It shows how to handle imputed values by removing them from the posterior before prediction.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nposterior.pop(\"age_impute\")\nsurvived_pred = Predictive(model, posterior)(random.PRNGKey(3), **new_data)\n```\n\n----------------------------------------\n\nTITLE: Building a NumPyro Docker Image\nDESCRIPTION: Command to build a Docker image for either the release or development version of NumPyro with CUDA acceleration. The command must be run from the root of the git repository.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docker/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t <name_for_image:tag> docker/[dev or release]/.\n```\n\n----------------------------------------\n\nTITLE: Generating Ordinal Data\nDESCRIPTION: Creates synthetic ordinal data with 50 samples and 3 classes using Categorical and Normal distributions.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/ordinal_regression.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsimkeys = random.split(random.PRNGKey(1), 2)\nnsim = 50\nnclasses = 3\nY = Categorical(logits=np.zeros(nclasses)).sample(simkeys[0], sample_shape=(nsim,))\nX = Normal().sample(simkeys[1], sample_shape=(nsim,))\nX += Y\n\nprint(\"value counts of Y:\")\ndf = pd.DataFrame({\"X\": X, \"Y\": Y})\nprint(df.Y.value_counts())\n\nfor i in range(nclasses):\n    print(f\"mean(X) for Y == {i}: {X[np.where(Y == i)].mean():.3f}\")\n```\n\n----------------------------------------\n\nTITLE: Ordinal Regression with ImproperUniform Prior\nDESCRIPTION: Implements ordinal regression using ImproperUniform prior with ordered vector constraints for cutpoints.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/ordinal_regression.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef model1(X, Y, nclasses=3):\n    b_X_eta = sample(\"b_X_eta\", Normal(0, 5))\n    c_y = sample(\n        \"c_y\",\n        ImproperUniform(\n            support=constraints.ordered_vector,\n            batch_shape=(),\n            event_shape=(nclasses - 1,),\n        ),\n    )\n    with numpyro.plate(\"obs\", X.shape[0]):\n        eta = X * b_X_eta\n        sample(\"Y\", OrderedLogistic(eta, c_y), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: Complete Case MCMC Analysis in NumPyro\nDESCRIPTION: Performs MCMC sampling using NUTS sampler on complete cases only. Uses 250 warmup samples and 750 actual samples for inference.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncckernel = NUTS(ccmodel)\nccmcmc = MCMC(cckernel, num_warmup=250, num_samples=750)\nccmcmc.run(mcmc_key, Acc, Bcc, Ycc)\nccmcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: This snippet enumerates the required Python packages for a project. It includes libraries for neural networks (dm-haiku, flax), probabilistic programming (funsor, pyro-ppl), scientific computing (jax, numpy), visualization (matplotlib), and documentation (sphinx). Some packages have specific version requirements.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndm-haiku\nflax\nfunsor\nipython\njax\njaxlib\njaxns==2.6.3\nJinja2\nmatplotlib\nmultipledispatch\nnbsphinx>=0.8.9\nnumpy\noptax\npillow\npylab-sdk\npyyaml\nreadthedocs-sphinx-search>=0.3.2\nsphinx>=5\nsphinx-gallery\nsphinx_rtd_theme\ntensorflow_probability\ntqdm\n```\n\n----------------------------------------\n\nTITLE: Selecting and Visualizing a True Process from Prior Samples\nDESCRIPTION: Code that selects a specific sample from the prior as the 'true' data generating process, extracting its parameters and observed data, then visualizing the distribution with the discrete plotting utility.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n# Take any prior sample as the true process.\ntrue_idx = 6\ntrue_low = prior_samples[\"low\"][true_idx]\ntrue_rate = prior_samples[\"rate\"][true_idx]\ntrue_x = prior_samples[\"x\"][true_idx]\ndiscrete_distplot(true_x.copy());\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro with CUDA Support\nDESCRIPTION: Command to install NumPyro with GPU support using CUDA. This requires prior installation of CUDA on the system.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install 'numpyro[cuda]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents Structure in RST for NumPyro Documentation\nDESCRIPTION: This RST code defines the structure of the NumPyro documentation, organizing content into sections such as API reference, tutorials, and examples. It uses toctree and nbgallery directives to create a hierarchical structure for easy navigation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   getting_started\n\n.. toctree::\n   :maxdepth: 1\n   :caption: API and Developer Reference\n\n   primitives\n   distributions\n   infer\n   handlers\n   contrib\n   Change Log <https://github.com/pyro-ppl/numpyro/releases>\n\n.. nbgallery::\n   :maxdepth: 1\n   :caption: Introductory Tutorials\n   :name: introductory-tutorials\n\n   tutorials/bayesian_regression\n   tutorials/bayesian_hierarchical_linear_regression\n   examples/baseball\n   examples/vae\n   examples/funnel\n   examples/stochastic_volatility\n   examples/prodlda\n   tutorials/variationally_inferred_parameterization\n   tutorials/model_rendering\n   tutorials/bad_posterior_geometry\n   tutorials/truncated_distributions\n   tutorials/censoring\n   tutorials/hsgp_example\n   tutorials/other_samplers\n   tutorials/circulant_gp\n   tutorials/nnx_example\n\n.. nbgallery::\n   :maxdepth: 1\n   :caption: Discrete Latent Variables\n   :name: discrete-latent-variables\n\n   tutorials/gmm\n   examples/toy_mixture_model_discrete_enumeration\n   examples/annotation\n   examples/hmm_enum\n   examples/capture_recapture\n   examples/gaussian_shells\n   tutorials/discrete_imputation\n\n.. nbgallery::\n   :maxdepth: 1\n   :caption: Applications\n   :name: applications\n\n   tutorials/time_series_forecasting\n   tutorials/ordinal_regression\n   tutorials/bayesian_imputation\n   examples/gp\n   examples/bnn\n   examples/dais_demo\n   examples/sparse_regression\n   examples/horseshoe_regression\n   examples/proportion_test\n   examples/ucbadmit\n   examples/hmm\n   examples/hsgp\n   tutorials/hsgp_nd_example\n   examples/ode\n   tutorials/lotka_volterra_multiple\n   examples/neutra\n   examples/thompson_sampling\n   tutorials/bayesian_hierarchical_stacking\n   examples/ssbvm_mixture\n   examples/ar2\n   examples/holt_winters\n   examples/mortality\n   examples/zero_inflated_poisson\n   examples/cvae\n   tutorials/tbip\n   examples/var2\n   tutorials/hierarchical_forecasting\n\n.. nbgallery::\n   :maxdepth: 1\n   :caption: Other Inference Algorithms\n   :name: other-inference-algorithms\n\n   examples/covtype\n   examples/hmcecs\n   examples/stein_bnn\n   examples/stein_dmm\n```\n\n----------------------------------------\n\nTITLE: Generating Missing Values Based on Covariate Itself (MNAR)\nDESCRIPTION: Creates a dataset with missing values in covariate A, where the probability of missingness depends on the value of A itself (Missing Not At Random scenario).\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nA_isobs = random.bernoulli(simkeys[5], 0.9 - 0.8 * A)\nAobs = jnp.where(A_isobs, A, -1)\nA_obsidx = jnp.where(A_isobs)\n```\n\n----------------------------------------\n\nTITLE: Defining a Truncated Normal Model in NumPyro\nDESCRIPTION: This snippet demonstrates how to define a model using the TruncatedNormal distribution in NumPyro. It includes parameters for location, scale, and the truncation point.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef truncated_normal_model(num_observations, high, x=None):\n    loc = numpyro.sample(\"loc\", dist.Normal())\n    scale = numpyro.sample(\"scale\", dist.LogNormal())\n    with numpyro.plate(\"observations\", num_observations):\n        numpyro.sample(\"x\", TruncatedNormal(loc, scale, high=high), obs=x)\n```\n\n----------------------------------------\n\nTITLE: Visualizing the MNAR Missingness Model with Covariate Dependence\nDESCRIPTION: Creates a directed graph showing the Missing Not At Random (MNAR) mechanism where missingness M depends directly on the potentially missing value A itself.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndot_mnar_x = Digraph()\nwith dot_mnar_y.subgraph() as s:\n    s.attr(rank=\"same\")\n    s.node(\"A\")\n    s.node(\"M\")\ndot_mnar_x.node(\"B\")\ndot_mnar_x.node(\"Z\")\ndot_mnar_x.node(\"Y\")\ndot_mnar_x.edges([\"AM\", \"ZA\", \"ZB\", \"AY\", \"BY\"])\ndot_mnar_x\n```\n\n----------------------------------------\n\nTITLE: Documenting BlockNeuralAutoregressiveTransform Class in NumPyro\nDESCRIPTION: Autoclass documentation for the BlockNeuralAutoregressiveTransform class in NumPyro's distributions.flows module. It includes members, undocumented members, inheritance, and orders members by source.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/distributions.rst#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: numpyro.distributions.flows.BlockNeuralAutoregressiveTransform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Generating Missing Values Based on Outcome (MAR)\nDESCRIPTION: Creates a dataset with missing values in covariate A, where the probability of missingness depends on the outcome Y (Missing At Random scenario). Also prepares a complete case dataset.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nA_isobs = random.bernoulli(simkeys[4], expit(3 * (Y - Y.mean())))\nAobs = jnp.where(A_isobs, A, -1)\nA_obsidx = jnp.where(A_isobs)\n\n# generate complete case arrays\nAcc = Aobs[A_obsidx]\nBcc = B[A_obsidx]\nYcc = Y[A_obsidx]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Age Imputation Results in Python\nDESCRIPTION: Extracts and displays the inferred age means by title from the MCMC samples.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nage_by_title = age_mean + age_std * mcmc.get_samples()[\"age_mu\"].mean(axis=0)\ndict(zip(title_cat.categories, age_by_title))\n```\n\n----------------------------------------\n\nTITLE: Visualizing the MAR Missingness Model with Outcome Dependence\nDESCRIPTION: Creates a directed graph showing the Missing At Random (MAR) mechanism where missingness M depends on the outcome Y, not directly on the missing value A.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndot_mnar_y = Digraph()\nwith dot_mnar_y.subgraph() as s:\n    s.attr(rank=\"same\")\n    s.node(\"Y\")\n    s.node(\"M\")\ndot_mnar_y.node(\"A\")\ndot_mnar_y.node(\"B\")\ndot_mnar_y.node(\"Z\")\ndot_mnar_y.node(\"M\")\ndot_mnar_y.edges([\"YM\", \"ZA\", \"ZB\", \"AY\", \"BY\"])\ndot_mnar_y\n```\n\n----------------------------------------\n\nTITLE: Plotting Synthetic Data for Gaussian Process Model in Python\nDESCRIPTION: This function plots the synthetic data generated for the Gaussian Process model, showing the latent Gaussian process and binary observations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/circulant_gp.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef plot_data(x, trace, ax):\n    ax.plot(\n        x, expit(trace[\"z\"][\"value\"]), label=\"latent Gaussian process $z(x)$\", color=\"k\"\n    )\n    ax.scatter(\n        x, y, label=\"binary observations $y$\", alpha=0.5, edgecolor=\"none\", color=\"k\"\n    )\n    ax.set_xlabel(\"covariate $x$\")\n\n\nfig, ax = plt.subplots()\nplot_data(x, trace, ax=ax)\nax.legend(loc=\"lower right\", bbox_to_anchor=(1, 0.1))\nfig.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Implementing sample and icdf Methods for Left-truncated Poisson\nDESCRIPTION: Sample and inverse CDF implementations for the LeftTruncatedPoisson distribution. The sample method generates uniform random values and maps them through the inverse CDF (icdf). The icdf uses a brute-force approach with a while loop to find the correct values.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n    # ...\n    def sample(self, key, sample_shape=()):\n        shape = sample_shape + self.batch_shape\n        minval = jnp.finfo(jnp.result_type(float)).tiny\n        u = random.uniform(key, shape, minval=minval)\n        return self.icdf(u)\n\n    def icdf(self, u):\n        def cond_fn(val):\n            n, cdf = val\n            return jnp.any(cdf < u)\n\n        def body_fn(val):\n            n, cdf = val\n            n_new = jnp.where(cdf < u, n + 1, n)\n            return n_new, self.cdf(n_new)\n        \n        low = self.low * jnp.ones_like(u)\n        cdf = self.cdf(low)\n        n, _ = lax.while_loop(cond_fn, body_fn, (low, cdf))\n        return n.astype(jnp.result_type(int))\n\n    def cdf(self, value):\n        m = 1 - poisson.cdf(self.low - 1, self.rate)\n        f = poisson.cdf(value, self.rate) - poisson.cdf(self.low - 1, self.rate)\n        return jnp.where(k >= self.low, f / m, 0)\n```\n\n----------------------------------------\n\nTITLE: Plotting Population Data in Python with Matplotlib\nDESCRIPTION: Creates a subplot grid to visualize population data for hares and lynx across multiple datasets. Plots raw data points with time series lines.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(2, n_datasets, figsize=(15, 4))\n\nfor i in range(n_datasets):\n    loc = jnp.where(jnp.isfinite(data[i, :, 0]))[0][-1]\n\n    axs[0, i].plot(\n        ts[i, :], data[i, :, 0], \"ko\", mfc=\"none\", ms=4, label=\"true hare\", alpha=0.67\n    )\n    axs[0, i].plot(ts[i, :], data[i, :, 0], label=\"true hare\", alpha=0.67)\n    axs[0, i].set_xlabel(\"Time, year\")\n    axs[0, i].set_ylabel(\"Population\")\n    axs[0, i].set_xlim([-5, jnp.nanmax(ts)])\n\n    axs[1, i].plot(ts[i, :], data[i, :, 1], \"bx\", label=\"true lynx\")\n    axs[1, i].plot(ts[i, :], data[i, :, 1], label=\"true lynx\")\n    axs[1, i].set_xlabel(\"Time, year\")\n    axs[1, i].set_ylabel(\"Population\")\n    axs[1, i].set_xlim([-5, jnp.nanmax(ts)])\n\nfig.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Creating Time Matrix for Multiple Datasets with Varying Durations in Python\nDESCRIPTION: This code generates a time matrix for multiple datasets with varying durations and number of points. It uses JAX random number generation and padding to create a uniform-shaped matrix.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# generate array with random integers between t_min and t_max, representing tiem duration in the data set\nrand_duration = jax.random.randint(\n    PRNGKey(1), shape=(n_datasets,), minval=t_min, maxval=t_max\n)\n\n# generate array with random integers between n_points_min and n_points_max,\n# representing number of time points per dataset\nrand_n_points = jax.random.randint(\n    PRNGKey(1), shape=(n_datasets,), minval=n_points_min, maxval=n_points_max\n)\n\n# Note that arrays have different length and are stored in a list\ntime_arrays = [\n    jnp.linspace(0, j, num=rand_n_points[i]).astype(float)\n    for i, j in enumerate(rand_duration)\n]\nlongest = jnp.max(jnp.array([len(i) for i in time_arrays]))\n\n# Make a time matrix\nts = jnp.array(\n    [\n        jnp.pad(arr, pad_width=(0, longest - len(arr)), constant_values=jnp.nan)\n        for arr in time_arrays\n    ]\n)\n\nprint(f\"The shape of the time matrix is {ts.shape}\")\nprint(f\"First values are \\n {ts[:, :10]}\")\nprint(f\"Last values are \\n {ts[:, -10:]}\")\n```\n\n----------------------------------------\n\nTITLE: Running NumPyro Development Tasks with Make\nDESCRIPTION: Commands for running common development tasks using Make, including linting, code formatting, unit tests, and documentation tests.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/CONTRIBUTING.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmake lint              # linting\nmake format            # runs black and isort\nmake test              # linting and unit tests\nmake doctest           # test module's docstrings\n```\n\n----------------------------------------\n\nTITLE: Setting up NumPyro Development Environment\nDESCRIPTION: Instructions for cloning the NumPyro repository and installing it in development mode with additional dependencies for development, testing, documentation, and examples.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/CONTRIBUTING.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/pyro-ppl/numpyro.git\n# install jax/jaxlib first for CUDA support\npip install -e '.[dev,test,doc,examples]'  # contains additional dependencies for NumPyro development\n```\n\n----------------------------------------\n\nTITLE: Preparing Titanic Data for Modeling in Python\nDESCRIPTION: Converts categorical data to numeric, standardizes the Age column, and prepares the data dictionary for modeling.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntitle_cat = pd.CategoricalDtype(\n    categories=[\"Mr.\", \"Miss.\", \"Mrs.\", \"Master.\", \"Misc.\"], ordered=True\n)\nembarked_cat = pd.CategoricalDtype(categories=[\"S\", \"C\", \"Q\"], ordered=True)\nage_mean, age_std = train_df.Age.mean(), train_df.Age.std()\ndata = dict(\n    age=train_df.Age.pipe(lambda x: (x - age_mean) / age_std).values,\n    pclass=train_df.Pclass.values - 1,\n    title=train_df.Title.astype(title_cat).cat.codes.values,\n    sex=(train_df.Sex == \"male\").astype(int).values,\n    sibsp=train_df.SibSp.values,\n    parch=train_df.Parch.values,\n    embarked=train_df.Embarked.astype(embarked_cat).cat.codes.values,\n)\nsurvived = train_df.Survived.values\n# compute the age mean for each title\nage_notnan = data[\"age\"][jnp.isfinite(data[\"age\"])]\ntitle_notnan = data[\"title\"][jnp.isfinite(data[\"age\"])]\nage_mean_by_title = jnp.stack([age_notnan[title_notnan == i].mean() for i in range(5)])\n```\n\n----------------------------------------\n\nTITLE: Applying TransformReparam to Horseshoe Regression Model\nDESCRIPTION: Demonstrates an alternative reparameterization using NumPyro's TransformReparam, showcasing the versatility of the reparam library.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom numpyro.distributions.transforms import AffineTransform\nfrom numpyro.infer.reparam import TransformReparam\n\n\ndef _rep_hs_model3(X, Y):\n    lambdas = numpyro.sample(\"lambdas\", dist.HalfCauchy(jnp.ones(X.shape[1])))\n    tau = numpyro.sample(\"tau\", dist.HalfCauchy(jnp.ones(1)))\n\n    # instruct NumPyro to do the reparameterization automatically.\n    reparam_config = {\"betas\": TransformReparam()}\n    with numpyro.handlers.reparam(config=reparam_config):\n        betas_root_variance = tau * lambdas\n        # in order to use TransformReparam we have to express the prior\n        # over betas as a TransformedDistribution\n        betas = numpyro.sample(\n            \"betas\",\n            dist.TransformedDistribution(\n                dist.Normal(0.0, jnp.ones(X.shape[1])),\n                AffineTransform(0.0, betas_root_variance),\n            ),\n        )\n\n    mean_function = jnp.dot(X, betas)\n    numpyro.sample(\"Y\", dist.Normal(mean_function, 0.05), obs=Y)\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro with Compatible CPU JAX Version\nDESCRIPTION: Command to force installation of NumPyro with a known compatible CPU version of JAX. This can be used if compatibility issues arise with the standard installation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install 'numpyro[cpu]'\n```\n\n----------------------------------------\n\nTITLE: Building API Documentation and HTML Pages\nDESCRIPTION: Commands to build API documentation from docstrings and generate HTML pages using Sphinx and make.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-apidoc [options] -o <output_path> <module_path> [exclude_pattern, ...]\nmake html\n```\n\n----------------------------------------\n\nTITLE: Using NumPyro Seed Handler for Random Sampling\nDESCRIPTION: Examples showing how to use the seed handler in NumPyro either as a context manager or as a function wrapper to properly generate random samples with PRNGKey.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith handlers.seed(rng_seed=0):  # random.PRNGKey(0) is used\n    x = numpyro.sample('x', dist.Beta(1, 1))    # uses a PRNGKey split from random.PRNGKey(0)\n    y = numpyro.sample('y', dist.Bernoulli(x))  # uses different PRNGKey split from the last one\n```\n\nLANGUAGE: python\nCODE:\n```\ndef fn():\n    x = numpyro.sample('x', dist.Beta(1, 1))\n    y = numpyro.sample('y', dist.Bernoulli(x))\n    return y\n\nprint(handlers.seed(fn, rng_seed=0)())\n```\n\n----------------------------------------\n\nTITLE: Computing Autocorrelation Lags - Python\nDESCRIPTION: Prints lag values sorted by their autocorrelation values to help determine seasonality parameter for the SGT model.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Lag values sorted according to their autocorrelation values:\\n\")\nprint(jnp.argsort(autocorrelation(y_train))[::-1])\n```\n\n----------------------------------------\n\nTITLE: Building NumPyro Documentation with Make\nDESCRIPTION: Command to build the NumPyro documentation from the toplevel directory using the make utility.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake docs\n```\n\n----------------------------------------\n\nTITLE: Setting Hyperparameters for TBIP Model\nDESCRIPTION: Initializes the key hyperparameters for the model, including the number of topics (K=50) and a random seed for reproducibility using JAX's random module.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom jax import random\n\nnum_topics = 50\nrng_seed = random.PRNGKey(0)\n```\n\n----------------------------------------\n\nTITLE: Visualizing SVI Convergence and Gradient Norms in NumPyro\nDESCRIPTION: This code creates plots to visualize the convergence of SVI and the gradient norms during optimization. It helps in understanding the training process and identifying potential issues.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10, 3), dpi=100).set_facecolor(\"white\")\nplt.plot(full_svi_result.losses)\nplt.xlabel(\"iters\")\nplt.ylabel(\"loss\")\nplt.yscale(\"log\")\nplt.title(\"Convergence of SVI\")\nplt.show()\n\nplt.figure(figsize=(10, 4), dpi=100).set_facecolor(\"white\")\nfor name, grad_norms in gradient_norms.items():\n    plt.plot(grad_norms, label=name)\nplt.xlabel(\"iters\")\nplt.ylabel(\"gradient norm\")\nplt.yscale(\"log\")\nplt.legend(loc=\"best\")\nplt.title(\"Gradient norms during SVI\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Rendering Model with Neural Network\nDESCRIPTION: Visualizes the model that includes a neural network, showing distributions and parameters.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nnumpyro.render_model(\n    model, model_args=(data,), render_distributions=True, render_params=True\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Class Attributes for Left-truncated Poisson in NumPyro\nDESCRIPTION: Implementation of class attributes for the LeftTruncatedPoisson distribution, including argument constraints and support definition. The support property is defined to ensure the distribution only accepts values greater than or equal to the low parameter.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass LeftTruncatedPoisson:\n    arg_constraints = {\n        \"low\": constraints.nonnegative_integer,\n        \"rate\": constraints.positive,\n    }\n    \n    # ... \n    @constraints.dependent_property(is_discrete=True)\n    def support(self):\n        return constraints.integer_greater_than(self.low - 1)\n```\n\n----------------------------------------\n\nTITLE: Rendering Model with Deterministic Sites\nDESCRIPTION: Visualizes the model that includes a deterministic site.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata = jnp.ones(10)\nnumpyro.render_model(model, model_args=(data,))\n```\n\n----------------------------------------\n\nTITLE: Implementing the __init__ Method for Left-truncated Poisson\nDESCRIPTION: Constructor implementation for the LeftTruncatedPoisson class, handling parameter promotion and shape broadcasting. This ensures proper handling of both scalar and array parameters for rate and low values.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n    # ...\n    def __init__(self, rate=1.0, low=0, validate_args=None):\n        batch_shape = lax.broadcast_shapes(\n            jnp.shape(low), jnp.shape(rate)\n        )\n        self.low, self.rate = promote_shapes(low, rate)\n        super().__init__(batch_shape, validate_args=validate_args)\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Generating Prior Samples and Synthetic Data in NumPyro\nDESCRIPTION: This snippet generates prior samples and selects a random sample as ground truth for synthetic data generation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nnum_observations = 1000\nnum_prior_samples = 100\nprior = Predictive(truncated_normal_model, num_samples=num_prior_samples)\nprior_samples = prior(PRIOR_RNG, num_observations)\n\ntrue_idx = 0\ntrue_loc = prior_samples[\"loc\"][true_idx]\ntrue_scale = prior_samples[\"scale\"][true_idx]\ntrue_high = prior_samples[\"high\"][true_idx]\ntrue_x = prior_samples[\"x\"][true_idx]\n```\n\n----------------------------------------\n\nTITLE: Defining Model with Deterministic Sites\nDESCRIPTION: Creates a model that includes a deterministic site using numpyro.deterministic.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    m = numpyro.sample(\"m\", dist.Normal(0, 1))\n    sd = numpyro.sample(\"sd\", dist.LogNormal(m, 1))\n    # deterministic site\n    m_transformed = numpyro.deterministic(\"m_transformed\", m + 1)\n    with numpyro.plate(\"N\", len(data)):\n        numpyro.sample(\"obs\", dist.Normal(m_transformed, sd), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Implementing log_prob Method for Left-truncated Poisson\nDESCRIPTION: Log probability calculation for the LeftTruncatedPoisson distribution. It computes the Poisson log probability and adjusts by the normalization factor to account for truncation. Returns -infinity for values below the truncation point.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n    # ...\n    def log_prob(self, value):\n        m = 1 - poisson.cdf(self.low - 1, self.rate)\n        log_p = poisson.logpmf(value, self.rate)\n        return jnp.where(value >= self.low, log_p - jnp.log(m), -jnp.inf)\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Importing NumPyro Components for Probabilistic Programming\nDESCRIPTION: Imports the necessary NumPyro modules and functions for defining the probabilistic model and setting up variational inference, including distributions, parameters, and plate notation for batched operations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom numpyro import param, plate, sample\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints\n```\n\n----------------------------------------\n\nTITLE: Defining Complex MACE Model\nDESCRIPTION: Defines a more complex model called MACE (Multi-Annotator Competence Estimation) with multiple plates and distributions.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef mace(positions, annotations):\n    \"\"\"\n    This model corresponds to the plate diagram in Figure 3 of https://www.aclweb.org/anthology/Q18-1040.pdf.\n    \"\"\"\n    num_annotators = int(np.max(positions)) + 1\n    num_classes = int(np.max(annotations)) + 1\n    num_items, num_positions = annotations.shape\n\n    with numpyro.plate(\"annotator\", num_annotators):\n        epsilon = numpyro.sample(\"epsilon\", dist.Dirichlet(jnp.full(num_classes, 10)))\n        theta = numpyro.sample(\"theta\", dist.Beta(0.5, 0.5))\n\n    with numpyro.plate(\"item\", num_items, dim=-2):\n        c = numpyro.sample(\"c\", dist.DiscreteUniform(0, num_classes - 1))\n\n        with numpyro.plate(\"position\", num_positions):\n            s = numpyro.sample(\"s\", dist.Bernoulli(1 - theta[positions]))\n            probs = jnp.where(\n                s[..., None] == 0, nn.one_hot(c, num_classes), epsilon[positions]\n            )\n            numpyro.sample(\"y\", dist.Categorical(probs), obs=annotations)\n```\n\n----------------------------------------\n\nTITLE: Creating Helper Function for Printing Topics in TBIP Model\nDESCRIPTION: Defines a function to extract and format top words for each topic, including neutral, positive, and negative aspects. This helper function is used for visualizing the learned topics during model training.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef get_topics(\n    neutral_mean, negative_mean, positive_mean, vocabulary, print_to_terminal=True\n):\n    num_topics, num_words = neutral_mean.shape\n    words_per_topic = 10\n    top_neutral_words = np.argsort(-neutral_mean, axis=1)\n    top_negative_words = np.argsort(-negative_mean, axis=1)\n    top_positive_words = np.argsort(-positive_mean, axis=1)\n    topic_strings = []\n    for topic_idx in range(num_topics):\n        neutral_start_string = \"Neutral  {}:\".format(topic_idx)\n        neutral_row = [\n            vocabulary[word] for word in top_neutral_words[topic_idx, :words_per_topic]\n        ]\n        neutral_row_string = \", \".join(neutral_row)\n        neutral_string = \" \".join([neutral_start_string, neutral_row_string])\n\n        positive_start_string = \"Positive {}:\".format(topic_idx)\n        positive_row = [\n            vocabulary[word] for word in top_positive_words[topic_idx, :words_per_topic]\n        ]\n        positive_row_string = \", \".join(positive_row)\n        positive_string = \" \".join([positive_start_string, positive_row_string])\n\n        negative_start_string = \"Negative {}:\".format(topic_idx)\n        negative_row = [\n            vocabulary[word] for word in top_negative_words[topic_idx, :words_per_topic]\n        ]\n        negative_row_string = \", \".join(negative_row)\n        negative_string = \" \".join([negative_start_string, negative_row_string])\n\n        if print_to_terminal:\n            topic_strings.append(negative_string)\n            topic_strings.append(neutral_string)\n            topic_strings.append(positive_string)\n            topic_strings.append(\"==========\")\n        else:\n            topic_strings.append(\n                \"  \\n\".join([negative_string, neutral_string, positive_string])\n            )\n\n    if print_to_terminal:\n        all_topics = \"{}\".format(np.array(topic_strings))\n    else:\n        all_topics = np.array(topic_strings)\n    return all_topics\n```\n\n----------------------------------------\n\nTITLE: Loading Political Text Data for TBIP Analysis\nDESCRIPTION: Loads the required data files for the TBIP model: word counts matrix, author indices, vocabulary list, and author mapping. The data is from Senate speeches from the 114th Congress session.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom scipy import sparse\n\nimport jax\nimport jax.numpy as jnp\n\ndataPath = \"tbip/data/senate-speeches-114/clean/\"\n\n# Load data\nauthor_indices = jax.device_put(\n    jnp.load(dataPath + \"author_indices.npy\"), jax.devices(\"gpu\")[0]\n)\n\ncounts = sparse.load_npz(dataPath + \"counts.npz\")\n\nwith open(dataPath + \"vocabulary.txt\", \"r\") as f:\n    vocabulary = f.readlines()\n\nwith open(dataPath + \"author_map.txt\", \"r\") as f:\n    author_map = f.readlines()\n\nauthor_map = np.array(author_map)\n\nnum_authors = int(author_indices.max() + 1)\nnum_documents, num_words = counts.shape\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Gaussian Mixture Model\nDESCRIPTION: Imports necessary libraries and modules for implementing a Gaussian Mixture Model in NumPyro, including JAX, NumPyro, and visualization utilities.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\nimport os\n\nimport matplotlib.pyplot as plt\nimport scipy.stats\n\nfrom jax import pure_callback, random\nimport jax.numpy as jnp\nimport optax\n\nimport numpyro\nfrom numpyro import handlers\nfrom numpyro.contrib.funsor import config_enumerate, infer_discrete\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints\nfrom numpyro.infer import SVI, TraceEnum_ELBO, init_to_value\nfrom numpyro.infer.autoguide import AutoDelta\n\n%matplotlib inline\n\nsmoke_test = \"CI\" in os.environ\nassert numpyro.__version__.startswith(\"0.18.0\")\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro from Source\nDESCRIPTION: Commands to clone the NumPyro repository and install it from source with additional dependencies for development.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pyro-ppl/numpyro.git\ncd numpyro\n# install jax/jaxlib first for CUDA support\npip install -e '.[dev]'  # contains additional dependencies for NumPyro development\n```\n\n----------------------------------------\n\nTITLE: MCMC Summary Output for Gaussian Process Model in NumPyro\nDESCRIPTION: This snippet shows the formatted output of mcmc.print_summary() for a Gaussian Process model. It displays posterior statistics (mean, std, median, percentiles) as well as MCMC diagnostics (n_eff, r_hat) for model parameters including alpha, beta coefficients, length scale, and noise.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/contrib.rst#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n                  mean       std    median      5.0%     95.0%     n_eff     r_hat\n       alpha      1.24      0.34      1.18      0.72      1.74   1804.01      1.00\n     beta[0]     -0.10      0.66     -0.10     -1.24      0.92   1819.91      1.00\n     beta[1]      0.00      0.71     -0.01     -1.09      1.26   1872.82      1.00\n     beta[2]     -0.05      0.69     -0.03     -1.09      1.16   2105.88      1.00\n     beta[3]      0.25      0.74      0.26     -0.98      1.42   2281.30      1.00\n     beta[4]     -0.17      0.69     -0.17     -1.21      1.00   2551.39      1.00\n     beta[5]      0.09      0.75      0.10     -1.13      1.30   3232.13      1.00\n     beta[6]     -0.49      0.75     -0.49     -1.65      0.82   3042.31      1.00\n     beta[7]      0.42      0.75      0.44     -0.78      1.65   2885.42      1.00\n     beta[8]      0.69      0.71      0.71     -0.48      1.82   2811.68      1.00\n     beta[9]     -1.43      0.75     -1.40     -2.63     -0.21   2858.68      1.00\n    beta[10]      0.33      0.71      0.33     -0.77      1.51   2198.65      1.00\n    beta[11]      1.09      0.73      1.11     -0.23      2.18   2765.99      1.00\n    beta[12]     -0.91      0.72     -0.91     -2.06      0.31   2586.53      1.00\n    beta[13]      0.05      0.70      0.04     -1.16      1.12   2569.59      1.00\n    beta[14]     -0.44      0.71     -0.44     -1.58      0.73   2626.09      1.00\n    beta[15]      0.69      0.73      0.70     -0.45      1.88   2626.32      1.00\n    beta[16]      0.98      0.74      0.98     -0.15      2.28   2282.86      1.00\n    beta[17]     -2.54      0.77     -2.52     -3.82     -1.29   3347.56      1.00\n    beta[18]      1.35      0.66      1.35      0.30      2.46   2638.17      1.00\n    beta[19]      1.10      0.54      1.09      0.25      2.01   2428.37      1.00\n      length      0.07      0.01      0.07      0.06      0.09   2321.67      1.00\n       noise      0.33      0.03      0.33      0.29      0.38   2472.83      1.00\n\nNumber of divergences: 0\n```\n\n----------------------------------------\n\nTITLE: Using SciPy's Implementation for Truncated Poisson ICDF\nDESCRIPTION: Helper function for computing the inverse CDF of a truncated Poisson using SciPy's implementation. This function adjusts the uniform random variable to account for the truncation before passing it to SciPy's Poisson ppf (percent point function).\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n    def scipy_truncated_poisson_icdf(args): # Note: all arguments are passed inside a tuple\n        rate, low, u = args\n        rate = np.asarray(rate)\n        low = np.asarray(low)\n        u = np.asarray(u)\n        density = sp_poisson(rate)\n        low_cdf = density.cdf(low - 1)\n        normalizer = 1.0 - low_cdf\n        x = normalizer * u + low_cdf\n        return density.ppf(x)\n```\n\n----------------------------------------\n\nTITLE: Running a NumPyro Docker Container with Shell Access\nDESCRIPTION: Command to start a Docker container from a previously built NumPyro image and open an interactive shell session. This allows direct interaction with the CUDA-enabled NumPyro environment.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docker/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -ti <name_for_image>\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Topic Model Results\nDESCRIPTION: Loads the saved topic means and author data, with basic preprocessing of author names to remove newlines.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nneutral_topic_mean = np.load(\"neutral_topic_mean.npy\")\nnegative_topic_mean = np.load(\"negative_topic_mean.npy\")\npositive_topic_mean = np.load(\"positive_topic_mean.npy\")\nauthors = pd.read_csv(\"authors.csv\")\nauthors[\"name\"] = authors[\"name\"].str.replace(\"\\n\", \"\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Pre-commit Hooks for NumPyro Development\nDESCRIPTION: Instructions for installing and using pre-commit hooks to automatically format code before committing changes to the repository.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/CONTRIBUTING.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up NumPyro\nDESCRIPTION: This code block imports necessary libraries including NumPyro, JAX, and visualization tools. It also sets up NumPyro to use 4 devices and checks the version.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom IPython.display import set_matplotlib_formats\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom jax import random\nimport jax.numpy as jnp\n\nimport numpyro\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.diagnostics import autocorrelation, hpdi\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\nif \"NUMPYRO_SPHINXBUILD\" in os.environ:\n    set_matplotlib_formats(\"svg\")\n\nnumpyro.set_host_device_count(4)\nassert numpyro.__version__.startswith(\"0.18.0\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing MCMC Posterior Density in NumPyro\nDESCRIPTION: This snippet creates a 2D histogram and contour plot to visualize the posterior density of the mixture component locations as estimated by collapsed NUTS. It helps in understanding the multimodal nature of the posterior.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nX, Y = posterior_samples[\"locs\"].T\n\nplt.figure(figsize=(8, 8), dpi=100).set_facecolor(\"white\")\nh, xs, ys, image = plt.hist2d(X, Y, bins=[20, 20])\nplt.contour(\n    jnp.log(h + 3).T,\n    extent=[xs.min(), xs.max(), ys.min(), ys.max()],\n    colors=\"white\",\n    alpha=0.8,\n)\nplt.title(\"Posterior density as estimated by collapsed NUTS\")\nplt.xlabel(\"loc of component 0\")\nplt.ylabel(\"loc of component 1\")\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Running NumPyro Tests in Parallel\nDESCRIPTION: Instructions for running tests in parallel using pytest-xdist to speed up the testing process.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/CONTRIBUTING.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install pytest-xdist\npytest -vs -n auto\n```\n\n----------------------------------------\n\nTITLE: Basic NumPyro Distribution Template\nDESCRIPTION: Base template showing the required methods and attributes for implementing a custom NumPyro distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass MyDistribution(Distribution):\n    # class attributes\n    arg_constraints = {}\n    support = None\n    def __init__(self):\n        pass\n    \n    def log_prob(self, value):\n        pass\n    \n    def sample(self, key, sample_shape=()):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Running Individual NumPyro Tests\nDESCRIPTION: Commands for running a single test, including an example with GPU and double precision enabled using environment variables.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/CONTRIBUTING.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npytest -vs {path_to_test}::{test_name}\n# or in cuda mode and double precision\nJAX_PLATFORM_NAME=gpu JAX_ENABLE_X64=1 pytest -vs {path_to_test}::{test_name}\n```\n\n----------------------------------------\n\nTITLE: Documenting logdiffexp Utility Function in NumPyro\nDESCRIPTION: Autofunction documentation for the logdiffexp utility function in NumPyro's distributions.util module.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/distributions.rst#2025-04-16_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: numpyro.distributions.util.logdiffexp\n```\n\n----------------------------------------\n\nTITLE: Normal Distribution Attributes Reference\nDESCRIPTION: Source code reference showing the class attributes for NumPyro's Normal distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\narg_constraints = {\"loc\": constraints.real, \"scale\": constraints.positive}\nsupport = constraints.real\nreparametrized_params = [\"loc\", \"scale\"]\n```\n\n----------------------------------------\n\nTITLE: Plotting Predictions with Confidence Intervals\nDESCRIPTION: Visualizes the predicted population dynamics with uncertainty bands. Shows both observed data points and model predictions with 80% prediction intervals.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(2, n_datasets, figsize=(15, 4))\n\nfor i in range(n_datasets):\n    loc = jnp.where(jnp.isfinite(data[i, :, 0]))[0][-1]\n\n    axs[0, i].plot(\n        ts_pred[i, :], mu[i, :, 0], \"k-.\", label=\"pred hare\", lw=1, alpha=0.67\n    )\n    axs[0, i].plot(\n        ts[i, :], data[i, :, 0], \"ko\", mfc=\"none\", ms=4, label=\"true hare\", alpha=0.67\n    )\n    axs[0, i].fill_between(\n        ts_pred[i, :], pi[0, i, :, 0], pi[1, i, :, 0], color=\"k\", alpha=0.2\n    )\n    axs[0, i].set_xlabel(\"Time, year\")\n    axs[0, i].set_ylabel(\"Population\")\n    axs[0, i].set_xlim([-5, jnp.nanmax(ts)])\n\n    axs[1, i].plot(ts_pred[i, :], mu[i, :, 1], \"b--\", label=\"pred lynx\")\n    axs[1, i].plot(ts[i, :], data[i, :, 1], \"bx\", label=\"true lynx\")\n    axs[1, i].fill_between(\n        ts_pred[i, :], pi[0, i, :, 1], pi[1, i, :, 1], color=\"b\", alpha=0.2\n    )\n    axs[1, i].set_xlabel(\"Time, year\")\n    axs[1, i].set_ylabel(\"Population\")\n    axs[1, i].set_xlim([-5, jnp.nanmax(ts)])\n\nfig.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Documenting InverseAutoregressiveTransform Class in NumPyro\nDESCRIPTION: Autoclass documentation for the InverseAutoregressiveTransform class in NumPyro's distributions.flows module. It includes members, undocumented members, inheritance, and orders members by source.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/distributions.rst#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: numpyro.distributions.flows.InverseAutoregressiveTransform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple NumPyro Model\nDESCRIPTION: Creates a basic NumPyro model with normal and lognormal distributions.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef model(data):\n    m = numpyro.sample(\"m\", dist.Normal(0, 1))\n    sd = numpyro.sample(\"sd\", dist.LogNormal(m, 1))\n    with numpyro.plate(\"N\", len(data)):\n        numpyro.sample(\"obs\", dist.Normal(m, sd), obs=data)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries and Setting Up Random Keys\nDESCRIPTION: Imports necessary libraries for Bayesian modeling, visualization, and JAX-based computation. Sets up random keys for simulation and MCMC sampling.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom math import inf\n\nfrom graphviz import Digraph\n\nfrom jax import numpy as jnp, random\nfrom jax.scipy.special import expit\n\nimport numpyro\nfrom numpyro import distributions as dist, sample\nfrom numpyro.infer.hmc import NUTS\nfrom numpyro.infer.mcmc import MCMC\n\nsimkeys = random.split(random.PRNGKey(0), 10)\nnsim = 5000\nmcmc_key = random.PRNGKey(1)\n```\n\n----------------------------------------\n\nTITLE: Displaying Value Counts of Categorical Columns in Python\nDESCRIPTION: Prints the value counts for selected categorical columns in the Titanic dataset.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor col in [\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]:\n    print(train_df[col].value_counts(), end=\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Running MCMC with NUTS Sampler for Gaussian Process Model in NumPyro\nDESCRIPTION: This code snippet demonstrates how to use NumPyro's NUTS sampler with MCMC to fit a Gaussian Process model. It shows setting up the sampler, running MCMC with specified parameters, and printing the summary statistics of the posterior samples.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/contrib.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> sampler = NUTS(model)\n>>> mcmc = MCMC(sampler=sampler, num_warmup=500, num_samples=1_000, num_chains=2)\n\n>>> rng_key, rng_subkey = random.split(rng_key)\n\n>>> ell = 1.3\n>>> m = 20\n>>> non_centered = True\n\n>>> mcmc.run(rng_subkey, x, ell, m, non_centered, y_obs)\n\n>>> mcmc.print_summary()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Bayesian Imputation in Python\nDESCRIPTION: Imports necessary libraries including NumPyro, JAX, pandas, and matplotlib for data analysis and Bayesian modeling.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom IPython.display import set_matplotlib_formats\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom jax import numpy as jnp, random\n\nimport numpyro\nfrom numpyro import distributions as dist\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\nplt.style.use(\"seaborn\")\nif \"NUMPYRO_SPHINXBUILD\" in os.environ:\n    set_matplotlib_formats(\"svg\")\n\nassert numpyro.__version__.startswith(\"0.18.0\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Eight Schools Data in Python\nDESCRIPTION: Sets up the data for the Eight Schools example, including the number of schools, treatment effects, and standard errors.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n\n>>> J = 8\n>>> y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\n>>> sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\n```\n\n----------------------------------------\n\nTITLE: Cloning the TBIP Repository for Data Access\nDESCRIPTION: Clones the Text-Based Ideal Point model repository from GitHub to access the political text datasets needed for the analysis.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n! git clone https://github.com/keyonvafa/tbip\n```\n\n----------------------------------------\n\nTITLE: Querying True Rate Parameter Value in NumPyro\nDESCRIPTION: Simple code to output the true rate parameter value for comparison with inferred results.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ntrue_rate\n```\n\n----------------------------------------\n\nTITLE: Conditioning Model with Known Truncation Point in NumPyro\nDESCRIPTION: Creates a conditioned version of the truncated Poisson model with a fixed, known truncation point (low parameter) using NumPyro's handler functionality.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nmodel_with_known_low = numpyro.handlers.condition(\n    truncated_poisson_model, {\"low\": true_low}\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary libraries including NumPy, JAX, Flax, and NumPyro. Also asserts the correct version of NumPyro.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimport flax.linen as flax_nn\nfrom jax import nn\nimport jax.numpy as jnp\n\nimport numpyro\nfrom numpyro.contrib.module import flax_module\nimport numpyro.distributions as dist\nimport numpyro.distributions.constraints as constraints\n\nassert numpyro.__version__.startswith(\"0.18.0\")\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro with CPU-only JAX\nDESCRIPTION: Command to install NumPyro with the latest CPU version of JAX using pip. This is the standard installation method for most users.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install numpyro\n```\n\n----------------------------------------\n\nTITLE: Documenting AffineTransform Class in NumPyro\nDESCRIPTION: Autoclass documentation for the AffineTransform class in NumPyro's distributions.transforms module. It includes members, undocumented members, inheritance, and orders members by source.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/distributions.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: numpyro.distributions.transforms.AffineTransform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Simple Folded Distribution Sampling\nDESCRIPTION: Implementation of sampling from a folded distribution by taking the absolute value of samples from the original distribution.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsamples = np.random.normal(size=1000)\nfolded_samples = np.abs(samples)\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro with Conda\nDESCRIPTION: Command to install NumPyro using the conda package manager from the conda-forge channel.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge numpyro\n```\n\n----------------------------------------\n\nTITLE: Visualizing Author Ideal Points\nDESCRIPTION: Creates a scatter plot visualization of author ideal points with selected author labels, using matplotlib and seaborn.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nselected_authors = np.array([\n    \"Dean Heller (R)\",\n    \"Bernard Sanders (I)\",\n    \"Elizabeth Warren (D)\",\n    \"Charles Schumer (D)\",\n    \"Susan Collins (R)\",\n    \"Marco Rubio (R)\",\n    \"John Mccain (R)\",\n    \"Ted Cruz (R)\",\n])\n\nsns.set(style=\"whitegrid\")\nfig = plt.figure(figsize=(12, 1))\nax = plt.axes([0, 0, 1, 1], frameon=False)\nfor index in range(authors.shape[0]):\n    ax.scatter(authors[\"ideal_point\"][index], 0, c=\"black\", s=20)\n    if authors[\"name\"][index] in selected_authors:\n        ax.annotate(\n            author_map[index],\n            xy=(authors[\"ideal_point\"][index], 0.0),\n            xytext=(authors[\"ideal_point\"][index], 0),\n            rotation=30,\n            size=14,\n        )\nax.set_yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Adding Index Reference to NumPyro Documentation\nDESCRIPTION: Includes a reference to the general index of the documentation using the genindex directive. This allows users to access an alphabetical list of all terms and topics covered in the tutorials.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/index.rst#2025-04-16_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n* :ref:`genindex`\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro and Dependencies for TBIP Model\nDESCRIPTION: Installs the required NumPyro package (version 0.10.1) and optax optimization library for implementing the Text-Based Ideal Point model. Uses pip with the capture magic to suppress installation output.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/tbip.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n%pip install numpyro==0.10.1\n%pip install optax\n```\n\n----------------------------------------\n\nTITLE: Documenting CholeskyTransform Class in NumPyro\nDESCRIPTION: Autoclass documentation for the CholeskyTransform class in NumPyro's distributions.transforms module. It includes members, undocumented members, inheritance, and orders members by source.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/distributions.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: numpyro.distributions.transforms.CholeskyTransform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro from GitHub\nDESCRIPTION: Command to install the latest version of NumPyro directly from the GitHub repository.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q git+https://github.com/pyro-ppl/numpyro.git\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro and Funsor Dependencies\nDESCRIPTION: Installs the NumPyro package from GitHub along with the Funsor package, which is required for enumeration of discrete variables.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro funsor\n```\n\n----------------------------------------\n\nTITLE: Basic Rejection Sampling for Truncated Distribution\nDESCRIPTION: Simple implementation of rejection sampling for a truncated normal distribution. Demonstrates the basic but inefficient approach of discarding samples outside the desired range.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nupper = 1\nsamples = np.random.normal(size=1000)\ntruncated_samples = samples[samples < upper]\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro Package in Python\nDESCRIPTION: Installs the latest development version of NumPyro from GitHub using pip.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro\n```\n\n----------------------------------------\n\nTITLE: Filling NaN Values in Time Matrix for NUTS Compatibility in Python\nDESCRIPTION: This code fills NaN values in the time matrix with dummy variables greater than t_max to ensure compatibility with the NUTS algorithm in NumPyro.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# fill_nans\ndef fill_nans(ts):\n    n_nan = jnp.sum(jnp.isnan(ts))\n    if n_nan > 0:\n        loc_first_nan = jnp.where(jnp.isnan(ts))[0][0]\n        ts_filled_nans = ts.at[loc_first_nan:].set(\n            jnp.linspace(t_max, t_max + 20, n_nan)\n        )\n        return ts_filled_nans\n    else:\n        return ts\n\n\nts_filled_nans = jnp.array([fill_nans(t) for t in ts])\n```\n\n----------------------------------------\n\nTITLE: Documenting log1mexp Utility Function in NumPyro\nDESCRIPTION: Autofunction documentation for the log1mexp utility function in NumPyro's distributions.util module.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/distributions.rst#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: numpyro.distributions.util.log1mexp\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer with Gradient Monitoring\nDESCRIPTION: Defines a helper function to track gradient norms during training and sets up the Adam optimizer with SVI for training the model.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Helper function to collect gradient norms during training\ndef hook_optax(optimizer):\n    gradient_norms = defaultdict(list)\n\n    def append_grad(grad):\n        for name, g in grad.items():\n            gradient_norms[name].append(float(jnp.linalg.norm(g)))\n        return grad\n\n    def update_fn(grads, state, params=None):\n        grads = pure_callback(append_grad, grads, grads)\n        return optimizer.update(grads, state, params=params)\n\n    return optax.GradientTransformation(optimizer.init, update_fn), gradient_norms\n\n\noptim, gradient_norms = hook_optax(optax.adam(learning_rate=0.1, b1=0.8, b2=0.99))\nglobal_svi = SVI(model, global_guide, optim, loss=elbo)\n```\n\n----------------------------------------\n\nTITLE: Visualizing SVI Loss Convergence\nDESCRIPTION: Creates a plot showing the convergence of the SVI loss function over training iterations.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10, 3), dpi=100).set_facecolor(\"white\")\nplt.plot(global_svi_result.losses)\nplt.xlabel(\"iters\")\nplt.ylabel(\"loss\")\nplt.yscale(\"log\")\nplt.title(\"Convergence of SVI\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Rendering MACE Model\nDESCRIPTION: Visualizes the complex MACE model using numpyro.render_model.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmace_graph = numpyro.render_model(mace, model_args=(positions, annotations))\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Data Generation Model\nDESCRIPTION: Creates a directed graph showing the causal relationships between variables Z (latent), A and B (covariates), and Y (outcome).\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/discrete_imputation.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndot = Digraph()\ndot.node(\"A\")\ndot.node(\"B\")\ndot.node(\"Z\")\ndot.node(\"Y\")\ndot.edges([\"ZA\", \"ZB\", \"AY\", \"BY\"])\ndot\n```\n\n----------------------------------------\n\nTITLE: Visualizing Predictive Samples Without Truncation\nDESCRIPTION: Visualizes the distribution of predictive samples from the untruncated model using the discrete plotting utility.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndiscrete_distplot(thinned_samples.copy());\n```\n\n----------------------------------------\n\nTITLE: Summarizing Synthetic Dataset Properties in Python\nDESCRIPTION: This code prints a summary of the synthetic dataset, including its shape, time matrix properties, percentage of missing observations, and true parameter values.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/lotka_volterra_multiple.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"The dataset has the shape {data.shape}, (n_datasets, n_points, n_observables)\")\nprint(f\"The time matrix has the shape {ts.shape}, (n_datasets, n_timepoints)\")\nprint(f\"The time matrix has different spacing between timepoints: \\n {ts[:, :5]}\")\nprint(f\"The final timepoints are: {jnp.nanmax(ts, 1)} years.\")\nprint(\n    f\"The dataset has {jnp.sum(jnp.isnan(data)) / jnp.size(data):.0%} missing observations\"\n)\nprint(f\"True params mean: {sample['theta'][0]}\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Visualizing Lynx Dataset\nDESCRIPTION: This snippet loads the lynx dataset from a URL, prints the length of the time series, and creates a plot of the data over time.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nURL = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/lynx.csv\"\nlynx = pd.read_csv(URL, index_col=0)\ndata = lynx[\"value\"].values\nprint(\"Length of time series:\", data.shape[0])\nplt.figure(figsize=(8, 4))\nplt.plot(lynx[\"time\"], data)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying Titanic Dataset in Python\nDESCRIPTION: Loads the Titanic dataset from a CSV file using pandas and displays basic information and the first few rows.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_imputation.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv\"\n)\ntrain_df.info()\ntrain_df.head()\n```\n\n----------------------------------------\n\nTITLE: Rendering Model with Overlapping Plates\nDESCRIPTION: Visualizes the model with overlapping plates.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nnumpyro.render_model(model)\n```\n\n----------------------------------------\n\nTITLE: Rendering a Simple NumPyro Model\nDESCRIPTION: Demonstrates how to use numpyro.render_model to visualize the simple model.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = jnp.ones(10)\nnumpyro.render_model(model, model_args=(data,))\n```\n\n----------------------------------------\n\nTITLE: Custom Right Extended Real Constraint\nDESCRIPTION: Implementation of a custom constraint for the right-extended real line (-inf, inf].\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/truncated_distributions.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass _RightExtendedReal(constraints.Constraint):\n    \"\"\"\n    Any number in the interval (-inf, inf].\n    \"\"\"\n    def __call__(self, x):\n        return (x == x) & (x != float(\"-inf\"))\n    \n    def feasible_like(self, prototype):\n        return jnp.zeros_like(prototype)\n\nright_extended_real = _RightExtendedReal()\n```\n\n----------------------------------------\n\nTITLE: Rendering Distributions and Constraints\nDESCRIPTION: Demonstrates how to display distributions and constraints in the model visualization.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnumpyro.render_model(\n    model, model_args=(data,), render_params=True, render_distributions=True\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Gaussian Process Inference in Python\nDESCRIPTION: This code snippet installs the required packages for the Gaussian Process inference example, including numpyro from GitHub and matplotlib.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/circulant_gp.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro matplotlib\n```\n\n----------------------------------------\n\nTITLE: Rendering Model with Parameters\nDESCRIPTION: Visualizes the model with parameters using render_params=True.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata = jnp.ones(10)\nnumpyro.render_model(model, model_args=(data,), render_params=True)\n```\n\n----------------------------------------\n\nTITLE: Saving Model Visualization to File\nDESCRIPTION: Shows how to save the model visualization to a PDF file.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngraph = numpyro.render_model(model, model_args=(data,), filename=\"model.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up NumPyro Environment with GPU Support\nDESCRIPTION: Imports necessary libraries, confirms NumPyro version, and sets the platform to GPU for accelerated computation.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/logistic_regression.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport numpy as np\n\nfrom jax import random\nimport jax.numpy as jnp\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import COVTYPE, load_dataset\nfrom numpyro.infer import HMC, MCMC, NUTS\n\nassert numpyro.__version__.startswith(\"0.18.0\")\n\n# NB: replace gpu by cpu to run this notebook in cpu\nnumpyro.set_platform(\"gpu\")\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for NumPyro Inference\nDESCRIPTION: Imports necessary libraries and sets up NumPyro for CPU inference. Includes version assertion and platform setting.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nimport numpy as np\n\nfrom jax import random\nimport jax.numpy as jnp\n\nimport numpyro\nfrom numpyro.diagnostics import summary\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\n\nassert numpyro.__version__.startswith(\"0.18.0\")\n\n# NB: replace cpu by gpu to run this notebook on gpu\nnumpyro.set_platform(\"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Ordinal Regression\nDESCRIPTION: Sets up the necessary imports from NumPyro, JAX, pandas and seaborn for implementing ordinal regression models.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/ordinal_regression.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport seaborn as sns\n\nfrom jax import numpy as np, random\n\nimport numpyro\nfrom numpyro import handlers, sample\nfrom numpyro.distributions import (\n    Categorical,\n    Dirichlet,\n    ImproperUniform,\n    Normal,\n    OrderedLogistic,\n    TransformedDistribution,\n    constraints,\n    transforms,\n)\nfrom numpyro.infer import MCMC, NUTS\nfrom numpyro.infer.reparam import TransformReparam\n\nassert numpyro.__version__.startswith(\"0.18.0\")\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro from GitHub Repository\nDESCRIPTION: Installs the NumPyro package directly from the GitHub repository using pip.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/logistic_regression.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro\n```\n\n----------------------------------------\n\nTITLE: Creating Data for Gaussian Mixture Model\nDESCRIPTION: Defines a small dataset with five points to be modeled by a Gaussian mixture model.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = jnp.array([0.0, 1.0, 10.0, 11.0, 12.0])\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro from GitHub\nDESCRIPTION: Installs the latest development version of NumPyro from GitHub using pip.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/bad_posterior_geometry.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro\n```\n\n----------------------------------------\n\nTITLE: Documenting AbsTransform Class in NumPyro\nDESCRIPTION: Autoclass documentation for the AbsTransform class in NumPyro's distributions.transforms module. It includes members, undocumented members, inheritance, and orders members by source.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/source/distributions.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: numpyro.distributions.transforms.AbsTransform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Command to install required Python packages for building the documentation from a requirements file.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/docs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro from GitHub\nDESCRIPTION: This snippet installs the latest version of NumPyro directly from the GitHub repository using pip.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/time_series_forecasting.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro\nDESCRIPTION: Installs the latest development version of NumPyro from GitHub.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/model_rendering.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro\n```\n\n----------------------------------------\n\nTITLE: Installing NumPyro from GitHub\nDESCRIPTION: Installs the NumPyro package directly from its GitHub repository using pip.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/gmm.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Table of Contents for NumPyro Tutorials\nDESCRIPTION: Sets up the table of contents for NumPyro tutorials using the toctree directive. It specifies the maximum depth and includes links to various tutorial files covering topics like Bayesian regression, time series forecasting, and hierarchical models.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/notebooks/source/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :caption: Tutorials:\n\n   bayesian_regression\n   time_series_forecasting\n   lotka_volterra_multiple\n   bayesian_imputation\n   ordinal_regression\n   bayesian_hierarchical_linear_regression\n   discrete_imputation\n   model_rendering\n   truncated_distributions\n```\n\n----------------------------------------\n\nTITLE: NumPyro Citation in BibTeX Format\nDESCRIPTION: BibTeX citation entries for citing NumPyro and Pyro in academic work, including references to the original papers.\nSOURCE: https://github.com/pyro-ppl/numpyro/blob/master/README.md#2025-04-16_snippet_12\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{phan2019composable,\n  title={Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro},\n  author={Phan, Du and Pradhan, Neeraj and Jankowiak, Martin},\n  journal={arXiv preprint arXiv:1912.11554},\n  year={2019}\n}\n```\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{bingham2019pyro,\n  author    = {Eli Bingham and\n               Jonathan P. Chen and\n               Martin Jankowiak and\n               Fritz Obermeyer and\n               Neeraj Pradhan and\n               Theofanis Karaletsos and\n               Rohit Singh and\n               Paul A. Szerlip and\n               Paul Horsfall and\n               Noah D. Goodman},\n  title     = {Pyro: Deep Universal Probabilistic Programming},\n  journal   = {J. Mach. Learn. Res.},\n  volume    = {20},\n  pages     = {28:1--28:6},\n  year      = {2019},\n  url       = {http://jmlr.org/papers/v20/18-403.html}\n}\n```"
  }
]